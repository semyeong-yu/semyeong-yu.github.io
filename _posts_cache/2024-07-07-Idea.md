---
layout: distill
title: Research Idea
date: 2024-07-07 11:00:00
description: Brain Storming
tags: idea
categories: cv-tasks
giscus_comments: false
disqus_comments: true
related_posts: true
# toc:
#   beginning: true
#   sidebar: right
# featured: true
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## NeRF

- iteratively fine sampling하는데, fine sampling할 때마다 rendering에 도움되는 guidance 추가 제공

- 동영상이라면 bg mask 도움 받아서 fg pixel(ray) 위주로 frame 간의 rendering 차이 크게

## Gaussian Splatting

- Mip-NeRF에서는 scale 잘 반영하도록 pre-filtering했고  
Mip-NeRF360에서는 sampling 기법 개선하고 bounded space로 warp했듯이  
결국 scale 고려해서 scene content가 있는 정확한 위치를 잘 찾는 게 포인트!!!  
GS에서도 adaptive density control 기법을 개선하여 scene이 있는 정확한 위치를 찾도록?  
$$\rightarrow$$ rad-splat??

- optical flow deblur를 view synthesis에 적용

- splats가 pixel size만큼 작아서 alpha-blending approx.에 따른 오차를 무시할 수 있다고 하는데, splats size에 따른 퀄리티 차이 조절할 수?

- 4DGS에서는 encoder(HexPlane)으로 time t 정보를 encode했는데, 애초에 d=3 Gaussian 대신 d=4 Gaussian을 써서 특정 time stamp t에서의 Gaussian을 알고 싶다면 4D > 3D projection하는 식으로 할 수 있나?

- unseen view에 대해서도 모든 영역을 잘 recon.하기 위해서는 input image의 view selection에 민감한 것 같은데, view selection을 대충 해도 잘 recon.하는 방법

- DUSt3R, PixelSplat 등 regression model만 쓰면 artifacts 있으니까 ViewCrafter, MVSplat360에서처럼 generative model(Video Diffusion Model)을 써서 refine해야 high-quality output 가능. 대신 recon. 말고 generative. 하려면 VRAM 많이 필요

## Depth Estimation

## Optical Flow

## Super Resolution

## Video Understanding

## Deblur