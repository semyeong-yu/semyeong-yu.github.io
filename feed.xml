<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://semyeong-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://semyeong-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-07-08T09:22:11+00:00</updated><id>https://semyeong-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Solar LLM with Langchain</title><link href="https://semyeong-yu.github.io/blog/2024/LLM/" rel="alternate" type="text/html" title="Solar LLM with Langchain"/><published>2024-07-03T14:00:00+00:00</published><updated>2024-07-03T14:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/LLM</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/LLM/"><![CDATA[<h2 id="solar-llm-by-upstage">Solar LLM by Upstage</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-03-LLM/1-480.webp 480w,/assets/img/2024-07-03-LLM/1-800.webp 800w,/assets/img/2024-07-03-LLM/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-03-LLM/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="solar-llm-as-personalized-llm">Solar LLM as Personalized LLM</h3> <ul> <li> <p>Referenced Github :<br/> <a href="https://github.com/UpstageAI/cookbook/tree/main">UpstageAI-cookbook</a><br/> <a href="https://github.com/semyeong-yu/LLM-cookbook">UpstageAI-cookbook</a><br/> <a href="https://github.com/iamtaewan/solarllm-oracle-cookbook">OracleDB-cookbook</a></p> </li> <li> <p>Solar mini LLM :<br/> small-size<br/> best LLM for fine-tuning<br/> can be used as personalized LLM</p> </li> <li> <p>Future of AI Ecosystem Hierarchy :<br/> Domain-specific and self-fine-tuned LLMs<br/> Solar LLM O/S<br/> O/S<br/> AI chips</p> </li> <li> <p>Langchain :<br/> LLM과 application의 통합을 간소화하는 SDK</p> </li> <li> <p>핵심 기능 : 앞으로 아래에서 배울 예정!!</p> <ul> <li>LLM 사용 (query, context)</li> <li>Groundedness Check (팩트체크)</li> <li>Layout Analyzer (PDF 또는 img에서 정보 추출)</li> <li>Embedding and DB vector store (embedding vector를 DB에 저장)</li> <li>Define Custom Tools (img 생성, 뉴스 검색, 스케쥴 관리 등)</li> </ul> </li> </ul> <h3 id="chat">Chat</h3> <pre><code class="language-Python">from langchain_upstage import ChatUpstage

llm = ChatUpstage()
llm.invoke("What's the best season to get to Korean?") # invoke llm

llm = ChatUpstage(model="solar-1-mini-chat-ja")
llm.invoke("ソーラーLLM、こんにちは。ソーラーLLM、こんにちは。")
</code></pre> <h3 id="few-shot-learning---chain">Few-shot Learning - Chain</h3> <pre><code class="language-Python"># 1. use Chat Prompt Template
from langchain_core.prompts import ChatPromptTemplate

# 2. 농담조로 말하도록 Few-shot Learning
chat_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant."),
        ("human", "What is the capital of France?"),
        ("ai", "I know of it. It's Paris!!"),
        ("human", "What about Korea?"),
    ]
)

# 3. define and invoke chain
from langchain_core.output_parsers import StrOutputParser

chain = chat_prompt | llm | StrOutputParser()
chain.invoke({})
</code></pre> <pre><code class="language-Python"># 1. use Prompt Template
from langchain_core.prompts import PromptTemplate

# 2. 한 번에 답을 내도록 (Standard Prompting) Few-shot Learning
# 한 번에 답을 내려다보니 llm이 답 틀리게 내놓음
prompt_template = PromptTemplate.from_template(
    """
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?

A: The answer is 11.

Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?

A: the answer is
"""
)

# 3. define and invoke chain
chain = prompt_template | llm | StrOutputParser()
chain.invoke({})
</code></pre> <pre><code class="language-Python"># 1. use Prompt Template
from langchain_core.prompts import PromptTemplate

# 2. 설명하면서 답을 내도록 (Chain-of-Thought Prompting) Few-shot Learning
# 설명하면서 답을 내니 llm이 알맞게 답을 내놓음
prompt_template = PromptTemplate.from_template(
    """
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?

A: Roger started with 5 balls. 2 cans of 3 tennis balls
each is 6 tennis balls. 5 + 6 = 11. The answer is 11.

Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?"""
)

# 3. define and invoke chain
chain = prompt_template | llm | StrOutputParser()
chain.invoke({})
</code></pre> <h3 id="zero-shot-learning---chain">Zero-shot Learning - Chain</h3> <pre><code class="language-Python">from langchain_core.prompts import PromptTemplate

# Zero-shot, 즉 예시를 주지 않고
# "Let's think step by step"이라는 마법의 한 문장만 써줬는데도
# 답 잘 내놓음
prompt_template = PromptTemplate.from_template(
    """
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?

A: The answer is 11.

Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?

A: Let's think step by step.
"""
)
chain = prompt_template | llm | StrOutputParser()
chain.invoke({})
</code></pre> <h3 id="divide-and-conquer">Divide-and-Conquer</h3> <ul> <li>Please provide three questions from the following text</li> </ul> <p>보다는</p> <ul> <li>Please extract three keywords from the following text 한 다음<br/> Please provide one question from the following text regarding “Depth up-scaling (DUS)”</li> </ul> <h3 id="prompt-반복">Prompt 반복</h3> <p>python f-string과 비슷한 원리</p> <pre><code class="language-Python">from langchain_core.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template(
    "Tell me a {adjective} joke about {content}."
)
# prompt_template.format(adjective="funny", content="chickens")

chain = prompt_template | llm | StrOutputParser()
chain.invoke({"adjective": "funny", "content": "chickens"})
</code></pre> <h3 id="keep-message-history-in-langchain-prompts">Keep Message History in LangChain Prompts</h3> <p>MessagesPlaceholder</p> <pre><code class="language-Python">from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# General Chat form with Message History
rag_with_history_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant."),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{input}"),
    ]
)

from langchain_core.messages import AIMessage, HumanMessage

# Message History argument
history = [
    HumanMessage("What is the capital of France?"),
    AIMessage("It's Paris!!"),
]

chain = rag_with_history_prompt | llm | StrOutputParser()
chain_result = chain.invoke({"history": history, "input": "What about Korea?"})
print(chain_result)
</code></pre> <h3 id="groundedness-check-with-langchain">Groundedness Check with LangChain</h3> <p>Groundedness Check :<br/> 답(answer)이 주어진 문맥(context)과 일맥상통하는지 (구라가 아닌지) <code class="language-plaintext highlighter-rouge">팩트 체크</code>!</p> <pre><code class="language-Python">from langchain_upstage import UpstageGroundednessCheck

groundedness_check = UpstageGroundednessCheck()

answer = chain.invoke(
    {
        "question": "What is DUS?",
        "Context": context,
    }
)
print("Potential answer: ", answer)

gc_result = groundedness_check.invoke({"context": context, "answer": answer})
print("GC check result: ", gc_result)
if gc_result.lower().startswith("grounded"):
    print("Groundedness check passed")
else:
    print("Groundedness check failed")
</code></pre> <h3 id="pdf-loader-context로-사용">PDF Loader (Context로 사용)</h3> <p>PDF에 있는 내용을 읽어와서 Context로 사용!</p> <pre><code class="language-Python">from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("pdfs/solar_sample.pdf")
docs = loader.load()  # or layzer.lazy_load()
print(docs[0].page_content[:1000])
</code></pre> <h3 id="layout-analyzer-context로-사용">Layout Analyzer (Context로 사용)</h3> <p>LLM이 받아들이기 좋은 형태로 문서를 읽기 위해<br/> Extract layouts, tables, and figures from any document to .html file<br/> Maximize RAG performance (RAG는 이후에 설명 예정)</p> <pre><code class="language-Python">! pip3 install -qU  markdownify  langchain-upstage  requests
import os
from langchain_upstage import UpstageLayoutAnalysisLoader
from IPython.display import display, HTML

os.environ["UPSTAGE_API_KEY"] = "UPSTAGE_API_KEY"

loader = UpstageLayoutAnalysisLoader("invoice.png", split="page", use_ocr=True)
# For improved memory efficiency, consider using the lazy_load method to load documents page by page.
pages = loader.load()  # or loader.lazy_load()
for page in pages:
    print(page)

loader = UpstageLayoutAnalysisLoader("pdfs/solar_sample.pdf", output_type="html")
docs = layzer.load() # or loader.lazy_load()
display(HTML(docs[0].page_content[:5000]))
</code></pre> <h3 id="rag-retrieval-augmented-generation">RAG: Retrieval Augmented Generation</h3> <ul> <li>RAG (Retrieval Augmented Generation) : <ul> <li><code class="language-plaintext highlighter-rouge">pdf, html 등 주어진 파일에서 query와 관련 있는 부분만 검색해서 context로서 사용</code>!</li> <li>Large language models (LLMs) have a limited context size</li> <li>Not all context is relevant to a given question</li> <li><code class="language-plaintext highlighter-rouge">Relevant context is retrieved(검색) from external data sources</code> and added to the prompt</li> <li>LLM generates a response based on this augmented context prompt</li> <li>RAG is particularly useful for Question Answering on custom datasets</li> <li>Query \(\rightarrow\) Retrieve (Search) \(\rightarrow\) Augmented Prompt \(\rightarrow\) LLM \(\rightarrow\) Answer</li> </ul> </li> <li>Chunking, Splitting : <ul> <li>Fixed-size chunking : split text into equal-sized chunks based on character or token count</li> <li>Semantic chunking : split text based on semantic boundaries like sentences, paragraphs, or sections</li> <li>Hierarchical chunking : create chunks at multiple levels of granularity (The ideal chunk size depends on the embedding model, retrieval use-case, and downstream task)</li> </ul> </li> </ul> <pre><code class="language-Python">from langchain_upstage import UpstageLayoutAnalysisLoader
from langchain_community.retrievers import BM25Retriever
from langchain_text_splitters import (
    Language,
    RecursiveCharacterTextSplitter,
)

layzer = UpstageLayoutAnalysisLoader(
    "pdfs/kim-tse-2008.pdf", use_ocr=True, output_type="html"
)
docs = layzer.load()

text_splitter = RecursiveCharacterTextSplitter.from_language(
    chunk_size=1000, chunk_overlap=100, language=Language.HTML
)
splits = text_splitter.split_documents(docs)

retriever = BM25Retriever.from_documents(splits)

query = "What is bug classficiation?"
context_docs = retriever.invoke("bug") # keyword search
chain.invoke({"question": query, "Context": context_docs})
</code></pre> <h3 id="keyword-search-대신-semantic-search-with-embedding-space">Keyword Search 대신 Semantic Search with Embedding space</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-03-LLM/2-480.webp 480w,/assets/img/2024-07-03-LLM/2-800.webp 800w,/assets/img/2024-07-03-LLM/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-03-LLM/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Solar-Embedding-1-Large (v1.0)<br/> Convert unstructured text data into embedding vectors</p> <pre><code class="language-Python">from langchain_upstage import UpstageEmbeddings
from langchain_chroma import Chroma
from langchain_upstage import UpstageEmbeddings
from langchain.docstore.document import Document

embeddings_model = UpstageEmbeddings(
  api_key="UPSTAGE_API_KEY", 
  model="solar-embedding-1-large"
)
embeddings = embeddings_model.embed_documents(
    [
        "What is the best season to visit Korea?",
    ]
)
query_result = embeddings.embed_query("What does Sam do?") # vector

sample_text_list = [
    "Korea is a beautiful country to visit in the spring.",
    "The best time to visit Korea is in the fall.",
    "Best way to find bug is using unit test.",
    "Python is a great programming language for beginners.",
    "Sung Kim is a great teacher.",
    "맛있는 좋은 과일을 많이 먹어 볼까?"
]

sample_docs = [Document(page_content=text) for text in sample_text_list]

vectorstore = Chroma.from_documents(
    documents=sample_docs,
    embedding=UpstageEmbeddings(model="solar-embedding-1-large"),
)

retriever = vectorstore.as_retriever()
result_docs = retriever.invoke("When to visit Korea?")
print(result_docs[0].page_content[:100])
</code></pre> <h3 id="rag-summary">RAG Summary</h3> <ol> <li>load doc</li> <li>chunking, splits</li> <li>embedding, indexing (vector store)</li> <li>retrieval, augmenting context (find Top-k most similar doc chunks in vector store with the query embedding)</li> <li>invoke chain based on the augmented context by retrieval</li> </ol> <pre><code class="language-Python">from langchain_upstage import UpstageLayoutAnalysisLoader
from langchain_text_splitters import (
    Language,
    RecursiveCharacterTextSplitter,
)
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_upstage import ChatUpstage

# 1. load doc
layzer = UpstageLayoutAnalysisLoader("pdfs/kim-tse-2008.pdf", output_type="html")
docs = layzer.load()

# 2. chunking &amp; splits
text_splitter = RecursiveCharacterTextSplitter.from_language(
    chunk_size=1000, chunk_overlap=100, language=Language.HTML
)
splits = text_splitter.split_documents(docs)

# 3. Embedding &amp; indexing
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=UpstageEmbeddings(model="solar-embedding-1-large"),
)

# 4. retrieve
retriever = vectorstore.as_retriever()
result_docs = retriever.invoke("What is Bug Classification?")
print(len(result_docs))
print(result_docs[0].page_content[:100])

# 5. invoke chain based on the augmented context by retrieval 
llm = ChatUpstage()

prompt_template = PromptTemplate.from_template(
    """
    Please provide most correct answer from the following context. 
    If the answer is not present in the context, please write "The information is not present in the context."
    ---
    Question: {question}
    ---
    Context: {Context}
    """
)

chain = prompt_template | llm | StrOutputParser()
chain.invoke({"question": "What is bug classficiation?", "Context": result_docs})
</code></pre> <h3 id="oracle-db를-persistent-memory로-쓰기">Oracle DB를 persistent memory로 쓰기</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-03-LLM/3-480.webp 480w,/assets/img/2024-07-03-LLM/3-800.webp 800w,/assets/img/2024-07-03-LLM/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-03-LLM/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>매번 100만 개의 pdf를 load해서 index할 수는 없는 일!</p> <ol> <li>load doc</li> <li>chunking, splits</li> <li>embedding, indexing (<code class="language-plaintext highlighter-rouge">vector store with Oracle DB</code>)</li> <li>retrieval, augmenting context</li> <li>split한 text가 이미 Oracle DB vector store에 있는지 체크</li> <li>없다면 embedding, indexing again</li> <li>invoke chain based on the augmented context by retrieval</li> </ol> <pre><code class="language-Python">! pip3 install -qU  markdownify  langchain-upstage rank_bm25

import oracledb
from langchain_upstage import UpstageLayoutAnalysisLoader

# 0. connect to Oracle DB
username=os.environ["DB_USER"]
password=os.environ["DB_PASSWORD"]
dsn=os.environ["DSN"]

con = oracledb.connect(user=username, password=password, dsn=dsn)

try: 
    conn23c = oracledb.connect(user=username, password=password, dsn=dsn)
    print("Connection successful!", conn23c.version)
except Exception as e:
    print("Connection failed!")

# 1. load doc
layzer = UpstageLayoutAnalysisLoader("pdfs/kim-tse-2008.pdf", output_type="html")
docs = layzer.load()

# 2. chunking &amp; splits
text_splitter = RecursiveCharacterTextSplitter.from_language(
    chunk_size=1000, chunk_overlap=100, language=Language.HTML
)
splits = text_splitter.split_documents(docs)

# check if text is in the vector store
def is_in_vectorstore(vectorstore, text):
    search_results = vectorstore.get(ids=[text])
    if search_results and search_results["ids"]:
        return True
    else:
        return False

# 3. Embedding &amp; indexing 방법 1.
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=UpstageEmbeddings(model="solar-embedding-1-large"),
)

# 3. Embedding &amp; indexing 방법 2.

knowledge_base = OracleVS.from_documents(docs, upstage_embeddings, client=conn23c, table_name="text_embeddings2", distance_strategy=DistanceStrategy.DOT_PRODUCT)

# result_chunks = knowledge_base.similarity_search(user_question)

vectorstore = OracleVS(client=conn23c, embedding_function=upstage_embeddings, table_name="text_embeddings2", distance_strategy=DistanceStrategy.DOT_PRODUCT) # create vector store

oraclevs.create_index(
    client=conn23c,
    vector_store=vectorstore,
    params={
        "idx_name": "ivf_idx1",
        "idx_type": "IVF",
    },
) # index 추가

# 4. retrieve
retriever = vectorstore.as_retriever()

unique_splits = [
    split for split in splits if not is_in_vectorstore(vectorstore, split.page_content)
]
print(len(unique_splits))

# 5. split한 text가 이미 Oracle DB vector store에 있는지 체크
# 6. 없다면 embedding, indexing again
if len(unique_splits) &gt; 0:
    vectorstore = Chroma.from_documents(
        ids=[split.page_content for split in unique_splits],
        persist_directory="./chroma_db",
        documents=unique_splits,
        embedding=UpstageEmbeddings(model="solar-embedding-1-large"),
)

# 7. invoke chain based on the augmented context by retrieval 
llm = ChatUpstage()

prompt_template = PromptTemplate.from_template(
    """
    Please provide most correct answer from the following context. 
    If the answer is not present in the context, please write "The information is not present in the context."
    ---
    Question: {question}
    ---
    Context: {Context}
    ---
    Output: please, response in Korean
    """
)

chain = ({"context": retriever, "question": RunnablePassthrough()} | prompt_template | llm | StrOutputParser())
response = chain.invoke("What is bug classficiation?")
</code></pre> <h3 id="smart-rag">Smart RAG</h3> <p>local vector store에 검색했을 때</p> <ul> <li>내가 아는 건 <code class="language-plaintext highlighter-rouge">local RAG</code> 로 처리</li> <li>내가 모르는 건 <code class="language-plaintext highlighter-rouge">external search</code> 로 처리</li> </ul> <pre><code class="language-Python"># 주어진 context만으로 주어진 question에 답변할 수 있는지 판단
# RAG or Search?
def is_in(question, context):
    is_in_conetxt = """As a helpful assistant, 
please use your best judgment to determine if the answer to the question is within the given context. 
If the answer is present in the context, please respond with "yes". 
If not, please respond with "no". 
Only provide "yes" or "no" and avoid including any additional information. 
Please do your best. Here is the question and the context:
---
CONTEXT: {context}
---
QUESTION: {question}
---
OUTPUT (yes or no):"""

    is_in_prompt = PromptTemplate.from_template(is_in_conetxt)
    chain = is_in_prompt | ChatUpstage() | StrOutputParser()

    response = chain.invoke({"context": context, "question": question})
    print(response)
    return response.lower().startswith("yes")
</code></pre> <pre><code class="language-Python"># Smart RAG, Self-Improving RAG
import os
from tavily import TavilyClient

def smart_rag(question, context):
    if not is_in(question, context):
        print("Searching in tavily")
        tavily = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])
        context = tavily.search(query=question)

    chain = prompt_template | llm | StrOutputParser()
    return chain.invoke({"context": context, "question": question})
</code></pre> <pre><code class="language-Python">smart_rag("What is DUS?", solar_summary)
# 질문에 대한 답변이 solar_summary에 있는 내용이므로 RAG  
# yes  
# 'The answer to the question "What is DUS?" is:\n\nDepth Up-Scaling (DUS)'
</code></pre> <pre><code class="language-Python">smart_rag("How to get to Seoul from SF?", solar_summary)
# solar_summary에 없는 내용이므로 Search  
# no  
# Searching in tavily
# 'The answer to "How to get to Seoul from SF?" is:\n\n1. Fly from San Francisco (SFO) to Seoul (ICN) with airlines such as ANA, Japan Airlines, Asiana Airlines, Korean Air, and United Airlines.\n2. Take a train from Incheon Int\'l Airport T1 to Seoul Station.\n3. Take the BART from Civic Center / UN Plaza to Milpitas and then fly from San Jose (SJC) to Incheon (ICN).\n\nPlease note that the cheapest flights from San Francisco to Seoul start at $453 with AIR PREMIA.'
</code></pre> <h3 id="smart-rag-with-tools">Smart RAG with Tools</h3> <ol> <li>Define <code class="language-plaintext highlighter-rouge">Custom Tools</code></li> <li>Create a list of tools</li> <li>Bind the tools to LLM</li> </ol> <p>특정 task (산수 계산 혹은 뉴스기사 검색 등) 맞춤형으로<br/> custom tools를 정의함으로써<br/> LLM 답변의 질을 높일 수 있음!</p> <pre><code class="language-Python">! pip3 install -qU  markdownify  langchain-upstage rank_bm25

from langchain_core.tools import tool
import requests
import os
from tavily import TavilyClient

# external API to search
tavily = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])

# 1. Define Custom Tools
@tool
def add(a: int, b: int) -&gt; int:
    """Adds a and b."""
    return a + b

@tool
def solar_paper_search(query: str) -&gt; str:
    """Query for research paper about solarllm, dus, llm and general AI.
    If the query is about DUS, Upstage, AI related topics, use this.
    """
    return solar_summary

@tool
def internet_search(query: str) -&gt; str:
    """This is for query for internet search engine like Google.
    Query for general topics.
    """
    return tavily.search(query=query)

@tool
def get_news(topic: str) -&gt; str:
    """Get latest news about a topic.
    If users are more like recent news, use this.
    """
    # https://newsapi.org/v2/everything?q=tesla&amp;from=2024-04-01&amp;sortBy=publishedAt&amp;apiKey=API_KEY
    # change this to request news from a real API
    news_url = f"https://newsapi.org/v2/everything?q={topic}&amp;apiKey={os.environ['NEWS_API_KEY']}"
    respnse = requests.get(news_url)
    return respnse.json()

# 2. Create a list of tools 
tools = [add, solar_paper_search, internet_search, get_news]

# 3. Bind the tools to LLM
llm_with_tools = llm.bind_tools(tools)

llm_with_tools.invoke("What is Solar LLM?").tool_calls
# 출력 : [{'name': 'solar_paper_search', 'args': {'query': 'Solar LLM'}, 'id': 'cb1687d2-7c6a-45dc-8287-19376c335cd4'}]
llm_with_tools.invoke("What's best place in Seoul?").tool_calls
# 출력 : [{'name': 'internet_search', 'args': {'query': 'best place in Seoul'}, 'id': '1f86d563-de15-460a-abc0-0e644e284518'}]
</code></pre> <pre><code class="language-Python"># Smart RAG, Self-Improving RAG
import os
from tavily import TavilyClient

def call_tool_func(tool_call):
    tool_name = tool_call["name"].lower()
    if tool_name not in globals():
        print("Tool not found", tool_name)
        return None
    selected_tool = globals()[tool_name]
    return selected_tool.invoke(tool_call["args"])

def tool_rag(question):
    for _ in range(3): # try 3 times
        tool_calls = llm_with_tools.invoke(question).tool_calls
        if tool_calls:
            break
        else:
            print("try again")

    if not tool_calls:
        return "I'm sorry, I don't have an answer for that."
    
    print(tool_calls)
    context = ""
    for tool_call in tool_calls:
        context += str(call_tool_func(tool_call))

    chain = prompt_template | llm | StrOutputParser()
    return chain.invoke({"context": context, "question": question})

tool_rag("What is Solar llm?")
# 출력 : [{'name': 'solar_paper_search', 'args': {'query': 'What is Solar llm?'}, 'id': 'cb291b01-a1aa-4839-84a8-a473f4eb0920'}] 'Solar llm is a large language model (LLM) with 10.7 billion parameters.'
tool_rag("What is news about Tesla?")
# 출력 : [{'name': 'get_news', 'args': {'topic': 'Tesla'}, 'id': 'aade5002-b9e2-4a23-92d7-fd66f12cfeb6'}] "The news about Tesla is that the company has issued a voluntary recall for nearly 4,000 Cybertrucks due to a fault with the accelerator pedal that could get trapped, pushing the car to full speed."
</code></pre> <h3 id="fine-tuning-with-predibase">Fine-tuning with Predibase</h3> <p>CFT (Continued Fine-Tuning) : feedback database에 기반하여 계속 fine-tuning</p> <pre><code class="language-Python">adapter = pb.adapters.create(
  config=FinetuningConfig(
    base_model = "solar-1-mini-chat-240612",
    epochs = 1, # default: 3
    rank = 1, # default: 16
  ),
  dataset = pb_dataset, # also accepts the dataset name as str
  repo = repo,
  description = "initial model with defaults"
)
</code></pre>]]></content><author><name></name></author><category term="generative"/><category term="generative"/><category term="LLM"/><summary type="html"><![CDATA[Upstage Solar LLM as Personalized LLM]]></summary></entry><entry><title type="html">3D Rotation-Quaternion</title><link href="https://semyeong-yu.github.io/blog/2024/Quaternion/" rel="alternate" type="text/html" title="3D Rotation-Quaternion"/><published>2024-07-01T14:00:00+00:00</published><updated>2024-07-01T14:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Quaternion</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Quaternion/"><![CDATA[<h2 id="lecture-06-3d-rotations-and-complex-representations-cmu-15-462662">Lecture 06: 3D Rotations and Complex Representations (CMU 15-462/662)</h2> <blockquote> <p>referenced video :<br/> <a href="https://www.youtube.com/watch?v=YF5ZUlKxSgE&amp;list=PL9_jI1bdZmz2emSh0UQ5iOdT2xRHFHL7E&amp;index=7">3D Rotations and Quaternion</a><br/> referenced blog :<br/> <a href="https://blog.naver.com/hblee4119/223188806834">Quaternion</a></p> </blockquote> <h2 id="3d-rotation">3D Rotation</h2> <ul> <li>2D rotation에서는 order of rotations 노상관, but<br/> 3D rotation에서는 <code class="language-plaintext highlighter-rouge">order of rotations 중요</code></li> </ul> <h2 id="gimbal-lock">Gimbal Lock</h2> <ul> <li>Gimbal Lock :<br/> Euler angles \(\theta_{x}, \theta_{y}, \theta_{z}\) 로 회전시킬 때 두 축이 맞물려서 <code class="language-plaintext highlighter-rouge">한 축이 소실</code>되는 상황</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-Quaternion/2-480.webp 480w,/assets/img/2024-07-01-Quaternion/2-800.webp 800w,/assets/img/2024-07-01-Quaternion/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-Quaternion/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 1 -&gt; 2번째 그림 : x축(초록) 회전 / 2 -&gt; 3번째 그림 : z축(파랑) 회전 / 3 -&gt; 4번째 그림 : y축(빨강) 회전 </div> <ul> <li>위의 그림에 따르면 Euler angles는 \(x\)(초록), \(y\)(빨강), \(z\)(파랑) 순으로 <code class="language-plaintext highlighter-rouge">상속관계</code>여서<br/> \(x\)축(초록)을 회전시키면 그의 자식들인 \(y, z\)축(빨강, 파랑)도 같이 회전한다.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-Quaternion/3-480.webp 480w,/assets/img/2024-07-01-Quaternion/3-800.webp 800w,/assets/img/2024-07-01-Quaternion/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-Quaternion/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>이 때, <code class="language-plaintext highlighter-rouge">Gimbal Lock</code>은 위의 그림과 같이<br/> <code class="language-plaintext highlighter-rouge">상속관계에서의 2번째 축(빨강)이 -90도 혹은 90도 회전</code>했을 때<br/> <code class="language-plaintext highlighter-rouge">상속관계에서의 1번째 축(초록)과 3번째 축(파랑)이 겹쳐서</code> 같은 방향으로 회전하기 때문에 발생한다.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-Quaternion/1-480.webp 480w,/assets/img/2024-07-01-Quaternion/1-800.webp 800w,/assets/img/2024-07-01-Quaternion/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-Quaternion/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>예를 들어, 만약 \(\theta_{y} = \frac{\pi}{2}\) 로 고정한다면<br/> \(R_x R_y R_z = \begin{bmatrix} 0 &amp; 0 &amp; 1 \\ sin(\theta_{x}+\theta_{z}) &amp; cos(\theta_{x}+\theta_{z}) &amp; 0 \\ - cos(\theta_{x}+\theta_{z}) &amp; sin(\theta_{x}+\theta_{z}) &amp; 0 \end{bmatrix}\)<br/> 이므로 \(\theta_{x}, \theta_{z}\) 값(자유도=2)과 관계없이 <code class="language-plaintext highlighter-rouge">특정 하나의 axis에 대한 회전(자유도=1)으로 제약 생겨버림</code>!</li> </ul> <h2 id="quaternion">Quaternion</h2> <ul> <li> <p>Euler angles vs Quaternion :<br/> Euler angles는 상속관계이므로 한 번에 계산이 불가능하여 순서대로 회전시켜야 하고, 짐벌락 현상이 발생할 수 있지만<br/> Quaternion은 <code class="language-plaintext highlighter-rouge">한 번에 계산 가능</code>하여 <code class="language-plaintext highlighter-rouge">동시에 회전</code>시킬 수 있으며, 짐벌락 현상이 없다!</p> </li> <li>2D rotation : <ul> <li>real, rectangular form : 2D rotation matrix 복잡</li> <li>complex, polar form : 단순히 크기 곱하고, 각도 더하고!</li> </ul> </li> <li>3D rotation : <ul> <li>real, xyz form : 3D rotation matrix 복잡</li> <li>quaternion : only need <code class="language-plaintext highlighter-rouge">FOUR</code> coordinates!(one real, three imaginary)<br/> \(H\) = span(\(\{1, i, j, k\}\))<br/> \(q = a + bi + cj + dk \in H\)<br/> \(i^2 = j^2 = k^2 = ijk = -1\) \(\leftarrow\) <code class="language-plaintext highlighter-rouge">new property!</code><br/> \(ij = k\), \(ji = -k\)<br/> \(jk = i\), \(kj = -i\)<br/> \(ki = j\), , \(ik = -j\)</li> </ul> </li> <li>Quaternion : <ul> <li>distributive and associative</li> <li><code class="language-plaintext highlighter-rouge">not commutative</code> : \(qp \neq pq\) for \(q, p \in H\)</li> <li>quaternion is <code class="language-plaintext highlighter-rouge">a pair of scalar and vector</code><br/> \(q = a + bi + cj + dk\)<br/> \(= (a, \boldsymbol u) = (a, (b, c, d)) \in H\)<br/> where \(a \in Re(H) = R\) and \(\boldsymbol u \in Im(H) = R^3\)</li> <li><code class="language-plaintext highlighter-rouge">quaternion product</code> :<br/> \((a, \boldsymbol u)(b, \boldsymbol v) = (ab - \boldsymbol u \cdot \boldsymbol v, a \boldsymbol v + b \boldsymbol u + \boldsymbol u \times \boldsymbol v)\)<br/> \(\boldsymbol u \boldsymbol v = \boldsymbol u \times \boldsymbol v - \boldsymbol u \cdot \boldsymbol v\)</li> <li><code class="language-plaintext highlighter-rouge">quaternion conjugate</code> :<br/> \(q = (w, x, y, z)\)<br/> \(q^{\ast} = (w, -x, -y, -z)\)<br/> \(\| q \| = \sqrt{w^2 + x^2 + y^2 + z^2}\)<br/> \(q \cdot q^{\ast} = (w, x, y, z) \cdot (w, -x, -y, -z) = w^2 + x^2 + y^2 + z^2 = \| q \|^2\)<br/> \(q^{-1} = \frac{q^{\ast}}{\| q \|^2} = \frac{q^{\ast}}{q \cdot q^{\ast}} = \frac{1}{q}\)<br/> \((q_1 q_2)^{\ast} = q_2^{\ast} q_1^{\ast}\)</li> </ul> </li> <li>3D Transformations via Quaternions : <ul> <li><code class="language-plaintext highlighter-rouge">3D Rotation</code> : \(q x \bar q\) \(\leftrightarrow\) \(x\)를 \(u\)에 대해 \(\theta\)만큼 회전<br/> for \(q = cos(\frac{\theta}{2}) + sin(\frac{\theta}{2})u\)<br/> where pure imaginary 3D vector \(x, u \in Im(H) = R^3\)<br/> where unit quaternion \(q \in H = (R, R^3)\) where \(\| q \|^2 = 1\)<br/> where \(\bar q\) 는 \(q\)의 conjugate</li> <li><code class="language-plaintext highlighter-rouge">Interpolating Rotation</code> :<br/> interpolating Euler angles는 strange-looking paths 및 non-uniform rotation speed를 야기할 수 있음<br/> 대신 Quaternion으로 나타내면,<br/> <code class="language-plaintext highlighter-rouge">spherical linear interpolation (SLERP)</code> :<br/> Slerp(\(q_0, q_1, t\)) = \(q_0(q_0^{-1} q_1)^t\)<br/> where \(t \in [0, 1]\)</li> <li>Generating Coordinates for <code class="language-plaintext highlighter-rouge">Texture Maps</code> :<br/> (hyper)complex numbers는 <code class="language-plaintext highlighter-rouge">angle-preserving(conformal)</code> maps에 쓰임!<br/> texture에서 angle-preserving 특성은 사람 눈으로 보기에 매우 그럴 듯하게 보이게 함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-Quaternion/4-480.webp 480w,/assets/img/2024-07-01-Quaternion/4-800.webp 800w,/assets/img/2024-07-01-Quaternion/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-Quaternion/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Beyond Quaternion … :<br/> <code class="language-plaintext highlighter-rouge">Lie algebras</code> and <code class="language-plaintext highlighter-rouge">Lie Groups</code> 으로도 3D rotations를 나타낼 수 있으며,<br/> 특히 <code class="language-plaintext highlighter-rouge">statistics(averages) of rotations</code> 를 구할 때 매우 유용!<br/> <code class="language-plaintext highlighter-rouge">exponential map</code> : axis/angle \(\rightarrow\) rotation matrix<br/> <code class="language-plaintext highlighter-rouge">logarithmic map</code> : rotation matrix \(\rightarrow\) axis/angle</p> </li> <li> <p>4 \(\times\) 1 <code class="language-plaintext highlighter-rouge">quaternion</code> \(q\) 으로 3 \(\times\) 3 <code class="language-plaintext highlighter-rouge">rotation matrix</code> 만드는 방법 : <a href="https://github.com/graphdeco-inria/gaussian-splatting/blob/b2ada78a779ba0455dfdc2b718bdf1726b05a1b6/utils/general_utils.py#L78">build_rotation(r)</a></p> <pre><code class="language-Python">def build_rotation(r):
  norm = torch.sqrt(r[:,0]*r[:,0] + r[:,1]*r[:,1] + r[:,2]*r[:,2] + r[:,3]*r[:,3])
  q = r / norm[:, None] # use normalized quaternion

  R = torch.zeros((q.size(0), 3, 3), device='cuda')

  r = q[:, 0]
  x = q[:, 1]
  y = q[:, 2]
  z = q[:, 3]

  R[:, 0, 0] = 1 - 2 * (y*y + z*z)
  R[:, 0, 1] = 2 * (x*y - r*z)
  R[:, 0, 2] = 2 * (x*z + r*y)
  R[:, 1, 0] = 2 * (x*y + r*z)
  R[:, 1, 1] = 1 - 2 * (x*x + z*z)
  R[:, 1, 2] = 2 * (y*z - r*x)
  R[:, 2, 0] = 2 * (x*z - r*y)
  R[:, 2, 1] = 2 * (y*z + r*x)
  R[:, 2, 2] = 1 - 2 * (x*x + y*y)
  return R
</code></pre> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="quaternion"/><category term="rotation"/><summary type="html"><![CDATA[Quaternion for Rotation Matrix]]></summary></entry><entry><title type="html">3D Gaussian Splatting</title><link href="https://semyeong-yu.github.io/blog/2024/GS/" rel="alternate" type="text/html" title="3D Gaussian Splatting"/><published>2024-07-01T10:00:00+00:00</published><updated>2024-07-01T10:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/GS</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/GS/"><![CDATA[<h2 id="3d-gaussian-splatting-for-real-time-radiance-field-rendering">3D Gaussian Splatting for Real-Time Radiance Field Rendering</h2> <h4 id="bernhard-kerbl-georgios-kopanas-thomas-leimkühler-george-drettakis">Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, George Drettakis</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2308.04079">https://arxiv.org/abs/2308.04079</a><br/> project website :<br/> <a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/</a><br/> code :<br/> <a href="https://github.com/graphdeco-inria/gaussian-splatting">https://github.com/graphdeco-inria/gaussian-splatting</a><br/> referenced blog :<br/> <a href="https://xoft.tistory.com/51">https://xoft.tistory.com/51</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>DD</li> </ol> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-GS/2-480.webp 480w,/assets/img/2024-07-01-GS/2-800.webp 800w,/assets/img/2024-07-01-GS/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-GS/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="abstract">Abstract</h2> <ul> <li>novel 3D Gaussian scene representation with real-time differentiable renderer<br/> <code class="language-plaintext highlighter-rouge">수많은 3D Gaussian이 모여 scene을 구성</code>하고 있다!</li> <li>Very Fast rendering (\(\geq\) 100 FPS)</li> <li>Higher Quality than SOTA Mip-NeRF360(2022)</li> <li>Faster Training than SOTA InstantNGP(2022)</li> </ul> <h2 id="introduction">Introduction</h2> <h3 id="why-3d-gaussian">Why 3D Gaussian?</h3> <p>3D scene representation 방법</p> <ol> <li><code class="language-plaintext highlighter-rouge">Mesh or Point</code> <ul> <li>explicit</li> <li>good for fast GPU/CUDA-based rasterization(3D \(\rightarrow\) 2D)</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">NeRF</code> method <ul> <li>implicit (MLP)</li> <li>ray marching</li> <li>continuous coordinate-based representation</li> <li>interpolate values stored in voxels, hash grids, or points</li> <li>But,,, <code class="language-plaintext highlighter-rouge">stochastic sampling</code> for rendering 때문에 <code class="language-plaintext highlighter-rouge">연산량이 많고 noise</code> 생김</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">3D Gaussian</code> method <ul> <li>explicit</li> <li>differentiable volumetric representation</li> <li>efficient rasterization(projection and \(\alpha\)-blending)</li> </ul> </li> </ol> <h3 id="rendering-nerf-vs-3dgs">Rendering (NeRF vs 3DGS)</h3> <ul> <li>NeRF : <ul> <li>ray per pixel 쏴서 coarse(stratified) and fine(PDF) sampling하고,</li> <li>MLP로 sampled points의 color 및 volume density를 구하고,</li> <li>이 값들을 volume rendering 식으로 summation</li> </ul> </li> <li>3DGS : <ul> <li>image를 tile(14 \(\times\) 14 pixel)들로 나누고,</li> <li>tile마다 Gaussian을 Depth에 따라 정렬한 뒤</li> <li>앞에서부터 뒤로 \(\alpha\)-blending</li> </ul> </li> </ul> <h2 id="related-work">Related Work</h2> <p>생략 (추후에 다시 볼 수도)</p> <h2 id="overview">Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-GS/1-480.webp 480w,/assets/img/2024-07-01-GS/1-800.webp 800w,/assets/img/2024-07-01-GS/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-GS/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For unbounded and complete scenes,<br/> For 1080p high resolution and real-time(\(\geq\) 30 fps) rendering,</p> <ol> <li><code class="language-plaintext highlighter-rouge">input</code> : <ul> <li>Most point-based methods require <code class="language-plaintext highlighter-rouge">MVS</code>(Multi-View Stereo) data,<br/> but 3DGS only needs <code class="language-plaintext highlighter-rouge">SfM points</code> for initialization</li> <li>COLMAP 등 SfM(Structure-from-Motion) camera calibration으로 얻은 <code class="language-plaintext highlighter-rouge">sparse point cloud</code>에서 시작해서<br/> scene을 3D Gaussians로 나타냄으로써<br/> <code class="language-plaintext highlighter-rouge">empty space에서의 불필요한 계산을 하지 않도록</code> continuous volumetric radiance fields 정보를 저장</li> <li>NeRF-synthetic dataset의 경우 3DGS 는 random initialization으로도 좋은 퀄리티 달성</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">optimization</code> interleaved with <code class="language-plaintext highlighter-rouge">adaptive density control</code> : <ul> <li>optimize 4 parameters :<br/> 3D position(mean), anisotropic covariance, opacity, and spherical harmonic coeff.(color)<br/> <code class="language-plaintext highlighter-rouge">highly anisotropic volumetric splats</code>는 <code class="language-plaintext highlighter-rouge">fine structures</code>를 compact하게 나타낼 수 있음!!<br/> <code class="language-plaintext highlighter-rouge">spherical harmonics</code>를 통해 <code class="language-plaintext highlighter-rouge">directional appearance(color)</code>를 잘 나타낼 수 있음!!<d-cite key="Plenoxels">[1]</d-cite><d-cite key="InstantNGP">[2]</d-cite></li> <li>adaptive density control :<br/> gradient 기반으로 Gaussian 형태를 변화시키기 위해, add and occasionally remove 3D Gaussians during optimization</li> </ul> </li> <li>differentiable visibility-aware <code class="language-plaintext highlighter-rouge">real-time rendering</code> :<br/> perform \(\alpha\)-blending of <code class="language-plaintext highlighter-rouge">anisotropic splats</code> respecting visibility order<br/> by fast <code class="language-plaintext highlighter-rouge">GPU sorting</code> algorithm and <code class="language-plaintext highlighter-rouge">tile-based rasterization</code>(projection and \(\alpha\)-blending)<br/> 한편, accumulated \(\alpha\) values를 tracking함으로써 <code class="language-plaintext highlighter-rouge">Gaussians 수에 제약 없이</code> 빠른 backward pass도 가능</li> </ol> <hr/> <h3 id="pseudo-code">Pseudo-Code</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-GS/3-480.webp 480w,/assets/img/2024-07-01-GS/3-800.webp 800w,/assets/img/2024-07-01-GS/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-GS/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>빨간 박스 : initialization<br/> 파란 박스 : optimization<br/> 초록 박스 : 특정 iter.마다 Gaussian을 clone, split, remove</p> <h2 id="differentiable-3d-gaussian-splatting">Differentiable 3D Gaussian Splatting</h2> <h3 id="3d-gaussian">3D Gaussian</h3> <ul> <li> <p><code class="language-plaintext highlighter-rouge">differentiable</code> volumetric representation의 특성을 가지고 있으면서도 빠른 rendering을 위해 <code class="language-plaintext highlighter-rouge">unstructured and explicit</code>한 게 무엇이 있을까?<br/> \(\rightarrow\) 3D Gaussian !!</p> </li> <li> <p>a point를 a small planar circle with a normal이라고 가정하는 이전 Point-based rendering 논문들 <d-cite key="Point1">[3]</d-cite> <d-cite key="Point2">[4]</d-cite> 과 달리<br/> <code class="language-plaintext highlighter-rouge">SfM points는 sparse해서 normals(법선)를 estimate하기 어려울</code> 뿐만 아니라, estimate 한다 해도 very noisy normals를 optimize하는 것은 매우 어렵<br/> \(\rightarrow\) normals 필요 없는 3D Gaussians !!<br/> k-dim. Gaussian : \(G(\boldsymbol x) = (2\pi)^{-\frac{k}{2}}det(\Sigma)^{-\frac{1}{2}}e^{-\frac{1}{2}(\boldsymbol x - \boldsymbol \mu)^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu)}\)</p> </li> </ul> <h2 id="parameters-to-train">Parameters to train</h2> <ol> <li><code class="language-plaintext highlighter-rouge">scale vector</code> \(s\) and <code class="language-plaintext highlighter-rouge">quaternion</code> \(q\) for <code class="language-plaintext highlighter-rouge">covariance matrix</code></li> <li><code class="language-plaintext highlighter-rouge">spherical harmonics</code>(SH) coeff. for <code class="language-plaintext highlighter-rouge">color</code></li> <li><code class="language-plaintext highlighter-rouge">opacity</code> \(\alpha\)</li> <li><code class="language-plaintext highlighter-rouge">3D position</code> for <code class="language-plaintext highlighter-rouge">mean</code></li> </ol> <h3 id="parameter-1-covariance-matrix">Parameter 1. Covariance matrix</h3> <blockquote> <p>covariance matrix and scale vector(scale) and quaternion(rotation)</p> </blockquote> <ul> <li>covariance matrix는 positive semi-definite \(x^T M x \geq 0\) for all \(x \in R^n\)이어야만 physical meaning을 가지는데,<br/> \(\Sigma\) 를 직접 바로 optimize하면 invalid covariance matrix가 될 수 있음<br/> 그렇다면!!</li> </ul> <p>\(\Sigma\) 가 <code class="language-plaintext highlighter-rouge">symmetric</code> and <code class="language-plaintext highlighter-rouge">positive semi-definite</code>이도록 \(\Sigma = R S S^T R^T\) 로 정의해서<br/> \(\Sigma\) 대신 <code class="language-plaintext highlighter-rouge">x,y,z-axis scale</code>을 나타내는 <code class="language-plaintext highlighter-rouge">3D vector</code> \(s\) 와 <code class="language-plaintext highlighter-rouge">rotation</code>을 나타내는 <code class="language-plaintext highlighter-rouge">4D quaternion</code> \(q\) 를 optimize 하자!!<br/> quaternion에 대한 설명은 <a href="https://semyeong-yu.github.io/blog/2024/Quaternion">Quaternion</a> 링크 참고!!</p> <ul> <li><code class="language-plaintext highlighter-rouge">scale</code> 3D vector \(s\) <code class="language-plaintext highlighter-rouge">초기값</code> :<br/> <a href="https://github.com/graphdeco-inria/gaussian-splatting/blob/b2ada78a779ba0455dfdc2b718bdf1726b05a1b6/scene/gaussian_model.py#L134C1-L134C1">GaussianModel().create_from_pcd()</a><br/> SfM sparse point cloud의 각 점에 대해 가장 가까운 점 3개까지의 거리의 평균을 각 axis(\(x, y, z\))별로 구한 것을 3 \(\times\) 1 \(s\)라 할 때<br/> 3 \(\times\) 1 \(log(\sqrt{s})\) 의 값을 3번 복사하여 3 \(\times\) 3 scale matrix \(S\)를 초기화 <pre><code class="language-Python">dist2 = torch.clamp_min(distCUDA2(torch.from_numpy(np.asarray(pcd.points)).float().cuda()), 0.0000001)
scales = torch.log(torch.sqrt(dist2))[...,None].repeat(1, 3)
</code></pre> </li> <li><code class="language-plaintext highlighter-rouge">quaternion</code> \(q\) <code class="language-plaintext highlighter-rouge">초기값</code> :<br/> 각 점에 대해 \(\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}\) 으로 quaternion을 초기화하고<br/> 이를 이용하여 rotation matrix \(R\) 초기화 <pre><code class="language-Python">rots = torch.zeros((fused_point_cloud.shape[0], 4), device="cuda")
rots[:, 0] = 1
</code></pre> </li> <li><code class="language-plaintext highlighter-rouge">anisotropic covariance</code>는 다양한 모양의 geometry를 나타내기 위해 optimize하기에 적합!</li> </ul> <blockquote> <p>param. gradient 직접 유도</p> </blockquote> <p>training할 때 automatic differentiation으로 인한 <code class="language-plaintext highlighter-rouge">overhead를 방지</code>하기 위해 <code class="language-plaintext highlighter-rouge">param. gradient를 직접 유도</code>함!</p> <p>Appendix A. <code class="language-plaintext highlighter-rouge">?????</code></p> <blockquote> <p>EWA volume splatting (2001) :<br/> world-to-camera 는 linear transformation 이지만,<br/> <code class="language-plaintext highlighter-rouge">camera-to-image (projection)</code> 는 <code class="language-plaintext highlighter-rouge">non-linear transformation</code> 이다!!</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-GS/4-480.webp 480w,/assets/img/2024-07-01-GS/4-800.webp 800w,/assets/img/2024-07-01-GS/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-GS/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 위 그림 : camera coordinate / 아래 그림 : image coordinate (ray space) </div> <ul> <li><code class="language-plaintext highlighter-rouge">world</code> coordinate (3D) : <ul> <li> \[\boldsymbol u = \begin{bmatrix} u_0 \\ u_1 \\ u_2 \end{bmatrix}\] </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">camera</code> coordinate (3D) : <ul> <li>\(\boldsymbol t = \begin{bmatrix} t_0 \\ t_1 \\ t_2 \end{bmatrix}\)<br/> \(= W \boldsymbol u + d\)<br/> where \(W\) : <code class="language-plaintext highlighter-rouge">viewing transformation</code> affine matrix from world coordinate to camera coordinate</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">image</code> coordinate (2D) : <ul> <li>\(\boldsymbol x = \begin{bmatrix} x_0 \\ x_1 \\ x_2 \end{bmatrix}\)<br/> \(= \phi(\boldsymbol t) = \begin{bmatrix} \frac{t_0}{t_2} \\ \frac{t_1}{t_2} \\ \| (t_0, t_1, t_2)^T \| \end{bmatrix}\)</li> <li>function \(\phi\) 는 non-linear하므로 Affine transformation이 불가능하다.</li> <li><code class="language-plaintext highlighter-rouge">Local Affine (Linear) transform으로 Approx.</code>하기 위해 \(\boldsymbol t = \boldsymbol t_{k}\) 에서의 <code class="language-plaintext highlighter-rouge">Taylor Approx.</code>를 이용하면,<br/> \(\phi_{k}(\boldsymbol t) = \phi(\boldsymbol t_{k}) + \boldsymbol J_{k} \cdot (\boldsymbol t - \boldsymbol t_{k})\)<br/> where<br/> \(\boldsymbol J_{k} = \frac{d\phi}{d \boldsymbol t}(\boldsymbol t_{k}) = \begin{bmatrix} \frac{d\phi}{d \boldsymbol t_{0}}(\boldsymbol t_{k}) &amp; \frac{d\phi}{d \boldsymbol t_{1}}(\boldsymbol t_{k}) &amp; \frac{d\phi}{d \boldsymbol t_{2}}(\boldsymbol t_{k}) \end{bmatrix} = \begin{bmatrix} \frac{1}{t_{k, 2}} &amp; 0 &amp; -\frac{t_{k, 0}}{t_{k, 2}^2} \\ 0 &amp; \frac{1}{t_{k, 2}} &amp; -\frac{t_{k, 1}}{t_{k, 2}^2} \\ \frac{t_{k, 0}}{l} &amp; \frac{t_{k, 1}}{l} &amp; \frac{t_{k, 2}}{l} \end{bmatrix}\)<br/> and \(l = \| (t_{k, 0}, t_{k, 1}, t_{k, 2})^T \|\)<br/> Here, \(J\) : <code class="language-plaintext highlighter-rouge">Jacobian</code>(각 axis로 편미분한 matrix) of the <code class="language-plaintext highlighter-rouge">affine approx.</code> of the <code class="language-plaintext highlighter-rouge">projective transformation</code> from camera coordinate to image coordinate</li> <li>즉, camera coordinate에서 임의의 좌표 \(\boldsymbol t_{k}\) 주변에 존재하는 입력 좌표 \(\boldsymbol t\)에 대해서는 image coordinate으로의 affine(linear) transformation이 충족된다.</li> <li>Gaussian Splatting 논문의 경우 <code class="language-plaintext highlighter-rouge">Gaussian의 중심점</code>을 \(\boldsymbol t_{k}\) 로 두면 그 주변의 \(\boldsymbol t\)에 대해서는 Jacobian을 이용한 affine(linear) transformation 가능!</li> </ul> </li> </ul> <blockquote> <p><code class="language-plaintext highlighter-rouge">Projection</code> of 3D Gaussian <code class="language-plaintext highlighter-rouge">covariance</code> to 2D</p> </blockquote> <ul> <li> <p><code class="language-plaintext highlighter-rouge">world coordinate</code> :<br/> \(\Sigma\) : 3 \(\times\) 3 covariance matrix of 3D Gaussian</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">image coordiante</code> (z=1) :<br/> \(\Sigma^{\ast} = J W \Sigma W^T J^T\) : covariance matrix of 2D splat</p> <ul> <li>Step 1. world-to-camera (<code class="language-plaintext highlighter-rouge">affine</code>) :<br/> \(\boldsymbol u \rightarrow W \boldsymbol u + d\)</li> <li>Step 2. camera-to-image (<code class="language-plaintext highlighter-rouge">local affine approx.</code>) :<br/> Projection<br/> \(W \boldsymbol u + d \rightarrow \phi_{k}(W \boldsymbol u + d) = x_k + \boldsymbol J_{k} W \boldsymbol u + \boldsymbol J_{k} (d - \boldsymbol t_{k})\)<br/> 상수 부분을 제외하면 \(\boldsymbol x = \boldsymbol J_{k} W \boldsymbol u\)</li> <li>Step 3. covariance 특성 :<br/> \(Cov[Ax] = E[(Ax - E[Ax])(Ax - E[Ax])^T]\)<br/> \(= E[A(x - E[x])(x - E[x])^TA^T] = A Cov[x] A^T\)</li> <li>Step 4. <code class="language-plaintext highlighter-rouge">world-to-image covariance</code> :<br/> \(\boldsymbol u \rightarrow \boldsymbol J_{k} W \boldsymbol u\) 이므로<br/> \(\Sigma \rightarrow \boldsymbol J \boldsymbol W \Sigma \boldsymbol W^T \boldsymbol J^T\)</li> <li>Step 5. <code class="language-plaintext highlighter-rouge">covariance dimension reduction</code> :<br/> 추가로, \(\boldsymbol J \boldsymbol W \Sigma \boldsymbol W^T \boldsymbol J^T\) 로 계산한 \(\Sigma^{\ast}\) 는 3-by-3 matrix 인데,<br/> 3D Gaussian을 한쪽 축으로 적분하면 2D Gaussian과 동일한 값을 가지게 되므로<br/> 3-by-3 covariance matrix의 3번째 행과 열의 값을 버린<br/> 2-by-2 matrix를 projected 2D covariance matrix 로 사용하면 됨!</li> </ul> </li> </ul> <h3 id="parameter-2-spherical-harmonicssh-coeff">Parameter 2. Spherical Harmonics(SH) coeff.</h3> <ul> <li><code class="language-plaintext highlighter-rouge">Spherical Harmonics</code> (SH) :<br/> spherical coordinate 에서 <code class="language-plaintext highlighter-rouge">각도</code> (\(\theta, \phi\))를 입력받아 <code class="language-plaintext highlighter-rouge">구의 표면 위치에서의 값</code>을 출력하는 함수<br/> spherical coordinate 에서 라플라스 방정식을 풀면 아래 수식과 같음</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-GS/5-480.webp 480w,/assets/img/2024-07-01-GS/5-800.webp 800w,/assets/img/2024-07-01-GS/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-GS/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-GS/6-480.webp 480w,/assets/img/2024-07-01-GS/6-800.webp 800w,/assets/img/2024-07-01-GS/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-GS/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-GS/7-480.webp 480w,/assets/img/2024-07-01-GS/7-800.webp 800w,/assets/img/2024-07-01-GS/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-GS/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> l이 같은 함수들은 same band l에 있다고 말함 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-GS/8-480.webp 480w,/assets/img/2024-07-01-GS/8-800.webp 800w,/assets/img/2024-07-01-GS/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-GS/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 가로축 : theta, 세로축 : phi, 채도 : SH magnitude, 색상 : SH phase </div> <ul> <li> <p>SH coeff. <code class="language-plaintext highlighter-rouge">초기값</code> :<br/> 0-band SH (\(\theta, \phi\) 와 관계없는 view-independent color) 라고 가정한 뒤<br/> SfM으로 얻은 point cloud의 RGB color값을 이용하여,<br/> 이를 만드는 SH coeff. 값으로 초기화</p> </li> <li> <p>SH 의 역할 :<br/> SH에서 band 수를 제한해서 쓴다는 것은 높은 band (high freq. 또는 detail info.)는 자른다는 의미이므로 <code class="language-plaintext highlighter-rouge">smoothing</code> 역할</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">SH coeff.</code>로 <code class="language-plaintext highlighter-rouge">color</code> 나타내는 법 :<br/> Fourier Series 에서처럼,<br/> SH coeff. \(k_{l}^{m}\) 의 optimal 값을 구해서<br/> \(k_{l}^{m}\) 와 \(Y_l^m(\theta, \phi)\) 의 weighted sum!<br/> \(C = \Sigma_{l=0}^{l_{max}} \Sigma_{m=-l}^{l} k_l^m Y_l^m(\theta, \phi)\)<br/> 즉, <code class="language-plaintext highlighter-rouge">trainable parameter</code> : SH coeff.인 \(k_{l}^{m}\)<br/> (<code class="language-plaintext highlighter-rouge">light source</code>마다 SH coeff. \(k_{l}^{m}\) 다르므로 find optimal value)</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-GS/9-480.webp 480w,/assets/img/2024-07-01-GS/9-800.webp 800w,/assets/img/2024-07-01-GS/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-GS/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="parameter-3-opacity">Parameter 3. opacity</h3> <ul> <li>opacity <code class="language-plaintext highlighter-rouge">초기값</code> :<br/> 임의의 실수값으로 초기화</li> </ul> <h3 id="parameter-4-3d-positionmean">Parameter 4. 3D position(mean)</h3> <p><code class="language-plaintext highlighter-rouge">?????</code></p> <h2 id="fast-differentiable-rasterizer-for-gaussians">Fast Differentiable Rasterizer for Gaussians</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-GS/10-480.webp 480w,/assets/img/2024-07-01-GS/10-800.webp 800w,/assets/img/2024-07-01-GS/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-GS/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Tile Rasterizer</p> </blockquote> <ul> <li> <p>기능 : 3D Gaussians로 구성된 3D model을 특정 camera pose에 대해 2D rendering</p> </li> <li><code class="language-plaintext highlighter-rouge">input</code> : <ul> <li>image의 rendering할 width, height</li> <li>3D Gaussian의 xyz-mean, covariance in world-coordinate</li> <li>3D Gaussian의 color, opacity</li> <li>current camera pose</li> </ul> </li> <li> <p><code class="language-plaintext highlighter-rouge">Frustum Culling</code> :<br/> 주어진 camera pose에서 view frustum을 그려서<br/> view frustum과 교차하는 확률이 99% confidence interval 범위 밖에 있는 3D Gaussians는 제거(culling)</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Guard Band</code> :<br/> image plane에 가까우면서 view frustum에서 멀리 떨어진 3D Gaussians의 경우 \(\leftarrow\) <code class="language-plaintext highlighter-rouge">?????</code><br/> projected 2D covariance 계산이 불안정하기 때문에 개별적으로 제거</p> </li> <li><code class="language-plaintext highlighter-rouge">Create Tiles</code> :<br/> <code class="language-plaintext highlighter-rouge">CUDA 병렬 처리</code>를 위해<br/> \(w \times h\)의 image를 \(16 \times 16\) pixel의 tiles로 쪼갬</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-GS/11-480.webp 480w,/assets/img/2024-07-01-GS/11-800.webp 800w,/assets/img/2024-07-01-GS/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-GS/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p><code class="language-plaintext highlighter-rouge">Parallelism</code> :<br/> <code class="language-plaintext highlighter-rouge">tile마다</code> 개별 <code class="language-plaintext highlighter-rouge">CUDA thread</code> block으로 실행하여<br/> forward, backward processing, data loading, sharing을 병렬처리</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Duplicate with Keys</code> :</p> <ul> <li>tile마다 각 Gaussian의 key를 생성</li> <li>CUDA 병렬처리 덕분에 2D Gaussian 하나가 3개의 tiles에 걸쳐 있다면, 3개의 2D Gaussians로 복제(instance화)되는 것처럼 작동</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-GS/12-480.webp 480w,/assets/img/2024-07-01-GS/12-800.webp 800w,/assets/img/2024-07-01-GS/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-GS/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Sort by Keys</code> :<br/> tile마다 Depth 기준으로 Radix Sort</li> </ul> <pre><code class="language-Python">from collections import deque
# 양방향에서 삽입/삭제 가능한 queue형 자료구조

# 1의 자릿수 기준으로 정렬한 뒤
# 10의 자릿수 기준으로 정렬한 뒤
# ...
def radixSort():
    nums = list(map(int, input().split(' ')))
    buckets = [deque() for _ in range(10)] # 각 자릿수(0~9)에 대응되는 10개의 empty deque()
    
    max_val = max(nums)
    queue = deque(nums) # 정렬할 숫자들
    digit = 1 # 정렬 기준이 되는 자릿수
    
    while (max_val &gt;= digit): # 가장 큰 수의 자릿수일 때까지만 실행
        while queue:
            num = queue.popleft() # 정렬할 숫자
            buckets[(num // digit) % 10].append(num) # 각 자릿수(0~9)에 따라 buckets에 num을 넣는다.
        
        # 해당 정렬 기준 자릿수에서 buckets에 다 넣었으면, buckets에 담겨있는 순서대로 꺼내와서 정렬한다.
        for bucket in buckets:
            while bucket:
                queue.append(bucket.popleft())

        digit *= 10 # 정렬 기준이 되는 자릿수 증가시키기
    
    print(list(queue))
</code></pre> <ul> <li> <p><code class="language-plaintext highlighter-rouge">Identify Tile Ranges</code> :<br/> tile별 Gaussian list를 효율적으로 관리하기 위해<br/> tile마다 Gaussian list 범위 식별</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Get Tile Ranges</code> :<br/> 모든 tile에 대해 Gaussian list 범위 읽어옴</p> </li> <li>\(\alpha\)-<code class="language-plaintext highlighter-rouge">Blending</code> in Order : <ul> <li>tile별 CUDA 병렬처리에 의해 각 pixel에 대해 <code class="language-plaintext highlighter-rouge">color</code> 및 <code class="language-plaintext highlighter-rouge">opacity</code> \(\alpha\) 값을 Gaussian list의 앞에서 뒤로 accumulate<br/> <code class="language-plaintext highlighter-rouge">?????</code> NeRF의 단점이 stochastic sampling해서 accumulate하느라 inefficient하다는 거였는데 얘는 Gaussians 어떻게 더함??</li> <li>i-th tile에 있는 pixels 중 a pixel opacity 값이 target saturation threshold를 넘어서면, 해당 i-th thread STOP</li> <li><code class="language-plaintext highlighter-rouge">Gaussian의 개수를 제한하지 않음</code>으로써 hyper-param. tuning 없이 arbitrary scene 커버 가능<br/> (GPU Radix Sort 덕분에 parallelism(병렬) 및 amortized(분할상환) 가능하여 Gaussian 개수 늘릴 수 있었음)</li> <li><code class="language-plaintext highlighter-rouge">기존 기법들은 pixel마다 정렬이 필요</code>해서 inefficient했지만<br/> 본 논문은 tile별 CUDA 병렬처리 덕분에 efficient<br/> (e.g. NeRF : ray per pixel 쏴서 accumulate per pixel for volume rendering <code class="language-plaintext highlighter-rouge">???</code>)</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Backward process</code> : <ul> <li>backward process를 위해 <d-cite key="Pointbased">[5]</d-cite>처럼 global memory에 blended points list를 저장할 수도 있지만 global memory에 부하가 올 수도 있어서 tile마다 구했던 range 및 sorted Gaussian list를 재사용</li> <li>직접 backward gradient update 식을 구해서 이용</li> <li>각 tile의 Gaussian list에 대해 Gaussian의 <code class="language-plaintext highlighter-rouge">opacity 비율에 따라</code> 뒤에서 앞으로 gradient update</li> <li>backward process할 때 accumulated opacity를<br/> 각 Gaussian opacity로 반복적으로 나누기 때문에<br/> gradient 계산을 위한 중간 opacity 값을 구성<br/> <code class="language-plaintext highlighter-rouge">?????</code></li> <li>0으로 나눠지는 경우를 방지하기 위해 \(\alpha\) 값이 \(\frac{1}{255}\)보다 작다면 blending update 안 함</li> <li>rasterization 중에 blending 값이 0.9999를 초과하기 전에 STOP<br/> <code class="language-plaintext highlighter-rouge">?????</code></li> </ul> </li> <li>Efficient Rasterization : <ul> <li>Pulsar 논문<d-cite key="Pulsar">[6]</d-cite>에 언급되었듯이<br/> 가장 작은 원소(<code class="language-plaintext highlighter-rouge">primitives</code>)를 미리 정렬(<code class="language-plaintext highlighter-rouge">pre-sort</code>)하는 기법 사용<br/> <code class="language-plaintext highlighter-rouge">?????</code></li> <li>differentiable</li> <li>2D projection 가능</li> <li>runtime : pixel마다 O(1)</li> <li>memory : low</li> </ul> </li> </ul> <h2 id="optimization-with-adaptive-density-control-of-3d-gaussians">Optimization with Adaptive Density Control of 3D Gaussians</h2> <h3 id="optimization">Optimization</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-GS/13-480.webp 480w,/assets/img/2024-07-01-GS/13-800.webp 800w,/assets/img/2024-07-01-GS/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-GS/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Loss :<br/> predicted image와 GT image를 비교하는<br/> <code class="language-plaintext highlighter-rouge">L1 loss</code> 및 <code class="language-plaintext highlighter-rouge">D-SSIM loss</code></p> </li> <li> <p>3D Gaussian의 xyz-mean에 대해서만 standard exponential decay scheduling <d-cite key="Plenoxels">[1]</d-cite> 사용</p> </li> <li>Adam optimizer로 네 가지 param. 업데이트 <ul> <li>3D xyz-mean</li> <li>3D covariance</li> <li>color</li> <li>opacity</li> </ul> </li> <li>optimization 세부 사항 : <ul> <li><code class="language-plaintext highlighter-rouge">low resol.에서 연산을 warm-up</code> :<br/> 목적 : model이 효율적으로 coarse info.부터 학습하도록 하여 <code class="language-plaintext highlighter-rouge">stability</code> 향상<br/> 초기에 4배 작은 image로 optimization 진행하고 250, 500 iter.에서 2배씩 upsampling</li> <li>Spherical Harmonics <code class="language-plaintext highlighter-rouge">low band부터 warm-up</code> :<br/> 목적 : scene의 corner를 촬영하거나 inside-out 방식(카메라가 촬영 대상의 내부에 위치하여 바깥쪽을 촬영) 때문에 <code class="language-plaintext highlighter-rouge">놓친 angular 영역이 있을 경우 SH의 0-band coeff.가 부적절</code>하게 만들어질 수 있어서<br/> 0-band coeff.를 optimize하고 매 1000 iter.마다 band 수 늘려서 4-band coeff.까지 optimization</li> </ul> </li> </ul> <h3 id="adaptive-density-control-of-gaussians">Adaptive Density Control of Gaussians</h3> <p>optimization of 4 param.의 경우 매 iter.마다 update하지만,<br/> Adaptive Density Control of Gaussians의 경우 <code class="language-plaintext highlighter-rouge">100 iter.마다</code> update</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-GS/14-480.webp 480w,/assets/img/2024-07-01-GS/14-800.webp 800w,/assets/img/2024-07-01-GS/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-GS/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p><code class="language-plaintext highlighter-rouge">Remove</code> :<br/> ddd</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Split</code> :<br/> ddd</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Clone</code> :<br/> ddd</p> </li> </ul> <h2 id="results">Results</h2> <h3 id="evaluation">Evaluation</h3> <h3 id="ablation-study">Ablation Study</h3> <h2 id="discussion">Discussion</h2> <h3 id="limitations">Limitations</h3>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="gaussian"/><category term="splatting"/><category term="rendering"/><category term="3d"/><category term="view"/><category term="synthesis"/><summary type="html"><![CDATA[3D GS for Real-Time Radiance Field Rendering]]></summary></entry><entry><title type="html">Diffusion-DDPM</title><link href="https://semyeong-yu.github.io/blog/2024/Diffusion/" rel="alternate" type="text/html" title="Diffusion-DDPM"/><published>2024-06-25T15:00:00+00:00</published><updated>2024-06-25T15:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Diffusion</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Diffusion/"><![CDATA[<h2 id="diffusion">Diffusion</h2> <h3 id="diffusion-model">Diffusion Model</h3> <ul> <li>forward process : <code class="language-plaintext highlighter-rouge">fixed</code> Gaussian noise 더함</li> <li>reverse process : <code class="language-plaintext highlighter-rouge">learned</code> Gaussian noise 뺌 (mean, std를 학습)</li> </ul> <h3 id="likelihood">Likelihood</h3> <p>아래 둘 다 관측값 \(x\)가 나올 확률인데,</p> <ul> <li><code class="language-plaintext highlighter-rouge">probability</code> \(P(x | \theta)\) : <code class="language-plaintext highlighter-rouge">확률 분포가 고정</code>된 상태에서, <code class="language-plaintext highlighter-rouge">관측되는 사건이 변화</code>할 때의 확률<br/> 예: 선택 가능한 정수를 1~5로 제한(확률 분포 고정)했을 때, 관측 목표값이 1~5 중 한 개의 숫자(관측 사건 변화)일 경우</li> <li><code class="language-plaintext highlighter-rouge">likelihood</code> \(L(\theta | x)\) : <code class="language-plaintext highlighter-rouge">관측된 사건이 고정</code>된 상태에서, <code class="language-plaintext highlighter-rouge">확률 분포 몰라서 가정</code>할 때의 확률<br/> 예: 선택 가능한 정수를 1~5가 아니라 1~10 또는 4~50으로 바꾸면서(확률 분포 모름), 2가 관측될 확률을 계산(관측 사건 고정)할 경우<br/> 예: 어떤(모르는) 확률 분포를 따르는 task를 n회 반복 수행하여 관측했을 때 random var. 종류를 가정할 수도 있고 특정 random var.의 parameter를 가정할 수도 있다</li> </ul> <h3 id="markov-process">Markov process</h3> <ul> <li><code class="language-plaintext highlighter-rouge">Markov</code> process (= Markov chain = <code class="language-plaintext highlighter-rouge">memoryless</code> process) : Markov property를 가지는 discrete stochastic process<br/> \(P[s_{t+1}|s_t] = P[s_{t+1}|s_1, \ldots, s_t]\)</li> </ul> <h3 id="kl-divergence">KL-divergence</h3> <ul> <li>\(H(p, q) = - \sum p_i log q_i\) : 두 확률분포 p, q의 cross entropy<br/> (보통 \(p\)는 GT, \(q\)는 predicted)</li> <li>\(H(p) = - \sum p_i log p_i\) : p’s entropy (상수값)</li> <li>\(KL(p \| q) = H(p, q) - H(p) = \sum p_i log \frac{p_i}{q_i}\) : 두 확률분포 p, q의 차이<br/> \(H(p)\)는 상수값이므로 <code class="language-plaintext highlighter-rouge">KL-divergence minimize = cross entropy minimize</code></li> <li>모르는 분포 \(p(x)\)를 N개 sampling하여 trained \(q(x | \theta)\)로 근사하고자 할 때,<br/> \(KL(p \| q) \simeq \frac{1}{N} \sum_{n=1}^{N} {-log q(x_n | \theta) + log p(x_n)}\) :<br/> \(log p(x_n)\)은 \(\theta\)에 독립이므로 <code class="language-plaintext highlighter-rouge">KL-divergence minimize = negative log likelihood minimize = MLE</code></li> </ul> <p>KL-diverence 특성 :</p> <ol> <li>\(KL(p \| q) \geq 0\) : 확률분포 p = q일 때 최소</li> <li>\(KL(p \| q) \neq KL(q \| p)\) (asymmetric) : 거리 개념이 아님<br/> 거리 개념으로 쓰는 방법 : 2가지 KL-divergence를 평균내는 방식의 \(JSD(p \| q)\)</li> </ol> <h3 id="diffusion-algorithm">Diffusion Algorithm</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-06-25-Diffusion/1-480.webp 480w,/assets/img/2024-06-25-Diffusion/1-800.webp 800w,/assets/img/2024-06-25-Diffusion/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-06-25-Diffusion/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>forward process (\(q\)) :<br/> \(q(X_t | X_{t-1}) = N(X_t ; \mu_{X_{t-1}}, \Sigma_{X_{t-1}}) = N(X_t ; \sqrt{1-\beta_t} \cdot X_{t-1}, \beta_t \cdot I)\)<br/> where \(\beta_t\) : noise 주입 정도 (상수값)<br/> t가 증가할수록 \(\beta_t\)가 증가하여 다른 pixel(\(I\))을 선택하므로 noise가 강해진다</p> </li> <li> <p>backward process (\(p_{\theta}\)) :<br/> image prior \(q(X_t)\)를 모르기 때문에 \(q(X_{t-1} | X_t)\)를 계산할 수 없으므로<br/> 목표 : \(q(X_{t-1} | X_t)\)를 근사하는 \(p_{\theta}(X_{t-1} | X_t)\) 학습<br/> 즉, 확률분포 \(q\)에서 관측한 값 \(x\)로 \(p_{\theta | x}\)의 likelihood를 구했을 때 그 값이 최대가 되도록 하는 <code class="language-plaintext highlighter-rouge">MLE Problem</code><br/> 즉, minimize \(E_q [- log p_{\theta} (x_0)]\)</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Diffusion Model</code> Naive Loss 수식 :<br/> 확률분포 \(q\)로 sampling했을 때,<br/> \(E_{x_T \sim q(x_T|x_0)}[- log p_{\theta}(x_0)] \leq\)<br/> \(E_q [D_{KL}(q(x_T | x_0) \| p_{\theta} (x_T)) + \sum_{t \gt 1} D_{KL}(q(x_{t-1} | x_t, x_0) \| p_{\theta} (x_{t-1} | x_t)) - log p_{\theta} (x_0 | x_1)]\)</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">DDPM</code>(Denoising Diffusion Probabilistic Model)(2020) Loss 수식 :<br/> \(E_{t, x_0, \epsilon} [\| \epsilon - \epsilon_{\theta}(\sqrt{\bar \alpha_{t}}x_0 + \sqrt{1-\bar \alpha_{t}} \epsilon, t) \|^{2}]\)<br/> where \(\epsilon \sim N(0, I)\)<br/> 즉, \(\epsilon_{\theta}\)가 Standard Gaussian 분포 \(\epsilon\)를 따르도록!<br/> 이 때, \(\epsilon_{\theta}\)의 input은 \(q(x_t | x_0)\)와 \(t\) !</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Distribution Summary</code> :</p> </li> </ul> <p>Let \(\alpha_t = 1 - \beta_t\) and \(\bar \alpha_t = \prod_{s=1}^t \alpha_s\) and \(\epsilon \sim N(0, I)\)</p> <ol> <li> \[x_t \sim q(x_t | x_{t-1}, x_0) = q(x_t | x_{t-1}) = N(x_t ; \sqrt{\alpha_{t}} \cdot x_{t-1}, \beta_{t} \cdot \boldsymbol I)\] </li> <li> \[x_t \sim q(x_t | x_0) = N(x_t; \sqrt{\bar \alpha_{t}} x_{0}, (1-\bar \alpha_{t}) \boldsymbol I) = \sqrt{\bar \alpha_{t}}x_0 + \sqrt{1-\bar \alpha_{t}} \epsilon\] </li> <li>\(x_{t-1} \sim q(x_{t-1} | x_t, x_0) = N(x_{t-1}; \tilde \mu_{t}(x_t), \tilde \beta_{t})\)<br/> where \(\tilde \mu_{t}(x_t) = \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{t})\)<br/> and \(\tilde \beta_{t} = \frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_{t}} \beta_{t}\)</li> <li>\(x_{t-1} \sim p_{\theta}(x_{t-1} | x_t) = N(x_{t-1}; \mu_{\theta}(x_t, t), \tilde \beta_{t})\)<br/> where \(\mu_{\theta}(x_t, t) = \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{\theta}(x_t, t))\)<br/> and \(\tilde \beta_{t} = \frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_{t}} \beta_{t}\)<br/> (training param.로 학습하는 부분은 \(\epsilon_{\theta}(x_t, t)\) 뿐!!)</li> </ol> <p>(위의 수식 유도과정은 지금부터 아래에서 다룰 예정)</p> <h3 id="diffusion-model-및-ddpm-loss-수식-유도">Diffusion Model 및 DDPM Loss 수식 유도</h3> <blockquote> <p>Step 1. ELBO (<code class="language-plaintext highlighter-rouge">Evidence Lower Bound</code>) 꼴로 변환</p> </blockquote> <p>\(log p_{\theta}(x_0)\)<br/> \(= E_{x_T \sim q(x_T|x_0)}[log p_{\theta}(x_0)]\)<br/> \(= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[log \frac{p_{\theta}(x_{0:T})}{p_{\theta}(x_{1:T}|x_0)}]\)<br/> \(= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[log \frac{p_{\theta}(x_{0:T})}{q(x_{1:T}|x_0)}] + E_{x_{1:T} \sim q(x_{1:T}|x_0)}[log \frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{1:T}|x_0)}] \cdots (\ast)\)<br/> (\(p_{\theta}(x_{1:T}|x_0)\)은 <code class="language-plaintext highlighter-rouge">intractable</code>하므로 KL divergence 항에 넣어서 제거!)</p> <p>마지막 식의 오른쪽 항은 아래와 같이 <code class="language-plaintext highlighter-rouge">KL divergence</code> 꼴이다.<br/> \(E_{x_{1:T} \sim q(x_{1:T}|x_0)}[log \frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{1:T}|x_0)}] = \sum q(x_{1:T}|x_0) log \frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{1:T}|x_0)} = D_{KL}(q(x_{1:T}|x_0) \| p_{\theta}(x_{1:T}|x_0))\)<br/> \(q(x_{1:T}|x_0)\)는 계산할 수 있지만 \(p_{\theta}(x_{1:T}|x_0)\)는 계산할 수 없으므로 KL divergence의 특성 \(KL(p \| q) \geq 0\)을 이용하면<br/> \((\ast)\) 으로부터<br/> \(log p_{\theta}(x_0) \geq E_{x_{1:T} \sim q(x_{1:T}|x_0)}[log \frac{p_{\theta}(x_{0:T})}{q(x_{1:T}|x_0)}]\)<br/> 즉, \(E_{x_T \sim q(x_T|x_0)}[- log p_{\theta}(x_0)] \leq E_{x_{1:T} \sim q(x_{1:T}|x_0)}[- log \frac{p_{\theta}(x_{0:T})}{q(x_{1:T}|x_0)}]\)</p> <blockquote> <p>Step 2. <code class="language-plaintext highlighter-rouge">Markov property</code> 이용하여 <code class="language-plaintext highlighter-rouge">Diffusion Model Naive Loss</code> 유도</p> </blockquote> \[E_{x_{1:T} \sim q(x_{1:T}|x_0)}[- log \frac{p_{\theta}(x_{0:T})}{q(x_{1:T}|x_0)}]\] \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[log \frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{0:T})}]\] \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[log \frac{\prod_{t=1}^{T} q(x_t|x_{t-1})}{p_{\theta}(x_T) \prod_{t=1}^T p_{\theta}(x_{t-1}|x_t)}]\] <p>by memoryless <code class="language-plaintext highlighter-rouge">Markov chain property</code></p> \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[- log p_{\theta}(x_T) + \sum_{t=1}^{T} log \frac{q(x_t|x_{t-1})}{p_{\theta}(x_{t-1}|x_t)}]\] \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[- log p_{\theta}(x_T) + \sum_{t=2}^{T} log \frac{q(x_t|x_{t-1})}{p_{\theta}(x_{t-1}|x_t)} + log \frac{q(x_1|x_0)}{p_{\theta}(x_0|x_1)}]\] \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[- log p_{\theta}(x_T) + \sum_{t=2}^{T} log \frac{q(x_t|x_{t-1}, x_0)}{p_{\theta}(x_{t-1}|x_t)} + log \frac{q(x_1|x_0)}{p_{\theta}(x_0|x_1)}]\] <p>by memoryless <code class="language-plaintext highlighter-rouge">Markov property</code><br/> <code class="language-plaintext highlighter-rouge">tractable</code>하도록 만들기 위해 \(q(x_t|x_{t-1})\) 의 조건부에 \(x_0\) 추가</p> \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[- log p_{\theta}(x_T) + \sum_{t=2}^{T} log (\frac{q(x_{t-1}|x_t, x_0)}{p_{\theta}(x_{t-1}|x_t)} \cdot \frac{q(x_t|x_0)}{q(x_{t-1}|x_0)}) + log \frac{q(x_1|x_0)}{p_{\theta}(x_0|x_1)}]\] <p>by <code class="language-plaintext highlighter-rouge">Bayes</code> 정리<br/> \(P(A|B \bigcap C) = \frac{P(B|A \bigcap C) \cdot P(A|C)}{P(B|C)}\)</p> <p>\(= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[- log p_{\theta}(x_T) + \sum_{t=2}^{T} log \frac{q(x_{t-1}|x_t, x_0)}{p_{\theta}(x_{t-1}|x_t)} + log \frac{q(x_T|x_0)}{q(x_1|x_0)} + log \frac{q(x_1|x_0)}{p_{\theta}(x_0|x_1)}]\)<br/> by log 곱셈으로 소거</p> \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[log \frac{q(x_T|x_0)}{p_{\theta}(x_T)} + \sum_{t=2}^{T} log \frac{q(x_{t-1}|x_t, x_0)}{p_{\theta}(x_{t-1}|x_t)} - log p_{\theta}(x_0|x_1)]\] \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[D_{KL}(q(x_T|x_0) \| p_{\theta}(x_T)) + \sum_{t=2}^{T} D_{KL}(q(x_{t-1}|x_t, x_0) \| p_{\theta}(x_{t-1}|x_t)) - log p_{\theta}(x_0|x_1)]\] <p>by <code class="language-plaintext highlighter-rouge">KL divergence</code> 식 \(D_{KL}(P \| Q) = \sum P(x) log (\frac{P(x)}{Q(x)})\)</p> <blockquote> <p>Step 3. <code class="language-plaintext highlighter-rouge">DDPM Loss</code> 유도</p> </blockquote> <ol> <li> <p>\(L_T = D_{KL}(q(x_T | x_0) \| p(x_T))\) : <code class="language-plaintext highlighter-rouge">regularization</code> loss<br/> <code class="language-plaintext highlighter-rouge">마지막 상태</code> \(x_T\)에서 확률분포 q, p의 차이를 최소화<br/> noise 주입 정도인 \(\beta_t\)는 미리 정해둔 schedule에 따른 상수값(fixed)이므로<br/> \(q(x_T | x_0)\)는 training과 관계없이 \(x_T\)가 항상 Gaussian 분포를 따르도록 한다.<br/> \(x_T\)가 <code class="language-plaintext highlighter-rouge">Gaussian 분포</code>를 따르므로 \(q(x_T | x_0)\)와 \(p(x_T)\)는 거의 유사하고,<br/> 결과적으로 둘의 KL divergence인 \(L_T\)는 항상 0에 가까운 값을 가지므로 training에서 \(L_T\) term은 <code class="language-plaintext highlighter-rouge">제외</code></p> </li> <li> <p>\(L_{t-1} = D_{KL}(q(x_{t-1} | x_t, x_0) \| p(x_{t-1} | x_t))\) : <code class="language-plaintext highlighter-rouge">denoising process</code> loss<br/> <code class="language-plaintext highlighter-rouge">현재 상태</code> \(x_t\)가 주어질 때 <code class="language-plaintext highlighter-rouge">이전 상태</code> \(x_{t-1}\)가 나올 확률 분포 q, p의 차이를 최소화</p> </li> <li> <p>\(L_0 = - log p_{\theta} (x_0 | x_1)\) : <code class="language-plaintext highlighter-rouge">reconstruction</code> loss<br/> q를 sampling했을 때 \(p_{\theta} (x_0 | x_1)\)를 최대화하여 (MLE) 확률분포 q, p의 차이를 최소화<br/> 전체적으로 봤을 때 \(L_0\)는 무수히 많은 time step \(T \sim 1000\) 중 단일 시점에서의 log likelihood 값이므로<br/> <code class="language-plaintext highlighter-rouge">값이 너무 작아서</code> training에서 \(L_0\) term은 <code class="language-plaintext highlighter-rouge">제외</code></p> </li> </ol> <p>Let’s only minimize the second term<br/> \(E_{x_{1:T} \sim q(x_{1:T}|x_0)}[\sum_{t=2}^{T} L_{t-1}] = E_{x_{1:T} \sim q(x_{1:T}|x_0)}[\sum_{t=2}^{T} D_{KL}(q(x_{t-1} | x_t, x_0) \| p(x_{t-1} | x_t))]\)</p> <blockquote> <p>Step 4. Gaussian param. \(\mu, \sigma\)로 KL-divergence 나타내기</p> </blockquote> <ul> <li> <p><code class="language-plaintext highlighter-rouge">Gaussian Integral</code> :<br/> \(\int_{-\infty}^{\infty} e^{-x^2}dx = \sqrt{\pi}\) and \(\int_{-\infty}^{\infty} x^2 e^{-ax^2}dx = \frac{1}{2}\sqrt{\pi}a^{-\frac{3}{2}}\)</p> </li> <li> <p>Integral of \(p(x)logp(x)\) for Gaussian \(p(x)\) :<br/> For \(p(x) = \frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} e^{-\frac{(x-\mu_{1})^2}{2 \sigma_{1}^2}} \sim N(\mu_{1}, \sigma_{1})\),<br/> \(\int p(x) log p(x) dx\)<br/> \(= \int \frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} e^{-\frac{(x-\mu_{1})^2}{2 \sigma_{1}^2}} log (\frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} e^{-\frac{(x-\mu_{1})^2}{2 \sigma_{1}^2}}) dx\)<br/> \(= \int \frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} e^{-t^2} (log \frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} - t^2) \sqrt{2} \sigma_{1} dt\)<br/> by 치환 \(t = \frac{x-\mu_{1}}{\sqrt{2}\sigma_{1}}\)<br/> \(= \frac{1}{\sqrt{\pi}} \int e^{-t^2} (log \frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} - t^2) dt\)<br/> \(= - \frac{log(2\pi \sigma_{1}^{2})}{2 \sqrt{\pi}} \int e^{-t^2} dt - \frac{1}{\sqrt{\pi}} \int t^2 e^{-t^2} dt\)<br/> \(= - \frac{log(2\pi \sigma_{1}^{2})}{2 \sqrt{\pi}} \cdot \sqrt{\pi} - \frac{1}{\sqrt{\pi}} \cdot \frac{\sqrt{\pi}}{2}\) by <code class="language-plaintext highlighter-rouge">Gaussian integral</code><br/> \(= - \frac{log(2\pi \sigma_{1}^{2})}{2} - \frac{1}{2}\)<br/> \(= - \frac{1}{2} (1 + log(2\pi \sigma_{1}^{2}))\)</p> </li> <li> <p>Integral of \(p(x)logq(x)\) for Gaussian \(p(x)\) and \(q(x)\) :<br/> For \(p(x) = \frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} e^{-\frac{(x-\mu_{1})^2}{2 \sigma_{1}^2}} \sim N(\mu_{1}, \sigma_{1})\)<br/> and \(q(x) = \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} e^{-\frac{(x-\mu_{2})^2}{2 \sigma_{2}^2}} \sim N(\mu_{2}, \sigma_{2})\),<br/> \(\int p(x) log q(x) dx\)<br/> \(= \int p(x) log \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} e^{-\frac{(x-\mu_{2})^2}{2 \sigma_{2}^2}} dx\)<br/> \(= \int p(x) log \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} dx - \int p(x) \frac{(x-\mu_{2})^2}{2 \sigma_{2}^2} dx\)<br/> \(= log \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} - \int p(x) \frac{(x-\mu_{2})^2}{2 \sigma_{2}^2} dx\)<br/> \(= log \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} - \frac{\int p(x) x^2 dx - \int 2 \mu_{2} x p(x) dx + \mu_{2}^2 \int p(x) dx}{2 \sigma_{2}^2}\)<br/> \(= log \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} - \frac{E_{1}(x^2) - 2 \mu_{2} E_{1}(x) + \mu_{2}^2}{2 \sigma_{2}^2}\)<br/> \(= log \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} - \frac{\sigma_{1}^2 + \mu_{1}^2 - 2 \mu_{2} \mu_{1} + \mu_{2}^2}{2 \sigma_{2}^2}\)<br/> by \(Var(X) = E[X^2] - (E[X])^2\)<br/> \(= - \frac{1}{2} log (2 \pi \sigma_{2}^{2}) - \frac{\sigma_{1}^2 + (\mu_{1} - \mu_{2})^2}{2 \sigma_{2}^2}\)</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">KL divergence</code> for Gaussian \(p(x)\) and \(q(x)\) :<br/> For \(p(x) = \frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} e^{-\frac{(x-\mu_{1})^2}{2 \sigma_{1}^2}} \sim N(\mu_{1}, \sigma_{1})\)<br/> and \(q(x) = \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} e^{-\frac{(x-\mu_{2})^2}{2 \sigma_{2}^2}} \sim N(\mu_{2}, \sigma_{2})\),<br/> \(D_{KL}(p \| q)\)<br/> \(= \int p(x) log \frac{p(x)}{q(x)} dx\)<br/> \(= \int p(x) logp(x) dx - \int p(x) log q(x)dx\)<br/> \(= - \frac{1}{2} (1 + log(2\pi \sigma_{1}^{2})) - (- \frac{1}{2} log (2 \pi \sigma_{2}^{2}) - \frac{\sigma_{1}^2 + (\mu_{1} - \mu_{2})^2}{2 \sigma_{2}^2})\)<br/> \(= - \frac{1}{2} + \frac{1}{2} log (\frac{2 \pi \sigma_{2}^2}{2 \pi \sigma_{1}^2}) + \frac{\sigma_{1}^2 + (\mu_{1} - \mu_{2})^2}{2 \sigma_{2}^2}\)<br/> \(= - \frac{1}{2} + log (\frac{\sigma_{2}}{\sigma_{1}}) + \frac{\sigma_{1}^2 + (\mu_{1} - \mu_{2})^2}{2 \sigma_{2}^2}\)</p> </li> </ul> <blockquote> <p>Step 5. Only Minimize the second term in Diffusion Loss</p> </blockquote> \[E_{x_{1:T} \sim q(x_{1:T}|x_0)}[\sum_{t=2}^{T} L_{t-1}]\] \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[\sum_{t=2}^{T} D_{KL}(q(x_{t-1} | x_t, x_0) \| p(x_{t-1} | x_t))]\] <p>Let \(\sigma\) <code class="language-plaintext highlighter-rouge">std have no learning param. (상수값)</code><br/> Let \(q(x_{t-1} | x_t, x_0)\) have Gaussian mean \(\tilde \mu_{t}\)<br/> Let \(p_{\theta}(x_{t-1} | x_t)\) have Gaussian mean \(\mu_{\theta}\)</p> <p>By Step 4., since \(\sigma\) is fixed, we have to minimize</p> \[E_{x_{1:T} \sim q(x_{1:T}|x_0)}[\frac{1}{2 \sigma_{t}^2} \| \tilde \mu_{t} (x_t, x_0) - \mu_{\theta} (x_t, t) \|^2] + C\] <p><code class="language-plaintext highlighter-rouge">Now we have to know</code> \(\tilde \mu_{t}\) and \(\mu_{\theta}\)</p> <blockquote> <p>Step 6. Obtain \(q(x_t \| x_0)\) from \(q(x_t \| x_{t-1})\)</p> </blockquote> <ol> <li> <p>Let’s define \(q(x_t | x_{t-1}) = N(x_t ; \sqrt{1-\beta_{t}} \cdot x_{t-1}, \beta_{t} \cdot \boldsymbol I)\)<br/> where noise 주입 비율인 \(\beta_{t}\)는 t에 따라 증가하는 상수값이고,<br/> noise 주입 비율이 커질수록 분산이 커지는 건 reasonable</p> </li> <li> <p>Let’s define \(\alpha_{t} = 1 - \beta_{t}\) and \(\bar \alpha_{t} = \prod_{s=1}^t \alpha_{s}\)<br/> where \(\bar \alpha_{t}\)는 \(s=1\)부터 \(s=t\)까지 \(\alpha_{s} = 1 - \beta_{s}\)의 누적곱</p> </li> </ol> <p>When \(\epsilon_{t-1}, \epsilon_{t-2}, \cdots, \epsilon_0 \sim N(0, I)\),<br/> \(x_t = \mu + \sigma \cdot \epsilon = \sqrt{\alpha_{t}} x_{t-1} + \sqrt{1-\alpha_{t}} \cdot \epsilon_{t-1}\)<br/> \(\cdots\)<br/> \(x_t = \sqrt{\bar \alpha_{t}} x_{0} + \sqrt{1-\bar \alpha_{t}} \cdot \epsilon\)<br/> where \(\epsilon \sim N(0, I)\)<br/> by merging two Gaussians \(N(0, \sigma_{1}^2 I), N(0, \sigma_{2}^2 I) \rightarrow N(0, (\sigma_{1}^2 + \sigma_{2}^2) I)\)</p> <p>Therefore,<br/> \(q(x_t | x_0) = N(x_t; \sqrt{\bar \alpha_{t}} x_{0}, (1-\bar \alpha_{t}) \boldsymbol I)\)</p> <p>즉, 우리가 정의한 Gaussian \(q(x_t \| x_{t-1})\) 으로부터 Gaussian \(q(x_t\|x_0)\) 를 얻어냈다!</p> <blockquote> <p>Step 7. Obtain \(q(x_{t-1} \| x_t, x_0)\)</p> </blockquote> <p><code class="language-plaintext highlighter-rouge">Remind that</code></p> <ol> <li> \[q(x_t | x_{t-1}, x_0) = q(x_t | x_{t-1}) = N(x_t ; \sqrt{\alpha_{t}} \cdot x_{t-1}, \beta_{t} \cdot \boldsymbol I)\] </li> <li> \[q(x_t | x_0) = N(x_t; \sqrt{\bar \alpha_{t}} x_{0}, (1-\bar \alpha_{t}) \boldsymbol I)\] </li> </ol> <p>\(q(x_{t-1} | x_t, x_0)\)<br/> \(= q(x_t | x_{t-1}, x_0) \frac{q(x_{t-1} | x_0)}{q(x_t | x_0)}\)<br/> \(\propto \exp (- \frac{(x_t - \sqrt{\alpha_{t}} x_{t-1})^2}{2 \beta_{t}} - \frac{(x_{t-1} - \sqrt{\bar \alpha_{t-1}} x_{0})^2}{2 (1-\bar \alpha_{t-1})} + \frac{(x_{t} - \sqrt{\bar \alpha_{t}} x_{0})^2}{2 (1-\bar \alpha_{t})})\)<br/> \(= \exp (- \frac{1}{2} ((\frac{\alpha_{t}}{\beta_{t}} + \frac{1}{1 - \bar \alpha_{t-1}})x_{t-1}^2 - (\frac{2 \sqrt{\alpha_t}}{\beta_{t}} x_t + \frac{2\sqrt{\bar \alpha_{t-1}}}{1 - \bar \alpha_{t-1}} x_0) x_{t-1} + C(x_t, x_0)))\)</p> <p>\(q(x_{t-1} | x_t, x_0)\) 또한 Gaussian이라서<br/> \(\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x-\mu)^2}{2 \sigma^2}}\) 꼴이므로<br/> \(q(x_{t-1} | x_t, x_0)\)의 지수부분을 \(x_{t-1}\)에 대한 이차식 꼴로 정리하면<br/> 계수 비교를 통해 \(q(x_{t-1} | x_t, x_0)\)의 mean, variance를 알 수 있음!</p> <ul> <li>\(q(x_{t-1} | x_t, x_0)\) 의 variance :<br/> \(\frac{1}{\sigma^{2}}\)<br/> \(= \frac{\alpha_{t}}{\beta_{t}} + \frac{1}{1 - \bar \alpha_{t-1}}\)<br/> \(= \frac{\alpha_{t} - \alpha_{t} \bar \alpha_{t-1} + \beta_{t}}{\beta_{t}(1 - \bar \alpha_{t-1})}\)<br/> \(= \frac{\alpha_{t} - \bar \alpha_{t} + 1 - \alpha_{t}}{\beta_{t}(1 - \bar \alpha_{t-1})}\)<br/> \(= \frac{1 - \bar \alpha_{t}}{\beta_{t}(1 - \bar \alpha_{t-1})}\)</li> </ul> <p>따라서 \(\sigma^{2} = \frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_{t}} \beta_{t}\)</p> <ul> <li>\(q(x_{t-1} | x_t, x_0)\) 의 mean :<br/> \(- \frac{2 \mu}{\sigma^{2}}\)<br/> \(= - (\frac{2 \sqrt{\alpha_t}}{\beta_{t}} x_t + \frac{2\sqrt{\bar \alpha_{t-1}}}{1 - \bar \alpha_{t-1}} x_0)\)<br/> \(\rightarrow \mu = (\frac{\sqrt{\alpha_t}}{\beta_{t}} x_t + \frac{\sqrt{\bar \alpha_{t-1}}}{1 - \bar \alpha_{t-1}} x_0) \cdot \sigma^{2}\)<br/> \(= (\frac{\sqrt{\alpha_t}}{\beta_{t}} x_t + \frac{\sqrt{\bar \alpha_{t-1}}}{1 - \bar \alpha_{t-1}} x_0) \cdot (\frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_{t}} \beta_{t})\)<br/> \(= \frac{\sqrt{\alpha_t} x_t (1 - \bar \alpha_{t-1}) + \beta_{t} x_0 \sqrt{\bar \alpha_{t-1}}}{\beta_{t}(1 - \bar \alpha_{t-1})} \cdot (\frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_{t}} \beta_{t})\)<br/> \(= \frac{\sqrt{\alpha_t} x_t (1 - \bar \alpha_{t-1}) + \beta_{t} x_0 \sqrt{\bar \alpha_{t-1}}}{1 - \bar \alpha_{t}}\)</li> </ul> <p>따라서<br/> \(\mu = \frac{\sqrt{\bar \alpha_{t-1}} \beta_{t}}{1 - \bar \alpha_{t}} x_0 + \frac{\sqrt{\alpha_t} (1 - \bar \alpha_{t-1})}{1 - \bar \alpha_{t}} x_t\)</p> <p>\(q(x_t | x_0) = N(x_t; \sqrt{\bar \alpha_{t}} x_{0}, (1-\bar \alpha_{t}) \boldsymbol I)\)이므로<br/> \(x_0\) <code class="language-plaintext highlighter-rouge">소거</code>하기 위해<br/> \(x_t = \sqrt{\bar \alpha_{t}} x_{0} + \sqrt{1-\bar \alpha_{t}} \epsilon\) 대입하면</p> <p>\(\mu_{t} = \frac{\sqrt{\bar \alpha_{t-1}} \beta_{t}}{1 - \bar \alpha_{t}} x_0 + \frac{\sqrt{\alpha_t} (1 - \bar \alpha_{t-1})}{1 - \bar \alpha_{t}} x_t\)<br/> \(= \frac{\sqrt{\bar \alpha_{t-1}} \beta_{t}}{1 - \bar \alpha_{t}} (\frac{1}{\sqrt{\bar \alpha_{t}}}(x_t - \sqrt{1 - \bar \alpha_{t}} \epsilon_{t})) + \frac{\sqrt{\alpha_t} (1 - \bar \alpha_{t-1})}{1 - \bar \alpha_{t}} x_t\)<br/> \(= \frac{\sqrt{\bar \alpha_{t-1}} (1 - \alpha_{t})}{1 - \bar \alpha_{t}} (\frac{1}{\sqrt{\bar \alpha_{t}}}(x_t - \sqrt{1 - \bar \alpha_{t}} \epsilon_{t})) + \frac{\sqrt{\alpha_t} (1 - \bar \alpha_{t-1})}{1 - \bar \alpha_{t}} x_t\)<br/> \(= \frac{\sqrt{k} (1 - \alpha_{t})}{1 - \alpha_{t} k} (\frac{1}{\sqrt{\alpha_{t} k}}(x_t - \sqrt{1 - \alpha_{t} k} \epsilon_{t})) + \frac{\sqrt{\alpha_t} (1 - k)}{1 - \alpha_{t} k} x_t\)<br/> by \(k = \bar \alpha_{t-1}\) 및 \(\alpha_{t}k = \bar \alpha_{t}\)로 치환<br/> \(= \frac{1}{\sqrt{\alpha_{t}}} x_t - \frac{1 - \alpha_{t}}{\sqrt{\alpha_{t}}\sqrt{1 - \alpha_{t}k}} \epsilon_{t}\)<br/> \(= \frac{1}{\sqrt{\alpha_{t}}} x_t - \frac{1 - \alpha_{t}}{\sqrt{\alpha_{t}}\sqrt{1 - \bar \alpha_{t}}} \epsilon_{t}\)<br/> \(= \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{t})\)</p> <ul> <li>\(q(x_{t-1} | x_t, x_0)\) <code class="language-plaintext highlighter-rouge">결과</code> :<br/> \(q(x_{t-1} | x_t, x_0) = N(x_{t-1}; \tilde \mu_{t}(x_t), \tilde \beta_{t})\)<br/> where \(\tilde \mu_{t}(x_t) = \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{t})\)<br/> and \(\tilde \beta_{t} = \frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_{t}} \beta_{t}\)</li> </ul> <blockquote> <p>Step 8. Obtain \(p_{\theta}(x_{t-1} \| x_t)\)</p> </blockquote> <p>우리의 목적은<br/> \(D_{KL}(q(x_{t-1} | x_t, x_0) \| p_{\theta}(x_{t-1} | x_t))\) 최소화<br/> 즉, \(p\)의 분포를 \(q\)의 분포에 approx.하는 것이다</p> <p>\(q(x_{t-1} | x_t, x_0)\) 의 mean, variance 인<br/> \(\tilde \mu_{t}(x_t) = \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{t})\)<br/> \(\tilde \beta_{t} = \frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_{t}} \beta_{t}\) 에서<br/> \(x_t, \alpha_{t}, \beta_{t}\)는 입력값 및 미리 정해놓는 상수값이라서<br/> deep learning network인 \(\epsilon_{\theta}\) 가 시간 t에 따라 \(\epsilon_{t} \sim N(0, I)\) 을 학습하도록 하기 위해서<br/> <code class="language-plaintext highlighter-rouge">training param.로 학습할 수 있는 부분</code>은 \(\epsilon_{t} \sim N(0, I)\) 뿐이다<br/> 즉, <code class="language-plaintext highlighter-rouge">p와 q의 분포에서 차이가 날 수 있는 부분은 epsilon 뿐!</code></p> <p>따라서 \(p_{\theta}(x_{t-1} | x_t)\)의 평균인 \(\mu_{\theta}(x_t, t)\) 는<br/> \(\tilde \mu_{t}(x_t) = \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{t})\) 에서<br/> \(\epsilon_{t}\) 만 \(\epsilon_{\theta}(x_t, t)\) 로 바꾼 값이다<br/> \(\mu_{\theta}(x_t, t) = \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{\theta}(x_t, t))\)</p> <blockquote> <p>Step 9. Final <code class="language-plaintext highlighter-rouge">DDPM Loss</code></p> </blockquote> <p>Step 5.에 따르면 we have to minimize<br/> \(E_{x_{1:T} \sim q(x_{1:T}|x_0)}[\frac{1}{2 \sigma_{t}^2} \| \tilde \mu_{t} (x_t, x_0) - \mu_{\theta} (x_t, t) \|^2] + C\)</p> <p>\(E_{x_0, \epsilon}[\frac{1}{2 \|\Sigma(x_t, t) \|^2} \| \tilde \mu_{t} (x_t, x_0) - \mu_{\theta} (x_t, t) \|^2]\)<br/> \(= E_{x_0, \epsilon}[\frac{1}{2 \|\Sigma \|^2} \| \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{t}) - \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{\theta}(x_t, t)) \|^2]\)<br/> \(= E_{x_0, \epsilon}[\frac{(1 - \alpha_{t})^2}{2 \|\Sigma \|^2 \alpha_{t} (1 - \bar \alpha_{t})} \| \epsilon_{t} - \epsilon_{\theta}(x_t, t) \|^2]\)<br/> \(= E_{x_0, \epsilon}[\frac{(1 - \alpha_{t})^2}{2 \|\Sigma \|^2 \alpha_{t} (1 - \bar \alpha_{t})} \| \epsilon_{t} - \epsilon_{\theta}(\sqrt{\bar \alpha_{t}} x_{0} + \sqrt{1-\bar \alpha_{t}} \epsilon, t) \|^2]\)<br/> since \(x_t = \sqrt{\bar \alpha_{t}} x_{0} + \sqrt{1-\bar \alpha_{t}} \epsilon\)</p> <p>앞의 weight term을 제거하면<br/> <code class="language-plaintext highlighter-rouge">최종 Loss 값</code>은 드디어!!!<br/> \(= E_{x_0, \epsilon}[\| \epsilon_{t} - \epsilon_{\theta}(\sqrt{\bar \alpha_{t}} x_{0} + \sqrt{1-\bar \alpha_{t}} \epsilon, t) \|^2]\)<br/> where \(x_t = q(x_t | x_0) = \sqrt{\bar \alpha_{t}} x_{0} + \sqrt{1-\bar \alpha_{t}} \epsilon\) and \(\epsilon \sim N(0, I)\)</p> <h3 id="ddpm-pseudo-code">DDPM Pseudo-Code</h3> <ul> <li><code class="language-plaintext highlighter-rouge">forward</code> process : <code class="language-plaintext highlighter-rouge">Training</code> \(\epsilon_{\theta}\) for given input image \(x_0\)</li> </ul> <pre><code class="language-Python">while (converge){
  x_0 ~ q(x_0) # input image
  t ~ Uniform({1, ..., T}) # time step (integer)
  epsilon ~ N(0, I) # Gaussian target epsilon

  # gradient descent by DDPM loss
}
</code></pre> <p>DDPM loss :<br/> \(E_{x_0, \epsilon}[\| \epsilon_{t} - \epsilon_{\theta}(\sqrt{\bar \alpha_{t}} x_{0} + \sqrt{1-\bar \alpha_{t}} \epsilon, t) \|^2]\)</p> <ul> <li><code class="language-plaintext highlighter-rouge">backward</code> process : <code class="language-plaintext highlighter-rouge">Sampling</code> from Gaussian noise img to new img by trained \(\epsilon_{\theta}\)</li> </ul> <pre><code class="language-Python">x_T ~ N(0, I) # start with Gaussian noise image

for (t = T, ..., 1){
  z ~ N(0, I) if t &gt; 1 else z = 0  
  # sampling x_{t-1} from x_t by p_{theta}(x_{t-1} | x_t)
}
</code></pre> <p>Sampling :<br/> \(x_{t-1} = \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{\theta}(x_t, t)) + \sigma_{t} z\)</p> <blockquote> <p>출처 블로그 :<br/> <a href="https://xoft.tistory.com/32">Diffusion Model</a><br/> <a href="https://xoft.tistory.com/33?category=1156151">DDPM 수식 유도</a><br/> <a href="https://woongchan789.tistory.com/12">DDPM 수식 유도</a></p> </blockquote>]]></content><author><name></name></author><category term="generative"/><category term="diffusion"/><category term="generative"/><summary type="html"><![CDATA[Diffusion Study]]></summary></entry><entry><title type="html">MipNeRF</title><link href="https://semyeong-yu.github.io/blog/2024/MipNeRF/" rel="alternate" type="text/html" title="MipNeRF"/><published>2024-06-17T21:00:00+00:00</published><updated>2024-06-17T21:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/MipNeRF</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/MipNeRF/"><![CDATA[<h2 id="mip-nerf-a-multiscale-representation-for-anti-aliasing-neural-radiance-fields">Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</h2> <h4 id="jonathan-t-barron-ben-mildenhall-matthew-tancik-peter-hedman-ricardo-martin-brualla-pratul-p-srinivasan">Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2103.13415">https://arxiv.org/abs/2103.13415</a><br/> project website :<br/> <a href="https://jonbarron.info/mipnerf/">https://jonbarron.info/mipnerf/</a><br/> pytorch code :<br/> <a href="https://github.com/bebeal/mipnerf-pytorch">https://github.com/bebeal/mipnerf-pytorch</a><br/> <a href="https://github.com/google/mipnerf">https://github.com/google/mipnerf</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>Mip-NeRF는 training하는 동안, <code class="language-plaintext highlighter-rouge">임의의 scale</code>에 대해 query 받을 수 있는, scene의 <code class="language-plaintext highlighter-rouge">anti-aliased</code> <code class="language-plaintext highlighter-rouge">pre-filtered representation</code>을 학습한다.</li> <li>camera center로부터 각 pixel로 3D conical frustum을 쏜 다음, the <code class="language-plaintext highlighter-rouge">frustum을 multi-variate Gaussian으로 근사</code>한 뒤, Gaussian 내 좌표를 positional encoding한 것에 대해 <code class="language-plaintext highlighter-rouge">integral</code> \(E \left[ \gamma (x) \right]\) 계산</li> </ol> </blockquote> <h2 id="introduction">Introduction</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-06-17-MipNeRF/2-480.webp 480w,/assets/img/2024-06-17-MipNeRF/2-800.webp 800w,/assets/img/2024-06-17-MipNeRF/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-06-17-MipNeRF/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>기존 NeRF의 문제점 :<br/> rendering 위해 sampling할 때 <code class="language-plaintext highlighter-rouge">single ray</code> per pixel 쏴서 composite 하므로<br/> dataset images에 있는 물체의 크기가 일정하지 않을 때 (multiple resolutions) multi-scales images에 대해 학습하더라도<br/> high-resolution은 <code class="language-plaintext highlighter-rouge">blurry</code> rendering<br/> low-resolution은 <code class="language-plaintext highlighter-rouge">aliased</code> rendering<br/> 그렇다고 multiple rays per pixel through its footprint로 brute-force supersampling하는 것은 정확하긴 하겠지만 too costly 비현실적</p> </li> <li> <p>Minmapping Approach :<br/> classic 컴퓨터 그래픽스 분야에서 rendering할 때 aliasing을 없애기 위한 <code class="language-plaintext highlighter-rouge">pre-filtering</code> 기법<br/> 본 논문인 Mip-NeRF가 여기서 영감을 얻음<br/> signal(e.g. image)을 diff. downsampling scales로 나타낸 뒤 pixel footprint를 근거로 ray에 사용하기 위한 <code class="language-plaintext highlighter-rouge">적절한 scale을 고른다</code><br/> render time에 할 복잡할 일을 precomputation phase에 미리 하는 것일 뿐이긴 하지만, 주어진 texture마다 한 번만 minmap을 만들면 된다는 장점이 있다</p> </li> <li> <p>Mip-NeRF :</p> <ul> <li>represent pre-filtered scene at <code class="language-plaintext highlighter-rouge">continuous space of scales</code></li> <li>ray 대신 <code class="language-plaintext highlighter-rouge">conical frustum</code> 사용해서 <code class="language-plaintext highlighter-rouge">anti-aliased</code> rendering with fine details</li> <li>multiscale variant of dataset에 대해 평균 error rate 60% 감소</li> <li>NeRF가 hierarchical sampling을 위해 coarse and fine MLP를 분리했다면, Mip-NeRF는 <code class="language-plaintext highlighter-rouge">scale-aware</code>하므로 <code class="language-plaintext highlighter-rouge">single MLP만으로 충분</code><br/> 따라서 NeRF보다 7% 빠르고, param. 수는 절반</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-06-17-MipNeRF/1-480.webp 480w,/assets/img/2024-06-17-MipNeRF/1-800.webp 800w,/assets/img/2024-06-17-MipNeRF/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-06-17-MipNeRF/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>차이 : 기존 NeRF는 <code class="language-plaintext highlighter-rouge">a single point</code>를 encode하고, Mip-NeRF는 <code class="language-plaintext highlighter-rouge">a region of space</code>를 encode</p> </li> <li> <p>기존 NeRF :<br/> camera center로부터 각 pixel로 ray를 하나 쏜 다음 point sampling한 뒤 positional encoding</p> </li> <li> <p>Mip-NeRF :<br/> camera center로부터 각 pixel로 3D conical frustum을 쏜 다음, 3D point 및 그 주위의 Gaussian region을 encode하기 위해 <code class="language-plaintext highlighter-rouge">IPE(integrated positional encoding)</code><br/> IPE : the <code class="language-plaintext highlighter-rouge">frustum을 multi-variate Gaussian으로 근사</code>한 뒤, Gaussian 내 좌표를 positional encoding한 것에 대해 <code class="language-plaintext highlighter-rouge">integral</code> \(E \left[ \gamma (x) \right]\) 계산</p> </li> </ul> <h2 id="related-work">Related Work</h2> <h4 id="anti-aliasing-in-rendering">Anti-aliasing in Rendering</h4> <blockquote> <p><code class="language-plaintext highlighter-rouge">anti-aliasing</code>을 위한 고전적인 방법으로는 두 가지가 있다.</p> </blockquote> <ol> <li><code class="language-plaintext highlighter-rouge">supersampling</code> : <ul> <li>rendering할 때 <code class="language-plaintext highlighter-rouge">multiple rays per pixel</code>을 쏴서 Nyquist frequency에 가깝게 supersampling</li> <li><code class="language-plaintext highlighter-rouge">expensive</code> as runtime increases linearly with the supersampling rate, so used only in <code class="language-plaintext highlighter-rouge">offline</code> rendering</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">pre-filtering</code> : <ul> <li>target sampling rate에 맞춰서 Nyquist frequency를 줄이기 위해 scene에 <code class="language-plaintext highlighter-rouge">lowpass-filter</code>를 씌운 버전 사용</li> <li>scene을 미리 downsampling <code class="language-plaintext highlighter-rouge">multi-scales</code> (sparse voxel octree 또는 minmap)로 나타낸 뒤, <code class="language-plaintext highlighter-rouge">ray 대신 cone</code>을 추적하여 cone과 scene이 만나는 곳의 cone’s footprint에 대응되는 <code class="language-plaintext highlighter-rouge">적절한 scale</code>을 골라서 사용 (target sampling rate에 맞는 적절한 scale)</li> <li>scene에 filter 씌운 버전을 한 번만 미리 계산하면 되므로, better for <code class="language-plaintext highlighter-rouge">real-time</code> rendering</li> </ul> </li> </ol> <blockquote> <p>그런데 아래의 이유로 고전적인 multi-scale representation은 적용 불가능<br/> input scene의 geometry를 미리 알 수 없음<br/> input scene의 <code class="language-plaintext highlighter-rouge">scale이 continuous</code>하므로 a fixed number of scales (discrete)과 상황이 다름</p> </blockquote> <p>\(\rightarrow\) 결론 : 따라서 Mip-NeRF는 training하는 동안, <code class="language-plaintext highlighter-rouge">임의의 scale</code>에 대해 query 받을 수 있는, scene의 <code class="language-plaintext highlighter-rouge">pre-filtered representation</code>을 학습한다.</p> <h4 id="scene-representations-for-view-synthesis">Scene Representations for View Synthesis</h4> <ul> <li> <p>만약 images of scene are captured <code class="language-plaintext highlighter-rouge">densely</code> 라면, novel view synthesis 위해, intermediate representation of scene을 reconstruct하지 않고 <code class="language-plaintext highlighter-rouge">light field interpolation</code> 기법 사용</p> </li> <li>만약 images of scene are captured <code class="language-plaintext highlighter-rouge">sparsely</code> 라면, novel view synthesis 위해, <code class="language-plaintext highlighter-rouge">explicit representation</code> of the scene’s 3D geometry and appearance를 reconstruct <ul> <li><code class="language-plaintext highlighter-rouge">polygon-mesh-based</code> representation (<code class="language-plaintext highlighter-rouge">discrete, classic</code>) :<br/> with either diffuse or view-dependent textures<br/> can be stored efficiently, but mesh geometry optimization에 gradient descent를 이용하는 것은 불연속점 및 극소점 때문에 어렵</li> <li><code class="language-plaintext highlighter-rouge">volumetric</code> representation : <ul> <li><code class="language-plaintext highlighter-rouge">voxel-grid-based</code> representation (<code class="language-plaintext highlighter-rouge">discrete, classic</code>) : deep learning으로 학습 가능, but 고해상도의 scene에는 부적합</li> <li><code class="language-plaintext highlighter-rouge">coordinate-based</code> neural representation (<code class="language-plaintext highlighter-rouge">continuous, recent</code>) : 3D 좌표를 그 위치에서의 scene의 특징(e.g. volume density, radiance)으로 mapping하는 continuous function을 MLP로 예측 <ul> <li>implicit surface representation</li> <li><code class="language-plaintext highlighter-rouge">implicit volumetric NeRF representation</code></li> </ul> </li> </ul> </li> </ul> </li> <li>문제점 :<br/> 그동안 view synthesis를 할 때 sampling 및 aliasing에는 덜 주목했다<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">classic discrete</code> representation의 경우 앞에서 소개한 minmap, octree와 같은 고전적인 multi-scale pre-filtering 기법을 쓰면 aliasing 없이 rendering 가능하다<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">coordinate-based</code> representation의 경우 scale이 continuous하므로 고전적인 anti-aliasing 기법은 사용할 수 없다<br/> \(\rightarrow\) sparse voxel octree 기반의 multi-scale representation으로 implicit surface의 continuous neural representation을 구현한 논문 <d-cite key="continuouspriori">[1]</d-cite> 있긴 하지만, scene geometry의 priori를 알아야 한다는 제약이 있다<br/> \(\rightarrow\) Mip-NeRF는 training하는 동안, <code class="language-plaintext highlighter-rouge">임의의 scale</code>에 대해 query 받을 수 있는, scene의 anti-aliased <code class="language-plaintext highlighter-rouge">pre-filtered representation</code>을 학습한다.</li> </ul> <h2 id="method">Method</h2> <h4 id="cone-tracing-and-positional-encoding">Cone Tracing and Positional Encoding</h4> <p>TBD<br/> (<code class="language-plaintext highlighter-rouge">작성 중 ...</code>)</p> <h4 id="architecture">Architecture</h4> <p>TBD</p> <h2 id="results">Results</h2> <p>TBD</p> <h2 id="conclusion">Conclusion</h2> <p>TBD</p>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="rendering"/><category term="3d"/><category term="multiscale"/><category term="antialiasing"/><summary type="html"><![CDATA[A Multiscale Representation for Anti-Aliasing Neural Radiance Fields]]></summary></entry><entry><title type="html">SegmentAnything</title><link href="https://semyeong-yu.github.io/blog/2024/SegmentAnything/" rel="alternate" type="text/html" title="SegmentAnything"/><published>2024-05-29T14:00:00+00:00</published><updated>2024-05-29T14:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/SegmentAnything</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/SegmentAnything/"><![CDATA[<h3 id="segmentanything">SegmentAnything</h3> <h4 id="alexander-kirillov-eric-mintun-nikhila-ravi-hanzi-mao-chloe-rolland-laura-gustafson-tete-xiao-spencer-whitehead-alexander-c-berg-wan-yen-lo-piotr-dollár-ross-girshick">Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2304.02643">https://arxiv.org/abs/2304.02643</a><br/> 출처 : Vision study mkd님</p> </blockquote> <hr/> <h2 id="abstract">Abstract</h2> <ul> <li> <p>Task<br/> Promptable Image Segmentation</p> </li> <li> <p>Model Architecture<br/> image encoder + prompt encoder + mask decoder</p> </li> <li> <p>Generate Data (Data Engine)<br/> assisted-manual stage \(\rightarrow\) semi-automatic stage \(\rightarrow\) fully-automatic stage<br/> data ‘SA-1B’ : 1B masks with 11M images</p> </li> <li> <p>Enable Zero-Shot Generalization<br/> Zero-Shot transfer to various tasks</p> </li> <li> <p>Code Review</p> </li> </ul> <h2 id="overview">Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/2-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/2-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>prompt : mask를 생성할 대상을 지정<br/> point, BB, mask(rough area), text(preliminary) 중 하나</p> </li> <li> <p>valid masks : segmented mask를 하나가 아닌 3개 (whole, part, sub-part) 생성<br/> ambiguous prompt에 대응하기 위해, zero shot을 위해<br/> 3개의 masks 중 GT와 가장 유사한(confidence score가 가장 높은) mask의 loss만 사용</p> </li> </ul> <h2 id="model">Model</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/3-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/3-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Image Encoder : MAE (Masked AutoEncoder) 방식의 ViT<br/> MAE 요약 : 이미지를 grid로 나누고 patches 중 일부를 가린 뒤 원본을 복원하도록 학습하고, 학습이 끝난 후에는 encoder embedding만 사용<br/> ViT-H/16 : 14 \(\times\) 14 windowed attention and 4 global attention blocks</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/4-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/4-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Prompt Encoder :<br/> Mask (dense prompt) : conv. 거친 후 image embedding에 pixel-wise sum (mask가 없는 pixel의 경우 ‘no mask’ prompt 사용)<br/> Point (sparse prompt) : positional encoding + learned embedding(fg or bg)<br/> BB (sparse prompt) : positional encoding + learned embedding(top-left or bottom-right)<br/> text (sparse prompt) : by CLIP text encoder</p> </li> <li> <p>Loss :</p> <ol> <li>Mask loss : related to mask prediction<br/> 1-1. focal loss : \(L(p_{t}) = - (1-p_{t})^{r}log(p_{t})\) where \((1-p_{t})^{r}\) gives more weight to few hard examples (\(p_{t} \sim 0\))<br/> 1-2. dice loss : 1 - dice score where dice score = \(\frac{2 \times Area(A \cap B)}{Area(A) + Area(B)}\)</li> </ol> </li> </ul> <ol> <li>IoU loss : related to confidence score<br/> MSE loss</li> </ol> <h2 id="data--develop-data-engine-by-curriculum-learning">Data : Develop Data Engine by Curriculum Learning</h2> <ul> <li> <p>Assisted-manual stage :<br/> public segmentation dataset \(\rightarrow\) SAM \(\rightarrow\) pixel-wise manual augmentation \(\rightarrow\) re-train<br/> After re-training, the number of masks per image increased from 20 to 44 in average<br/> Collect 4.3M masks from 0.12M images</p> </li> <li> <p>Semi-automatic stage :<br/> dataset from previous stage (4.3M masks) \(\rightarrow\) SAM \(\rightarrow\) mask predict 실패한(제외된) object를 annotate \(\rightarrow\) re-train<br/> After re-training, the number of masks per image increased from 44 to 72 in average<br/> Collect 5.9M masks from 0.18M images (totally 4.3M + 5.9M = 10.2M masks)</p> </li> <li> <p>Fully-automatic stage :<br/> dataset from previous stage (10.2M masks) : image에 32 \(\times\) 32 grid points 찍음 \(\rightarrow\) SAM<br/> ambiguity-aware training (whole, part, sub-part 구분 가능)<br/> After filtering masks with high confidence score,<br/> Collect SA-1B dataset : 1.1B masks from 11M images (various HR masks)<br/> 99.1% is fully-automatically generated<br/> follow RAI (Responsible AI) : no bias and blur human faces</p> </li> </ul> <h2 id="task">Task</h2> <p>generalizable (zero-shot transfer to various tasks)</p> <ul> <li>Zero-Shot Transfer Tasks : <ol> <li>Zero-Shot Single Point Valid Mask Evaluation</li> <li>Zero-Shot Edge Detection</li> <li>Zero-Shot Object Proposals</li> <li>Zero-Shot Instance Segmentation</li> <li>Zero-Shot Text-to-Mask (CLIP)</li> </ol> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/4-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/4-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Zero-Shot Single Point Valid Mask Evaluation :<br/> point 찍었을 때 그에 해당하는 mask를 얼마나 잘 생성하는가<br/> use one most-confident mask<br/> compare with RITM model on 23 datasets</p> </li> <li> <p>Zero-Shot Edge Detection :</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/6-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/6-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>About filter : 블로그 맨 아랫 부분에 설명해놓음</p> <ul> <li> <p>Zero-Shot Object Proposals :<br/> mask 예측 후 object의 identity(class)를 얼마나 잘 맞추는가</p> </li> <li> <p>Zero-Shot Instance Segmentation :</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/7-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/7-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Zero-Shot Text-to-Mask :<br/> image \(\rightarrow\) CLIP \(\rightarrow\) image embedding as input<br/> text \(\rightarrow\) CLIP \(\rightarrow\) text embedding as SAM prompt</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/8-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/8-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>SAM’s latent space에서 similar mask embedding vectors within threshold를 추출한 결과 실제로도 semantically similar</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/9-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/9-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A query is indicated by magenta box : top row shows matches at a low threshold and bottom row shows matches at a high threshold </div> <h2 id="code-review">Code Review</h2> <p>다음에 해야지… 라고 미뤄둠..ㅎㅎ</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/10-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/10-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/11-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/11-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/11.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/12-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/12-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/13-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/13-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/13.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/14-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/14-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/14.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/15-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/15-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/15.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/16-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/16-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/16-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/16.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/17-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/17-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/17-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/17.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/18-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/18-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/18-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/18.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/19-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/19-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/19-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/19.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/20-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/20-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/20-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/20.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/21-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/21-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/21-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/21.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/22-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/22-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/22-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/22.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/23-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/23-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/23-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/23.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/24-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/24-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/24-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/24.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/25-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/25-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/25-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/25.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/26-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/26-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/26-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/26.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/27-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/27-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/27-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/27.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/28-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/28-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/28-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/28.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/29-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/29-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/29-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/29.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/30-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/30-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/30-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/30.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/31-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/31-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/31-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/31.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/32-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/32-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/32-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/32.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/33-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/33-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/33-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/33.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/34-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/34-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/34-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/34.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/35-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/35-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/35-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/35.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/36-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/36-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/36-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/36.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/37-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/37-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/37-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/37.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/38-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/38-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/38-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/38.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/39-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/39-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/39-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/39.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/40-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/40-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/40-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/40.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/41-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/41-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/41-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/41.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container>]]></content><author><name></name></author><category term="cv-tasks"/><category term="image"/><category term="segmentation"/><summary type="html"><![CDATA[Promptable Image Segmentation]]></summary></entry><entry><title type="html">FMANet</title><link href="https://semyeong-yu.github.io/blog/2024/FMANet/" rel="alternate" type="text/html" title="FMANet"/><published>2024-05-02T14:00:00+00:00</published><updated>2024-05-02T14:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/FMANet</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/FMANet/"><![CDATA[<h3 id="fma-net--flow-guided-dynamic-filtering-and-iterative-feature-refinement-with-multi-attention-for-joint-video-super-resolution-and-deblurring">FMA-Net : Flow-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring</h3> <h4 id="geunhyuk-youk-jihyong-oh-munchurl-kim">Geunhyuk Youk, Jihyong Oh, Munchurl Kim</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2401.03707">https://arxiv.org/abs/2401.03707</a><br/> project website :<br/> <a href="https://kaist-viclab.github.io/fmanet-site/">https://kaist-viclab.github.io/fmanet-site/</a><br/> pytorch code :<br/> <a href="https://github.com/KAIST-VICLab/FMA-Net">https://github.com/KAIST-VICLab/FMA-Net</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> </blockquote> <hr/> <img-comparison-slider> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/9-480.webp 480w,/assets/img/2024-05-02-FMANet/9-800.webp 800w,/assets/img/2024-05-02-FMANet/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/10-480.webp 480w,/assets/img/2024-05-02-FMANet/10-800.webp 800w,/assets/img/2024-05-02-FMANet/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> <img-comparison-slider> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/11-480.webp 480w,/assets/img/2024-05-02-FMANet/11-800.webp 800w,/assets/img/2024-05-02-FMANet/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/12-480.webp 480w,/assets/img/2024-05-02-FMANet/12-800.webp 800w,/assets/img/2024-05-02-FMANet/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> <h2 id="abstract">Abstract</h2> <p><strong>Task : Joint learning of VSRDB (<code class="language-plaintext highlighter-rouge">video super-resolution and deblurring</code>)</strong></p> <ul> <li>restore HR video from blurry LR video<br/> challenging because should handle two types of degradation (SR and deblurring) simultaneously</li> <li>super-resolution : LR vs HR</li> <li>deblurring : blurry vs sharp</li> </ul> <p><strong>FGDF (<code class="language-plaintext highlighter-rouge">flow-guided dynamic filtering</code>)</strong></p> <ul> <li>precise estimation of both <code class="language-plaintext highlighter-rouge">spatio-temporally-variant</code> <code class="language-plaintext highlighter-rouge">degradation</code> and <code class="language-plaintext highlighter-rouge">restoration</code> kernels that are aware of motion trajectories (not stick to fixed positions)</li> <li>effectively <code class="language-plaintext highlighter-rouge">handle large motions with small-sized kernels</code> (naive dynamic filtering의 한계 극복)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/2-480.webp 480w,/assets/img/2024-05-02-FMANet/2-800.webp 800w,/assets/img/2024-05-02-FMANet/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>DCN (Deformable Conv.) : learn position-invariant \(n \times n\) filter coeff.<br/> vs<br/> DF (Dynamic filtering) : learn position-wise \(n \times n\) dynamic filter coeff.</p> <p>DF (Dynamic Filtering) : fixed surroundings<br/> vs<br/> FGDF (Flow Guided DF) : variable surroundings by learned optical flow</p> <p><strong>FRMA (<code class="language-plaintext highlighter-rouge">iterative feature refinement with multi-attention</code>)</strong></p> <p>refine features by iterative updates<br/> loss : TA (temporal anchor)<br/> multi-attention :</p> <ul> <li><code class="language-plaintext highlighter-rouge">center-oriented</code> attention (focus on target frame)</li> <li><code class="language-plaintext highlighter-rouge">degradation-aware</code> attention (use degradation kernels in globally adaptive manner)</li> </ul> <hr/> <h2 id="related-work">Related Work</h2> <p><strong>VSR (Video Super-Resolution)</strong></p> <p>Based on the number of input frames,</p> <ol> <li><code class="language-plaintext highlighter-rouge">sliding window</code>-based method : recover HR frames by using neighboring frames within a sliding window<br/> use CNN, optical flow estimation, deformable conv., or transformer focusing on temporal alignment<br/> vs</li> <li><code class="language-plaintext highlighter-rouge">recurrent</code>-based method : sequentially propagate the latent features of one frame to the next frame<br/> Chan et al. <d-cite key="vsr">[1]</d-cite> BasicVSR++ : combine bidirectional propagation of past and future frames into current frame features<br/> limit : gradient vanishing</li> </ol> <p><strong>DB (Video Deblurring)</strong></p> <p>Zhang et al. <d-cite key="adversarial">[2]</d-cite> 3D CNN<br/> Li et al. <d-cite key="groupshift">[3]</d-cite> grouped spatial-temporal shifts<br/> transformer-based : Restormer <d-cite key="restormer">[4]</d-cite>, Stripformer <d-cite key="stripformer">[5]</d-cite>, RVRT <d-cite key="rvrt">[6]</d-cite></p> <p><strong>Joint learning of VSRDB (not sequential cascade of VSR and DB)</strong></p> <p>Previous works are mostly designed for ISRDB</p> <p>Fang et al. <d-cite key="HOFFR">[7]</d-cite> HOFFR : the first deep-learning-based VSRDB<br/> limit : struggle to deblur spatially-variant motion blur because 2D CNN has spatially-equivariant and input-independent filters</p> <p><strong>Dynamic Filter Network</strong></p> <p>predict spatially-variant degradation or restoration kernels</p> <p>Zhou et al. <d-cite key="adaptivefilter">[8]</d-cite> :<br/> spatially adaptive deblurring filter for recurrent video deblurring<br/> Kim et al. <d-cite key="koalanet">[9]</d-cite> KOALAnet :<br/> blind SR predicts spatially-variant degradation and upsampling filters</p> <ul> <li>limit : apply dynamic filtering only to the reference frame (target position and its fixed surrounding neighbors), so cannot accurately exploit spatio-temporally-variant-motion info. from adjacent frames</li> <li>limit : if apply dynamic filtering to adjacent frames \(\rightarrow\) large-sized filters are required to capture large motions \(\rightarrow\) high computational complexity</li> <li>limit : <d-cite key="separableconv">[10]</d-cite> suggested two separable large 1D kernels to approximate a large 2D kernel \(\rightarrow\) does not capture fine detail, so inappropriate for video</li> </ul> <hr/> <h2 id="method">Method</h2> <p><strong>Overview</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/1-480.webp 480w,/assets/img/2024-05-02-FMANet/1-800.webp 800w,/assets/img/2024-05-02-FMANet/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>FMA-Net : VSRDB framework based on FGDF and FRMA<br/> allow for small-to-large motion representation learning</p> <ul> <li>input : <code class="language-plaintext highlighter-rouge">blurry LR sequence</code> \(X = \left\lbrace X_{c-N}:X_{c+N} \right\rbrace \in R^{T \times H \times W \times 3}\) where \(T=2N+1\) and \(c\) is a center frame index</li> <li>goal : predict <code class="language-plaintext highlighter-rouge">sharp HR center frame</code> \(\hat Y_{c} \in R^{sH \times sW \times 3}\) where \(s\) is SR scale factor</li> </ul> <ol> <li><code class="language-plaintext highlighter-rouge">degradation</code> learning network \(Net^{D}\) : learn <code class="language-plaintext highlighter-rouge">motion-aware</code> <code class="language-plaintext highlighter-rouge">spatio-temporally-variant</code> degradation kernels</li> <li><code class="language-plaintext highlighter-rouge">restoration</code> network \(Net^{R}\) : utilize these degradation kernels in a globally adaptive manner to restore center frame \(X_c\)</li> <li>\(Net^{D}\) and \(Net^{R}\) consist of FRMA blocks and FGDF module</li> </ol> <p><strong>FRMA block</strong></p> <p>pre-trained optical flow network : unstable for blurry frames and computationally expensive</p> <p>vs</p> <blockquote> <p>FRMA block :<br/> learn <code class="language-plaintext highlighter-rouge">self-induced</code> optical flow in a residual learning manner<br/> learn <code class="language-plaintext highlighter-rouge">multiple</code> optical flows with corresponding occlusion masks<br/> \(\rightarrow\) flow diversity enables to learn one-to-many relations b.w. pixels in a target frame and its neighbor frames<br/> \(\rightarrow\) beneficial since <code class="language-plaintext highlighter-rouge">blurry frame's pixel info. is spread due to light accumulation</code></p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/3-480.webp 480w,/assets/img/2024-05-02-FMANet/3-800.webp 800w,/assets/img/2024-05-02-FMANet/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Three features</p> <ol> <li>\(F \in R^{T \times H \times W \times C}\) :<br/> <code class="language-plaintext highlighter-rouge">temporally-anchored (unwarped)</code> feature at each frame index \(0 \sim T-1\)<br/> dim. T에 걸친 전체 feature</li> <li>\(F_w \in R^{H \times W \times C}\) :<br/> <code class="language-plaintext highlighter-rouge">warped</code> feature<br/> target frame feature 관련</li> <li>\(\boldsymbol f = \left \lbrace f_{c \rightarrow c+t}^{j}, o_{c \rightarrow c+t}^{j} \right \rbrace _{j=1:n}^{t=-N:N} \in R^{T \times H \times W \times (2+1)n}\) :<br/> multi-<code class="language-plaintext highlighter-rouge">flow-mask</code> pairs<br/> \(f_{c \rightarrow c+t}^{j}\) : learnable optical flow<br/> \(o_{c \rightarrow c+t}^{j}\) : learnable occlusion mask (sigmoid for stability)<br/> \(n\) is the number of multi-flow-mask pairs from the center frame index \(c\) to each frame index<br/> <code class="language-plaintext highlighter-rouge">왜 dim. (2+1)???</code> \(\rightarrow\) optical flow \(R^2\) and occlusion mask \(R^1\)</li> </ol> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/16-480.webp 480w,/assets/img/2024-05-02-FMANet/16-800.webp 800w,/assets/img/2024-05-02-FMANet/16-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>(i+1)-th Feature Refinement : 위첨자로 표기<br/> <code class="language-plaintext highlighter-rouge">feature refine 식 기원??</code> \(\rightarrow\) BasicVSR++에서 아이디어 따와서 iterative하게 변형</p> <ol> <li>\(F^{i+1}\)=RDB(\(F^{i}\)) :<br/> RDB <d-cite key="RDB">[11]</d-cite></li> <li>\(\boldsymbol f^{i+1}\) = \(\boldsymbol f^{i}\) + Conv3d(concat(\(\boldsymbol f^{i}\), \(W\)(\(F^{i+1}\), \(\boldsymbol f^{i}\)), \(F_{c}^{0}\)))<br/> \(W\)(\(F^{i+1}\), \(\boldsymbol f^{i}\)) : warp \(F^{i+1}\) to center frame index \(c\) based on \(f^{i}\)<br/> \(W\) : occlusion-aware backward warping<br/> concat : along channel dim.<br/> \(F_{c}^{0} \in R^{H \times W \times C}\) : feature map at center frame index \(c\) of the initial feature \(F^{0} \in R^{T \times H \times W \times C}\)</li> <li>\(\tilde F_{w}^{i}\) = Conv2d(concat(\(F_{w}^{i}\), \(r_{4 \rightarrow 3}\)(\(W\)(\(F^{i+1}\), \(\boldsymbol f^{i+1}\)))))<br/> \(r_{4 \rightarrow 3}\) : reshape from \(R^{T \times H \times W \times C}\) to \(R^{H \times W \times TC}\) for feature aggregation</li> <li>\(F_w^{i+1}\) = Multi-Attn(\(\tilde F_{w}^{i}\), \(F_{c}^{0}\)(, \(k^{D, i}\)))</li> </ol> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/15-480.webp 480w,/assets/img/2024-05-02-FMANet/15-800.webp 800w,/assets/img/2024-05-02-FMANet/15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>RDB Network <d-cite key="RDB">[11]</d-cite> :<br/> TBD</p> </blockquote> <blockquote> <p>RRDB Network <d-cite key="rrdb">[15]</d-cite> :<br/> TBD</p> </blockquote> <blockquote> <p>Occlusion-Aware Backward Warping <d-cite key="warp">[12]</d-cite> <d-cite key="warpp">[13]</d-cite> <d-cite key="warppp">[14]</d-cite> :<br/> TBD</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/3-480.webp 480w,/assets/img/2024-05-02-FMANet/3-800.webp 800w,/assets/img/2024-05-02-FMANet/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Multi-Attention :</p> <ul> <li><code class="language-plaintext highlighter-rouge">CO(center-oriented)</code> attention :<br/> better align \(\tilde F_{w}^{i}\) to \(F_{c}^{0}\) (center feature map of initial temporally-anchored feature)</li> <li><code class="language-plaintext highlighter-rouge">DA(degradation-aware)</code> attention :<br/> \(\tilde F_{w}^{i}\) becomes globally adaptive to spatio-temporally variant degradation by using degradation kernels \(K^{D}\)</li> </ul> </blockquote> <ul> <li> <p>CO attention :<br/> \(Q=W_{q} F_{c}^{0}\)<br/> \(K=W_{k} \tilde F_{w}^{i}\)<br/> \(V=W_{v} \tilde F_{w}^{i}\)<br/> \(COAttn(Q, K, V) = softmax(\frac{QK^{T}}{\sqrt{d}})V\)<br/> 실험 결과, \(\tilde F_{w}^{i}\)가 자기 자신(self-attention)이 아니라 \(F_{c}^{0}\)과의 relation에 집중할 때 better performance</p> </li> <li> <p>DA attention :<br/> CO attention과 비슷하지만,<br/> Query 만들 때 \(F_{c}^{0}\) 대신 \(k^{D, i}\) 사용<br/> \(\tilde F_{w}^{i}\) becomes globally adaptive to spatio-temporally-variant degradation<br/> \(k^{D, i} \in R^{H \times W \times C}\) : degradation features adjusted by conv. with \(K^{D}\) (motion-aware spatio-temporally-variant degradation kernels) 에 대해<br/> \(Q=W_{q} k^{D, i}\)<br/> DA attention은 \(Net^{D}\) 말고 \(Net^{R}\) 에서만 사용</p> </li> </ul> <p><strong>FGDF</strong></p> <ul> <li> <p>spatio-temporal Dynamic Filter :<br/> \(y(p) = \sum_{t=-N}^{N} \sum_{k=1}^{n^2} F_{c+t}^{p}(p_k) x_{c+t}(p+p_k)\)<br/> where<br/> \(c\) : center frame index<br/> \(p_k \in \{ (- \lfloor \frac{n}{2} \rfloor, - \lfloor \frac{n}{2} \rfloor), \cdots , (\lfloor \frac{n}{2} \rfloor, \lfloor \frac{n}{2} \rfloor) \}\) : sampling offset for conv. with \(n \times n\) kernel<br/> \(F \in R^{T \times H \times W \times n^{2}}\) : predicted \(n \times n\) dynamic filter<br/> \(F^p \in R^{T \times n^{2}}\) : predicted \(n \times n\) dynamic filter at position p</p> </li> <li> <p>limit :<br/> fixed position (\(p\)) and fixed surrounding neighbors (\(p_k\))<br/> \(\rightarrow\) To capture large motion, require large-sized filter</p> </li> </ul> <blockquote> <p>solution : <code class="language-plaintext highlighter-rouge">FGDF</code><br/> kernels - dynamically generated / pixel-wise (position-wise) / variable surroundings guided by optical flow<br/> \(\rightarrow\) can handle large motion with relatively small-sized filter<br/> \(y(p) = \sum_{t=-N}^{N} \sum_{k=1}^{n^2} F_{c+t}^{p}(p_k) x_{c+t}^{\ast}(p+p_k)\)<br/> where <br/> \(x_{c+t}^{\ast} = W(x_{c+t}, \boldsymbol f_{c+t})\) : <code class="language-plaintext highlighter-rouge">warped input feature</code> based on \(\boldsymbol f_{c+t}\)<br/> \(\boldsymbol f_{c+t}\) : <code class="language-plaintext highlighter-rouge">flow-mask pair</code> from frame index \(c\) to \(c+t\)</p> </blockquote> <p><strong>Overall Architecture</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/1-480.webp 480w,/assets/img/2024-05-02-FMANet/1-800.webp 800w,/assets/img/2024-05-02-FMANet/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Degradation Network \(Net^{D}\)<br/> input : blurry LR sequence \(\boldsymbol X\) and sharp HR sequence \(\boldsymbol Y\)<br/> goal : <code class="language-plaintext highlighter-rouge">predict flow and degradation kernels in sharp HR sequence</code> \(\boldsymbol Y\)</p> <ol> <li>an image flow-mask pair \(\boldsymbol f^{Y}\)</li> <li>motion-aware spatio-temporally-variant degradation kernels \(K^{D}\)<br/> \(\rightarrow\) obtain blurry LR center frame \(\boldsymbol X_{c}\) from sharp HR counterpart \(\boldsymbol Y\)</li> </ol> </blockquote> <ul> <li> <p>step 1-1. initialize<br/> RRDB : <d-cite key="rrdb">[15]</d-cite><br/> \(\boldsymbol X \rightarrow\) 3D RRDB \(\rightarrow F^{0}\)</p> </li> <li> <p>step 1-2. initialize<br/> \(F_{w}^{0} = 0\), \(\boldsymbol f = \left \lbrace f_{c \rightarrow c+t}^{j} = 0, o_{c \rightarrow c+t}^{j} = 1 \right \rbrace _{j=1:n}^{t=-N:N}\)</p> </li> <li> <p>step 2. M FRMA blocks<br/> \(F^{0}, F_{w}^{0}, \boldsymbol f^{0} \rightarrow\) \(M\) FRMA blocks \(\rightarrow F^{M}, F_{w}^{M}, \boldsymbol f^{M}\)</p> </li> <li> <p>step 3-1. <code class="language-plaintext highlighter-rouge">an</code> image flow-mask pair \(\boldsymbol f^{Y} \in R^{T \times H \times W \times (2+1) 1}\)<br/> \(\boldsymbol f^{M} \rightarrow\) Conv3d \(\rightarrow \boldsymbol f^{Y}\)</p> </li> <li> <p>step 3-2. \(\hat X_{sharp}^{D}\) only used in Temporal Anchor (TA) loss<br/> \(F^{M} \rightarrow\) Conv3d \(\rightarrow \hat X_{sharp}^{D} \in R^{T \times H \times W \times 3}\) in image domain</p> </li> <li> <p>step 3-3. motion-aware spatio-temporally-variant degradation kernels \(K^{D} \in R^{T \times H \times W \times k_{d}^{2}}\)<br/> \(K^{D}\) = softmax(Conv3d(\(r_{3 \rightarrow 4}\)(\(F_{w}^{M}\))))<br/> where<br/> \(k_{d}\) : degradation kernel size<br/> sigmoid for normalization : all kernels have <code class="language-plaintext highlighter-rouge">positive</code> values, which mimics <code class="language-plaintext highlighter-rouge">blur generation process</code></p> </li> <li> <p>step 4. FGDF downsampling to predict blurry center frame \(\hat X_{c}\)<br/> \(\hat X_{c}\) = \(W(\boldsymbol Y, s (\boldsymbol f^{Y} \uparrow _{s}))\) \(\circledast K^{D} \downarrow _{s}\)<br/> where<br/> \(\uparrow\) : \(\times s\) bilinear upsampling<br/> \(W(\boldsymbol Y, s (\boldsymbol f^{Y} \uparrow _{s}))\) : warped sharp HR sequence based on an upsampled image flow-mask pair<br/> \(\circledast K^{D} \downarrow _{s}\) : FGDF with filter \(K^{D}\) with stride \(s\)</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/17-480.webp 480w,/assets/img/2024-05-02-FMANet/17-800.webp 800w,/assets/img/2024-05-02-FMANet/17-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/17.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/1-480.webp 480w,/assets/img/2024-05-02-FMANet/1-800.webp 800w,/assets/img/2024-05-02-FMANet/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Restoration Network \(Net^{R}\)<br/> input : blurry LR sequence \(\boldsymbol X\) and \(F^{M}, \boldsymbol f^{M}, K^{D}\) from \(Net^{D}\)<br/> goal : <code class="language-plaintext highlighter-rouge">predict flow and restoration kernels in blurry LR sequence</code> \(\boldsymbol X\)</p> <ol> <li>an image flow-mask pair \(\boldsymbol f^{X}\)</li> <li>restoration kernels \(K^{R}\)<br/> \(\rightarrow\) obtain sharp HR center frame \(\hat Y_{c}\) from blurry LR counterpart \(\boldsymbol X\)</li> </ol> </blockquote> <ul> <li> <p>step 1-1. initialize \(F^{0}\)<br/> RRDB : <d-cite key="rrdb">[15]</d-cite><br/> concat(\(\boldsymbol X\), \(F^{M}\) from \(Net^{D}\)) \(\rightarrow\) 3D RRDB \(\rightarrow\) \(F^{0}\)</p> </li> <li> <p>step 1-2. initialize \(F_{w}^{0}\), \(\boldsymbol f^{0}\)<br/> \(F_{w}^{0} = 0\), \(\boldsymbol f^{0} = \boldsymbol f^{M}\) from \(Net^{D}\)</p> </li> <li> <p>step 2-1. compute \(k^{D, i} \in R^{H \times W \times C}\) for DA attention</p> </li> <li> <p>step 2-2. M FRMA blocks<br/> \(F^{0}, F_{w}^{0}, \boldsymbol f^{0}, k^{D, i} \rightarrow\) \(M\) FRMA blocks \(\rightarrow F^{M}, F_{w}^{M}, \boldsymbol f^{M}\)</p> </li> <li> <p>step 3-1. <code class="language-plaintext highlighter-rouge">an</code> image flow-mask pair \(\boldsymbol f^{X} \in R^{T \times H \times W \times (2+1) 1}\)<br/> \(\boldsymbol f^{M} \rightarrow\) Conv3d \(\rightarrow \boldsymbol f^{X}\)</p> </li> <li> <p>step 3-2. \(\hat X_{sharp}^{R}\) only used in Temporal Anchor (TA) loss<br/> \(F^{M} \rightarrow\) Conv3d \(\rightarrow \hat X_{sharp}^{R} \in R^{T \times H \times W \times 3}\) in image domain</p> </li> <li> <p>step 3-3. motion-aware spatio-temporally-variant \(\times s\) upsampling and restoration kernels \(K^{R} \in R^{T \times H \times W \times s^{2} k_{r}^{2}}\)<br/> \(K^{R}\) = Normalize(Conv3d(\(r_{3 \rightarrow 4}\)(\(F_{w}^{M}\))))<br/> where<br/> \(k_{r}\) : restoration kernel size<br/> Normalize : w.r.t all kernels at temporally co-located positions over \(X\) (\(T\) dim.에 대해 normalize)</p> </li> <li> <p>step 3-4. high-frequency detail \(\hat Y_{r}\)<br/> \(F_{w}^{M} \rightarrow\) stacked conv. and pixel shuffle \(\rightarrow \hat Y_{r}\)</p> </li> <li> <p>step 4. FGDF upsampling to predict sharp center frame \(\hat Y_{c}\)<br/> \(\hat Y_{c}\) = \(\hat Y_{r}\) + \(W(\boldsymbol X, \boldsymbol f^{X})\) \(\circledast K^{D} \uparrow _{s}\)<br/> where<br/> \(W(\boldsymbol X, \boldsymbol f^{X})\) : warped blurry LR sequence based on an image flow-mask pair<br/> \(\circledast K^{D} \uparrow _{s}\) : \(\times s\) dynamic upsampling with kernel \(K^{R}\)</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/17-480.webp 480w,/assets/img/2024-05-02-FMANet/17-800.webp 800w,/assets/img/2024-05-02-FMANet/17-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/17.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Training</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/4-480.webp 480w,/assets/img/2024-05-02-FMANet/4-800.webp 800w,/assets/img/2024-05-02-FMANet/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Stage 1. Pre-train \(Net^{D}\)</p> <ul> <li>loss 1. <code class="language-plaintext highlighter-rouge">reconstruction loss</code> for blurry LR \(X_{c}\)<br/> \(\hat X_{c}\) \(\leftrightarrow\) \(X_{c}\)</li> <li>loss 2. <code class="language-plaintext highlighter-rouge">optical flow warping loss</code> (warping from c to c+t) in \(\boldsymbol Y\)<br/> \(W(Y_{t+c}, s (\boldsymbol f_{t+c}^{Y} \uparrow _{s}))\) \(\leftrightarrow\) \(Y_{c}\)</li> <li>loss 3. <code class="language-plaintext highlighter-rouge">optical flow refining loss</code> in \(\boldsymbol Y\)<br/> \(f^{Y}\) \(\leftrightarrow\) \(f_{RAFT}^{Y}\)<br/> where<br/> \(f^{Y}\) is image optical flow (no occlusion mask) contained in \(\boldsymbol f^{Y}\)<br/> \(f_{RAFT}^{Y}\) is pseudo-GT optical flow by pre-trained RAFT model <d-cite key="Raft">[16]</d-cite></li> <li>loss 4. <code class="language-plaintext highlighter-rouge">Temporal Anchor (TA) loss</code> for sharp LR \(X_{sharp}\)<br/> <code class="language-plaintext highlighter-rouge">It anchors and sharpens each feature w.r.t corresponding frame index</code><br/> \(\hat X_{sharp}^{D}\) \(\leftrightarrow\) \(X_{sharp}\)<br/> where<br/> sharp HR sequence \(\boldsymbol Y \rightarrow\) bicubic downsampling \(\rightarrow\) GT sharp LR sequence \(X_{sharp}\)<br/> \(\rightarrow\) keep each feature temporally anchored for the corresponding frame index<br/> \(\rightarrow\) constrain the solution space to distinguish warped and unwarped features<br/> <code class="language-plaintext highlighter-rouge">???</code><br/> \(\rightarrow\) iteratively 학습하다보니 frame 0, 1, 2의 features인 \(F \in R^{T \times H \times W \times C}\) 가 점점 target frame 1의 feature인 \(F_w\) 에 가깝게 frame 0.7,, 1, 1.3 느낌으로 업데이트됨<br/> \(\rightarrow\) \(F \in R^{T \times H \times W \times C}\) 의 특성을 유지하도록 downsampled \(\boldsymbol Y\)와 비교하는 Temporal Anchor (TA) loss 추가!</li> </ul> <blockquote> <p>RAFT: Recurrent all-pairs field transforms for optical flow <d-cite key="Raft">[16]</d-cite> :<br/> 핵심 아이디어 : TBD</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/5-480.webp 480w,/assets/img/2024-05-02-FMANet/5-800.webp 800w,/assets/img/2024-05-02-FMANet/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Stage 2. Jointly train \(Net^{D}\) and \(Net^{R}\)</p> <ul> <li>loss 1. <code class="language-plaintext highlighter-rouge">restoration loss</code> for sharp HR \(Y_{c}\)<br/> \(\hat Y_{c}\) \(\leftrightarrow\) \(Y_{c}\)</li> <li>loss 2. <code class="language-plaintext highlighter-rouge">optical flow warping loss</code> (warping from c to c+t) in \(\boldsymbol X\)<br/> Stage 1.의 loss 2.와 동일한 원리</li> <li>loss 3. <code class="language-plaintext highlighter-rouge">Temporal Anchor (TA) loss</code> for sharp LR \(X_{sharp}\)<br/> Stage 1.의 loss 4.와 동일한 원리</li> <li>loss 4. \(L_{D}\)<br/> Stage 1.의 loss들<br/> <code class="language-plaintext highlighter-rouge">왜 X optical flow에 대해선 RAFT loss 안 했지??</code><br/> \(\rightarrow\) RAFT model에서 구한 optical flow는 sharp HR sequence에 대한 거라서!</li> </ul> <hr/> <h2 id="results">Results</h2> <p><strong>Settings</strong></p> <p>LR patch size : 64 \(\times\) 64<br/> the number of FRMA blocks : \(M\) = 4<br/> the number of multi-flow-mask pairs : \(n\) = 9<br/> degradation and restoration kernel size : \(k_{d}\), \(k_{r}\) = 20, 5<br/> the number of frames in sequence : \(T\) = 3 (\(N\) = 1)<br/> ratio b.w. HR and LR : \(s\) = 4<br/> multi-attention block : utilize multi-Dconv head transposed attention (MDTA) and Gated-Dconv feed-forward network (GDFN) from Restormer <d-cite key="restormer">[4]</d-cite></p> <blockquote> <p>multi-Dconv head transposed attention and Gated-Dconv feed-forward network <d-cite key="restormer">[4]</d-cite> :<br/> TBD</p> </blockquote> <p><strong>Datasets and Evaluation Metrics</strong></p> <ul> <li> <p>Datasets :<br/> REDS dataset : train and test<br/> GoPro and YouTube dataset : test (generalization)<br/> \(\rightarrow\) spatially bicubic downsampling to make LR sequence and temporally downsampling to make lower fps sequence</p> </li> <li> <p>Evaluation Metrics :<br/> PSNR and SSIM for image quality<br/> tOF for temporal consistency</p> </li> </ul> <p><strong>Comparision with SOTA</strong></p> <blockquote> <p>SOTA methods (SR) :<br/> single-image SR : SwinIR <d-cite key="swinir">[17]</d-cite> and HAT <d-cite key="hat">[18]</d-cite><br/> video SR : BasicVSR++ <d-cite key="vsr">[1]</d-cite> and FTVSR <d-cite key="ftvsr">[19]</d-cite></p> </blockquote> <blockquote> <p>SOTA methods (DB) :<br/> single-image deblurring : Restormer <d-cite key="restormer">[4]</d-cite> and FFTformer <d-cite key="fftformer">[20]</d-cite><br/> video deblurring : RVRT <d-cite key="rvrt">[6]</d-cite> and GShiftNet <d-cite key="gshiftnet">[21]</d-cite></p> </blockquote> <blockquote> <p>SOTA methods (VSRDB) :<br/> HOFFR <d-cite key="HOFFR">[7]</d-cite></p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/18-480.webp 480w,/assets/img/2024-05-02-FMANet/18-800.webp 800w,/assets/img/2024-05-02-FMANet/18-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/18.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>VSRDB methods have superior performance compared to sequential cascade of SR and DB<br/> \(\rightarrow\) SR and DB tasks are highly inter-correlated</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/7-480.webp 480w,/assets/img/2024-05-02-FMANet/7-800.webp 800w,/assets/img/2024-05-02-FMANet/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/8-480.webp 480w,/assets/img/2024-05-02-FMANet/8-800.webp 800w,/assets/img/2024-05-02-FMANet/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <p><strong>Ablation Study</strong></p> <ul> <li>FGDF<br/> FGDF is better than conventional dynamic filtering for all ranges of motion magnitudes<br/> conventional dynamic filtering is especially not good for large motion</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/13-480.webp 480w,/assets/img/2024-05-02-FMANet/13-800.webp 800w,/assets/img/2024-05-02-FMANet/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> PSNR/tOF according to the average optical flow magnitude b.w. two consecutive frames </div> <ul> <li>Design of FMA-Net <ol> <li>the number of multi-flow-mask pairs \(n\) \(\propto\) performance</li> <li>motion info. from multi-flow-mask pairs \(\boldsymbol f\) is better than motion info. from DCN (Deformable Conv.) due to self-induced sharper optical flows and occlusion masks</li> <li>RAFT loss and TA loss</li> <li>two-stage (\(Net^{D} \rightarrow\) both) training is better than end-to-end training</li> <li>multi-attention (CO + DA) is better than self-attention + SFT(spatial feature transform) <d-cite key="SFT">[22]</d-cite></li> </ol> </li> </ul> <blockquote> <p>SFT (spatial feature transform) <d-cite key="SFT">[22]</d-cite><br/> ddd</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/14-480.webp 480w,/assets/img/2024-05-02-FMANet/14-800.webp 800w,/assets/img/2024-05-02-FMANet/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="conclusion">Conclusion</h2> <p>VSRDB framework based on FGDF and FRMA</p> <ul> <li>FRMA :<br/> iteratively update features (e.g. self-induced optical flow)<br/> multi-attention (CO + DA attention)</li> <li>FGDF :<br/> predict flow-mask pair with flow-guided dynamic filters \(K^{D}\) and \(K^{R}\) that are aware of motion<br/> can handle large motion</li> <li>TA loss :<br/> temporally anchors and sharpens unwarped features</li> <li>2-stage training :<br/> because, during multi-attention of \(Net^{R}\), warped feature \(F_{w}\) is adjusted by predicted degradation \(K^{D}\) from \(Net^{D}\) in globally adaptive manner</li> </ul> <h2 id="limitation">Limitation</h2> <ul> <li>2-stage approach has longer training time than end-to-end approach</li> <li>In extreme contidions such as object rotation, it is hard to predict accurate optical flow<br/> \(\rightarrow\) learnable homography parameters or quaternion representations can be one option to handle rotational motions</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/19-480.webp 480w,/assets/img/2024-05-02-FMANet/19-800.webp 800w,/assets/img/2024-05-02-FMANet/19-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/19.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="super-resolution"/><category term="super-resolution"/><category term="deblur"/><category term="flow"/><category term="dynamic"/><category term="attention"/><summary type="html"><![CDATA[Flow-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring]]></summary></entry><entry><title type="html">NeRF</title><link href="https://semyeong-yu.github.io/blog/2024/NeRF/" rel="alternate" type="text/html" title="NeRF"/><published>2024-04-10T21:00:00+00:00</published><updated>2024-04-10T21:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/NeRF</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/NeRF/"><![CDATA[<h2 id="nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h2> <h4 id="ben-mildenhall-pratul-psrinivasan-matthew-tancik">Ben Mildenhall, Pratul P.Srinivasan, Matthew Tancik</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2003.08934">https://arxiv.org/abs/2003.08934</a><br/> project website :<br/> <a href="https://www.matthewtancik.com/nerf">https://www.matthewtancik.com/nerf</a><br/> pytorch code :<br/> <a href="https://github.com/yenchenlin/nerf-pytorch">https://github.com/yenchenlin/nerf-pytorch</a><br/> <a href="https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file">https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file</a><br/> tiny tensorflow code :<br/> <a href="https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb">https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb</a><br/> referenced blog :<br/> <a href="https://csm-kr.tistory.com/64">https://csm-kr.tistory.com/64</a><br/> <a href="https://yconquesty.github.io/blog/ml/nerf/nerf_rendering.html#the-rendering-formula">https://yconquesty.github.io/blog/ml/nerf/nerf_rendering.html#the-rendering-formula</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>여러 각도의 camera center에서 each input image pixel 방향으로 ray(r=o+td)를 쏜다.</li> <li>ray를 discrete points로 sampling한다.</li> <li>3D coordinate x와 viewing direction d를 r(x)와 r(d)로 positional encoding한다.</li> <li>r(x)를 MLP에 넣어 volume density를 얻고 여기에 r(d)까지 넣어 RGB color를 얻는다.</li> <li>coarse network와 fine network(hierarchical sampling) 각각에서 volume density와 color를 이용한 volume rendering으로 ray마다 rendering pixel color를 구한다.</li> </ol> </blockquote> <h2 id="introduction">Introduction</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/3-480.webp 480w,/assets/img/2024-04-10-NeRF/3-800.webp 800w,/assets/img/2024-04-10-NeRF/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="pipeline">Pipeline</h4> <ul> <li> <p>(a) march camera rays and generate sampling of 5D coordinates</p> </li> <li> <p>(b) represents volumetric static scene by optimizing continuous 5D function(fully-connected network)</p> </li> </ul> <ol> <li>input: single continuous <code class="language-plaintext highlighter-rouge">5D coordinate</code><br/> 3D location \(x, y, z\)<br/> 2D direction \(\theta, \phi\)</li> <li>output:<br/> <code class="language-plaintext highlighter-rouge">volume density</code> (differential opacity) (how much radiance is accumulated by a ray)<br/> <code class="language-plaintext highlighter-rouge">view-dependent RGB color</code> (emitted radiance) \(c = (r, g, b)\)</li> </ol> <ul> <li> <p>(c) synthesizes novel view by classic <code class="language-plaintext highlighter-rouge">volume rendering</code> techniques(differentiable) to accumulate(project)(composite) the color/density samples into 2D image along rays</p> </li> <li> <p>(d) loss between synthesized and GT observed images</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/2-480.webp 480w,/assets/img/2024-04-10-NeRF/2-800.webp 800w,/assets/img/2024-04-10-NeRF/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Pipeline of NeRF architecture </div> <h4 id="problem--solution">Problem &amp; Solution</h4> <p>Problem :</p> <ol> <li>not sufficiently high-resolution representation</li> <li>inefficient in the number of samples per camera ray</li> </ol> <p>Solution :</p> <ol> <li>input <code class="language-plaintext highlighter-rouge">positional encoding</code> for MLP to represent higher frequency function</li> <li><code class="language-plaintext highlighter-rouge">hierarchical sampling</code> to reduce the number of queries</li> </ol> <h4 id="contribution">Contribution</h4> <ul> <li>represent continuous scenes as 5D neural radiance fields with basic MLP to render high-resolution novel views</li> <li>differentiable volume rendering + hierarchical sampling</li> <li>positional encoding to map input 5D coordinate into higher dim. space for high-frequency scene representation</li> <li>overcome the storage costs of discretized voxel grids by encoding continuous volume into network’s parameters<br/> =&gt; require only storage costs of sampled volumetric representations</li> </ul> <h2 id="related-work">Related Work</h2> <h4 id="neural-3d-shape-representation">Neural 3D shape representation</h4> <ul> <li> <p>deep networks that map \(xyz\) coordinates to signed distance functions or occupancy fields<br/> =&gt; limit : need GT 3D geometry</p> </li> <li> <p>Niemeyer et al.<br/> =&gt; input : find directly surface intersection for each ray (can calculate exact derivatie)<br/> =&gt; output : diffuse color at each ray intersection location</p> </li> <li> <p>Sitzmann et al.<br/> =&gt; input : each 3D coordinate<br/> =&gt; output : feature vector and RGB color at each 3D coordinate<br/> =&gt; rendering by RNN that marches along each ray to decide where the surface is.</p> </li> </ul> <blockquote> <p>Limit : oversmoothed renderings, so limited to simple shapes with low geometric complexity</p> </blockquote> <h4 id="view-synthesis-and-image-based-rendering">View synthesis and image-based rendering</h4> <ul> <li> <p>Given <code class="language-plaintext highlighter-rouge">dense sampling of views</code>, novel view synthesis is possible by <code class="language-plaintext highlighter-rouge">simple light field sample interpolation</code></p> </li> <li> <p>Given <code class="language-plaintext highlighter-rouge">sparser sampling of views</code>, there are 2 ways :<br/> mesh-based representation and volumetric representation</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Mesh-based</code> representation with either diffuse(난반사) or view-dependent appearance :<br/> Directly optimize mesh representations by differentiable rasterizers or pathtracers so that we reproject and reconstruct images</p> </li> </ul> <blockquote> <p>Limit :<br/> gradient-based optimization is often difficult because of <code class="language-plaintext highlighter-rouge">local minima or discontinuities or poor loss landscape</code><br/> mesh 구조를 유지하면서 <code class="language-plaintext highlighter-rouge">gradient-based optimization하는 게 어렵</code><br/> needs a <code class="language-plaintext highlighter-rouge">template mesh</code> with fixed topology for initialization, which is unavailable in real-world</p> </blockquote> <ul> <li><code class="language-plaintext highlighter-rouge">Volumetric</code> representation :<br/> well-suited for gradient-based optimization and less distracting artifacts<br/> train : predict a sampled volumetric representation (voxel grids) from input images<br/> test : use alpha-(or learned-)compositing along rays to render novel views<br/> (alpha-compositing : 아래 volume rendering section에서 설명 예정)<br/> CNN compensates discretization artifacts from low resolution voxel grids or CNN allows voxel grids to vary on input time</li> </ul> <blockquote> <p>Limit :<br/> good results, but limited by poor time, space complexity due to discrete sampling<br/> \(\rightarrow\) discrete sampling : rendering high resol. image =&gt; finer sampling of 3D space</p> </blockquote> <blockquote> <p>Author’s solution :<br/> encode <code class="language-plaintext highlighter-rouge">continuous</code> volume into network’s parameters<br/> =&gt; higher quality rendering + require only storage cost of those <code class="language-plaintext highlighter-rouge">sampled</code> volumetric representations</p> </blockquote> <h2 id="neural-radiance-field-scene-representation">Neural Radiance Field Scene Representation</h2> <p>represent continuous scene by 5D MLP : (x, d) =&gt; (c, \(\sigma\))</p> <p>Here, there are 2 key-points!</p> <blockquote> <p>multiview consistent :<br/> c is dependent on both x and d, but \(\sigma\) is only dependent on location x<br/> 3D coordinate x =&gt; 8 fc-layers =&gt; volume-density and 256-dim. feature vector</p> </blockquote> <blockquote> <p>Lambertian reflection : diffuse(난반사) vs Specular reflection : 전반사</p> </blockquote> <blockquote> <p>non-Lambertian effects : view-dependent color change to represent specular reflection<br/> feature vector is concatenated with direction d =&gt; 1 fc-layer =&gt; view-dependent RGB color</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/6-480.webp 480w,/assets/img/2024-04-10-NeRF/6-800.webp 800w,/assets/img/2024-04-10-NeRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="volume-rendering-with-radiance-fields">Volume Rendering with Radiance Fields</h2> <h4 id="ray-from-input-image-pre-processing">Ray from input image (pre-processing)</h4> <p>We use <code class="language-plaintext highlighter-rouge">Ray</code> to synthesize <code class="language-plaintext highlighter-rouge">continuous</code>-viewpoint images from <code class="language-plaintext highlighter-rouge">discrete</code> input images</p> <blockquote> <p>\(r(t) = o + td\)<br/> o : camera’s center of projection<br/> d : viewing direction<br/> t \(\in [ t_n , t_f ]\) : distance from camera center b.w. camera’s predefined near and far planes</p> </blockquote> <blockquote> <p>How to calculate viewing direction d??</p> <ul> <li>2D pixel coordinate : \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)</li> <li>2D normalized coordinate (\(z = 1\)) by intrinsic matrix :<br/> \(\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\) = \(K^{-1}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\) = \(\begin{bmatrix} 1/f_x &amp; 0 &amp; W/2 \\ 0 &amp; 1/f_y &amp; H/2 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)<br/> Since \(y, z\) have opposite direction between the real-world coordinate and pixel coordinate, we multiply (-1)<br/> \(\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\) = \(\begin{bmatrix} 1/f_x &amp; 0 &amp; W/2 \\ 0 &amp; -1/f_y &amp; H/2 \\ 0 &amp; 0 &amp; -1 \end{bmatrix}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)<br/> Here, focal length in intrinsic matrix K is usually calculated using camear angle \(\alpha\) as \(\tan{\alpha / 2} = \frac{h/2}{f}\)</li> <li>3D coordinate by extrinsic matrix :<br/> For extrinsic matrix \([R \vert t']\),<br/> \(o = t'\)<br/> \(d = R * \begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\)<br/> Therefore, we can obtain \(r(t) = o + td\)</li> </ul> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/9-480.webp 480w,/assets/img/2024-04-10-NeRF/9-800.webp 800w,/assets/img/2024-04-10-NeRF/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="volume-rendering-from-mlp-output">Volume Rendering from MLP output</h4> <p>We use differential classical volume rendering</p> <blockquote> <p>idea : 빛이 들어올 확률이 클수록, 물체의 밀도가 높을수록 2D 이미지 픽셀에서 색상 채널의 값이 크게 나타나므로<br/> pixel’s color along a ray is equal to the integral of (transmittance) x (volume density) x (color) for all points along one ray.</p> </blockquote> <p><code class="language-plaintext highlighter-rouge">transmittance가 클수록 투명해서 no color에 가까우니까 색상 채널의 값이 작게 나타나야 하는 것 아니야?????</code></p> <blockquote> <p>Let ray \(r\) (traced through desired virtual camera) have near and far bounds \(t_n, t_f\)<br/> expected color of ray \(r\) = \(C(r) = \int_{t_n}^{t_f} T(t) \sigma (r(t)) c(r(t), d) dt\)</p> </blockquote> <ul> <li>\(T(t) = \exp(- \int_{t_n}^{t} \sigma (r(s)) ds)\) :<br/> <code class="language-plaintext highlighter-rouge">transmittance</code><br/> transmittance = 투과도 = <code class="language-plaintext highlighter-rouge">CDF</code> that a ray <code class="language-plaintext highlighter-rouge">does not hit</code> any particles from \(z_0\) to \(z\)<br/> 투과도가 클수록 투명 (no color)</li> <li>\(\sigma (r(t))\) : <code class="language-plaintext highlighter-rouge">volume density</code> along the ray (learned by MLP)<br/> volume density = <code class="language-plaintext highlighter-rouge">opacity</code> = 불투명도 = <code class="language-plaintext highlighter-rouge">extinction coefficient</code> = <code class="language-plaintext highlighter-rouge">alpha value</code> for alpha-compositing</li> <li>\(c(r(t), d)\) : object’s <code class="language-plaintext highlighter-rouge">color</code> along the ray (learned by MLP)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/10-480.webp 480w,/assets/img/2024-04-10-NeRF/10-800.webp 800w,/assets/img/2024-04-10-NeRF/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/11-480.webp 480w,/assets/img/2024-04-10-NeRF/11-800.webp 800w,/assets/img/2024-04-10-NeRF/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>volume rendering 식 유도 과정</p> </blockquote> <p>occluding objects are modeled as spherical particles with radius \(r\)<br/> There are \(A \cdot \Delta z \cdot \rho (z)\)개의 particles in the slice where \(\rho (z)\) is particle density (the number of particles per unit volume)</p> <p>Since solid particles do not overlap for \(\Delta z \rightarrow 0\),<br/> \(A \cdot \Delta z \cdot \rho (z) \cdot \pi r^2\)만큼 area is occluded<br/> 즉, cross section \(A\)에서 \(\frac{A \cdot \Delta z \cdot \rho (z) \cdot \pi r^2}{A} = \pi r^2 \cdot \rho (z) \cdot \Delta z\)의 비율만큼 occluded</p> <p>If \(\frac{A \cdot \Delta z \cdot \rho (z) \cdot \pi r^2}{A}\)만큼 rays are occluded, the light intensity decreases as<br/> \(I(z + \Delta z) = (1 - \pi r^2 \rho (z) \Delta z) \times I(z)\)</p> <p>Then the light density difference \(\Delta I = I(z + \Delta z) - I(z) = - \pi r^2 \rho (z) \Delta z \cdot I(z)\)<br/> 즉, \(dI(z) = - \pi r^2 \rho (z) I(z) dz = - \sigma (z) I(z) dz\)<br/> where <code class="language-plaintext highlighter-rouge">volume density (or opacity)</code> is \(\sigma(z) = \pi r^2 \rho (z)\)<br/> It makes sense because particle area와 particle density(particle 수)가 클수록 ray 감소량 (volume density)이 커지기 때문<br/> ODE 풀면, \(I(z) = I(z_0)\exp(- \int_{z_0}^{z} \sigma (r(s)) ds)\)</p> <p>Let’s define transmittance \(T(z) = \exp(- \int_{z_0}^{z} \sigma (r(s)) ds)\)<br/> where \(I(z) = I(z_0)T(z)\) means the <code class="language-plaintext highlighter-rouge">remainning</code> intensity after rays travel from \(z_0\) to \(z\)<br/> where <code class="language-plaintext highlighter-rouge">transmittance</code> \(T(z)\) means <code class="language-plaintext highlighter-rouge">CDF</code> that a ray <code class="language-plaintext highlighter-rouge">does not hit</code> any particles from \(z_0\) to \(z\)</p> <p>If a ray passes empty space, there is no color<br/> If a ray hits particles, there exists color (<code class="language-plaintext highlighter-rouge">radiance is emitted</code>)<br/> Let’s define \(H(z) = 1 - T(z)\), which means CDF that a ray <code class="language-plaintext highlighter-rouge">hits</code> particles from \(z_0\) to \(z\)<br/> CDF를 미분하면 PDF이므로<br/> Then PDF is \(p_{hit}(z) = \frac{dH}{dz} = - \frac{dT}{dz} = \exp(- \int_{z_0}^{z} \sigma (r(s)) ds) \sigma (z) = T(z) \sigma (z)\)</p> <p>Let a random variable \(R\) be the emitted randiance.<br/> Then PDF \(p_R(ray) = P[R = c(z)] = p_{hit}(z) = T(z) \sigma (z)\)<br/> Then the <code class="language-plaintext highlighter-rouge">color of a pixel is expected radiance for ray</code> bounded from \(t_n\) to \(t_f\)<br/> \(C(ray) = E[R] = \int_{t_n}^{t_f} R \cdot p_R dz = \int_{t_n}^{t_f} c \cdot p_{hit} dz = \int_{t_n}^{t_f} T(z) \sigma (z) c(z) dz\)</p> <p>\(t_n, t_f = 0., 1.\) for scaled-bounded and front-facing scenes after conversion to <code class="language-plaintext highlighter-rouge">NDC (normalized device coordinates)</code><br/> <a href="https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#analysis">How NDC Works?</a></p> <blockquote> <p>To apply the equation to our model by numerical quadrature,<br/> we have to sample discrete points from continuous ray</p> </blockquote> <p>Instead of deterministic quadrature(typically used for rendering voxel grids, but may limit resolution),<br/> author divides a ray \(\left[t_n, t_f\right]\) into N = 64 bins(intervals), and chooses one point \(t_i\) for each bin by uniform sampling<br/> \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N}(t_f - t_n), t_n + \frac{i}{N}(t_f - t_n)\right]\)</p> <p>Although we use discrete N samples, <code class="language-plaintext highlighter-rouge">stratified sampling(층화 표집)</code> enables MLP to be evaluated at continuous positions by optimization</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/5-480.webp 480w,/assets/img/2024-04-10-NeRF/5-800.webp 800w,/assets/img/2024-04-10-NeRF/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Discretized version for N samples by Numerical Quadrature :<br/> expected color \(\hat{C}(r) = \sum_{i=1}^{N} T_i (1 - \exp(- \sigma_{i} \delta_{i})) c_i\)</p> </blockquote> <ul> <li>\(\hat{C}(r)\) : differentiable about (\(\sigma_{i}, c_i\))</li> <li>\(\delta_{j} = t_{j+1} - t_j\) : the distance between adjacent samples</li> <li>\(T(t) = \exp(- \int_{t_n}^{t} \sigma (r(s)) ds) ~~ \rightarrow ~~ T_i = \exp(- \sum_{j=1}^{i-1} \sigma_{j} \delta_{j})\) where T_1 = 1</li> <li>\(\sigma (r(t)) dt ~~ \rightarrow ~~ \sigma_{j} \delta_{j} = 1 - \exp(-\sigma_{j} \delta_{j})\) by Taylor Series Expansion</li> <li> \[c(r(t), d) ~~ \rightarrow ~~ c_i\] </li> </ul> <p>또는</p> <p>\(p_{hit}(z_i) = \frac{dH}{dz} |_{z_i} ~~ \rightarrow ~~ H(z_{i+1}) - H(z_i) = (1 - T(z_{i+1})) - (1 - T(z_i)) = T(z_i) - T(z_{i+1}) = e^{- \sum_{j=1}^{i-1} \sigma_{j} \delta_{j}} - e^{- \sum_{j=1}^{i} \sigma_{j} \delta_{j}} = T(z_i)(1 - e^{- \sigma_{i} \delta_{i}})\)<br/> Then the <code class="language-plaintext highlighter-rouge">color of a pixel is expected radiance for ray</code> bounded from \(t_n\) to \(t_f\)<br/> \(\hat{C}(ray) = E[R] = \int_{t_n}^{t_f} R \cdot p_R dz ~~ \rightarrow ~~ \sum_{i=1}^{N} c_i \cdot p_{hit}(z_i) dz = \sum_{i=1}^{N} c_i T_i (1 - \exp(- \sigma_{i} \delta_{i}))\)</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">Final version</code> :<br/> expected color \(\hat{C}(r) = \sum_{i=1}^{N} T_i \alpha_{i} c_i\)<br/> where \(T_i = \prod_{j=1}^{i-1} (1-\alpha_{j})\) and \(\alpha_{i} = 1 - \exp(-\sigma_{i} \delta_{i})\)<br/> which reduces to traditional <code class="language-plaintext highlighter-rouge">alpha-compositing</code> problem</p> </blockquote> <p>이 때, this volume rendering 식은 <code class="language-plaintext highlighter-rouge">differentiable</code>하므로 end-to-end learning 가능!!<br/> a sequence of samples \(\boldsymbol t = {t_1, t_2, \ldots, t_N}\)에 대해<br/> \(\frac{d\hat{C}}{dc_i} |_{\boldsymbol t} = T_i \alpha_{i}\) \(\frac{d\hat{C}}{d \sigma_{i}} |_{\boldsymbol t} = c_i \times (\frac{dT_i}{d \sigma_{i}} \alpha_{i} + \frac{d \alpha_{i}}{d \sigma_{i}} T_i) = c_i \times (0 + \delta_{i}e^{-\sigma_{i}\delta_{i}} T_i) = \delta_{i} T_i c_i e^{- \sigma_{i} \delta_{i}}\)</p> <blockquote> <p>alpha-compositing :<br/> 여러 frame을 합쳐서 하나의 image로 합성하는 과정으로, 각 frame pixel마다 alpha 값(불투명도 값)(0~1)이 있어 겹치는 부분의 pixel 값을 결정</p> </blockquote> <p>By divide-and-conquer approach (tail recursion),<br/> \(c = \alpha_{1}c_{1} + (1 - \alpha_{1})(\alpha_{2}c_{2} + (1 - \alpha_{2})(\cdots)) = \alpha_{1}c_{1} + (1 - \alpha_{1})\alpha_{2}c_{2} + (1 - \alpha_{1})(1 - \alpha_{2})(\cdots) = \cdots = \sum_{i=1}^{N}(\alpha_{i}c_{i}\prod_{j=1}^{i-1}(1-\alpha_{j}))\) where \(\alpha_{0} = 0\)</p> <p>If \(\alpha_{i} = 1 - \exp(-\sigma_{i} \delta_{i})\),<br/> NeRF volume rendering 식 \(\hat{C}(r) = \sum_{i=1}^{N} T_i \alpha_{i} c_i\)과<br/> alpha-compositing 식 \(c = \sum_{i=1}^{N}(\alpha_{i}c_{i}\prod_{j=1}^{i-1}(1-\alpha_{j}))\)은<br/> SAME!!</p> <h2 id="optimizing-a-neural-radiance-field">Optimizing a Neural Radiance Field</h2> <h4 id="positional-encoding-pre-processing">Positional encoding (pre-processing)</h4> <p>If we use input directly, MLP is biased to learn low-frequency function (oversmoothed appearance) (no detail)<br/> If we map input into <code class="language-plaintext highlighter-rouge">higher dim.</code> space which contains info. from low frequency to high frequency, MLP can fit data with <code class="language-plaintext highlighter-rouge">high-frequency variation</code><br/> Due to positional encoding, MLP can behave as <code class="language-plaintext highlighter-rouge">interpolation function</code> where \(L\) determines the bandwidth of the interpolation kernel <d-cite key="interpolation">[1]</d-cite><br/> \(r : R \rightarrow R^{2L}\) <br/> \(r(p) = (sin(2^0\pi p), cos(2^0\pi p), \cdots, sin(2^{L-1}\pi p), cos(2^{L-1}\pi p))\)<br/> \(L=10\) for \(r(x)\) where x has three coordinates<br/> \(L=4\) for \(r(d)\) where d has three components of the cartesian viewing direction unit vector</p> <h4 id="hierarchical-volume-sampling">Hierarchical volume sampling</h4> <p>Densely evaluating N points by stratified sampling is inefficient<br/> =&gt; We don’t need much sampling at free space or occluded regions<br/> =&gt; Hierarchical sampling enables us to allocate more samples to regions we expect to contain visible content</p> <p>We simultaneously optimize 2 networks with different sampling : <code class="language-plaintext highlighter-rouge">coarse</code> and <code class="language-plaintext highlighter-rouge">fine</code></p> <blockquote> <p>coarse sampling \(N_c\)개 : 위에서 배웠던 내용<br/> author divides a ray into \(N_c\) = 64 bins(intervals), and chooses one point \(t_i\) for each bin by uniform sampling<br/> \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N_c}(t_f - t_n), t_n + \frac{i}{N_c}(t_f - t_n)\right]\)</p> </blockquote> <blockquote> <p>fine sampling \(N_f\)개 : 새로운 내용<br/> coarse sampling model’s output is a <code class="language-plaintext highlighter-rouge">weighted sum of all coarse-sampled colors</code><br/> \(\hat{C}(r) = \sum_{i=1}^{N_c} T_i \alpha_{i} c_i = \sum_{i=1}^{N_c} w_i c_i\)<br/> where we define \(w_i = T_i \alpha_{i} = T_i (1 - \exp(-\sigma_{i} \delta_{i}))\) for \(i=1,\cdots,N_c\)<br/> =&gt; Given the output of <code class="language-plaintext highlighter-rouge">coarse</code> network, we try more informed (better) sampling where samples are biased toward the <code class="language-plaintext highlighter-rouge">relevant parts of the scene volume</code><br/> =&gt; We sample \(N_f\)=128 <code class="language-plaintext highlighter-rouge">fine</code> points following a <code class="language-plaintext highlighter-rouge">piecewise-constant PDF</code> of normalized \(\frac{w_i}{\sum_{j=1}^{N_c} w_j}\)<br/> =&gt; Here, we use <code class="language-plaintext highlighter-rouge">Inverse CDF Method</code> for sampling fine points</p> </blockquote> <blockquote> <p>Inverse transform sampling = Inverse CDF Method :<br/> =&gt; PDF (probability density function) : \(f_X(x)\)<br/> =&gt; CDF (cumulative distribution function) : \(F_X(x) = P(X \leq x) = \int_{-\infty}^x f_X(x) dx\)<br/> idea : 모든 확률 분포의 CDF는 Uniform distribution을 따른다!<br/> =&gt; 따라서 CDF의 y값을 Uniform sampling하면, 그 y에 대응되는 x에 대한 (특정 PDF를 따르는) sampling을 구현할 수 있다!<br/> =&gt; 즉, CDF의 역함수를 계산할 수 있다면, 기본 난수 생성기인 Uniform sampling을 이용해서 확률 분포 X에 대한 sampling을 할 수 있다!<br/> \(F_X(x)\) ~ \(U\left[0, 1\right]\)<br/> \(X\) ~ \(F^{-1}(U\left[0, 1\right])\)</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/4-480.webp 480w,/assets/img/2024-04-10-NeRF/4-800.webp 800w,/assets/img/2024-04-10-NeRF/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We evaluate <code class="language-plaintext highlighter-rouge">coarse</code> network using \(N_c\)=64 points per ray<br/> We evaluate <code class="language-plaintext highlighter-rouge">fine</code> network using \(N_c+N_f\)=64+128=192 points per ray where we sample 128 fine points following PDF of <code class="language-plaintext highlighter-rouge">coarse</code> sampled points<br/> In result, we use a total of 64+192=256 samples per ray to compute the final rendering color \(C(r)\)</p> <h4 id="implementation-details--loss">Implementation details &amp; Loss</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/6-480.webp 480w,/assets/img/2024-04-10-NeRF/6-800.webp 800w,/assets/img/2024-04-10-NeRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>Prepare RGB images, corresponding camera poses, intrinsic parameters and scene bounds (use COLMAP structure-from-motion package to estimate these parameters)</li> <li>From H x W input image, randomly sample a batch of 4096 pixels</li> <li>calculate continuous ray from each pixel \(r(t) = o + kd\)</li> <li>coarse sampling of \(N_c\)=64 points per each ray \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N_c}(t_f - t_n), t_n + \frac{i}{N_c}(t_f - t_n)\right]\)</li> <li>positional encoding \(r(x)\) and \(r(d)\) for input</li> <li>obtain volume density \(\sigma\) by MLP with \(r(x, y, z)\) as input</li> <li>obtain color \(c\) by MLP with \(r(x, y, z)\) and \(r(\theta, \phi)\) as input</li> <li>obtain rendering color of each ray by volume rendering \(\hat{C}(r) = \sum_{i=1}^{N} T_i (1 - \exp(- \sigma_{i} \delta_{i})) c_i\) from two networks ‘coarse’ and ‘fine’</li> <li>compute loss</li> <li>Adam optimizer with learning rate from \(5 \times 10^{-4}\) to \(5 \times 10^{-5}\)</li> <li>optimization for a single scene typically takes around 100-300k iterations (1~2 days) to converge on a single NVIDIA V100 GPU</li> </ol> <p>Here, we use L2 norm for loss<br/> \(L = \sum_{r \in R} \left[{\left\|\hat{C_c}(r)-C(r)\right\|}^2+{\left\|\hat{C_f}(r)-C(r)\right\|}^2\right]\)<br/> \(C(r)\) : GT pixel RGB color<br/> \(\hat{C_c}(r)\) : rendering RGB color from coarse network : to allocate better samples in fine network<br/> \(\hat{C_f}(r)\) : rendering RGB color from fine network : our goal<br/> \(R\) : the set of all pixels(rays) across all images</p> <h2 id="results">Results</h2> <h4 id="datasets">Datasets</h4> <p>Synthetic renderings of objects</p> <ul> <li>Diffuse Synthetic 360 : 4 Lambertian objects with simple geometry</li> <li>Realistic Synthetic 360 : 8 non-Lambertian objects with complicated geometry</li> </ul> <p>Real images of complex scenes</p> <ul> <li>Real Forward-Facing : 8 scenes captured with a handheld cellphone</li> </ul> <h4 id="measurement">Measurement</h4> <ul> <li>PSNR(Peak Signal-to-Noise Ratio) \(\uparrow\) : the ratio between the maximum possible power of a signal and the power of corrupting noise \(10\log_{10}\left(\frac{(MAX)^2}{MSE}\right)\)[dB]</li> <li>SSIM(Structural Similarity Index Map) \(\uparrow\) : compare image qualities in three ways: Lumincance(\(l\)), Contrast(\(c\)), Structural(\(s\))<br/> SSIM(x, y) = \([l(x,y)]^{\alpha}[c(x,y)]^{\beta}[s(x,y)]^{\gamma}=\frac{(2\mu_{x}\mu_{y}+C_1)(2\sigma_{xy}+C_2)}{(\mu_{x}^2+\mu_{y}^2+C_1)(\sigma_{x}^2+\sigma_{y}^2+C_2)}\) where \(l(x,y)=\frac{(2\mu_{x}\mu_{y}+C_1)}{\mu_{x}^2+\mu_{y}^2+C_1}\) and \(c(x,y)=\frac{(2\sigma_{x}\sigma_{y}+C_2)}{\sigma_{x}^2+\sigma_{y}^2+C_2}\) and \(s(x,y)=\frac{\sigma_{xy}+C_3}{\sigma_{x}\sigma_{y}+C_3}\)<br/> SSIM calculator :<br/> https://darosh.github.io/image-ssim-js/test/browser_test.html</li> <li>LPIPS \(\downarrow\)</li> </ul> <h4 id="comparisons">Comparisons</h4> <ul> <li>Neural Volumes (NV) :<br/> It synthsizes novel views of objects that lie entirely within a bounded volume in front of a distinct background.<br/> It optimizes 3D conv. network to predict a discretized RGB\(\alpha\) voxel grid and a 3D warp grid.<br/> It renders novel views by marching rays through the warped voxel grid</li> <li>Scene Representation Networks (SRN) :<br/> It represents continuous scene as an opaque surface.<br/> MLP maps each 3D coordinate to a feature vector, and we optimize RNN to predict the next step size along the ray using the feature vector.<br/> The feature vector from the final step is decoded into a color for that point on the surface. Note that SRN is followup to DeepVoxels by the same authors.</li> <li>Local Light Field Fusion (LLFF) :<br/> designed for producing novel views for well-sampled forward facing scenes<br/> trained 3D conv. network directly predicts a discretized frustum-sampled RGB\(\alpha\) grid (multiplane image), and then renders novel views by alpha-compositing and blending</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/7-480.webp 480w,/assets/img/2024-04-10-NeRF/7-800.webp 800w,/assets/img/2024-04-10-NeRF/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Test for scenes from author's new synthetic dataset </div> <p>LLFF exhibits banding and ghosting artifacts<br/> SRN produces blurry and distorted renderings<br/> NV cannot capture the details<br/> NeRF captures fine details in both geometry and appearance</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/8-480.webp 480w,/assets/img/2024-04-10-NeRF/8-800.webp 800w,/assets/img/2024-04-10-NeRF/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Test for read-world scenes </div> <p>LLFF may have repeated edges because of blending between multiple renderings<br/> NeRF also correctly reconstruct partially-occluded regions<br/> SRN does not capture any high-frequency fine detail</p> <h4 id="discussion">Discussion</h4> <h4 id="ablation-studies">Ablation Studies</h4> <h2 id="conclusion">Conclusion</h2> <p>prior : MLP outputs discretized voxel representations<br/> author : MLP outputs volume density and view-dependent emitted radiance</p> <h2 id="future-work">Future Work</h2> <p>efficiency :<br/> Rather than hierarchical sampling, there is still much more progress to be made for efficient optimization and rendering of neural radiance fields</p> <p>interpretability :<br/> voxel grids or meshes admits reasoning about the expected quality, but it is unclear to analyze these issues when we encode scenes into the weights of MLP</p>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="rendering"/><category term="3d"/><summary type="html"><![CDATA[representing scenes as neural radiance fields for view synthesis]]></summary></entry><entry><title type="html">SfMLearner</title><link href="https://semyeong-yu.github.io/blog/2024/SfMLearner/" rel="alternate" type="text/html" title="SfMLearner"/><published>2024-04-06T17:00:00+00:00</published><updated>2024-04-06T17:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/SfMLearner</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/SfMLearner/"><![CDATA[<h1 id="unsupervised-learning-of-depth-and-ego-motion-from-video">Unsupervised Learning of Depth and Ego-Motion from Video</h1> <h4 id="tinghui-zhou-matthew-brown-noah-snavely-david-g-lowe">Tinghui Zhou, Matthew Brown, Noah Snavely, David G. Lowe</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/1704.07813">https://arxiv.org/abs/1704.07813</a><br/> code :<br/> <a href="https://github.com/tinghuiz/SfMLearner">https://github.com/tinghuiz/SfMLearner</a></p> </blockquote> <h2 id="introduction">Introduction</h2> <ul> <li>SfM : Structure from Motion<br/> end-to-end unsupervised learning from monocular video (only one camera lens)</li> <li><code class="language-plaintext highlighter-rouge">single-view</code> depth estimation by per-pixel depth map</li> <li><code class="language-plaintext highlighter-rouge">multi-view</code> camera motion (= <code class="language-plaintext highlighter-rouge">ego-motion</code> = <code class="language-plaintext highlighter-rouge">pose</code>) by <code class="language-plaintext highlighter-rouge">6-DoF transformation matrices</code></li> <li><code class="language-plaintext highlighter-rouge">unsupervised</code> learning : 직접적인 GT data가 아니라 <code class="language-plaintext highlighter-rouge">view synthesis (reconstruction term)를 supervision</code>으로 씀</li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li>simultaneous estimation of structure and motion through deep learning</li> <li>end-to-end learning of transformation matrix without learning geometry explicitly</li> <li>learning of 3D single-view from registered 2D views</li> <li>unsupervised/self-supervised learning from video</li> </ul> <h2 id="method">Method</h2> <h4 id="approach">Approach</h4> <p>Assumption :<br/> Scenes, which we are interested in, are mostly rigid, so changes across different frames are dominated by camera motion</p> <h4 id="view-synthesis-as-supervision">View Synthesis as supervision</h4> <ul> <li>View Synthesis : as supervision of depth and pose (추후 설명 예정)</li> <li>loss function (reconstruction term) :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/1-480.webp 480w,/assets/img/2024-04-06-SfMLearner/1-800.webp 800w,/assets/img/2024-04-06-SfMLearner/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>\(p\) : index of target view’s pixel coordinates<br/> \(s\) : index of source views<br/> \(I_{t}(p)\) : target view<br/> \(\hat I_{s}(p)\) : source view warped to target coordinate frame (= reconstructed target view) using predicted depth \(\hat D_{t}\) and \(4 \times 4\) camera transformation matrix \(\hat T_{t \rightarrow s}\) and source view \(I_{s}\)</p> <ul> <li>pipeline for depth and pose estimation :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/2-480.webp 480w,/assets/img/2024-04-06-SfMLearner/2-800.webp 800w,/assets/img/2024-04-06-SfMLearner/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="differentiable-depth-image-based-rendering">Differentiable depth image-based rendering</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/3-480.webp 480w,/assets/img/2024-04-06-SfMLearner/3-800.webp 800w,/assets/img/2024-04-06-SfMLearner/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li> <p><code class="language-plaintext highlighter-rouge">Depth CNN</code>을 통해 <code class="language-plaintext highlighter-rouge">target view</code> (single view)로부터 <code class="language-plaintext highlighter-rouge">depth prediction</code> \(\hat D_{t}\) 얻기</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Pose CNN</code>을 통해 <code class="language-plaintext highlighter-rouge">target &amp; source view</code> (multi-view)로부터 \(4 \times 4\) <code class="language-plaintext highlighter-rouge">camera transformation matrix</code> \(\hat T_{t \rightarrow s}\) 얻기</p> </li> <li>target view의 pixels를 source view coordinate으로 <code class="language-plaintext highlighter-rouge">project</code>하기<br/> 값이 아니라 <code class="language-plaintext highlighter-rouge">대응되는 위치</code>를 구하기 위해<br/> projection할 때 depth와 pose 이용 <ul> <li>monocular camera이므로 두 카메라 사이의 상대적인 위치를 설명하는 \([R \vert t]\)는 고려 안함</li> <li>\(K^{-1}p_{t}\) : target view coordinate에서 2D 좌표 \(\rightarrow\) 3D 좌표</li> <li>\(\hat D_{t}(p_{t})K^{-1}p_{t}\) : target view의 3D depth map (= 2D depth \(\times\) 3D 좌표)<br/> full 3D volumetric은 아니고, surface만 나타내는 3D target</li> <li>\(\hat T_{t \rightarrow s} \hat D_{t}(p_{t})K^{-1}p_{t}\) : 3D depth map projected from target view to source view</li> <li>\(K \hat T_{t \rightarrow s} \hat D_{t}(p_{t})K^{-1}p_{t}\) : source view coordinate에서 3D 좌표 \(\rightarrow\) 2D 좌표<br/> <code class="language-plaintext highlighter-rouge">target view의 pixel 좌푯값을 source view의 좌푯값으로 project하는 데 중간에 depth map이 왜 필요한 거지???</code></li> </ul> </li> <li>source view coordinate에서 differentiable bilinear <code class="language-plaintext highlighter-rouge">interpolation</code>으로 value 얻은 뒤 <code class="language-plaintext highlighter-rouge">warp to target coordinate</code> (= <code class="language-plaintext highlighter-rouge">reconstructed target view</code>)<br/> source view의 pixel 값들을 이용해서 reconstruct target view</li> </ol> <h4 id="modeling-the-model-limitation">Modeling the model limitation</h4> <p>Assumption :</p> <ol> <li> <p>objects are static except camera (changes are dominated by camera motion)<br/> 물체들이 움직이지 않아야 Depth CNN과 Pose CNN이 같은 coordinate에 대해 project할 수 있다.</p> </li> <li> <p>there is no occlusion/disocclusion between target view and source view<br/> target view와 source views 중 하나라도 물체가 가려져서 안보인다면 projection 정보가 없어 학습에 문제가 된다.</p> </li> <li> <p>surface is Lambertain so that photo-consistency error is meaningful<br/> 어떤 방향에서 보든 표면이 isotropic 똑같은 밝기로 보인다고 가정 \(\rightarrow\) photo-consistency에 차이가 있을 경우 이는 다른 surface를 의미함</p> </li> </ol> <h4 id="overcoming-the-gradient-locality-at-loss-term">Overcoming the gradient locality at loss term</h4> <ol> <li> <p>To improve robustness, train additional network which predicts <code class="language-plaintext highlighter-rouge">explainability soft mask</code> \(\hat E_{s}\) (= <code class="language-plaintext highlighter-rouge">per-pixel weight</code>), and add it to reconstruction loss term.<br/> deep-learning model은 black-box이므로 explainablity는 중요한 요소</p> </li> <li> <p>trivial sol. \(\hat E_{s} = 0\)을 방지하기 위해, add <code class="language-plaintext highlighter-rouge">regularization</code> term that encourages nonzero prediction of \(\hat E_{s}\)</p> </li> <li> <p>직접 pixel intensity difference로 reconstruction loss를 얻으므로, GT depth &amp; pose로 project하여 얻은 \(p_{s}\) 가 low-texture region or far region에 있을 경우 training 방해 (common issue in motion estimation)<br/> \(\rightarrow\) 해결 1. use conv. encoder-decoder with small bottleneck<br/> \(\rightarrow\) 해결 2. add <code class="language-plaintext highlighter-rouge">multi-scale</code> and <code class="language-plaintext highlighter-rouge">smoothness loss</code> term<br/> (less sensitive to architecture choice, so 이 논문은 해결 2. 적용)</p> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/4-480.webp 480w,/assets/img/2024-04-06-SfMLearner/4-800.webp 800w,/assets/img/2024-04-06-SfMLearner/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> s : source view image index / p : target view pixel index </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/5-480.webp 480w,/assets/img/2024-04-06-SfMLearner/5-800.webp 800w,/assets/img/2024-04-06-SfMLearner/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> l : multi-scale / s : source view image index </div> <h4 id="network-architecture">Network Architecture</h4> <ul> <li>Network 1. <code class="language-plaintext highlighter-rouge">Single-view Depth CNN</code><br/> input : target view<br/> output : per-pixel depth map<br/> DispNet encoder-decoder architecture</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/6-480.webp 480w,/assets/img/2024-04-06-SfMLearner/6-800.webp 800w,/assets/img/2024-04-06-SfMLearner/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Network 2. <code class="language-plaintext highlighter-rouge">Multi-view Pose CNN</code> (아래 figure의 파란 부분)<br/> input : target view concatenated with all source views<br/> output : 6-DoF relative poses between target view and each source view<br/> (Pose CNN estimates <code class="language-plaintext highlighter-rouge">6 channels (3 Euler angles + 3D translation vector)</code> for each source view, and then it is converted to \(4 \times 4\) <code class="language-plaintext highlighter-rouge">transformation matrix</code>)</li> </ul> <p><code class="language-plaintext highlighter-rouge">어떻게 transformation matrix로 변환???</code></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/7-480.webp 480w,/assets/img/2024-04-06-SfMLearner/7-800.webp 800w,/assets/img/2024-04-06-SfMLearner/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/7-480.webp 480w,/assets/img/2024-04-06-SfMLearner/7-800.webp 800w,/assets/img/2024-04-06-SfMLearner/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Network 3. <code class="language-plaintext highlighter-rouge">Explainablity soft mask</code> (= <code class="language-plaintext highlighter-rouge">reconstruction weight per pixel</code>) (위의 figure의 빨간 부분)<br/> output : multi-scale explainability masks<br/> (it estimates <code class="language-plaintext highlighter-rouge">2 channels</code> for each source view at each prediction layer)</li> </ul> <p><code class="language-plaintext highlighter-rouge">weight per pixel인데 왜 2 channels are needed for explainability mask???</code></p> <h2 id="experiments">Experiments</h2> <p>Train : BN, Adam optimizer, monocular camera (one camera lens), resize input image<br/> Test : arbitrary input image size</p> <h4 id="single-view-depth-estimation">Single-view depth estimation</h4> <ul> <li>train model on the split (exclude frames from test sequences and exclude static scene’s pixels with mean optical flow magnitude &lt; 1)</li> <li>pre-trained on Cityscapes dataset / fine-tuned on KITTI dataset / test on Make3D dataset</li> <li>may improve if we also use left-right cycle consistency loss</li> <li>ablation study 결과, explainablity mask를 추가하고 fine-tuning하는 게 더 좋은 성능 도출</li> </ul> <h4 id="multi-view-pose-estimation">Multi-view pose estimation</h4> <ul> <li>trained on KITTI odometry(change in position over time by motion sensor) dataset</li> <li>measurement :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/8-480.webp 480w,/assets/img/2024-04-06-SfMLearner/8-800.webp 800w,/assets/img/2024-04-06-SfMLearner/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>ATE : Absolute Trajectory Error<br/> left/right turning magnitude : coordinate diff. in the side-direction between start and ending frame at test<br/> Mean Odom. : mean of car motion for 5-frame snippets from GT odometry dataset<br/> ORB-SLAM(full) : recover odometry using all frames for loop closure and re-localization<br/> ORB-SLAM(short) : Ours에서처럼, use 5-frame snippets as input<br/> \(\rightarrow\) 특히 small left/right turning magnitude (car is mostly driving forward) 상황에서 Ours가 ORB-SLAM(short)보다 성능 더 좋으므로 monocular SLAM system의 local estimation module을 Ours가 대체할 수 있을 것이라 예상​<br/> (<code class="language-plaintext highlighter-rouge">SLAM 논문 아직 안 읽어봄. 읽어보자.</code>)</p> <h4 id="visualizing-explainability-prediction">Visualizing Explainability Prediction</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/9-480.webp 480w,/assets/img/2024-04-06-SfMLearner/9-800.webp 800w,/assets/img/2024-04-06-SfMLearner/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> highlighted pixels at explainability mask : predicted to be unexplainable </div> <p>explainability = per-pixel weight (confidence 느낌) for reconstruction</p> <p>row 1 ~ 3 : due to motion (dynamic objects are unexplainable)<br/> row 4 ~ 5 : due to occlusion/visibility (disappeared objects are unexplainable)<br/> row 6 ~ 7 : due to other factors (e.g. depth CNN has low confidence on thin structures)</p> <h2 id="discussion">Discussion</h2> <h4 id="contribution">Contribution</h4> <ul> <li>end-to-end <code class="language-plaintext highlighter-rouge">unsupervised</code> learning from <code class="language-plaintext highlighter-rouge">monocular</code> sequences<br/> (기존에는 gt depth로 depth supervision 또는 calibrated stereo images로 pose supervision이었지만, 본 논문은 <code class="language-plaintext highlighter-rouge">view synthesis (reconstruction)을 supervision으로</code> 써서 unsupervised learning으로도 comparable performance 달성)</li> <li>depth CNN recognizes common structural features of objects, and pose CNN uses image correspondence with estimating camera motion</li> </ul> <h4 id="limitation">Limitation</h4> <ol> <li> <p><code class="language-plaintext highlighter-rouge">dynamic objects (X) / occlusion (X) / must be Lambertain surface / vast open scenes (X) / when objects are close to the front of camera (X) / thin structure (X)</code><br/> \(\rightarrow\) 위의 한계들을 개선하고자 explainablity mask (= per-pixel reconstruction confidence 느낌) 도입했지만, it is implicit consideration</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">assume that camera intrinsic K is given</code>, so not generalized to the random videos with unknown camera types</p> </li> <li> <p>predict simplified 3D depth map of <code class="language-plaintext highlighter-rouge">surface</code> (<code class="language-plaintext highlighter-rouge">not full 3D volumetric representation</code>)</p> </li> </ol> <p>중간중간에 있는 질문들은 아직 이해하지 못해서 남겨놓은 코멘트입니다.<br/> 추후에 다시 읽어보고 이해했다면 업데이트할 예정입니다.<br/> 혹시 알고 계신 분이 있으면 댓글로 남겨주시면 감사하겠습니다!</p>]]></content><author><name></name></author><category term="depth-estimation"/><category term="unsupervised"/><category term="depth"/><category term="ego"/><category term="motion"/><category term="video"/><summary type="html"><![CDATA[Unsupervised Learning of Depth and Ego-Motion from Video]]></summary></entry><entry><title type="html">Monocular Depth Estimation with Left Right Consistency</title><link href="https://semyeong-yu.github.io/blog/2024/Monocular_Depth_Estimation_LR_Consistency/" rel="alternate" type="text/html" title="Monocular Depth Estimation with Left Right Consistency"/><published>2024-04-05T17:00:00+00:00</published><updated>2024-04-05T17:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Monocular_Depth_Estimation_LR_Consistency</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Monocular_Depth_Estimation_LR_Consistency/"><![CDATA[<h1 id="unsupervised-monocular-depth-estimation-with-left-right-consistency">Unsupervised Monocular Depth Estimation with Left-Right Consistency</h1> <h4 id="clement-godard-oisin-mac-aodha-gabriel-j-brostow">Clement Godard, Oisin Mac Aodha, Gabriel J. Brostow</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/1609.03677">https://arxiv.org/abs/1609.03677</a><br/> referenced blog :<br/> <a href="https://blog.naver.com/dncks1107/223104039030">https://blog.naver.com/dncks1107/223104039030</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ul> <li>unsupervised mono (single image as input) depth estimation</li> <li>need for binocular stereo image pairs at training</li> <li>novel loss function : left-right consistency b.w. disparity maps</li> </ul> </blockquote> <h2 id="backgrounds">Backgrounds</h2> <h4 id="stereo-depth-estimation">Stereo Depth Estimation</h4> <p>인간은 물체를 두 개의 눈을 통해 바라보고 그 차이를 이용하여 대상까지의 거리를 예측한다. AI는 이러한 인간의 시각 시스템을 모방하여 stereo depth estimation을 통해 대상까지의 깊이를 추정할 수 있다.<br/> Stereo image란 카메라 두 대를 사용하여 찍은 두 이미지를 의미하고, disparity는 한 쌍의 stereo image 간의 pixel difference를 의미한다.<br/> <code class="language-plaintext highlighter-rouge">stereo depth estimation은 stereo image 한 쌍 (left image, right image)을 network의 input</code>으로 넣어 이미지 간의 disparity를 통해 depth를 추정하는 것이다.<br/> <code class="language-plaintext highlighter-rouge">stereo depth estimation의 경우, epipolar geometry라는 수학적 원리에 의해 depth를 계산하기 때문에 비교적 정확하지만 카메라와 물체 사이의 거리가 멀어질수록 불리해진다.</code></p> <h4 id="monocular-depth-estimation">Monocular Depth Estimation</h4> <p><code class="language-plaintext highlighter-rouge">monocular depth estimation은 위와 달리 하나의 image만을 network의 input</code>으로 넣어 depth를 추정하는 방법이다. 물론 test-phase에서 하나의 image를 input으로 넣겠다는 의미이고, 이 논문의 경우 training loss를 구할 때는 stereo image 쌍을 모두 이용하였다.<br/> <code class="language-plaintext highlighter-rouge">mono depth estimation의 경우, 믿을 만한 근거(epipolar geometry와 같은 수학적 원리)가 없기 때문에 정확도가 떨어지지만 하나의 image만 input으로 넣기 때문에 전처리 과정이 간단하고 메모리도 덜 필요로 하여, 어느 정도의 정확도만 확보된다면 실생활에서 적용 가능 범위가 더 넓다.</code></p> <h4 id="monocular-and-stereo-camera">Monocular and Stereo Camera</h4> <p><code class="language-plaintext highlighter-rouge">monocular camera</code> : 특정 시간 t에 한 개의 camera 렌즈를 사용<br/> <code class="language-plaintext highlighter-rouge">stereo camera</code> : 특정 시간 t에 6~7cm 떨어진 두 개의 camera 렌즈를 사용</p> <h2 id="abstract">Abstract</h2> <p>기존의 supervised depth estimation 방식은 성능은 좋지만, 구하기 어려운 pixel-wise ground-truth depth data를 대량으로 필요로 한다는 단점이 있었다. 그래서 본 논문의 저자는 ground-truth depth 정보가 없는 stereo image 쌍으로부터 pixel-level depth map을 합성하도록 훈련하는 unsupervised depth estimation 방식을 제안한다. 효과적인 표기를 위해 아래의 notation을 사용하자.<br/> \(I^{l}\) : left image<br/> \(I^{r}\) : right image<br/> \(d^{r}\) : disparity map from left to right<br/> \(d^{l}\) : disparity map from right to left</p> <p>그렇다면 stereo image \(I^{l}, I^{r}\)로부터 어떻게 depth를 추정할까? <code class="language-plaintext highlighter-rouge">image rectfiication</code>을 거친 뒤, depth를 직접 예측하는 게 아니라 우선 두 개의 <code class="language-plaintext highlighter-rouge">disparity map (dense correspondence field)</code> \(d^{r}, d^{l}\) 을 생성한다. 여기서 disparity map이란, image의 한 pixel이 다른 image의 어느 pixel에 대응하는지에 대한 정보를 의미한다. 이후 \(I^{l}\)과 \(d^{r}\)을 이용하여 \(I^{r \ast} = I^{l}(d^{r})\)을 reconstruct하고, \(I^{r}\)과 \(d^{l}\)을 이용하여 \(I^{l \ast} = I^{r}(d^{l})\)을 reconstruct 한 뒤, \(I^{r \ast}\)과 \(I^{r}\) 간의 reconstruction loss와 \(I^{l \ast}\)과 \(I^{l}\) 간의 <code class="language-plaintext highlighter-rouge">reconstruction loss</code>를 이용하여 모델을 학습시킨다. 그런데 reconstruction loss만 사용한다면 depth image의 quality가 저하된다고 한다. 따라서 본 논문의 저자는 ​\(d^{r}\)과 (projected \(d^{l}\)) = \(d^{l}(d^{r})\) 간의 차이도 고려하는 <code class="language-plaintext highlighter-rouge">left-right disparity consistency loss</code>라는 논문의 핵심 아이디어를 제안하였다.</p> <h2 id="contribution">Contribution</h2> <ul> <li>end-to-end unsupervised monocular depth estimation</li> <li>new training loss that enforces left-right disparity consistency</li> </ul> <h2 id="related-work">Related Work</h2> <h4 id="supervised-stereo-depth-estimation">supervised stereo depth estimation</h4> <p>DispNet (Mayer et al.) :<br/> directly predict the disparity for each pixel by regression loss<br/> 단점 : need lots of ground-truth disparity data and stereo image pairs, which are hard to obtain in real-world</p> <h4 id="unsupervised-depth-estimation">unsupervised depth estimation</h4> <p>Deep Stereo (Flynne et al.) :<br/> select pixels from nearby images and generate new views by using the relative pose of multiple cameras<br/> 단점 : At test phase, need several nearby posed images, which is not mono depth estimation</p> <p>Deep3D (Xie et al.) :<br/> make a distribution over all the possible disparities for each pixel and generate right view from an input left image by using image reconstruction loss<br/> 단점 : need much memory if there are lots of possible disparities. So, it is not scalable to bigger output resolutions</p> <p>Garg et al. :<br/> 본 논문과 유사하게 unsupervised mono depth estimation with image reconstruction loss<br/> 단점 : not fully differentiable (이를 보완하고자 Taylor approximation을 수행하긴 했지만 이는 more challenging to optimize)</p> <h2 id="method">Method</h2> <h4 id="depth-estimation-as-image-reconstruction">Depth Estimation as Image Reconstruction</h4> <p>핵심 아이디어 : calibrated binocular(stereo) camera로 같은 시간에 찍은 한 쌍의 stereo image가 주어졌을 때, <code class="language-plaintext highlighter-rouge">하나의 image로부터 다른 image를 reconstruct 할 수 있다면 그 장면의 3D 구조를 알 수 있다!</code></p> <p>우선 a stereo image pair에 대해 image rectification을 거친 뒤 만약 <code class="language-plaintext highlighter-rouge">disparity map을 얻었다면 아래의 도식에 의해 depth map으로 변환</code>할 수 있다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/1.JPG-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/1.JPG-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/1.JPG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/1.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>b : baseline distance between two camera centers (상수)<br/> f : camera focal length (상수)<br/> 가로로 뻗은 직선 : rectified image plane<br/> d : predicted disparity<br/> d^ : depth<br/> b : d^ = b - d : d^ - f 이므로 d^ (b - d) = b (d^ - f) 이고, 이를 정리하면 d^d = bf, 즉 <code class="language-plaintext highlighter-rouge">d^ = bf / d</code> 이다.<br/> 만약 disparity \(d = x_{r} - x_{l}\) 과 depth Z = d^를 얻었다면, 아래의 도식으로 X, Y 값도 얻을 수 있어서 3D point 좌표를 알 수 있다.<br/> <code class="language-plaintext highlighter-rouge">(아래의 도식은 뭘 말하는거지?)</code><br/> \(x = \frac{f \cdot X}{Z} + p_x\)</p> <h4 id="depth-estimation-network">Depth Estimation Network</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/2.JPG-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/2.JPG-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/2.JPG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/2.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>위의 figure에서 볼 수 있듯이 Naive 버전은 input left image와 align할 output reconstructed left image가 없다. 한편, No LR 버전은 align할 output reconstructed left image는 존재하지만, <code class="language-plaintext highlighter-rouge">left-right consistency가 보장되지 않기 때문에 'texture-copy' artifacts와 depth discontinuities(boundaries)에서의 errors가 생기는 문제가 있다.</code> 본 논문의 model은 disparity \(d^{r}, d^{l}\) 을 동시에 추론함으로써 이러한 문제들을 모두 해결하였다.<br/> 위의 figure에서 볼 수 있듯이 mono depth estimation이므로 CNN의 <code class="language-plaintext highlighter-rouge">input으로 left image만을 넣어서 disparity dr, dl 을 동시에 추론</code>하였다. 이를 통해 두 disparity 간의 consistency를 어느 정도 강제할 수 있고 결과적으로 더 정확한 depth estimation이 가능해진다. 참고로 <code class="language-plaintext highlighter-rouge">right image는 image reconstruction과 training loss를 구할 때만 사용</code>된다.<br/> disparity를 구한 뒤에는 <code class="language-plaintext highlighter-rouge">bilinear sampler와 backward mapping을 통해 image reconstruction</code>을 수행한다. 이 때, <code class="language-plaintext highlighter-rouge">STN(spatial transformer network)의 bilinear sampler를 이용하기 때문에 위의 일련의 과정은 fully convolutional and fully differentiable</code>하다.</p> <p>backward mapping :<br/> 결과 영상으로 mapping 되는 원본 영상에서의 좌표를 계산하여 해당 밝기값을 가져온다. 이 때, 원본 영상에서의 좌표는 실숫값이므로 bilinear interpolation (output pixel = the weighted sum of four input pixel)을 사용한다. 결과 영상의 각 pixel에 대해 값을 가져오므로 forward mapping에서의 hole 발생은 일어나지 않는다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/3-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/3-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/4-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/4-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>본 논문의 model은 크게 두 부분으로 나뉜다. : encoder (conv1~conv7b) and decoder (upconv7~)<br/> 본 논문의 model은 output으로서 disparity \(d^{r}, d^{l}\)을 동시에 추론하는데, 이를 <code class="language-plaintext highlighter-rouge">four different output scales</code>에 대해 반복한다.</p> <h4 id="train-loss">Train Loss</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/5-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/5-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/6-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/6-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>\(C_s\) : loss at output scale s</p> <blockquote> <p>\(C_{ap}^{l}\) :<br/> appearance matching loss for left image (<code class="language-plaintext highlighter-rouge">image reconstruction term</code>)<br/> How much \(I^{r}(d^{l})\) appears similar to \(I^{l}\)</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/7-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/7-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>이 때, <code class="language-plaintext highlighter-rouge">SSIM (Structural Similarity Index Measure)</code>는 두 images 간의 차이가 작을수록 1 에 가까운 값을 가지며, 정확한 정의는 아래를 참고하자.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/8-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/8-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>\(C_{ds}^{l}\) :<br/> disparity smoothness loss (<code class="language-plaintext highlighter-rouge">smoothness term</code>)<br/> How much \(d^{l}\) is smooth</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/9-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/9-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>real-world에서 depth가 급격하게 변하는 경우 image boundary 혹은 texture change가 있는 부분이므로 image plane에서도 해당 부분의 image gradient가 크게 나타난다. 따라서 image gradient가 큰 부분에서는 disparity (depth) 변화를 허용하지만, image gradient가 작은 부분에서는 disparity (depth)가 부드럽게 변하도록 하는 것이 disparity smoothness loss의 역할이다.</p> <blockquote> <p>\(C_{lr}^{l}\) :<br/> left-right consistency (<code class="language-plaintext highlighter-rouge">left-right disparity consistency term</code>)<br/> How much \(d^{l}\) and \(d^{r}(d^{l})\) are consistent</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/10-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/10-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>image reconstruction 뿐만 아니라 left-right disparity consistency까지 고려함으로써 <code class="language-plaintext highlighter-rouge">depth estimation의 accuracy</code>를 향상시킬 수 있다.</p> <h2 id="results--limitations">Results &amp; Limitations</h2> <h4 id="results">Results</h4> <p>Train : on Cityscapes and KITTI 2015 dataset using two different test splits<br/> Test : on other datasets like Make3D and CamVid</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/11-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/11-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/12-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/12-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><code class="language-plaintext highlighter-rouge">Post-processing</code> :<br/> original left image로부터 구한 disparity map을 \(d^{l}\) 라 하고,<br/> flipped left image로부터 구한 disparity map을 \(d^{l \ast}\) 라 하고,<br/> 이를 다시 flip한 걸 \(d^{l \ast \ast}\) 라 할 때,<br/> \(d^{l}\)의 경우 stereo disocclusions which create disparity ramps(경사) on both the left side of the image and the left of occluders 가 있을 수 있는데,<br/> \(d^{l \ast \ast}\)의 경우 disparity ramps are located on the right side of the image and the right of occluders 이므로<br/> <code class="language-plaintext highlighter-rouge">We combine both disparity maps to form the final disparity map</code> by assigning the first 5% on the left of the image using \(d^{l \ast \ast}\) and the last 5% on the right to the disparities from \(d^{l}\). The central part of the final disparity map is the average of \(d^{l \ast \ast}\) and \(d^{l}\).<br/> 이러한 post-processing을 통해 can reduce the effect of stereo disocclusions, and lead to better accuracy and less visual artifacts,<br/> but double the amount of test time<br/> (<code class="language-plaintext highlighter-rouge">stereo disocclusions의 영향을 줄이기 위한 post-processing 과정 아직 완벽하게 이해하지는 못했음</code>)</p> <h4 id="limitations">Limitations</h4> <ol> <li> <p>left-right consistency와 post-processing으로 quality 향상을 이룬 건 맞지만, <code class="language-plaintext highlighter-rouge">두 images에서 모두 안 보이는 occlusion region에서의 pixels 때문에 occlusion boundaries에서는 여전히 artifacts가 존재</code>한다. training phase에서 occclusion에 대해 explicitly reasoning하는 것으로 이 문제를 개선할 수는 있지만, supervised methods 또한 모든 pixels에 대해 항상 valid depth를 가지는 것은 아님에 주목할 필요가 있다.</p> </li> <li> <p>training phase에서 <code class="language-plaintext highlighter-rouge">rectified and temporally aligned (image rectification을 거치고 동시에 찍은) stereo image pairs가 필요</code>하다. 이 말은 즉슨, single-view dataset은 training에 쓸 수 없다. (fine-tune하는 것만 가능하다.)</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">image reconstruction term에 의존</code>한다. 이 말은 즉슨, <code class="language-plaintext highlighter-rouge">specular and transparent (거울 같이 반사하는 and 투명한) surfaces에서는 inconsistent depth</code>가 생긴다. 이는 더 정교한 similarity measures를 사용함으로써 개선될 수 있다.</p> </li> </ol> <h2 id="conclusion">Conclusion</h2> <ul> <li>unsupervised mono (single image as input) depth estimation \(\rightarrow\) no need for expensive GT depth</li> <li>need for binocular stereo image pairs at training</li> <li>novel loss function : left-right consistency b.w. disparity maps \(\rightarrow\) improve quality of depth map</li> <li>can generalize to unseen datasets</li> </ul> <h2 id="future-work">Future Work</h2> <ul> <li>extend to videos (can add temporal consistency)</li> <li>investigate sparse input as an alternative training signal<br/> (<code class="language-plaintext highlighter-rouge">?? 이해 못함</code>)</li> <li>our model estimates per-pixel depth, but it would be also interesting to predict the full occupancy of the scene<br/> (<code class="language-plaintext highlighter-rouge">?? 이해 못함</code>)</li> </ul> <p>중간중간에 있는 질문들은 아직 이해하지 못해서 남겨놓은 코멘트입니다.<br/> 추후에 다시 읽어보고 이해했다면 업데이트할 예정입니다.<br/> 혹시 알고 계신 분이 있으면 댓글로 남겨주시면 감사하겠습니다!</p>]]></content><author><name></name></author><category term="depth-estimation"/><category term="unsupervised"/><category term="monocular"/><category term="depth"/><category term="consistency"/><summary type="html"><![CDATA[Unsupervised Monocular Depth Estimation with Left-Right Consistency]]></summary></entry></feed>