<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://semyeong-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://semyeong-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-11T00:49:48+00:00</updated><id>https://semyeong-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">4D Gaussian Splatting</title><link href="https://semyeong-yu.github.io/blog/2024/4DGS/" rel="alternate" type="text/html" title="4D Gaussian Splatting"/><published>2024-09-10T12:00:00+00:00</published><updated>2024-09-10T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/4DGS</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/4DGS/"><![CDATA[<h2 id="4d-gaussian-splatting-for-real-time-dynamic-scene-rendering">4D Gaussian Splatting for Real-Time Dynamic Scene Rendering</h2> <h4 id="guanjun-wu-taoran-yi-jiemin-fang-lingxi-xie-xiaopeng-zhang-wei-wei-wenyu-liu-qi-tian-xinggang-wang">Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, Xinggang Wang</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2310.08528">https://arxiv.org/abs/2310.08528</a><br/> project website :<br/> <a href="https://guanjunwu.github.io/4dgs/index.html">https://guanjunwu.github.io/4dgs/index.html</a><br/> code :<br/> <a href="https://github.com/hustvl/4DGaussians">https://github.com/hustvl/4DGaussians</a></p> </blockquote> <h2 id="abstract">Abstract</h2> <ul> <li> <p>complex motion을 정확하게 모델링하면서 high efficiency로 real-time dynamic scene을 rendering하는 건 매우 challenging task</p> </li> <li>3DGS를 각 frame에 적용하는 게 아니라 4DGS라는 새로운 모델 제시 <ul> <li>3DGS</li> <li>4D neural voxel : <ul> <li>HexPlane에 영감을 받아 decomposed neural voxel encoding algorithm을 이용해서<br/> 4D neural voxel로부터 Gaussian features를 얻음</li> <li>가벼운 MLP를 이용해서<br/> Gaussian deformation을 예측함</li> </ul> </li> </ul> </li> <li>4DGS :<br/> real-time (82 FPS) rendering at high (800 \(\times\) 800) resolution on RTX 3090 GPU</li> </ul> <h2 id="introduction">Introduction</h2> <h2 id="related-works">Related Works</h2> <h3 id="dynamic-nerf-with-deformation-fields">Dynamic NeRF with Deformation Fields</h3> <h2 id="method">Method</h2> <h3 id="overview-gaussian-deformation-field-network">Overview (Gaussian Deformation Field Network)</h3> <h3 id="spatial-temporal-structure-encoder">Spatial-Temporal Structure Encoder</h3> <h3 id="multi-head-gaussian-deformation-decoder">Multi-head Gaussian Deformation Decoder</h3> <h3 id="optimization">Optimization</h3> <h2 id="experiment">Experiment</h2> <h3 id="dataset">Dataset</h3> <h3 id="results">Results</h3> <h3 id="ablation-study">Ablation Study</h3> <h2 id="discussion">Discussion</h2> <h3 id="discussion-1">Discussion</h3> <h3 id="limitation">Limitation</h3> <h3 id="conclusion">Conclusion</h3> <h2 id="code-flow">Code Flow</h2> <h2 id="question">Question</h2>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="GS"/><category term="4d"/><category term="dynamic"/><category term="rendering"/><summary type="html"><![CDATA[4D Gaussian Splatting for Real-Time Dynamic Scene Rendering (CVPR 2024)]]></summary></entry><entry><title type="html">EE534 Pattern Recognition</title><link href="https://semyeong-yu.github.io/blog/2024/Pattern/" rel="alternate" type="text/html" title="EE534 Pattern Recognition"/><published>2024-09-10T11:00:00+00:00</published><updated>2024-09-10T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Pattern</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Pattern/"><![CDATA[<blockquote> <p>Lecture :<br/> 24F EE534 Pattern Recognition<br/> by KAIST Munchurl Kim <a href="https://www.viclab.kaist.ac.kr/">VICLab</a></p> </blockquote> <h2 id="chapter-1-overview">Chapter 1. Overview</h2> <h3 id="discriminative-vs-generative">Discriminative vs Generative</h3> <ul> <li>Discriminative model : <ul> <li> <table> <tbody> <tr> <td>learn $$P(Y</td> <td>X)\(s.t. maximize\)P(Y</td> <td>X)$$ directly</td> </tr> </tbody> </table> </li> <li>e.g. logistic regression, SVM, nearest neighbor, CRF, Decision Tree and Random Forest, traditional NN</li> </ul> </li> <li>Generative model : <ul> <li> <table> <tbody> <tr> <td>learn $$P(X</td> <td>Y)\(and\)P(Y)\(s.t. maximize\)P(X, Y) = P(X</td> <td>Y)P(Y)$$</td> </tr> <tr> <td>where can learn $$P(Y</td> <td>X) \propto P(X</td> <td>Y)P(Y)$$ indirectly</td> </tr> </tbody> </table> </li> <li>e.g. Bayesian network, Autoregressive model, GAN, Diffuson model</li> </ul> </li> </ul> <h2 id="chapter-2-bayes-decision-theory">Chapter 2. Bayes Decision Theory</h2> <h3 id="bayes-decision-rule">Bayes Decision Rule</h3> <ul> <li>conditional probability density :<br/> Let \(w\) be state (class)<br/> Let \(x\) be data (continous-valued sample) <ul> <li>prior : \(P(w=w_k)\)</li> <li> <table> <tbody> <tr> <td>likelihood : pdf $$P(x</td> <td>w_k)$$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>posterior : $$P(w_k</td> <td>x) = \frac{P(x</td> <td>w_k)P(w_k)}{P(x)}$$ (Bayes Rule)</td> <td> </td> </tr> <tr> <td>where $$P(w_1</td> <td>x) + P(w_2</td> <td>x) + \cdots + P(w_N</td> <td>x) = 1$$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>evidence : $$P(x) = \sum_{k=1}^N P(x</td> <td>w_k)P(w_k) = \sum_{k=1}^N P(x, w_k)$$</td> </tr> </tbody> </table> </li> </ul> </li> <li>Bayes Decision Rule :<br/> posterior 더 큰 쪽 고름! <ul> <li>Two-class (\(w_1, w_2\)) problem :<br/> choose \(w_1\)<br/> if \(P(w_1 | x) \gt P(w_2 | x)\)<br/> if \(P(x|w_1)P(w_1) \gt P(x|w_2)P(w_2)\)<br/> if \(\frac{P(x|w_1)}{P(x|w_2)} \gt \frac{P(w_2)}{P(w_1)}\)<br/> (likehood ratio \(\gt\) threshold)</li> <li>multi-class problem :<br/> choose \(w_i\) where \(P(w_i | x)\) is the largest</li> </ul> </li> <li>minimum error :<br/> GT가 \(w_1, w_2\) 이고, Predicted가 \(R_1, R_2\) 일 때, <ul> <li>\(P(error) = \int_{-\infty}^{\infty} P(error, x)dx = \int_{-\infty}^{\infty} P(error|x)P(x)dx\)<br/> \(= \int_{R_2}P(w_1|x)P(x)dx + \int_{R_1}P(w_2|x)P(x)dx\)<br/> \(= \int_{R_2}P(x|w_1)P(w_1)dx + \int_{R_1}P(x|w_2)P(w_2)dx\)<br/> \(= \begin{cases} A+B+D &amp; \text{if} &amp; x_B \\ A+B+C+D &amp; \text{if} &amp; x^{\ast} \end{cases}\)<br/> where \(A+B+D\) is minimum error and \(C\) is reducible error<br/> (아래 그림 참고)</li> <li>\(P(correct)\)<br/> \(= \int \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_1}P(x|w_1)P(w_1)dx + \int_{R_2}P(x|w_2)P(w_2)dx\)</li> <li>\(P(error) = 1 - P(correct)\)<br/> \(= 1 - \int \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_2}P(x|w_1)P(w_1)dx + \int_{R_1}P(x|w_2)P(w_2)dx\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/1-480.webp 480w,/assets/img/2024-09-10-Pattern/1-800.webp 800w,/assets/img/2024-09-10-Pattern/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>minimum error with rejection :<br/> decision이 확실하지 않을 때는 classification 자체를 reject하는 게 적절<br/> (classification error도 줄어들고, correct classification도 줄어듬) <ul> <li>feature space \(x\) 를 rejection region \(R\) 과 acceptance region \(A\) 으로 나눠서<br/> rejection region \(R=\{ x | \text{max}_{i} P(w_i | x) \leq 1 - t\}\) 에서는 reject decision<br/> acceptance region \(A=\{ x | \text{max}_{i} P(w_i | x) \gt 1 - t\}\) 에서는 \(w_1\) or \(w_2\) 로 classification decision 수행</li> <li>\(P_c(t) = P(correct)\)<br/> \(= \int_{A} \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_1}P(x|w_1)P(w_1)dx + \int_{R_2}P(x|w_2)P(w_2)dx\)</li> <li>\(P_r(t) = P(reject)\)<br/> \(= \int_{R}P(x|w_1)P(w_1)dx + \int_{R}P(x|w_2)P(w_2)dx\)<br/> \(= \int_{R} P(x)dx\)</li> <li>\(P_e(t) = P(error)\)<br/> \(= P(error, w_1) + P(error, w_2)\)<br/> \(= 1 - P_r(t) - P_c(t)\) by 아래 식 대입<br/> where \(P(error, w_1) = \int_{-\infty}^{\infty} P(x|w_1)P(w_1)dx - P(reject, w_1) - P(correct, w_1)\)<br/> where \(P(error, w_2) = \int_{-\infty}^{\infty} P(x|w_2)P(w_2)dx - P(reject, w_2) - P(correct, w_2)\)</li> </ul> </li> <li>Summary : <ul> <li> <table> <tbody> <tr> <td>$$P(w_i</td> <td>x)$$ : rejection/acceptance region 구하는 데 사용</td> </tr> </tbody> </table> </li> <li>\(P(x|w_i)P(w_i)\) : \(P(correct, w_i), P(reject, w_i), P(error, w_i)\) 구해서<br/> \(P_c(t), P_r(t), P_e(t)\) 구하는 데 사용</li> <li> \[P_c(t) + P_r(t) + P_e(t) = 1\] </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/2-480.webp 480w,/assets/img/2024-09-10-Pattern/2-800.webp 800w,/assets/img/2024-09-10-Pattern/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/3-480.webp 480w,/assets/img/2024-09-10-Pattern/3-800.webp 800w,/assets/img/2024-09-10-Pattern/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Bayes risk (minimum overall risk) :<br/> \(\Omega = \{ w_1, \cdots w_c \}\) 에서 \(w_j\) 는 \(j\) -th class<br/> \(A = \{ \alpha_{1}, \cdots, \alpha_{c} \}\) 에서 \(\alpha_{i}\) 는 class \(w_i\) 라고 예측하는 action<br/> \(\lambda(\alpha_{i} | w_j) = \lambda_{ij}\) : class \(w_j\) 가 GT일 때, class \(w_i\) 로 pred. 했을 때의 loss <ul> <li>conditional risk for taking action \(\alpha_{i}\) :<br/> 특정 input \(x\) 에 대해<br/> \(R(\alpha_{i}|x) = \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j|x)\)</li> <li>overall risk for taking action \(\alpha_{i}\) :<br/> 모든 input \(x\) 에 대해 적분<br/> \(R(\alpha_{i}) = \int R(\alpha_{i}|x)P(x)dx\)<br/> \(= \int \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j|x) P(x)dx\)<br/> \(= \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j) \int P(x|w_j)dx\)<br/> \(= \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j)\)<br/> \(= \sum_{j=1}^c \lambda_{ij}P(w_j)\)<br/> where pdf(likelihood) 합 \(\int P(x|w_j)dx = 1\)</li> <li>모든 input \(x\) 에 대해 가장 loss가 최소인 class \(w_i\) 로 예측하면,<br/> minimum overall risk (= Bayes risk) 를 가짐</li> </ul> </li> <li>Bayes Decision Rule for Bayes risk : <ul> <li>Two-class (\(w_1, w_2\)) problem :<br/> choose \(w_1\)<br/> if \(R(\alpha_{1} | x) \lt R(\alpha_{2} | x)\)<br/> if \(\lambda_{11}P(w_1 | x) + \lambda_{12}P(w_2 | x) \lt \lambda_{21}P(w_1 | x) + \lambda_{22}P(w_2 | x)\)<br/> if \((\lambda_{21} - \lambda_{11})P(w_1 | x) \gt (\lambda_{12} - \lambda_{22})P(w_2 | x)\)<br/> if \(\frac{P(x | w_1)}{P(x | w_2)} \gt \frac{(\lambda_{12} - \lambda_{22})P(w_2)}{(\lambda_{21} - \lambda_{11})P(w_1)}\)<br/> if \(\frac{P(x | w_1)}{P(x | w_2)} \gt \frac{P(w_2)}{P(w_1)}\) for \(\lambda_{11}=\lambda_{22}=0\) and \(\lambda_{12}=\lambda_{21}\)<br/> (likehood ratio \(\gt\) threshold) (위에서 구한 식과 same)</li> <li> <table> <tbody> <tr> <td>loss $$\lambda(\alpha_{i}</td> <td>w_j) = \begin{cases} 0 &amp; \text{if} &amp; i=j &amp; (\text{no penalty}) \ 1 &amp; \text{if} &amp; i \neq j &amp; (\text{equal penalty}) \end{cases}$$ 일 때</td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td>conditional risk $$R(\alpha_{i}</td> <td>x) = \sum_{j=1}^c \lambda(\alpha_{i}</td> <td>w_j)P(w_j</td> <td>x) = \sum_{j=1, j \neq i}^c P(w_j</td> <td>x) = 1 - P(w_i</td> <td>x)$$ 이므로</td> </tr> <tr> <td>Bayes Decision Rule에서 conditional risk $$R(\alpha_{i}</td> <td>x)\(최소화는 posterior\)P(w_i</td> <td>x)$$ 최대화와 같음</td> <td> </td> <td> </td> <td> </td> </tr> </tbody> </table> </li> <li>multi-class problem :<br/> choose \(w_i\) where classifier (discriminant func.) \(g_{i}(x)\) is the largest<br/> e.g. Bayes classifier : \(g_{i}(x) = - R(\alpha_{i} | x)\) or \(g_{i}(x) = P(w_i | x)\) or \(g_{i}(x) = P(x | w_i)P(w_i)\) or \(g_{i}(x) = \text{log}P(x | w_i) + \text{log}P(w_i)\)</li> </ul> </li> <li>TBD</li> </ul>]]></content><author><name></name></author><category term="cv-tasks"/><category term="3d"/><category term="rendering"/><summary type="html"><![CDATA[Lecture Summary (24F)]]></summary></entry><entry><title type="html">Nabla (Del) operator</title><link href="https://semyeong-yu.github.io/blog/2024/nabla/" rel="alternate" type="text/html" title="Nabla (Del) operator"/><published>2024-09-06T11:00:00+00:00</published><updated>2024-09-06T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/nabla</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/nabla/"><![CDATA[<p>본 포스팅 출처 : <a href="https://xoft.tistory.com/71">Link</a></p> <h3 id="del">Del</h3> <ul> <li>\(\nabla = \frac{\partial}{\partial x}i + \frac{\partial}{\partial y}j\)<br/> where \(\nabla\) : vector</li> <li>Del operator의 피연산자가 scalar인지 vector인지에 따라 다르게 불림</li> </ul> <h3 id="gradient">Gradient</h3> <ul> <li>\(\nabla f = \frac{\partial f}{\partial x}i + \frac{\partial f}{\partial y}j\)<br/> where \(f\) : scalar field<br/> where \(\nabla f\) : vector field</li> <li>scalar 함수 각 점에서의 방향</li> </ul> <h3 id="divergence">Divergence</h3> <ul> <li>\(\nabla f = \nabla \cdot f = (\frac{\partial}{\partial x}i + \frac{\partial}{\partial y}j) \cdot (v_x i + v_y j) = \frac{\partial v_x}{\partial x} + \frac{\partial v_y}{\partial y}\)<br/> where \(f = v_x i + v_y j\) : vector field<br/> where \(\nabla f\) : scalar field</li> <li>vector 함수 각 점에서의 발산하는 크기</li> </ul> <h3 id="curl">Curl</h3> <ul> <li>\(\nabla \times f = \frac{\partial v_x}{\partial x}i + \frac{\partial v_y}{\partial y}j\)<br/> where \(f = v_x i + v_y j\) : vector field<br/> where \(\nabla f\) : scalar field</li> <li>점의 rotation</li> </ul> <h3 id="laplacian">Laplacian</h3> <ul> <li>\(\Delta = \nabla \cdot \nabla = \text{Divergence} \cdot \text{Gradient} = \frac{\partial^{2}}{\partial x} + \frac{\partial^{2}}{\partial y}\)<br/> where \(\Delta\) : scalar (Divergence of Gradient)</li> <li>image에 Laplacian filter를 쓰면<br/> Gradient로 색상이 급격히 변하는 vector를 검출한 뒤<br/> Divergence로 vector의 발산 크기 균일 정도를 파악하여<br/> Edge를 검출할 수 있음</li> </ul>]]></content><author><name></name></author><category term="math"/><category term="nabla"/><category term="del"/><category term="scalar"/><category term="vector"/><summary type="html"><![CDATA[del, gradient, divergence, curl, laplacian]]></summary></entry><entry><title type="html">SuGaR</title><link href="https://semyeong-yu.github.io/blog/2024/SuGaR/" rel="alternate" type="text/html" title="SuGaR"/><published>2024-09-05T11:00:00+00:00</published><updated>2024-09-05T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/SuGaR</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/SuGaR/"><![CDATA[<h2 id="sugar-surface-aligned-gaussian-splatting-for-efficient-3d-mesh-reconstruction-and-high-quality-mesh-rendering">SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering</h2> <h4 id="antoine-guédon-vincent-lepetit">Antoine Guédon, Vincent Lepetit</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2311.12775">https://arxiv.org/abs/2311.12775</a><br/> project website :<br/> <a href="https://anttwo.github.io/sugar/">https://anttwo.github.io/sugar/</a><br/> code :<br/> <a href="https://github.com/Anttwo/SuGaR">https://github.com/Anttwo/SuGaR</a><br/> reference :<br/> NeRF and 3DGS Study</p> </blockquote> <h2 id="abstract">Abstract</h2> <ul> <li> <p>surface 점 sampling :<br/> surface 근처의 점 \(p\) 를<br/> <code class="language-plaintext highlighter-rouge">Gaussians의 곱 분포</code>로 sampling</p> </li> <li> <p>regularization term :<br/> 3DGS가 surface 잘 나타내도록 (well-distributed) 하기 위해<br/> <code class="language-plaintext highlighter-rouge">density</code> function 또는 <code class="language-plaintext highlighter-rouge">SDF</code>로 <code class="language-plaintext highlighter-rouge">regularization</code> loss term</p> </li> <li> <p>obtain mesh using level set points :<br/> 점 \(p\) 주위(\(3 \sigma (v)\))의 points를 sampling하고<br/> density 계산하여 oriented <code class="language-plaintext highlighter-rouge">level set points</code> 구한 뒤<br/> Poisson equation으로 <code class="language-plaintext highlighter-rouge">mesh</code> 구함</p> </li> <li> <p>mesh refinement :<br/> triangle mesh에 new Gaussians binding하여<br/> mesh optimize할 때 new Gaussians도 함께 optimize</p> </li> </ul> <h2 id="surface-aligned-3dgs">Surface-Aligned 3DGS</h2> <h3 id="regularization">Regularization</h3> <ul> <li> <p>문제 :<br/> 3DGS는 <code class="language-plaintext highlighter-rouge">unstructured</code><br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">surface</code> 나타내지 않음</p> </li> <li> <p>해결 :<br/> <code class="language-plaintext highlighter-rouge">regularization</code> loss term<br/> \(\rightarrow\) 3DGS가 well-distributed and aligned with surface (flat)</p> <ul> <li>well-distributed : <ul> <li>Gaussians끼리 <code class="language-plaintext highlighter-rouge">overlap 적음</code></li> <li>(surface에 가까운) point \(p\) 와 <code class="language-plaintext highlighter-rouge">가장 가까운 Gaussian</code> \(g^{\ast}\) 가 다른 Gaussians보다 \(p\) 의 <code class="language-plaintext highlighter-rouge">density에 훨씬 많이 기여</code><br/> \(g^{\ast} = \text{argmin}_{g}(p - \mu_{g})^T \Sigma_{g}^{-1}(p - \mu_{g})\)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/1-480.webp 480w,/assets/img/2024-09-05-SuGaR/1-800.webp 800w,/assets/img/2024-09-05-SuGaR/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>surface 근처의 점 sampling : <ul> <li>assumption :<br/> 거의 surface 위에 있다고 볼 수 있을 정도로 아주 가까운<br/> <code class="language-plaintext highlighter-rouge">surface 근처</code>의 \(p\) 를 <code class="language-plaintext highlighter-rouge">Gaussian들의 곱 분포로 sampling</code><br/> \(p \sim \prod_{g} N(\cdot; \mu_{g}, \Sigma{g})\) <ul> <li>‘3DGS가 잘 학습됐다면’ small Gaussians는 surface에 아주 가까운 점들의 확률처럼 생각할 수 있고,<br/> Gaussian이 작을수록 sampling이 중심에 집중되므로<br/> 그 small Gaussians의 곱이 나타내는 분포는 surface 근처의 좁은 영역에 집중된 분포를 나타낼 것이고,<br/> 이로부터 sampling한 점 \(p\) 는 실제 object surface에 가까울 확률이 높다는 가정</li> <li>이렇게 sampling한 points는 regularization term에 대해 high gradient를 가지는 부분임</li> </ul> </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">density function</code> : <ul> <li>\(d(p) = \sum_{g} \alpha_{g} \text{exp}(-\frac{1}{2}(p - \mu_{g})^T \Sigma_{g}^{-1}(p - \mu_{g}))\)<br/> where \(\text{exp}(-\frac{1}{2}(p - \mu_{g})^T \Sigma_{g}^{-1}(p - \mu_{g}))\) : posterior<br/> (점 \(p\) 에 더 가까운 Gaussian의 \(\alpha_{g}\) 가 \(p\) 의 density에 더 많이 기여)<br/> where \((p - \mu_{g})^T \Sigma_{g}^{-1}(p - \mu_{g})\) : <code class="language-plaintext highlighter-rouge">Mahalanobis distance</code><br/> (\(p\) 가 Gaussian distribution 평균 \(\mu_{g}\) 에서 “상대적으로” 얼마나 떨어져 있는지)<br/> (\(p\) 가 평균으로부터 같은 거리만큼 떨어져있더라도 convariance가 작은 방향에 있을수록 Mahalanobis distance가 커짐)</li> <li>approx. ideal density function \(\bar d(p) \in [0, 1]\) : <ul> <li>가정 1) well-distributed Gaussians by regularization term 이므로<br/> overlap 없다는 전제 하에 <code class="language-plaintext highlighter-rouge">하나</code>의 Gaussian \(g^{\ast}\) 가 point \(p\) 의 density 결정</li> <li>가정 2) Gaussians가 진짜 surface를 묘사하려면 <code class="language-plaintext highlighter-rouge">semi-transparent하지 않아야</code> 좋음<br/> \(\rightarrow\) \(a_{g} = 1\) for any Gaussians</li> <li>위의 가정과 아래 수식 유도(<strong>Approximation of Density function</strong>) 에 따르면<br/> \((p - \mu_{g})^T \Sigma_{g}^{-1}(p - \mu_{g}) \approx \frac{1}{s_{g}^2} \langle p-\mu_{g}, n_g \rangle^{2}\) 이고<br/> 근사해서 구한 ideal density function은<br/> \(\bar d(p) = \text{exp}(-\frac{1}{2s_{g^{\ast}}^2} \langle p-\mu_{g^{\ast}}, n_{g^{\ast}} \rangle^{2})\)<br/> where \(g^{\ast} = \text{argmin}_{g}(p - \mu_{g})^T \Sigma_{g}^{-1}(p - \mu_{g})\)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/2-480.webp 480w,/assets/img/2024-09-05-SuGaR/2-800.webp 800w,/assets/img/2024-09-05-SuGaR/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/3-480.webp 480w,/assets/img/2024-09-05-SuGaR/3-800.webp 800w,/assets/img/2024-09-05-SuGaR/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Regularization on density</code> : <ul> <li> \[R = | d(p) - \bar d(p) |\] <ul> <li>\(d\) : <code class="language-plaintext highlighter-rouge">density</code> function</li> <li>\(\bar d\) : approx. <code class="language-plaintext highlighter-rouge">ideal density</code> function<br/> where 하나의 불투명한 Gaussian이 point density 결정</li> </ul> </li> <li>근데 density function \(d\) 로 regularize하면 아래의 문제가 있다 <ul> <li>\(d\) 는 exponential term으로 이루어져 있으므로 scale이 너무 커서 optimization에 별로다</li> <li>approx. ideal density function을 구할 때 flat Gaussian으로 surface를 나타내는 게 목적이라고 가정하였는데,<br/> Gaussian이 완전히 flat 하면 \(s_{g} = 0\) 이 되어 \(\bar d(p) \rightarrow 0\) 이므로<br/> 모든 level set (표면)이 \(\mu_{g}\) 를 지나고 normal \(n_{g}\) 를 가지는 2D 상의 plane이 되어<br/> level sets 고려하는 게 무의미해진다<br/> 따라서 surface를 나타내기 위해 flat하게 Gaussian을 만드는 게 목적이지만<br/> 그렇다고 완전히 flat하면 안 됨</li> </ul> </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Regularization on SDF</code> : <ul> <li>density function 말고 <code class="language-plaintext highlighter-rouge">SDF</code> <a href="https://semyeong-yu.github.io/blog/2024/SDF/">Link</a> 로 loss 만들면 optimization 더 잘 됨<br/> (Gaussians가 surface에 더 잘 align됨)<br/> \(R = \frac{1}{| P |} \sum_{p \in P} | \hat f(p) - f(p) |\) <ul> <li>\(f(p) = \langle p-\mu_{g^{\ast}}, n_{g^{\ast}} \rangle = \pm s_{g^{\ast}} \sqrt{-2log(\bar d(p))}\) :<br/> <code class="language-plaintext highlighter-rouge">ideal distance</code> (SDF) b.w. point \(p\) and true surface<br/> (\(\bar d(p) = 1\) 이면, 즉 SDF \(f(p) = 0\) (zero level-set)이면, true surface를 나타냄)</li> <li>\(\hat f(p)\) :<br/> <code class="language-plaintext highlighter-rouge">estimated distance</code> b.w. point \(p\) and depth at projection of \(p\)<br/> (\(f(p)\) 를 직접 계산하는 건 빡세므로 training view-points에 대해 Radix Sort로 Gaussian rasterize할 때 사용한 Gaussian depth 값들을 rendering하여 depth map을 만들어서 estimated \(\hat f(p)\) 구함)</li> </ul> </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Regularization on normal</code> vector : <ul> <li>normal vector의 방향 \(n_{g}\) 을 SDF gradient 방향으로 맞춰주기 위해<br/> (normal vector 방향을 잘 잡아줘야 surface에 잘 align됨)<br/> \(R_{Norm} = \frac{1}{| P |} \sum_{p \in P} \| \frac{\nabla f(p)}{\| \nabla f(p) \|} - n_{g^{\ast}} \|^2\)</li> </ul> </li> </ul> <h3 id="approximation-of-density-function">Approximation of Density function</h3> <ul> <li> <p>density function이 실제 surface를 잘 나타낸다면<br/> \(p\) 에 가장 기여가 큰 Gaussian이 surface에 align되어 flat해야 한다<br/> 이 때, <code class="language-plaintext highlighter-rouge">flat Gaussian</code>의 경우 Mahalanobis distance의 주 요인은 <code class="language-plaintext highlighter-rouge">covariance의 가장 짧은 축</code> \(s_{g}\) 이므로<br/> 아래와 같이 approx. ideal density function 식을 유도할 수 있다</p> </li> <li> <p>\(\bar d(p) = \text{exp}(-\frac{1}{2s_{g^{\ast}}^2} \langle p-\mu_{g^{\ast}}, n_{g^{\ast}} \rangle^{2})\) 유도 TBD <code class="language-plaintext highlighter-rouge">????</code><br/> Eigendecomposition을 하면 \(\Sigma_{g} = Q \Lambda Q^T\)<br/> where \(s_g\) : convariance가 가장 작은 방향의 vector<br/> where \(n_g = \frac{s_g}{\| s_g \|}\)</p> </li> </ul> <h2 id="mesh-reconstruction">Mesh reconstruction</h2> <h3 id="obtain-mesh">Obtain Mesh</h3> <ul> <li> <p>문제 :<br/> <code class="language-plaintext highlighter-rouge">Densification</code>을 거치면 3DGS 수가 너무 <code class="language-plaintext highlighter-rouge">많아</code>지고 너무 <code class="language-plaintext highlighter-rouge">작아</code>져서<br/> texture나 detail을 나타내기 힘듦<br/> \(\rightarrow\) 거의 모든 곳에서 density function \(d = 0\) 이고,<br/> 위에서 언급했듯이 level sets 고려하는 게 의미가 없어져서<br/> Marching Cubes 기법으로 이러한 <code class="language-plaintext highlighter-rouge">sparse density function</code>의 <code class="language-plaintext highlighter-rouge">level sets</code>를 추출하기 어렵</p> </li> <li> <p>해결 :</p> <ul> <li>과정 1)<br/> Gaussians로 계산한 density function level set 상의 <code class="language-plaintext highlighter-rouge">visible</code> part에 대해 3D <code class="language-plaintext highlighter-rouge">point sampling</code><br/> \(n\) 개의 3D points \(\{ p + t_i v_i \}_{i=1}^n\) sampling<br/> where \(p\) : depth map에 따른 3D point<br/> where \(t_i \in [-3 \sigma_{g}(v), 3\sigma_{g}(v)]\) (visible part)<br/> where \(v_i\) : ray direction</li> <li>과정 2)<br/> \(d_i = d(p + t_i v_i) = \sum_{g} \alpha_{g} \text{exp}(-\frac{1}{2}((p + t_i v_i) - \mu_{g})^T \Sigma_{g}^{-1}((p + t_i v_i) - \mu_{g}))\) 로<br/> <code class="language-plaintext highlighter-rouge">density 계산</code>한 뒤 level parameter \(\lambda\) 에 대해<br/> \(d_i \lt \lambda \lt d_j\) 이면,<br/> range \([d_i, d_j]\) 안에 <code class="language-plaintext highlighter-rouge">level set point</code> 있다고 판단<br/> (아! 그 범위 안에 표면 위의 점이 있구나!)</li> <li>과정 3)<br/> 해당 level set points와 normals (oriented 3d point clouds \(\vec V\))를 이용하여<br/> <code class="language-plaintext highlighter-rouge">Poisson reconstruction</code>으로 surface <code class="language-plaintext highlighter-rouge">mesh</code> 얻음<br/> (아래에서 설명)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/4-480.webp 480w,/assets/img/2024-09-05-SuGaR/4-800.webp 800w,/assets/img/2024-09-05-SuGaR/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="mesh-by-poisson-reconstruction">Mesh by Poisson Reconstruction</h3> <ul> <li> <p>Poisson surface reconstruction :<br/> <code class="language-plaintext highlighter-rouge">3D Point Clouds</code>를 <code class="language-plaintext highlighter-rouge">3D Mesh</code>로 변환하는 고전적인 방법 (출처 : <a href="https://xoft.tistory.com/72">Link</a>)</p> </li> <li> <p>Let indicator function \(\chi_{M}(p) = \begin{cases} 1 &amp; \text{if} &amp; p \in M \\ 0 &amp; \text{if} &amp; p \notin M \end{cases}\)<br/> where \(M\) : object mesh 내부</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/5-480.webp 480w,/assets/img/2024-09-05-SuGaR/5-800.webp 800w,/assets/img/2024-09-05-SuGaR/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>주어진 <code class="language-plaintext highlighter-rouge">oriented 3d point clouds</code> \(\vec V\) 를 approx.하는 <code class="language-plaintext highlighter-rouge">indicator gradient</code> \(\nabla \chi\) 를 찾아야 한다<br/> 이를 풀기 위해 Possion Equation을 사용하자 <ul> <li><code class="language-plaintext highlighter-rouge">Possion Equation</code> :<br/> \(\nabla^{2} \phi = f\)<br/> where \(\nabla = (\frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z})\)<br/> \(\rightarrow\)<br/> \((\frac{\partial^{2}}{\partial x^2}+\frac{\partial^{2}}{\partial y^2}+\frac{\partial^{2}}{\partial z^2}) \phi (x, y, z) = f(x, y, z)\)<br/> 여기서 scalar field \(f\) 가 주어지면,<br/> scalar field \(\phi\) 를 찾을 수 있다</li> <li>\(\nabla \chi \approx \vec V\) 원하는 상황인데<br/> 양변에 divergence를 취하면<br/> \(\nabla \cdot \nabla \chi = \nabla \cdot \vec V\) 은 Poisson Equation 꼴이므로<br/> \(\nabla \cdot \vec V\) 를 알면 \(\chi\) 를 알 수 있다</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/6-480.webp 480w,/assets/img/2024-09-05-SuGaR/6-800.webp 800w,/assets/img/2024-09-05-SuGaR/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Implementation : <ul> <li>oriented 3d point clouds가 주어지면<br/> 모든 points를 포함하는 큰 육면체를 만들고<br/> 이를 <code class="language-plaintext highlighter-rouge">Octree</code> (육면체를 8등분하는 tree)를 사용하여 분할 (Fig 1.)</li> <li>input point clouds \(\vec V\) 는 주변 octree들의 합으로 설계하고,<br/> octree node의 depth는 Gaussian의 variance로 설계하여<br/> input point clouds 근처의 octreee들을 Gaussian으로 표현하면<br/> vector field \(\vec V\) (Fig 2.) 를 얻을 수 있다</li> <li>각 차원을 편미분 (Divergence)하면 scalar field \(\nabla \cdot \vec V\) (Fig 3.)를 얻을 수 있고,<br/> Poisson equation \(\nabla \cdot \nabla \chi = \nabla \cdot \vec V\) 에 의해<br/> indicator function \(\chi\) 도 알 수 있다<br/> octree의 깊이 별로 각 node의 \(\nabla \vec V\) 값 (Fig 3.)과 \(\nabla \nabla \chi\) 값 (Fig 4.)의 차이를 최소화함으로써 indicator function \(\chi\) 를 구한다</li> <li>mesh화 : input point clouds를 indicator function \(\chi\) 의 입력으로 넣어서 나온 결과값들을 평균 내고, 이 값과 같은 값을 출력하는 좌표들을 surface로 간주 (Fig 5.)하여 Marching Cube 알고리즘으로 mesh 생성<br/> (Octree Node마다 Marching Cube Polygon 생성)<br/> (여러 fine Octree Node가 하나의 coarse Octree Node를 공유할 때 생기는 문제를 해결하기 위해 fine Octree Node 면의 부분을 coarse한 면으로 projection하는 방법 사용)</li> <li>octree 깊이가 깊어질수록 시간과 memory를 많이 잡아먹긴 하지만, recon.하는 mesh 수가 더 많아서 mesh fine detail을 살릴 수 있음 (Fig 6.)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/7-480.webp 480w,/assets/img/2024-09-05-SuGaR/7-800.webp 800w,/assets/img/2024-09-05-SuGaR/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 1. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/8-480.webp 480w,/assets/img/2024-09-05-SuGaR/8-800.webp 800w,/assets/img/2024-09-05-SuGaR/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 2. vector field (oriented point clouds) </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/10-480.webp 480w,/assets/img/2024-09-05-SuGaR/10-800.webp 800w,/assets/img/2024-09-05-SuGaR/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 3. scalar field (divergence of oriented point clouds) </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/9-480.webp 480w,/assets/img/2024-09-05-SuGaR/9-800.webp 800w,/assets/img/2024-09-05-SuGaR/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 4. scalar field (laplacian of indicator function) </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/11-480.webp 480w,/assets/img/2024-09-05-SuGaR/11-800.webp 800w,/assets/img/2024-09-05-SuGaR/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 5. surface mesh </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/12-480.webp 480w,/assets/img/2024-09-05-SuGaR/12-800.webp 800w,/assets/img/2024-09-05-SuGaR/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 6. octree 깊이에 따른 mesh recon. 비교 </div> <h2 id="refine-mesh">Refine Mesh</h2> <h3 id="refine-mesh-by-gaussians">Refine Mesh by Gaussians</h3> <ul> <li> <p>문제 :<br/> Poisson reconstruction으로 구한 mesh만 사용하면 rendering quality가 좋지 않음</p> </li> <li> <p>해결 :<br/> 새로 sampling한 new Gaussians를 (triangle) mesh에 binding하고,<br/> 해당 <code class="language-plaintext highlighter-rouge">Gaussians</code>과 <code class="language-plaintext highlighter-rouge">mesh</code>를 GS rasterizer로 <code class="language-plaintext highlighter-rouge">함께 optimize</code></p> <ul> <li>과정 1)<br/> mesh surface 상에서 <code class="language-plaintext highlighter-rouge">triangle</code> 당 \(n\) 개의 <code class="language-plaintext highlighter-rouge">new thin 3D Gaussians를 sampling</code>하여<br/> Gaussians를 triangle에 bind</li> <li>과정 2)<br/> mesh vertices in barycentric coordinate (무게중심 좌표계) 이용해서<br/> 각 Gaussian의 mean을 explicitly 계산할 수 있음<br/> (barycentric coordinate : 삼각형 내부의 점을 세 꼭짓점의 가중치로 표현)</li> <li>과정 3)<br/> Gaussians를 mesh triangle에 aligned되도록 flat하게 유지하기 위해<br/> each Gaussian은 2개의 learnable scaling factor \(s_x, s_y\) 와 1개의 learnable 2D quaternion \(q=a+bi\) 을 가지고 있음<br/> (Gaussians optimize하여 mesh optimze될 때 new thin Gaussians도 함께 optimize)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/13-480.webp 480w,/assets/img/2024-09-05-SuGaR/13-800.webp 800w,/assets/img/2024-09-05-SuGaR/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="code-flow">Code Flow</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/14-480.webp 480w,/assets/img/2024-09-05-SuGaR/14-800.webp 800w,/assets/img/2024-09-05-SuGaR/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 출처 : NeRF and 3DGS Study </div> <h2 id="question">Question</h2> <ul> <li>Q1 : well-distributed 가정을 따르는 approx. ideal density function을 직접 구해서 이를 density function과 비교하는데, GT 역할을 하는 approx. ideal density function이, 변하는 learnable Gaussian으로 구한 것이어도 학습이 안정적임?</li> <li>A1 : TBD</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="gaussian"/><category term="splatting"/><category term="rendering"/><category term="surface"/><summary type="html"><![CDATA[Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering (CVPR 2024)]]></summary></entry><entry><title type="html">CLIP</title><link href="https://semyeong-yu.github.io/blog/2024/CLIP/" rel="alternate" type="text/html" title="CLIP"/><published>2024-09-03T11:00:00+00:00</published><updated>2024-09-03T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/CLIP</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/CLIP/"><![CDATA[<h2 id="clip-contrastive-language-image-pre-training">CLIP: Contrastive Language-Image Pre-training</h2> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2103.00020">https://arxiv.org/abs/2103.00020</a><br/> code :<br/> <a href="https://github.com/openai/CLIP">https://github.com/openai/CLIP</a><br/> referenced blog :<br/> <a href="https://xoft.tistory.com/67">https://xoft.tistory.com/67</a></p> </blockquote> <h3 id="intro">Intro</h3> <ul> <li>CLIP : text와 image 간의 관계를 사용하는 다양한 task에 적용 가능</li> </ul> <h3 id="contrastive-pre-training">Contrastive Pre-training</h3> <ul> <li><code class="language-plaintext highlighter-rouge">contrastive learning</code> :<br/> labeling 없는 self-supervised learning 기법 중 하나로,<br/> 같은 class라면 embedding distance를 최소화하고<br/> 다른 class라면 embedding distance를 최대화한다 <ul> <li>contrastive loss<br/> \(L_{cont}^m(x_i, x_j, f) = 1 \{ y_i = y_j \} \| f(x_i) - f(x_j) \|^2 + 1 \{ y_i \neq y_j \} \text{max}(0, m - \| f(x_i) - f(x_j) \|^2)\)</li> <li>triplet loss<br/> \(L_{trip}^m(x, x^{+}, x^{-}, f) = max(0, \| f(x) - f(x^{+}) \|^2 - \| f(x) - f(x^{-}) \|^2 + m)\)</li> <li>\(N+1\) - Tuplet loss<br/> \(L_{tupl}(x, x^{+}, \{ x_{i}^{-} \}_{i=1}^{N-1}, f) = log(1 + \Sigma_{i=1}^{N-1}\text{exp}(f(x)^T f(x_{i}^{-}) - f(x)^T f(x^{+}))) = - log(\frac{\text{exp}(f(x)^T f(x^{+}))}{\text{exp}(f(x)^T f(x^{+})) + \Sigma_{i=1}^{N-1} \text{exp}(f(x)^T f(x_{i}^{-}))})\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-03-CLIP/1-480.webp 480w,/assets/img/2024-09-03-CLIP/1-800.webp 800w,/assets/img/2024-09-03-CLIP/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-03-CLIP/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> cosine similarity matrix가 identity matrix (I) 에 가깝도록 학습 </div> <ul> <li>image-text pair로 구성된 dataset에 대해<br/> image, text를 각각 encoder로 embedding한 뒤<br/> 같은 pair는 거리 최소화하고<br/> 다른 pair는 거리 최대화하도록<br/> constrative learning으로 두 encoder를 학습</li> </ul> <h3 id="application">Application</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-03-CLIP/2-480.webp 480w,/assets/img/2024-09-03-CLIP/2-800.webp 800w,/assets/img/2024-09-03-CLIP/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-03-CLIP/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>dataset classifier 만들기 또는 zero-shot prediction 등에<br/> pre-trained CLIP model 사용 가능</li> </ul>]]></content><author><name></name></author><category term="cv-tasks"/><category term="contrastive"/><category term="image"/><category term="text"/><summary type="html"><![CDATA[Contrastive Language-Image Pre-training]]></summary></entry><entry><title type="html">Server</title><link href="https://semyeong-yu.github.io/blog/2024/Server/" rel="alternate" type="text/html" title="Server"/><published>2024-09-03T11:00:00+00:00</published><updated>2024-09-03T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Server</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Server/"><![CDATA[<ul> <li><code class="language-plaintext highlighter-rouge">Server 종류</code> <ul> <li>Tower Server :<br/> Rack Server보다 크기 작고 소음 적어서 설치 자유로움</li> <li><code class="language-plaintext highlighter-rouge">Rack Server</code> :<br/> Rack(Server 거치대) 에 server를 마운트<br/> (선반형 rack 설치 금지) <ul> <li>width 고정하고, 사양 높아질수록 depth가 길게 나온다<br/> e.g. 일반 Rack : 1000, 1075mm<br/> e.g. Deep Rack : 1200mm</li> <li>height : 단위 <code class="language-plaintext highlighter-rouge">U</code> 사용<br/> e.g. DL360 : 1U Server<br/> e.g. DL380 : 2U Server</li> <li>Rack 총 높이 : 보통 42U</li> <li>구성 <ul> <li>렉 : 서버 거치대 프레임</li> <li>서버 : network에서 local computer(client)에 data, resource, service 제공</li> <li>스위치 : network 내의 장치들을 서로 연결하여 정확한 목적지에 data packet 전송<br/> 여러 개의 포트가 있고, data 링크 계층(2계층)에서 작동하며, MAC 주소를 기반으로 data 전송</li> <li>스토리지 : data 저장 <ul> <li>HDD, SSD</li> <li>NAS (Network Attached Storage) : network에 연결된 file 기반의 스토리지<br/> 여러 user가 파일을 저장 및 공유 가능<br/> TCP/IP 같은 표준 Ethernet 네트워크로 스위치와 연결하여 사용 <ul> <li>장점 : 파일 공유 접근성 좋음</li> <li>단점 : network 환경이 불안정하면 트래픽 문제 및 latency 증가로 인한 성능 저하</li> <li>사용 : 다수 user의 동시 접속 파일 공유, 비정형 data(동영상, 오디오, 문서, …)에 적합</li> </ul> </li> <li>SAN (Storage Area Network) : 스토리지를 별도의 network로 관리<br/> 대규모 user를 위한 고속 네트워크 시스템<br/> Fibre Channel network 사용 <ul> <li>장점 : 연결된 server와 상관없이 분산된 스토리지에서 data를 주고받을 수 있으므로 속도 빠르고 안정적</li> <li>단점 : 비쌈</li> <li>사용 : DB처럼 구조화된 data에 대해 고용량 및 고성능 I/O 업무</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li>Blade Server :<br/> 고밀도 Server<br/> Composable Infra-Structure (Server, Storage, Network module을 하나의 frame에 구성)</li> </ul> </li> <li>Server 등급 <ul> <li>Entry :<br/> 보급형 (CPU 1~2)<br/> 수백만원대 <ul> <li>web server, application server</li> <li>ML350, DL20/360/380</li> </ul> </li> <li>Midrange :<br/> 중형급 (CPU 4~)<br/> 수천만원대 <ul> <li>database hosting, application hosting, mission critical system</li> <li>DL560/580</li> </ul> </li> <li>High-End :<br/> 고사양 (CPU 수십개)<br/> 수억원대 <ul> <li>large scale data process, mission critical system</li> <li>Superdome</li> </ul> </li> </ul> </li> <li>DL380 <code class="language-plaintext highlighter-rouge">Server 구조</code> : <ul> <li>Disk : 앞면 베젤 열면 SAS 300GB disk 8개</li> <li>CPU : 윗면 열면 Intel cpu 2개</li> <li>RAM : CPU 양옆에 Memory ~24개</li> <li>Fan : CPU, Memory 옆에 열 식히는 팬</li> <li>Power Supply : 뒷면에 2개</li> <li>PCI 라이저 킷 : Power 옆에 2개</li> <li>iLO : HPE 원격 관리 기술</li> <li>Raid Controller : SAS cable에 연결</li> <li>I/O port : USB, display, dvd, …</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-03-Server/1-480.webp 480w,/assets/img/2024-09-03-Server/1-800.webp 800w,/assets/img/2024-09-03-Server/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-03-Server/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>CPU x86 (intel Pentium) <ul> <li>Core : PC CPU</li> <li><code class="language-plaintext highlighter-rouge">Xeon : Server CPU</code> <ul> <li>1번 : 등급 <ul> <li>8 : Platinum</li> <li>6/5 : Gold</li> <li>4 : Silver</li> <li>3 : Bronze</li> </ul> </li> <li>2번 : CPU 세대 <ul> <li>3 : 3세대 (Icelake)</li> <li>4 : 4세대 (Sapphire Rapids)</li> </ul> </li> <li>3번 : Product Line Suffix (CPU 기능) <ul> <li>U, Y, …</li> </ul> </li> <li>4번 : Clock Speed</li> <li>5번 : CPU core 수</li> <li>6번 : CPU에 필요한 전력량</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-03-Server/2-480.webp 480w,/assets/img/2024-09-03-Server/2-800.webp 800w,/assets/img/2024-09-03-Server/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-03-Server/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>RAM (Random Access Memory)<br/> 휘발성 메모리라서 전원 공급이 중단되면 지워짐 <ul> <li><code class="language-plaintext highlighter-rouge">DRAM</code> (Dynamic) : data 유지 위해 주기적으로 refresh<br/> main memory <ul> <li>DIMM (dual in-line memory module) :<br/> DRAM 여러 개를 회로기판 위에 장착한 memory module</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">SRAM</code> (Static) : 전원 공급 동안 data 유지<br/> cache</li> <li>RAM <ul> <li>1번 : memory 용량</li> <li>2번 : DIMM 구성에 따라 memory chip에 access하는 방법 <ul> <li>Single Rank : 한 번에 access하는 1개의 memory chip이 있어서 용량이 적고 느림</li> <li>Dual Rank : 동시에 access할 수 있는 두 세트의 memory chip이 있어서 용량이 많고 빠름</li> <li>Quad, Octal Rank</li> </ul> </li> <li>3번 : DDR 규격, 세대 <ul> <li>DDR SDRAM : Double data rate synchronous DRAM<br/> clock speed 높이지 않아도 SDRAM보다 전송 속도 2배 빠름</li> </ul> </li> <li>4번 : memory 동작 clock bandwidth (높을수록 전송 속도 빠름)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-03-Server/3-480.webp 480w,/assets/img/2024-09-03-Server/3-800.webp 800w,/assets/img/2024-09-03-Server/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-03-Server/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-03-Server/4-480.webp 480w,/assets/img/2024-09-03-Server/4-800.webp 800w,/assets/img/2024-09-03-Server/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-03-Server/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>DISK <ul> <li><code class="language-plaintext highlighter-rouge">HDD</code> (Hard Disk Drive) : 레코드판처럼 기계적으로 정보 저장 <ul> <li>SSD 규격 설명이랑 비슷한데 아래 사항들은 다름</li> <li>7.2K : disk RPM<br/> RPM이 클수록 속도가 빠르지만, 그만큼 발열, 전력소모도 많아서 안정성 떨어지고 비쌈</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">SSD</code> (Solid State Drive) : 반도체를 이용하여 정보 저장 (더 빠름) <ul> <li>1번 : disk 용량</li> <li>2번 : disk 연결 방식 <ul> <li>SATA (Serial ATA) : 저렴하고 느림</li> <li>SAS (Serial Attached SCSI) : 비싸지만 내구성 좋고 빠름<br/> 안정성 좋은 SCSI 방식을 직렬 구조로 변경하여 속도, 안정성 모두 챙김</li> </ul> </li> <li>3번 : disk 성능 (data 전송 속도)</li> <li>4번 : disk type <ul> <li>Read Intensive</li> <li>Mixed Use</li> <li>Write Intensive</li> </ul> </li> <li>5번 : disk 규격 <ul> <li>LFF (large form factor) : 3.5 inch</li> <li>SFF (small form factor) : 2.5 inch</li> </ul> </li> <li>6번 : disk 담는 carrier type <ul> <li>BC (Basic Carrier) : LED 무</li> <li>SC (Smart Carrier) : LED 유 (drive 제거할 때 data 손실에 대해 사전 경고 기능)</li> <li>Megaraid controller로 구성하는 server에는 BC disk만 장착 가능</li> </ul> </li> <li>7번 : disk 제조사</li> <li>8번 : HDD or SSD</li> </ul> </li> </ul> </li> </ul>]]></content><author><name></name></author><category term="others"/><category term="server"/><category term="rack"/><category term="cpu"/><category term="ram"/><category term="disk"/><summary type="html"><![CDATA[Server]]></summary></entry><entry><title type="html">DreamFusion</title><link href="https://semyeong-yu.github.io/blog/2024/Dreamfusion/" rel="alternate" type="text/html" title="DreamFusion"/><published>2024-08-29T11:00:00+00:00</published><updated>2024-08-29T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Dreamfusion</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Dreamfusion/"><![CDATA[<h2 id="dreamfusion-text-to-3d-using-2d-diffusion-iclr-2023">DreamFusion: Text-to-3D using 2D Diffusion (ICLR 2023)</h2> <h4 id="ben-poole-ajay-jain-jonathan-t-barron-ben-mildenhall">Ben Poole, Ajay Jain, Jonathan T. Barron, Ben Mildenhall</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2209.14988">https://arxiv.org/abs/2209.14988</a><br/> project website :<br/> <a href="https://dreamfusion3d.github.io/">https://dreamfusion3d.github.io/</a><br/> pytorch code :<br/> <a href="https://github.com/ashawkey/stable-dreamfusion">https://github.com/ashawkey/stable-dreamfusion</a></p> </blockquote> <h2 id="contribution">Contribution</h2> <ul> <li><code class="language-plaintext highlighter-rouge">SDS(Score Distillation) Loss</code> 처음 제시 <ul> <li>scalable, high-quality 2D diffusion model의 능력을 3D domain renderer로 distill</li> <li>3D 또는 multi-view training data 필요없고, pre-trained 2D diffusion model만 있으면, 3D synthesis 수행 가능!</li> </ul> </li> <li>NeRF가 Diffusion(Imagen) model with text에서 내놓을 만한 그럴 듯한 image를 합성하도록 함</li> <li><code class="language-plaintext highlighter-rouge">text-to-3D</code> synthesis 발전 시작</li> </ul> <h2 id="overview">Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/1-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/1-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Overview <ul> <li>initialize NeRF with random weight</li> <li>for each iter. <ul> <li>camera 위치와 각도, light 위치와 색상을 randomly sampling<br/> \(P(camera), P(light)\)</li> <li>NeRF로 image rendering</li> <li>text embedding \(\tau\) 이용해서 NeRF param. \(\theta\) 에 대한 SDS loss 계산</li> <li>update NeRF weight</li> </ul> </li> </ul> </li> </ul> <h3 id="random-camera-light-sampling">Random camera, light sampling</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/2-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/2-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">camera</code> : <ul> <li>3D model을 <code class="language-plaintext highlighter-rouge">bounded sphere</code> 내부로 제한하고,<br/> spherical coordinate(구 표면)에서 camera 위치를 sampling하여<br/> 구의 원점을 바라보도록 camera 각도 설정</li> <li>width(64)에 0.7 ~ 1.35의 상수값을 곱하여 focal length 설정</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">light</code> : <ul> <li>camera 위치를 중심으로 한 확률분포로부터 light의 위치를 sampling하고<br/> (어떤 확률분포 <code class="language-plaintext highlighter-rouge">????</code>)<br/> light 색상도 sampling</li> </ul> </li> </ul> <h3 id="nerf-rendering-with-shading">NeRF Rendering with shading</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/3-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/3-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> albedo : NeRF가 예측한 color </div> <ul> <li>rendering 방법 : <ol> <li>albedo \(\rho\) 만으로 rendering<br/> (기존 NeRF와 동일)</li> <li>albedo \(\rho\) 뿐만 아니라 shading하여 rendering</li> <li>albedo \(\rho\) 를 white \((1, 1, 1)\) 로 바꾼 뒤 shading하여 rendering</li> </ol> </li> <li><code class="language-plaintext highlighter-rouge">Shading</code>의 역할 : <ul> <li>shading 없이 \(\rho\) 만으로 rendering하면<br/> 평평한 3D model이 나와도 점수 높게 나옴</li> <li>shading으로 (빛 반사에 따른) shape 정보까지 고려해서 rendering하면<br/> <code class="language-plaintext highlighter-rouge">volume 있는</code> 3D model이 되도록 촉구</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">NeRF MLP</code> \(\theta\) : <ul> <li>MLP output : volume density \(\tau\) 와 albedo \(\rho\)</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Normal</code> \(n\) :<br/> \(n = - \frac{\nabla_{\mu} \tau}{\| \nabla_{\mu} \tau \|}\)<br/> where \(n\) 은 물체 표면의 법선벡터 <ul> <li>normal vector의 방향은<br/> volume density \(\tau\) 가 가장 급격하게 변하는 방향, 즉 \(\nabla_{\mu} \tau\) 의<br/> 반대 방향</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Shading</code> \(s\) :<br/> \(s = (l_p \circ \text{max}(0, n \cdot \frac{l - \mu}{\| l - \mu \|})) + l_a\)<br/> where \(l_p\) 는 light 좌표 \(l\) 에서 나오는 light(광원) 색상<br/> where \(l_a\) 는 ambient light(환경 조명) 색상<br/> where \(\mu\) 는 shading 값을 계산할 surface 위 point 좌표<br/> where \(\circ\) 는 element-wise multiplication <ul> <li>\(n \cdot (l - \mu)\) 는 표면에서의 normal vector와 표면에서 광원까지의 vector 간의 내적이며,<br/> 이는 Lambertian(diffuse) reflectance(난반사)에 의해 광원의 빛이 반사되는 정도를 나타냄<br/> 왜냐하면, 빛이 표면에 수직으로 들어올수록 많이 반사됨</li> <li>만약 빛이 표면 반대쪽에 있어서 또는 back-facing normal로 잘못 예측해서<br/> 내적 값 \(n \cdot (l - \mu)\) 이 음수일 경우<br/> 난반사에 의해 광원의 빛이 반사되는 정도는 0</li> <li>\(l_p \circ \text{난반사 정도} + l_a\) 에 의해<br/> <code class="language-plaintext highlighter-rouge">광원</code>의 색상 \(l_p\) 는 물체 <code class="language-plaintext highlighter-rouge">표면의 난반사 정도에 따라</code> 반영되고<br/> <code class="language-plaintext highlighter-rouge">환경 조명</code>의 색상 \(l_a\) 는 물체의 <code class="language-plaintext highlighter-rouge">모든 표면에 일정하게</code> 반영됨</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Color</code> \(c\) :<br/> \(c = \rho \circ s\) 또는 \(c = \rho\)</li> </ul> <h3 id="diffusion-loss-with-conditioning">Diffusion loss with conditioning</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/5-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/5-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Latent Diffusion</code> model : <ul> <li>image \(x\) 가 아니라 encoder를 거친 image latent vector \(z\) 에 대해 noising, denoising 수행</li> <li>noisy \(z_T\) 와 text embedding vector \(\tau_{\theta}\) 를 concat한 걸<br/> denoising하여 input image와 유사한 확률 분포를 갖도록 학습<br/> (text embedding vector \(\tau_{\theta}\) 을 conditioning (query) 로 넣어줌)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/4-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/4-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>text embedding vector \(\tau_{\theta}\) :<br/> T5-XXL text embedding을 거치기 전에<br/> text prompt engineering 수행 <ul> <li>Elevation angle(고각)이 60도 이상일 때 “overhead view”</li> <li>azimuth angle(방위각)에 따라 “front view”, “side view”, “back view”</li> <li>text prompt engineering은 원래 좀 투박하게 하나?</li> </ul> </li> <li>Imagen : <ul> <li>latent diffusion model with \(64 \times 64\) resolution<br/> (for fast training)</li> </ul> </li> </ul> <h3 id="sample-in-parmater-space-not-pixel-space">Sample in Parmater Space, not Pixel Space</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/10-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/10-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>\(x=g(\theta)\) : differentiable image parameterization (DIP)<br/> where \(x\) 는 image이고 \(g\) 는 renderer이고 \(\theta\) 는 param. <ul> <li>more compact param. space \(\theta\) 에서 optimize ㄱㄴ<br/> (더 강력한 optimization algorithm 사용 ㄱㄴ)</li> </ul> </li> <li>loss optimization으로 tractable sample 만들기 위해 diffusion model의 힘을 이용해서<br/> \(x\) in pixel space 가 아니라, \(\theta\) in parameter space 를 optimize<br/> s.t. \(x=g(\theta)\) 가 그럴 듯한 diffusion model sample처럼 보이도록</li> </ul> <h3 id="optimization">Optimization</h3> <ul> <li>실험적인 implementation : <ul> <li>noise level (time) sampling \(t\) :<br/> \(z_t, t \sim U[0, 1]\) 에서 noise level이 너무 크거나(\(t=1\)) 너무 작을 경우(\(t=0\)) instability 생기므로<br/> noise level \(t \sim U[0.02, 0.98]\) 로 sampling</li> <li>guidance weight \(w\) :<br/> Imagen이 NeRF에 얼만큼 영향을 미칠지(guide할지)인데,<br/> high-quality 3D model을 학습하기 위해서는<br/> CFG(classifier-free guidance) weight \(w\) 를 큰 값(100)으로 설정<br/> (NeRF MLP output color가 sigmoid에 의해 [0, 1]로 bounded되어있으므로 constrained optimization 문제라서 guidance weight 커도 딱히 artifacts 없음)<br/> (SDS loss는 mode-seeking property를 가지고 있어서 작은 guidance weight 값을 사용할 경우 over-smoothing됨 <code class="language-plaintext highlighter-rouge">????</code>)</li> <li>seed :<br/> noise level이 높을 때 smoothed density는 distinct modes를 많이 가지지 않고<br/> SDS Loss는 mode-seeking property를 가지고 있으므로<br/> random seed 바꿔도 실험 결과는 큰 차이 없음</li> </ul> </li> <li>implementation : <ul> <li>train : TPUv4, 15000 iter., 1.5h with Distributed Shampoo optimizer</li> <li>rendering : 각 cpu는 개별 view를 rendering하는데 사용</li> </ul> </li> </ul> <h2 id="rendering">Rendering</h2> <h3 id="structure">Structure</h3> <ul> <li>Mip-NeRF 360 구조 사용</li> <li>entire scene 대신 single object를 generate할 때<br/> <code class="language-plaintext highlighter-rouge">bounded sphere</code> 내에서 NeRF view-synthesis 하면 빠르게 수렴 및 좋은 성능</li> <li>\(\gamma(d)\) 를 input으로 받아 배경 색상을 계산하는 별도의 MLP로 <code class="language-plaintext highlighter-rouge">environment map</code>을 생성한 뒤 그 위에 ray rendering하면 좋은 성능 <ul> <li>배경이 보이는 부분은 배경에서의 누적 \(\alpha\) 값이 1이도록</li> <li>물체 때문에 배경이 안 보이는 부분은 배경에서의 누적 \(\alpha\) 값이 0이도록</li> </ul> </li> </ul> <h3 id="geometry-regularizer">Geometry Regularizer</h3> <ul> <li>DreamField의 regularization : <ul> <li><code class="language-plaintext highlighter-rouge">empty space가 불필요하게 채워지는</code> 것을 방지</li> <li>\(L_T = - \text{min} (\tau, \text{mean}(T(\theta, p)))\) :<br/> 평균 <code class="language-plaintext highlighter-rouge">transmittance가 클수록</code> loss가 작음<br/> where \(T(\theta, p)\) : transmittance with NeRF parameter \(\theta\) and camera pose \(p\)<br/> where \(\tau\) : 최대값 상수</li> </ul> </li> <li>Ref-NeRF의 regularization : <ul> <li>normal vector \(n_i\) 의 back-facing (<code class="language-plaintext highlighter-rouge">물체 안쪽을 향하는</code>) 문제를 방지</li> <li>orientation loss \(L = \Sigma_{i} w_i max(0, n_i \cdot d)^2\) :<br/> ray를 쏘면 물체의 앞면만 보이니까<br/> 물체 표면의 normal vector 방향과 ray 방향의 내적이 음수여야 한다<br/> 따라서 \(n_i\) 와 \(d\) 의 <code class="language-plaintext highlighter-rouge">내적이 양수일 경우</code> back-facing normal vector이므로 penalize <ul> <li>textureless shading을 쓸 때 해당 regularization이 중요<br/> 만약 해당 regularization 안 쓰면<br/> density field로 구한 normal 방향이 camera 반대쪽을 향하게 되어 shading이 더 어두워짐</li> </ul> </li> </ul> </li> </ul> <h2 id="sds-loss">SDS Loss</h2> <ul> <li> <p>NeRF로 rendering한 image \(x\) 에 noise를 더한 것을 \(z_t\) 로 두고<br/> U-Net \(\hat \epsilon_{\phi}(z_t | y, t)\) 을 빼서 denoising하여 얻은 image의 확률분포가<br/> 2D diffusion prior가 내놓는 image의 확률분포와 비슷하도록 하는 loss이며,<br/> 그 차이만큼 NeRF \(\theta\) 로 back-propagation</p> </li> <li> <p>배경지식 :</p> <ul> <li>DDPM Loss : \(E_{t, x_0, \epsilon} [\| \epsilon - \hat \epsilon_{\phi}(\alpha_{t}x_0 + \sigma_{t} \epsilon, t) \|^{2}]\)<br/> where \(\epsilon \sim N(0, I)\)<br/> where \(\alpha_{t} = \sqrt{\bar \alpha_{t}}\)<br/> where \(\sigma_{t} = \sqrt{1-\bar \alpha_{t}}\)</li> <li>만약 \(\theta\) 를 업데이트하기 위해 DDPM Loss를 직접 이용할 경우<br/> diffusion training의 multiscale 특성을 이용하고<br/> timestep schedule을 잘 선택한다면 <d-cite key="diffprior">[1]</d-cite> 잘 작동할 수 있다고 하지만<br/> 실험해봤을 때 timestep schedule을 tune하기 어려웠고 DDPM Loss는 불안정했음</li> <li>위의 DDPM Loss는 denoising U-Net param.을 업데이트하기 위함이었고,<br/> 우리는 fixed denoising U-Net을 이용하여<br/> NeRF param. \(\theta\) 업데이트하기 위한 SDS Loss를 새로 만들겠다!</li> </ul> </li> </ul> <h3 id="simple-derivation-of-sds-loss">Simple Derivation of SDS Loss</h3> <ul> <li>DDPM Loss를 \(\phi\) 말고 \(\theta\) 에 대해 미분하고<br/> constant \(\frac{dz_t}{dx} = \alpha_{t} \boldsymbol I\) 를 \(w(t)\) 에 넣으면</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/6-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/6-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> x는 NeRF가 생성한 image이고, y는 text embedding vector </div> <ul> <li>위의 U-Net Jacobian은 상당한 연산량을 가지는 데 비해<br/> 작은 noise만 줄 뿐 큰 영향이 없으므로<br/> SDS Loss에서 U-Net Jacobian term은 생략</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/7-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/7-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="derivation-of-sds-loss">Derivation of SDS Loss</h3> <ul> <li>SDS Loss gradient : <ul> <li>inspired by <code class="language-plaintext highlighter-rouge">gradient of weighted probability density distillation loss</code> <d-cite key="WaveNet">[2]</d-cite></li> <li> \[\nabla_{\theta} L_{SDS}(\phi, x=g(\theta)) = \nabla_{\theta} E_{t, z_t|x}[w(t)\frac{\sigma_{t}}{\alpha_{t}}\text{KL}(q(z_t|g(\theta)) \| p_{\phi}(z_t | y, t))]\] </li> </ul> </li> <li>KL-divergence : <ul> <li><a href="https://semyeong-yu.github.io/blog/2024/Diffusion/">Diffusion</a> 의 KL-divergence 부분에 따르면<br/> 모르는 분포 \(q(x)\) ( \(\epsilon\) ) 을 N개 sampling하여 trained \(p(x | \theta)\)로 근사하고자 할 때,<br/> \(KL(q \| p) \simeq \frac{1}{N} \sum_{n=1}^{N} {log q(x_n) - log p(x_n | \theta)}\) 이므로<br/> \(\text{KL}(q(z_t|g(\theta)) \| p_{\phi}(z_t | y, t)) = E_{\epsilon}[\text{log} q(z_t | x = g(\theta)) - \text{log} p_{\phi}(z_t | y)]\)<br/> \(\rightarrow\)<br/> \(\nabla_{\theta}\text{KL}(q(z_t|g(\theta)) \| p_{\phi}(z_t; y, t)) = E_{\epsilon}[\nabla_{\theta}\text{log} q(z_t | x = g(\theta)) - \nabla_{\theta}\text{log} p_{\phi}(z_t | y)]\)</li> </ul> </li> <li>\(\theta\) 에 대한 \(\text{log}q\) 의 미분 : <ul> <li>gradient of <code class="language-plaintext highlighter-rouge">forward process entropy</code> w.r.t mean param. \(\theta\)<br/> (variance는 고정)</li> <li>아래 수식을 \(\nabla_{\theta}log q(z_t | x = g(\theta))\) 계산에 이용<br/> \(z_t = \alpha_{t} x + \sigma_{t} \epsilon \sim N(\alpha_{t} x, \sigma_{t}^2)\)<br/> \(\rightarrow \text{log} q(z_t|x=g(\theta)) = -\frac{1}{2\sigma_{t}^2} \| z_t - \alpha_{t} x \|^2 + \text{constant}\)<br/> \(\rightarrow \frac{d\text{log}q(z_t | x)}{dx} = \frac{\alpha_{t}}{\sigma_{t}^2}(z_t - \alpha_{t} x) = \frac{\alpha_{t}}{\sigma_{t}^2}\sigma_{t}\epsilon = \frac{\alpha_{t}}{\sigma_{t}}\epsilon\)<br/> and \(\frac{d\text{log}q(z_t | x)}{dz_t} = -\frac{1}{\sigma_{t}^2}(z_t - \alpha_{t} x) = -\frac{1}{\sigma_{t}^2}\sigma_{t}\epsilon = -\frac{1}{\sigma_{t}}\epsilon\)<br/> and \(\frac{dz_t}{dx} = \alpha_{t}\)</li> <li>\(\nabla_{\theta}log q(z_t | x = g(\theta)) = (\frac{d\text{log}q(z_t | x)}{dx} + \frac{d\text{log}q(z_t | x)}{dz_t}\frac{dz_t}{dx})\frac{dx}{d\theta}\)<br/> \(= (\frac{\alpha_{t}}{\sigma_{t}}\epsilon - \frac{1}{\sigma_{t}}\epsilon \alpha_{t})\frac{dx}{d\theta}\)<br/> \(= 0\)<br/> (\(q\) 는 <code class="language-plaintext highlighter-rouge">고정된 variance의 noise</code>를 사용하므로 \(\theta\) 에 대한 entropy \(\text{log}q\) 의 미분 값은 0) <ul> <li>위의 식에서 \(\frac{d\text{log}q(z_t | x)}{dx}\) :<br/> <code class="language-plaintext highlighter-rouge">parameter score function</code><br/> gradient of log probability w.r.t parameter \(x\)<br/> (\(x\) 에 대한 \(\text{log}q\) 의 gradient 계산)</li> <li>\(\frac{d\text{log}q(z_t | x)}{dz_t}\frac{dz_t}{dx}\) :<br/> <code class="language-plaintext highlighter-rouge">path derivative</code><br/> gradient of log probability w.r.t sample \(z_t\)<br/> (\(q\) 를 따르는 sample \(z_t\) 를 통해 \(x\) 에 대한 \(\text{log}q\) 의 gradient 계산)</li> </ul> </li> <li>Sticking-the-Landing <d-cite key="vargrad">[3]</d-cite> 에 따르면<br/> path derivative term은 냅두고<br/> parameter score function term을 제거하여<br/> SDS loss gradient에 \(\epsilon\) 항을 포함할 경우<br/> <code class="language-plaintext highlighter-rouge">control-variates</code> 기법 <a href="https://en.wikipedia.org/wiki/Control_variates">Wikipedia</a>에 의해<br/> \(E[\cdot]\) 으로 gradient 구할 때 <code class="language-plaintext highlighter-rouge">variance를 줄일 수</code> 있음!<br/> (자세한 설명은 아래의 SDS Loss gradient Summary 부분 참고)<br/> (variance가 작으면 optimization이 빨라지고 더 나은 결과를 도출할 수 있음)</li> </ul> </li> <li>\(\theta\) 에 대한 \(\text{log}p_{\phi}\) 의 미분 : <ul> <li>gradient of <code class="language-plaintext highlighter-rouge">backward process entropy</code> (denoising U-Net) w.r.t mean param. \(\theta\)</li> <li>아래 수식을 \(\nabla_{\theta}log p_{\phi}(z_t | y)\) 계산에 이용<br/> \(\frac{d\text{log}q(z_t | x)}{dz_t}\) 구했듯이 \(\epsilon\) 대신 \(\epsilon_{\phi}\) 넣으면<br/> \(\nabla_{z_t} \text{log}p_{\phi}(z_t | y) = \frac{d\text{log}p_{\phi}(z_t | y)}{dz_t} = -\frac{1}{\sigma_{t}}\hat \epsilon_{\phi}\)<br/> and \(\frac{dz_t}{dx} = \alpha_{t}\)</li> <li> \[\nabla_{\theta}\text{log} p_{\phi}(z_t | y) = \nabla_{z_t} \text{log}p_{\phi}(z_t | y) \frac{dz_t}{dx} \frac{dx}{d\theta} = - \frac{\alpha_{t}}{\sigma_{t}} \hat \epsilon_{\phi}(z_t | y) \frac{dx}{d\theta}\] </li> </ul> </li> <li>SDS Loss gradient <code class="language-plaintext highlighter-rouge">Summary</code> : <ul> <li>SDS Loss gradient :<br/> \(\nabla_{\theta} L_{SDS}(\phi, x=g(\theta)) = E_{t, z_t|x}[w(t)\frac{\sigma_{t}}{\alpha_{t}}\nabla_{\theta}\text{KL}(q(z_t|g(\theta)) \| p_{\phi}(z_t | y, t))]\)<br/> \(= E_{t, \epsilon}[w(t)\frac{\sigma_{t}}{\alpha_{t}}E_{\epsilon}[\nabla_{\theta}\text{log} q(z_t | x = g(\theta)) - \nabla_{\theta}\text{log} p_{\phi}(z_t | y)]]\)<br/> \(= E_{t, \epsilon}[w(t)\frac{\sigma_{t}}{\alpha_{t}} (-\frac{\alpha_{t}}{\sigma_{t}}\epsilon \frac{dx}{d\theta} + \frac{\alpha_{t}}{\sigma_{t}} \hat \epsilon_{\phi}(z_t | y) \frac{dx}{d\theta})]\)<br/> \(= E_{t, \epsilon}[w(t)(\hat \epsilon_{\phi}(z_t | y) - \epsilon)\frac{dx}{d\theta}]\)</li> <li>\(\nabla_{\theta}log q(z_t | x = g(\theta))\) 의 path derivative term은 \(\epsilon\) 과 관련 있고!<br/> \(\nabla_{\theta}\text{log} p_{\phi}(z_t | y)\) 은 \(\epsilon\) 의 예측, 즉 \(\hat \epsilon_{\phi}\) 와 관련 있고!<br/> 둘의 KL-divergence를 loss term으로 사용한다!<br/> (\(\epsilon\) 을 \(\hat \epsilon\) 의 control-variate로 생각하여 <d-cite key="vargrad">[3]</d-cite> 방식처럼 둘의 차이로 SDS Loss gradient 만들 수 있음!)</li> </ul> </li> <li>Other Papers : <ul> <li>Graikos et al. (2022) <d-cite key="diffprior">[1]</d-cite> :<br/> \(KL(h(x) \| p_{\phi}(x|y))\) 로부터<br/> \(E_{\epsilon, t}[\| \epsilon - \hat \epsilon_{\theta}(z_t | y; t) \|^2] - \text{log} c(x, y)\) 를 유도해서 loss로 썼지만,<br/> SDS와 달리 error 제곱 꼴이라서 costly back-propagation</li> <li>DDPM-PnP 또한 auxiliary classifier \(c\) 를 썼지만,<br/> SDS에서는 CFG(classifier-free-guidance) 사용<br/> (별도의 image label 없이 image caption만 conditioning으로 넣어줘서 model 학습)</li> <li>지금까지 implicit model의 entropy의 gradient는 single noise level <d-cite key="ARDAE">[4]</d-cite> 에서의 amortized score model (control-variate 사용 안 함) 로 측정하였는데,<br/> SDS에서는 multiple noise level을 사용함으로써 optimization 더 쉽게 ㄱㄴ<br/> (multiple noise level <code class="language-plaintext highlighter-rouge">?????</code>)</li> <li>GAN-like amortized samplers는 Stein discrepancy 최소화 <d-cite key="Stein">[5]</d-cite> , <d-cite key="Stein2">[6]</d-cite> 로 학습하는데,<br/> 이는 SDS loss의 score 차이와 비슷</li> </ul> </li> </ul> <h2 id="pseudo-code">Pseudo Code</h2> <pre><code class="language-Python">params = generator.init() # NeRF param.
opt_state = optimizer.init(params) # optimizer
diffusion_model = diffusion.load_model() # Imagen diffusion model
for iter in iterations:
  t = random.uniform(0., 1.) # noise level (time step)
  alpha_t, sigma_t = diffusion_model.get_coeffs(t) # determine noisy z_t's mean, std.
  eps = random.normal(img_shape) # gaussian noise (epsilon)
  x = generator(params, ...) # NeRF rendered image
  z_t = alpha_t * x + sigma_t * eps # noisy NeRF image
  epshat_t = diffusion_model.epshat(z_t, y, t) # denoising U-Net
  g = grad(weight(t) * dot(stopgradient[epshat_t - eps], x), params) # derivative of SDS loss; stopgradient since do not update diffusion model
  params, opt_state = optimizer.update(g, opt_state) # update NeRF param.
return params
</code></pre> <h2 id="experiment">Experiment</h2> <h3 id="metric">Metric</h3> <ul> <li><code class="language-plaintext highlighter-rouge">CLIP R-Precision</code> <d-cite key="dreamfield">[7]</d-cite> : <ul> <li><code class="language-plaintext highlighter-rouge">rendered image의 text 일관성</code>을 측정<br/> (rendered image가 주어졌을 때 CLIP이 오답 texts 중 적절한 text를 찾는 accuracy로 계산)</li> <li>기존 CLIP R-Precision은 geometry quality는 측정할 수 없으므로<br/> 평평한 flat geometry에 대해서도 높은 점수가 나올 수 있음</li> <li>textureless render의 R-Precision(Geo)도 추가로 측정!</li> </ul> </li> <li>PSNR :<br/> zero-shot text-to-3D generation에서는<br/> text에 대한 3D Ground-Truth를 만들 수 없으므로<br/> GT를 필요로 하는 PSNR 같은 metric은 사용하지 못함</li> </ul> <h3 id="result">Result</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/8-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/8-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Geo(metry)의 CLIP R-Precision 점수가 높다는 것은 평평한 3D model이 아니라 shape 정보까지 고려했다는 것! </div> <ul> <li>위의 표 설명 : <ul> <li>GT Images : oracle (CLIP training에 사용된 dataset)</li> <li>CLIP-Mesh : CLIP으로 mesh를 optimize한 연구</li> </ul> </li> <li> <p>DreamFusion은 training할 때 <code class="language-plaintext highlighter-rouge">Imagen</code>을 썼고,<br/> Dream Fields와 CLIP-Mesh는 training할 때 <code class="language-plaintext highlighter-rouge">CLIP</code>을 썼으므로<br/> Dream Fields와 CLIP-Mesh를 사용하는 게<br/> DreamFusion보다 성능이 더 좋아야 하는데,<br/> 위의 표를 보면 Color와 Geometry 평가에서 DreamFusion이 높은 성능(text 일관성)을 보인다는 것을 확인할 수 있다</p> </li> <li>아쉬운 점 :<br/> 비슷한 다른 모델이 있다면 PSNR, SSIM 등으로 비교할 수 있었을텐데<br/> 비교군이 없어서 R-Precision으로 consistency 측정만 했음</li> </ul> <h3 id="ablation-study">Ablation Study</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/9-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/9-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>어떤 기법이 얼마나 성능에 기여했는지 파악하기 위해<br/> 4가지 기법을 점진적으로 추가 <ul> <li>(i) <code class="language-plaintext highlighter-rouge">ViewAug</code> : view-points의 범위를 넓힘</li> <li>(ii) <code class="language-plaintext highlighter-rouge">ViewDep</code> : view-dependent text prompt-engineering 사용<br/> (e.g. “overhead view”, “side view”)</li> <li>(iii) <code class="language-plaintext highlighter-rouge">Lighting</code> : 조명 사용</li> <li>(iv) <code class="language-plaintext highlighter-rouge">Textureless</code> : albedo를 white로 만들어서 (color 없이) shading</li> </ul> </li> <li>geometry quality를 확인하기 위해<br/> 3가지 rendering 기법을 비교 <ul> <li>(Top) <code class="language-plaintext highlighter-rouge">Albedo</code> : albedo \(\rho\) 만으로 rendering<br/> (기존 NeRF와 동일)</li> <li>(Middle) <code class="language-plaintext highlighter-rouge">Full Shaded</code> : albedo \(\rho\) 뿐만 아니라 shading하여 rendering</li> <li>(Bottom) <code class="language-plaintext highlighter-rouge">Textureless</code> : albedo \(\rho\) 를 white \((1, 1, 1)\) 로 바꾼 뒤 shading</li> </ul> </li> <li>결과 설명 : <ul> <li>기법 추가 없이 Albedo rendering 하면 R-Precision은 높게 나오는데<br/> Geometry가 엄청 이상함 (e.g. 머리 2개 가진 개)</li> <li>ViewDep, Lighting, Textureless 기법 사용해야 정확한 <code class="language-plaintext highlighter-rouge">geometry</code>까지 recon할 수 있음</li> <li>(ii) ViewDep의 영향 :<br/> geometry 개선되지만, surface가 non-smooth하고 Shaded rendering 결과가 bad</li> <li>(iii) Lighting의 영향 :<br/> geometry 개선되지만, 어두운 부분은 (e.g. 해적 모자) 여전히 non-smooth</li> <li>(iv) Textureless의 영향 :<br/> geometry smooth하게 만드는 데 도움 되지만, color detail (e.g. 해골 뼈)이 geometry에 carved 되는 문제 발생</li> </ul> </li> </ul> <h2 id="limitation">Limitation</h2> <ul> <li>SDS를 적용하여 만든 2D image sample은 <code class="language-plaintext highlighter-rouge">over-saturated</code> 혹은 <code class="language-plaintext highlighter-rouge">over-smoothed</code> result <ul> <li>dynamic thresholding <d-cite key="dynathres">[8]</d-cite> 을 사용하면 SDS를 image에 적용할 때의 문제를 완화시킬 수 있다고 알려져 있긴 하지만, NeRF context에 대해서는 해결하지 못함<br/> (dynamic thresholding이 뭔지 아직 몰라서 읽어 봐야 됨 <code class="language-plaintext highlighter-rouge">?????</code>)</li> </ul> </li> <li>SDS를 적용하여 만든 2D image sample은 <code class="language-plaintext highlighter-rouge">diversity</code> 부족<br/> (random seed 바꿔도 3D result에 큰 차이 없음)</li> </ul> <p>This may be fundamental to our use of reverse KL divergence, which has been previously noted to have mode-seeking properties in the context of variational inference and probability density distillation <code class="language-plaintext highlighter-rouge">?????</code></p> <ul> <li>\(64 \times 64\) Imagen (<code class="language-plaintext highlighter-rouge">low resol.</code>)을 사용하여 3D model의 fine-detail이 부족할 수 있음 <ul> <li>diffusion model 또는 NeRF를 더 큰 걸 사용하면 문제 해결할 수 있지만, 그만큼 겁나 느려지지…</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">2D image로부터 3D recon.</code>하는 게 원래 어렵고 애매한 task임<br/> e.g. inverse rendering, dreamfusion <ul> <li>같은 2D images로부터 무수히 많은 3D worlds가 존재할 수 있으니까</li> <li>optimization landscape가 highly non-convex하므로 local minima에 빠지지 않기 위한 기법들 필요<br/> (local minima : e.g. 모든 scene content가 하나의 flat surface에 painted된 경우)</li> <li>more <code class="language-plaintext highlighter-rouge">robust 3D prior</code>가 도움 될 것임</li> </ul> </li> </ul> <h2 id="latest-papers">Latest Papers</h2> <ul> <li>본 논문 DreamFusion과 관련된 논문들 <ul> <li>ProlificDreamer</li> <li>CLIP Goes 3D</li> <li>Magic3D</li> <li>Fantasia3D</li> <li>CLIP-Forge</li> <li>CLIP-NeRF</li> <li>Text2Mesh</li> <li>DDS (Delta Denoising Score)</li> </ul> </li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 : diffusion의 mode-seeking property?</p> </li> <li>A1 : <ul> <li>A1-1 : diffusion에서 forward process는 mean-seeking property 가지고 있고, backward process는 mode-seeking property 가지고 있는 걸로 알고 있는데 이거랑 관련 있을까요?</li> <li>A1-2 : 그 아까 말씀하신 mode-seeking property에 대해 찾아봤는데 diffusion의 Langevin Dynamics 등의 샘플링 방법이 갖고 있는 특징으로 특정 mode에서 sample을 집중적으로 생성하는 특징을 의미하네요. 제 생각에 “mode-seeking property를 갖고있어 guidance weight가 작으면 over-smoothing 된다”는 말은 diffusion model에서는 특정 모드에 집중되어 sample들이 생성되는데 낮은 guidance weight를 쓰면 여러 모드 사이를 부드럽게(?) 연결하려는 (over-smoothing) 말이지 않을까 싶네요.(즉 너무 매끄럽거나 디테일이 떨어지는 이미지 생성)</li> </ul> </li> <li> <p>Q2 : reverse KL-divergence를 최소화하는 과정의 경우 mode-seeking property (확률 높은 중요한 부분 찾는 경향)가 있다는데,<br/> reverse KL-divergence와 mode-seeking property가 무슨 관계인가요?</p> </li> <li> <p>A2 : TBD</p> </li> <li> <p>Q3 : SDS loss로 image rendering한 samples의 경우 diversity가 부족하고 그 이유가 mode-seeking property라는 거 같은데,<br/> 오히려 diversity가 부족한 게 단점이 아니라,<br/> mode-seeking property로 중요한 부분을 잘 캐치해서 consistent하게 그려내는 게 장점이 될 수 있지 않나요?</p> </li> <li> <p>A3 : TBD<br/> While modes of generative models in high dimensions are often far from typical samples (Nalisnick et al., 2018), the multiscale nature of diffusion model training may help to avoid these pathologies. <code class="language-plaintext highlighter-rouge">?????</code></p> </li> <li> <p>Q4 : \(\theta\) 에 대한 \(\text{log}q\) 의 미분에서 path derivative term은 냅두고 parameter score function term은 제거해서 control-variates에 의해 variance를 줄였다고 하는데,<br/> parameter score function term을 걍 제거해버리는 게 좀 야매 아닌가요?</p> </li> <li>A4 : TBD</li> </ul>]]></content><author><name></name></author><category term="generative"/><category term="sds"/><category term="diffusion"/><category term="nerf"/><category term="3d"/><category term="rendering"/><summary type="html"><![CDATA[Text-to-3D using 2D Diffusion (ICLR 2023)]]></summary></entry><entry><title type="html">GaussianEditor</title><link href="https://semyeong-yu.github.io/blog/2024/GSeditor/" rel="alternate" type="text/html" title="GaussianEditor"/><published>2024-08-25T11:00:00+00:00</published><updated>2024-08-25T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/GSeditor</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/GSeditor/"><![CDATA[<h2 id="gaussianeditor-swift-and-controllable-3d-editing-with-gaussian-splatting">GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting</h2> <h4 id="yiwen-chen-zilong-chen-chi-zhang-feng-wang-xiaofeng-yang-yikai-wang-zhongang-cai-lei-yang-huaping-liu-guosheng-lin">Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, Guosheng Lin</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2311.14521">https://arxiv.org/abs/2311.14521</a><br/> project website :<br/> <a href="https://gaussianeditor.github.io/">https://gaussianeditor.github.io/</a><br/> code :<br/> <a href="https://github.com/buaacyw/GaussianEditor">https://github.com/buaacyw/GaussianEditor</a></p> </blockquote> <h2 id="paper-review-후기">Paper Review 후기</h2> <ul> <li>novelty : <ul> <li>SAM mask를 GS로 inverse rendering해서 target GS identify</li> <li>기존 GS에서 크게 벗어나지 않도록(stability) anchor loss</li> </ul> </li> <li>3DGS 나오고나서 3DGS 이용한 Editing에 대해 잽싸게 낸 논문이라<br/> 비교 대상도 없고<br/> Editing loss도 기존 기법을 그대로 써서 novelty 흐음…?</li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li>NeRF-based 3D Editing : <ul> <li>Instruct-nerf2nerf: Editing 3D scenes with instructions</li> <li>Ed-nerf: Efficient text-guided editing of 3D scene using latent space nerf</li> <li>Clip-nerf: Text-and-Image driven manipulation of neural radiance fields</li> <li>Nerf-art: Text-driven neural radiance fields stylization</li> <li>Dreameditor: Text-driven 3D scene editing with neural fields</li> </ul> </li> <li>NeRF-based 3D Editing by MLP의 문제점 : <ul> <li><code class="language-plaintext highlighter-rouge">specific</code> scene parts를 직접 수정하는 데 제한</li> <li>inpainting 및 scene composition 과정이 <code class="language-plaintext highlighter-rouge">복잡</code></li> <li>strictly <code class="language-plaintext highlighter-rouge">masked area</code> 내에서만 editing 가능</li> </ul> </li> <li>3DGS-based 3D Editing의 문제점 : <ul> <li>Editing할 Gaussian을 <code class="language-plaintext highlighter-rouge">identify</code>(분류)해야 함</li> <li>SDS처럼 Diffusion model로 얻은 <code class="language-plaintext highlighter-rouge">random generative guidance</code>를 3DGS에 적용할 때<br/> randomness in loss로 인해<br/> view마다 non-consistent(random)한 image를 합성(Editing)하므로<br/> GS is directly affected by randomness,<br/> so 업데이트 불안정</li> <li><code class="language-plaintext highlighter-rouge">수많은</code> Gaussian points를 업데이트해야 함<br/> NeRF-based에서처럼 MLP NN buffering이 불가능하므로 불안정하여<br/> finely detailed result로 수렴하는 걸 방해</li> </ul> </li> </ul> <h2 id="method">Method</h2> <h3 id="gaussian-semantic-tracing">Gaussian Semantic Tracing</h3> <ul> <li> <p>전제 : 3DGS가 이미 잘 구성되어 있다고 가정하고, 특정 scene part를 제거 또는 추가하거나 inpainting하는 등 3D Editing 수행</p> </li> <li> <p>Gaussian Semantic Tracing :<br/> 훈련하는 동안 3D Editing할 target을 trace하기 위해 semantic label(mask) 생성</p> </li> </ul> <h4 id="parameters">Parameters</h4> <ul> <li> <p>\(x, s, q, \alpha , c\) (position, covariance(scale, quaternion), opacity, color) 뿐만 아니라<br/> \(m_{ij}\) (<code class="language-plaintext highlighter-rouge">semantic Gaussian mask</code> for i-th Gaussian and j-th semantic label) 추가</p> </li> <li> <p>densification할 때 clone/split된 points는 parent point의 semantic label를 그대로 물려받음</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-25-GSeditor/1-480.webp 480w,/assets/img/2024-08-25-GSeditor/1-800.webp 800w,/assets/img/2024-08-25-GSeditor/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-25-GSeditor/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 처음에 inaccurate segmentation mask에서 출발했더라도 Gaussian semantic tracing하는 동안 3DGS 업데이트하면서 semantic Gaussian mask도 알맞게 업데이트됨 </div> <h4 id="initial-labeling-process">Initial Labeling Process</h4> <ul> <li> <p>camera pose 하나 골라서 <code class="language-plaintext highlighter-rouge">SAM</code>(Segment Anything)으로 2D segmentation 수행한 뒤<br/> <code class="language-plaintext highlighter-rouge">inverse rendering</code>으로 2D mask를 3D Gaussian으로 unproject<br/> \(w_i^j = \Sigma o_i (p) \ast T_i^j (p) \ast M^j (p)\)<br/> where \(w_i^j\) : weight of i-th Gaussian for j-th semantic label<br/> where \(o, T, M, p\) : opacity, transmittance, mask, pixel</p> </li> <li> <p>average weight가 threshold를 넘는 경우에만 해당 i-th Gaussian이 j-th semantic class를 갖는다고 선별</p> </li> </ul> <h3 id="hierarchical-gaussian-splatting">Hierarchical Gaussian Splatting</h3> <ul> <li> <p>Hierarchical Gaussian Splatting :<br/> stabilized and fine results 만들기 위해 anchor loss 사용</p> </li> <li> <p>3D Editing 위해 densification할 때<br/> threshold를 manually 정하는 게 아니라,<br/> 3D position gradients가 top k% 안에 드는 3DGS들만 선택적으로 densify<br/> (k값이 점점 증가)</p> </li> <li><code class="language-plaintext highlighter-rouge">anchor loss</code> : <ul> <li>3D Editing 때문에 densification할 때마다 <code class="language-plaintext highlighter-rouge">기존</code>의 Gaussian param.을 anchor에 record</li> <li>3D Editing에 따라 변형되는 Gaussian param.가 각 anchor로부터 크게 벗어나지 않도록 함<br/> \(L_{anchor}^P = \Sigma_{i=0}^n \lambda_{i} (P_i - \hat P_i)^2\)<br/> where \(P : x, s, q, \alpha , c, m_{ij}\)<br/> where \(\lambda_{i}\) 값이 점점 증가 (새로 만들어지는 Gaussian param.의 영향이 크도록)</li> <li><code class="language-plaintext highlighter-rouge">stable</code> geometry formation under stochastic loss 보장</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Edit loss</code> : <ul> <li>3DGS model로 rendering한 image와 diffusion model 간의 차이<br/> \(L_{Edit} = D(\theta ; p, e)\)<br/> where \(D, \theta , p, e\) : Diffusion model, 3D model, camera pose, prompt</li> <li>2D diffusion model로 3D Editing하는 방법 :<br/> 1) <code class="language-plaintext highlighter-rouge">DreamFusion</code> <d-cite key="Dreamfusion">[1]</d-cite> 의 <code class="language-plaintext highlighter-rouge">SDS loss</code>처럼 3D model의 rendering과 other conditions를 2D diffusion model에 넣어준 뒤, noise 넣고 <code class="language-plaintext highlighter-rouge">denoising하는 과정에서 내놓은 score</code>가 3D model의 업데이트 방향을 guide<br/> 즉, 3D model로 만든 image가 2D diffusion에서의 그럴 듯한 image distribution에 부합하도록 함<br/> 2) <code class="language-plaintext highlighter-rouge">Instruct-nerf2nerf</code>처럼 3D model의 rendering과 prompts 이용해서 <code class="language-plaintext highlighter-rouge">2D Editing</code> 수행하는 데 초점을 두고, Edited 2D multi-view images를 training target으로 사용하여 3D model에게 guidance 줌</li> </ul> </li> <li>total loss :<br/> \(L = L_{Edit} + \Sigma_{P \in [x, s, q, \alpha , c]} \lambda_{P} L_{anchor}^P\)</li> </ul> <h3 id="3d-inpainting">3D Inpainting</h3> <ul> <li> <p>외부 모델들 사용해서 Efficient 3D Editing 구현</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Object Removal</code> (object 제거) :</p> <ul> <li>semantic label(mask) 가지는 3D Gaussian만 삭제하면 target object와 scene 사이의 interface에서 artifacts 생김</li> <li><code class="language-plaintext highlighter-rouge">precise mask</code>를 생성할 필요가 있음 <ul> <li>삭제한 3DGS와 가장 가까운 Gaussian을 KNN으로 identify</li> <li>이를 다양한 view-points로 project하여 mask를 <code class="language-plaintext highlighter-rouge">확장</code> (dilate)</li> <li>hole을 메꿔서 interface area를 정확하게 표현하도록 refined mask를 생성</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Diffusion model</code>을 이용해서 해당 area를 <code class="language-plaintext highlighter-rouge">2D inpainting</code> (object 삭제)</li> <li>inpainted image를 기반으로 <code class="language-plaintext highlighter-rouge">3DGS 업데이트</code></li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-25-GSeditor/4-480.webp 480w,/assets/img/2024-08-25-GSeditor/4-800.webp 800w,/assets/img/2024-08-25-GSeditor/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-25-GSeditor/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Object Incorporation by text</code> (object 추가 혹은 수정) : <ul> <li>editing area에 BB 만듦</li> <li><code class="language-plaintext highlighter-rouge">Stable Diffusion XL</code> model <d-cite key="SDXL">[2]</d-cite> 을 이용해서 해당 area에 <code class="language-plaintext highlighter-rouge">넣을 image</code>를 생성하고<br/> fg object is segmented</li> <li><code class="language-plaintext highlighter-rouge">Wonder3D</code> model <d-cite key="Wonder3D">[3]</d-cite> 을 이용해서 fg-segmented image를 3D textured <code class="language-plaintext highlighter-rouge">mesh</code>로 변환</li> <li>Hierarchical Gaussian Splatting을 이용해서 mesh를 새로운 <code class="language-plaintext highlighter-rouge">3DGS</code>로 변환</li> <li><code class="language-plaintext highlighter-rouge">DPT</code> <d-cite key="DPT">[4]</d-cite> 로 depth estimation해서 기존의 3DGS와 생성된 3DGS의 <code class="language-plaintext highlighter-rouge">depth를 align</code>해주고 기존의 3DGS와 생성된 3DGS를 <code class="language-plaintext highlighter-rouge">concatenate</code>(결합)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-25-GSeditor/5-480.webp 480w,/assets/img/2024-08-25-GSeditor/5-800.webp 800w,/assets/img/2024-08-25-GSeditor/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-25-GSeditor/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-25-GSeditor/3-480.webp 480w,/assets/img/2024-08-25-GSeditor/3-800.webp 800w,/assets/img/2024-08-25-GSeditor/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-25-GSeditor/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="experiments">Experiments</h2> <ul> <li>Implementation : <ul> <li>view-point (camera-pose) 개수 : 24-96개</li> <li>optimization : 3DGS가 이미 구성되었다는 전제 하에<br/> 3D Editing하는 데만 500-1000 steps, 5-10 min.<br/> (3 min. for Wonder3D mesh 생성 + 2 min. for 3DGS로 변환 및 refine)</li> </ul> </li> <li>Ablation Study : <ul> <li>w/o Semantic Tracing :<br/> target object만 Editing되는 게 아니라 image 전체 Editing</li> <li>w/o Hierarchical GS :<br/> uncontrolled densification 및 image blurring 초래</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-25-GSeditor/6-480.webp 480w,/assets/img/2024-08-25-GSeditor/6-800.webp 800w,/assets/img/2024-08-25-GSeditor/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-25-GSeditor/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-25-GSeditor/7-480.webp 480w,/assets/img/2024-08-25-GSeditor/7-800.webp 800w,/assets/img/2024-08-25-GSeditor/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-25-GSeditor/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="conclusion">Conclusion</h2> <ul> <li>2 strategies <ul> <li><code class="language-plaintext highlighter-rouge">Gaussian Semantic Tracing</code><br/> for precise Gaussian <code class="language-plaintext highlighter-rouge">identification</code> of editing areas</li> <li><code class="language-plaintext highlighter-rouge">Hierarchical GS</code><br/> for balance b.w. fluidity and <code class="language-plaintext highlighter-rouge">stability</code><br/> to achieve <code class="language-plaintext highlighter-rouge">detailed(fine) results</code> under stochastic guidance</li> </ul> </li> </ul> <h2 id="limitation">Limitation</h2> <ul> <li>supervision을 위해 <code class="language-plaintext highlighter-rouge">2D diffusion model에 의존</code>하여 3D editing을 수행하는데<br/> 현재 2D diffusion model은 특정 복잡한 prompts에 대해서는 effective guidance를 제공하는 데 어려움이 있어 3D editing에도 한계 있음</li> </ul> <h2 id="question">Question</h2> <ul> <li>Q1 : Edit loss에서 사용하는 SDS loss나 Instruct-nerf2nerf 기법은 이미 있는 내용이고,<br/> 본 논문에서 볼 건 아래의 두 가지 정도인데 (SAM mask를 GS로 inverse rendering해서 target GS identify하고<br/> 기존 GS에서 크게 벗어나지 않도록(stability) anchor loss)<br/> 별로 novelty가 없는 것 같다</li> <li>A1 : ㅇㅈ</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="GS"/><category term="3d"/><category term="editing"/><summary type="html"><![CDATA[Swift and Controllable 3D Editing with Gaussian Splatting (CVPR 2024)]]></summary></entry><entry><title type="html">COLMAP-Free 3D Gaussian Splatting</title><link href="https://semyeong-yu.github.io/blog/2024/Colmapfree/" rel="alternate" type="text/html" title="COLMAP-Free 3D Gaussian Splatting"/><published>2024-08-24T11:00:00+00:00</published><updated>2024-08-24T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Colmapfree</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Colmapfree/"><![CDATA[<h2 id="colmap-free-3d-gaussian-splatting">COLMAP-Free 3D Gaussian Splatting</h2> <h4 id="yang-fu-sifei-liu-amey-kulkarni-jan-kautz-alexei-a-efros-xiaolong-wang">Yang Fu, Sifei Liu, Amey Kulkarni, Jan Kautz, Alexei A. Efros, Xiaolong Wang</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2312.07504">https://arxiv.org/abs/2312.07504</a><br/> project website :<br/> <a href="https://oasisyang.github.io/colmap-free-3dgs/">https://oasisyang.github.io/colmap-free-3dgs/</a><br/> pytorch code :<br/> <a href="https://github.com/NVlabs/CF-3DGS">https://github.com/NVlabs/CF-3DGS</a></p> </blockquote> <h2 id="introduction">Introduction</h2> <ul> <li>기존 novel-view-synthesis : <ul> <li>input images<br/> \(\rightarrow\) COLMAP library for SfM <code class="language-plaintext highlighter-rouge">pcd, camera pose</code> 계산<br/> \(\rightarrow\) NeRF or 3DGS</li> <li>단점 : 시간 많이 걸리고, feature 추출 오차에 대해 민감성이 있고, 반복적인 영역을 처리하는 데 어려움</li> </ul> </li> <li> <p>Motivation :<br/> <code class="language-plaintext highlighter-rouge">pose estimation</code>과 <code class="language-plaintext highlighter-rouge">novel-view-synthesis</code>를<br/> COLMAP과 3DGS로 나눠서 하지 말고<br/> <code class="language-plaintext highlighter-rouge">end-to-end로 동시에</code> 할 수는 없을까?</p> </li> <li>Related Work :<br/> 사전에 COLMAP library 사용하지 않기 위해<br/> <code class="language-plaintext highlighter-rouge">BARF, Nope-NeRF, L2G-NeRF</code> 등<br/> 여러 방법들이 제안되어 왔지만<br/> 여러 한계 있음 <ul> <li>perturbation이 적어야 함</li> <li><code class="language-plaintext highlighter-rouge">camera motion의 범위</code>가 너무 넓으면 안 됨<br/> (Nope-NeRF 등은 pose를 직접 optimize하는 게 아니라 ray casting process를 optimize하는 간접적인 방법이라서 camera 이동이 커지면 optimize 난이도가 복잡해짐)</li> <li><code class="language-plaintext highlighter-rouge">training time</code>이 너무 오래 걸림</li> <li>NeRF-based 기법들은 MLP-based implicit method이므로<br/> 3DGS처럼 explicit pcd를 요구하는 method에 적용하기 어렵</li> <li>regularization term이 많아져서 복잡하고 geometric prior를 요구하기도 함</li> </ul> </li> <li>COMALP-Free 3D GS : <ul> <li>3DGS가 <code class="language-plaintext highlighter-rouge">explicit</code> representation (pcd 등) 을 활용할 수 있기 때문에 새로운 접근이 가능해졌다 <ul> <li>temporal continuity data (video sequence)와<br/> explicit representation data (3DGS)를 이용해서<br/> pose estimation과 novel view synthesis를 동시에 수행</li> </ul> </li> <li>Local 3DGS : <ul> <li><code class="language-plaintext highlighter-rouge">initialization</code> 위해 Nope-Nerf 랑 비슷하게 monocular <code class="language-plaintext highlighter-rouge">depth-map</code> 사용</li> <li>목표 :<br/> 주어진 frame \(t-1\) 에서의 local 3D Gaussian 집합을 구성하고,<br/> frame \(t\) 에서의 local 3D Gaussian 집합으로 변환할 수 있는<br/> <code class="language-plaintext highlighter-rouge">relative camera pose (affine transformation)</code> 학습</li> </ul> </li> <li>Global 3DGS : <ul> <li>목표 :<br/> Local 3DGS에서 구한 relative camera pose를 기반으로<br/> Global 3DGS를 순차적으로 점진적으로 계속 업데이트해서<br/> entire scene <code class="language-plaintext highlighter-rouge">reconstruction</code> 결과가 깔끔하게 나타나도록 하자</li> </ul> </li> </ul> </li> <li>COLMAP vs 본 논문 : <ul> <li>COLMAP : 100장의 images를 <code class="language-plaintext highlighter-rouge">한 번에</code> 넣고 camera pose를 optimize</li> <li>본 논문 : video sequence를 <code class="language-plaintext highlighter-rouge">순차적으로</code> 실시간으로 받으며 점진적으로 optimize</li> </ul> </li> </ul> <h2 id="overview">Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-24-Colmapfree/1-480.webp 480w,/assets/img/2024-08-24-Colmapfree/1-800.webp 800w,/assets/img/2024-08-24-Colmapfree/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-24-Colmapfree/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="local-3dgs-for-relative-pose-estimation">Local 3DGS for Relative Pose Estimation</h2> <ul> <li>Initialization from a Single View : <ul> <li>initial frame을 monocular depth network (DPT)에 넣어 depth map \(D_1\) 생성</li> <li>3D mean :<br/> <code class="language-plaintext highlighter-rouge">initial frame</code> (2D 정보)과<br/> <code class="language-plaintext highlighter-rouge">initial depth map</code> \(D_1\) (3D 정보)와<br/> <code class="language-plaintext highlighter-rouge">intrinsic</code> param. 이용해서<br/> 3D pcd로 투영하고, 이를 initial 3DGS mean point로 사용</li> <li>opacity, SH-coeff., covariance(rotation, scale) :<br/> L1, D-SSIM photometric loss로 <code class="language-plaintext highlighter-rouge">optimal (initial) Local 3DGS</code>를 5초 정도만에 구함<br/> initial frame \(t = 1\) 에 대해<br/> \(G_t^{\ast} = \text{argmin}_{\alpha_{t}, c_t, r_t, s_t} L_{rgb}(R(G_t), I_t) = (1 - \lambda) L_1 + \lambda L_{D-SSIM}\)</li> </ul> </li> <li>Pose Estimation by 3D Gaussian Transformation : <ul> <li>Gaussian 집합 \(G_t\) 를 \(G_{t+1}\) 로 올바르게 변환할 수 있는 learnable SE-3 affine transformation \(T_t\) 를 찾아야 함</li> <li>전제 : video로 찍은 연속적인 frame이므로 \(T_t\) 의 값이 크지 않음</li> <li>photometric loss로 <code class="language-plaintext highlighter-rouge">optimal relative camera pose(affine transformation)</code>을 10초 안에 구함<br/> \(T_t^{\ast} = \text{argmin}_{T_t} L_{rbg} (R(T_t \odot G_t), I_{t+1})\)<br/> where \(G_t\) is <code class="language-plaintext highlighter-rouge">freezed</code> (self-rotation 등 방지)<br/> (geometric transformation(camera movement)에만 집중)</li> </ul> </li> </ul> <h2 id="global-3dgs-with-progressively-growing">Global 3DGS with Progressively Growing</h2> <ul> <li>Local 3DGS를 통해 optimal relative camera pose를 구했다 <ul> <li>한계 : frame \(F\) 와 frame \(F+t\) 간의 relative camera pose를 단순히 \(\prod_{k=F}^{F+t} T_k\) 처럼 곱으로 두면 오차가 점점 커져서<br/> entire <code class="language-plaintext highlighter-rouge">scene reconstruction 결과가 noisy</code></li> </ul> </li> <li>Global 3DGS : <ul> <li>frame이 들어올 때마다 relative camera pose \(T_t\) 와 frame \(t, t+1\) 이용해서 <code class="language-plaintext highlighter-rouge">optimal Global 3DGS</code> 업데이트 (progressively growing)</li> <li>어떻게 업데이트? :<br/> frame \(t+1\) 에는 frame \(t\) 에서 <code class="language-plaintext highlighter-rouge">보지 못한 일부 영역</code> 들이 있으므로<br/> 새로운 frame에 대한 <code class="language-plaintext highlighter-rouge">under-reconstruction densification</code>에 초점을 두어<br/> last frame까지 계속해서 점진적으로 scene reconstruction 수행<br/> (last frame까지 계속 under-reconstruction 상황(보지 못했던 영역)이 발생할 것이라는 전제)<br/> (새로운 테크닉은 아니고 3DGS에서의 adaptive density control과 동일)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-24-Colmapfree/2-480.webp 480w,/assets/img/2024-08-24-Colmapfree/2-800.webp 800w,/assets/img/2024-08-24-Colmapfree/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-24-Colmapfree/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Local 3DGS와 Global 3DGS는 iteratively optimized</li> </ul> <h2 id="experiment">Experiment</h2> <ul> <li>GS 말고 pose-free NeRF methods와 비교했을 때<br/> pose trajectory와 scene reconstruction 측면에서<br/> 본 논문이 훨씬 더 좋은 성능</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-24-Colmapfree/3-480.webp 480w,/assets/img/2024-08-24-Colmapfree/3-800.webp 800w,/assets/img/2024-08-24-Colmapfree/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-24-Colmapfree/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-24-Colmapfree/4-480.webp 480w,/assets/img/2024-08-24-Colmapfree/4-800.webp 800w,/assets/img/2024-08-24-Colmapfree/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-24-Colmapfree/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-24-Colmapfree/5-480.webp 480w,/assets/img/2024-08-24-Colmapfree/5-800.webp 800w,/assets/img/2024-08-24-Colmapfree/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-24-Colmapfree/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>COLMAP + 3DGS와 비교했을 때<br/> 본 논문과 동일한 성능</p> </li> <li>우리는 pose estimation을 할 때 photometric loss에만 의존했음 <ul> <li>photometric loss에만 의존해서 relative camera pose를 구했기 때문에 \(RPE_r, RPE_t\) 값이 Nope-NeRF보다 조금 높게 나타날 수 있음</li> <li>Nope-NeRF에서는 chamfer distance(point cloud 집합인 \(P_i\) 와 \(P_j\) 가 서로 가까워지도록 하는 point cloud loss) 추가하여 pose accuracy 높임</li> </ul> </li> <li>Nope-NeRF에서와 달리 본 논문에서 depth loss를 쓰면 pose accuracy는 비슷하고 novel view synthesis performance는 오히려 떨어지므로 depth loss는 안 씀</li> </ul> <h2 id="limitation">Limitation</h2> <ul> <li>camera pose와 3DGS를 동시에 순차적으로 optimize하므로<br/> video stream 혹은 ordered image 집합에만 적용 가능<br/> \(\rightarrow\) unordered image 집합에도 적용하는 future work 필요</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="COLMAP"/><category term="SfM"/><category term="GS"/><category term="depth"/><category term="pose"/><category term="rendering"/><category term="3d"/><summary type="html"><![CDATA[COLMAP-Free 3D Gaussian Splatting (CVPR 2024)]]></summary></entry><entry><title type="html">Mip-NeRF 360</title><link href="https://semyeong-yu.github.io/blog/2024/MipNeRF360/" rel="alternate" type="text/html" title="Mip-NeRF 360"/><published>2024-08-11T01:03:00+00:00</published><updated>2024-08-11T01:03:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/MipNeRF360</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/MipNeRF360/"><![CDATA[<h2 id="mip-nerf-360-unbounded-anti-aliased-neural-radiance-fields">Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields</h2> <h4 id="jonathan-t-barron-ben-mildenhall-dor-verbin-pratul-p-srinivasan-peter-hedman">Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, Peter Hedman</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2111.12077">https://arxiv.org/abs/2111.12077</a><br/> project website :<br/> <a href="https://jonbarron.info/mipnerf360/">https://jonbarron.info/mipnerf360/</a><br/> pytorch code :<br/> <a href="https://github.com/google-research/multinerf">https://github.com/google-research/multinerf</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>sampling 기법 개선하고, bounded scene으로 warp</li> <li>non-linear scene, ray parameterization :<br/> disparity에 비례하도록 sampling 개선하고<br/> bounded space로 mapping하여<br/> 임의의 방향과 깊이에 대한 unbounded scene 다룸</li> <li>efficient proposal-based online-distillation :<br/> higher capacity MLP을 조금만 evaluate해서<br/> 효율적으로 large scene 다룸</li> <li>interval-distortion-based regularizer :<br/> artifacts 줄이기 위해<br/> step-function을 delta-function에 가깝게 regularize</li> </ol> </blockquote> <h2 id="introduction">Introduction</h2> <ul> <li>임의의 <code class="language-plaintext highlighter-rouge">direction</code>(360 degrees)과 임의의 <code class="language-plaintext highlighter-rouge">depth</code>로 <code class="language-plaintext highlighter-rouge">unbounded</code> 되어있는 scene 문제 해결 <ul> <li>non-linear scene parameterization :<br/> sampling 개선하고 bounded space로 mapping하여<br/> 임의의 방향과 깊이에 대한 unbounded scene 다룰 수 있음</li> <li>online-distillation :<br/> higher capacity MLP을 조금만 evaluate해서 효율적으로 large scene 다룰 수 있음</li> <li>distortion-based regularizer :<br/> artifacts 줄이기 위한 regularization</li> </ul> </li> <li>NeRF model을 large unbounded scene에 적용하는 데 3가지 문제가 있다<br/> (자세한 내용은 스킵했는데 나중에 읽어보자) <ul> <li>Parametrization : Mip-NeRF는 3D coordinate가 bounded domain 안에 있는 경우만 처리 가능</li> <li>Efficiency : large-and-detailed scene은 large MLP를 필요로 해서 expensive</li> <li>Ambiguity : scene content가 임의의 distance에 있고 이는 only 적은 수의 ray로 관찰되기 때문에 inherent ambiguity 발생</li> </ul> </li> </ul> <h2 id="scene-and-ray-parameterization">Scene and Ray Parameterization</h2> <h3 id="ray-interval-parameterization">Ray Interval Parameterization</h3> <ul> <li> <p>Ray Interval Parameterization :<br/> samples의 경우 distance가 아니라 그의 역수인 <code class="language-plaintext highlighter-rouge">disparity에 비례</code>하여 분포하도록 하면<br/> 가까이 있는 content는 많이 sampling하고 멀리 있는 content는 덜 sampling함으로써<br/> <code class="language-plaintext highlighter-rouge">임의의 scale의 unbounded scene</code>을 잘 다룰 수 있음</p> </li> <li>NeRF : <ul> <li>NeRF에서는 distance에 비례하여 stratified uniform sampling 했음</li> <li>만약 NDC parameterization을 쓴다면<br/> NDC-space에서 distance에 비례하여 stratified uniform sampling 하면<br/> disparity (distance의 역수)에 비례하여 uniform sampling 한 것과 같은 효과를 가짐<br/> 그 이유는 <a href="https://semyeong-yu.github.io/blog/2024/NDC/">Normalized-Device-Coordinates</a> 의 Linear in Disparity 파트 참고</li> <li>그런데 NDC는 single direction으로만 unbounded된 scene (front-facing camera)에 대해서만 적합하고<br/> 모든 방향으로 unbounded된 scene에 대해서는 적합하지 않음</li> </ul> </li> <li>Mip-NeRF 360 : <ul> <li>처음부터 ray interval을 disparity (distance의 역수)에 비례하도록 <d-cite key="LLFF">[2]</d-cite> parameterize 한다</li> </ul> </li> </ul> <h3 id="ray-interval-parameterization-in-disparity">Ray Interval Parameterization in Disparity</h3> <ul> <li>distance along ray를 t-space 또는 s-space에서 나타내자 <ul> <li>t-space :<br/> Euclidean ray distance \(t \in [t_n, t_f]\)<br/> \(t = g^{-1}(s \cdot g(t_f) + (1-s) \cdot g(t_n))\)</li> <li>s-space :<br/> normalized ray distance \(s \in [0, 1]\)<br/> \(s = \frac{g(t)-g(t_n)}{g(t_f)-g(t_n)}\)</li> </ul> </li> <li>사용 예시 : <ul> <li>\(g(x) = \frac{1}{x}\) 로 설정할 경우<br/> <code class="language-plaintext highlighter-rouge">s-space에서 uniform sampling</code>하면<br/> <code class="language-plaintext highlighter-rouge">t-space에서 disparity에 비례</code>하여 distributed</li> <li>\(g(x) = log(x)\) 로 설정할 경우<br/> s-space에서 uniform sampling하면<br/> t-space에서는 logarithmic spacing <d-cite key="DONeRF">[3]</d-cite> 으로 distributed</li> </ul> </li> <li>기존 NeRF 모델에서는 t-distance를 따라 uniform sampling했지만<br/> 본 논문에서는 <code class="language-plaintext highlighter-rouge">s-distance</code>를 따라 uniform sampling하여 나타낸다</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/4-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/4-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="scene-parameterization">Scene Parameterization</h3> <ul> <li> <p>Scene Parameterization :<br/> unbounded scene을 radius-2 내부의 <code class="language-plaintext highlighter-rouge">bounded space</code>로 mapping하기 위해 <code class="language-plaintext highlighter-rouge">contract 함수</code>를 사용<br/> ray parameterization을 할 때 disparity에 비례하게 sampling 했으므로<br/> contract 함수도 consistently <code class="language-plaintext highlighter-rouge">disparity에 비례</code>하게 bounded space로 mapping<br/> \(\rightarrow\) scene origin에서 cast된 ray의 경우 contract 함수를 적용한 후에는 아래 그림의 주황색 영역에서 일정한 길이의 interval을 가진다</p> </li> <li>Define smooth coordinate-transformation function as \(f(x) : R^3 \rightarrow R^3\) <ul> <li>\(\mu, \Sigma\) 를 갖는 Gaussian에 non-linear \(f\) 를 적용하여 \(\mu_{c}, \Sigma_{c}\) 를 갖는 Gaussian으로 변형하려면<br/> \((\mu_{c}, \Sigma_{c}) = f(\mu, \Sigma) = (f(\mu), J_{f}(\mu) \Sigma J_{f}(\mu)^T)\)<br/> where \(f(x) \approx f(\mu) + J_{f}(\mu)(x-\mu)\) (linear approx.)</li> <li>이는 state transition model \(f = \text{contract}(x) = \begin{cases} x &amp; \text{if} \| x \| \leq 1 \\ (2 - \frac{1}{\| x \|})(\frac{x}{\| x \|}) &amp; \text{if} \| x \| \gt 1 \end{cases}\) 을 사용했을 때<br/> classic Extended Kalman filter <d-cite key="kalman">[1]</d-cite> 와 수학적으로 동일</li> <li>MipNeRF360에서는 contract 함수를<br/> <code class="language-plaintext highlighter-rouge">point가 아니라</code> Euclidean 3D-space에 있는 <code class="language-plaintext highlighter-rouge">Gaussian</code>에 적용!<br/> 또한<br/> <code class="language-plaintext highlighter-rouge">모든 방향</code> (360 degress)에 대해 적용!</li> </ul> </li> <li>IPE (integrated positional encoding) : <ul> <li>Mip-NeRF :<br/> \(\gamma (\mu, \Sigma) = \left[ \begin{bmatrix} sin(2^l \mu) \circledast exp(-\frac{1}{2} 4^l diag(\Sigma)) \\ cos(2^l \mu) \circledast exp(-\frac{1}{2} 4^l diag(\Sigma)) \end{bmatrix} \right]_{l=0}^{l=L-1}\)</li> <li>Mip-NeRF 360 :<br/> \(\gamma (\text{contract}(\mu, \Sigma))\)<br/> where<br/> \(f(x) = \text{contract}(x) = \begin{cases} x &amp; \text{if} \| x \| \leq 1 \\ (2 - \frac{1}{\| x \|})(\frac{x}{\| x \|}) &amp; \text{if} \| x \| \gt 1 \end{cases}\)<br/> and \(f(x) \approx f(\mu) + J_{f}(\mu)(x-\mu)\)<br/> and \(f(\mu, \Sigma) = (f(\mu), J_{f}(\mu) \Sigma J_{f}(\mu)^T)\)</li> <li>Mip-NeRF 360 procedure :<br/> casting cone<br/> \(\rightarrow\) uniform sampling in s-space<br/> \(\rightarrow\) contract 3D Gaussians in t-space into bounded sphere<br/> \(\rightarrow\) IPE \(\gamma\)<br/> \(\rightarrow\) MLP</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/1-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/1-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> contract 함수는 파란색 구(radius 1) 외부의 Gaussian(회색)을 주황색 영역(radius 1 ~ 2)의 Gaussian(빨간색)으로 mapping </div> <h2 id="coarse-to-fine-online-distillation">Coarse-to-Fine Online Distillation</h2> <ul> <li>기존 NeRF :<br/> coarse-MLP와 fine-MLP</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/2-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/2-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 위의 그림은 Mip-NeRF, 아래의 그림은 Mip-NeRF 360 </div> <ul> <li>Mip-NeRF 360 :<br/> proposal-MLP와 NeRF-MLP <ul> <li><code class="language-plaintext highlighter-rouge">small</code> proposal-MLP는 <code class="language-plaintext highlighter-rouge">many</code> samples로 <code class="language-plaintext highlighter-rouge">여러 번</code> evaluate하고,<br/> <code class="language-plaintext highlighter-rouge">large</code> NeRF-MLP는 <code class="language-plaintext highlighter-rouge">less</code> samples로 <code class="language-plaintext highlighter-rouge">딱 한 번</code> evaluate함으로써<br/> Mip-NeRF보다 조금만 더 costly하지만 훨씬 더 <code class="language-plaintext highlighter-rouge">higher capacity</code>를 가진 것과 같은 효과<br/> \(\rightarrow\) 효율적으로 <code class="language-plaintext highlighter-rouge">large unbounded scene</code>을 표현하기에 적절<br/> distill 효과가 좋아서 proposal-MLP의 경우 크기 줄이더라도 accuracy 감소하지 않음</li> <li>small proposal-MLP : <ul> <li>color 말고 volume density만 예측하여 weight \(\hat w\) 구함</li> </ul> </li> <li>large NeRF-MLP : <ul> <li>color, volume density 예측하여 weight \(w\) 구하고 rendering</li> </ul> </li> </ul> </li> <li>Loss :<br/> 아래 두 가지 loss로 각 MLP를 jointly train <ul> <li><code class="language-plaintext highlighter-rouge">reconstruction loss</code> : <ul> <li>large NeRF-MLP에서 rendering해서 구함<br/> 기존 NeRF 방식과 동일</li> <li><code class="language-plaintext highlighter-rouge">GT-image를 supervision</code>으로 하여 <code class="language-plaintext highlighter-rouge">NeRF-MLP만 업데이트</code></li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">proposal loss</code> : <ul> <li>두 MLP의 <code class="language-plaintext highlighter-rouge">weight histogram이 consistent</code>하도록 함<br/> 즉, proposal-MLP의 weight histogram이 NeRF-MLP의 weight histogram을 따라잡도록 함<br/> (Mip-NeRF 계열은 point가 아니라 interval 별로 weight를 구하므로 histogram을 만들 수 있음)</li> <li><code class="language-plaintext highlighter-rouge">NeRF-MLP의 weight를 supervision</code>으로 하여 <code class="language-plaintext highlighter-rouge">proposal-MLP만 업데이트</code><br/> (<code class="language-plaintext highlighter-rouge">online distillation</code> of NeRF-MLP’s knowledge into proposal-MLP)</li> <li>문제 :<br/> 하나의 histogram bin의 distribution에 대해 아무 것도 가정할 수 없음<br/> (하나의 bin의 distribution이 uniform일 수도 있고 특정 지점에 몰빵된 delta function일 수도 있음…)<br/> coarse \(\hat t\) 와 fine \(t\) (bins)가 매우 다를 수 있음</li> <li>가정 :<br/> 두 개의 histogram이 매우 달라보이더라도<br/> 둘 다 <code class="language-plaintext highlighter-rouge">어떤 하나의 동일한 (underlying continuous) true mass distribution으로부터 유래되었다고 설명할 수 있다면</code> 둘의 차이인 loss는 0 이다</li> <li>위의 가정에 따라<br/> NeRF-MLP (\(t\), \(w\))의 구간 \(T\) 와 겹치는 모든 proposal-MLP의 weight \(\hat w_{j}\) 를 더해서 아래와 같이 NeRF-MLP weight \(w\) 의 <code class="language-plaintext highlighter-rouge">upper bound</code>를 구하자<br/> \(\text{bound}(\hat t, \hat w, T) = \sum_{j: T \cap \hat T_{j} \neq \emptyset} \hat w_{j}\)<br/> (\(t\) 와 \(\hat t\) 가 정렬되어 있으므로 summed-area table로 효율적으로 계산 가능)</li> <li>만약 두 개의 histogram이 consistent하다면,<br/> NeRF-MLP (\(t\), \(w\))의 모든 구간 (\(T_i, w_i\))에 대해<br/> \(w_i \leq \text{bound}(\hat t, \hat w, T_i)\) 이어야 한다<br/> \(\rightarrow\)<br/> 아래와 같이 <code class="language-plaintext highlighter-rouge">proposal loss는 이를 위반하는 경우</code>에 해당한다<br/> \(L_{prop}(t, w, \hat t, \hat w) = \sum_{i}\frac{1}{w_i} \text{max}(0, w_i - \text{bound}(\hat t, \hat w, T_i))^2\)</li> <li>즉, <code class="language-plaintext highlighter-rouge">proposal-MLP가 NeRF-MLP의 upper-bound를 형성</code>하도록 한다는 것은<br/> proposal-MLP가 NeRF-MLP histogram의 개형을 <code class="language-plaintext highlighter-rouge">따라잡도록</code> 하는 효과!</li> <li>proposal loss가 asymmetirc loss인 이유 :<br/> proposal-MLP가 NeRF-MLP보다 coarse하기 때문에<br/> proposal-MLP weight가 NeRF-MLP weight의 upper bound를 형성하는 게 (overestimate) 당연하고,<br/> proposal-MLP weight가 NeRF-MLP weight를 underestimate (\(\text{bound}(\hat t, \hat w, T_i) &lt; w_i\)) 하는 경우에만 penalize</li> <li>proposal loss term에서 \(w_i\) 로 나누는 이유 :<br/> bound가 0일 때 \(\frac{dL_{prop}}{d\text{bound}} = \sum_{i} \frac{1}{w_i} \cdot 2 \cdot \text{max}(0, w_i - \text{bound}) \cdot (-1) = -2\sum_{i}1\) 와 같이<br/> \(w_i\) 크기와 상관없이 <code class="language-plaintext highlighter-rouge">gradient 값이 상수값</code>이 되어 균등하게 penalize하여 optimization에 도움됨</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/8-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/8-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 위의 histogram이 NeRF-MLP, 아래의 histogram이 proposal-MLP, 보통 proposal-MLP가 coarse하고 NeRF-MLP가 fine한데 여기선 반대로 그려져 있음 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/3-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/3-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> fine NeRF-MLP는 점점 scene content의 surface 쪽으로 weight가 집중되고, coarse proposal-MLP는 이를 따라잡으며 upper bound를 형성 </div> <h2 id="regularization-for-interval-based-models">Regularization for Interval-Based Models</h2> <ul> <li>Artifacts :<br/> NeRF 계열은 pose 문제 때문에 두 가지 주된 artifacts가 나타난다 <ul> <li><code class="language-plaintext highlighter-rouge">floater</code> :<br/> 특정 view를 너무 잘 설명하려던 나머지<br/> 실제로 물체가 존재하지 않는 small disconnected regions of dense volume에서 불필요하게 opacity를 예측하여<br/> 다른 view에서 보면 반투명한 blurry cloud처럼 보이는 부분</li> <li><code class="language-plaintext highlighter-rouge">background collapse</code> :<br/> 멀리 있는 surface가<br/> 반투명한 가까운 content로 잘못 모델링된 경우</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/5-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/5-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 반투명하게 떠다니는 게 floater, 좌하단에서 background surface가 가깝게 보이는 게 background collapse </div> <ul> <li>Artifacts 완화 : <ul> <li>기존 NeRF : <code class="language-plaintext highlighter-rouge">random noise</code><br/> <a href="https://semyeong-yu.github.io/blog/2024/NeRFcode/">NeRF-Code</a> 의 raw2outputs()에서 볼 수 있듯이<br/> raw-opacity에 random noise 더해서 \(\sigma_{i}\) 구함<br/> noise 덕분에 <code class="language-plaintext highlighter-rouge">불필요한 특정 지점에 overfit 되는 게 아니라 일관성 있게</code> 학습<br/> But, 부분적으로 artifacts 완화하고 reconstruction quality를 떨어뜨림</li> <li>Mip-NeRF 360 : <code class="language-plaintext highlighter-rouge">regularize</code><br/> ray-sampling은 이미 했고 weight를 구할 때<br/> <code class="language-plaintext highlighter-rouge">물체가 있을만한 정확한 지점에서 집중적으로 예측</code>하여<br/> <code class="language-plaintext highlighter-rouge">부정확한 지점에서의 불필요한 예측을 억제</code></li> </ul> </li> <li>Regularization for Interval-Based Models : <ul> <li><code class="language-plaintext highlighter-rouge">distortion loss</code> :<br/> step-function인 weight-histogram \(s, w\) 을 regularize하기 위해<br/> \(L_{dist}(s, w) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} w_s(u)w_s(v)|u-v|d_ud_v\) <ul> <li><code class="language-plaintext highlighter-rouge">NeRF-MLP 업데이트</code>할 때 artifacts 완화(<code class="language-plaintext highlighter-rouge">regularization</code>)하는 역할</li> </ul> </li> <li>위의 loss를 최소화하기 위해선<br/> \(w\) 를 매우 작은 \(|u-v|\) 에 몰빵하면 된다<br/> 즉, 위의 loss term만 있을 경우 histogram(step-function)이 <code class="language-plaintext highlighter-rouge">delta-function</code>에 가까워지면 된다</li> <li>t-distance 대신 <code class="language-plaintext highlighter-rouge">s-distance</code> 사용 :<br/> t-distance 사용하면 먼 거리에 있는 interval 길이가 길기 때문에 무조건 먼 거리의 interval 쪽으로 distortion 됨<br/> s-distance 기준으로 weight-histogram 만들어서 distortion loss 구하자!</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/6-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/6-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">distortion loss</code> : <ul> <li>weight \(w\) 는 step-function (각 interval 안에선 constant) 이므로<br/> 아래와 같이 계산하기 쉬운 꼴로 유도할 수 있음<br/> \(L_{dist}(s, w) = \sum_{i, j} w_i w_j |\frac{s_i + s_{i+1}}{2} - \frac{s_j + s_{j+1}}{2}| + \frac{1}{3} \sum_{i} w_i^2 (s_{i+1} - s_i)\)</li> <li>유도 과정 :<br/> 출처 : https://charlieppark.kr/ <ul> <li>\(L_{dist}(s, w) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} w_s(u)w_s(v)|u-v|d_ud_v\)<br/> where \(w_s(u) = w_i\) for \(u \in [s_i, s_{i+1})\)</li> <li>case 1. \(u, v\) are in the same interval : \(u, v \in [s_i, s_{i+1})\)<br/> \(\int_{s_i}^{s_{i+1}}\int_{s_i}^{s_{i+1}}w_i^2|u-v|d_ud_v\)<br/> \(= w_i^2 \frac{(s_{i+1}-s_i)^3}{3}\)</li> <li>case 2. \(u, v\) are in different intervals : \(u \in [s_i, s_{i+1}), v \in [s_j, s_{j+1})\) where \(i \neq j\)<br/> \(\int_{s_i}^{s_{i+1}}\int_{s_j}^{s_{j+1}}w_iw_j|u-v|d_ud_v\)<br/> \(\simeq \int_{s_i}^{s_{i+1}}\int_{s_j}^{s_{j+1}}w_iw_j|\frac{s_i + s_{i+1}}{2} - \frac{s_j + s_{j+1}}{2}|d_ud_v\)<br/> \(= w_iw_j|\frac{s_i + s_{i+1}}{2} - \frac{s_j + s_{j+1}}{2}|\cdot (s_{i+1}-s_i)(s_{j+1}-s_j)\)</li> <li> \[L_{dist}(s, w) = \sum_{i} w_i^2 \frac{(s_{i+1}-s_i)^3}{3} + \sum_{i, j} w_iw_j|\frac{s_i + s_{i+1}}{2} - \frac{s_j + s_{j+1}}{2}|\cdot (s_{i+1}-s_i)(s_{j+1}-s_j)\] </li> <li>\((s_{i+1} - s_i)^2\) 항과 \((s_{i+1}-s_i)(s_{j+1}-s_j)\) 항을 제거하여 학습의 안정성을 높임<br/> \(L_{dist}(s, w) = \frac{1}{3} \sum_{i} w_i^2 (s_{i+1} - s_i) + \sum_{i, j} w_i w_j |\frac{s_i + s_{i+1}}{2} - \frac{s_j + s_{j+1}}{2}|\)</li> <li>\(u, v\) 가 <code class="language-plaintext highlighter-rouge">same interval</code>에 있을 경우에는 \((s_{i+1}-s_i)\) 항으로 <code class="language-plaintext highlighter-rouge">각 구간의 (weighted) 너비</code>를 줄이고,<br/> \(u, v\) 가 <code class="language-plaintext highlighter-rouge">different interval</code>에 있을 경우에는 \(|\frac{s_i + s_{i+1}}{2} - \frac{s_j + s_{j+1}}{2}|\) 항으로 <code class="language-plaintext highlighter-rouge">두 구간 사이의 (weighted) 중심 거리</code>를 줄임<br/> 이 원리를 통해<br/> entire ray is unoccupied이 가능하다면 모든 weight가 0에 가까워지려 하고<br/> 불가능하다면 <code class="language-plaintext highlighter-rouge">weight를 few interval에 몰빵</code>하려 해서<br/> weight-histogram이 delta-function에 가까워질 수 있음</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/7-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/7-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="optimization">Optimization</h2> <ul> <li>Setting : <ul> <li>proposal-MLPs with 4 layers and 256 hidden_dim<br/> two proposal-MLP \((\hat s^0, \hat w^0)\) and \((\hat s^1, \hat w^1)\) each using 64 samples</li> <li>NeRF-MLP \((s, w)\) with 8 layers and 1024 hidden_dim<br/> one NeRF-MLP using 32 samples<br/> (NeRF와 Mip-NeRF에서는 MLP with 8 layers and 256 hidden_dim 사용했었음)</li> <li>total loss :<br/> \(L_{tot} = L_{recon}(C(t), C^{\ast}) + \lambda L_{dist}(s, w) + \sum_{k=0}^{1}L_{prop}(s, w, \hat s^k, \hat w^k)\)<br/> averaged over all rays in batch<br/> where author sets \(\lambda = 0.01\)</li> <li>\(L_{recon}\) and \(L_{dist}\) for NeRF-MLP <ul> <li>\(L_{recon}(x, x^{\ast}) = \sqrt{(x - x^{\ast})^2 + \epsilon^{2}}\) : Charbonnier loss<br/> slightly more stable than MSE</li> </ul> </li> <li>\(L_{prop}\) for proposal-MLP</li> <li>learning schedule :<br/> 250k iter. with batch size \(2^{14}\)<br/> Adam optimizer with \(\beta_{1} = 0.9, \beta_{2} = 0.999, \epsilon = 10^{-6}\)<br/> lr is annealed log-linearly from \(2 \times 10^{-3}\) to \(2 \times 10^{-5}\)<br/> warm-up phase of 512 iter.<br/> gradient clipping to norm of \(10^{-3}\)</li> </ul> </li> </ul> <h2 id="conclusion">Conclusion</h2> <ul> <li>Mip-NeRF extension for real-world unbounded scenes with unconstrained camera depth and orientations</li> <li>(Kalman-like) scene and ray parameterization</li> <li>efficient proposal-based coarse-to-fine distillation framework</li> <li>interval-distortion-based regularizer</li> <li>Mip-NeRF에 비해 57% reduction in MSE</li> </ul> <h2 id="question">Question</h2> <ul> <li>Q&amp;A reference : 3DGS online study</li> <li>Q1 : 아래의 문구가 이해되지 않습니다<br/> recall that the “bins” of those histograms \(t\) and \(\hat t\) need not be similar; indeed, if the proposal MLP successfully culls the set of distances where scene content exists, \(\hat t\) and \(t\) will be highly dissimilar</li> <li>A1 : 아래 사진의 (c)에서처럼 충분히 optimize되어 만약 coarse proposal-MLP가 이미 scene content가 있는 곳을 성공적으로 예측하고 있다면 이를 이용한 fine NeRF-MLP의 fine-samples는 그 곳에 더 촘촘히 존재할 것이므로 bin 간격이 달라져서 두 histogram이 크게 달라보인다<br/> 달라보이더라도 두 개의 histogram이 어떤 하나의 (true continuous underlying) mass distribution에서 유래되었다고 설명할 수 있으면 둘의 차이가 0이라고 가정하여 upper bound를 이용해서 proposal loss 만듬</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/3-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/3-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Q2 : 갑자기 든 생각인데 Mip-NeRF 360의 sampling 기법과 contract 함수가 background collapse의 원인이 될 수도 있지 않을까요?<br/> disparity에 비례하게 sampling하므로 먼 거리에 대해서는 덜 sampling한 채로 bounded space로 warp하는데,<br/> 먼 거리의 content에 대해 정보가 부족한 채로 warp하는 과정에서 왜곡이 일어날 수 있을 것 같다</li> <li>A2 : 그럴 수 있을 것 같습니다</li> </ul> <h2 id="code-review">Code Review</h2> <p>TBD</p> <h2 id="appendix">Appendix</h2> <h3 id="off-axis-positional-encoding">Off-Axis Positional Encoding</h3> <ul> <li> <p>Mip-NeRF의 IPE 식 :<br/> PE-basis를 identity matrix로 설정하였으므로<br/> \(P = \begin{pmatrix} \begin{matrix} 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2\end{matrix} &amp; \cdots &amp; \begin{matrix} 2^{L-1} &amp; 0 &amp; 0 \\ 0 &amp; 2^{L-1} &amp; 0 \\ 0 &amp; 0 &amp; 2^{L-1}\end{matrix} \end{pmatrix}^T\)<br/> \(diag(\Sigma_{r})\) 계산을 위해 \(diag(\Sigma)\) 만 알면 됨</p> </li> <li> <p>Mip-NeRF 360의 IPE 식 :<br/> IPE를 하기 전에 우선 Gaussian을 radius-2의 구 안으로 contract 해야 해서<br/> <code class="language-plaintext highlighter-rouge">어차피 full covariance matrix</code> \(\Sigma\) 가 필요하므로<br/> PE-basis를 아래와 같이 둔다<br/> (\(P\) 의 각 column이 twice-tessellated icosahedron(두 번 테셀레이션 된 정이십면체)의 unit-norm vertex이고, 음의 같은 방향으로 중복된 vertex는 제거)<br/> \(P = \begin{bmatrix} 0.8506508 &amp; 0 &amp; 0.5257311 \\ 0.809017 &amp; 0.5 &amp; 0.309017 \\ 0.5257311 &amp; 0.8506508 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0.809017 &amp; 0.5 &amp; -0.309017 \\ 0.8506508 &amp; 0 &amp; -0.5257311 \\ 0.309017 &amp; 0.809017 &amp; -0.5 \\ 0 &amp; 0.5257311 &amp; -0.8506508 \\ 0.5 &amp; 0.309017 &amp; -0.809017 \\ 0 &amp; 1 &amp; 0 \\ -0.5257311 &amp; 0.8506508 &amp; 0 \\ -0.309017 &amp; 0.809017 &amp; -0.5 \\ 0 &amp; 0.5257311 &amp; 0.8506508 \\ -0.309017 &amp; 0.809017 &amp; 0.5 \\ 0.309017 &amp; 0.809017 &amp; 0.5 \\ 0.5 &amp; 0.309017 &amp; 0.809017 \\ 0.5 &amp; -0.309017 &amp; 0.809017 \\ 0 &amp; 1 &amp; 0 \\ -0.5 &amp; 0.309017 &amp; 0.809017 \\ -0.809017 &amp; 0.5 &amp; 0.309017 \\ -0.809017 &amp; 0.5 &amp; -0.309017 \end{bmatrix}\)</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/9-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/9-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> column of PE-basis P </div> <ul> <li>Mip-NeRF 360의 IPE 식 : <ul> <li><code class="language-plaintext highlighter-rouge">off-axis feature</code>를 쓰면 <code class="language-plaintext highlighter-rouge">anisotropic Gaussian의 모양</code>까지 잘 encode할 수 있어서 rendering quality 향상</li> <li>IPE 식에서 \(diag(\Sigma_{r}) = diag(P \Sigma P^T)\) 를 계산할 때<br/> large \(P\) matrix에 대해 행렬곱을 하려면 너무 비싸므로<br/> \(diag(P \Sigma P^T)\) 대신 \(\text{sum}(P^T \circledast (\Sigma P^T), \text{dim}=0, \text{keepdim=False})\) 로 계산하면<br/> Mip-NeRF의 Axis-Aligned IPE보다 Mip-NeRF 360의 Off-Axis IPE가 살짝만 더 expensive</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/10-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/10-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 왼쪽은 Mip-NeRF의 Axis-Aligned IPE이고, 오른쪽은 Mip-NeRF 360의 Off-Axis IPE, 각 encoding basis로 Gaussian을 projection해서 marginal distribution을 구했을 때 Off-Axis Projection을 해야 Gaussian shape를 더 잘 구분할 수 있음 </div> <h3 id="annealing-weight">Annealing Weight</h3> <ul> <li> <p>proposal-weight PDF로 fine-sampling할 때<br/> proposal-weight \(\hat w\) 대신 \(\hat w_n \propto \hat w^{\frac{bn/N}{(b-1)n/N+1}}\)<br/> where \(n\) iter. out of \(N\) steps<br/> where 지수부분 \(\frac{bn/N}{(b-1)n/N+1}\) 은 0부터 1까지 빠르게 증가<br/> where bias hyper-param. \(b=10\)</p> </li> <li> <p>\(n=0\) 일 때는 \(\hat w_0 \propto 1\) 을 따라 sampling하고<br/> \(n=N\) 일 때는 \(\hat w_N \propto \hat w\) 을 따라 sampling하므로<br/> <code class="language-plaintext highlighter-rouge">훈련 초기</code> 단계에서 NeRF-MLP가 더 다양한 proposal-interval 범위를 <code class="language-plaintext highlighter-rouge">모험</code>(exploration)할 수 있도록 함</p> </li> </ul> <h3 id="dilation">Dilation</h3> <ul> <li> <p>proposal-weight PDF로 fine-sampling할 때<br/> aliasing-artifacts 줄이기 위해 먼저 proposal-histogram \((\hat t, \hat w)\) 을 dilate</p> </li> <li> <p>이유 :<br/> proposal-MLP는 오직 input ray로만 supervised되므로<br/> 특정 각도에 대해서만 예측 가능할 수 있다<br/> (<code class="language-plaintext highlighter-rouge">proposal-MLP는 rotationally aliased</code>)</p> </li> <li> <p>해결 : dilation<br/> proposal-MLP의 <code class="language-plaintext highlighter-rouge">interval을 넓힘</code>으로써 anti-aliasing<br/> <code class="language-plaintext highlighter-rouge">왜 interval 넓히면 anti-aliasing??? weight 변화가 급격하지 않아서???</code></p> <ul> <li>Step 1) proposal-MLP의 histogram \((\hat s, \hat w)\) 에서 weight를 구간 길이로 나눈 뒤 normalize해서 probability density \(\hat p\) 로 만듬<br/> \(\hat p_i = \frac{\hat w_i}{(\hat s_{i+1} - \hat s_{i})}\)</li> <li>Step 2) dilation factor (얼만큼 각 구간을 넓힐지) 계산<br/> fine intervals 개수가 많을수록 평균적인 구간 길이가 짧아지고 dilation factor가 작아짐<br/> \(\epsilon = \frac{a}{\prod_{k'=1}^{k-1} n_{k'}} + b\)<br/> where \(k\) 번째 coarse-to-fine resampling 단계에서 \(n_k\) 개의 fine intervals을 resample</li> <li>Step 3) 각 구간 \([\hat s_{i}, \hat s_{i+1}]\) 를 \([\hat s_{i} - \epsilon, \hat s_{i+1} + \epsilon]\) 로 확장</li> <li>Step 4) 확장한 각 구간마다 probability density 최대값을 구함<br/> \(\text{max}_{\hat s - \epsilon \leq s \lt \hat s + \epsilon} \hat p_{\hat s} (s)\)<br/> where \(\hat p_{\hat s} (s)\) is interpolation into the step function defined by \(\hat s, \hat p\) at \(s\)</li> <li>Step 5) 다시 구간 길이로 곱한 뒤 normalize해서 weight-histogram으로 만듬</li> </ul> </li> </ul> <h3 id="sampling">Sampling</h3> <ul> <li>Mip-NeRF : <ul> <li>sampling 방식 :<br/> coarse-histogram으로부터 \(n+1\) 개의 t-distance를 sampling한 뒤<br/> 각 fine-samples를 끝점으로 하여 \(n\) 개의 fine-intervals을 얻음</li> <li>단점 :<br/> 아래 그림과 같이 samples가 coarse-histogram 각 구간의 전범위를 전부 span하지 못하여<br/> 일부 구간이 sampling에서 제외되므로(충분히 다뤄지지 않으므로)<br/> coarse-histogram의 영향을 서서히 약화시킴</li> </ul> </li> <li>Mip-NeRF 360 : <ul> <li>sampling 방식 :<br/> coarse-histogram으로부터 \(n\) 개의 s-distance를 sampling한 뒤<br/> 각 구간의 mid-points \(n-1\) 개와 <code class="language-plaintext highlighter-rouge">coarse-sample의 양끝 점 2개</code>를 끝점으로 하여 \(n\) 개의 fine-intervals를 얻음</li> <li>효과 :<br/> samples가 coarse-histogram의 처음과 끝 구간도 전부 span하고<br/> 불규칙한 resampling도 감소하여<br/> rendering quality는 큰 변화 없지만 aliasing 줄이는 데 도움됨</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/11-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/11-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> sampling한 건 막대로 표시하였고, 이로부터 8개의 fine-intervals를 얻음 </div> <h3 id="background-colors">Background Colors</h3> <ul> <li>NeRF와 Mip-NeRF : <ul> <li>훈련시킬 때 background color를 <code class="language-plaintext highlighter-rouge">black or white</code>로 설정하는데<br/> scene recon.할 때 background가 <code class="language-plaintext highlighter-rouge">반투명</code>하게 reconstruct될 수도 있다</li> <li>문제 :<br/> 반투명한 background는<br/> view synthesis 자체에는 문제가 없지만<br/> ray-termination-distance가 부정확하여<br/> <code class="language-plaintext highlighter-rouge">부정확한 depth-map</code>을 생성할 수 있다</li> </ul> </li> <li>Mip-NeRF 360 : <ul> <li>Blender dataset :<br/> 이전과 똑같이 black or white로 background color 설정</li> <li>360 and LLFF dataset :<br/> 훈련시킬 때 background color를 \([0, 1]^3\) 사이의 <code class="language-plaintext highlighter-rouge">random color</code>로 설정하여<br/> scene recon.할 때 fully-opaque background이도록 함</li> </ul> </li> </ul> <h3 id="implementation-details">Implementation Details</h3> <ul> <li>\(\mu, \Sigma\) 를 갖는 Gaussian에 non-linear \(f\) 를 적용하여 \(\mu_{c}, \Sigma_{c}\) 를 갖는 Gaussian으로 변형하기 위해<br/> \((\mu_{c}, \Sigma_{c}) = (f(\mu), J_{f}(\mu) \Sigma J_{f}(\mu)^T)\)<br/> where \(f(x) \approx f(\mu) + J_{f}(\mu)(x-\mu)\) (linear approx.)</li> <li>이 때, \(J_{f}(\mu)\) matrix는 autodiff framework로 직접 계산할 수 있지만<br/> 사실 우리는 직접 explicitly matrix를 만들 필요가 없음</li> <li><code class="language-plaintext highlighter-rouge">less expensive 계산</code> 위해<br/> \(J_{f}(\mu)\)와 행렬곱하는 것과 똑같은 역할을 하는 함수를 이용<br/> e.g. \(J_{f}(\mu) \Sigma J_{f}(\mu)^T\) 계산 위해<br/> <code class="language-plaintext highlighter-rouge">Jax의 linearize</code> operator <d-cite key="Jax">[4]</d-cite>를 두 번 적용</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="rendering"/><category term="3d"/><category term="multiscale"/><category term="antialiasing"/><summary type="html"><![CDATA[Unbounded Anti-Aliased Neural Radiance Fields]]></summary></entry></feed>