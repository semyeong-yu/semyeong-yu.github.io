<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://semyeong-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://semyeong-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-30T08:02:40+00:00</updated><id>https://semyeong-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">CUDA Programming</title><link href="https://semyeong-yu.github.io/blog/2024/CUDA/" rel="alternate" type="text/html" title="CUDA Programming"/><published>2024-09-21T12:00:00+00:00</published><updated>2024-09-21T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/CUDA</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/CUDA/"><![CDATA[<h2 id="cuda-programming">CUDA Programming</h2> <blockquote> <p>reference :<br/> <a href="https://www.youtube.com/watch?v=n6M8R8-PlnE&amp;t=557s">How CUDA Programming Works</a><br/> <a href="https://www.youtube.com/watch?v=ot1wyQCutSA&amp;list=PLTgRMOcmRb3O5Xc8PJckYdbyCr5HPGx4e">Learning CUDA 10 Programming</a><br/> NeRF and 3DGS Study</p> </blockquote> <h3 id="introduction-to-cuda">Introduction to CUDA</h3> <ul> <li>API : <ul> <li>Drive API :<br/> low-level API (not conveninent)<br/> shipped along with display driver</li> <li>Runtime API :<br/> high-level API (with nvcc)<br/> included with the CUDA toolkit</li> <li>standard CUDA API functions :<br/> \(\#\) include \(&lt;\) cuda_runtime_api.h \(&gt;\)</li> <li>Driver API ver.이 Runtime API ver.보다 최신꺼여야 함</li> </ul> </li> <li>Execution Model : <ul> <li>Kernel :<br/> executed by N CUDA threads in parallel<br/> threads \(\rightarrow\) blocks \(\rightarrow\) grid</li> <li>Hardware Architecture :<br/> 스펙은 CUDA Capability version number로 확인 가능 <ul> <li>Streaming multiprocessors (SMs) :<br/> 각 GPU 당 여러 SMs (global memory 공유)<br/> 각 SM 당 여러 CUDA cores (cache, registers 공유)<br/> 예측 (branch prediction 또는 speculative execution) 없음</li> <li>SIMT architecture (Single-Instruction-Multiple-Thread) :</li> <li>warp :<br/> 32 threads를 그룹지어서 scheduling한 걸 하나의 unit으로 run<br/> warp에 필요한 execution context는 lifetime 내내 SM에 있기 때문에 switching warps 위한 overhead 없어서 좋음</li> <li>running a Kernel :<br/> available SM에 blocks 할당<br/> \(\rightarrow\) block을 warps로 split<br/> \(\rightarrow\) SM 당 multiple warps on block 실행<br/> \(\rightarrow\) block 실행이 끝나면 SM을 free시키고 new block을 scheduling until entire grid is done</li> </ul> </li> </ul> </li> </ul> <h3 id="performance-optimization">Performance Optimization</h3> <ul> <li>NVIDIA Visual Profiler in CUDA toolkit :<br/> Hardware performance counters on GPU (admin user <a href="https://developer.nvidia.com/nvidia-development-tools-solutions-err_nvgpuctrperm-permission-issue-performance-counters">Link</a> 자격 있어야 접근 가능) 이용해서<br/> device code와 CUDA API calls를 추적</li> </ul> <h3 id="parallel-algorithm">Parallel Algorithm</h3> <h3 id="gpu-accelerated-libraries">GPU Accelerated Libraries</h3> <h3 id="advanced-cuda-topics">Advanced CUDA Topics</h3> <h3 id="summary">Summary</h3>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="radiance"/><category term="field"/><category term="tensor"/><category term="decomposition"/><summary type="html"><![CDATA[.cu coding]]></summary></entry><entry><title type="html">TensoRF</title><link href="https://semyeong-yu.github.io/blog/2024/TensoRF/" rel="alternate" type="text/html" title="TensoRF"/><published>2024-09-17T12:00:00+00:00</published><updated>2024-09-17T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/TensoRF</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/TensoRF/"><![CDATA[<h2 id="tensorf-tensorial-radiance-fields">TensoRF: Tensorial Radiance Fields</h2> <h4 id="anpei-chen-zexiang-xu-andreas-geiger-jingyi-yu-hao-su">Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, Hao Su</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2203.09517">https://arxiv.org/abs/2203.09517</a><br/> project website :<br/> <a href="https://apchenstu.github.io/TensoRF/">https://apchenstu.github.io/TensoRF/</a><br/> code :<br/> <a href="https://github.com/apchenstu/TensoRF">https://github.com/apchenstu/TensoRF</a><br/> referenced blog :<br/> <a href="https://xoft.tistory.com/42">https://xoft.tistory.com/42</a></p> </blockquote> <h2 id="abstract">Abstract</h2> <ul> <li> <p>Radiance Field (Scene)에 대해 Tensor Decomposition을 적용해보자!</p> </li> <li> <p>fast training and less computational cost</p> </li> </ul> <h2 id="tensor-decomposition">Tensor Decomposition</h2> <ul> <li> <p>외적 (outer product) :<br/> \(\begin{bmatrix} 1 &amp; 2 \\ 2 &amp; 4 \\ 4 &amp; 8 \end{bmatrix} = \begin{bmatrix} 1 \\ 2 \\ 4 \end{bmatrix} \circ \begin{bmatrix} 1 &amp; 2 \end{bmatrix}\)<br/> 위의 예시는<br/> shape (3, 2) matrix를<br/> shape (3,) vector와 shape (2,) vector의 외적으로 표현</p> </li> <li> <p>Tensor Decomposition :</p> <ul> <li>\(n\)-dim.의 data를 \(n\)개의 1D vector들의 외적으로 표현할 수 있다!<br/> 이 때, 정보 손실이 발생할 수 있으므로<br/> \(R\) 개의 rank에 대해 외적들을 더해 \(n\)-dim. data를 근사</li> <li>장점 :<br/> 고차원 data를 1D vector들로 표현할 수 있으므로<br/> speed 개선</li> <li>단점 :<br/> 수많은 1D vector들로 표현하므로<br/> GPU memory 많이 소요</li> <li>종류 :<br/> CP(CANDECOMP/PARAFAC) decomposition<br/> Tucker Decomposition<br/> Block Term Decomposition</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/1-480.webp 480w,/assets/img/2024-09-17-TensoRF/1-800.webp 800w,/assets/img/2024-09-17-TensoRF/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> reference : https://www.kolda.net/publication/TensorReview.pdf </div> <ul> <li>Tensor Decomposition w. Trilinear Interpolation :<br/> interpolation으로 1D vector A와 B의 길이를 증가시키고<br/> 그 값으로 원본 matrix 표현</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/2-480.webp 480w,/assets/img/2024-09-17-TensoRF/2-800.webp 800w,/assets/img/2024-09-17-TensoRF/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> https://xoft.tistory.com/42 </div> <h2 id="tensorf-tensor-decomposition">TensoRF Tensor Decomposition</h2> <h3 id="cpcandecompparafac-decomposition">CP(CANDECOMP/PARAFAC) Decomposition</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/3-480.webp 480w,/assets/img/2024-09-17-TensoRF/3-800.webp 800w,/assets/img/2024-09-17-TensoRF/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> https://xoft.tistory.com/42 </div> <ul> <li>shape (i, j, k)의 \(T\) 에 대해<br/> \(T = \sum_{r=1}^R v_{r}^1 \circ v_{r}^2 \circ v_{r}^3\)</li> </ul> <h3 id="vmvector-matrix-decomposition">VM(vector-matrix) Decomposition</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/4-480.webp 480w,/assets/img/2024-09-17-TensoRF/4-800.webp 800w,/assets/img/2024-09-17-TensoRF/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> https://xoft.tistory.com/42 </div> <ul> <li>shape (i, j, k)의 \(T\) 에 대해<br/> \(T = \sum_{r=1}^{R_1} v_{r}^1 \circ M_{r}^{2,3} + \sum_{r=1}^{R_2} v_{r}^2 \circ M_{r}^{1,3} + \sum_{r=1}^{R_3} v_{r}^3 \circ M_{r}^{1,2}\)</li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li>Grid-based 연구들이 training speed 높이는 데 많은 기여를 하고 있으니<br/> 다른 논문들도 한 번 읽어보자 <ul> <li>Plenoxel (CVPR 2022)</li> <li>Instant-NGP (SIGGRAPH 2022)</li> <li>DVGO (CVPR 2022)</li> </ul> </li> <li>Grid-based 연구들 <ul> <li>장점 : speed 개선</li> <li>단점 : 해상도가 증가하면 GPU memory 많이 소요<br/> 기존 연구들은 space complexity \(O(N^3)\) 인데,<br/> TensoRF는 이를 \(O(N^2)\) 으로 줄임</li> </ul> </li> </ul> <h2 id="algorithm">Algorithm</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/5-480.webp 480w,/assets/img/2024-09-17-TensoRF/5-800.webp 800w,/assets/img/2024-09-17-TensoRF/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Algorithm : <ul> <li>step 1)<br/> scene을 bounded cubic (grid)로 제한</li> <li>step 2)<br/> ray를 쏴서 sampled points를 구한 뒤<br/> 각 rank의 선과 면으로 projection하고<br/> 외적한 값들을 이용해서<br/> color와 volume density 계산</li> <li>step 2-1)<br/> volume density는 단순히 외적한 값들을 더해서 구함<br/> (VM Decomposition)</li> <li>step 2-2)<br/> color는 외적한 값들을 concat한 뒤<br/> function B와 function S에 통과시켜 얻음 <ul> <li>function B :<br/> 1개의 FC-layer<br/> appearance commonalities를 추상화는 Global Apperance Dictionary 역할</li> <li>function S :<br/> MLP 또는 SH(Spherical Harmonics) 함수</li> </ul> </li> </ul> </li> </ul> <h2 id="loss">Loss</h2> <ul> <li>sparse input images일 경우<br/> 적게 관측된 view에서는 outlier 혹은 noise가 발생할 수 있어<br/> overfitting 혹은 local minima 문제 발생<br/> \(\rightarrow\)<br/> regularization term 추가한 loss 사용<br/> e.g. TV(total variation) loss :<br/> pixel 값 간의 급격한 변화 (noise or outlier)를 억제하기 위해<br/> \(I_{i+1, j} - I_{i, j}\) 항과 \(I_{i, j+1} - I_{i, j}\) 항을 loss에 추가</li> </ul> <h2 id="coarse-to-fine">Coarse-to-Fine</h2> <ul> <li> <p>NeRF의 coarse-to-fine 기법 :<br/> \(w_i = T_i \alpha_{i}\) 의 PDF 분포에 따라<br/> 일부 구간을 더 많이 sampling</p> </li> <li>Mip-NeRF 360의 coarse-to-fine 기법 : <ul> <li>small coarse proposal-MLP는 many samples로 여러 번 evaluate하여 weight \(\hat w\) 를 구하고<br/> large fine NeRF-MLP는 less samples로 딱 한 번 evaluate하여 weight \(w\) 와 color \(c\) 를 구함</li> <li>proposal loss를 이용하여 NeRF-MLP의 지식을 proposal-MLP가 따라잡도록 함</li> </ul> </li> <li>TensoRF의 coarse-to-fine 기법 :<br/> 단순히 grid 크기를 upsampling<br/> Grid size(resolution)이 커질수록 선 또는 면이 더 촘촘해져서 3D scene의 high-freq. feature를 더 잘 잡아낼 수 있음</li> </ul> <h2 id="implementation">Implementation</h2> <ul> <li>Decomposition Rank : 총 48개 <ul> <li>RGB : 16, 4, 4</li> <li>volume density : 16, 4, 4</li> </ul> </li> <li> <p>Grid size : coarse-to-fine<br/> \(128^3\) 에서 \(300^3\) 으로 점점 증가시키면서 학습<br/> (2000, 3000, 4000, 5500, 7000 step에서 점차 증가시킴)</p> </li> <li> <p>Batch size : 4096 pixels</p> </li> <li>Adam optimizer, V100 GPU(16GB)</li> </ul> <h2 id="evaluation">Evaluation</h2> <ul> <li>기존 Grid-based 연구들과<br/> training speed는 유사하지만<br/> PSNR이 높고<br/> 모델 사이즈 및 GPU memory 사용량이 적음</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/6-480.webp 480w,/assets/img/2024-09-17-TensoRF/6-800.webp 800w,/assets/img/2024-09-17-TensoRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/7-480.webp 480w,/assets/img/2024-09-17-TensoRF/7-800.webp 800w,/assets/img/2024-09-17-TensoRF/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Ours-VM-192 : 4DGS, VM Decomposition, 192개의 rank <ul> <li>speed, PSNR : Ours-VM-192를 15000 iter.만큼만 진행했을 때 8분만에 기존 연구들보다 PSNR 높음</li> <li>memory : 기존 연구들보다 확연히 memory size 적음</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/8-480.webp 480w,/assets/img/2024-09-17-TensoRF/8-800.webp 800w,/assets/img/2024-09-17-TensoRF/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/9-480.webp 480w,/assets/img/2024-09-17-TensoRF/9-800.webp 800w,/assets/img/2024-09-17-TensoRF/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Param. 실험 : <ul> <li>Grid size가 증가할수록 성능 좋아지지만 speed 느려지고 model size 커짐</li> <li>CP Decomposition보다 VM Decomposition이 성능 더 좋음</li> <li>rank 개수가 증가할수록 성능 좋아짐</li> </ul> </li> <li>iter. : <ul> <li>iter.이 증가할수록 PSNR이 증가<br/> 5k iter.만 해도 PSNR이 30에 가까워지고, 점점 변동폭이 작아짐</li> </ul> </li> </ul> <h2 id="limitation">Limitation</h2> <ul> <li>scene을 bounded cubic 안에 제한해야<br/> projection을 통해 VM Decomposition이 가능하므로<br/> unbounded scene은 다루지 못함</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="radiance"/><category term="field"/><category term="tensor"/><category term="decomposition"/><summary type="html"><![CDATA[Tensorial Radiance Fields (ECCV 2022)]]></summary></entry><entry><title type="html">4D Gaussian Splatting</title><link href="https://semyeong-yu.github.io/blog/2024/4DGS/" rel="alternate" type="text/html" title="4D Gaussian Splatting"/><published>2024-09-14T12:00:00+00:00</published><updated>2024-09-14T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/4DGS</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/4DGS/"><![CDATA[<h2 id="4d-gaussian-splatting-for-real-time-dynamic-scene-rendering">4D Gaussian Splatting for Real-Time Dynamic Scene Rendering</h2> <h4 id="guanjun-wu-taoran-yi-jiemin-fang-lingxi-xie-xiaopeng-zhang-wei-wei-wenyu-liu-qi-tian-xinggang-wang">Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, Xinggang Wang</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2310.08528">https://arxiv.org/abs/2310.08528</a><br/> project website :<br/> <a href="https://guanjunwu.github.io/4dgs/index.html">https://guanjunwu.github.io/4dgs/index.html</a><br/> code :<br/> <a href="https://github.com/hustvl/4DGaussians">https://github.com/hustvl/4DGaussians</a><br/> referenced blog :<br/> <a href="https://xoft.tistory.com/54">https://xoft.tistory.com/54</a></p> </blockquote> <blockquote> <p>핵심 요약 :<br/> <code class="language-plaintext highlighter-rouge">3DGS를 dynamic scene에 적용하고자 할 때</code><br/> x, y, z, t를 input으로 갖는 encoder로서<br/> <code class="language-plaintext highlighter-rouge">4D scene을 2D planes로 표현하는 HexPlane 기법을 이용하겠다!</code></p> </blockquote> <h2 id="abstract">Abstract</h2> <ul> <li> <p>spatially-temporally-sparse input으로부터<br/> complex point motion을 정확하게 모델링하면서<br/> high efficiency로 real-time dynamic scene을 rendering하는 건 매우 challenging task</p> </li> <li>3DGS를 각 frame에 적용하는 게 아니라 4DGS라는 새로운 모델 제시 <ul> <li><code class="language-plaintext highlighter-rouge">오직 3DGS 한 세트</code> 필요</li> <li>4DGS framework : <ul> <li><code class="language-plaintext highlighter-rouge">Spatial-Temporal Structure Encoder</code> :<br/> HexPlane <d-cite key="neuralvoxel1">[22]</d-cite> 에서 영감을 받아<br/> decomposed neural voxel encoding algorithm을 이용해서<br/> <code class="language-plaintext highlighter-rouge">4D neural voxel을 2D voxel planes로 decompose</code>하여<br/> 2D voxel plane (param.)에 Gaussian <code class="language-plaintext highlighter-rouge">point-clouds (pts)의 spatial-temporal 정보를 encode</code></li> <li><code class="language-plaintext highlighter-rouge">Extremely Tiny Multi-head Gaussian Deformation Decoder</code> :<br/> 가벼운 MLP를 이용해서<br/> <code class="language-plaintext highlighter-rouge">Gaussian deformation을 예측</code>함</li> </ul> </li> </ul> </li> <li>4DGS :<br/> real-time (82 FPS) rendering at high (800 \(\times\) 800) resolution on RTX 3090 GPU</li> </ul> <h2 id="contribution">Contribution</h2> <ul> <li> <p>Gaussian <code class="language-plaintext highlighter-rouge">motion</code>과 <code class="language-plaintext highlighter-rouge">shape</code>-deformation을 모두 모델링할 수 있는 4DGS framework 제시<br/> w. efficient <code class="language-plaintext highlighter-rouge">Gaussian deformation field</code> network</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">multi-resolution</code> encoding<br/> (only on spatial planes)<br/> (connect nearby 3D Gaussians to build rich Gaussian features)<br/> by efficient <code class="language-plaintext highlighter-rouge">spatial-temporal structure encoder</code></p> </li> <li> <p>SOTA <code class="language-plaintext highlighter-rouge">performance</code>이면서 <code class="language-plaintext highlighter-rouge">real-time</code> rendering on <code class="language-plaintext highlighter-rouge">dynamic</code> scenes<br/> e.g. 82 FPS at resol. 800 \(\times\) 800 for synthetic dataset<br/> e.g. 30 FPS at resol. 1352 \(\times\) 1014 for real dataset</p> </li> <li> <p>4D scenes에서의 editing 및 tracking에 활용 가능</p> </li> </ul> <h2 id="related-works">Related Works</h2> <h3 id="novel-view-synthesis">Novel View Synthesis</h3> <ul> <li>static scene : <ul> <li>light fields <d-cite key="lightfield">[1]</d-cite>, mesh <d-cite key="mesh1">[2]</d-cite> <d-cite key="mesh2">[3]</d-cite> <d-cite key="mesh3">[4]</d-cite> <d-cite key="mesh4">[5]</d-cite>, voxels <d-cite key="voxel1">[6]</d-cite> <d-cite key="voxel2">[7]</d-cite> <d-cite key="voxel3">[8]</d-cite>, multi-planes <d-cite key="multiplane">[9]</d-cite> 이용한 methods</li> <li>NeRF-based methods <a href="https://semyeong-yu.github.io/blog/2024/NeRF/">NeRF</a> <a href="https://semyeong-yu.github.io/blog/2024/MipNeRF/">MipNeRF</a> <d-cite key="nerf++">[10]</d-cite></li> </ul> </li> <li>dynamic scene : <ul> <li>NeRF-based methods <d-cite key="dynerf1">[11]</d-cite> <d-cite key="dynerf2">[12]</d-cite> <d-cite key="dynerf3">[13]</d-cite></li> <li><code class="language-plaintext highlighter-rouge">explicit voxel grid</code> <d-cite key="voxeltemp1">[14]</d-cite> <d-cite key="voxeltemp2">[15]</d-cite> <d-cite key="voxeltemp3">[16]</d-cite> <d-cite key="voxeltemp4">[17]</d-cite> :<br/> temporal info. 모델링하기 위해 explicit voxel grid 사용</li> <li><code class="language-plaintext highlighter-rouge">flow-based</code> methods <d-cite key="flow1">[18]</d-cite> <d-cite key="flow2">[19]</d-cite> <d-cite key="voxeltemp3">[16]</d-cite> <d-cite key="flow3">[20]</d-cite> <d-cite key="flow4">[21]</d-cite> :<br/> nearby frames를 blending하는 warping algorithm 사용</li> <li><code class="language-plaintext highlighter-rouge">decomposed neural voxels</code> <d-cite key="neuralvoxel1">[22]</d-cite> <d-cite key="neuralvoxel2">[23]</d-cite> <d-cite key="neuralvoxel3">[24]</d-cite> <d-cite key="neuralvoxel4">[25]</d-cite> <d-cite key="neuralvoxel5">[26]</d-cite> <d-cite key="neuralvoxel6">[27]</d-cite> :<br/> 빠른 training on dynamic scenes 가능<br/> (Fig 1.의 (b))</li> <li><code class="language-plaintext highlighter-rouge">multi-view</code> setups 다루기 위한 methods <d-cite key="multi1">[28]</d-cite> <d-cite key="multi2">[29]</d-cite> <d-cite key="multi3">[30]</d-cite> <d-cite key="multi4">[31]</d-cite> <d-cite key="multi5">[32]</d-cite> <d-cite key="multi6">[33]</d-cite></li> <li>본 논문 (4DGS) :<br/> 위에서 언급된 methods는 빠른 training은 가능했지만 real-time rendering on dynamic scenes는 여전히 어려웠음<br/> \(\rightarrow\)<br/> 본 논문은 빠른 training 및 rendering pipeline 제시<br/> (Fig 1.의 (c))</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/1-480.webp 480w,/assets/img/2024-09-14-4DGS/1-800.webp 800w,/assets/img/2024-09-14-4DGS/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 1. dynamic scene rendering methods </div> <ul> <li>Fig 1. 설명 :<br/> dynamic scene을 rendering하는 여러 방법들 소개 <ul> <li>(a) : Deformation-based (Canonical Mapping) Volume Rendering<br/> point-deformation-field를 이용해서<br/> sampled points를 canonical space로 mapping<br/> (하나의 ray 위의 sampled points가 다같이 canonical space로 mapping되므로<br/> 각 point의 서로 다른 속도를 잘 모델링하지 못함)</li> <li>(b) : Time-aware Volume Rendering<br/> 각 timestamp에서의 각 point의 feature를 직접 개별적으로 계산<br/> (path는 그대로)</li> <li>(c) : 4DGS <br/> compact <code class="language-plaintext highlighter-rouge">Gaussian-deformation-field</code>를 이용해서<br/> 기존의 3D Gaussians를 특정 timestamp의 3D Gaussians로 변환<br/> ((a)와 유사하긴 하지만<br/> 각 Gaussian이 <code class="language-plaintext highlighter-rouge">ray에 의존하지 않고 서로 다른 속도로 이동</code> 가능)</li> </ul> </li> </ul> <h3 id="neural-rendering-w-point-clouds">Neural Rendering w. Point Clouds</h3> <ul> <li> <p>3D scenes를 나타내기 위해 meshes, point-clouds, voxels, hybrid ver. 등 여러 분야가 연구되어 왔는데<br/> 그 중 point-cloud representation을 volume rendering과 결합하면<br/> dynamic novel view synthesis task도 잘 수행 가능</p> </li> <li> <p>3DGS :<br/> <code class="language-plaintext highlighter-rouge">explicit</code> representation이라서,<br/> <code class="language-plaintext highlighter-rouge">differentiable</code> <code class="language-plaintext highlighter-rouge">point</code>-based splatting이라서,<br/> <code class="language-plaintext highlighter-rouge">real-time</code> renderer라서 주목받음</p> </li> <li> <p>3DGS on dynamic scenes :</p> <ul> <li>Dynamic3DGS <d-cite key="dyna3DGS">[34]</d-cite> : <ul> <li>3D Gaussian 개수를 고정하고<br/> 각 timestamp \(t_i\) 마다 각 3D Gaussian의 position, variance를 tracking</li> <li>문제점 : <ul> <li>need dense multi-view input images</li> <li>prev. frame의 모델링이 부적절하면 전체적인 성능이 떨어짐</li> <li>linear memory consumption \(O(tN)\)<br/> for \(t\)-time steps and \(N\)-3D Gaussians</li> </ul> </li> </ul> </li> <li>4DGS (본 논문) : <ul> <li>very compact network로 3D Gaussian motion을 모델링하기 때문에<br/> training 효율적이고 real-time rendering</li> <li>memory consumption \(O(N+F)\)<br/> for \(N\)-3D Gaussians, \(F\)-parameters of Gaussian-deformation-field network</li> </ul> </li> <li>4DGS (Zeyu Yang) <d-cite key="4DGS1">[35]</d-cite> : <ul> <li>marginal temporal Gaussian 분포를 기존의 3D Gaussian 분포에 추가하여<br/> 3D Gaussians를 4D로 uplift</li> <li>However, 그러면 각 3D Gaussian은 오직 their local temporal space에만 focus</li> </ul> </li> <li>Deformable-3DGS (Ziyi Yang) <d-cite key="deformable3DGS">[36]</d-cite> : <ul> <li>본 논문처럼 MLP deformation network를 도입하여 dynamic scenes의 motion을 모델링</li> <li>본 논문 (4DGS)도 이와 유사하지만 training을 효율적으로 만듦</li> </ul> </li> <li>Spacetime-GS (Zhan Li) <d-cite key="spacetimeGS">[37]</d-cite> : <ul> <li>각 3D Gaussian을 individually tracking</li> </ul> </li> </ul> </li> </ul> <h3 id="dynamic-nerf-with-deformation-fields">Dynamic NeRF with Deformation Fields</h3> <ul> <li> <p>모든 dynamic NeRF는 아래의 식을 따른다<br/> \(c, \sigma = M(x, d, t, \lambda)\)<br/> where \(c \in R^3, \sigma \in R, x \in R^3, d \in R^2, t \in R, \lambda \in R\)<br/> where \(\lambda\) is optional input (frame-dependent code to build topological and appearance changes) <d-cite key="dynerf2">[12]</d-cite> <d-cite key="wild">[38]</d-cite></p> </li> <li> <p>deformation NeRF-based methods는<br/> Fig 1. (a)에서처럼<br/> deformation network \(\phi_{t} : (x, t) \rightarrow \Delta x\) 로 world-to-canonical mapping 한 뒤<br/> RGB color와 volume density를 뽑는다<br/> \(c, \sigma = NeRF(x+\Delta x, d, \lambda)\)</p> </li> <li> <p>4DGS (본 논문)은<br/> <code class="language-plaintext highlighter-rouge">Gaussian deformation field</code> network \(F\) 이용해서<br/> time \(t\) 에서의 <code class="language-plaintext highlighter-rouge">canonical-to-world mapping</code>을 직접 계산한 뒤<br/> differential splatting(rendering) 수행</p> </li> </ul> <h2 id="method">Method</h2> <h3 id="overview-gaussian-deformation-field-network">Overview (Gaussian Deformation Field Network)</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/2-480.webp 480w,/assets/img/2024-09-14-4DGS/2-800.webp 800w,/assets/img/2024-09-14-4DGS/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 2. Pipeline of this model (Gaussian Deformation Field Network) </div> <ul> <li>Fig 2. 설명 : <ul> <li>static 3D Gaussian set을 만듦</li> <li>각 3D Gaussian의 center 좌표 \(x, y, z\) 와 timestamp \(t\) 를<br/> Gaussian Deformation Field Network의 input으로 준비</li> <li>Spatial-Temporal Structure Encoder :<br/> multi-resolution voxel planes를 query하여<br/> voxel feature를 계산<br/> (temporal 및 spatial feature를 둘 다 encode 가능)</li> <li>Tiny Multi-head Gaussian Deformation Decoder :<br/> position, rotation, scaling head에서 각각 해당 feature를 decode하여<br/> 각 3D Gaussian의 position, rotation, scaling 변화량을 얻어서<br/> timestamp \(t\) 에서의 변형된 3D Gaussians를 얻음</li> </ul> </li> </ul> <h3 id="spatial-temporal-structure-encoder">Spatial-Temporal Structure Encoder</h3> <ul> <li> <p>근처에 있는 3D Gaussians끼리는 항상 spatial 및 temporal 정보를 비슷하게 공유하고 있다.<br/> 따라서 HexPlane 기법에서는 각 Gaussian이 따로 변형되는 게 아니라,<br/> 여러 <code class="language-plaintext highlighter-rouge">adjacent 3D Gaussian</code>들이 군집처럼 연결되어 함께 변형되므로<br/> motion과 shape-deformation을 정확하게 예측할 수 있다<br/> 이로써 변형된 geometry를 더 정확히 모델링하고 avulsion(벗겨짐?)을 방지할 수 있음</p> </li> <li> <p>기존 논문 설명 (Backgrounds) :</p> <ul> <li>TensoRF : <a href="https://semyeong-yu.github.io/blog/2024/TensoRF/">Link</a></li> <li>HexPlane <d-cite key="neuralvoxel1">[22]</d-cite> :<br/> 4차원(\(XYZT\))을 모델링하기 위해<br/> 3개 타입의 rank로 decomposition (\(XY\) 평면 - \(ZT\) 평면, \(XZ\) 평면 - \(YT\) 평면, \(YZ\) 평면 - \(XT\) 평면)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/3-480.webp 480w,/assets/img/2024-09-14-4DGS/3-800.webp 800w,/assets/img/2024-09-14-4DGS/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> HexPlane Overview </div> <ul> <li>Spatial-Temporal Structure Encoder (1) : <ul> <li>vanilla 4D neural voxel은 memory를 많이 잡아먹기 때문에<br/> 4D neural voxel(\(XYZT\))을 6개의 multi-resol. planes로 decompose하는<br/> 4D K-Planes module <d-cite key="neuralvoxel2">[23]</d-cite> 사용</li> <li>3D Gaussians는 bounding plane voxels에 포함되어<br/> Gaussians의 deformation도 nearby temporal voxels에 encode될 수 있음 <code class="language-plaintext highlighter-rouge">????</code></li> <li>기존 논문들 <d-cite key="voxeltemp1">[14]</d-cite> <d-cite key="neuralvoxel1">[22]</d-cite> <d-cite key="neuralvoxel2">[23]</d-cite> <d-cite key="neuralvoxel5">[26]</d-cite> 에서 영감을 받아<br/> Spatial-Temporal Structure Encoder는<br/> multi-resolution HexPlane \(R(i, j)\) 와 tiny MLP \(\phi_{d}\) 로 구성됨</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/4-480.webp 480w,/assets/img/2024-09-14-4DGS/4-800.webp 800w,/assets/img/2024-09-14-4DGS/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Spatial-Temporal Structure Encoder (2) : <ul> <li>multi-resolution HexPlane \(R(i, j)\) :<br/> 본 논문에서는 TensoRF와 달리 Grid resol.을 점점 증가시키지 않고, 애초에 multi-resolution으로 decomposition의 rank를 구성함<br/> \(f_{h} = \cup_{l} \prod \text{interp}(R_{l}(i, j))\) <ul> <li>where<br/> \(f_h \in R^{h \ast l}\) : feature of decomposed neural voxel<br/> \(R_{l}(i, j) \in R^{h \times lN_i \times lN_j}\) : 2D voxel plane (nn.Parameter())<br/> \(h\) : hidden dim.<br/> \(\{ i, j \} \in \{ (x, y), (x, z), (y, z), (x, t), (y, t), (z, t) \}\) : 6 종류의 planes<br/> \(N\) : voxel grid의 basic resol.<br/> \(l \in \{ 1, 2 \}\) : upsampling scale (multi-resol.)<br/> \(\text{interp}\) : bilinear interpolation (plane의 grid의 네 꼭짓점으로부터 interpolation으로 voxel feature 뽑아냄)<br/> \(\prod\) : product over planes (K-Planes <d-cite key="neuralvoxel2">[23]</d-cite> 참고)<br/> \(\cup_{l}\) : multi-resol.에 대해 concat 또는 add</li> <li><a href="https://github.com/hustvl/4DGaussians/blob/master/scene/hexplane.py">Github Code</a> 에서 <ul> <li>forward()</li> <li>get_density() <ul> <li>self.grids : multi-resol. HexPlane<br/> 즉, nn.ModuleList() of init_grid_param()</li> <li>init_grid_param() : HexPlane<br/> 즉, nn.ParameterList() of nn.Parameter()<br/> where<br/> range(in_dim) = [0, 1, 2, 3] (x, y, z, t) 중에 grid_nd = 2개의 조합(plane)을 뽑아서<br/> 각 nn.Parameter()는 2D grid plane \(R_{l}(i, j)\) for \(\{ i, j \} \in \{ (x, y), (x, z), (y, z), (x, t), (y, t), (z, t) \}\)<br/> w. shape \((1, D_{out}, \text{resol.}[j], \text{resol.}[i])\)<br/> e.g. \(R_{l}(x, t)\), 즉 \(XT\) plane은 nn.Parameter()<br/> w. shape \((1, D_{out}, \text{resol.}[3], \text{resol.}[0])\)</li> </ul> </li> <li>interpolate_ms_features() : \(f_{h} = \cup_{l} \prod \text{interp}(R_{l}(i, j))\) 반환</li> <li>grid_sample_wrapper() : \(\text{interp}(R_{l}(i, j))\) 반환</li> <li>grid_sampler() : F.grid_sample() <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html">Link</a><br/> second argument(pts) 좌표에서의 값을 구하기 위해 first argument(grid \(R_{l}(i, j)\))의 값을 interpolate<br/> 그럼 이제 dynamic 3D scene을 <code class="language-plaintext highlighter-rouge">4D neural voxel</code> 대신 <code class="language-plaintext highlighter-rouge">2D voxel plane</code> \(R_{l}(i, j)\) 이라는 param.들로 표현 가능</li> </ul> </li> </ul> </li> </ul> </li> <li>Spatial-Temporal Structure Encoder (3) : <ul> <li>tiny MLP \(\phi_{d}\) :<br/> \(f_d = \phi_{d} (f_h)\)<br/> merge all the features</li> <li>공간상(e.g. \(XY\) 평면) 또는 시간상(e.g. \(XT\) 평면)으로 인접한 voxel은<br/> HexPlane \(R(i, j)\) 에서 유사한 feature를 가져서 유사한 Gaussian param. 변화량을 가지므로<br/> optimization 진행됨에 따라<br/> Gaussian의 covariance가 줄어들면서 작은 3D Gaussian들이 모여서 dense해진다 <code class="language-plaintext highlighter-rouge">?????</code></li> </ul> </li> </ul> <h3 id="extremely-tiny-multi-head-gaussian-deformation-decoder">Extremely Tiny Multi-head Gaussian Deformation Decoder</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/5-480.webp 480w,/assets/img/2024-09-14-4DGS/5-800.webp 800w,/assets/img/2024-09-14-4DGS/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Multi-head Gaussian Deformation Decoder : <ul> <li>매우 작은 multi-head decoder로 position, scaling, rotation 변화량을 얻음<br/> \(\Delta \chi = \phi_{x}(f_d)\)<br/> \(\Delta r = \phi_{r}(f_d)\)<br/> \(\Delta s = \phi_{s}(f_d)\)</li> <li>그러면 변형된 deformed 3D Gaussians 계산할 수 있음<br/> \((\chi ' , r ' , s ') = (\chi + \Delta \chi, r + \Delta r, s + \Delta s)\) 에 대해<br/> next time \(t\) 의 deformed 3D Gaussian set은<br/> \(G ' = \{ \chi ' , r ' , s ', \sigma, c \}\)</li> <li>근데 실제 implementation 할 때는 speed 증가 위해<br/> scaling(size), rotation, color, opacity는 고정하고<br/> position 변화량만 구함</li> <li><a href="https://github.com/hustvl/4DGaussians/blob/master/scene/deformation.py">Github Code</a> 에서 <ul> <li>Class deform_network()의 forward_dynamic()</li> <li>Class Deformation()의 forward_dynamic() <ul> <li>hidden : encoder(HexPlane과 MLP) 거쳐 얻은 feature</li> <li>self.pos_deform, self.scales_deform, self.rotations_deform : tiny Multi-head decoder<br/> hidden으로부터 \(\Delta \chi, \Delta r, \Delta s\) 얻음</li> <li>self.static_mlp :<br/> hidden으로부터 \(\text{mask}\) 얻음</li> <li>position :<br/> \(\chi ' = \gamma(\chi) \times \text{mask} + \Delta \chi\)</li> <li>scaling :<br/> \(s ' = \gamma(s) \times \text{mask} + \Delta s\)</li> <li>rotation :<br/> \(r ' = \gamma(r) + \Delta r\)<br/> 또는<br/> \(r ' =\) quaternion product of \(\gamma(r)\) and \(\Delta r\)</li> <li>opacity, SH 도 deform 가능하게 짜놓긴 함<br/> \(\alpha ' = \alpha \times \text{mask} + \Delta \alpha\)<br/> \(k ' = k \times \text{mask} + \Delta k\)</li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/16-480.webp 480w,/assets/img/2024-09-14-4DGS/16-800.webp 800w,/assets/img/2024-09-14-4DGS/16-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>self._deformation = deform_network(args)</p> <h3 id="optimization">Optimization</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/6-480.webp 480w,/assets/img/2024-09-14-4DGS/6-800.webp 800w,/assets/img/2024-09-14-4DGS/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p><code class="language-plaintext highlighter-rouge">warm-up</code> :<br/> 처음 3000 iter. 동안은<br/> Gaussian Deformation Field Network 없이<br/> 3DGS의 SfM points initialization 이용하여<br/> <code class="language-plaintext highlighter-rouge">static 3DGS</code> optimize 하고,<br/> 그 후에 dynamic scene에 대해 4DGS framework를 fine-tuning 형태로 학습</p> </li> <li> <p>Loss :<br/> \(L = | \hat I - I | + L_{tv}\)</p> <ul> <li>L1 recon. loss</li> <li><code class="language-plaintext highlighter-rouge">total-variational loss</code> : <ul> <li>sparse input images일 경우에 적게 관측된 view에서는 noise 및 outlier 때문에 overfitting 및 local minima 문제가 발생할 수 있으므로<br/> <code class="language-plaintext highlighter-rouge">regularization</code> term 필요</li> <li>pixel 값 간의 급격한 변화를 억제하기 위해<br/> \(I_{i+1, j} - I_{i, j}\) 항과 \(I_{i, j+1} - I_{i, j}\) 항을 loss에 추가</li> </ul> </li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <ul> <li> <p>single RTX 3090 GPU</p> </li> <li> <p>Synthetic Dataset :</p> <ul> <li>designed for monocular settings</li> <li>camera poses for each timestamp은 거의 randomly generated 수준</li> <li>scene 당 50-200 frames</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/12-480.webp 480w,/assets/img/2024-09-14-4DGS/12-800.webp 800w,/assets/img/2024-09-14-4DGS/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Real-world Dataset : <ul> <li>by HyperNeRF <d-cite key="dynerf2">[12]</d-cite> and Neu3D <d-cite key="neuralvoxel4">[25]</d-cite></li> <li>HyperNeRF dataset :<br/> one or two cameras<br/> with straightforward camera motion<br/> (1,2개의 camera를 직관적인 경로로 움직이며 촬영)</li> <li>Neu3D dataset :<br/> 15 to 20 static cameras<br/> with extended periods and complex camera motions<br/> (15-20개의 많은 정적인 camera로 오랜 시간 동안씩 촬영하며 복잡한 경로로 camera를 움직임)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/13-480.webp 480w,/assets/img/2024-09-14-4DGS/13-800.webp 800w,/assets/img/2024-09-14-4DGS/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="results">Results</h3> <ul> <li>Metrics : <ul> <li>quality :<br/> PSNR<br/> LPIPS<br/> SSIM<br/> DSSIM<br/> MS-SSIM</li> <li>speed :<br/> FPS<br/> training times</li> <li>memory :<br/> storage</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/7-480.webp 480w,/assets/img/2024-09-14-4DGS/7-800.webp 800w,/assets/img/2024-09-14-4DGS/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/8-480.webp 480w,/assets/img/2024-09-14-4DGS/8-800.webp 800w,/assets/img/2024-09-14-4DGS/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Im4D <d-cite key="multi2">[29]</d-cite> 는 본 논문과 유사하게 high-quality이지만<br/> multi-cam 방식을 쓰기 때문에 monocular scene을 모델링하기 어렵</li> </ul> <h3 id="ablation-study">Ablation Study</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/9-480.webp 480w,/assets/img/2024-09-14-4DGS/9-800.webp 800w,/assets/img/2024-09-14-4DGS/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Spatial-Temporal Structure Encoder : <ul> <li>explicit HexPlane encoder는<br/> 3DGS의 spatial 및 temporal 정보를 모두 encode 하면서<br/> purely explicit method <d-cite key="dyna3DGS">[34]</d-cite> 보다 storage 공간 아낄 수 있음</li> <li>만약에 HexPlane encoder 없이 shallow MLP encoder만 쓰면<br/> 복잡한 deformation 모델링 어렵</li> </ul> </li> <li>3D Gaussian Initialization : <ul> <li>처음에 warm-up으로 SfM points initialization 한 뒤 static 3DGS optimize 부터 해야<br/> 아래의 장점들 있음 <ul> <li>3DGS 일부가 dynamic part에 분포되도록 함</li> <li>3DGS를 미리 학습해야 deformation field가 dynamic part에 더 집중 가능</li> <li>deformation field 학습 시 numerical errors를 방지하여 훈련 과정이 더 stable</li> </ul> </li> </ul> </li> <li>Multi-head Gaussian Deformation Decoder : <ul> <li>3D Gaussian motion을 modeling함으로써 dynamic scene을 잘 표현할 수 있도록 해줌</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/15-480.webp 480w,/assets/img/2024-09-14-4DGS/15-800.webp 800w,/assets/img/2024-09-14-4DGS/15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Neural Voxel Encoder : <ul> <li>implicit MLP-based neural voxel encoder (voxel grid)가 아니라<br/> explicit Dynamic 3DGS 기법을 사용할 경우<br/> rendering quality는 떨어지지만 FPS 및 storage는 향상</li> </ul> </li> <li>Two-stage Training : <ul> <li>static 3DGS stage \(\rightarrow\) dynamic 4DGS stage (fine-tuning) 으로<br/> 분할해서 학습할 경우 성능 향상<br/> (참고로 D-NeRF, DyNeRF에서는 point-clouds가 주어지지 않아서 어려운 task를 다룸)</li> </ul> </li> <li>Image-based Loss : <ul> <li>LPIPS loss, SSIM loss 같은 image-based loss를 사용할 경우<br/> training speed도 느려지고 quality도 떨어짐</li> <li>그 이유는<br/> image-based loss로 motion 부분을 fine-tuning하는 건 어렵고 복잡</li> </ul> </li> <li>Model Capacity (MLP size) : <ul> <li>voxel plane resol. 또는 MLP 크기가 증가할수록<br/> quality 향상되지만 FPS 및 storage 악화</li> </ul> </li> <li>Fast Training : <ul> <li>7k iter. 까지만 학습해도(training 시간 짧음) 괜찮은 PSNR 달성</li> </ul> </li> </ul> <h3 id="discussion">Discussion</h3> <ul> <li>Tracking with 3D Gaussians : <ul> <li>Dynamic3DGS <d-cite key="dyna3DGS">[34]</d-cite> 와 달리<br/> 본 논문은 monocular setting에서도 low storage로 3D object tracking 가능<br/> (e.g. 10MB for 3DGS and 8MB for deformation field network)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/10-480.webp 480w,/assets/img/2024-09-14-4DGS/10-800.webp 800w,/assets/img/2024-09-14-4DGS/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Composition(Editing) with 4D Gaussians : <ul> <li>Dynamic3DGS <d-cite key="dyna3DGS">[34]</d-cite> 에서처럼<br/> 4DGS editing 가능</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/11-480.webp 480w,/assets/img/2024-09-14-4DGS/11-800.webp 800w,/assets/img/2024-09-14-4DGS/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Rendering Speed (FPS) : <ul> <li>3DGS 수와 FPS는 반비례 관계인데<br/> Gaussians 수가 30,000개 이하이면 single RTX 3090 GPU에서 90 FPS 까지 가능</li> <li>이처럼 real-time FPS를 달성하려면<br/> resolution, Gaussian 수, Gaussian deformation field network 용량, hardware constraints 등 여러 요인을 조절해야 함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/14-480.webp 480w,/assets/img/2024-09-14-4DGS/14-800.webp 800w,/assets/img/2024-09-14-4DGS/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="limitation">Limitation</h3> <ul> <li>아래의 경우엔 학습 잘 안 됨 <ul> <li><code class="language-plaintext highlighter-rouge">large motions</code>일 경우</li> <li><code class="language-plaintext highlighter-rouge">background points</code>가 없을 경우</li> <li><code class="language-plaintext highlighter-rouge">camera pose</code>가 <code class="language-plaintext highlighter-rouge">unprecise</code>(부정확)할 경우</li> </ul> </li> <li> <p>추가적인 supervision 없이<br/> <code class="language-plaintext highlighter-rouge">static</code> Gaussians와 <code class="language-plaintext highlighter-rouge">dynamic</code> Gaussians의 joint motion을 구분하는 건 아직 어려운 과제</p> </li> <li><code class="language-plaintext highlighter-rouge">urban(large)-scale</code> recon.일 경우엔<br/> 3DGS 수가 훨씬 많아서<br/> Gaussian deformation field network를 query하기에 너무 무거우므로 좀 더 compact한 algorithm이 필요</li> </ul> <h3 id="conclusion">Conclusion</h3> <ul> <li> <p>4DGS framework for <code class="language-plaintext highlighter-rouge">real-time</code> <code class="language-plaintext highlighter-rouge">dynamic</code> scene rendering</p> </li> <li>efficient deformation field network to model motions and shape-deformation <ul> <li>Spatial-temporal structure encoder :<br/> adjacent Gaussians가 비슷하게 encode되도록 spatial-temporal 정보를 encode</li> <li>Multi-head Gaussian deformation decoder :<br/> position, scaling, rotation을 각각 modeling</li> </ul> </li> <li>dynamic scenes 모델링 뿐만 아니라<br/> 4D object tracking 및 editing에도 활용 가능</li> </ul> <h2 id="code-flow">Code Flow</h2> <ul> <li>TBD</li> </ul> <h2 id="question">Question</h2> <ul> <li>Q1 : 본 논문을 한 문장으로 요약하자면,<br/> 3DGS를 dynamic scene에 적용하고자 할 때 4D 정보를 효율적으로 encode하기 위해 2D planes로 scene을 표현하는 HexPlane 기법을 이용하겠다!인데,<br/> 본 논문이 novelty가 있는지 의구심이 듭니다.</li> <li> <p>A1 : 3DGS 논문 자체가 나온 지 얼마 안 돼서<br/> 기존 논문(HexPlane) 아이디어를 3DGS에 적용하는 논문들이 아직까지는 많이 채택되는 것 같다.</p> </li> <li>Q2 : 본 포스팅에서 코드 리뷰는 encoder (HexPlane) 쪽만 진행하였는데,<br/> Multi-head Gaussian deformation decoder로 position, scaling, rotation 변화량을 구해서<br/> Deformed(변형된) 3DGS를 구하는 부분의 코드도 보고 싶습니다.</li> <li>A2 : 포스팅의 “Extremely Tiny Multi-head Gaussian Deformation Decoder” 부분에 해당 내용을 추가하였습니다.</li> </ul> <h2 id="appendix">Appendix</h2> <ul> <li>TBD</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="GS"/><category term="4d"/><category term="dynamic"/><category term="rendering"/><summary type="html"><![CDATA[4D Gaussian Splatting for Real-Time Dynamic Scene Rendering (CVPR 2024)]]></summary></entry><entry><title type="html">Lagrange Multiplier Method</title><link href="https://semyeong-yu.github.io/blog/2024/Lagrange/" rel="alternate" type="text/html" title="Lagrange Multiplier Method"/><published>2024-09-14T11:00:00+00:00</published><updated>2024-09-14T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Lagrange</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Lagrange/"><![CDATA[<p>본 포스팅 출처 : <a href="https://untitledtblog.tistory.com/96">Link</a></p> <h3 id="lagrange-multiplier-method">Lagrange Multiplier Method</h3> <ul> <li> <p>언제? :<br/> multi-variate function을 optimize할 때<br/> <code class="language-plaintext highlighter-rouge">constraint</code>가 존재할 경우<br/> 최적점의 필요조건을 찾기 위해<br/> Lagrange Multiplier Method 사용</p> </li> <li> <p>핵심 아이디어 :<br/> 주어진 function \(f\) 와 constraint \(g_{i}\) 에 대해<br/> \(f\) 와 \(g_{i}\) 의 <code class="language-plaintext highlighter-rouge">접점 (경계)</code>에 \(f\) 의 최댓(솟)값이 존재할 수도 있다!<br/> 그리고 접점에서는 \(\nabla f\) 를 \(\nabla g_{i}\) 들의 linear comb.로 표현할 수 있다!<br/> (다만, 접점은 극점이므로 반드시 최댓값 또는 최솟값이 존재하는 건 아니다)</p> </li> </ul> <h3 id="equality-constraint">Equality Constraint</h3> <ul> <li>\(g_{i}\) 가 등식일 경우 (e.g. \(g_{i} = 1 - \phi_{i}^T\phi_{i} = 0\)) :<br/> 접점에서 gradient가 평행하므로 (이에 대한 수식 증명은 참고한 포스팅 <a href="https://untitledtblog.tistory.com/96">Link</a> 에 있음)<br/> \(\nabla f = \sum_{i=1}^N \lambda_{i} \nabla g_{i}\) 로부터<br/> 아래처럼 풀면 된다<br/> (단, \(\lambda \neq 0\))<br/> (단, \(\nabla f\) 과 \(\nabla g_{i}\) 은 평행할 뿐 방향은 반대여도 됨) <ul> <li>방법 1)<br/> \(\nabla f + \sum_{i=1}^N \lambda_{i} \nabla g_{i} = 0\) 와 \(g_{i} = 0\) 을 연립하여 풀면 된다</li> <li>방법 2)<br/> Equivalently,<br/> \(L = f + \sum_{i=1}^N \lambda_{i} g_{i}\) 에 대해<br/> \(L\) 의 극소(대)점을 찾으면 된다<br/> 즉, \(f, g_{i}\) 가 \(x_{j}\) 에 대한 함수일 경우<br/> \(\frac{\partial L}{\partial x_{j}} = 0\) 과 \(\frac{\partial L}{\partial \lambda_{i}} = 0\) 을 연립하여 풀면 된다</li> </ul> </li> </ul> <h3 id="inequality-constraint">Inequality Constraint</h3> <p>등식 constraint일 때의 Lagrange Multiplier Method는 완전히 이해했는데,<br/> 부등식 constraint일 때의 Lagrange Multiplier Method는 아직 이해 못함.<br/> 추후에 고칠(이해할) 필요 있음. TBD</p> <ul> <li>부등식 constraint일 경우 <code class="language-plaintext highlighter-rouge">KKT (Karush-Kuhn-Tucker) 조건</code>을 만족해야 한다 <ul> <li>1) \(f\) 는 모든 variable (e.g. \(x, y\))에 대해 differentiable</li> <li>2) \(\lambda_{i} \nabla g_{i} = 0\)</li> <li>3) \(\lambda{i} \geq 0\)<br/> (만약 \(\lambda_{i} \lt 0\) 일 경우 \(\nabla f\) 와 \(\nabla g_{i}\) 가 평행하지만 방향이 반대라는 의미이므로 두 함수의 최적점이 서로 반대 방향에 위치하여 constraint를 만족할 수 없다)<br/> (따라서 \(\lambda_{i} \geq 0\) 이어야만 (\(\nabla f\) 방향과 \(\nabla g_{i}\) 방향이 일치해야만) \(\nabla f\) 를 \(\nabla g_{i}\) 들의 linear comb.로 표현 가능한지 아닌지를 판정할 수 있다)</li> </ul> </li> <li>\(g_{i}\) 가 부등식일 경우 (e.g. \(g_{i} = 1 - \phi_{i}^T\phi_{i} \leq 0\)) :<br/> \(\lambda_{i} \nabla g_{i} = 0\) <ul> <li>\(\nabla g_{i} = 0\) 일 경우 :<br/> constraint \(g_{i} \leq 0\) 을 항상 만족하므로<br/> \(\nabla f \geq 0\) 을 푸는 문제로 바꿔 쓸 수 있다<br/> (constraint 없이 \(f\) 만 최적화하면 됨!)</li> <li>\(\lambda_{i} = 0\) 일 경우 :<br/> \(\nabla f\) 를 \(\nabla g_{i}\) 들의 linear comb.로 표현 불가능하다는 의미이므로<br/> 비교하는 두 gradient가 평행하지 않다<br/> 따라서 gradient 방향에 따라 constraint 만족하는 지 여부가 달라지므로<br/> \(\nabla g_{i} \gt 0\) 인 경우와 \(\nabla g_{i} \lt 0\) 인 경우를 모두 따져봐서<br/> 어떤 경우(방향)가 constraint를 만족하는지 확인해야 한다</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="math"/><category term="Lagrange"/><category term="min"/><category term="max"/><category term="constraint"/><summary type="html"><![CDATA[find min(max) with constraint]]></summary></entry><entry><title type="html">EE534 Pattern Recognition</title><link href="https://semyeong-yu.github.io/blog/2024/Pattern/" rel="alternate" type="text/html" title="EE534 Pattern Recognition"/><published>2024-09-10T11:00:00+00:00</published><updated>2024-09-10T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Pattern</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Pattern/"><![CDATA[<blockquote> <p>Lecture :<br/> 24F EE534 Pattern Recognition<br/> by KAIST Munchurl Kim <a href="https://www.viclab.kaist.ac.kr/">VICLab</a></p> </blockquote> <h2 id="chapter-1-overview">Chapter 1. Overview</h2> <h3 id="discriminative-vs-generative">Discriminative vs Generative</h3> <ul> <li>Discriminative model : <ul> <li> <table> <tbody> <tr> <td>learn $$P(Y</td> <td>X)\(to maximize\)P(Y</td> <td>X)$$ directly</td> </tr> </tbody> </table> </li> <li>e.g. logistic regression, SVM, nearest neighbor, CRF, Decision Tree and Random Forest, traditional NN</li> </ul> </li> <li>Generative model : <ul> <li> <table> <tbody> <tr> <td>learn $$P(X</td> <td>Y)\(and\)P(Y)\(to maximize\)P(X, Y) = P(X</td> <td>Y)P(Y)$$</td> </tr> <tr> <td>where can learn $$P(Y</td> <td>X) \propto P(X</td> <td>Y)P(Y)$$ indirectly</td> </tr> </tbody> </table> </li> <li>e.g. Bayesian network, Autoregressive model, GAN, Diffuson model</li> </ul> </li> </ul> <h2 id="chapter-2-bayes-decision-theory">Chapter 2. Bayes Decision Theory</h2> <h3 id="bayes-decision-rule">Bayes Decision Rule</h3> <ul> <li>conditional probability density :<br/> Let \(w\) be state (class)<br/> Let \(x\) be data (continous-valued sample) <ul> <li>prior : \(P(w=w_k)\)</li> <li> <table> <tbody> <tr> <td>likelihood : PDF $$P(x</td> <td>w_k)$$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>posterior : $$P(w_k</td> <td>x) = \frac{P(x</td> <td>w_k)P(w_k)}{P(x)}$$ (Bayes Rule)</td> <td> </td> </tr> <tr> <td>where $$P(w_1</td> <td>x) + P(w_2</td> <td>x) + \cdots + P(w_N</td> <td>x) = 1$$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>evidence : $$P(x) = \sum_{k=1}^N P(x</td> <td>w_k)P(w_k) = \sum_{k=1}^N P(x, w_k)$$</td> </tr> </tbody> </table> </li> </ul> </li> <li>Bayes Decision Rule :<br/> posterior 더 큰 쪽 고름! <ul> <li>Two-class (\(w_1, w_2\)) problem :<br/> choose \(w_1\)<br/> if \(P(w_1 | x) \gt P(w_2 | x)\)<br/> if \(P(x|w_1)P(w_1) \gt P(x|w_2)P(w_2)\)<br/> if \(\frac{P(x|w_1)}{P(x|w_2)} \gt \frac{P(w_2)}{P(w_1)}\)<br/> (likehood ratio \(\gt\) threshold)</li> <li>multi-class problem :<br/> choose \(w_i\) where \(P(w_i | x)\) is the largest</li> </ul> </li> </ul> <h3 id="minimum-error">minimum error</h3> <ul> <li>minimum error :<br/> GT가 \(w_1, w_2\) 이고, Predicted가 \(R_1, R_2\) 일 때, <ul> <li>\(P(error) = \int_{-\infty}^{\infty} P(error, x)dx = \int_{-\infty}^{\infty} P(error|x)P(x)dx\)<br/> \(= \int_{R_2}P(w_1|x)P(x)dx + \int_{R_1}P(w_2|x)P(x)dx\)<br/> \(= \int_{R_2}P(x|w_1)P(w_1)dx + \int_{R_1}P(x|w_2)P(w_2)dx\)<br/> \(= \begin{cases} A+B+D &amp; \text{if} &amp; x_B \\ A+B+C+D &amp; \text{if} &amp; x^{\ast} \end{cases}\)<br/> where \(A+B+D\) is minimum error and \(C\) is reducible error<br/> (아래 그림 참고)</li> <li>\(P(correct)\)<br/> \(= \int \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_1}P(x|w_1)P(w_1)dx + \int_{R_2}P(x|w_2)P(w_2)dx\)</li> <li>\(P(error) = 1 - P(correct)\)<br/> \(= 1 - \int \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_2}P(x|w_1)P(w_1)dx + \int_{R_1}P(x|w_2)P(w_2)dx\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/1-480.webp 480w,/assets/img/2024-09-10-Pattern/1-800.webp 800w,/assets/img/2024-09-10-Pattern/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>minimum error with rejection :<br/> decision이 확실하지 않을 때는 classification 자체를 reject하는 게 적절<br/> (classification error도 줄어들고, correct classification도 줄어듬) <ul> <li>feature space \(x\) 를 rejection region \(R\) 과 acceptance region \(A\) 으로 나눠서<br/> rejection region \(R=\{ x | \text{max}_{i} P(w_i | x) \leq 1 - t\}\) 에서는 reject decision<br/> acceptance region \(A=\{ x | \text{max}_{i} P(w_i | x) \gt 1 - t\}\) 에서는 \(w_1\) or \(w_2\) 로 classification decision 수행</li> <li>\(P_c(t) = P(correct)\)<br/> \(= \int_{A} \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_1}P(x|w_1)P(w_1)dx + \int_{R_2}P(x|w_2)P(w_2)dx\)</li> <li>\(P_r(t) = P(reject)\)<br/> \(= \int_{R}P(x|w_1)P(w_1)dx + \int_{R}P(x|w_2)P(w_2)dx\)<br/> \(= \int_{R} P(x)dx\)</li> <li>\(P_e(t) = P(error)\)<br/> \(= P(error, w_1) + P(error, w_2)\)<br/> \(= 1 - P_r(t) - P_c(t)\) by 아래 식 대입<br/> where \(P(error, w_1) = \int_{-\infty}^{\infty} P(x|w_1)P(w_1)dx - P(reject, w_1) - P(correct, w_1)\)<br/> where \(P(error, w_2) = \int_{-\infty}^{\infty} P(x|w_2)P(w_2)dx - P(reject, w_2) - P(correct, w_2)\)<br/> where \(\int_{-\infty}^{\infty} P(x|w_1)P(w_1)dx + \int_{-\infty}^{\infty} P(x|w_2)P(w_2)dx = \int_{-\infty}^{\infty} P(x)dx = 1\)</li> </ul> </li> <li>Summary : <ul> <li> <table> <tbody> <tr> <td>$$P(w_i</td> <td>x)$$ : rejection/acceptance region 구하는 데 사용</td> </tr> </tbody> </table> </li> <li>\(P(x|w_i)P(w_i)\) : \(P(correct, w_i), P(reject, w_i), P(error, w_i)\) 구해서<br/> \(P_c(t), P_r(t), P_e(t)\) 구하는 데 사용</li> <li> \[P_c(t) + P_r(t) + P_e(t) = 1\] </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/2-480.webp 480w,/assets/img/2024-09-10-Pattern/2-800.webp 800w,/assets/img/2024-09-10-Pattern/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/3-480.webp 480w,/assets/img/2024-09-10-Pattern/3-800.webp 800w,/assets/img/2024-09-10-Pattern/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="bayes-decision-rule-w-bayes-risk">Bayes Decision Rule w. Bayes risk</h3> <ul> <li>Bayes risk (minimum overall risk) :<br/> \(\Omega = \{ w_1, \cdots w_c \}\) 에서 \(w_j\) 는 \(j\) -th class<br/> \(A = \{ \alpha_{1}, \cdots, \alpha_{c} \}\) 에서 \(\alpha_{i}\) 는 class \(w_i\) 라고 예측하는 action<br/> \(\lambda(\alpha_{i} | w_j) = \lambda_{ij}\) : class \(w_j\) 가 GT일 때, class \(w_i\) 로 pred. 했을 때의 loss <ul> <li>conditional risk for taking action \(\alpha_{i}\) :<br/> 특정 input \(x\) 에 대해<br/> \(R(\alpha_{i}|x) = \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j|x)\)</li> <li>overall risk for taking action \(\alpha_{i}\) :<br/> 모든 input \(x\) 에 대해 적분<br/> \(R(\alpha_{i}) = \int R(\alpha_{i}|x)P(x)dx\)<br/> \(= \int \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j|x) P(x)dx\)<br/> \(= \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j) \int P(x|w_j)dx\)<br/> \(= \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j)\)<br/> \(= \sum_{j=1}^c \lambda_{ij}P(w_j)\)<br/> where pdf(likelihood) 합 \(\int P(x|w_j)dx = 1\)</li> <li>모든 input \(x\) 에 대해 가장 loss가 최소인 class \(w_i\) 로 예측하면,<br/> minimum overall risk (= Bayes risk) 를 가짐</li> </ul> </li> <li>Bayes Decision Rule for Bayes risk : <ul> <li>Two-class (\(w_1, w_2\)) problem :<br/> choose \(w_1\)<br/> if \(R(\alpha_{1} | x) \lt R(\alpha_{2} | x)\)<br/> if \(\lambda_{11}P(w_1 | x) + \lambda_{12}P(w_2 | x) \lt \lambda_{21}P(w_1 | x) + \lambda_{22}P(w_2 | x)\)<br/> if \((\lambda_{21} - \lambda_{11})P(w_1 | x) \gt (\lambda_{12} - \lambda_{22})P(w_2 | x)\)<br/> if \(\frac{P(x | w_1)}{P(x | w_2)} \gt \frac{(\lambda_{12} - \lambda_{22})P(w_2)}{(\lambda_{21} - \lambda_{11})P(w_1)}\)<br/> if \(\frac{P(x | w_1)}{P(x | w_2)} \gt \frac{P(w_2)}{P(w_1)}\) for \(\lambda_{11}=\lambda_{22}=0\) and \(\lambda_{12}=\lambda_{21}\)<br/> (likehood ratio \(\gt\) threshold) (위의 Bayes Decision Rule에서 구한 식과 same)</li> <li> <table> <tbody> <tr> <td>loss $$\lambda(\alpha_{i}</td> <td>w_j) = \begin{cases} 0 &amp; \text{if} &amp; i=j &amp; (\text{no penalty}) \ 1 &amp; \text{if} &amp; i \neq j &amp; (\text{equal penalty}) \end{cases}$$ 일 때</td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td>conditional risk $$R(\alpha_{i}</td> <td>x) = \sum_{j=1}^c \lambda(\alpha_{i}</td> <td>w_j)P(w_j</td> <td>x) = \sum_{j=1, j \neq i}^c P(w_j</td> <td>x) = 1 - P(w_i</td> <td>x)$$ 이므로</td> </tr> <tr> <td>Bayes Decision Rule에서 conditional risk $$R(\alpha_{i}</td> <td>x)\(최소화는 posterior\)P(w_i</td> <td>x)$$ 최대화와 같음</td> <td> </td> <td> </td> <td> </td> </tr> </tbody> </table> </li> <li>multi-class problem :<br/> classifieer (discriminant function) (space-partitioning function) \(g(x)\) 에 대해<br/> choose \(w_i\) where \(g_{i}(x)\) is the largest<br/> s.t. decision boundary is \(g_{i}(x) = g_{j}(x)\) where they are the two largest discriminant functions<br/> e.g. Bayes classifier : \(g_{i}(x) = - R(\alpha_{i} | x)\) or \(g_{i}(x) = P(w_i | x)\) or \(g_{i}(x) = P(x | w_i)P(w_i)\) or \(g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i)\)</li> </ul> </li> </ul> <h3 id="discriminant-function-for-gaussian-pdf">Discriminant Function for Gaussian PDF</h3> <ul> <li> <p>\(G(\boldsymbol x) = \frac{1}{(2\pi)^{\frac{d}{2}} | \Sigma |^{\frac{1}{2}}}e^{-\frac{1}{2}(\boldsymbol x - \boldsymbol \mu)^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu)}\)<br/> where \(d \times d\) covariance \(\Sigma = E[(\boldsymbol x - \boldsymbol \mu)(\boldsymbol x - \boldsymbol \mu)^T] = E[\boldsymbol x \boldsymbol x^{T}] - \boldsymbol \mu \boldsymbol \mu^{T} = S - \boldsymbol \mu \boldsymbol \mu^{T}\)<br/> where \(S = E[\boldsymbol x \boldsymbol x^{T}]\) : standard autocorrelation matrix</p> </li> <li> <p>Discriminant function for Gaussian PDF :<br/> likelihood \(P(x | w_i)\) 를 Gaussian PDF로 둘 경우,<br/> \(g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{d}{2} \text{ln}(2\pi) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\)</p> <ul> <li>case 1) \(\Sigma_{i} = \sigma^{2} \boldsymbol I\) (모든 classes에 대해 equal covariance) (등방성(sphere)) <br/> \(g_{i}(x) = -\frac{\| \boldsymbol x - \boldsymbol \mu_{i} \|^2}{2 \sigma^{2}} + \text{ln}P(w_i)\)<br/> \(i\) 와 관련된 term만 남기면<br/> \(g_{i}(x) = \frac{1}{\sigma^{2}} \boldsymbol \mu_{i}^T \boldsymbol x - \frac{1}{2\sigma^{2}} \boldsymbol \mu_{i}^T \boldsymbol \mu_{i} + \text{ln}P(w_i)\)<br/> \(= \boldsymbol w_i^T \boldsymbol x + \boldsymbol w_{i0}\) (linear) <ul> <li>decision boundary :<br/> hyperplane \(g(x) = g_{i}(x) - g_{j}(x) = (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T(\boldsymbol x - \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) + \frac{\sigma^{2}}{(\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T} \text{ln}\frac{P(w_i)}{P(w_j)})\)<br/> \(= \boldsymbol w^T (\boldsymbol x - \boldsymbol x_0) = 0\)</li> <li>\(\boldsymbol x_0\) 를 지나고 \(\boldsymbol w = \boldsymbol \mu_{i} - \boldsymbol \mu_{j}\) 에 수직인 hyperplane</li> <li>\(\boldsymbol x_0 = \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) - \frac{\sigma^{2}}{\| \boldsymbol \mu_{i} - \boldsymbol \mu_{j} \|^2} \text{ln}\frac{P(w_i)}{P(w_j)} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})\) 이므로<br/> \(\boldsymbol x_0\) 의 위치는 \(\boldsymbol \mu_{i}\) 와 \(\boldsymbol \mu_{j}\) 의 중점에서 \(\begin{cases} \boldsymbol \mu_{j} \text{쪽으로 이동} &amp; \text{if} &amp; P(w_i) \gt P(w_j) \\ \boldsymbol \mu_{i} \text{쪽으로 이동} &amp; \text{if} &amp; P(w_i) \lt P(w_j) \end{cases}\)<br/> (\(P(w_i)\) 와 \(P(w_j)\) 중 더 작은 쪽으로 이동)<br/> (\(\sigma^{2}\) 이 (\(\| \mu_{i} - \mu_{j} \|^2\) 에 비해 비교적) 작은 경우 \(P(w_i)\) 와 \(P(w_j)\) 에 따른 \(x_0\) shift는 미약)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/4-480.webp 480w,/assets/img/2024-09-10-Pattern/4-800.webp 800w,/assets/img/2024-09-10-Pattern/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Discriminant function for Gaussian PDF :<br/> likelihood \(P(x | w_i)\) 를 Gaussian PDF로 둘 경우,<br/> \(g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{d}{2} \text{ln}(2\pi) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\) <ul> <li>case 2) \(\Sigma_{i} = \Sigma\) (symmetric) (모든 classes에 대해 equal covariance) (비등방성(hyper-ellipsoidal))<br/> \(g_{i}(x) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) + \text{ln}P(w_i)\)<br/> \(i\) 와 관련된 term만 남기면<br/> \(g_{i}(x) = \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol x - \frac{1}{2} \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol \mu_{i} + \text{ln}P(w_i)\)<br/> \(= \boldsymbol w_i^T \boldsymbol x + \boldsymbol w_{i0}\) (linear) <ul> <li>decision boundary :<br/> hyperplane \(g(x) = g_{i}(x) - g_{j}(x) = (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T \Sigma^{-1} (\boldsymbol x - \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) + \frac{1}{(\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T \Sigma^{-1}} \text{ln}\frac{P(w_i)}{P(w_j)})\)<br/> \(= \boldsymbol w^T (\boldsymbol x - \boldsymbol x_0) = 0\)</li> <li>\(\boldsymbol x_0\) 를 지나고 \(\boldsymbol w = \Sigma^{-1} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})\) 에 수직인 hyperplane</li> <li>\(\boldsymbol x_0 = \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) - \frac{\text{ln}\frac{P(w_i)}{P(w_j)}}{(\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T \Sigma^{-1} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})\) 이므로<br/> 마찬가지로 \(\boldsymbol x_0\) 의 위치는 \(\boldsymbol \mu_{i}\) 와 \(\boldsymbol \mu_{j}\) 의 중점에서 \(P(w_i)\) 와 \(P(w_j)\) 중 더 작은 쪽으로 이동</li> <li>\(\boldsymbol w = \Sigma^{-1} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})\) 는<br/> vector \(\boldsymbol \mu_{i} - \boldsymbol \mu_{j}\) 를 \(\Sigma^{-1}\) 로 회전시킨 vector를 의미</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/5-480.webp 480w,/assets/img/2024-09-10-Pattern/5-800.webp 800w,/assets/img/2024-09-10-Pattern/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Discriminant function for Gaussian PDF :<br/> likelihood \(P(x | w_i)\) 를 Gaussian PDF로 둘 경우,<br/> \(g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{d}{2} \text{ln}(2\pi) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\) <ul> <li>case 2) \(\Sigma_{i}\) is arbitrary (symmetric) (class마다 covariance 다름) (비등방성(hyper-ellipsoidal))<br/> \(g_{i}(x) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\)<br/> \(\Sigma_{i}\) 가 \(i\) 에 대한 term이므로<br/> \(g_{i}(x) = - \frac{1}{2} \boldsymbol x^T \Sigma^{-1} \boldsymbol x + \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol x - \frac{1}{2} \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol \mu_{i} - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\)<br/> \(= - \frac{1}{2} \boldsymbol x^T \Sigma^{-1} \boldsymbol x + \boldsymbol w_i^T \boldsymbol x + \boldsymbol w_{i0}\) (quadratic) 는<br/> quadratic discriminant function in \(x\) <ul> <li>decision surface :<br/> hyperquadratic (hyperplane, hypersphere, hyperellipsoidal, hyperparaboloid, hyperhyperboloid)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/6-480.webp 480w,/assets/img/2024-09-10-Pattern/6-800.webp 800w,/assets/img/2024-09-10-Pattern/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="bayes-rule-for-discrete-case">Bayes Rule for Discrete Case</h3> <ul> <li> <table> <tbody> <tr> <td>pdf 적분 $$\int p(x</td> <td>w_j) dx$$ 대신</td> <td> </td> </tr> <tr> <td>확률 합 $$lim_{\Delta x \rightarrow 0} \Sigma_{k=-\infty}^{\infty} p(x_k</td> <td>w_j) \Delta x\(\)\rightarrow\(\)\Sigma_{k=1}^m P(v_k</td> <td>w_j)$$</td> </tr> </tbody> </table> </li> <li> <p>Bayes Decision Rule은 discrete case에서도 same<br/> Bayes risk minimize 위해 conditional risk \(R(\alpha_{i} | x)\) minimize<br/> (posterior maximize와 same)</p> </li> <li>\(\boldsymbol x = [x_1, x_2, \ldots, x_d]^T\) 에서 \(x_i\) 가 0 혹은 1의 값을 갖는 Bernoulli random var.일 때 <ul> <li>class \(w_1\) 일 때 :<br/> \(x_i \sim p_i^{x_i}(1-p_i)^{1-x_i}\)<br/> \(P(\boldsymbol x | w_1) = P([x_1, x_2, \ldots, x_d]^T | w_1) = \prod_{i=1}^d P(x_i | w_1) = \prod_{i=1}^d p_i^{x_i}(1-p_i)^{1-x_i}\)</li> <li>class \(w_2\) 일 때 :<br/> \(x_i \sim q_i^{x_i}(1-q_i)^{1-x_i}\)<br/> \(P(\boldsymbol x | w_2) = P([x_1, x_2, \ldots, x_d]^T | w_2) = \prod_{i=1}^d P(x_i | w_2) = \prod_{i=1}^d q_i^{x_i}(1-q_i)^{1-x_i}\)</li> <li>likelihood ratio :<br/> \(\frac{P(\boldsymbol x | w_1)}{P(\boldsymbol x | w_2)} = \prod_{i=1}^d (\frac{p_i}{q_i})^{x_i}(\frac{1-p_i}{1-q_i})^{1-x_i}\)</li> <li>discriminant function :<br/> choose \(w_1\)<br/> if \(g(x) = \text{ln} \frac{P(\boldsymbol x | w_1)P(w_1)}{P(\boldsymbol x | w_2)P(w_2)} = \sum_{i=1}^d(x_i \text{ln}\frac{p_i}{q_i} + (1-x_i)\text{ln}\frac{1-p_i}{1-q_i}) + \text{ln}\frac{P(w_1)}{P(w_2)} = \sum_{i=1}^d w_ix_i + w_0 = \boldsymbol w^T \boldsymbol x + w_0 \gt 0\)<br/> where \(w_i = \text{ln}\frac{p_i(1-q_i)}{q_i(1-p_i)}\) and \(w_0 = \sum_{i=1}^d(\text{ln}\frac{1-p_i}{1-q_i}) + \text{ln}\frac{P(w_1)}{P(w_2)}\) <ul> <li>case 1-1) \(p_i = q_i\)<br/> \(w_i = 0\) , so \(x_i\) 는 class 결정에 영향 없음</li> <li>case 1-2) \(p_i \gt q_i\)<br/> \(w_i \gt 0\) , so \(x_i = 1\) 은 class \(w_1\) 선택에 보탬</li> <li>case 1-3) \(p_i \lt q_i\)<br/> \(w_i \lt 0\) , so \(x_i = 1\) 은 class \(w_2\) 선택에 보탬</li> <li>case 2-1) \(P(w_1)\) 값 증가 (\(\gt P(w_2)\))<br/> \(w_0\) 값이 커지므로 class \(w_1\) 선택에 보탬</li> <li>case 2-2) \(P(w_1)\) 값 감소 (\(\lt P(w_2)\))<br/> \(w_0\) 값이 작아지므로 class \(w_1\) 선택에 보탬</li> </ul> </li> </ul> </li> </ul> <h2 id="chapter-2-linear-transformation">Chapter 2. Linear Transformation</h2> <h3 id="linear-transformation">Linear Transformation</h3> <ul> <li> \[y = A^Tx\] <ul> <li>mean and variance :<br/> \(\mu_{y} = A^T \mu_{x}\)<br/> \(\Sigma_{y} = E[(y - \mu_{y})(y - \mu_{y})^T] = A^T \Sigma_{x} A\)</li> <li>Mahalanobis distance :<br/> \(d_y^2 = (y - \mu_{y})^T\Sigma_{y}^{-1}(y - \mu_{y}) = \cdots = d_x^2\)<br/> <code class="language-plaintext highlighter-rouge">linear transformation</code>을 해도 Mahalanobis distance는 <code class="language-plaintext highlighter-rouge">그대로</code>임<br/> (Euclidean distance \((x - \mu_{x})^T(x - \mu_{x})\) 는 linear transformation을 하면 variant)</li> <li>Gaussian distribution :<br/> \(x \sim N(\mu_{x}, \Sigma_{x})\) 일 때<br/> \(P(y) = (2 \pi)^{- \frac{d}{2}} | \Sigma_{y} |^{-\frac{1}{2}} \exp(-\frac{1}{2}(y - \mu_{y})^T \Sigma_{y}^{-1} (y - \mu_{y})) = (2 \pi)^{- \frac{d}{2}} | A |^{-1} | \Sigma_{x} |^{-\frac{1}{2}} \exp(-\frac{1}{2}(x - \mu_{x})^T \Sigma_{x}^{-1} (x - \mu_{x})) = \frac{1}{|A|} P(x)\)</li> </ul> </li> </ul> <h3 id="orthonormal-transformation">Orthonormal Transformation</h3> <ul> <li>\(x = \sum_{i=1}^d y_i \phi_{i}\)<br/> where \(\{ \phi_{i}, \cdots, \phi_{d} \}\) is orthonormal basis<br/> Equivalently,<br/> \(y_i = x^T \phi_{i}\)<br/> where vector \(x\) 를 i-th eigenvector \(\phi_{i}\) 에 project한 게 \(y_i\) <ul> <li>approx. \(x\) : <ul> <li>\(\{ y_{m+1}, \cdots, y_{d} \}\) 를 pre-defined constants \(\{ b_{m+1}, \cdots, b_{d} \}\) 로 대체했을 때<br/> \(\hat x(m) = \sum_{i=1}^m y_i \phi_{i} + \sum_{i=m+1}^d b_i \phi_{i}\)</li> </ul> </li> <li>optimal \(b_i\) : <ul> <li>error \(\Delta x(m) = x - \hat x(m) = \sum_{i=m+1}^d (y_i - b_i) \phi_{i}\)<br/> MSE \(\bar \epsilon^{2}(m) = E[| \Delta x(m) |^2] = E[\Delta x^T(m) \Delta x(m)] = \sum_{i=m+1}^d E[(y_i - b_i)^2]\)</li> <li>orthonormal basis \(\phi_{i}, \phi_{j}\) 에 대해<br/> \(\frac{\partial}{\partial b_i} E[(y_i - b_i)^2] = -2(E[y_i] - b_i) = 0\) 이므로<br/> MSE 최소화하는 optimal \(b_i = E[y_i]\)</li> </ul> </li> <li>optimal \(\phi_{i}\) : <ul> <li>\(x = \sum_{j=1}^d y_j \phi_{j}\) 의 양변에 \(\phi_{i}^T\) 를 곱하면<br/> \(y_i = x^T \phi_{i}\) 이고<br/> optimal \(b_i = E[y_i]\) 이므로<br/> MSE \(\bar \epsilon^{2}(m) = \sum_{i=m+1}^d E[(y_i - b_i)^2] = \sum_{i=m+1}^d E[(x^T \phi_{i} - E[x^T \phi_{i}])^T(x^T \phi_{i} - E[x^T \phi_{i}])] = \sum_{i=m+1}^d \text{Var}(\phi_{i}^{T} x) = \sum_{i=m+1}^d \phi_{i}^T \Sigma_{x} \phi_{i}\)</li> <li>orthonormality equality constraint \(\phi_{i}^T\phi_{i} = \| \phi_{i} \| = 1\) 을 만족하면서 MSE \(\bar \epsilon^{2}(m)\) 를 최소화하는 \(\phi_{i}\) 는 Lagrange multiplier Method <a href="https://semyeong-yu.github.io/blog/2024/Lagrange/">Link</a> 로 찾을 수 있다<br/> \(\rightarrow\)<br/> goal : minimize \(U(m) = \sum_{i=m+1}^d \phi_{i}^T \Sigma_{x} \phi_{i} + \sum_{i=m+1}^d \lambda_{i}(1 - \phi_{i}^T\phi_{i})\)<br/> \(\frac{\partial}{\partial x}(x^TAx) = (A + A^T)x = 2Ax\) for symmetric \(A\) 이므로<br/> \(\frac{\partial}{\partial \phi_{i}} U(m) = 2(\Sigma_{x}\phi_{i} - \lambda_{i}\phi_{i}) = 0\) 이므로<br/> MSE 최소화하는 optimal \(\phi_{i}\) 는 \(\Sigma_{x}\phi_{i} = \lambda_{i}\phi_{i}\) 을 만족하므로<br/> \(\phi_{i}\) 와 \(\lambda_{i}\) 는 covariance matrix \(\Sigma_{x}\) 의 eigenvector and eigenvalue 이다</li> </ul> </li> </ul> </li> <li>Eigenvector and Eigenvalue : <ul> <li>\(\Sigma \Phi = \Phi \Lambda\) where \(\Phi \Phi^{T} = I\)</li> <li>If \(\Sigma\) is non-singular (\(| \Sigma | \neq 0\)),<br/> all eigenvalues \(\lambda\) are non-zero</li> <li>If \(\Sigma\) is positive-definite (\(x^T \Sigma x \geq 0\) for all \(x \neq 0\)),<br/> all eigenvalues \(\lambda\) are positive</li> <li>If \(\Sigma\) is real and symmetric,<br/> all eigenvalues \(\lambda\) are real<br/> and eigenvectors(w. distinct eigenvalues) are orthogonal <ul> <li>pf)<br/> \(\Sigma \phi_{i} = \lambda_{i} \phi_{i}\) and \(\Sigma \phi_{j} = \lambda_{j} \phi_{j}\)<br/> \(\phi_{j}^T \Sigma \phi_{i} - \phi_{i}^T \Sigma \phi_{j} = \phi_{j}^T \lambda_{i} \phi_{i} - \phi_{i}^T \lambda_{j} \phi_{j}\)<br/> \(0 = (\lambda_{i} - \lambda_{j}) \phi_{j}^T \phi_{i}\) since \(\Sigma\) is symmetric<br/> \(\rightarrow \phi_{j}^T \phi_{i} = 0\) (eigenvectors are orthogonal)</li> </ul> </li> </ul> </li> <li>Orthonormal Transformation :<br/> \(y = \Phi^{T} x\)<br/> for \(\Phi = [\phi_{1}, \cdots \phi_{d}]\) and \(\Phi \Phi^{T} = I\) <ul> <li>vector \(x\) 를 i-th eigenvector \(\phi_{i}\) 에 project한 게 \(y_{i}\)<br/> 즉, vector \(x\) 를 new coordinate \(\Phi = [\phi_{1}, \cdots \phi_{d}]\) 으로 나타낸 게 vector \(y\)</li> <li>eigenvector는 principal axis를 나타내고, eigenvalue는 해당 방향으로 퍼진 정도를 나타냄</li> <li>\(y\) 의 covariance matrix인 \(\Sigma_{y}\) 는 <code class="language-plaintext highlighter-rouge">diagonal matrix</code><br/> (uncorrelated random vector \(y\)) <ul> <li>\(\Sigma_{y}\)<br/> \(= \Phi^{T} \Sigma_{x} \Phi\)<br/> \(= \Phi^{T} \Phi \Lambda\) since \(\Sigma \Phi = \Phi \Lambda\)<br/> \(= \Phi^{-1} \Phi \Lambda\) since eigenvector matrix is orthogonal matrix (\(\Phi^{T} = \Phi^{-1}\))<br/> \(= \Lambda\)</li> </ul> </li> <li>distance : <ul> <li>Mahalanobis distance는 any linear transformation에 대해 보존됨</li> <li><code class="language-plaintext highlighter-rouge">Euclidean distance</code>는 linear transformation 중 orthonormal transformation일 때만 <code class="language-plaintext highlighter-rouge">보존</code>됨<br/> \(\| y \|^2 = y^Ty = x^T \Phi \Phi^{T} x = x^T \Phi \Phi^{-1} x = x^T x = \| x \|^2\)</li> </ul> </li> </ul> </li> </ul> <h3 id="whitening-transformation">Whitening Transformation</h3> <ul> <li>Whitening Transformation :<br/> \(y = \Lambda^{-\frac{1}{2}} \Phi^{T} x = (\Phi \Lambda^{-\frac{1}{2}})^T x\)<br/> (Orthonormal Transformation을 한 뒤 추가로 \(\Lambda^{-\frac{1}{2}}\) 로 transformation) <ul> <li>\(y\) 의 covariance matrix인 \(\Sigma_{y}\) 는 <code class="language-plaintext highlighter-rouge">identity matrix</code> \(I\) <ul> <li>\(\Sigma_{y}\)<br/> \(= (\Lambda^{-\frac{1}{2}} \Phi^{T}) \Sigma_{x} (\Phi \Lambda^{-\frac{1}{2}})\)<br/> \(= \Lambda^{-\frac{1}{2}} \Lambda \Lambda^{-\frac{1}{2}}\)<br/> \(= I\)</li> </ul> </li> <li>\(\Lambda^{-\frac{1}{2}}\) 은 principal components의 scale을 \(\frac{1}{\sqrt{\lambda_{i}}}\) 배 하는 효과</li> <li>Whitening Transformation을 한 번 하고나면,<br/> 그 후에 any Orthonormal Transformation(\(y = \Phi^{T} x\) for \(\Psi \Psi^{T} = I\))을 해도<br/> covariance matrix는 항상 \(\Psi I \Psi^{T} = I\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/7-480.webp 480w,/assets/img/2024-09-10-Pattern/7-800.webp 800w,/assets/img/2024-09-10-Pattern/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="sample-separation">Sample Separation</h3> <ul> <li>Sample Separation :<br/> uncorrelated normal samples \(\sim N(0, I)\) 로부터 correlated sample \(\sim N(\mu_{x}, \Sigma_{x})\) 만들기 <ul> <li>How? :<br/> given data \(x\) 에서 \(\mu_{x}\) 를 뺀 뒤 Whitening Transformation 적용하면 \(N(0, I)\) 이므로 이 과정을 역으로 실행</li> <li>Step 1) Normal distribution으로부터 N개의 \(d\) -dim. independent vectors를 sampling<br/> \(y_1, y_2, \cdots, y_N \sim N(0, I)\)</li> <li>Step 2) Inverse-Whitening-Transformation 적용하여 Normal distribution을 x-space로 변환 \(x_k = \Phi \Lambda^{\frac{1}{2}} y_k\)<br/> for given \(\Sigma_{x}\)<br/> and its eigen-decomposition \(\Sigma_{x} \Phi = \Phi \Lambda\)</li> <li>Step 3) x-space의 samples에 \(\mu_{x}\) 더함<br/> \(x_k = \Phi \Lambda^{\frac{1}{2}} y_k + \mu_{x} \sim N(\mu_{x}, \Sigma_{x})\)<br/> for given \(\mu_{x}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/8-480.webp 480w,/assets/img/2024-09-10-Pattern/8-800.webp 800w,/assets/img/2024-09-10-Pattern/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="chapter-3-maximum-likelihood-and-bayesian-parameter-estimation">Chapter 3. Maximum-likelihood and Bayesian Parameter Estimation</h2> <ul> <li>parameter estimation : <ul> <li>Maximum Likelihood Estimation (MLE) :<br/> (true) parameters are <code class="language-plaintext highlighter-rouge">unknown</code>, but <code class="language-plaintext highlighter-rouge">fixed</code><br/> estimators are random variable</li> <li>Bayesian Estimation :<br/> parameters are <code class="language-plaintext highlighter-rouge">random variables</code> and <code class="language-plaintext highlighter-rouge">prior is known</code></li> </ul> </li> </ul> <h3 id="maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</h3> <ul> <li> <p>Assumption :<br/> training data \(D_j\) \(\sim\) likelihood \(p(D_j | w_j) = N(\mu_{j}, \Sigma_{j})\)<br/> (i.i.d random samples)</p> </li> <li>MLE : <ul> <li>likelihood :<br/> \(\hat \theta = \text{argmax}_{\theta} p(D=\{ x_1, x_2, \ldots, x_n \} | \theta) = \text{argmax}_{\theta} \prod_{k=1}^n p(x_k | \theta)\)</li> <li>log-likelihood :<br/> \(\hat \theta = \text{argmax}_{\theta} p(D=\{ x_1, x_2, \ldots, x_n \} | \theta) = \text{argmax}_{\theta} \sum_{k=1}^n \text{ln} p(x_k | \theta)\)</li> </ul> </li> <li>Gaussian likelihood : <ul> <li>unknown \(\mu\) : <ul> <li>likelihood :<br/> \(p(x_k | \mu) = (2 \pi)^{-\frac{d}{2}} | \Sigma |^{-\frac{1}{2}} \text{exp}(-\frac{1}{2}(x_k - \mu)^T \Sigma^{-1} (x_k - \mu))\)<br/> \(p(D=\{ x_1, x_2, \ldots, x_N \} | \mu) = \prod_{k=1}^N p(x_k | \mu) = (2 \pi)^{-\frac{dN}{2}} | \Sigma |^{-\frac{N}{2}} \text{exp}(-\frac{1}{2} \sum_{k=1}^N (x_k - \mu)^T \Sigma^{-1} (x_k - \mu))\)</li> <li>log-likelihood :<br/> \(\text{ln} p(D | \mu) = -\frac{dN}{2} \text{ln}(2 \pi) -\frac{N}{2} \text{ln} | \Sigma | -\frac{1}{2} \sum_{k=1}^N (x_k - \mu)^T \Sigma^{-1} (x_k - \mu)\)</li> <li>matrix derivative :<br/> \(\frac{d}{dx}(Ax) = A\)<br/> \(\frac{d}{dx}(y^TAx) = A^Ty\)<br/> \(\frac{d}{dx}(x^TAx) = (A+A^T)x\)<br/> \(\frac{d}{dA}(x^TAx) = xx^T\)<br/> \(\frac{\partial |A|}{\partial A} = (\text{adj}(A))^T = |A|(A^{-1})^T\)<br/> \(\frac{\partial \text{ln}|A|}{\partial A} = (A^{-1})^T = (A^T)^{-1}\) where \(|A| = \frac{1}{|A^{-1}|}\)</li> <li>MLE problem :<br/> \(\nabla_{\mu} \text{ln} p(D | \mu) = -\frac{1}{2} \sum_{k=1}^N ((\Sigma^{-1} + (\Sigma^{-1})^T) (x_k - \mu)) \times (-1) = (\Sigma^{-1} + (\Sigma^{-1})^T)(\sum_{k=1}^N x_k - \sum_{k=1}^N \mu) = 0\)<br/> \(\sum_{k=1}^N x_k - N \mu = 0\)<br/> \(\hat \mu_{\text{MLE}} = \frac{1}{N} \sum_{k=1}^N x_k\)</li> <li>Summary : <ul> <li>\(\hat \mu_{\text{MLE}} = \frac{1}{N} \sum_{k=1}^N x_k\)<br/> (true mean의 MLE estimator는 sample mean)</li> <li>\(E[\hat \mu_{\text{MLE}}] = \mu\)<br/> (\(\hat \mu_{\text{MLE}}\) 는 <code class="language-plaintext highlighter-rouge">unbiased</code> estimator)</li> </ul> </li> </ul> </li> <li>unknown \(\mu\) and \(\Sigma\) : <ul> <li>log-likelihood :<br/> \(\text{ln} p(D | \mu) = -\frac{dN}{2} \text{ln}(2 \pi) -\frac{N}{2} \text{ln} | \Sigma | -\frac{1}{2} \sum_{k=1}^N (x_k - \mu)^T \Sigma^{-1} (x_k - \mu) = -\frac{dN}{2} \text{ln}(2 \pi) + \frac{N}{2} \text{ln} | \Sigma^{-1} | -\frac{1}{2} \sum_{k=1}^N (x_k - \mu)^T \Sigma^{-1} (x_k - \mu)\)</li> <li>MLE problem :<br/> \(\nabla_{\Sigma^{-1}} \text{ln} p(D | \mu) = \frac{N}{2}\Sigma^{T} - \frac{1}{2} \sum_{k=1}^N (x_k - \mu)(x_k - \mu)^T = 0\)<br/> \(N \Sigma^{T} = \sum_{k=1}^N (x_k - \mu)(x_k - \mu)^T\)<br/> \(\mu = \hat \mu_{\text{MLE}}\) 대입하고, \(\Sigma\) 는 symmetric(\(\Sigma^{T} = \Sigma\))하므로<br/> \(\hat \Sigma_{\text{MLE}} = \frac{1}{N} \sum_{k=1}^N (x_k - \hat \mu_{\text{MLE}})(x_k - \hat \mu_{\text{MLE}})^T\)</li> <li>Summary : <ul> <li>\(\hat \Sigma_{\text{MLE}} = \frac{1}{N} \sum_{k=1}^N (x_k - \hat \mu_{\text{MLE}})(x_k - \hat \mu_{\text{MLE}})^T\)<br/> (\(\mu\) 먼저 estimate한 뒤 \(\Sigma\) estimate)</li> <li>\(E[\hat \Sigma_{\text{MLE}}] = \frac{1}{N} E[\sum_{k=1}^N (x_k - \hat \mu_{\text{MLE}})(x_k - \hat \mu_{\text{MLE}})^T] = \frac{N-1}{N} \Sigma \neq \Sigma\)<br/> (\(\hat \Sigma_{\text{MLE}}\) 는 <code class="language-plaintext highlighter-rouge">biased</code> estimator) <ul> <li>pf) 아래 식 이용<br/> \(E[x_i x_j^T] = \begin{cases} \Sigma + \mu \mu^{T} &amp; \text{if} &amp; i = j \\ \mu \mu^{T} &amp; \text{if} &amp; i \neq j \end{cases}\)<br/> since \(\Sigma = E[(x - \mu)(x - \mu)^T] = \cdots = E[xx^T] - \mu \mu^{T}\)<br/> since \(0 = E[(x_i - \mu)(x_j - \mu)^T] = E[x_i x_j^T] - \mu \mu^{T}\) by independence \(i \neq j\)</li> </ul> </li> <li>\(\text{lim}_{N \rightarrow \infty}E[\hat \Sigma_{\text{MLE}}] = \text{lim}_{N \rightarrow \infty} \frac{N-1}{N} \Sigma = \Sigma\)<br/> (\(\hat \Sigma_{\text{MLE}}\) 는 <code class="language-plaintext highlighter-rouge">asymptotically unbiased</code> estimator)<br/> 또는<br/> \(\hat \Sigma_{\text{MLE}} = \frac{1}{N-1} \sum_{k=1}^N (x_k - \hat \mu_{\text{MLE}})(x_k - \hat \mu_{\text{MLE}})^T\)<br/> (위처럼 설정하면 \(\hat \Sigma_{\text{MLE}}\) 는 <code class="language-plaintext highlighter-rouge">unbiased</code> estimator)</li> </ul> </li> </ul> </li> </ul> </li> <li>MLE : <ul> <li>MLE is <code class="language-plaintext highlighter-rouge">asymptotically consistent</code><br/> if \(\text{lim}_{N \rightarrow \infty} P(\| \hat \theta_{\text{MLE}} - \theta_{\text{true}} \| \leq \epsilon) = 1\) for arbitrary small \(\epsilon\)<br/> (sample 수 \(N\) 이 크면 param. estimate은 true value랑 거의 비슷)<br/> by central limit theorem and the fact that MLE is related to the sum of random var.</li> <li>MLE is <code class="language-plaintext highlighter-rouge">asymptotically efficient</code><br/> since MLE는 Cramer-Rao lower bound(any estimate이 달성할 수 있는 the lowest value of variance)</li> </ul> </li> </ul> <p>17p</p>]]></content><author><name></name></author><category term="cv-tasks"/><category term="3d"/><category term="rendering"/><summary type="html"><![CDATA[Lecture Summary (24F)]]></summary></entry><entry><title type="html">Nabla (Del) operator</title><link href="https://semyeong-yu.github.io/blog/2024/nabla/" rel="alternate" type="text/html" title="Nabla (Del) operator"/><published>2024-09-06T11:00:00+00:00</published><updated>2024-09-06T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/nabla</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/nabla/"><![CDATA[<p>본 포스팅 출처 : <a href="https://xoft.tistory.com/71">Link</a></p> <h3 id="del">Del</h3> <ul> <li>\(\nabla = \frac{\partial}{\partial x}i + \frac{\partial}{\partial y}j\)<br/> where \(\nabla\) : vector</li> <li>Del operator의 피연산자가 scalar인지 vector인지에 따라 다르게 불림</li> </ul> <h3 id="gradient">Gradient</h3> <ul> <li>\(\nabla f = \frac{\partial f}{\partial x}i + \frac{\partial f}{\partial y}j\)<br/> where \(f\) : scalar field<br/> where \(\nabla f\) : vector field</li> <li>scalar 함수 각 점에서의 방향</li> </ul> <h3 id="divergence">Divergence</h3> <ul> <li>\(\nabla f = \nabla \cdot f = (\frac{\partial}{\partial x}i + \frac{\partial}{\partial y}j) \cdot (v_x i + v_y j) = \frac{\partial v_x}{\partial x} + \frac{\partial v_y}{\partial y}\)<br/> where \(f = v_x i + v_y j\) : vector field<br/> where \(\nabla f\) : scalar field</li> <li>vector 함수 각 점에서의 발산하는 크기</li> </ul> <h3 id="curl">Curl</h3> <ul> <li>\(\nabla \times f = \frac{\partial v_x}{\partial x}i + \frac{\partial v_y}{\partial y}j\)<br/> where \(f = v_x i + v_y j\) : vector field<br/> where \(\nabla f\) : scalar field</li> <li>점의 rotation</li> </ul> <h3 id="laplacian">Laplacian</h3> <ul> <li>\(\Delta = \nabla \cdot \nabla = \text{Divergence} \cdot \text{Gradient} = \frac{\partial^{2}}{\partial x} + \frac{\partial^{2}}{\partial y}\)<br/> where \(\Delta\) : scalar (Divergence of Gradient)</li> <li>image에 Laplacian filter를 쓰면<br/> Gradient로 색상이 급격히 변하는 vector를 검출한 뒤<br/> Divergence로 vector의 발산 크기 균일 정도를 파악하여<br/> Edge를 검출할 수 있음</li> </ul>]]></content><author><name></name></author><category term="math"/><category term="nabla"/><category term="del"/><category term="scalar"/><category term="vector"/><summary type="html"><![CDATA[del, gradient, divergence, curl, laplacian]]></summary></entry><entry><title type="html">SuGaR</title><link href="https://semyeong-yu.github.io/blog/2024/SuGaR/" rel="alternate" type="text/html" title="SuGaR"/><published>2024-09-05T11:00:00+00:00</published><updated>2024-09-05T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/SuGaR</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/SuGaR/"><![CDATA[<h2 id="sugar-surface-aligned-gaussian-splatting-for-efficient-3d-mesh-reconstruction-and-high-quality-mesh-rendering">SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering</h2> <h4 id="antoine-guédon-vincent-lepetit">Antoine Guédon, Vincent Lepetit</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2311.12775">https://arxiv.org/abs/2311.12775</a><br/> project website :<br/> <a href="https://anttwo.github.io/sugar/">https://anttwo.github.io/sugar/</a><br/> code :<br/> <a href="https://github.com/Anttwo/SuGaR">https://github.com/Anttwo/SuGaR</a><br/> reference :<br/> NeRF and 3DGS Study</p> </blockquote> <h2 id="abstract">Abstract</h2> <ul> <li> <p>surface 점 sampling :<br/> surface 근처의 점 \(p\) 를<br/> <code class="language-plaintext highlighter-rouge">Gaussians의 곱 분포</code>로 sampling</p> </li> <li> <p>regularization term :<br/> 3DGS가 surface 잘 나타내도록 (well-distributed) 하기 위해<br/> <code class="language-plaintext highlighter-rouge">density</code> function 또는 <code class="language-plaintext highlighter-rouge">SDF</code>로 <code class="language-plaintext highlighter-rouge">regularization</code> loss term</p> </li> <li> <p>obtain mesh using level set points :<br/> 점 \(p\) 주위(\(3 \sigma (v)\))의 points를 sampling하고<br/> density 계산하여 oriented <code class="language-plaintext highlighter-rouge">level set points</code> 구한 뒤<br/> Poisson equation으로 <code class="language-plaintext highlighter-rouge">mesh</code> 구함</p> </li> <li> <p>mesh refinement :<br/> triangle mesh에 new Gaussians binding하여<br/> mesh optimize할 때 new Gaussians도 함께 optimize</p> </li> </ul> <h2 id="surface-aligned-3dgs">Surface-Aligned 3DGS</h2> <h3 id="regularization">Regularization</h3> <ul> <li> <p>문제 :<br/> 3DGS는 <code class="language-plaintext highlighter-rouge">unstructured</code><br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">surface</code> 나타내지 않음</p> </li> <li> <p>해결 :<br/> <code class="language-plaintext highlighter-rouge">regularization</code> loss term<br/> \(\rightarrow\) 3DGS가 well-distributed and aligned with surface (flat)</p> <ul> <li>well-distributed : <ul> <li>Gaussians끼리 <code class="language-plaintext highlighter-rouge">overlap 적음</code></li> <li>(surface에 가까운) point \(p\) 와 <code class="language-plaintext highlighter-rouge">가장 가까운 Gaussian</code> \(g^{\ast}\) 가 다른 Gaussians보다 \(p\) 의 <code class="language-plaintext highlighter-rouge">density에 훨씬 많이 기여</code><br/> \(g^{\ast} = \text{argmin}_{g}(p - \mu_{g})^T \Sigma_{g}^{-1}(p - \mu_{g})\)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/1-480.webp 480w,/assets/img/2024-09-05-SuGaR/1-800.webp 800w,/assets/img/2024-09-05-SuGaR/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>surface 근처의 점 sampling : <ul> <li>assumption :<br/> 거의 surface 위에 있다고 볼 수 있을 정도로 아주 가까운<br/> <code class="language-plaintext highlighter-rouge">surface 근처</code>의 \(p\) 를 <code class="language-plaintext highlighter-rouge">Gaussian들의 곱 분포로 sampling</code><br/> \(p \sim \prod_{g} N(\cdot; \mu_{g}, \Sigma{g})\) <ul> <li>‘3DGS가 잘 학습됐다면’ small Gaussians는 surface에 아주 가까운 점들의 확률처럼 생각할 수 있고,<br/> Gaussian이 작을수록 sampling이 중심에 집중되므로<br/> 그 small Gaussians의 곱이 나타내는 분포는 surface 근처의 좁은 영역에 집중된 분포를 나타낼 것이고,<br/> 이로부터 sampling한 점 \(p\) 는 실제 object surface에 가까울 확률이 높다는 가정</li> <li>이렇게 sampling한 points는 regularization term에 대해 high gradient를 가지는 부분임</li> </ul> </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">density function</code> : <ul> <li>\(d(p) = \sum_{g} \alpha_{g} \text{exp}(-\frac{1}{2}(p - \mu_{g})^T \Sigma_{g}^{-1}(p - \mu_{g}))\)<br/> where \(\text{exp}(-\frac{1}{2}(p - \mu_{g})^T \Sigma_{g}^{-1}(p - \mu_{g}))\) : posterior<br/> (점 \(p\) 에 더 가까운 Gaussian의 \(\alpha_{g}\) 가 \(p\) 의 density에 더 많이 기여)<br/> where \((p - \mu_{g})^T \Sigma_{g}^{-1}(p - \mu_{g})\) : <code class="language-plaintext highlighter-rouge">Mahalanobis distance</code><br/> (\(p\) 가 Gaussian distribution 평균 \(\mu_{g}\) 에서 “상대적으로” 얼마나 떨어져 있는지)<br/> (\(p\) 가 평균으로부터 같은 거리만큼 떨어져있더라도 convariance가 작은 방향에 있을수록 Mahalanobis distance가 커짐)</li> <li>approx. ideal density function \(\bar d(p) \in [0, 1]\) : <ul> <li>가정 1) well-distributed Gaussians by regularization term 이므로<br/> overlap 없다는 전제 하에 <code class="language-plaintext highlighter-rouge">하나</code>의 Gaussian \(g^{\ast}\) 가 point \(p\) 의 density 결정</li> <li>가정 2) Gaussians가 진짜 surface를 묘사하려면 <code class="language-plaintext highlighter-rouge">semi-transparent하지 않아야</code> 좋음<br/> \(\rightarrow\) \(a_{g} = 1\) for any Gaussians</li> <li>위의 가정과 아래 수식 유도(<strong>Approximation of Density function</strong>) 에 따르면<br/> \((p - \mu_{g})^T \Sigma_{g}^{-1}(p - \mu_{g}) \approx \frac{1}{s_{g}^2} \langle p-\mu_{g}, n_g \rangle^{2}\) 이고<br/> 근사해서 구한 ideal density function은<br/> \(\bar d(p) = \text{exp}(-\frac{1}{2s_{g^{\ast}}^2} \langle p-\mu_{g^{\ast}}, n_{g^{\ast}} \rangle^{2})\)<br/> where \(g^{\ast} = \text{argmin}_{g}(p - \mu_{g})^T \Sigma_{g}^{-1}(p - \mu_{g})\)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/2-480.webp 480w,/assets/img/2024-09-05-SuGaR/2-800.webp 800w,/assets/img/2024-09-05-SuGaR/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/3-480.webp 480w,/assets/img/2024-09-05-SuGaR/3-800.webp 800w,/assets/img/2024-09-05-SuGaR/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Regularization on density</code> : <ul> <li> \[R = | d(p) - \bar d(p) |\] <ul> <li>\(d\) : <code class="language-plaintext highlighter-rouge">density</code> function</li> <li>\(\bar d\) : approx. <code class="language-plaintext highlighter-rouge">ideal density</code> function<br/> where 하나의 불투명한 Gaussian이 point density 결정</li> </ul> </li> <li>근데 density function \(d\) 로 regularize하면 아래의 문제가 있다 <ul> <li>\(d\) 는 exponential term으로 이루어져 있으므로 scale이 너무 커서 optimization에 별로다</li> <li>approx. ideal density function을 구할 때 flat Gaussian으로 surface를 나타내는 게 목적이라고 가정하였는데,<br/> Gaussian이 완전히 flat 하면 \(s_{g} = 0\) 이 되어 \(\bar d(p) \rightarrow 0\) 이므로<br/> 모든 level set (표면)이 \(\mu_{g}\) 를 지나고 normal \(n_{g}\) 를 가지는 2D 상의 plane이 되어<br/> level sets 고려하는 게 무의미해진다<br/> 따라서 surface를 나타내기 위해 flat하게 Gaussian을 만드는 게 목적이지만<br/> 그렇다고 완전히 flat하면 안 됨</li> </ul> </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Regularization on SDF</code> : <ul> <li>density function 말고 <code class="language-plaintext highlighter-rouge">SDF</code> <a href="https://semyeong-yu.github.io/blog/2024/SDF/">Link</a> 로 loss 만들면 optimization 더 잘 됨<br/> (Gaussians가 surface에 더 잘 align됨)<br/> \(R = \frac{1}{| P |} \sum_{p \in P} | \hat f(p) - f(p) |\) <ul> <li>\(f(p) = \langle p-\mu_{g^{\ast}}, n_{g^{\ast}} \rangle = \pm s_{g^{\ast}} \sqrt{-2log(\bar d(p))}\) :<br/> <code class="language-plaintext highlighter-rouge">ideal distance</code> (SDF) b.w. point \(p\) and true surface<br/> (\(\bar d(p) = 1\) 이면, 즉 SDF \(f(p) = 0\) (zero level-set)이면, true surface를 나타냄)</li> <li>\(\hat f(p)\) :<br/> <code class="language-plaintext highlighter-rouge">estimated distance</code> b.w. point \(p\) and depth at projection of \(p\)<br/> (\(f(p)\) 를 직접 계산하는 건 빡세므로 training view-points에 대해 Radix Sort로 Gaussian rasterize할 때 사용한 Gaussian depth 값들을 rendering하여 depth map을 만들어서 estimated \(\hat f(p)\) 구함)</li> </ul> </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Regularization on normal</code> vector : <ul> <li>normal vector의 방향 \(n_{g}\) 을 SDF gradient 방향으로 맞춰주기 위해<br/> (normal vector 방향을 잘 잡아줘야 surface에 잘 align됨)<br/> \(R_{Norm} = \frac{1}{| P |} \sum_{p \in P} \| \frac{\nabla f(p)}{\| \nabla f(p) \|} - n_{g^{\ast}} \|^2\)</li> </ul> </li> </ul> <h3 id="approximation-of-density-function">Approximation of Density function</h3> <ul> <li> <p>density function이 실제 surface를 잘 나타낸다면<br/> \(p\) 에 가장 기여가 큰 Gaussian이 surface에 align되어 flat해야 한다<br/> 이 때, <code class="language-plaintext highlighter-rouge">flat Gaussian</code>의 경우 Mahalanobis distance의 주 요인은 <code class="language-plaintext highlighter-rouge">covariance의 가장 짧은 축</code> \(s_{g}\) 이므로<br/> 아래와 같이 approx. ideal density function 식을 유도할 수 있다</p> </li> <li> <p>\(\bar d(p) = \text{exp}(-\frac{1}{2s_{g^{\ast}}^2} \langle p-\mu_{g^{\ast}}, n_{g^{\ast}} \rangle^{2})\) 유도 TBD <code class="language-plaintext highlighter-rouge">????</code><br/> Eigendecomposition을 하면 \(\Sigma_{g} = Q \Lambda Q^T\)<br/> where \(s_g\) : convariance가 가장 작은 방향의 vector<br/> where \(n_g = \frac{s_g}{\| s_g \|}\)</p> </li> </ul> <h2 id="mesh-reconstruction">Mesh reconstruction</h2> <h3 id="obtain-mesh">Obtain Mesh</h3> <ul> <li> <p>문제 :<br/> <code class="language-plaintext highlighter-rouge">Densification</code>을 거치면 3DGS 수가 너무 <code class="language-plaintext highlighter-rouge">많아</code>지고 너무 <code class="language-plaintext highlighter-rouge">작아</code>져서<br/> texture나 detail을 나타내기 힘듦<br/> \(\rightarrow\) 거의 모든 곳에서 density function \(d = 0\) 이고,<br/> 위에서 언급했듯이 level sets 고려하는 게 의미가 없어져서<br/> Marching Cubes 기법으로 이러한 <code class="language-plaintext highlighter-rouge">sparse density function</code>의 <code class="language-plaintext highlighter-rouge">level sets</code>를 추출하기 어렵</p> </li> <li> <p>해결 :</p> <ul> <li>과정 1)<br/> Gaussians로 계산한 density function level set 상의 <code class="language-plaintext highlighter-rouge">visible</code> part에 대해 3D <code class="language-plaintext highlighter-rouge">point sampling</code><br/> \(n\) 개의 3D points \(\{ p + t_i v_i \}_{i=1}^n\) sampling<br/> where \(p\) : depth map에 따른 3D point<br/> where \(t_i \in [-3 \sigma_{g}(v), 3\sigma_{g}(v)]\) (visible part)<br/> where \(v_i\) : ray direction</li> <li>과정 2)<br/> \(d_i = d(p + t_i v_i) = \sum_{g} \alpha_{g} \text{exp}(-\frac{1}{2}((p + t_i v_i) - \mu_{g})^T \Sigma_{g}^{-1}((p + t_i v_i) - \mu_{g}))\) 로<br/> <code class="language-plaintext highlighter-rouge">density 계산</code>한 뒤 level parameter \(\lambda\) 에 대해<br/> \(d_i \lt \lambda \lt d_j\) 이면,<br/> range \([d_i, d_j]\) 안에 <code class="language-plaintext highlighter-rouge">level set point</code> 있다고 판단<br/> (아! 그 범위 안에 표면 위의 점이 있구나!)</li> <li>과정 3)<br/> 해당 level set points와 normals (oriented 3d point clouds \(\vec V\))를 이용하여<br/> <code class="language-plaintext highlighter-rouge">Poisson reconstruction</code>으로 surface <code class="language-plaintext highlighter-rouge">mesh</code> 얻음<br/> (아래에서 설명)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/4-480.webp 480w,/assets/img/2024-09-05-SuGaR/4-800.webp 800w,/assets/img/2024-09-05-SuGaR/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="mesh-by-poisson-reconstruction">Mesh by Poisson Reconstruction</h3> <ul> <li> <p>Poisson surface reconstruction :<br/> <code class="language-plaintext highlighter-rouge">3D Point Clouds</code>를 <code class="language-plaintext highlighter-rouge">3D Mesh</code>로 변환하는 고전적인 방법 (출처 : <a href="https://xoft.tistory.com/72">Link</a>)</p> </li> <li> <p>Let indicator function \(\chi_{M}(p) = \begin{cases} 1 &amp; \text{if} &amp; p \in M \\ 0 &amp; \text{if} &amp; p \notin M \end{cases}\)<br/> where \(M\) : object mesh 내부</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/5-480.webp 480w,/assets/img/2024-09-05-SuGaR/5-800.webp 800w,/assets/img/2024-09-05-SuGaR/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>주어진 <code class="language-plaintext highlighter-rouge">oriented 3d point clouds</code> \(\vec V\) 를 approx.하는 <code class="language-plaintext highlighter-rouge">indicator gradient</code> \(\nabla \chi\) 를 찾아야 한다<br/> 이를 풀기 위해 Possion Equation을 사용하자 <ul> <li><code class="language-plaintext highlighter-rouge">Possion Equation</code> :<br/> \(\nabla^{2} \phi = f\)<br/> where \(\nabla = (\frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z})\)<br/> \(\rightarrow\)<br/> \((\frac{\partial^{2}}{\partial x^2}+\frac{\partial^{2}}{\partial y^2}+\frac{\partial^{2}}{\partial z^2}) \phi (x, y, z) = f(x, y, z)\)<br/> 여기서 scalar field \(f\) 가 주어지면,<br/> scalar field \(\phi\) 를 찾을 수 있다</li> <li>\(\nabla \chi \approx \vec V\) 원하는 상황인데<br/> 양변에 divergence를 취하면<br/> \(\nabla \cdot \nabla \chi = \nabla \cdot \vec V\) 은 Poisson Equation 꼴이므로<br/> \(\nabla \cdot \vec V\) 를 알면 \(\chi\) 를 알 수 있다</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/6-480.webp 480w,/assets/img/2024-09-05-SuGaR/6-800.webp 800w,/assets/img/2024-09-05-SuGaR/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Implementation : <ul> <li>oriented 3d point clouds가 주어지면<br/> 모든 points를 포함하는 큰 육면체를 만들고<br/> 이를 <code class="language-plaintext highlighter-rouge">Octree</code> (육면체를 8등분하는 tree)를 사용하여 분할 (Fig 1.)</li> <li>input point clouds \(\vec V\) 는 주변 octree들의 합으로 설계하고,<br/> octree node의 depth는 Gaussian의 variance로 설계하여<br/> input point clouds 근처의 octreee들을 Gaussian으로 표현하면<br/> vector field \(\vec V\) (Fig 2.) 를 얻을 수 있다</li> <li>각 차원을 편미분 (Divergence)하면 scalar field \(\nabla \cdot \vec V\) (Fig 3.)를 얻을 수 있고,<br/> Poisson equation \(\nabla \cdot \nabla \chi = \nabla \cdot \vec V\) 에 의해<br/> indicator function \(\chi\) 도 알 수 있다<br/> octree의 깊이 별로 각 node의 \(\nabla \vec V\) 값 (Fig 3.)과 \(\nabla \nabla \chi\) 값 (Fig 4.)의 차이를 최소화함으로써 indicator function \(\chi\) 를 구한다</li> <li>mesh화 : input point clouds를 indicator function \(\chi\) 의 입력으로 넣어서 나온 결과값들을 평균 내고, 이 값과 같은 값을 출력하는 좌표들을 surface로 간주 (Fig 5.)하여 Marching Cube 알고리즘으로 mesh 생성<br/> (Octree Node마다 Marching Cube Polygon 생성)<br/> (여러 fine Octree Node가 하나의 coarse Octree Node를 공유할 때 생기는 문제를 해결하기 위해 fine Octree Node 면의 부분을 coarse한 면으로 projection하는 방법 사용)</li> <li>octree 깊이가 깊어질수록 시간과 memory를 많이 잡아먹긴 하지만, recon.하는 mesh 수가 더 많아서 mesh fine detail을 살릴 수 있음 (Fig 6.)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/7-480.webp 480w,/assets/img/2024-09-05-SuGaR/7-800.webp 800w,/assets/img/2024-09-05-SuGaR/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 1. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/8-480.webp 480w,/assets/img/2024-09-05-SuGaR/8-800.webp 800w,/assets/img/2024-09-05-SuGaR/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 2. vector field (oriented point clouds) </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/10-480.webp 480w,/assets/img/2024-09-05-SuGaR/10-800.webp 800w,/assets/img/2024-09-05-SuGaR/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 3. scalar field (divergence of oriented point clouds) </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/9-480.webp 480w,/assets/img/2024-09-05-SuGaR/9-800.webp 800w,/assets/img/2024-09-05-SuGaR/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 4. scalar field (laplacian of indicator function) </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/11-480.webp 480w,/assets/img/2024-09-05-SuGaR/11-800.webp 800w,/assets/img/2024-09-05-SuGaR/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 5. surface mesh </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/12-480.webp 480w,/assets/img/2024-09-05-SuGaR/12-800.webp 800w,/assets/img/2024-09-05-SuGaR/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 6. octree 깊이에 따른 mesh recon. 비교 </div> <h2 id="refine-mesh">Refine Mesh</h2> <h3 id="refine-mesh-by-gaussians">Refine Mesh by Gaussians</h3> <ul> <li> <p>문제 :<br/> Poisson reconstruction으로 구한 mesh만 사용하면 rendering quality가 좋지 않음</p> </li> <li> <p>해결 :<br/> 새로 sampling한 new Gaussians를 (triangle) mesh에 binding하고,<br/> 해당 <code class="language-plaintext highlighter-rouge">Gaussians</code>과 <code class="language-plaintext highlighter-rouge">mesh</code>를 GS rasterizer로 <code class="language-plaintext highlighter-rouge">함께 optimize</code></p> <ul> <li>과정 1)<br/> mesh surface 상에서 <code class="language-plaintext highlighter-rouge">triangle</code> 당 \(n\) 개의 <code class="language-plaintext highlighter-rouge">new thin 3D Gaussians를 sampling</code>하여<br/> Gaussians를 triangle에 bind</li> <li>과정 2)<br/> mesh vertices in barycentric coordinate (무게중심 좌표계) 이용해서<br/> 각 Gaussian의 mean을 explicitly 계산할 수 있음<br/> (barycentric coordinate : 삼각형 내부의 점을 세 꼭짓점의 가중치로 표현)</li> <li>과정 3)<br/> Gaussians를 mesh triangle에 aligned되도록 flat하게 유지하기 위해<br/> each Gaussian은 2개의 learnable scaling factor \(s_x, s_y\) 와 1개의 learnable 2D quaternion \(q=a+bi\) 을 가지고 있음<br/> (Gaussians optimize하여 mesh optimze될 때 new thin Gaussians도 함께 optimize)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/13-480.webp 480w,/assets/img/2024-09-05-SuGaR/13-800.webp 800w,/assets/img/2024-09-05-SuGaR/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="code-flow">Code Flow</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/14-480.webp 480w,/assets/img/2024-09-05-SuGaR/14-800.webp 800w,/assets/img/2024-09-05-SuGaR/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 출처 : NeRF and 3DGS Study </div> <h2 id="question">Question</h2> <ul> <li>Q1 : well-distributed 가정을 따르는 approx. ideal density function을 직접 구해서 이를 density function과 비교하는데, GT 역할을 하는 approx. ideal density function이, 변하는 learnable Gaussian으로 구한 것이어도 학습이 안정적임?</li> <li>A1 : TBD</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="gaussian"/><category term="splatting"/><category term="rendering"/><category term="surface"/><summary type="html"><![CDATA[Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering (CVPR 2024)]]></summary></entry><entry><title type="html">CLIP</title><link href="https://semyeong-yu.github.io/blog/2024/CLIP/" rel="alternate" type="text/html" title="CLIP"/><published>2024-09-03T11:00:00+00:00</published><updated>2024-09-03T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/CLIP</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/CLIP/"><![CDATA[<h2 id="clip-contrastive-language-image-pre-training">CLIP: Contrastive Language-Image Pre-training</h2> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2103.00020">https://arxiv.org/abs/2103.00020</a><br/> code :<br/> <a href="https://github.com/openai/CLIP">https://github.com/openai/CLIP</a><br/> referenced blog :<br/> <a href="https://xoft.tistory.com/67">https://xoft.tistory.com/67</a></p> </blockquote> <h3 id="intro">Intro</h3> <ul> <li>CLIP : text와 image 간의 관계를 사용하는 다양한 task에 적용 가능</li> </ul> <h3 id="contrastive-pre-training">Contrastive Pre-training</h3> <ul> <li><code class="language-plaintext highlighter-rouge">contrastive learning</code> :<br/> labeling 없는 self-supervised learning 기법 중 하나로,<br/> 같은 class라면 embedding distance를 최소화하고<br/> 다른 class라면 embedding distance를 최대화한다 <ul> <li>contrastive loss<br/> \(L_{cont}^m(x_i, x_j, f) = 1 \{ y_i = y_j \} \| f(x_i) - f(x_j) \|^2 + 1 \{ y_i \neq y_j \} \text{max}(0, m - \| f(x_i) - f(x_j) \|^2)\)</li> <li>triplet loss<br/> \(L_{trip}^m(x, x^{+}, x^{-}, f) = max(0, \| f(x) - f(x^{+}) \|^2 - \| f(x) - f(x^{-}) \|^2 + m)\)</li> <li>\(N+1\) - Tuplet loss<br/> \(L_{tupl}(x, x^{+}, \{ x_{i}^{-} \}_{i=1}^{N-1}, f) = log(1 + \Sigma_{i=1}^{N-1}\text{exp}(f(x)^T f(x_{i}^{-}) - f(x)^T f(x^{+}))) = - log(\frac{\text{exp}(f(x)^T f(x^{+}))}{\text{exp}(f(x)^T f(x^{+})) + \Sigma_{i=1}^{N-1} \text{exp}(f(x)^T f(x_{i}^{-}))})\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-03-CLIP/1-480.webp 480w,/assets/img/2024-09-03-CLIP/1-800.webp 800w,/assets/img/2024-09-03-CLIP/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-03-CLIP/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> cosine similarity matrix가 identity matrix (I) 에 가깝도록 학습 </div> <ul> <li>image-text pair로 구성된 dataset에 대해<br/> image, text를 각각 encoder로 embedding한 뒤<br/> 같은 pair는 거리 최소화하고<br/> 다른 pair는 거리 최대화하도록<br/> constrative learning으로 두 encoder를 학습</li> </ul> <h3 id="application">Application</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-03-CLIP/2-480.webp 480w,/assets/img/2024-09-03-CLIP/2-800.webp 800w,/assets/img/2024-09-03-CLIP/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-03-CLIP/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>dataset classifier 만들기 또는 zero-shot prediction 등에<br/> pre-trained CLIP model 사용 가능</li> </ul>]]></content><author><name></name></author><category term="cv-tasks"/><category term="contrastive"/><category term="image"/><category term="text"/><summary type="html"><![CDATA[Contrastive Language-Image Pre-training]]></summary></entry><entry><title type="html">Server</title><link href="https://semyeong-yu.github.io/blog/2024/Server/" rel="alternate" type="text/html" title="Server"/><published>2024-09-03T11:00:00+00:00</published><updated>2024-09-03T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Server</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Server/"><![CDATA[<ul> <li><code class="language-plaintext highlighter-rouge">Server 종류</code> <ul> <li>Tower Server :<br/> Rack Server보다 크기 작고 소음 적어서 설치 자유로움</li> <li><code class="language-plaintext highlighter-rouge">Rack Server</code> :<br/> Rack(Server 거치대) 에 server를 마운트<br/> (선반형 rack 설치 금지) <ul> <li>width 고정하고, 사양 높아질수록 depth가 길게 나온다<br/> e.g. 일반 Rack : 1000, 1075mm<br/> e.g. Deep Rack : 1200mm</li> <li>height : 단위 <code class="language-plaintext highlighter-rouge">U</code> 사용<br/> e.g. DL360 : 1U Server<br/> e.g. DL380 : 2U Server</li> <li>Rack 총 높이 : 보통 42U</li> <li>구성 <ul> <li>렉 : 서버 거치대 프레임</li> <li>서버 : network에서 local computer(client)에 data, resource, service 제공</li> <li>스위치 : network 내의 장치들을 서로 연결하여 정확한 목적지에 data packet 전송<br/> 여러 개의 포트가 있고, data 링크 계층(2계층)에서 작동하며, MAC 주소를 기반으로 data 전송</li> <li>스토리지 : data 저장 <ul> <li>HDD, SSD</li> <li>NAS (Network Attached Storage) : network에 연결된 file 기반의 스토리지<br/> 여러 user가 파일을 저장 및 공유 가능<br/> TCP/IP 같은 표준 Ethernet 네트워크로 스위치와 연결하여 사용 <ul> <li>장점 : 파일 공유 접근성 좋음</li> <li>단점 : network 환경이 불안정하면 트래픽 문제 및 latency 증가로 인한 성능 저하</li> <li>사용 : 다수 user의 동시 접속 파일 공유, 비정형 data(동영상, 오디오, 문서, …)에 적합</li> </ul> </li> <li>SAN (Storage Area Network) : 스토리지를 별도의 network로 관리<br/> 대규모 user를 위한 고속 네트워크 시스템<br/> Fibre Channel network 사용 <ul> <li>장점 : 연결된 server와 상관없이 분산된 스토리지에서 data를 주고받을 수 있으므로 속도 빠르고 안정적</li> <li>단점 : 비쌈</li> <li>사용 : DB처럼 구조화된 data에 대해 고용량 및 고성능 I/O 업무</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li>Blade Server :<br/> 고밀도 Server<br/> Composable Infra-Structure (Server, Storage, Network module을 하나의 frame에 구성)</li> </ul> </li> <li>Server 등급 <ul> <li>Entry :<br/> 보급형 (CPU 1~2)<br/> 수백만원대 <ul> <li>web server, application server</li> <li>ML350, DL20/360/380</li> </ul> </li> <li>Midrange :<br/> 중형급 (CPU 4~)<br/> 수천만원대 <ul> <li>database hosting, application hosting, mission critical system</li> <li>DL560/580</li> </ul> </li> <li>High-End :<br/> 고사양 (CPU 수십개)<br/> 수억원대 <ul> <li>large scale data process, mission critical system</li> <li>Superdome</li> </ul> </li> </ul> </li> <li>DL380 <code class="language-plaintext highlighter-rouge">Server 구조</code> : <ul> <li>Disk : 앞면 베젤 열면 SAS 300GB disk 8개</li> <li>CPU : 윗면 열면 Intel cpu 2개</li> <li>RAM : CPU 양옆에 Memory ~24개</li> <li>Fan : CPU, Memory 옆에 열 식히는 팬</li> <li>Power Supply : 뒷면에 2개</li> <li>PCI 라이저 킷 : Power 옆에 2개</li> <li>iLO : HPE 원격 관리 기술</li> <li>Raid Controller : SAS cable에 연결</li> <li>I/O port : USB, display, dvd, …</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-03-Server/1-480.webp 480w,/assets/img/2024-09-03-Server/1-800.webp 800w,/assets/img/2024-09-03-Server/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-03-Server/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>CPU x86 (intel Pentium) <ul> <li>Core : PC CPU</li> <li><code class="language-plaintext highlighter-rouge">Xeon : Server CPU</code> <ul> <li>1번 : 등급 <ul> <li>8 : Platinum</li> <li>6/5 : Gold</li> <li>4 : Silver</li> <li>3 : Bronze</li> </ul> </li> <li>2번 : CPU 세대 <ul> <li>3 : 3세대 (Icelake)</li> <li>4 : 4세대 (Sapphire Rapids)</li> </ul> </li> <li>3번 : Product Line Suffix (CPU 기능) <ul> <li>U, Y, …</li> </ul> </li> <li>4번 : Clock Speed</li> <li>5번 : CPU core 수</li> <li>6번 : CPU에 필요한 전력량</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-03-Server/2-480.webp 480w,/assets/img/2024-09-03-Server/2-800.webp 800w,/assets/img/2024-09-03-Server/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-03-Server/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>RAM (Random Access Memory)<br/> 휘발성 메모리라서 전원 공급이 중단되면 지워짐 <ul> <li><code class="language-plaintext highlighter-rouge">DRAM</code> (Dynamic) : data 유지 위해 주기적으로 refresh<br/> main memory <ul> <li>DIMM (dual in-line memory module) :<br/> DRAM 여러 개를 회로기판 위에 장착한 memory module</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">SRAM</code> (Static) : 전원 공급 동안 data 유지<br/> cache</li> <li>RAM <ul> <li>1번 : memory 용량</li> <li>2번 : DIMM 구성에 따라 memory chip에 access하는 방법 <ul> <li>Single Rank : 한 번에 access하는 1개의 memory chip이 있어서 용량이 적고 느림</li> <li>Dual Rank : 동시에 access할 수 있는 두 세트의 memory chip이 있어서 용량이 많고 빠름</li> <li>Quad, Octal Rank</li> </ul> </li> <li>3번 : DDR 규격, 세대 <ul> <li>DDR SDRAM : Double data rate synchronous DRAM<br/> clock speed 높이지 않아도 SDRAM보다 전송 속도 2배 빠름</li> </ul> </li> <li>4번 : memory 동작 clock bandwidth (높을수록 전송 속도 빠름)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-03-Server/3-480.webp 480w,/assets/img/2024-09-03-Server/3-800.webp 800w,/assets/img/2024-09-03-Server/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-03-Server/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-03-Server/4-480.webp 480w,/assets/img/2024-09-03-Server/4-800.webp 800w,/assets/img/2024-09-03-Server/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-03-Server/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>DISK <ul> <li><code class="language-plaintext highlighter-rouge">HDD</code> (Hard Disk Drive) : 레코드판처럼 기계적으로 정보 저장 <ul> <li>SSD 규격 설명이랑 비슷한데 아래 사항들은 다름</li> <li>7.2K : disk RPM<br/> RPM이 클수록 속도가 빠르지만, 그만큼 발열, 전력소모도 많아서 안정성 떨어지고 비쌈</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">SSD</code> (Solid State Drive) : 반도체를 이용하여 정보 저장 (더 빠름) <ul> <li>1번 : disk 용량</li> <li>2번 : disk 연결 방식 <ul> <li>SATA (Serial ATA) : 저렴하고 느림</li> <li>SAS (Serial Attached SCSI) : 비싸지만 내구성 좋고 빠름<br/> 안정성 좋은 SCSI 방식을 직렬 구조로 변경하여 속도, 안정성 모두 챙김</li> </ul> </li> <li>3번 : disk 성능 (data 전송 속도)</li> <li>4번 : disk type <ul> <li>Read Intensive</li> <li>Mixed Use</li> <li>Write Intensive</li> </ul> </li> <li>5번 : disk 규격 <ul> <li>LFF (large form factor) : 3.5 inch</li> <li>SFF (small form factor) : 2.5 inch</li> </ul> </li> <li>6번 : disk 담는 carrier type <ul> <li>BC (Basic Carrier) : LED 무</li> <li>SC (Smart Carrier) : LED 유 (drive 제거할 때 data 손실에 대해 사전 경고 기능)</li> <li>Megaraid controller로 구성하는 server에는 BC disk만 장착 가능</li> </ul> </li> <li>7번 : disk 제조사</li> <li>8번 : HDD or SSD</li> </ul> </li> </ul> </li> </ul>]]></content><author><name></name></author><category term="others"/><category term="server"/><category term="rack"/><category term="cpu"/><category term="ram"/><category term="disk"/><summary type="html"><![CDATA[Server]]></summary></entry><entry><title type="html">DreamFusion</title><link href="https://semyeong-yu.github.io/blog/2024/Dreamfusion/" rel="alternate" type="text/html" title="DreamFusion"/><published>2024-08-29T11:00:00+00:00</published><updated>2024-08-29T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Dreamfusion</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Dreamfusion/"><![CDATA[<h2 id="dreamfusion-text-to-3d-using-2d-diffusion-iclr-2023">DreamFusion: Text-to-3D using 2D Diffusion (ICLR 2023)</h2> <h4 id="ben-poole-ajay-jain-jonathan-t-barron-ben-mildenhall">Ben Poole, Ajay Jain, Jonathan T. Barron, Ben Mildenhall</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2209.14988">https://arxiv.org/abs/2209.14988</a><br/> project website :<br/> <a href="https://dreamfusion3d.github.io/">https://dreamfusion3d.github.io/</a><br/> pytorch code :<br/> <a href="https://github.com/ashawkey/stable-dreamfusion">https://github.com/ashawkey/stable-dreamfusion</a></p> </blockquote> <h2 id="contribution">Contribution</h2> <ul> <li><code class="language-plaintext highlighter-rouge">SDS(Score Distillation) Loss</code> 처음 제시 <ul> <li>scalable, high-quality 2D diffusion model의 능력을 3D domain renderer로 distill</li> <li>3D 또는 multi-view training data 필요없고, pre-trained 2D diffusion model만 있으면, 3D synthesis 수행 가능!</li> </ul> </li> <li>NeRF가 Diffusion(Imagen) model with text에서 내놓을 만한 그럴 듯한 image를 합성하도록 함</li> <li><code class="language-plaintext highlighter-rouge">text-to-3D</code> synthesis 발전 시작</li> </ul> <h2 id="overview">Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/1-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/1-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Overview <ul> <li>initialize NeRF with random weight</li> <li>for each iter. <ul> <li>camera 위치와 각도, light 위치와 색상을 randomly sampling<br/> \(P(camera), P(light)\)</li> <li>NeRF로 image rendering</li> <li>text embedding \(\tau\) 이용해서 NeRF param. \(\theta\) 에 대한 SDS loss 계산</li> <li>update NeRF weight</li> </ul> </li> </ul> </li> </ul> <h3 id="random-camera-light-sampling">Random camera, light sampling</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/2-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/2-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">camera</code> : <ul> <li>3D model을 <code class="language-plaintext highlighter-rouge">bounded sphere</code> 내부로 제한하고,<br/> spherical coordinate(구 표면)에서 camera 위치를 sampling하여<br/> 구의 원점을 바라보도록 camera 각도 설정</li> <li>width(64)에 0.7 ~ 1.35의 상수값을 곱하여 focal length 설정</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">light</code> : <ul> <li>camera 위치를 중심으로 한 확률분포로부터 light의 위치를 sampling하고<br/> (어떤 확률분포 <code class="language-plaintext highlighter-rouge">????</code>)<br/> light 색상도 sampling</li> </ul> </li> </ul> <h3 id="nerf-rendering-with-shading">NeRF Rendering with shading</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/3-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/3-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> albedo : NeRF가 예측한 color </div> <ul> <li>rendering 방법 : <ol> <li>albedo \(\rho\) 만으로 rendering<br/> (기존 NeRF와 동일)</li> <li>albedo \(\rho\) 뿐만 아니라 shading하여 rendering</li> <li>albedo \(\rho\) 를 white \((1, 1, 1)\) 로 바꾼 뒤 shading하여 rendering</li> </ol> </li> <li><code class="language-plaintext highlighter-rouge">Shading</code>의 역할 : <ul> <li>shading 없이 \(\rho\) 만으로 rendering하면<br/> 평평한 3D model이 나와도 점수 높게 나옴</li> <li>shading으로 (빛 반사에 따른) shape 정보까지 고려해서 rendering하면<br/> <code class="language-plaintext highlighter-rouge">volume 있는</code> 3D model이 되도록 촉구</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">NeRF MLP</code> \(\theta\) : <ul> <li>MLP output : volume density \(\tau\) 와 albedo \(\rho\)</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Normal</code> \(n\) :<br/> \(n = - \frac{\nabla_{\mu} \tau}{\| \nabla_{\mu} \tau \|}\)<br/> where \(n\) 은 물체 표면의 법선벡터 <ul> <li>normal vector의 방향은<br/> volume density \(\tau\) 가 가장 급격하게 변하는 방향, 즉 \(\nabla_{\mu} \tau\) 의<br/> 반대 방향</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Shading</code> \(s\) :<br/> \(s = (l_p \circ \text{max}(0, n \cdot \frac{l - \mu}{\| l - \mu \|})) + l_a\)<br/> where \(l_p\) 는 light 좌표 \(l\) 에서 나오는 light(광원) 색상<br/> where \(l_a\) 는 ambient light(환경 조명) 색상<br/> where \(\mu\) 는 shading 값을 계산할 surface 위 point 좌표<br/> where \(\circ\) 는 element-wise multiplication <ul> <li>\(n \cdot (l - \mu)\) 는 표면에서의 normal vector와 표면에서 광원까지의 vector 간의 내적이며,<br/> 이는 Lambertian(diffuse) reflectance(난반사)에 의해 광원의 빛이 반사되는 정도를 나타냄<br/> 왜냐하면, 빛이 표면에 수직으로 들어올수록 많이 반사됨</li> <li>만약 빛이 표면 반대쪽에 있어서 또는 back-facing normal로 잘못 예측해서<br/> 내적 값 \(n \cdot (l - \mu)\) 이 음수일 경우<br/> 난반사에 의해 광원의 빛이 반사되는 정도는 0</li> <li>\(l_p \circ \text{난반사 정도} + l_a\) 에 의해<br/> <code class="language-plaintext highlighter-rouge">광원</code>의 색상 \(l_p\) 는 물체 <code class="language-plaintext highlighter-rouge">표면의 난반사 정도에 따라</code> 반영되고<br/> <code class="language-plaintext highlighter-rouge">환경 조명</code>의 색상 \(l_a\) 는 물체의 <code class="language-plaintext highlighter-rouge">모든 표면에 일정하게</code> 반영됨</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Color</code> \(c\) :<br/> \(c = \rho \circ s\) 또는 \(c = \rho\)</li> </ul> <h3 id="diffusion-loss-with-conditioning">Diffusion loss with conditioning</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/5-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/5-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Latent Diffusion</code> model : <ul> <li>image \(x\) 가 아니라 encoder를 거친 image latent vector \(z\) 에 대해 noising, denoising 수행</li> <li>noisy \(z_T\) 와 text embedding vector \(\tau_{\theta}\) 를 concat한 걸<br/> denoising하여 input image와 유사한 확률 분포를 갖도록 학습<br/> (text embedding vector \(\tau_{\theta}\) 을 conditioning (query) 로 넣어줌)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/4-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/4-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>text embedding vector \(\tau_{\theta}\) :<br/> T5-XXL text embedding을 거치기 전에<br/> text prompt engineering 수행 <ul> <li>Elevation angle(고각)이 60도 이상일 때 “overhead view”</li> <li>azimuth angle(방위각)에 따라 “front view”, “side view”, “back view”</li> <li>text prompt engineering은 원래 좀 투박하게 하나?</li> </ul> </li> <li>Imagen : <ul> <li>latent diffusion model with \(64 \times 64\) resolution<br/> (for fast training)</li> </ul> </li> </ul> <h3 id="sample-in-parmater-space-not-pixel-space">Sample in Parmater Space, not Pixel Space</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/10-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/10-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>\(x=g(\theta)\) : differentiable image parameterization (DIP)<br/> where \(x\) 는 image이고 \(g\) 는 renderer이고 \(\theta\) 는 param. <ul> <li>more compact param. space \(\theta\) 에서 optimize ㄱㄴ<br/> (더 강력한 optimization algorithm 사용 ㄱㄴ)</li> </ul> </li> <li>loss optimization으로 tractable sample 만들기 위해 diffusion model의 힘을 이용해서<br/> \(x\) in pixel space 가 아니라, \(\theta\) in parameter space 를 optimize<br/> s.t. \(x=g(\theta)\) 가 그럴 듯한 diffusion model sample처럼 보이도록</li> </ul> <h3 id="optimization">Optimization</h3> <ul> <li>실험적인 implementation : <ul> <li>noise level (time) sampling \(t\) :<br/> \(z_t, t \sim U[0, 1]\) 에서 noise level이 너무 크거나(\(t=1\)) 너무 작을 경우(\(t=0\)) instability 생기므로<br/> noise level \(t \sim U[0.02, 0.98]\) 로 sampling</li> <li>guidance weight \(w\) :<br/> Imagen이 NeRF에 얼만큼 영향을 미칠지(guide할지)인데,<br/> high-quality 3D model을 학습하기 위해서는<br/> CFG(classifier-free guidance) weight \(w\) 를 큰 값(100)으로 설정<br/> (NeRF MLP output color가 sigmoid에 의해 [0, 1]로 bounded되어있으므로 constrained optimization 문제라서 guidance weight 커도 딱히 artifacts 없음)<br/> (SDS loss는 mode-seeking property를 가지고 있어서 작은 guidance weight 값을 사용할 경우 over-smoothing됨 <code class="language-plaintext highlighter-rouge">????</code>)</li> <li>seed :<br/> noise level이 높을 때 smoothed density는 distinct modes를 많이 가지지 않고<br/> SDS Loss는 mode-seeking property를 가지고 있으므로<br/> random seed 바꿔도 실험 결과는 큰 차이 없음</li> </ul> </li> <li>implementation : <ul> <li>train : TPUv4, 15000 iter., 1.5h with Distributed Shampoo optimizer</li> <li>rendering : 각 cpu는 개별 view를 rendering하는데 사용</li> </ul> </li> </ul> <h2 id="rendering">Rendering</h2> <h3 id="structure">Structure</h3> <ul> <li>Mip-NeRF 360 구조 사용</li> <li>entire scene 대신 single object를 generate할 때<br/> <code class="language-plaintext highlighter-rouge">bounded sphere</code> 내에서 NeRF view-synthesis 하면 빠르게 수렴 및 좋은 성능</li> <li>\(\gamma(d)\) 를 input으로 받아 배경 색상을 계산하는 별도의 MLP로 <code class="language-plaintext highlighter-rouge">environment map</code>을 생성한 뒤 그 위에 ray rendering하면 좋은 성능 <ul> <li>배경이 보이는 부분은 배경에서의 누적 \(\alpha\) 값이 1이도록</li> <li>물체 때문에 배경이 안 보이는 부분은 배경에서의 누적 \(\alpha\) 값이 0이도록</li> </ul> </li> </ul> <h3 id="geometry-regularizer">Geometry Regularizer</h3> <ul> <li>DreamField의 regularization : <ul> <li><code class="language-plaintext highlighter-rouge">empty space가 불필요하게 채워지는</code> 것을 방지</li> <li>\(L_T = - \text{min} (\tau, \text{mean}(T(\theta, p)))\) :<br/> 평균 <code class="language-plaintext highlighter-rouge">transmittance가 클수록</code> loss가 작음<br/> where \(T(\theta, p)\) : transmittance with NeRF parameter \(\theta\) and camera pose \(p\)<br/> where \(\tau\) : 최대값 상수</li> </ul> </li> <li>Ref-NeRF의 regularization : <ul> <li>normal vector \(n_i\) 의 back-facing (<code class="language-plaintext highlighter-rouge">물체 안쪽을 향하는</code>) 문제를 방지</li> <li>orientation loss \(L = \Sigma_{i} w_i max(0, n_i \cdot d)^2\) :<br/> ray를 쏘면 물체의 앞면만 보이니까<br/> 물체 표면의 normal vector 방향과 ray 방향의 내적이 음수여야 한다<br/> 따라서 \(n_i\) 와 \(d\) 의 <code class="language-plaintext highlighter-rouge">내적이 양수일 경우</code> back-facing normal vector이므로 penalize <ul> <li>textureless shading을 쓸 때 해당 regularization이 중요<br/> 만약 해당 regularization 안 쓰면<br/> density field로 구한 normal 방향이 camera 반대쪽을 향하게 되어 shading이 더 어두워짐</li> </ul> </li> </ul> </li> </ul> <h2 id="sds-loss">SDS Loss</h2> <ul> <li> <p>NeRF로 rendering한 image \(x\) 에 noise를 더한 것을 \(z_t\) 로 두고<br/> U-Net \(\hat \epsilon_{\phi}(z_t | y, t)\) 을 빼서 denoising하여 얻은 image의 확률분포가<br/> 2D diffusion prior가 내놓는 image의 확률분포와 비슷하도록 하는 loss이며,<br/> 그 차이만큼 NeRF \(\theta\) 로 back-propagation</p> </li> <li> <p>배경지식 :</p> <ul> <li>DDPM Loss : \(E_{t, x_0, \epsilon} [\| \epsilon - \hat \epsilon_{\phi}(\alpha_{t}x_0 + \sigma_{t} \epsilon, t) \|^{2}]\)<br/> where \(\epsilon \sim N(0, I)\)<br/> where \(\alpha_{t} = \sqrt{\bar \alpha_{t}}\)<br/> where \(\sigma_{t} = \sqrt{1-\bar \alpha_{t}}\)</li> <li>만약 \(\theta\) 를 업데이트하기 위해 DDPM Loss를 직접 이용할 경우<br/> diffusion training의 multiscale 특성을 이용하고<br/> timestep schedule을 잘 선택한다면 <d-cite key="diffprior">[1]</d-cite> 잘 작동할 수 있다고 하지만<br/> 실험해봤을 때 timestep schedule을 tune하기 어려웠고 DDPM Loss는 불안정했음</li> <li>위의 DDPM Loss는 denoising U-Net param.을 업데이트하기 위함이었고,<br/> 우리는 fixed denoising U-Net을 이용하여<br/> NeRF param. \(\theta\) 업데이트하기 위한 SDS Loss를 새로 만들겠다!</li> </ul> </li> </ul> <h3 id="simple-derivation-of-sds-loss">Simple Derivation of SDS Loss</h3> <ul> <li>DDPM Loss를 \(\phi\) 말고 \(\theta\) 에 대해 미분하고<br/> constant \(\frac{dz_t}{dx} = \alpha_{t} \boldsymbol I\) 를 \(w(t)\) 에 넣으면</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/6-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/6-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> x는 NeRF가 생성한 image이고, y는 text embedding vector </div> <ul> <li>위의 U-Net Jacobian은 상당한 연산량을 가지는 데 비해<br/> 작은 noise만 줄 뿐 큰 영향이 없으므로<br/> SDS Loss에서 U-Net Jacobian term은 생략</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/7-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/7-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="derivation-of-sds-loss">Derivation of SDS Loss</h3> <ul> <li>SDS Loss gradient : <ul> <li>inspired by <code class="language-plaintext highlighter-rouge">gradient of weighted probability density distillation loss</code> <d-cite key="WaveNet">[2]</d-cite></li> <li> \[\nabla_{\theta} L_{SDS}(\phi, x=g(\theta)) = \nabla_{\theta} E_{t, z_t|x}[w(t)\frac{\sigma_{t}}{\alpha_{t}}\text{KL}(q(z_t|g(\theta)) \| p_{\phi}(z_t | y, t))]\] </li> </ul> </li> <li>KL-divergence : <ul> <li><a href="https://semyeong-yu.github.io/blog/2024/Diffusion/">Diffusion</a> 의 KL-divergence 부분에 따르면<br/> 모르는 분포 \(q(x)\) ( \(\epsilon\) ) 을 N개 sampling하여 trained \(p(x | \theta)\)로 근사하고자 할 때,<br/> \(KL(q \| p) \simeq \frac{1}{N} \sum_{n=1}^{N} {log q(x_n) - log p(x_n | \theta)}\) 이므로<br/> \(\text{KL}(q(z_t|g(\theta)) \| p_{\phi}(z_t | y, t)) = E_{\epsilon}[\text{log} q(z_t | x = g(\theta)) - \text{log} p_{\phi}(z_t | y)]\)<br/> \(\rightarrow\)<br/> \(\nabla_{\theta}\text{KL}(q(z_t|g(\theta)) \| p_{\phi}(z_t; y, t)) = E_{\epsilon}[\nabla_{\theta}\text{log} q(z_t | x = g(\theta)) - \nabla_{\theta}\text{log} p_{\phi}(z_t | y)]\)</li> </ul> </li> <li>\(\theta\) 에 대한 \(\text{log}q\) 의 미분 : <ul> <li>gradient of <code class="language-plaintext highlighter-rouge">forward process entropy</code> w.r.t mean param. \(\theta\)<br/> (variance는 고정)</li> <li>아래 수식을 \(\nabla_{\theta}log q(z_t | x = g(\theta))\) 계산에 이용<br/> \(z_t = \alpha_{t} x + \sigma_{t} \epsilon \sim N(\alpha_{t} x, \sigma_{t}^2)\)<br/> \(\rightarrow \text{log} q(z_t|x=g(\theta)) = -\frac{1}{2\sigma_{t}^2} \| z_t - \alpha_{t} x \|^2 + \text{constant}\)<br/> \(\rightarrow \frac{d\text{log}q(z_t | x)}{dx} = \frac{\alpha_{t}}{\sigma_{t}^2}(z_t - \alpha_{t} x) = \frac{\alpha_{t}}{\sigma_{t}^2}\sigma_{t}\epsilon = \frac{\alpha_{t}}{\sigma_{t}}\epsilon\)<br/> and \(\frac{d\text{log}q(z_t | x)}{dz_t} = -\frac{1}{\sigma_{t}^2}(z_t - \alpha_{t} x) = -\frac{1}{\sigma_{t}^2}\sigma_{t}\epsilon = -\frac{1}{\sigma_{t}}\epsilon\)<br/> and \(\frac{dz_t}{dx} = \alpha_{t}\)</li> <li>\(\nabla_{\theta}log q(z_t | x = g(\theta)) = (\frac{d\text{log}q(z_t | x)}{dx} + \frac{d\text{log}q(z_t | x)}{dz_t}\frac{dz_t}{dx})\frac{dx}{d\theta}\)<br/> \(= (\frac{\alpha_{t}}{\sigma_{t}}\epsilon - \frac{1}{\sigma_{t}}\epsilon \alpha_{t})\frac{dx}{d\theta}\)<br/> \(= 0\)<br/> (\(q\) 는 <code class="language-plaintext highlighter-rouge">고정된 variance의 noise</code>를 사용하므로 \(\theta\) 에 대한 entropy \(\text{log}q\) 의 미분 값은 0) <ul> <li>위의 식에서 \(\frac{d\text{log}q(z_t | x)}{dx}\) :<br/> <code class="language-plaintext highlighter-rouge">parameter score function</code><br/> gradient of log probability w.r.t parameter \(x\)<br/> (\(x\) 에 대한 \(\text{log}q\) 의 gradient 계산)</li> <li>\(\frac{d\text{log}q(z_t | x)}{dz_t}\frac{dz_t}{dx}\) :<br/> <code class="language-plaintext highlighter-rouge">path derivative</code><br/> gradient of log probability w.r.t sample \(z_t\)<br/> (\(q\) 를 따르는 sample \(z_t\) 를 통해 \(x\) 에 대한 \(\text{log}q\) 의 gradient 계산)</li> </ul> </li> <li>Sticking-the-Landing <d-cite key="vargrad">[3]</d-cite> 에 따르면<br/> path derivative term은 냅두고<br/> parameter score function term을 제거하여<br/> SDS loss gradient에 \(\epsilon\) 항을 포함할 경우<br/> <code class="language-plaintext highlighter-rouge">control-variates</code> 기법 <a href="https://en.wikipedia.org/wiki/Control_variates">Wikipedia</a>에 의해<br/> \(E[\cdot]\) 으로 gradient 구할 때 <code class="language-plaintext highlighter-rouge">variance를 줄일 수</code> 있음!<br/> (자세한 설명은 아래의 SDS Loss gradient Summary 부분 참고)<br/> (variance가 작으면 optimization이 빨라지고 더 나은 결과를 도출할 수 있음)</li> </ul> </li> <li>\(\theta\) 에 대한 \(\text{log}p_{\phi}\) 의 미분 : <ul> <li>gradient of <code class="language-plaintext highlighter-rouge">backward process entropy</code> (denoising U-Net) w.r.t mean param. \(\theta\)</li> <li>아래 수식을 \(\nabla_{\theta}log p_{\phi}(z_t | y)\) 계산에 이용<br/> \(\frac{d\text{log}q(z_t | x)}{dz_t}\) 구했듯이 \(\epsilon\) 대신 \(\epsilon_{\phi}\) 넣으면<br/> \(\nabla_{z_t} \text{log}p_{\phi}(z_t | y) = \frac{d\text{log}p_{\phi}(z_t | y)}{dz_t} = -\frac{1}{\sigma_{t}}\hat \epsilon_{\phi}\)<br/> and \(\frac{dz_t}{dx} = \alpha_{t}\)</li> <li> \[\nabla_{\theta}\text{log} p_{\phi}(z_t | y) = \nabla_{z_t} \text{log}p_{\phi}(z_t | y) \frac{dz_t}{dx} \frac{dx}{d\theta} = - \frac{\alpha_{t}}{\sigma_{t}} \hat \epsilon_{\phi}(z_t | y) \frac{dx}{d\theta}\] </li> </ul> </li> <li>SDS Loss gradient <code class="language-plaintext highlighter-rouge">Summary</code> : <ul> <li>SDS Loss gradient :<br/> \(\nabla_{\theta} L_{SDS}(\phi, x=g(\theta)) = E_{t, z_t|x}[w(t)\frac{\sigma_{t}}{\alpha_{t}}\nabla_{\theta}\text{KL}(q(z_t|g(\theta)) \| p_{\phi}(z_t | y, t))]\)<br/> \(= E_{t, \epsilon}[w(t)\frac{\sigma_{t}}{\alpha_{t}}E_{\epsilon}[\nabla_{\theta}\text{log} q(z_t | x = g(\theta)) - \nabla_{\theta}\text{log} p_{\phi}(z_t | y)]]\)<br/> \(= E_{t, \epsilon}[w(t)\frac{\sigma_{t}}{\alpha_{t}} (-\frac{\alpha_{t}}{\sigma_{t}}\epsilon \frac{dx}{d\theta} + \frac{\alpha_{t}}{\sigma_{t}} \hat \epsilon_{\phi}(z_t | y) \frac{dx}{d\theta})]\)<br/> \(= E_{t, \epsilon}[w(t)(\hat \epsilon_{\phi}(z_t | y) - \epsilon)\frac{dx}{d\theta}]\)</li> <li>\(\nabla_{\theta}log q(z_t | x = g(\theta))\) 의 path derivative term은 \(\epsilon\) 과 관련 있고!<br/> \(\nabla_{\theta}\text{log} p_{\phi}(z_t | y)\) 은 \(\epsilon\) 의 예측, 즉 \(\hat \epsilon_{\phi}\) 와 관련 있고!<br/> 둘의 KL-divergence를 loss term으로 사용한다!<br/> (\(\epsilon\) 을 \(\hat \epsilon\) 의 control-variate로 생각하여 <d-cite key="vargrad">[3]</d-cite> 방식처럼 둘의 차이로 SDS Loss gradient 만들 수 있음!)</li> </ul> </li> <li>Other Papers : <ul> <li>Graikos et al. (2022) <d-cite key="diffprior">[1]</d-cite> :<br/> \(KL(h(x) \| p_{\phi}(x|y))\) 로부터<br/> \(E_{\epsilon, t}[\| \epsilon - \hat \epsilon_{\theta}(z_t | y; t) \|^2] - \text{log} c(x, y)\) 를 유도해서 loss로 썼지만,<br/> SDS와 달리 error 제곱 꼴이라서 costly back-propagation</li> <li>DDPM-PnP 또한 auxiliary classifier \(c\) 를 썼지만,<br/> SDS에서는 CFG(classifier-free-guidance) 사용<br/> (별도의 image label 없이 image caption만 conditioning으로 넣어줘서 model 학습)</li> <li>지금까지 implicit model의 entropy의 gradient는 single noise level <d-cite key="ARDAE">[4]</d-cite> 에서의 amortized score model (control-variate 사용 안 함) 로 측정하였는데,<br/> SDS에서는 multiple noise level을 사용함으로써 optimization 더 쉽게 ㄱㄴ<br/> (multiple noise level <code class="language-plaintext highlighter-rouge">?????</code>)</li> <li>GAN-like amortized samplers는 Stein discrepancy 최소화 <d-cite key="Stein">[5]</d-cite> , <d-cite key="Stein2">[6]</d-cite> 로 학습하는데,<br/> 이는 SDS loss의 score 차이와 비슷</li> </ul> </li> </ul> <h2 id="pseudo-code">Pseudo Code</h2> <pre><code class="language-Python">params = generator.init() # NeRF param.
opt_state = optimizer.init(params) # optimizer
diffusion_model = diffusion.load_model() # Imagen diffusion model
for iter in iterations:
  t = random.uniform(0., 1.) # noise level (time step)
  alpha_t, sigma_t = diffusion_model.get_coeffs(t) # determine noisy z_t's mean, std.
  eps = random.normal(img_shape) # gaussian noise (epsilon)
  x = generator(params, ...) # NeRF rendered image
  z_t = alpha_t * x + sigma_t * eps # noisy NeRF image
  epshat_t = diffusion_model.epshat(z_t, y, t) # denoising U-Net
  g = grad(weight(t) * dot(stopgradient[epshat_t - eps], x), params) # derivative of SDS loss; stopgradient since do not update diffusion model
  params, opt_state = optimizer.update(g, opt_state) # update NeRF param.
return params
</code></pre> <h2 id="experiment">Experiment</h2> <h3 id="metric">Metric</h3> <ul> <li><code class="language-plaintext highlighter-rouge">CLIP R-Precision</code> <d-cite key="dreamfield">[7]</d-cite> : <ul> <li><code class="language-plaintext highlighter-rouge">rendered image의 text 일관성</code>을 측정<br/> (rendered image가 주어졌을 때 CLIP이 오답 texts 중 적절한 text를 찾는 accuracy로 계산)</li> <li>기존 CLIP R-Precision은 geometry quality는 측정할 수 없으므로<br/> 평평한 flat geometry에 대해서도 높은 점수가 나올 수 있음</li> <li>textureless render의 R-Precision(Geo)도 추가로 측정!</li> </ul> </li> <li>PSNR :<br/> zero-shot text-to-3D generation에서는<br/> text에 대한 3D Ground-Truth를 만들 수 없으므로<br/> GT를 필요로 하는 PSNR 같은 metric은 사용하지 못함</li> </ul> <h3 id="result">Result</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/8-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/8-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Geo(metry)의 CLIP R-Precision 점수가 높다는 것은 평평한 3D model이 아니라 shape 정보까지 고려했다는 것! </div> <ul> <li>위의 표 설명 : <ul> <li>GT Images : oracle (CLIP training에 사용된 dataset)</li> <li>CLIP-Mesh : CLIP으로 mesh를 optimize한 연구</li> </ul> </li> <li> <p>DreamFusion은 training할 때 <code class="language-plaintext highlighter-rouge">Imagen</code>을 썼고,<br/> Dream Fields와 CLIP-Mesh는 training할 때 <code class="language-plaintext highlighter-rouge">CLIP</code>을 썼으므로<br/> Dream Fields와 CLIP-Mesh를 사용하는 게<br/> DreamFusion보다 성능이 더 좋아야 하는데,<br/> 위의 표를 보면 Color와 Geometry 평가에서 DreamFusion이 높은 성능(text 일관성)을 보인다는 것을 확인할 수 있다</p> </li> <li>아쉬운 점 :<br/> 비슷한 다른 모델이 있다면 PSNR, SSIM 등으로 비교할 수 있었을텐데<br/> 비교군이 없어서 R-Precision으로 consistency 측정만 했음</li> </ul> <h3 id="ablation-study">Ablation Study</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/9-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/9-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>어떤 기법이 얼마나 성능에 기여했는지 파악하기 위해<br/> 4가지 기법을 점진적으로 추가 <ul> <li>(i) <code class="language-plaintext highlighter-rouge">ViewAug</code> : view-points의 범위를 넓힘</li> <li>(ii) <code class="language-plaintext highlighter-rouge">ViewDep</code> : view-dependent text prompt-engineering 사용<br/> (e.g. “overhead view”, “side view”)</li> <li>(iii) <code class="language-plaintext highlighter-rouge">Lighting</code> : 조명 사용</li> <li>(iv) <code class="language-plaintext highlighter-rouge">Textureless</code> : albedo를 white로 만들어서 (color 없이) shading</li> </ul> </li> <li>geometry quality를 확인하기 위해<br/> 3가지 rendering 기법을 비교 <ul> <li>(Top) <code class="language-plaintext highlighter-rouge">Albedo</code> : albedo \(\rho\) 만으로 rendering<br/> (기존 NeRF와 동일)</li> <li>(Middle) <code class="language-plaintext highlighter-rouge">Full Shaded</code> : albedo \(\rho\) 뿐만 아니라 shading하여 rendering</li> <li>(Bottom) <code class="language-plaintext highlighter-rouge">Textureless</code> : albedo \(\rho\) 를 white \((1, 1, 1)\) 로 바꾼 뒤 shading</li> </ul> </li> <li>결과 설명 : <ul> <li>기법 추가 없이 Albedo rendering 하면 R-Precision은 높게 나오는데<br/> Geometry가 엄청 이상함 (e.g. 머리 2개 가진 개)</li> <li>ViewDep, Lighting, Textureless 기법 사용해야 정확한 <code class="language-plaintext highlighter-rouge">geometry</code>까지 recon할 수 있음</li> <li>(ii) ViewDep의 영향 :<br/> geometry 개선되지만, surface가 non-smooth하고 Shaded rendering 결과가 bad</li> <li>(iii) Lighting의 영향 :<br/> geometry 개선되지만, 어두운 부분은 (e.g. 해적 모자) 여전히 non-smooth</li> <li>(iv) Textureless의 영향 :<br/> geometry smooth하게 만드는 데 도움 되지만, color detail (e.g. 해골 뼈)이 geometry에 carved 되는 문제 발생</li> </ul> </li> </ul> <h2 id="limitation">Limitation</h2> <ul> <li>SDS를 적용하여 만든 2D image sample은 <code class="language-plaintext highlighter-rouge">over-saturated</code> 혹은 <code class="language-plaintext highlighter-rouge">over-smoothed</code> result <ul> <li>dynamic thresholding <d-cite key="dynathres">[8]</d-cite> 을 사용하면 SDS를 image에 적용할 때의 문제를 완화시킬 수 있다고 알려져 있긴 하지만, NeRF context에 대해서는 해결하지 못함<br/> (dynamic thresholding이 뭔지 아직 몰라서 읽어 봐야 됨 <code class="language-plaintext highlighter-rouge">?????</code>)</li> </ul> </li> <li>SDS를 적용하여 만든 2D image sample은 <code class="language-plaintext highlighter-rouge">diversity</code> 부족<br/> (random seed 바꿔도 3D result에 큰 차이 없음)</li> </ul> <p>This may be fundamental to our use of reverse KL divergence, which has been previously noted to have mode-seeking properties in the context of variational inference and probability density distillation <code class="language-plaintext highlighter-rouge">?????</code></p> <ul> <li>\(64 \times 64\) Imagen (<code class="language-plaintext highlighter-rouge">low resol.</code>)을 사용하여 3D model의 fine-detail이 부족할 수 있음 <ul> <li>diffusion model 또는 NeRF를 더 큰 걸 사용하면 문제 해결할 수 있지만, 그만큼 겁나 느려지지…</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">2D image로부터 3D recon.</code>하는 게 원래 어렵고 애매한 task임<br/> e.g. inverse rendering, dreamfusion <ul> <li>같은 2D images로부터 무수히 많은 3D worlds가 존재할 수 있으니까</li> <li>optimization landscape가 highly non-convex하므로 local minima에 빠지지 않기 위한 기법들 필요<br/> (local minima : e.g. 모든 scene content가 하나의 flat surface에 painted된 경우)</li> <li>more <code class="language-plaintext highlighter-rouge">robust 3D prior</code>가 도움 될 것임</li> </ul> </li> </ul> <h2 id="latest-papers">Latest Papers</h2> <ul> <li>본 논문 DreamFusion과 관련된 논문들 <ul> <li>ProlificDreamer</li> <li>CLIP Goes 3D</li> <li>Magic3D</li> <li>Fantasia3D</li> <li>CLIP-Forge</li> <li>CLIP-NeRF</li> <li>Text2Mesh</li> <li>DDS (Delta Denoising Score)</li> </ul> </li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 : diffusion의 mode-seeking property?</p> </li> <li>A1 : <ul> <li>A1-1 : diffusion에서 forward process는 mean-seeking property 가지고 있고, backward process는 mode-seeking property 가지고 있는 걸로 알고 있는데 이거랑 관련 있을까요?</li> <li>A1-2 : 그 아까 말씀하신 mode-seeking property에 대해 찾아봤는데 diffusion의 Langevin Dynamics 등의 샘플링 방법이 갖고 있는 특징으로 특정 mode에서 sample을 집중적으로 생성하는 특징을 의미하네요. 제 생각에 “mode-seeking property를 갖고있어 guidance weight가 작으면 over-smoothing 된다”는 말은 diffusion model에서는 특정 모드에 집중되어 sample들이 생성되는데 낮은 guidance weight를 쓰면 여러 모드 사이를 부드럽게(?) 연결하려는 (over-smoothing) 말이지 않을까 싶네요.(즉 너무 매끄럽거나 디테일이 떨어지는 이미지 생성)</li> </ul> </li> <li> <p>Q2 : reverse KL-divergence를 최소화하는 과정의 경우 mode-seeking property (확률 높은 중요한 부분 찾는 경향)가 있다는데,<br/> reverse KL-divergence와 mode-seeking property가 무슨 관계인가요?</p> </li> <li> <p>A2 : TBD</p> </li> <li> <p>Q3 : SDS loss로 image rendering한 samples의 경우 diversity가 부족하고 그 이유가 mode-seeking property라는 거 같은데,<br/> 오히려 diversity가 부족한 게 단점이 아니라,<br/> mode-seeking property로 중요한 부분을 잘 캐치해서 consistent하게 그려내는 게 장점이 될 수 있지 않나요?</p> </li> <li> <p>A3 : TBD<br/> While modes of generative models in high dimensions are often far from typical samples (Nalisnick et al., 2018), the multiscale nature of diffusion model training may help to avoid these pathologies. <code class="language-plaintext highlighter-rouge">?????</code></p> </li> <li> <p>Q4 : \(\theta\) 에 대한 \(\text{log}q\) 의 미분에서 path derivative term은 냅두고 parameter score function term은 제거해서 control-variates에 의해 variance를 줄였다고 하는데,<br/> parameter score function term을 걍 제거해버리는 게 좀 야매 아닌가요?</p> </li> <li>A4 : TBD</li> </ul>]]></content><author><name></name></author><category term="generative"/><category term="sds"/><category term="diffusion"/><category term="nerf"/><category term="3d"/><category term="rendering"/><summary type="html"><![CDATA[Text-to-3D using 2D Diffusion (ICLR 2023)]]></summary></entry></feed>