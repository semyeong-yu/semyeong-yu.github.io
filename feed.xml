<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://semyeong-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://semyeong-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-27T08:50:53+00:00</updated><id>https://semyeong-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">DDIM SDS DDS PDS</title><link href="https://semyeong-yu.github.io/blog/2025/GenAI/" rel="alternate" type="text/html" title="DDIM SDS DDS PDS"/><published>2025-04-21T12:00:00+00:00</published><updated>2025-04-21T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/GenAI</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/GenAI/"><![CDATA[<h2 id="basic-generative-model">Basic Generative Model</h2> <blockquote> <p>reference :<br/> <a href="https://velog.io/@guts4/Basic-Generative-Model-DDIM-Score-based-CFG1">Blog</a></p> </blockquote> <h2 id="ddim">DDIM</h2> <ul> <li>Score Function : data 분포 내 특정 위치에서의 <code class="language-plaintext highlighter-rouge">prob. distribution 변화량</code><br/> \(\nabla_{x_{t}} \text{log} q(x_{t} | x_{0}) = \nabla_{x} (- \frac{\| x_{t} - \sqrt{\bar \alpha_{t}} x_{0} \|^{2}}{2(1 - \bar \alpha_{t})}) = - \frac{x_{t} - \sqrt{\bar \alpha_{t}} x_{0}}{1 - \bar \alpha_{t}} = - \frac{\epsilon_{t}}{\sqrt{1 - \bar \alpha_{t}}}\)<br/> where noise \(\epsilon_{t} = \frac{1}{\sqrt{1 - \bar \alpha_{t}}}(x_{t} - \sqrt{\bar \alpha_{t}} x_{0})\) <ul> <li>즉, <code class="language-plaintext highlighter-rouge">noise 예측 모델</code>은 <code class="language-plaintext highlighter-rouge">상수배 취한 score function을 예측하는 모델</code>과 equivalent!</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Tweedie's Formula</code> :<br/> 정규 분포에서 a sample \(x\) 를 sampling할 때 해당 data의 <code class="language-plaintext highlighter-rouge">true mean 값을 estimate</code>하는 방법 <ul> <li>a sample \(x\) 에, 분산과 score function을 곱한 값을 더함<br/> 확률 밀도가 높은 방향의 보정적인 값을 더해주기 위함.</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-21-GenAI/1.PNG-480.webp 480w,/assets/img/2025-04-21-GenAI/1.PNG-800.webp 800w,/assets/img/2025-04-21-GenAI/1.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-04-21-GenAI/1.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Langevin dynamics :<br/> <code class="language-plaintext highlighter-rouge">Score Function을 알고 있다면</code>, data distribution을 몰랐더라도 Langevin dynamics를 통해 <code class="language-plaintext highlighter-rouge">data distribution을 알아낼 수 있음</code><br/> (Score Function이 중요한 이유!) <ul> <li><code class="language-plaintext highlighter-rouge">initial sample을 점진적으로 업데이트하여 목표 확률 분포에 도달하게</code> 만드는 iterative process<br/> \(x \leftarrow x + \eta \nabla_{x} text{log} q(x) + \sqrt{2 \eta} \epsilon\)<br/> where \(\epsilon \sim N(0, I)\)</li> <li>Annealed Langevin dynamics : <ul> <li>분산 \(\sigma_{t}\) 가 작으면 high density region에서는 정확한 결과 얻을 수 있지만 low density region에서는 부정확함<br/> 반대로, 분산 \(\sigma_{t}\) 가 크면 low density region에서는 어느 정도 성능 낼 수 있지만 high density region에서는 비교적 정확도가 떨어짐</li> <li>초기에는 큰 분산 \(\sigma_{t}\) 값을 통해 low density region을 학습하고,<br/> 이후에 점점 분산 \(\sigma_{t}\) 값을 줄여 가면서<br/> 나중에서는 작은 분산 \(\sigma_{t}\) 값으로 high density region을 학습</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-21-GenAI/3.PNG-480.webp 480w,/assets/img/2025-04-21-GenAI/3.PNG-800.webp 800w,/assets/img/2025-04-21-GenAI/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-04-21-GenAI/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-21-GenAI/4.PNG-480.webp 480w,/assets/img/2025-04-21-GenAI/4.PNG-800.webp 800w,/assets/img/2025-04-21-GenAI/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-04-21-GenAI/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>DDIM (Denoising Diffusion Implicit Models) : <ul> <li>Comparison : <ul> <li>GAN : High Quality and Fast Sampling, but Mode Collapse</li> <li>VAE : Fast Smplint and Mode Converge, but Low Quality</li> <li>DDPM : High Quality and Mode Converge, but Slow Sampling</li> </ul> </li> <li>Comparison : <ul> <li>DDPM :<br/> \(q(x_{t} | x_{t-1})\) 라는 Markov Chain이 주어지고, 이를 기반으로 \(q(x_{t} | x_{0})\) 과 \(q(x_{t-1} | x_{t}, x_{0})\) 계산</li> <li><code class="language-plaintext highlighter-rouge">DDIM</code> : non-Markov Chain을 사용하기 위해 (더 빠르게 Sampling하기 위해)<br/> \(q(x_{t-1} | x_{t}, x_{0})\) 을 먼저 정의하고, 이를 기반으로 \(q(x_{t} | x_{0})\) 과 \(q(x_{t} | x_{t-1})\) 정의<br/> (유도 과정 : <a href="https://velog.io/@guts4/Basic-Generative-Model-DDIM-Score-based-CFG1">Link</a>)</li> </ul> </li> <li>DDIM Summary : <ul> <li>DDPM은 DDIM의 special case이다<br/> DDIM은 DDPM보다 <code class="language-plaintext highlighter-rouge">평균을 나타내는 수식이 복잡해졌을 뿐</code>, 두 모델의 Sampling 차이는 없다<br/> 즉, DDIM은 DDPM보다 <code class="language-plaintext highlighter-rouge">빠르게 Sampling</code>할 수 있는 거고 <code class="language-plaintext highlighter-rouge">Sampling 결과에는 차이가 없으므로</code> 다시 학습 시킬 필요가 없다</li> <li>DDPM에서는 고정된 분산 값으로 forward process에서 동작하는데, DDIM에서는 <code class="language-plaintext highlighter-rouge">선택 가능한 분산 값</code>으로 동작한다<br/> 이 때, variance = 0 으로 설정할 경우 deterministic하다 (데이터들이 모두 평균에 위치하므로 경로가 정해짐)</li> <li>적은 수의 step으로도 좋은 결과를 얻을 수 있다</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-21-GenAI/5.PNG-480.webp 480w,/assets/img/2025-04-21-GenAI/5.PNG-800.webp 800w,/assets/img/2025-04-21-GenAI/5.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-04-21-GenAI/5.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="cfg">CFG</h2> <ul> <li>CFG (Classifier-Free Guidance) :<br/> DDPM or DDIM을 통해 image를 generate하는데,<br/> 원하는 특정 class의 image를 generate하고 싶을 경우 CFG 사용! <ul> <li>핵심 idea : class label \(y\) 를 condition으로 줘서 noise 예측<br/> \(\tilde \epsilon_{\theta} (x_{t}, y, t) = \lambda \hat \epsilon_{\theta} (x_{t}, y, t) + (1 - \lambda) \hat \epsilon_{\theta} (x_{t}, \phi, t)\)<br/> (증명 과정 : <a href="https://velog.io/@guts4/Basic-Generative-Model-CFG2-Latent-Diffusion-ControlNet-LoRA">Link</a>) <ul> <li>classifier 없이 conditional 정보 학습 가능</li> <li>\(\lambda\) 값이 클수록 해당 class \(y\) 에 대한 더 정확한 image가 생성되지만 diversity가 떨어짐 (trade-off)</li> </ul> </li> <li>장단점 : <ul> <li>장점 : <ul> <li>classifier가 없으므로 추가 학습이 없어 적용하기 쉬움</li> <li>text 뿐만 아니라 image 등 어떠한 정보도 condition으로 들어갈 수 있음<br/> e.g. Camera Param.를 condition으로 넣은 Zero-1-to-3</li> <li>GAN의 단점인 Mode Collapse도 없음</li> </ul> </li> <li>단점 : <ul> <li>conditional noise term과 unconditional noise term을 평가해야 해서 시간이 더 걸림</li> </ul> </li> </ul> </li> <li>Negative Prompt :<br/> CFG에서의 Null condition \(\phi\) 대신<br/> 만들고 싶지 않은 형태의 text prompt (Negative Prompt)를 넣어주면<br/> 해당 text prompt와 반대되는 결과가 나옴<br/> \(\tilde \epsilon_{\theta} (x_{t}, y, t) = \lambda \hat \epsilon_{\theta} (x_{t}, y_{+}, t) + (1 - \lambda) \hat \epsilon_{\theta} (x_{t}, y_{-}, t)\)</li> </ul> </li> </ul> <h2 id="latent-diffusion">Latent Diffusion</h2> <h3 id="controlnet">ControlNet</h3> <h2 id="ddim-inversion">DDIM Inversion</h2> <h2 id="sds">SDS</h2> <h2 id="dds">DDS</h2> <h2 id="pds">PDS</h2> <h2 id="inverse-problem">Inverse Problem</h2> <h2 id="flow-matching">Flow Matching</h2>]]></content><author><name></name></author><category term="generative"/><category term="diffusion"/><category term="DDIM"/><category term="SDS"/><category term="DDS"/><category term="PDS"/><summary type="html"><![CDATA[Basic Generative Model]]></summary></entry><entry><title type="html">3DDST</title><link href="https://semyeong-yu.github.io/blog/2025/3DDST/" rel="alternate" type="text/html" title="3DDST"/><published>2025-04-14T12:00:00+00:00</published><updated>2025-04-14T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/3DDST</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/3DDST/"><![CDATA[<h2 id="3ddst---generating-images-with-3d-annotations-using-diffusion-models-iclr-2024-spotlight">3DDST - Generating Images with 3D Annotations Using Diffusion Models (ICLR 2024 Spotlight)</h2> <h4 id="wufei-ma-qihao-liu-jiahao-wang-angtian-wang-xiaoding-yuan-yi-zhang-zihao-xiao-guofeng-zhang-beijia-lu-ruxiao-duan-yongrui-qi-adam-kortylewski-yaoyao-liu-alan-yuille">Wufei Ma, Qihao Liu, Jiahao Wang, Angtian Wang, Xiaoding Yuan, Yi Zhang, Zihao Xiao, Guofeng Zhang, Beijia Lu, Ruxiao Duan, Yongrui Qi, Adam Kortylewski, Yaoyao Liu, Alan Yuille</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2306.08103">https://arxiv.org/abs/2306.08103</a><br/> project website :<br/> <a href="https://ccvl.jhu.edu/3D-DST/">https://ccvl.jhu.edu/3D-DST/</a></p> </blockquote> <blockquote> <p>핵심 :<br/> 3D structure(shape) 정보를 담고 있는, CAD model로부터 render한 image의 edge map을<br/> ControlNet의 visual prompts (3D geometry control)로 넣어줌으로써<br/> Diffusion model이 특정 3D structure를 가진 image를 generate할 수 있게 함!<br/> 즉, Diffusion model generates new images where its 3D geometry can be explicitly controlled<br/> 결과적으로 Diffusion model로 data generation할 때 we can conveniently acquire GT 3D annotations for the generated 2D images</p> </blockquote> <h2 id="background">Background</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-14-3DDST/2.PNG-480.webp 480w,/assets/img/2025-04-14-3DDST/2.PNG-800.webp 800w,/assets/img/2025-04-14-3DDST/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-04-14-3DDST/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>ControlNet : <ul> <li>대규모 Diffusion model (e.g. Stable Diffusion)의 weight를 trainable copy와 locked copy로 복제한 뒤<br/> locked copy는 수많은 internet-scale images로 학습한 network 능력을 보존하고<br/> trainable copy는 specific task별 dataset으로 학습하여 control을 학습 <ul> <li>원래 weight를 freeze하고 이를 copy해서 사본 weight를 학습하는 이유는<br/> dataset (for specific task)이 작을 때의 overfitting을 방지하고 internet-scale로 학습한 대형 Diffusion model의 품질을 보존</li> <li>zero-convolution : 처음에 weight, bias가 0으로 초기화되고, 점점 학습 <ul> <li>optimization 완전 처음에는 zero-convolution output이 0이라서 ControlNet이 없는 것처럼 작동하였다가<br/> 점점 zero-convolution의 weight, bias가 학습되면서 ControlNet이 쓰임</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">ControlNet</code>은 원래 Diffusion model의 <code class="language-plaintext highlighter-rouge">encoder의 trainable copy</code>로 구성되어 있고,<br/> <code class="language-plaintext highlighter-rouge">input</code>과 <code class="language-plaintext highlighter-rouge">control</code>을 <code class="language-plaintext highlighter-rouge">ControlNet에 입력</code>으로 넣어서 나온 <code class="language-plaintext highlighter-rouge">ControlNet의 출력</code>을<br/> 원래 Diffusion model의 <code class="language-plaintext highlighter-rouge">decoder</code>에 넣어줌</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-14-3DDST/3.PNG-480.webp 480w,/assets/img/2025-04-14-3DDST/3.PNG-800.webp 800w,/assets/img/2025-04-14-3DDST/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-04-14-3DDST/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>ControlNet for Stable Diffusion : <ul> <li>Component : <ul> <li>Text Encoder : OpenAI CLIP</li> <li>Time (diffusion time-step) Encoder : Positional Encoding</li> <li>Stable Diffusion transforms an 512x512 image into a 64x64 latent image,<br/> so use small network to also transform an image control \(c_{i}\) into a 64x64 feature map</li> </ul> </li> <li>Role : <ul> <li>ControlNet controls each level of U-Net of Stable Diffusion</li> <li>Efficient since<br/> 원래 모델은 잠겨 있기 때문에 원래 모델의 encoder에 대한 기울기 계산은 필요하지 않아서<br/> 원래 모델에서 기울기 계산의 절반을 피할 수 있으므로<br/> 학습 속도 빨라지고 GPU 메모리도 절약 가능</li> </ul> </li> </ul> </li> </ul> <h2 id="method">Method</h2> <h3 id="overview">Overview</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-14-3DDST/4.PNG-480.webp 480w,/assets/img/2025-04-14-3DDST/4.PNG-800.webp 800w,/assets/img/2025-04-14-3DDST/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-04-14-3DDST/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-14-3DDST/13.PNG-480.webp 480w,/assets/img/2025-04-14-3DDST/13.PNG-800.webp 800w,/assets/img/2025-04-14-3DDST/13.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-04-14-3DDST/13.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Contribution : <ul> <li>기존 diffusion models의 한계 : <ul> <li>generated image의 3D geometry를 control할 수 없음.<br/> 하려 해도 3D pose key points 등 3D annotations 필요</li> <li>주로 simple text prompts에만 의존</li> <li>robust model로 훈련시키기 위해 OOD scenarios가 필요한데,<br/> OOD scenarios를 위한 images를 생성하기 어렵.</li> </ul> </li> <li>본 논문 (3D-DST) :<br/> graphic-based rendering을 통해 3D shape 정보를 가진 3D visual prompts를 만든 뒤 ControlNet에 3D geometry control로 넣어줌으로써<br/> Diffusion model이 specific 3D structure를 가진 images를 generate할 수 있음<br/> 즉, shape-aware training process enables Diffusion model to generate new images where its 3D geometry can be explicitly controlled<br/> 결과적으로 Diffusion model로 data generation할 때 we can conveniently acquire GT 3D annotations for the generated 2D images</li> <li>ShapeNet, Pix3D 등 다양한 3D object dataset으로 evaluate함으로써<br/> 이전 diffusion model에 비해 본 논문의 3D-aware diffusion model이 generated images’ 3D shape control을 얼마나 잘 하는지 보여줌<br/> (3D shape similarity 등의 metrics로 평가) <ul> <li>여러 vision tasks (e.g. classification, 3D pose estimation)에서, 그리고 ID and OOD setting 모두에서,<br/> 3D-DST data로 pre-train한 뒤 specific vision task에 적용하면 generated images는 높은 performance 보임</li> <li>useful for applications like 3D modeling, product design, and VR where the precise 3D object shape is important</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-14-3DDST/1.PNG-480.webp 480w,/assets/img/2025-04-14-3DDST/1.PNG-800.webp 800w,/assets/img/2025-04-14-3DDST/1.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-04-14-3DDST/1.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-14-3DDST/6.PNG-480.webp 480w,/assets/img/2025-04-14-3DDST/6.PNG-800.webp 800w,/assets/img/2025-04-14-3DDST/6.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-04-14-3DDST/6.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="prompt-generation">Prompt Generation</h3> <ul> <li><code class="language-plaintext highlighter-rouge">3D Visual Prompt</code> Generation :<br/> 3D shape repository(e.g. ShapeNet, Objaverse, OmniObject3D)로부터 3D CAD model을 얻은 뒤<br/> <code class="language-plaintext highlighter-rouge">3D CAD model</code>로부터 render한 image의 <code class="language-plaintext highlighter-rouge">edge map</code>을 visual prompt (<code class="language-plaintext highlighter-rouge">3D geometry control</code>)로 사용 <ul> <li>rendered image by 3D CAD model from ShapeNet, Objaverse, OmniObject3D<br/> \(\rightarrow\) edge map by Canny edge filter<br/> \(\rightarrow\) 3D voxel grid representation by MLP<br/> \(\rightarrow\) combine 3D structure info. from 3D voxel grid features and 2D appearance info. from 2D image features</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Text Prompt</code> Generation :<br/> not only produce images with higher realsim and diversity<br/> but also improve OOD robustness of models pre-trainded on our 3DDST data <ul> <li>initial text prompt : class name \(w\) + keyword of CAD model \(k\)</li> <li>text prompt : improve the diversity and richness by LLM(LLaMA)</li> </ul> </li> </ul> <h2 id="result">Result</h2> <ul> <li>Result : <ul> <li>ID and OOD Image Classification : <ul> <li>Dataset :<br/> ImageNet-100 (ID), ImageNet-200 (ID), ImageNet-R (OOD)</li> <li>3D-DST Data Generation :<br/> 각 object class에 대해 ShapeNet과 Objaverse로부터 약 30개의 CAD models를 collect하여<br/> 각 object class에 대해 2,500개의 images를 generate</li> <li>Notation : <ul> <li>Baseline: pre-train 없이 train on target dataset</li> <li>Text2Img: 3D control 없이 Text2Img data로 pre-train한 뒤 fine-tune on target dataset</li> <li>3D-DST : 3D control 있는 3D-DST data로 pre-train한 뒤 fine-tune on target dataset</li> <li>LLM : LLM prompt 사용</li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-14-3DDST/7.PNG-480.webp 480w,/assets/img/2025-04-14-3DDST/7.PNG-800.webp 800w,/assets/img/2025-04-14-3DDST/7.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-04-14-3DDST/7.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-14-3DDST/5.PNG-480.webp 480w,/assets/img/2025-04-14-3DDST/5.PNG-800.webp 800w,/assets/img/2025-04-14-3DDST/5.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-04-14-3DDST/5.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-14-3DDST/9.PNG-480.webp 480w,/assets/img/2025-04-14-3DDST/9.PNG-800.webp 800w,/assets/img/2025-04-14-3DDST/9.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-04-14-3DDST/9.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Ablation Study : <ul> <li>Data Generation methods on Image Classification : <ul> <li>Notation : <ul> <li>ImageNet edges : Diffusion-based로 Data Generate하는 데 ImageNet으로부터 edges 얻어서 condition으로 사용</li> <li>Rendering + BG2D : CAD model로부터 rendering해서 Data Generate하는 데 random 2D image bg 사용</li> <li>Rendering + BG3D : CAD model로부터 rendering해서 Data Generate하는 데 random 3D environment bg 사용</li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-14-3DDST/8.PNG-480.webp 480w,/assets/img/2025-04-14-3DDST/8.PNG-800.webp 800w,/assets/img/2025-04-14-3DDST/8.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-04-14-3DDST/8.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Result : <ul> <li>Robust Category-level 3D Pose Estimation : <ul> <li>Dataset :<br/> PASCAL3D+, ObjectNet3D, OOD-CV</li> <li>Notation : <ul> <li>ResNet : ResNet extended with a pose classification head</li> <li>NeMo : SOTA 3D pose estimation model</li> <li>3D-DST : pre-train the model on 3D-DST data</li> <li>AugMix : pre-train the model with strong data augmentation (ICLR 2020)</li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-14-3DDST/10.PNG-480.webp 480w,/assets/img/2025-04-14-3DDST/10.PNG-800.webp 800w,/assets/img/2025-04-14-3DDST/10.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-04-14-3DDST/10.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-14-3DDST/11.PNG-480.webp 480w,/assets/img/2025-04-14-3DDST/11.PNG-800.webp 800w,/assets/img/2025-04-14-3DDST/11.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-04-14-3DDST/11.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Ablation Study : <ul> <li>Types of 3D controls :<br/> edge maps / MiDaS-predicted depths / Blender-rendered depths<br/> (use the same 3D model and text prompts) <ul> <li>result : edge maps 쓰는 게 realism 및 fg/bg clarity 측면에서 가장 좋음</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-14-3DDST/12.PNG-480.webp 480w,/assets/img/2025-04-14-3DDST/12.PNG-800.webp 800w,/assets/img/2025-04-14-3DDST/12.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-04-14-3DDST/12.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Data Release : <ul> <li>Aligned CAD models from 1000 classes in ImageNet-1k <a href="https://huggingface.co/datasets/ccvl/3D-DST-models">Link</a> :<br/> ImageNet-1k의 각 class에 대해<br/> ShapeNet, Objaverse, OmniObject3D로부터 3D CAD model을 얻은 뒤<br/> CAD model의 canonical pose를 align</li> <li>LLM-generated captions for 1000 classes in ImageNet-1k <a href="https://huggingface.co/datasets/ccvl/3D-DST-captions">Link</a></li> <li>3D-DST data for 1000 classes in ImageNet-1k <a href="https://huggingface.co/datasets/ccvl/3D-DST-data">Link</a> :<br/> 위의 3D Visual Prompt, Text Prompt를 이용하여 generate한 3D-DST image</li> </ul> </li> </ul> <h2 id="limitation">Limitation</h2> <ul> <li>Failure Case : <ul> <li>한계 :<br/> image with challenging and uncommon viewpoints<br/> e.g. car from below, guitar from side</li> <li>대응 :<br/> K-fold consistency filter (KCF)를 적용하여<br/> ensemble model의 predictions를 기반으로<br/> good images를 일부 제거하더라도 failed images를 자동으로 제거함으로써<br/> good images의 비율을 높임 <ul> <li>KCF는 여전히 limited..<br/> diffusion-generated dataset에서 failed samples를 감지하고 제거하는 건 여전히 challenging problem</li> </ul> </li> </ul> </li> <li>Limitation : <ul> <li>increased computational cost, complexity, training time, model size<br/> by additional 3D shape encoding <ul> <li>can be a barrier to real-world deployment</li> </ul> </li> <li>evaluation is focused on 비교적 간단한 3D object datasets <ul> <li>need to be generalized to more complex, real-world 3D scenes and geometires</li> </ul> </li> <li>3D shape control이 다른 desired attributes(photorealism, semantic consistency, coherence 등)와 얼마나 잘 합쳐지는지 balancing하는 건<br/> 앞으로 diffusion model architectures 및 training procedures 개선과 함께 고려되어야 함</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="postprocessing"/><category term="single"/><category term="step"/><category term="diffusion"/><summary type="html"><![CDATA[Generating Images with 3D Annotations Using Diffusion Models (ICLR 2024 Spotlight)]]></summary></entry><entry><title type="html">7DGS (6DGS)</title><link href="https://semyeong-yu.github.io/blog/2025/7DGS/" rel="alternate" type="text/html" title="7DGS (6DGS)"/><published>2025-04-08T12:00:00+00:00</published><updated>2025-04-08T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/7DGS</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/7DGS/"><![CDATA[<h2 id="7dgs---unified-spatial-temporal-angular-gaussian-splatting">7DGS - Unified Spatial-Temporal-Angular Gaussian Splatting</h2> <h4 id="zhongpai-gao-benjamin-planche-meng-zheng-anwesa-choudhuri-terrence-chen-ziyan-wu">Zhongpai Gao, Benjamin Planche, Meng Zheng, Anwesa Choudhuri, Terrence Chen, Ziyan Wu</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2503.07946">https://arxiv.org/abs/2503.07946</a><br/> project website :<br/> <a href="https://gaozhongpai.github.io/7dgs/">https://gaozhongpai.github.io/7dgs/</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>7D = 3D spatial + 3D directional + 1D temporal</li> <li>고정된 covariance를 가진 납작한 anisotropic 3DGS는 view-dependent effect에 취약함.<br/> 근데 7DGS는 각기 다른 \(t, d\) 에 맞춰서 학습되었기 때문에 (customized 느낌) view-dependent and dynamic 잘 표현 가능</li> <li>Adaptive Gaussian Refinement : \(t, d\) 에 따라 7DGS의 param.(spatial, temporal, directional mean and covariance)를 MLP로 변형시킴.</li> <li>Slice : 3DGS의 기존 framework를 그대로 사용하기 위해 rendering \(t, d\) 에 대해 7DGS를 3DGS로 slice하는데,<br/> covariance는 미리 계산해둘 수 있고 spatial mean과 opacity는 rendering할 때마다 \(t, d\) 에 따라 달라짐.<br/> 7DGS를 3DGS로 slice할 때 \(t, d\) 가 mean \(\mu_{t}, \mu_{d}\) 에서 멀수록 spatial mean \(\mu_{p}\) 을 많이 변형시킴.</li> </ol> </blockquote> <h2 id="direction-aware-6dgs">Direction-Aware 6DGS</h2> <ul> <li>Abstract : <ul> <li>3DGS와 N-dimensional Gaussian (N-DG) 을 결합하여<br/> direction 정보도 param.에 반영함으로써<br/> non-planar geometry, parallax effects, view-dependent effects를 효과적으로 모델링</li> <li>learnable param.만 달라졌을 뿐<br/> 특정 direction에 대해 6DGS를 3DGS로 slice하면 기존 3DGS framework와 동일하게 작동</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-08-7DGS/2.PNG-480.webp 480w,/assets/img/2025-04-08-7DGS/2.PNG-800.webp 800w,/assets/img/2025-04-08-7DGS/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-04-08-7DGS/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Parameter : <ul> <li>기존 3DGS param. : <ul> <li>mean</li> <li>covariance (quaternion, scale)</li> <li>opacity</li> <li>SH coeff.</li> </ul> </li> <li>6DGS param. :<br/> 3D position (공간) 정보 뿐만 아니라 <code class="language-plaintext highlighter-rouge">3D direction (방향) 정보</code>도 param.에 담음!<br/> 그래서 이제 color 뿐만 아니라 <code class="language-plaintext highlighter-rouge">position, covariance, opacity도 모두 view-dependent</code>! <ul> <li>position mean \(\in R^{3}\)</li> <li>direction mean \(\in R^{3}\)</li> <li>covariance matrix \(\Sigma = L L^{T} \in R^{6 \times 6}\)<br/> for lower triangular matrix \(L\) <ul> <li>diagonal elements : positive by exponential activation</li> <li>off-diagonal elements : \(\in [-1, 1]\) by sigmoid activation</li> </ul> </li> <li>opacity \(\in R^{1}\)</li> <li>color \(\in R^{3}\)</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Slice 6DGS into conditional 3DGS</code> :<br/> 공간, 방향 정보를 갖고 있는 6DGS에 대해<br/> 특정 방향이 정해지면<br/> 조건부 분포를 통해 해당 방향에서의 3DGS로 slice할 수 있음</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-08-7DGS/1.PNG-480.webp 480w,/assets/img/2025-04-08-7DGS/1.PNG-800.webp 800w,/assets/img/2025-04-08-7DGS/1.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-04-08-7DGS/1.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-08-7DGS/3.PNG-480.webp 480w,/assets/img/2025-04-08-7DGS/3.PNG-800.webp 800w,/assets/img/2025-04-08-7DGS/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-04-08-7DGS/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Slice 6DGS into conditional 3DGS </div> <ul> <li>Conditional Gaussian : <ul> <li>Conditional Mean \(\mu_{cond}\) :<br/> Best Linear Unbiased Estimator for<br/> position mean, adjusting dynamically based on the viewing direction <ul> <li>rendering할 때마다 view direction \(d\) 로 계산해야 함</li> </ul> </li> <li>Conditional Covariance \(\Sigma_{cond}\) :<br/> joint covariance \(\Sigma\) 에서 \(X_{d}\) 와의 correlation에 해당하는 부분을 제거하고 남은 residual uncertainty in \(X_{p}\) <ul> <li>rendering 이전에 미리 계산해놓을 수 있음</li> </ul> </li> <li>Conditional Opacity \(\alpha_{cond}\) :<br/> opacity \(\alpha\) 를 direction의 PDF로 attenuate하여<br/> view-dependent opacity 반영<br/> (각도에 따라 opacity도 달라짐!) <ul> <li>rendering할 때마다 view direction \(d\) 로 계산해야 함</li> <li>\(0 \lt \lambda_{opa} \lt 1\) : view-direction이 opacity에 얼마나 영향을 미칠 건지 (hyper-param. 또는 per-Gaussian learnable param.)</li> </ul> </li> </ul> </li> <li>Adaptive Control : <ul> <li>기존 3DGS에서처럼 Adaptive Density Control (cloning, splitting) 할 때만 \(R, S\) 필요한데,<br/> 6DGS의 covariance \(\Sigma = LL^{T}\) 에서는 바로 얻을 수 없으므로<br/> \(\Sigma_{cond} = UDU^{T}\) 로 분해 (SVD)해서 \(R, S\) 얻음 <ul> <li>\(R = U\) and \(S = \sqrt{\text{diag}(D)}\) 이 때, \(R\) 이 right-handed coord.를 따르도록 하기 위해<br/> last column (z축)의 부호 조정<br/> \(R_{:, 3} = R_{:, 3} \cdot \text{sign}(\text{det}(R))\)</li> </ul> </li> <li>opacity \(\alpha\) 값이 threshold보다 작거나, 크기가 매우 크면 prune</li> </ul> </li> </ul> <h2 id="spatial-temporal-angular-7dgs">Spatial-Temporal-Angular 7DGS</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-08-7DGS/5.PNG-480.webp 480w,/assets/img/2025-04-08-7DGS/5.PNG-800.webp 800w,/assets/img/2025-04-08-7DGS/5.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-04-08-7DGS/5.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>7DGS param. : <ul> <li>position mean \(\mu_{p} \in R^{3}\)</li> <li><code class="language-plaintext highlighter-rouge">temporal</code> mean \(\mu_{t} \in R^{1}\)</li> <li><code class="language-plaintext highlighter-rouge">directional</code> mean \(\mu_{d} \in R^{3}\)</li> <li>covariance matrix \(\Sigma = L L^{T} \in R^{7 \times 7}\)<br/> for lower triangular matrix \(L\)</li> <li>opacity \(\in R^{1}\)</li> <li>color \(\in R^{3}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-08-7DGS/4.PNG-480.webp 480w,/assets/img/2025-04-08-7DGS/4.PNG-800.webp 800w,/assets/img/2025-04-08-7DGS/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-04-08-7DGS/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Slice 7DGS into conditional 3DGS</code> :<br/> 공간, 방향, 시간 정보를 갖고 있는 7DGS에 대해<br/> 특정 방향, 시간이 정해지면<br/> 조건부 분포를 통해 해당 방향, 시간에서의 3DGS로 slice할 수 있음 <ul> <li>Conditional Mean \(\mu_{cond}\) : <ul> <li><code class="language-plaintext highlighter-rouge">rendering할 때마다</code>의 time \(t\), view-direction \(d\) 로 계산해야 함</li> </ul> </li> <li>Conditional Covariance \(\Sigma_{cond}\) :<br/> temporal, directional variance를 제거하고 남은 residual uncertainty in \(X_{p}\) <ul> <li>rendering 이전에 <code class="language-plaintext highlighter-rouge">미리</code> 계산해놓을 수 있음</li> </ul> </li> <li>Conditional Opacity \(\alpha_{cond}\) :<br/> opacity \(\alpha\) 를 direction의 PDF와 temporal PDF로 attenuate <br/> (각도와 시간에 따라 opacity도 달라짐!) <ul> <li><code class="language-plaintext highlighter-rouge">rendering할 때마다</code>의 time \(t\), view-direction \(d\) 로 계산해야 함</li> <li>positive \(\lambda_{t}, \lambda_{d}\) : time, view-direction이 얼마나 영향 미칠 건지</li> <li>만약 rendering하려는 current time \(t\) 와 view-direction \(d\) 가 평균값에서 멀다면<br/> 해당 Gaussian은 rendering에 덜 기여</li> </ul> </li> <li>그렇게 구한 conditional \(\mu_{cond}, \Sigma_{cond}, \alpha_{cond}\) 를 기존 3DGS framework에 그대로 적용</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Adaptive Gaussian Refinement</code> : <ul> <li>시간 \(t\) 가 지남에 따라<br/> sliced 3DGS의 spatial mean \(\mu_{cond}\) 과 opacity \(\alpha_{cond}\) 는 dynamically 변하지만<br/> sliced 3DGS의 shape (covariance) \(\Sigma_{cond}\) 는 static 이므로 complex dynamic behavior 표현에 방해될 수 있음</li> <li>Adaptive Gaussian Refinement :<br/> 가벼운 MLP로 7DGS param.를 dynamically update <ul> <li>feature \(f = \text{concat}[\mu_{p}, \mu_{t}, \mu_{d}, \gamma(t)]\)</li> <li>Refine Gaussian param. by two-layer MLP \(\phi\) :<br/> 그럼 이제 (slice하기 전) 7DGS의 spatial, temporal, directional <code class="language-plaintext highlighter-rouge">mean과 covariance가 dynamically 시간에 따라 변하는 값!</code> <ul> <li>mean :<br/> \(\hat \mu_{p} = \mu_{p} + \phi_{p} (f)\) and<br/> \(\hat \mu_{t} = \mu_{t} + \phi_{t} (f)\) and<br/> \(\hat \mu_{d} = \mu_{d} + \phi_{d} (f)\) and</li> <li>covariance :<br/> \(\hat l = l + \phi_{l} (f)\)<br/> where \(l\) is vectorized lower-triangular elements of \(L\)</li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-04-08-7DGS/6.PNG-480.webp 480w,/assets/img/2025-04-08-7DGS/6.PNG-800.webp 800w,/assets/img/2025-04-08-7DGS/6.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-04-08-7DGS/6.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Optimization and Rendering Pipeline : <ul> <li>Adaptive Control : <ul> <li>기존 3DGS에서처럼 Adaptive Density Control (cloning, splitting) 할 때만 \(R, S\) 필요한데,<br/> 7DGS의 covariance \(\Sigma = LL^{T}\) 에서는 바로 얻을 수 없으므로<br/> sliced 3DGS의 covariance를 \(\Sigma_{cond} = UDU^{T}\) 로 분해 (SVD)해서 \(R, S\) 얻음 <ul> <li>\(R = U\) and \(S = \sqrt{\text{diag}(D)}\) 이 때, \(R\) 이 right-handed coord.를 따르도록 하기 위해<br/> last column (z축)의 부호 조정<br/> \(R_{:, 3} = R_{:, 3} \cdot \text{sign}(\text{det}(R))\) s.t. \(\text{det}(R) \gt 0\)</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Cloning</code></li> <li><code class="language-plaintext highlighter-rouge">Splitting</code> :<br/> \(\Sigma_{pt}\) 의 크기 (spatial-temporal correlation)가 threshold보다 크거나<br/> \(\Sigma_{t}\) 로부터 유도한 normalized temporal scale이 threshold보다 크면<br/> Split! <ul> <li>이는 motion dynamic이 큰 (많이 움직이는) 영역에 7DGS가 많이 분포하도록 함</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Pruning</code> :<br/> opacity \(\alpha\) 값이 threshold보다 작거나, 크기가 매우 크면 prune</li> </ul> </li> <li>Rendering :<br/> 7DGS를 3DGS로 slice하면 기존 3DGS의 framework와 동일하게 작동<br/> (동일한 loss, optimizer, hyperparam. setting 사용<br/> except increased \(\alpha\) threshold \(\tau_{min} = 0.01\))</li> </ul> </li> <li>Results :<br/> D-NeRF, Technicolor, 7DGS-PBR Dataset에 대해<br/> 훨씬 적은 Gaussian points로도<br/> PSNR 7.36dB 이상 올리고 400 FPS 이상의 render speed 유지하여 SOTA 달성</li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br/> 7DGS를 3DGS로 slice할 때 3D spatial mean \(\mu_{cond}\) 이 \(t, d\) 에 따라 달라지는 것도 합리적인가요?</p> </li> <li> <p>A1 :<br/> 고정된 covariance를 가진 납작한 anisotropic 3DGS는 view-dependent effect에 취약함.<br/> \(t, d\) 가 바뀜에 따라 7DGS의 param.(spatial, temporal, directional mean and covariance)를 Adaptive Gaussian Refinement로 변형시키는 것도 합리적이고,<br/> 7DGS를 3DGS로 slice할 때 \(t, d\) 가 mean \(\mu_{t}, \mu_{d}\) 에 가까울수록 spatial mean \(\mu_{p}\) 을 덜 변형시키는 것도 합리적임.<br/> (7DGS를 3DGS로 slice할 때 \(t, d\) 가 mean \(\mu_{t}, \mu_{d}\) 에서 멀수록 spatial mean \(\mu_{p}\) 을 많이 변형시키는 것도 합리적임.)<br/> 결국 \(t, d\) 에 따라 mean, covariance가 전부 달라지므로 아예 새로운 Gaussian을 만드는 것과 같은데 학습만 잘 되면 성능 높아질 수 있음.</p> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="postprocessing"/><category term="single"/><category term="step"/><category term="diffusion"/><summary type="html"><![CDATA[Unified Spatial-Temporal-Angular Gaussian Splatting]]></summary></entry><entry><title type="html">Difix3D+</title><link href="https://semyeong-yu.github.io/blog/2025/Difix3D/" rel="alternate" type="text/html" title="Difix3D+"/><published>2025-03-15T12:00:00+00:00</published><updated>2025-03-15T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/Difix3D</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/Difix3D/"><![CDATA[<h2 id="difix3d---improving-3d-reconstructions-with-single-step-diffusion-models-cvpr-2025">Difix3D+ - Improving 3D Reconstructions with Single-Step Diffusion Models (CVPR 2025)</h2> <h4 id="jay-zhangjie-wu-yuxuan-zhang-haithem-turki-xuanchi-ren-jun-gao-mike-zheng-shou-sanja-fidler-zan-gojcic-huan-ling">Jay Zhangjie Wu, Yuxuan Zhang, Haithem Turki, Xuanchi Ren, Jun Gao, Mike Zheng Shou, Sanja Fidler, Zan Gojcic, Huan Ling</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2503.01774">https://arxiv.org/abs/2503.01774</a><br/> project website :<br/> <a href="https://research.nvidia.com/labs/toronto-ai/difix3d/">https://research.nvidia.com/labs/toronto-ai/difix3d/</a></p> </blockquote> <blockquote> <p>핵심 :<br/> nearly real-time single-step 2D diffusion model을<br/> 3D artifacts removing task에 맞게 fine-tune한 뒤,<br/> 3D model에 distill하여 progressively update하거나<br/> post-processing으로 씀!</p> </blockquote> <h2 id="contribution">Contribution</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-15-Difix3D/1.PNG-480.webp 480w,/assets/img/2025-03-15-Difix3D/1.PNG-800.webp 800w,/assets/img/2025-03-15-Difix3D/1.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-15-Difix3D/1.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Difix3D+ 설명 : <ul> <li>Stage 1)<br/> sparse view로 학습된 못난 3D model로 rendering한 novel-view의 <code class="language-plaintext highlighter-rouge">artifacts를 제거</code>하도록<br/> <code class="language-plaintext highlighter-rouge">single-step 2D diffusion</code> model (Difix)을 <code class="language-plaintext highlighter-rouge">minimal fine-tuning</code></li> <li>Stage 2)<br/> fine-tuned Difix를 이용하여<br/> 3D model로 distill하거나 post-processing <ul> <li>Difix 역할 1) <code class="language-plaintext highlighter-rouge">reconstruction at training phase</code> :<br/> fine-tuned Difix를 적용하여 clean pseudo-training views (training set)을 iteratively <code class="language-plaintext highlighter-rouge">augment</code>함으로써<br/> fine-tuned Difix의 prior를 3D model로 <code class="language-plaintext highlighter-rouge">distill</code></li> <li>Difix 역할 2) <code class="language-plaintext highlighter-rouge">neural enhancer at inference phase</code> :<br/> single-step 2D diffusion model은 inference speed 빠르므로<br/> 남은 residual artifacts를 제거하기 위해 improved recon. output에 직접 Difix 적용<br/> (<code class="language-plaintext highlighter-rouge">near real-time post-processing</code>)</li> </ul> </li> </ul> </li> <li>Difix3D+ Contribution : <ul> <li><code class="language-plaintext highlighter-rouge">single-step diffusion model</code> :<br/> ADD (Adversarial Diffusion Distillation)으로 학습된 single-step diffusion model은<br/> single-step만 U-Net을 query하므로 inference speed 빠름 <ul> <li>ADD (Adversarial Diffusion Distillation) <a href="https://ostin.tistory.com/305">Link</a> <a href="https://velog.io/@sckim0430/Adversarial-Diffusion-Distillation">Link</a> :<br/> SDS loss + GAN loss</li> <li>DMD (Distribution Matching Distillation) <a href="https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/dmd/">Link</a></li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">minimal fine-tuning</code> :<br/> 그래서 fine-tuning하는 데 single GPU로 only a few hours만 필요</li> <li><code class="language-plaintext highlighter-rouge">general</code> model :<br/> 모든 3D models (NeRF, 3DGS 등)에 사용 가능한 single model</li> <li>metrics : <ul> <li>artifacts 제거(fix)하므로 3D consistency 유지한 채 FID score 2배 이상 및 PSNR 1dB 이상 향상</li> <li>single-step diffusion model을 사용하므로 multi-step standard diffusion model보다 10배 이상 빠름</li> </ul> </li> </ul> </li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li>현재 NVS methods의 한계 :<br/> per-scene optimization framework <ul> <li>input이 <code class="language-plaintext highlighter-rouge">sparse</code>하거나<br/> input camera poses로부터 먼 <code class="language-plaintext highlighter-rouge">extreme novel view</code>를 rendering하거나<br/> <code class="language-plaintext highlighter-rouge">varying lighting</code> conditions 또는 <code class="language-plaintext highlighter-rouge">imperfect camera poses</code> 상황에서<br/> spurious(가짜) geometry 또는 missing regions 등 artifacts 생김</li> <li>underlying geometry를 제대로 반영하지 못한 채 (incorrect shape)<br/> inherent smoothness에만 의존하는 3D representation으로도<br/> training images’ radiance에 overfitting되는<br/> <code class="language-plaintext highlighter-rouge">shape-radiance ambiguity</code> 문제</li> </ul> </li> <li>large 2D generative model : <ul> <li>2D diffusion model prior :<br/> large internet-scale data를 학습하여 real-world images의 distribution을 잘 이해하고 있으므로 diffusion priors는 여러 분야에 generalize 가능 <ul> <li>inpainting <d-cite key="11">[1]</d-cite> <d-cite key="64">[2]</d-cite> <d-cite key="85">[3]</d-cite></li> <li>outpainting <d-cite key="5">[4]</d-cite> <d-cite key="62">[5]</d-cite> <d-cite key="76">[6]</d-cite></li> </ul> </li> <li>2D diffusion model prior to 3D : <ul> <li><code class="language-plaintext highlighter-rouge">multi-step으로 U-Net을 query</code> <d-cite key="25">[7]</d-cite> <d-cite key="41">[8]</d-cite> <d-cite key="72">[9]</d-cite> <d-cite key="89">[10]</d-cite> :<br/> object-centric scenes를 optimize하고, more expansive camera trajectories를 가진 larger env.로 scale하는 데 사용<br/> But,, <code class="language-plaintext highlighter-rouge">time-consuming</code>!</li> <li><code class="language-plaintext highlighter-rouge">single-step만 U-Net을 query</code> <d-cite key="difix">[11]</d-cite> <d-cite key="22">[12]</d-cite> <d-cite key="32">[13]</d-cite> <d-cite key="49">[14]</d-cite> <d-cite key="77">[15]</d-cite> <d-cite key="78">[16]</d-cite> :<br/> inference speed 빠르고,<br/> minimal fine-tuning만으로도 extreme novel-view에서도 NeRF/3DGS rendering의 artifacts를 “fix”할 수 있음!</li> </ul> </li> </ul> </li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li>3D recon. 개선 :<br/> imperfect noisy input data에 대응하기 위해 <ul> <li><code class="language-plaintext highlighter-rouge">optimize camera poses</code> <d-cite key="6">[17]</d-cite> <d-cite key="21">[18]</d-cite> <d-cite key="35">[19]</d-cite> <d-cite key="39">[20]</d-cite> <d-cite key="59">[21]</d-cite> <d-cite key="69">[22]</d-cite></li> <li><code class="language-plaintext highlighter-rouge">lighting variations</code> 고려 <d-cite key="34">[23]</d-cite> <d-cite key="60">[24]</d-cite> <d-cite key="73">[25]</d-cite></li> <li><code class="language-plaintext highlighter-rouge">transient occlusions</code> 완화 <d-cite key="48">[26]</d-cite></li> <li>위 방법들의 한계 :<br/> 완전히 artifacts를 해결하진 못함</li> </ul> </li> <li>Priors for NVS :<br/> under-observed (잘 보지 못한) 영역들을 잘 recon.하지 못하는 문제를 해결하기 위해 <ul> <li><code class="language-plaintext highlighter-rouge">Geometric priors</code> :<br/> noise에 민감하고, dense input일 때만 미미한 개선 <ul> <li>by <code class="language-plaintext highlighter-rouge">regularization term</code> <d-cite key="38">[27]</d-cite> <d-cite key="55">[28]</d-cite> <d-cite key="75">[29]</d-cite></li> <li>by <code class="language-plaintext highlighter-rouge">pre-trained models</code> which provide <code class="language-plaintext highlighter-rouge">depth GT</code> <d-cite key="7">[30]</d-cite> <d-cite key="45">[31]</d-cite> <d-cite key="63">[32]</d-cite> <d-cite key="90">[33]</d-cite> and <code class="language-plaintext highlighter-rouge">normal GT</code> <d-cite key="82">[34]</d-cite></li> </ul> </li> <li>여러 scenes’ data로 <code class="language-plaintext highlighter-rouge">feed-forward neural network</code> 훈련 :<br/> 이웃한 reference views의 정보를 aggregate하여 reference views 근처에서는 잘 수행하지만, <d-cite key="88">[35]</d-cite> <d-cite key="4">[36]</d-cite> <d-cite key="31">[37]</d-cite> <d-cite key="44">[38]</d-cite> <d-cite key="79">[39]</d-cite><br/> rendering 분포가 inherently multi-mode를 가지는 ambiguous regions에서는 잘 못 함</li> </ul> </li> <li>Generative Priors for NVS : <ul> <li>GAN :<br/> NeRF 개선하기 위해 per-scene GAN 훈련 <d-cite key="46">[40]</d-cite></li> <li>Diffusion : <ul> <li>diffusion model이 <code class="language-plaintext highlighter-rouge">직접 novel view를 generate</code> by minimal fine-tuning <d-cite key="8">[41]</d-cite> <d-cite key="13">[42]</d-cite> <d-cite key="81">[43]</d-cite> <d-cite key="83">[44]</d-cite> : <ul> <li>단점 : 3D model을 사용하지 않으므로 generative nature leads to <code class="language-plaintext highlighter-rouge">multi-view geometric inconsistency</code> across different frames/poses, especially in under-observed and noisy regions</li> </ul> </li> <li>diffusion model이 <code class="language-plaintext highlighter-rouge">3D model의 optimization을 guide</code> <d-cite key="12">[45]</d-cite> <d-cite key="25">[46]</d-cite> <d-cite key="70">[47]</d-cite> <d-cite key="72">[48]</d-cite> <d-cite key="89">[49]</d-cite> <ul> <li>단점 : <code class="language-plaintext highlighter-rouge">multi-step</code>으로 U-Net을 query (denoise)해야 해서 훈련 많이 <code class="language-plaintext highlighter-rouge">느림</code></li> </ul> </li> <li>diffusion model로 training image set을 augment하여 3D model을 fine-tuning <d-cite key="difix">[11]</d-cite> <d-cite key="27">[50]</d-cite> <d-cite key="28">[51]</d-cite> :<br/> (위의 두 방법을 합친 느낌?!) <ul> <li>본 논문 : 어설픈 3D model의 output에 대해 2D diffusion model (U-Net)을 먼저 fine-tuning한 뒤<br/> diffusion model로 training image set을 augment하여 diffusion prior를 3D model로 distill (fine-tuning)하거나<br/> diffusion model로 post-processing</li> </ul> </li> </ul> </li> </ul> </li> </ul> <h2 id="overall-pipeline">Overall Pipeline</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-15-Difix3D/2.PNG-480.webp 480w,/assets/img/2025-03-15-Difix3D/2.PNG-800.webp 800w,/assets/img/2025-03-15-Difix3D/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-15-Difix3D/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Architecture : <ul> <li>Stage 1)<br/> single-step 2D diffusion model (Difix)을 minimal fine-tuning</li> <li>Stage 2) <ul> <li>Step 1)<br/> Difix로 clean pseudo-training views (training set) 얻음 (augment!)<br/> 이 때, novel camera pose는 reference view부터 target view까지의 경로를 따라 pose interpolation으로 얻음</li> <li>Step 2)<br/> cleaned novel view를 이용하여 diffusion prior를 3D model로 distill <ul> <li>Step 1, 2를 반복하여 progressively update 3D representation<br/> (recon.하는 공간 크기를 키우고 diffusion model의 strong conditioning을 보장하기 위해)</li> </ul> </li> <li>Step 3)<br/> final updated 3D representation으로 rendering한 output을<br/> Difix로 real-time post-processing</li> </ul> </li> </ul> </li> </ul> <h2 id="difix---from-a-pretrained-diffusion-model-to-a-3d-artifact-fixer">Difix - from a Pretrained Diffusion Model to a 3D Artifact Fixer</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-15-Difix3D/3.PNG-480.webp 480w,/assets/img/2025-03-15-Difix3D/3.PNG-800.webp 800w,/assets/img/2025-03-15-Difix3D/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-15-Difix3D/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>based on <code class="language-plaintext highlighter-rouge">SD-Turbo</code> <d-cite key="49">[14]</d-cite> :<br/> SD-Turbo는 <code class="language-plaintext highlighter-rouge">single-step diffusion</code> model이라서 image-to-image translation task <d-cite key="40">[52]</d-cite> 를 effectively 수행 가능</li> </ul> <h3 id="fine-tuning">Fine Tuning</h3> <p>위의 Overall Pipeline에서 Stage 1)에 해당되는 내용!</p> <ul> <li>Fine Tuning :<br/> single-step 2D diffusion model인 SD-Turbo <d-cite key="49">[14]</d-cite> 가 3D Artifact Fixer 역할을 할 수 있도록<br/> Pix2pix-Turbo <d-cite key="40">[52]</d-cite> 와 유사한 방식으로 fine-tune <ul> <li>I/O : <ul> <li>input : clean reference view \(I_{ref}\) and degraded rendered target view \(\tilde I\)</li> <li>output : clean reference view \(I_{ref}\) and clean target view \(\hat I\)<br/> (training 및 inference에는 clean reference view \(I_{ref}\) 만 사용)</li> </ul> </li> <li>architecture :<br/> frozen VAE encoder - U-Net - reference mixing layer (self-attention layer) - LoRA fine-tuned decoder <ul> <li>frozen VAE encoder :<br/> reference view \(I_{ref}\) 와 degraded target novel view \(\tilde I\) 를 latent space로 encode한 뒤 concat<br/> as \(\epsilon (\tilde I, I_{ref}) = \boldsymbol z \in R^{(B \ast V) \times H \times W \times C}\)<br/> where \(V\) : the number of views (reference views and target views)<br/> where \(C\) : the number of latent channels</li> <li>U-Net</li> <li>reference-view conditioning by reference mixing layer (self-attention layer) :<br/> \(\boldsymbol z \in R^{B \times (V \ast H \ast W) \times C}\) 로 reshape한 뒤<br/> \(V \ast H \ast W\) dimension (dim=1)에 대해 <code class="language-plaintext highlighter-rouge">self-attention layer</code>를 적용하여<br/> <code class="language-plaintext highlighter-rouge">reference view 간의 cross-view consistency</code>를 포착<br/> (특히 rendered target novel view의 퀄리티가 좋지 않을 때<br/> <code class="language-plaintext highlighter-rouge">함께 input으로 넣어주는 clean reference view로부터</code> objects, color, texture 등 key info.를 잘 포착할 수 있음!)</li> <li>LoRA fine-tuned decoder</li> </ul> </li> <li>디테일 :<br/> lower noise level (e.g. \(\tau = 200\) instead of \(\tau = 1000\)) 부여<br/> s.t. diffusion model generates less-hallucinated results (덜 상상하며 생성)</li> <li>아이디어 :<br/> a specific noise level \(\tau\) 에서<br/> <code class="language-plaintext highlighter-rouge">3D model이 rendering한, artifacts를 가진 images의 분포</code>는<br/> <code class="language-plaintext highlighter-rouge">원래 diffusion model을 train하는 데 사용한, Gaussian noise를 가진 images의 분포</code>와 <code class="language-plaintext highlighter-rouge">유사할 것</code>이다!</li> <li>loss :<br/> \(L = L_{Recon} + L_{LPIPS} + 0.5 L_{Gram}\) <ul> <li>\(L_{Recon}\) : L2 loss</li> <li>\(L_{LPIPS}\) : perceptual loss (VGG-16 features끼리 비교)</li> <li>\(L_{Gram} = \frac{1}{L} \sum_{l=1}^{L} \beta_{l} \| G_{l}(\hat I) - G_{l}(I) \|_{2}\) :<br/> style loss for sharper detail (VGG-16 features의 Gram matrix끼리 비교)<br/> where \(G_{l}(I) = \phi_{l}(I)^{T} \phi_{l}(I)\)<br/> where \(\hat I\) (rendered noisy image) - \(I\) (GT clean image) pair는 아래에서 설명할 Data Curation 방법으로 생성</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-15-Difix3D/4.PNG-480.webp 480w,/assets/img/2025-03-15-Difix3D/4.PNG-800.webp 800w,/assets/img/2025-03-15-Difix3D/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-15-Difix3D/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> noise level이 높으면 model은 artifacts를 잘 제거하지만 image context도 함께 바꿈,, noise level이 낮으면 model은 image를 거의 안 건드림 </div> <h3 id="data-curation">Data Curation</h3> <ul> <li>Data Curation :<br/> single-step 2D diffusion model SD-Turbo 를 fine-tuning하기 위해<br/> loss \(L = L_{Recon} + L_{LPIPS} + 0.5 L_{Gram}\) 를 사용하려면<br/> novel-view synthesis artifacts를 가진 image \(\tilde I\) 와<br/> 이에 대응되는 깨끗해진 GT image \(I\) 를<br/> pair로 가진 large dataset를 구축해야 함 <ul> <li>방법 1) sparse reconstruction strategy :<br/> every \(n\)-th frame을 3D model training에 사용한 뒤<br/> 나머지 frames를 GT로 삼고, 해당 pose에 대해 novel-view-synthesis 수행하여 \(I\) - \(\tilde I\) pair 구축 <ul> <li>DL3DV dataset 처럼 camera trajectory가 있어서 novel views를 띄엄띄엄 sampling한 경우에는 잘 적용됨</li> <li>MipNeRF360 또는 LLFF dataset 처럼 training에 사용할 every \(n\)-th frame이 거의 같은 영역을 보고 있는 경우에는 최적의 방법이 아님<br/> \(\rightarrow\) 그럼 training sample 수를 최대한 늘리려면 어떻게 할까?<br/> \(\rightarrow\) 아래의 방법들 사용</li> </ul> </li> <li>방법 2) cycle reconstruction :<br/> Internal RDS (real driving scene) dataset 처럼 거의 linear trajectory인 경우<br/> original trajectory를 \(T_{o}\)라 하고, 이를 horizontally 1-6 m 옮긴 trajectory를 \(T_{s}\) 라 하면<br/> NeRF-1을 \(I\) on \(T_{o}\) 에 대해 학습한 뒤 \(T_{s}\) 에 대해 NeRF-1을 rendering한 뒤<br/> these rendered views on \(T_{s}\) 에 대해 NeRF-2를 학습한 뒤 \(T_{o}\) 에 대해 NeRF-2를 rendering한 걸 \(\tilde I\) 로 사용</li> <li>방법 3) model underfitting :<br/> artifacts 더 많은 \(\tilde I\) 를 generate하기 위해<br/> 기존 training schedule epoch 수의 25%-75% 정도로만 훈련시켜서<br/> render \(\tilde I\) with underfitted recon.</li> <li>방법 4) cross reference :<br/> multi-camera dataset의 경우에는<br/> 그 중 하나의 camera로만 3D model을 학습시킨 뒤<br/> 나머지 camera view \(I\) 에 해당되는 pose에 대해 render \(\tilde I\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-15-Difix3D/5.PNG-480.webp 480w,/assets/img/2025-03-15-Difix3D/5.PNG-800.webp 800w,/assets/img/2025-03-15-Difix3D/5.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-15-Difix3D/5.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="difix3d---nvs-with-diffusion-priors">Difix3D+ - NVS with Diffusion Priors</h2> <h3 id="difix3d---progressive-3d-update">Difix3D - Progressive 3D Update</h3> <p>위의 Overall Pipeline에서 Stage 2)의 Step 1), 2)에 해당되는 내용!</p> <ul> <li>Progressive 3D Update and Reference-View Conditioning : <ul> <li>desired novel-view trajectory가 input views로부터 너무 멀다면<br/> diffusion model의 reference-view conditioning signal이 약해서<br/> diffusion model은 더 상상하며(hallucinate) degraded rendered novel view를 깨끗하게 만듦</li> <li>Instruct-NeRF2NeRF와 비슷하게 iterative training scheme을 사용하여<br/> the set of 3D cues를 progressively 늘려가면서<br/> diffusion model의 reference-view conditioning 을 증가시키면<br/> diffusion model은 self-attention layer에서 clean reference view로부터 key info.를 많이 얻을 수 있음</li> </ul> </li> <li>Strategy : <ul> <li>처음에 sparse reference views만으로 optimize 3D model</li> <li>1.5k iter.마다 GT novel camera pose를 조금씩 perturb 하여<br/> (by reference view부터 target view까지의 경로를 따라 pose interpolation)<br/> 3D model로 novel view를 rendering한 뒤<br/> fine-tuned 2D diffusion model로 refine <ul> <li>the refined clean novel-view images를 다음 1.5k iter.에서 training set에 더함</li> <li>sparse reference views 뿐만 아니라 the refined clean novel views까지 training set으로 사용하므로 3D model의 consistency와 quality가 좋아짐!<br/> (3D model로 distill)</li> </ul> </li> <li>위의 과정을 반복하면서 reference views와 target views 사이의 3D cues’ overlap을 progressively 증가시켜서<br/> target view에서의 artifact-free rendering을 보장할 수 있게 됨!</li> </ul> </li> </ul> <h3 id="difix3d---real-time-post-render-processing">Difix3D+ - Real time Post Render Processing</h3> <p>위의 Overall Pipeline에서 Stage 2)의 Step 3)에 해당되는 내용!</p> <ul> <li>diffusion prior를 3D model로 distill하더라도<br/> 3D recon. model의 limited capacity로 인해<br/> under-observed regions에서는 sharp detail을 살리지 못함<br/> \(\rightarrow\)<br/> fine-tuned diffusion model (Difix)을 적용하여 final post-processing at render time 함으로써<br/> consistency를 유지하면서 novel-view를 enhance <ul> <li>fine-tuned diffusion model (Difix)는 single-step diffusion model이므로<br/> 이로 인한 부가적인 rendering time은 only 76 ms on NVIDIA A100 GPU</li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <h3 id="in-the-wild-artifact-removal">In-the-wild Artifact Removal</h3> <ul> <li>In-the-wild Artifact Removal :<br/> DL3DV dataset and Nerfbusters dataset 사용 <ul> <li>Difix Training : <ul> <li>DL3DV dataset의 경우 scenes의 80% (112 scenes)를 randomly select</li> <li>Data Curation Strategy로 80,000개의 noisy-clean image pair 만들어서 Difix 훈련</li> </ul> </li> <li>Evaluation : <ul> <li>DL3DV dataset의 경우 나머지 20% (28 scenes) 이용<br/> reference view와 target view 간에 상당한 차이가 있도록<br/> camera position에 따라 reference view와 target view를 분류</li> <li>Nerfbusters dataset의 경우 12 captures 이용<br/> Nerfbusters <d-cite key="70">[47]</d-cite> 의 recommended protocol을 따라<br/> reference view와 target view를 분류</li> </ul> </li> <li>Baseline : <ul> <li>Nerfbusters <d-cite key="70">[47]</d-cite> :<br/> NeRF의 artifacts를 제거하기 위해 3D diffusion model 사용</li> <li>GANeRF <d-cite key="46">[40]</d-cite> :<br/> NeRF의 artifacts 제거하기 위해 per-scene GAN 훈련</li> <li>NeRFLiX <d-cite key="88">[35]</d-cite> :<br/> feed-forward network를 사용하여 근처 reference views의 정보를 aggregate하여 퀄리티 향상</li> <li>3DGS-based methods :<br/> gsplat library <a href="https://github.com/nerfstudio-project/gsplat">Link</a> 사용</li> </ul> </li> <li>Metric : PSNR, SSIM, LPIPS, FID</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-15-Difix3D/6.PNG-480.webp 480w,/assets/img/2025-03-15-Difix3D/6.PNG-800.webp 800w,/assets/img/2025-03-15-Difix3D/6.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-15-Difix3D/6.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-15-Difix3D/7.PNG-480.webp 480w,/assets/img/2025-03-15-Difix3D/7.PNG-800.webp 800w,/assets/img/2025-03-15-Difix3D/7.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-15-Difix3D/7.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-15-Difix3D/8.PNG-480.webp 480w,/assets/img/2025-03-15-Difix3D/8.PNG-800.webp 800w,/assets/img/2025-03-15-Difix3D/8.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-15-Difix3D/8.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="automotive-scene-enhancement">Automotive Scene Enhancement</h3> <ul> <li>Automotive Scene Enhancement :<br/> RDS (real driving scene) dataset 사용<br/> (multi-view dataset이라서 서로 40도의 overlaps를 가지고 있는 세 개의 cameras 있음) <ul> <li>Difix Training : <ul> <li>세 개의 cameras 중 center camera를 reference 및 target view로 사용하고,<br/> 40 scenes 사용하여 훈련</li> <li>Data Curation Strategy로 100,000개의 noisy-clean image pair 만들어서 Difix 훈련</li> </ul> </li> <li>Evaluation : <ul> <li>나머지 두 개의 cameras를 novel view로 사용하고,<br/> 20 scenes 사용하여 평가</li> </ul> </li> </ul> </li> </ul> <h3 id="ablation-study">Ablation Study</h3> <ul> <li>Ablation Study on Pipeline :<br/> 하나씩 요소를 추가해가며 진행! <ul> <li>(a) <code class="language-plaintext highlighter-rouge">3D model update 없이</code><br/> rendered views에 그냥 직접 Difix 적용 <ul> <li>reference views에서 먼 less observed regions에서는 별로고, consistency 유지하지 못해서 flickering 발생</li> </ul> </li> <li>(b) <code class="language-plaintext highlighter-rouge">non-incremental (not progressive) 3D model update</code><br/> (pseudo-views를 한 번에 모두 training set에 추가하여 3D model update(distill))</li> <li>(c) <code class="language-plaintext highlighter-rouge">progressive 3D model update</code></li> <li>(d) <code class="language-plaintext highlighter-rouge">post-rendering</code>으로도 Difix 적용</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-15-Difix3D/9.PNG-480.webp 480w,/assets/img/2025-03-15-Difix3D/9.PNG-800.webp 800w,/assets/img/2025-03-15-Difix3D/9.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-15-Difix3D/9.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-15-Difix3D/10.PNG-480.webp 480w,/assets/img/2025-03-15-Difix3D/10.PNG-800.webp 800w,/assets/img/2025-03-15-Difix3D/10.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-15-Difix3D/10.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-15-Difix3D/11.PNG-480.webp 480w,/assets/img/2025-03-15-Difix3D/11.PNG-800.webp 800w,/assets/img/2025-03-15-Difix3D/11.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-15-Difix3D/11.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Ablation Study on Difix Training : <ul> <li><code class="language-plaintext highlighter-rouge">low noise value</code> \(\tau = 200\) 을 사용한 Difix3D+<br/> versus high noise value \(\tau = 1000\) 을 사용한 Pix2pix-Turbo <d-cite key="40">[52]</d-cite> <ul> <li>high noise value 를 사용할 경우<br/> diffusion model이 more hallucinated (더 상상하며) GT와 다른 결과를 generate하기 때문에<br/> poorer generalization on the test dataset</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">reference view conditioning (self-attention layer)</code>의 유무</li> <li><code class="language-plaintext highlighter-rouge">Gram style loss</code>의 유무</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-15-Difix3D/12.PNG-480.webp 480w,/assets/img/2025-03-15-Difix3D/12.PNG-800.webp 800w,/assets/img/2025-03-15-Difix3D/12.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-15-Difix3D/12.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="conclusion">Conclusion</h2> <ul> <li>Limitation : <ul> <li>inherently limited by initial 3D model의 성능</li> </ul> </li> <li>Future Work : <ul> <li>initial 3D model의 성능에 의존하는 문제를<br/> modern diffusion priors 사용하여 극복</li> <li>Difix는 single-step 2D diffusion model을 fine-tuning한 model인데,<br/> single-step video diffusion model로 확장하여<br/> long-context 3D consistency까지도 향상</li> </ul> </li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br/> single-step 2D diffusion model인 Difix는<br/> novel-view rendering이나 camera pose selection에 관여하지 않고<br/> 그냥 더러운 image를 깨끗하게 만드는 역할일 뿐인건가요?</p> </li> <li>A1 :<br/> 맞는 말씀인데, 깨끗하게 만듦으로써 distillation으로 3D model param.를 업데이트하는 역할도 있습니다.<br/> 다시 한 번 설명해보도록 하겠습니다.<br/> Difix는 더러운 image를 깨끗하게 만들 수 있도록 fine-tuning 되었습니다.<br/> 근데, <code class="language-plaintext highlighter-rouge">더러운 image를 깨끗하게 만듦으로써 두 가지 역할</code>을 수행합니다. <ul> <li>첫 번째는 3D model이 render한 더러운 image를 깨끗하게 만들어서 이를 다시 training image로 사용하는 과정에서 diffusion prior를 3D model로 <code class="language-plaintext highlighter-rouge">distill하여 3D model을 update</code>하는 데 사용할 수 있습니다.</li> <li>두 번째는 final updated 3D model이 render한 image를 마지막으로 좀 더 깨끗하게 만드는 <code class="language-plaintext highlighter-rouge">post-processing</code>에 사용합니다.</li> </ul> </li> <li> <p>Q2 :<br/> 한 번에 3D model을 update하는 non-incremental 방법에 비해<br/> progressively 3D model을 update하는 방법의 성능이 더 좋은 이유는 무엇이라고 생각하시나요?</p> </li> <li>A2 :<br/> 핵심은<br/> reference view부터 target view까지의 경로를 따라 <code class="language-plaintext highlighter-rouge">novel camera pose를 조금씩 perturb</code>한다는 것과,<br/> Difix의 <code class="language-plaintext highlighter-rouge">self-attention layer</code>에 있다고 생각합니다. <ul> <li>progressive update의 경우에는<br/> 먼저 reference view와 가까이 있는 novel view에서 더러운 image를 render한 뒤 이를 Difix에 넣어서<br/> self-attention layer에 의해 가까운 clean reference view의 도움으로 더러운 novel view를 쉽게 깨끗하게 만들 수 있습니다.<br/> 그리고 깨끗해진 novel view를 다시 training set (reference view)에 넣고, novel view를 reference view부터 target view까지 조금씩만 이동시키므로<br/> 마찬가지로 그 다음 novel view에 대해서도 nearby clean reference view의 도움으로 계속해서 novel view를 쉽게 깨끗하게 만들 수 있습니다.</li> <li>non-incremental update의 경우에는<br/> reference view부터 target view까지의 경로에 있는 임의의 모든 novel views에 대해 더러운 image를 한꺼번에 render한 뒤 이를 한꺼번에 Difix에 넣어서 한꺼번에 깨끗하게 만들어야 하는데,<br/> 만약 <code class="language-plaintext highlighter-rouge">reference view로부터 멀리 있는 novel view의 경우</code><br/> 둘의 3D content 차이가 커서 self-attention layer가 둘의 관계를 잘 포착할 수 없고 clean reference view의 도움으로 novel view를 깨끗하게 만들기가 쉽지 않습니다.</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="postprocessing"/><category term="single"/><category term="step"/><category term="diffusion"/><summary type="html"><![CDATA[Improving 3D Reconstructions with Single-Step Diffusion Models (CVPR 2025)]]></summary></entry><entry><title type="html">Radiant Foam</title><link href="https://semyeong-yu.github.io/blog/2025/radfoam/" rel="alternate" type="text/html" title="Radiant Foam"/><published>2025-02-26T12:00:00+00:00</published><updated>2025-02-26T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/radfoam</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/radfoam/"><![CDATA[<h2 id="radiant-foam---real-time-differentiable-ray-tracing">Radiant Foam - Real-Time Differentiable Ray Tracing</h2> <h4 id="shrisudhan-govindarajan-daniel-rebain-kwang-moo-yi-andrea-tagliasacchi">Shrisudhan Govindarajan, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2502.01157">https://arxiv.org/abs/2502.01157</a><br/> project website :<br/> <a href="https://radfoam.github.io/">https://radfoam.github.io/</a><br/> code :<br/> <a href="https://github.com/theialab/radfoam">https://github.com/theialab/radfoam</a><br/> reference :<br/> Presentation of https://charlieppark.kr from 3D-Nerd Community</p> </blockquote> <h2 id="contribution">Contribution</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-26-radfoam/2.PNG-480.webp 480w,/assets/img/2025-02-26-radfoam/2.PNG-800.webp 800w,/assets/img/2025-02-26-radfoam/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-26-radfoam/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Ray Tracing</code> with <code class="language-plaintext highlighter-rouge">Voronoi Diagram</code> : <ul> <li>Voronoi Diagram : <ul> <li><code class="language-plaintext highlighter-rouge">foam model</code> 중 하나</li> <li>3D <code class="language-plaintext highlighter-rouge">공간 자체를 partitioning</code>하는 3D scene representation</li> <li>edge-flip 하는 순간에 두 외접원이 겹쳤다가 continuous하게 변하므로 <code class="language-plaintext highlighter-rouge">gradient-based optimization</code> 적용 가능!</li> </ul> </li> <li>Ray Tracing : <ul> <li>장점 : <ul> <li>ray 단위로 처리하므로 굴절, 반사 등 <code class="language-plaintext highlighter-rouge">빛 효과 반영</code> 가능</li> <li><code class="language-plaintext highlighter-rouge">non-pinhole</code> 실제 camera 지원 가능<br/> (image-centric rasterization 기법과 달리 object-centric ray-tracing 기법이라서)</li> </ul> </li> <li>단점 및 극복? : <ul> <li>rasterization에 비해서는 <code class="language-plaintext highlighter-rouge">속도가 느림</code><br/> 현재 많은 3D 논문들은 volume rendering을 기반으로 하여, 특히 3DGS tile-based rasterization은 parallelism 덕분에 빠른 recon. 가능<br/> 하지만 ray-tracing의 경우 mesh-based representation이 regular하지 않아서 recon.에 불리했고, rasterization에 비해 느렸음.<br/> 하지만 본 논문에서는 voronoi representation에 많은 정보를 저장함으로써 FPS를 높였는데, 이를 계기로 앞으로 더 발전할지도?</li> </ul> </li> </ul> </li> <li>Question <a href="https://semyeong-yu.github.io/blog/2025/radfoam/#question">Link</a> 참고!!</li> </ul> </li> </ul> <h2 id="background">Background</h2> <ul> <li>Ray Tracing : <ul> <li>How : <ul> <li>Step 1)<br/> camera로부터 ray extend</li> <li>Step 2)<br/> ray가 scene을 가로지르며 transmit, reflect, sub-surface scattered, etc.</li> <li>Step 3)<br/> ray가 light source에 도달하면 illumination equation으로부터 pixel 값 얻음</li> </ul> </li> <li>장점 : <ul> <li>rasterizer : primitive 단위로 처리하기 때문에 그림자 같은 빛 효과 반영 어렵</li> <li>ray tracer : ray 단위로 처리하는데, ray는 intersected info.를 모두 가지고 있으므로 그림자나 global illumination effects 반영 쉬움</li> </ul> </li> </ul> </li> <li>Ray Tracing with Bounding Volume Hierarchy (BVH) : <ul> <li>단점 : overlapping Gaussian 때문에 속도 느림 (비효율)</li> </ul> </li> <li>3D Representation : <ul> <li>3DGS는 rasterization을 위해, 2D projection이 가능하도록 3D covariance를 가진 learnable 3D pcd이다</li> <li>3DGS에 ray-tracing을 적용하기 위해, learnable 3D pcd를 이용한 또 다른 3D scene representation은 없을까?<br/> \(\rightarrow\)<br/> ray-tracing을 적용하기 위해, 공간 자체를 partitioning하는 voronoi diagram을 사용하자!!</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-26-radfoam/1.PNG-480.webp 480w,/assets/img/2025-02-26-radfoam/1.PNG-800.webp 800w,/assets/img/2025-02-26-radfoam/1.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-26-radfoam/1.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="method">Method</h2> <h3 id="delaunay-triangulation">Delaunay Triangulation</h3> <p>triangle mesh를 표현하는 데 있어서<br/> point D가 triangle ABC의 외접원의 내부에 있으면 implausible<br/> \(\rightarrow\)<br/> edge-flip으로 해결!</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-26-radfoam/3.PNG-480.webp 480w,/assets/img/2025-02-26-radfoam/3.PNG-800.webp 800w,/assets/img/2025-02-26-radfoam/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-26-radfoam/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> reference: https://charlieppark.kr </div> <ul> <li>Delaunay Triangulation : <ul> <li>문제 :<br/> triangle mesh의 vertex location을 continuously 바꿔도<br/> edge-flip 일어나면 triangle mesh의 connectivity (edge)는 <code class="language-plaintext highlighter-rouge">discrete</code>하게 변해서 gradient-based optimization 적용 불가능</li> </ul> </li> </ul> <h3 id="voronoi-diagram">Voronoi Diagram</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-26-radfoam/4.PNG-480.webp 480w,/assets/img/2025-02-26-radfoam/4.PNG-800.webp 800w,/assets/img/2025-02-26-radfoam/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-26-radfoam/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Voronoi Diagram : <ul> <li>Delaunay Triangulation의 dual graph</li> <li>How : <ul> <li>Step 1)<br/> Delaunay Triangulation의 each triangle mesh의 외접원을 그림</li> <li>Step 2)<br/> triangle mesh의 외접원의 중심들을 잇기</li> </ul> </li> <li>장점 : <ul> <li>triangle mesh의 learanble vertex location (아래 그림의 빨간 점)을 continuously 바꾸다가<br/> 두 외접원이 만나서 edge-flip 일어나면<br/> triangle mesh의 edge (아래 그림의 초록색 삼각형)는 discrete하게 변하지만,<br/> 두 외접원 (아래 그림의 파란 원)은 겹쳤다가 continuously 변하므로 voronoi diagram (아래 그림의 검은색 벌집모양)은 <code class="language-plaintext highlighter-rouge">continuously</code> 변함<br/> 그래서 <code class="language-plaintext highlighter-rouge">gradient-based optimization</code> 적용 가능!</li> <li>외접원을 3DGS처럼 간주하여 <code class="language-plaintext highlighter-rouge">3D 공간을 partitioning</code></li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-26-radfoam/5.PNG-480.webp 480w,/assets/img/2025-02-26-radfoam/5.PNG-800.webp 800w,/assets/img/2025-02-26-radfoam/5.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-26-radfoam/5.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="ray-tracing-algorithm">Ray Tracing Algorithm</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-26-radfoam/6.PNG-480.webp 480w,/assets/img/2025-02-26-radfoam/6.PNG-800.webp 800w,/assets/img/2025-02-26-radfoam/6.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-26-radfoam/6.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-26-radfoam/7.PNG-480.webp 480w,/assets/img/2025-02-26-radfoam/7.PNG-800.webp 800w,/assets/img/2025-02-26-radfoam/7.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-26-radfoam/7.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Ray Tracing Foams : <ul> <li>How : <ul> <li>a ray와 Voronoi Diagram’s cell을 이루는 면의 교점을 sampling point로 사용</li> <li>a ray가 Voronoi Diagram의 a cell에 진입했을 때<br/> ray direction과 normal vector가 90도 미만의 각도를 가지는 면 (위 그림에서 green) 중 가장 먼저 만나는 면에 exit intersection이 존재하고<br/> ray direction과 normal vector가 90도 이상의 각도를 가지는 back-facing 면 (위 그림에서 blue)는 무시!<br/> exit intersection이 존재하는 면에 인접한 cell에 a ray가 진입하여 위의 과정을 똑같이 반복</li> </ul> </li> <li>Ray Tracing은 reflection 등 빛 효과인데<br/> 위의 알고리즘에는 surface reflection에 대한 설명이 없어서<br/> surface reflection과 volume rendering을 어떻게 섞어서 구현했는지는 코드로 확인해야 할 듯!<br/> TBD <code class="language-plaintext highlighter-rouge">???</code></li> </ul> </li> </ul> <h3 id="loss">Loss</h3> <ul> <li>Loss :<br/> \(L = L_{rgb} + \lambda L_{quantile}\) <ul> <li>L2 photometric loss</li> <li>quatile loss<br/> \(L_{quantile} = E_{t_{1}, t_{2} \sim U[0, 1]} | W^{-1}(t_{1}) - W^{-1}(t_{2}) |\)<br/> where \(W^{-1}(\cdot)\) : quantile function (inverse CDF) of the volume rendering weight distribution along the ray <ul> <li>distortion loss of MipNeRF360 <a href="https://semyeong-yu.github.io/blog/2024/MipNeRF360/#regularization-for-interval-based-models">Blog</a> 와 비슷한데,<br/> expensive quadratic nested sum 항을 제거</li> <li>실제로 object가 있는 곳에 Gaussian’s weight on a ray 가 높도록 하여 floater artifacts 제거하는 regularization</li> </ul> </li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-26-radfoam/8.PNG-480.webp 480w,/assets/img/2025-02-26-radfoam/8.PNG-800.webp 800w,/assets/img/2025-02-26-radfoam/8.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-26-radfoam/8.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-26-radfoam/9.PNG-480.webp 480w,/assets/img/2025-02-26-radfoam/9.PNG-800.webp 800w,/assets/img/2025-02-26-radfoam/9.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-26-radfoam/9.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-26-radfoam/10.PNG-480.webp 480w,/assets/img/2025-02-26-radfoam/10.PNG-800.webp 800w,/assets/img/2025-02-26-radfoam/10.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-26-radfoam/10.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="conclusion">Conclusion</h2> <ul> <li>Voronoi-based 3D representation : <ul> <li>Limitation : <ul> <li><code class="language-plaintext highlighter-rouge">high memory consumption</code> : <ul> <li>Ours는 이웃한 점들 사이의 cell boundaries가 equidistant해야 한다는 가정 필요</li> <li>그래서 surface를 정의하기 위해서는 많고 작은 empty cells (high VRAM) 필요</li> <li>심지어 Voronoi param. size에 비해 foam model의 3D 공간이 너무 넓다</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">need SfM</code> : <ul> <li>need SfM to initialize voronoi diagram</li> </ul> </li> </ul> </li> <li>Future Work : <ul> <li>Voronoi diagram을 넘어 representation을 generalize함으로써 위의 가정 완화</li> <li>여러 foam models를 compose together efficiently</li> <li>illumination이 계속 변하는 경우에 대응</li> <li>static scenes 말고 dynamic content에 대응</li> <li>scene editing via Voronoi representation</li> <li>generative model을 결합하여 unseen scene에 대응</li> <li>without SfM</li> <li>현재 real-time ray tracing은 대부분 triangle mesh로 수행되어 왔는데<br/> 위의 Future Work를 통해 foam model-based ray tracing도 발전 가능!</li> </ul> </li> </ul> </li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br/> voronoi diagram은 여러 3D representation 중 하나인 거고, 사실 꼭 voronoi diagram일 필요는 없는 거잖아요? <code class="language-plaintext highlighter-rouge">triangle mesh 대신 voronoi diagram</code>을 썼을 때의 장단점이 있을까요?</p> </li> <li>A1 : <ul> <li>장점 : <ul> <li>공간을 삼각형 대신 다각형(cell)로 분할하므로 <code class="language-plaintext highlighter-rouge">더 복잡한 기하구조</code>(움푹 패인 부분, 부드러운 곡면 등)를 표현할 수 있습니다.</li> <li>기존 triangle mesh보다 저장해야 하는 정보(VRAM)가 많지만, 실시간 ray intersection 연산이 더 적기 때문에 <code class="language-plaintext highlighter-rouge">더 빠르게 rendering</code>할 수 있습니다. (trade-off b.w. VRAM and FPS)</li> <li>각 cell로부터 SDF(수학적인 함수)를 정의하면 vertex 및 edge로 표현되는 explicit mesh 대신 <code class="language-plaintext highlighter-rouge">수학 식(SDF)으로 표현되는 implicit surface</code>를 만들 수 있고 implicit surface ray marching 방식으로 rendering하여 부드러운 곡면을 표현할 수 있습니다.</li> </ul> </li> <li>단점 :<br/> 기존 ray-tracing rendering pipeline은 triangle mesh 기반으로 짜여져 있어서 새로운 acceleration 및 intersection algorithm 필요</li> </ul> </li> <li> <p>Q2 :<br/> ray-tracing model 중에서 FPS가 제일 빠른 건 사실 <code class="language-plaintext highlighter-rouge">memory에 voronoi foam representation 방식으로 3D scene info.를 미리 잘 사전 저장</code>해놓았기 때문에 실시간으로 ray-tracing했을 때 <code class="language-plaintext highlighter-rouge">실시간 ray intersection 연산이 더 적을 수 있어서 빠르게 rendering</code>할 수 있는 거라고 생각합니다. 그래서 <code class="language-plaintext highlighter-rouge">VRAM memory와 FPS의 trade-off</code>가 있는 거 같은데 이 점에 대해 어떻게 생각하시나요?</p> </li> <li>A2 : <ul> <li>rasterization 대신 ray-tracing 쓰려는 이유 :<br/> <code class="language-plaintext highlighter-rouge">memory</code>와 <code class="language-plaintext highlighter-rouge">FPS</code>를 포기하고 <code class="language-plaintext highlighter-rouge">illumination 효과</code> 반영 가능</li> <li>ray-tracing에서 implicit MLP 대신 explicit voronoi diagram 쓰려는 이유 :<br/> <code class="language-plaintext highlighter-rouge">memory</code>를 포기하고 <code class="language-plaintext highlighter-rouge">FPS</code> 높임 <ul> <li>memory 줄이는 방법? (by ChatGPT) : <ul> <li>hierarchical LOD-based voronoi compression <code class="language-plaintext highlighter-rouge">???</code></li> <li>필요한 부분만 GPU에 load하여 update하는 on-demand streaming 방식 <code class="language-plaintext highlighter-rouge">???</code></li> </ul> </li> </ul> </li> <li>ray-tracing에서 triangle mesh 대신 voronoid diagram 쓰려는 이유 :<br/> for continuous change of planes <code class="language-plaintext highlighter-rouge">???</code><br/> (But, triangle-mesh-based ray-tracing rendering pipeline is already well-established..)</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="3DGS"/><category term="ray"/><category term="tracing"/><category term="voronoi"/><category term="diagram"/><summary type="html"><![CDATA[Real-Time Differentiable Ray Tracing]]></summary></entry><entry><title type="html">Spacetime Gaussian</title><link href="https://semyeong-yu.github.io/blog/2025/STGS/" rel="alternate" type="text/html" title="Spacetime Gaussian"/><published>2025-02-08T10:00:00+00:00</published><updated>2025-02-08T10:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/STGS</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/STGS/"><![CDATA[<h2 id="spacetime-gaussian-feature-splatting-for-real-time-dynamic-view-synthesis-cvpr-2024">Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis (CVPR 2024)</h2> <h4 id="zhan-li-zhang-chen-zhong-li-yi-xu">Zhan Li, Zhang Chen, Zhong Li, Yi Xu</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2312.16812">https://arxiv.org/abs/2312.16812</a><br/> project website :<br/> <a href="https://oppo-us-research.github.io/SpacetimeGaussians-website/">https://oppo-us-research.github.io/SpacetimeGaussians-website/</a><br/> code :<br/> <a href="https://github.com/oppo-us-research/SpacetimeGaussians">https://github.com/oppo-us-research/SpacetimeGaussians</a><br/> blog reference :<br/> <a href="https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/spacetime-gaussian/">https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/spacetime-gaussian/</a></p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-08-STGS/1.PNG-480.webp 480w,/assets/img/2025-02-08-STGS/1.PNG-800.webp 800w,/assets/img/2025-02-08-STGS/1.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-08-STGS/1.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="contribution">Contribution</h2> <ul> <li>Spacetime Gaussian (STG) :<br/> 3DGS를 dynamic 4D scene으로 확장하기 위해<br/> <code class="language-plaintext highlighter-rouge">time-dependent opacity, motion trajectory(mean), rotation</code> 사용 <ul> <li>사실 time-dependent opacity, mean, rotation를 polynomial 등 특정 func.에 regression하는 task!</li> </ul> </li> <li> <p>Splatted Feature Rendering :<br/> spherical harmonics (<code class="language-plaintext highlighter-rouge">SH</code>) coeff. <code class="language-plaintext highlighter-rouge">대신</code><br/> base color, view direction info., time info.를 encoding하는 <code class="language-plaintext highlighter-rouge">feature</code> \(f_{i}(t) \in R^{9}\) 사용</p> </li> <li>Guided Sampling of Gaussians :<br/> initialization 할 때 Gaussian이 희박한 먼 영역은 흐릿해지는 경향이 있는데,<br/> 이를 해결하기 위해 <code class="language-plaintext highlighter-rouge">학습 오차와 coarse depth를 guidance</code>로 삼아<br/> 4D scene에서 <code class="language-plaintext highlighter-rouge">새로운 Gaussian을 sampling</code></li> </ul> <h2 id="method">Method</h2> <h3 id="spacetime-gaussians">Spacetime Gaussians</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-08-STGS/2.PNG-480.webp 480w,/assets/img/2025-02-08-STGS/2.PNG-800.webp 800w,/assets/img/2025-02-08-STGS/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-08-STGS/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p><code class="language-plaintext highlighter-rouge">temporal-and-spatial opacity</code> :<br/> \(\alpha_{i}(\boldsymbol x, t) = \sigma_{i}(t) \text{exp}(-\frac{1}{2} (\boldsymbol x - \mu_{i}(t))^{T} \Sigma_{i}(t)^{-1} (\boldsymbol x - \mu_{i}(t)))\)<br/> (temporal opacity \(\sigma_{i}(t)\) 가 위치 \(\boldsymbol x\) 에 따라 (spatial) 희석됨)<br/> where \(\sigma_{i}(t)\) : temporal opacity<br/> where \(\mu_{i}(t), \Sigma_{i}(t)\) : time-dependent mean, covariance</p> </li> <li><code class="language-plaintext highlighter-rouge">Temporal Radial Basis Function</code> (<code class="language-plaintext highlighter-rouge">temporal opacity</code>) :<br/> \(\sigma_{i}(t) = \sigma_{i}^{s} \text{exp}(-s_{i}^{\tau} | t - \mu_{i}^{\tau} |^{2})\)<br/> where <code class="language-plaintext highlighter-rouge">temporal center</code> \(\mu_{i}^{\tau}\) : \(i\)-th STG \(G_{i}\) 가 가장 잘 보이는 timestamp<br/> where <code class="language-plaintext highlighter-rouge">temporal scaling factor</code> \(s_{i}^{\tau}\) : valid 지속 기간 결정<br/> where temporal-independent <code class="language-plaintext highlighter-rouge">spatial opacity</code> \(\sigma_{i}^{s}\) <ul> <li>1D Gaussian으로 모델링!<br/> 즉, timestamp \(t\) 가 temporal center \(\mu_{i}^{\tau}\) 에서 멀어질수록 opacity \(\sigma_{i}^{s}\) 가 옅어짐!</li> <li>시간에 따라 변하는 <code class="language-plaintext highlighter-rouge">opacity</code> \(\sigma_{i}(t)\) 을 통해<br/> <code class="language-plaintext highlighter-rouge">새로 나타나거나 사라지는</code> object를 효과적으로 모델링할 수 있음!</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Polynomial Motion Trajectory</code> :<br/> \(\mu_{i}(t) = \sum_{k=0}^{n_{p}} b_{i,k}(t - \mu_{i}^{\tau})^{k}\) <ul> <li>polynomial로 모델링!</li> <li>polynomical coeff. \(b_{i,k} \in R\) 은 learnable param.</li> <li>시간에 따라 변하는 <code class="language-plaintext highlighter-rouge">mean</code> \(\mu_{i}(t)\) 을 통해<br/> object <code class="language-plaintext highlighter-rouge">motion</code>을 모델링할 수 있음!</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Polynomial Rotation</code> :<br/> \(q_{i}(t) = \sum_{k=0}^{n_{q}} c_{i,k}(t - \mu_{i}^{\tau})^{k}\) <ul> <li>polynomial로 모델링!</li> <li>polynomical coeff. \(c_{i,k} \in R\) 은 learnable param.</li> <li>시간에 따라 변하는 <code class="language-plaintext highlighter-rouge">quaternion</code> \(q_{i}(t)\) 을 통해<br/> object <code class="language-plaintext highlighter-rouge">변형</code>을 모델링할 수 있음!</li> </ul> </li> <li>time-independent Scale : <ul> <li>Scaling matrix \(S_{i}\) 는 시간에 독립적</li> </ul> </li> </ul> <h3 id="splatted-feature-rendering">Splatted Feature Rendering</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-08-STGS/3.PNG-480.webp 480w,/assets/img/2025-02-08-STGS/3.PNG-800.webp 800w,/assets/img/2025-02-08-STGS/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-08-STGS/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Feature Splatting</code> : <ul> <li><code class="language-plaintext highlighter-rouge">color</code>의 경우 view direction 뿐만 아니라 시간에 따라 변하므로<br/> <code class="language-plaintext highlighter-rouge">spherical harmonics (SH) coeff.</code> 대신<br/> <code class="language-plaintext highlighter-rouge">feature</code> \(f_{i}(t) = [f_{i}^{base}, f_{i}^{dir}, (t - \mu_{i}^{\tau}) f_{i}^{time}]^{T} \in R^{9}\) 로 대체! <ul> <li>RGB base color : \(f_{i}^{base} \in R^{3}\)</li> <li>view direction : \(f_{i}^{dir} \in R^{3}\)</li> <li>time : \(f_{i}^{time} \in R^{3}\)</li> </ul> </li> <li>RGB color (SH coeff.) 대신 feature \(f_{i}(t)\) 를 image-space로 splatting한 뒤<br/> 2-layer MLP \(\Phi\) 를 거쳐 최종 RGB color를 얻음<br/> \(I = F^{base} + \Phi(F^{dir}, F^{time}, \boldsymbol r)\) <ul> <li>feature \(f_{i}(t)\) 를 image-space로 splatting한 feature를 \(F^{base}, F^{dir}, F^{time}\) 으로 분할</li> <li>target view direction : \(\boldsymbol r\)</li> </ul> </li> <li>장점 : <ul> <li>less param. than SH coeff. encoding</li> <li>still fast rendering using shallow MLP \(\Phi\)</li> </ul> </li> <li>light 버전 :<br/> rendering speed를 최대화하기 위해 선택적으로 MLP \(\Phi\) 를 삭제하고 \(F^{base}\) 만 유지</li> </ul> </li> </ul> <h3 id="loss">Loss</h3> <ul> <li>learnable param. : <ul> <li>temporal <code class="language-plaintext highlighter-rouge">opacity</code> : <ul> <li>time-independent spatial opacity \(\sigma_{i}^{s}\)</li> <li>temporal scaling factor \(s_{i}^{\tau}\)</li> <li>temporal center \(\mu_{i}^{\tau}\)</li> </ul> </li> <li>time-dependent <code class="language-plaintext highlighter-rouge">motion</code> (trajectory) : <ul> <li>polynomial coeff. \(\{ b_{i,k} \}_{k=0}^{n_{p}}\)</li> </ul> </li> <li>time-dependent <code class="language-plaintext highlighter-rouge">rotation</code> (quaternion) : <ul> <li>polynomial coeff. \(\{ c_{i,k} \}_{k=0}^{n_{q}}\)</li> </ul> </li> <li>time-dependent <code class="language-plaintext highlighter-rouge">color</code> : <ul> <li>feature \(f_{i}^{base}, f_{i}^{dir}, f_{i}^{time}\)</li> </ul> </li> </ul> </li> <li>loss :<br/> photometric loss (L1, D-SSIM)</li> </ul> <h3 id="guided-sampling-of-gaussians">Guided Sampling of Gaussians</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-08-STGS/4.PNG-480.webp 480w,/assets/img/2025-02-08-STGS/4.PNG-800.webp 800w,/assets/img/2025-02-08-STGS/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-08-STGS/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>issue :<br/> initialization 할 때 <code class="language-plaintext highlighter-rouge">Gaussian이 희박한 먼 영역은 흐릿</code>해지는 경향이 있음</p> </li> <li> <p>solution :</p> <ul> <li>이를 해결하기 위해 <code class="language-plaintext highlighter-rouge">학습 오차</code>와 <code class="language-plaintext highlighter-rouge">coarse depth</code>를 guidance로 삼아<br/> 4D scene에서 새로운 Gaussian을 sampling</li> <li>sampling 효율성을 보장하기 위해<br/> <code class="language-plaintext highlighter-rouge">loss가 안정된 후</code>에 sampling 진행</li> <li>Procedure : <ul> <li>Step 1)<br/> 학습 오차에 noise가 있을 수 있으므로<br/> patch-wise로 학습 오차를 계산하여 상당한 오차가 있는 patch 찾기</li> <li>Step 2)<br/> 학습 오차가 큰 patch의 중앙 pixel의 ray를 따라 Gaussian들을 sampling<br/> (coarse depth map을 이용해 Gaussian들이 희박한 깊이 범위를 찾은 뒤 해당 범위에서 uniform sampling)<br/> (3회 이하로 수행) <ul> <li>feature sampling 중에 생성되는 coarse depth map을 이용하므로<br/> additional overhead 거의 없음</li> <li>새로 sampling된 Gaussian들의 mean에 작은 noise를 추가<br/> (불필요한 Gaussian들은 학습 중에 자연스레 opacity가 낮아져 remove됨)</li> </ul> </li> </ul> </li> <li>의의 :<br/> <code class="language-plaintext highlighter-rouge">3DGS density control을 보완</code><br/> (3DGS density control은 기존 Gaussian들 근처에서 점진적으로 Gaussian들을 증가시키는데,<br/> 본 논문의 Guided Sampling은 Gaussian들이 희박한 새로운 영역에서 Gaussians들을 sampling)</li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <ul> <li>Implementation :<br/> \(n_{p}=3, n_{q}=1\)<br/> Adam optimizer<br/> initialize Spacetime Gaussians using SfM pcd of all timestamps<br/> density control의 pruning을 3DGS보다 더 공격적으로 수행하여 Gaussian 수를 줄이고 모델 크기 작게 유지<br/> 40~60 min. for 50 frames on NVIDIA A6000 GPU</li> </ul> <h3 id="result">Result</h3> <ul> <li>Neural 3D Video Dataset :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-08-STGS/5.PNG-480.webp 480w,/assets/img/2025-02-08-STGS/5.PNG-800.webp 800w,/assets/img/2025-02-08-STGS/5.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-08-STGS/5.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-08-STGS/6.PNG-480.webp 480w,/assets/img/2025-02-08-STGS/6.PNG-800.webp 800w,/assets/img/2025-02-08-STGS/6.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-08-STGS/6.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Google Immersive Dataset :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-08-STGS/7.PNG-480.webp 480w,/assets/img/2025-02-08-STGS/7.PNG-800.webp 800w,/assets/img/2025-02-08-STGS/7.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-08-STGS/7.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-08-STGS/8.PNG-480.webp 480w,/assets/img/2025-02-08-STGS/8.PNG-800.webp 800w,/assets/img/2025-02-08-STGS/8.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-08-STGS/8.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Technicolor Dataset :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-08-STGS/9.PNG-480.webp 480w,/assets/img/2025-02-08-STGS/9.PNG-800.webp 800w,/assets/img/2025-02-08-STGS/9.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-08-STGS/9.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="ablation-study">Ablation Study</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-08-STGS/10.PNG-480.webp 480w,/assets/img/2025-02-08-STGS/10.PNG-800.webp 800w,/assets/img/2025-02-08-STGS/10.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-08-STGS/10.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-08-STGS/11.PNG-480.webp 480w,/assets/img/2025-02-08-STGS/11.PNG-800.webp 800w,/assets/img/2025-02-08-STGS/11.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-08-STGS/11.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="limitation">Limitation</h2> <ul> <li>Limitation : <ul> <li>need SfM for Spacetime Gaussians’ Initialization</li> <li>per-Scene model (<code class="language-plaintext highlighter-rouge">???</code> maybe)</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="dynamic"/><category term="3DGS"/><summary type="html"><![CDATA[Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis (CVPR 2024)]]></summary></entry><entry><title type="html">SplineGS</title><link href="https://semyeong-yu.github.io/blog/2025/SplineGS/" rel="alternate" type="text/html" title="SplineGS"/><published>2025-02-06T10:00:00+00:00</published><updated>2025-02-06T10:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/SplineGS</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/SplineGS/"><![CDATA[<h2 id="splinegs---robust-motion-adaptive-spline-for-real-time-dynamic-3d-gaussians-from-monocular-video-cvpr-2025">SplineGS - Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video (CVPR 2025)</h2> <h4 id="jongmin-park-minh-quan-viet-bui-juan-luis-gonzalez-bello-jaeho-moon-jihyong-oh-munchurl-kim">Jongmin Park, Minh-Quan Viet Bui, Juan Luis Gonzalez Bello, Jaeho Moon, Jihyong Oh, Munchurl Kim</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2412.09982">https://arxiv.org/abs/2412.09982</a><br/> project website :<br/> <a href="https://kaist-viclab.github.io/splinegs-site/">https://kaist-viclab.github.io/splinegs-site/</a></p> </blockquote> <blockquote> <p>핵심 :</p> <ol> <li>COLMAP-free :<br/> two-stage training strategy 사용<br/> 즉, camera param.을 먼저 roughly estimate한 뒤 jointly optimize camera param. and 3DGS param.</li> <li>dynamic scenes from in-the-wild monocular videos :<br/> static 3DGS와 dynamic 3DGS의 union</li> <li>dynamic 3DGS’s mean :<br/> apply spline-based model (MAS) to each dynamic 3DGS mean (trajectories)<br/> 이 때, depthmap과 camera param.를 이용해 2D track을 unproject하여 3D mean trajectories 초기화</li> <li>thousands time faster than SOTA :<br/> more efficient than MLP-based or grid-based</li> <li>loss :<br/> RGB image recon. loss<br/> depth recon. loss<br/> 2D projection alignment loss<br/> 3D alignment loss<br/> motion mask loss</li> </ol> </blockquote> <h2 id="contribution">Contribution</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/1.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/1.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/1.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/1.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>novelty : <ul> <li>Motion-Adaptive Spline (MAS) :<br/> continuous dynamic <code class="language-plaintext highlighter-rouge">3DGS trajectories</code> (deformation) 을 효율적으로 모델링하기 위해<br/> <code class="language-plaintext highlighter-rouge">cubic Hermite splines</code> with a small number of control points 사용 <ul> <li>control point : <ul> <li>learnable param.</li> <li>determines each piecewise cubic func.’s curvature and direction</li> </ul> </li> <li>initialization :<br/> <code class="language-plaintext highlighter-rouge">2D track</code>을 <code class="language-plaintext highlighter-rouge">depthmap</code> 이용하여 3D로 unproject</li> </ul> </li> <li>Motion-Adaptive Control points Pruning (MACP) :<br/> quality, efficiency 모두 챙기기 위해 계속 <code class="language-plaintext highlighter-rouge">control points를 prune</code>하여 수 조절</li> <li>joint optimization strategy :<br/> <code class="language-plaintext highlighter-rouge">photometric and geometric consistency</code> loss 이용해서<br/> (external estimators 필요 X)<br/> <code class="language-plaintext highlighter-rouge">camera param.</code> 와 <code class="language-plaintext highlighter-rouge">3DGS param.</code>를 jointly optimize<br/> (COLMAP-free!)</li> </ul> </li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li>dynamic novel-view-synthesis : <ul> <li>implicit representation (<code class="language-plaintext highlighter-rouge">MLP</code>) 이용하여 deformation 모델링 in canonical space <d-cite key="Deform1">[1]</d-cite>, <d-cite key="Deform2">[2]</d-cite>, <d-cite key="Deform3">[3]</d-cite>, <d-cite key="Deform4">[4]</d-cite>, <d-cite key="Deform5">[5]</d-cite> <ul> <li>단점 : 아무리 tiny MLP더라도 computational <code class="language-plaintext highlighter-rouge">overhead</code> and low speed</li> </ul> </li> <li>4D space-time domain을 <code class="language-plaintext highlighter-rouge">multiple 2D planes로 decompose</code>하는 grid-based model <d-cite key="Grid1">[6]</d-cite>, <a href="https://semyeong-yu.github.io/blog/2024/4DGS/">4DGS</a>, <d-cite key="Grid3">[7]</d-cite>, <d-cite key="Grid4">[8]</d-cite> <ul> <li>단점 : grid representation으로는 scene의 dynamic 특징의 <code class="language-plaintext highlighter-rouge">fine detail을 fully capture할 수 없음</code></li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">polynomial trajectories</code> 적용 <d-cite key="trajectory">[9]</d-cite> <ul> <li>장점 : efficient (low cost)</li> <li>단점 : polynomial trajectory의 <code class="language-plaintext highlighter-rouge">fixed degree</code>는 complex motion을 표현하는 flexibility 측면에서 제한적임</li> </ul> </li> </ul> </li> <li>spline : <ul> <li>minimal number of control points로 complex shape를 smooth and continuous representation으로 표현할 수 있음</li> </ul> </li> <li>SplineGS (본 논문) : <ul> <li>논문 <d-cite key="Mosca">[10]</d-cite>, <d-cite key="GauFRe">[11]</d-cite>에서처럼<br/> 각각 static bg와 moving object를 표현하기 위해<br/> 3DGS를 <code class="language-plaintext highlighter-rouge">static 3DGS와 dynamic 3DGS의 union</code>으로 확장 <ul> <li>static region :<br/> diffuse and specular features는 보존한 채<br/> time-encoded feature는 제거</li> <li>dynamic region :<br/> mean \(\mu_{i}\) 는 deformation modeling에 의해 결정되는 time-dependent var.<br/> rotation \(q_{i}\) 와 scale \(s_{i}\) 도 time-dependent var.</li> </ul> </li> <li>논문 <a href="https://semyeong-yu.github.io/blog/2025/STGS/">STGS</a>에서처럼<br/> final pixel <code class="language-plaintext highlighter-rouge">color</code>를 예측할 때 splatted feature rendering 사용 (<code class="language-plaintext highlighter-rouge">SH coeff. 대신 feature</code>!)</li> </ul> </li> </ul> <h2 id="method">Method</h2> <h3 id="architecture">Architecture</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/2.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/2.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>goal :<br/> jointly optimize 3DGS param. and camera param. <ul> <li>camera param. :<br/> extrinsic \([\hat R_{t} | \hat T_{t}] \in R^{3 \times 4}\) for each time \(t\)<br/> and shared intrinsic \(\hat K \in R^{3 \times 3}\) across all \(t\)</li> <li>how :<br/> two-stage optimization<br/> (warm-up stage and main traning stage) <ul> <li><code class="language-plaintext highlighter-rouge">warm-up stage</code> :<br/> optimize <code class="language-plaintext highlighter-rouge">coarse camera param.</code><br/> using photometric and geometric consistency<br/> (<code class="language-plaintext highlighter-rouge">SfM 사용하지 않기 위해!</code>)</li> <li><code class="language-plaintext highlighter-rouge">main training stage</code> :<br/> initialize 3DGS based on the estimated camera poses<br/> and<br/> jointly optimize 3DGS param. and camera param. with MAS and MACP</li> </ul> </li> </ul> </li> </ul> <h3 id="motion-adaptive-spline-for-3dgs">Motion-Adaptive Spline for 3DGS</h3> <p>time \(t\) 에서 each dynamic 3DGS의 mean \(\mu(t)\) (continuous trajectory)를 모델링하기 위해<br/> cubic Hermite spline function with a set of learnable control points 사용 (MAS)<br/> 즉, each dynamic Gaussian마다 a set of control points가 있고 얘네들의 spline curve로 Gaussian mean \(\mu(t)\) 을 결정!</p> <ul> <li>Motion-Adaptive Spline (<code class="language-plaintext highlighter-rouge">MAS</code>) :<br/> \(\mu(t) = S(t, \boldsymbol P)\) <ul> <li>input : <ul> <li>time \(t\)</li> <li>a set of \(N_{c}\) learnable control points<br/> \(\boldsymbol P = \{ \boldsymbol p_{k} | \boldsymbol p_{k} \in R^{3} \}\) where \(k \in [0, N_{c}-1]\)</li> </ul> </li> <li>piece-wise cubic Hermite spline function \(S(\cdot)\) :<br/> \(S(t, \boldsymbol P) = (2t_{r}^{3} - 3t_{r}^{2} + 1) \boldsymbol p_{\lfloor t_{s} \rfloor} + (t_{r}^{3} - 2t_{r}^{2} + t_{r}) \boldsymbol m_{\lfloor t_{s} \rfloor} + (-2t_{r}^{3} + 3t_{r}^{2}) \boldsymbol p_{\lfloor t_{s} \rfloor + 1} + (t_{r}^{3} - t_{r}^{2}) \boldsymbol m_{\lfloor t_{s} \rfloor + 1}\) <ul> <li>\(N_{f}\) : frame (timestamp) 개수</li> <li>\(N_{c}\) : control point 개수 (estimated by MACP)</li> <li> \[t \in [0, N_{f} - 1]\] </li> <li>\(t_{s} = t \frac{N_{c} - 1}{N_{f} - 1} \in [0, N_{c} - 1]\)<br/> e.g. 3.7</li> <li>\(t_{r} = t_{s} - \lfloor t_{s} \rfloor\)<br/> e.g. 0.7</li> <li>\(\boldsymbol m_{k} = (\boldsymbol p_{k+1} - \boldsymbol p_{k-1})/2\) : approx. tangent(기울기) of control point \(\boldsymbol p_{k}\)</li> <li><code class="language-plaintext highlighter-rouge">piece-wise cubic Hermite spline function</code> :<br/> \(\lfloor t_{s} \rfloor = 3\) 에서의 control point 및 tangent와<br/> \(\lfloor t_{s} \rfloor + 1 = 4\) 에서의 control point 및 tangent와<br/> 그 사이 어디쯤 있는지 \(t_{r} = 0.7\) 를 이용하여<br/> \(\lfloor t_{s} \rfloor = 3\) 과 \(\lfloor t_{s} \rfloor + 1 = 4\) 사이의 piece-wise cubic Hermite spline function을 그림</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/11m.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/11m.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/11m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/11m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/13m.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/13m.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/13m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/13m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Initialization of 3D Control Points</code> :<br/> intialization은 quality에 매우 중요!<br/> long-range <code class="language-plaintext highlighter-rouge">2D track</code> <d-cite key="cotracker">[12]</d-cite>과 <code class="language-plaintext highlighter-rouge">depth</code> <d-cite key="unidepth">[13]</d-cite> prior 사용 <ul> <li>notation : <ul> <li>2D track by <d-cite key="cotracker">[12]</d-cite> : \(\mathcal{T} = \left\{ \varphi_{t}^{tr} | \varphi_{t}^{tr} \in R^{2} \right\}_{t \in [0, N_{f} - 1]}\)<br/> where \(\varphi_{t}^{tr}\) : 2D track on pixel-coordinate at time \(t\)</li> <li>projection func. from 3D camera-space to 2D image-space by intrinsic \(K\) : \(\pi_{K}(\cdot)\)</li> </ul> </li> <li>Step 1)<br/> <code class="language-plaintext highlighter-rouge">unproject 2D track</code> \(\mathcal{T}\) on image-space into 3D track curve on world-space<br/> using <code class="language-plaintext highlighter-rouge">depth</code> \(d_{t}\) and coarsely-estimated <code class="language-plaintext highlighter-rouge">camera param.</code> \(\hat K, [\hat R_{t} | \hat T_{t}]\)<br/> \(W_{t}(\varphi_{t}^{tr}) = \hat R_{t}^{T} \pi_{\hat K}^{-1}(\varphi_{t}^{tr}, d_{t}(\varphi_{t}^{tr})) - \hat R_{t}^{T} \hat T_{t}\) <ul> <li>we estimate camera param. \(\hat K, \hat R, \hat T\) from only frames (without any GT)</li> </ul> </li> <li>Step 2)<br/> initialize per-Gaussian control points set \(\boldsymbol P\)<br/> by least-square approx. s.t. <code class="language-plaintext highlighter-rouge">spline curve</code> \(S(t, \boldsymbol P)\) fits the initial <code class="language-plaintext highlighter-rouge">tracker curve</code> \(W_{t}(\varphi_{t}^{tr})\)<br/> \(\text{min}_{\boldsymbol P} \sum_{t=0}^{N_{f} - 1} \| W_{t}(\varphi_{t}^{tr}) - S(t, \boldsymbol P) \|^{2}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/14m.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/14m.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/14m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/14m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Motion-Adaptive Control Points Pruning (<code class="language-plaintext highlighter-rouge">MACP</code>) : <ul> <li>issue : <ul> <li>control points 수가 너무 많으면<br/> spline curve가 over-fitting되고 speed가 느려짐</li> <li>scene마다 motion의 종류와 정도가 각기 다르므로<br/> control points 수 for each dynamic 3DGS 는 scene에 맞춰서 need to be adaptively adjusted</li> </ul> </li> <li>solution :<br/> sparser control points로 prune하기 위해<br/> <code class="language-plaintext highlighter-rouge">every 3DGS densification이 끝날 때마다</code> new spline function \(\mu(t) = S(t, \boldsymbol P')\) 계산<br/> where \(\boldsymbol P' = \left\{ \boldsymbol p_{l}' | \boldsymbol p_{l}' \in R^{3} \right\}_{l \in [0, N_{c} - 2]}\) : a set of \(N_{c} - 1\) control points<br/> (current set \(\boldsymbol P\) 보다 control point 1개 더 적음)</li> <li>Step 1)<br/> <code class="language-plaintext highlighter-rouge">1개 적은 control point set</code>으로도 최대한 비슷한 spline curve를 만들도록 least-square approx.<br/> \(\text{min}_{\boldsymbol P'} \sum_{t=0}^{N_{f}-1} \| S(t, \boldsymbol P) - S(t, \boldsymbol P') \|^{2}\)</li> <li>Step 2)<br/> \(S(t, \boldsymbol P)\) 와 \(S(t, \boldsymbol P')\) 간의 error \(E\) 가 작을 때만 a set of control points 업데이트<br/> \(\boldsymbol P = \begin{cases} \boldsymbol P' &amp; \text{if} &amp; E \lt \epsilon \\ \boldsymbol P &amp; O.W. \end{cases}\)<br/> where error \(E = \frac{1}{N_{f}} \sum_{t=0}^{N_{f} - 1} \| \pi_{\hat K}(\hat R_{t} S(t, \boldsymbol P) + \hat T_{t}) - \pi_{\hat K} (\hat R_{t} S(t, \boldsymbol P') + \hat T_{t} \|^{2}\)<br/> (각 timestamp \(t\) 에서 <code class="language-plaintext highlighter-rouge">3D mean on spline curve를 2D로 project시킨 뒤 차이</code> 비교)</li> <li>의의 :<br/> each dynamic 3DGS마다 a set of control points를 따로 가지고 있는데,<br/> MACP 덕분에 각 dynamic 3DGS가 각기 다른 수의 control points를 가질 수 있고,<br/> <code class="language-plaintext highlighter-rouge">motion이 복잡한 part는 control points 수가 많고</code><br/> <code class="language-plaintext highlighter-rouge">motion이 단순한 part는 control poitns 수가 적은</code> 방식으로<br/> scene에 adaptively adjust 가능</li> </ul> </li> </ul> <h3 id="camera-pose-estimation">Camera Pose Estimation</h3> <ul> <li>Camera Pose : <ul> <li><code class="language-plaintext highlighter-rouge">extrinsic</code> :<br/> \([\hat R_{t} | \hat T_{t}] = F_{\theta}(\gamma(t))\) <ul> <li>extrinsic 은 <code class="language-plaintext highlighter-rouge">time에 대한 function</code></li> <li>notation : <ul> <li>\(\gamma(\cdot)\) : positional encoding</li> <li>\(F_{\theta}\) : shallow MLP</li> </ul> </li> </ul> </li> <li>intrinsic (<code class="language-plaintext highlighter-rouge">focal length</code>) : <ul> <li>focal length \(\hat f\) 는 learnable param. <code class="language-plaintext highlighter-rouge">shared across all frames</code> in monocular video</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/12m.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/12m.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/12m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/12m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Loss for optimizing Camera Pose : <ul> <li>Loss 1) <code class="language-plaintext highlighter-rouge">photometric consistency</code> : <code class="language-plaintext highlighter-rouge">projection alignment</code> <ul> <li>목적 :<br/> target frame \(t\) 의 pixel \(i\) 가 reference frame \(t_{ref}\) 의 pixel \(j\) 로 projection 되었을 때<br/> reference frame’s pixel \(j\) 의 color \(I_{t_{ref}}(\varphi_{t \rightarrow t_{ref}})\) 가<br/> target frame’s pixel \(i\) 의 color \(I_{t}(\varphi_{t})\) 와 일치하도록</li> <li>notation : <ul> <li>\(\varphi_{t}\) : target frame’s pixel-coordinate</li> <li>\(\varphi_{t \rightarrow t_{ref}} = \pi_{\hat K} (\hat R_{t_{ref}} (\hat R_{t}^{T} \pi_{\hat K}^{-1} (\varphi_{t}, d_{t}(\varphi_{t})) - \hat R_{t}^{T} \hat T_{t}) + \hat T_{t_{ref}})\) : reference frame’s pixel-coordinate corresponding to \(\varphi_{t}\)<br/> (2D target frame \(t\)’s pixel-coordinate \(\rightarrow\) 3D location world-coordinate \(\rightarrow\) 2D reference frame \(t_{ref}\)’s pixel-coordinate)</li> </ul> </li> <li>loss :<br/> \(L_{pc} = \sum_{\varphi_{t}} \| M_{t, t_{ref}}(\varphi_{t}) \circledast (I_{t}(\varphi_{t}) - I_{t_{ref}}(\varphi_{t \rightarrow t_{ref}})) \|^{2}\) <ul> <li>\(M_{t, t_{ref}} = M_{t}(\varphi_{t}) M_{t_{ref}}(\varphi_{t \rightarrow t_{ref}})\) : union motion mask<br/> (dynamic objects는 color 변하는 게 당연하니까 제거하고, static region에 대해서만 loss 걸어줌)<br/> (\(M_{t}\) 와 \(M_{t_{ref}}\) 는 각각 \(I_{t}\) 와 \(I_{t_{ref}}\) 로부터 미리 계산한 motion mask <d-cite key="TrackAnything">[14]</d-cite>)</li> </ul> </li> </ul> </li> <li>Loss 2) <code class="language-plaintext highlighter-rouge">geometric consistency</code> : <code class="language-plaintext highlighter-rouge">3D alignment</code> <ul> <li>목적 :<br/> target frame \(t\) 의 pixel \(i\) 가 reference frame \(t_{ref}\) 의 pixel \(j\) 로 projection 되었을 때<br/> reference frame’s pixel \(j\) 를 3D location on world-coordinate으로 unproject시킨 \(W_{t_{ref}}(\varphi_{t \rightarrow t_{ref}})\) 가<br/> target frame’s pixel \(i\) 를 3D location on world-coordinate으로 unproject시킨 \(W_{t}(\varphi_{t})\) 와 일치하도록</li> <li>notation : <ul> <li>\(W_{t}(\varphi_{t}) = \hat R_{t}^{T} \pi_{\hat K}^{-1}(\varphi_{t}, d_{t}(\varphi_{t})) - \hat R_{t}^{T} \hat T_{t}\) : unproject from pixel-coordinate to 3D world-coordinate</li> </ul> </li> <li>loss :<br/> \(L_{gc} = \sum_{\varphi_{t}} \| M_{t, t_{ref}}(\varphi_{t}) \circledast (W_{t}(\varphi_{t}) - W_{t_{ref}}(\varphi_{t \rightarrow t_{ref}})) \|^{2}\)</li> </ul> </li> </ul> </li> </ul> <h3 id="loss">Loss</h3> <ul> <li>Two-stage Optimization : <ul> <li>Stage 1) warm-up stage <ul> <li>optimize <code class="language-plaintext highlighter-rouge">only camera param.</code></li> <li>loss :<br/> \(L_{total}^{warm} = \lambda_{pc} L_{pc} + \lambda_{gc} L_{gc}\) <ul> <li>\(L_{pc}\) : photometric consistency (projection alignment)</li> <li>\(L_{gc}\) : geometric consistency (3D alignment)</li> </ul> </li> </ul> </li> <li>Stage 2) main training stage <ul> <li>Step 2-1)<br/> Stage 1)에서 coarsely 예측한 camera param. \(\hat K, \hat R, \hat T\) 를 이용하여<br/> 각 dynamic 3DGS의 <code class="language-plaintext highlighter-rouge">a set of control points 초기화</code><br/> (how? : 위의 Motion-Adaptive Spline for 3DGS 섹션에서 설명함)</li> <li>Step 2-2)<br/> <code class="language-plaintext highlighter-rouge">jointly optimize 3DGS param. and camera param.</code></li> <li>loss :<br/> \(L_{total}^{main} = \lambda_{rgb} L_{rgb} + \lambda_{d} L_{d} + \lambda_{M} L_{M} + \lambda_{pc} L_{pc} + \lambda_{d-pc} L_{d-pc} + \lambda_{gc} L_{gc}\) <ul> <li><code class="language-plaintext highlighter-rouge">recon. loss</code> : <ul> <li>\(L_{rgb}\) : L1 recon. loss b.w. rendered frame and GT frame</li> <li>\(L_{d}\) : L1 recon. loss b.w. rendered depth and GT depth</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">alignment loss</code> : <ul> <li>\(L_{pc}\) : photometric consistency (projection alignment)</li> <li>\(L_{gc}\) : geometric consistency (3D alignment)</li> <li>\(L_{d-pc}\) : additional photometric consistency (projection alignment) <ul> <li><code class="language-plaintext highlighter-rouge">prior depth</code> <d-cite key="unidepth">[13]</d-cite> <code class="language-plaintext highlighter-rouge">대신</code> 3DGS를 이용한 <code class="language-plaintext highlighter-rouge">rendered depth</code> 사용하여<br/> photometric consistency 계산</li> <li>prior depth 대신 rendered depth를 사용하면<br/> estimated 3DGS geometry 의 도움을 받아<br/> joint optimization of camera param. and 3DGS param. 가능!</li> </ul> </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">motion mask loss</code> : <ul> <li>\(L_{M} = 1 - \text{f1-score} = 1 - \frac{2(\sum_{\varphi_{t}} M_{t}(\varphi_{t}) \hat M_{t}(\varphi_{t})) + \epsilon}{(\sum_{\varphi_{t}} M_{t}(\varphi_{t}) + \hat M_{t}(\varphi_{t})) + \epsilon}\) : binary dice loss<br/> b.w. <code class="language-plaintext highlighter-rouge">pre-computed GT motion mask</code> \(M_{t}\) from prior <d-cite key="TrackAnything">[14]</d-cite><br/> and <code class="language-plaintext highlighter-rouge">rendered motion mask</code> \(\hat M_{t}\) from dynamic 3D Gaussians <ul> <li>rendered motion mask :<br/> \(\hat M_{t}(\varphi_{t}) = \sum_{i \in N} m_{i} \alpha_{i} \prod_{j=1}^{i-1} (1 - \alpha_{j})\)<br/> where \(m_{i} = 0\) if \(i\)-th 3DGS is static 3DGS, and \(m_{i} = 1\) if \(i\)-th 3DGS is dynamic 3DGS<br/> (즉, \(i\)-th 3DGS가 static인지, dynamic인지에 따른 \(m_{i}\) 를 accumulate 하여 motion mask로 rendering!)</li> </ul> </li> <li>binary dice loss는 highly imbalanced segmentation을 위해 제안되었듯이<br/> dynamic 3DGS와 static 3DGS를 더 잘 분리할 수 있게 해줌</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <ul> <li>Dataset : <ul> <li>NVIDIA dataset <ul> <li>evaluation configuration : <d-cite key="RoDynRF">[RoDynRF]</d-cite> 를 따름</li> <li>dataset sampling : <d-cite key="NSFF">[NSFF]</d-cite> 를 따름 <ul> <li>sample 24 timestamps</li> <li>larger motion을 simulate하기 위해<br/> 홀수 frames 제외</li> <li>generalization을 위해<br/> test 시에 사용할 timestamps를 training할 때 제외</li> </ul> </li> </ul> </li> <li>DAVIS dataset (avg. 70 frames per video)</li> </ul> </li> </ul> <h3 id="result">Result</h3> <ul> <li>Novel-View-Synthesis : <ul> <li>SOTA baseline : <ul> <li>COLMAP-based : <d-cite key="colmap1">[DynNeRF]</d-cite>, <d-cite key="colmap2">[MonoNeRF]</d-cite>, <d-cite key="colmap3">[STGS]</d-cite>, <d-cite key="colmap4">[SCGS]</d-cite>, <d-cite key="Deform5">[D3DGS]</d-cite>, <d-cite key="4DGS">[4DGS]</d-cite>, <d-cite key="RoDynRF">[RoDynRF]</d-cite>, <d-cite key="CasualFVS">[CasualFVS]</d-cite>, <d-cite key="Ex4DGS">[Ex4DGS]</d-cite>, <d-cite key="Mosca">[Mosca]</d-cite> <ul> <li>RoDynRF, DynNeRF : 느림</li> <li>Ex4DGS, STGS : multi-view setting으로 설계되어 monocular video로 학습하면 시간에 따라 점점 inconsistent geometry alignment</li> <li>D3DGS, STGS : SfM(COLMAP)이 DAVIS dataset에서 camera param. 및 initial pcd 잘 추정 못함</li> </ul> </li> <li>COLMAP-free : <d-cite key="RoDynRF">[RoDynRF]</d-cite>, <d-cite key="Mosca">[Mosca]</d-cite></li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/3.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/3.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/4.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/4.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/5.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/5.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/5.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/5.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Novel View and Time Synthesis : <ul> <li>SOTA baseline : <ul> <li>NeRF-based : <d-cite key="colmap1">[DynNeRF]</d-cite>, <d-cite key="RoDynRF">[RoDynRF]</d-cite> <ul> <li>RoDynRF, DynNeRF : unseen timestamp에 대해 artifacts 및 blurriness 생김</li> </ul> </li> <li>3DGS-based : <d-cite key="colmap3">[STGS]</d-cite>, <d-cite key="Deform5">[D3DGS]</d-cite>, <d-cite key="4DGS">[4DGS]</d-cite> <ul> <li>STGS, D3DGS, 4DGS : unseen timestamp에 대해 더 심각한 degradation 생김</li> </ul> </li> </ul> </li> <li>본 논문 (SplineGS) :<br/> MAS(Motion-Adaptive Spline) 덕분에<br/> dynamic 3D Gaussian들을 효과적으로 deform시켜서<br/> 시간에 따라 움직이는 물체의 continuous trajectories를 정확히 캡처할 수 있음 <ul> <li>unseen timestamp에 대해서도 continuous trajectory로 잘 캡처 가능</li> <li>temporal consistency는 아래의 tOF score로 확인 가능</li> <li>continuous trajectories 모델링 능력을 확인하기 위해<br/> 아래 그림에 dynamic objects의 projected 2D motion tracking 결과도 있음</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/6.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/6.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/6.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/6.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/7.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/7.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/7.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/7.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="ablation-study">Ablation Study</h3> <ul> <li>Motion-Adaptive Spline (MAS) : <ul> <li>baseline : various deformation models <ul> <li>MLP<br/> e.g. D3DGS</li> <li>grid-based model<br/> e.g. 4DGS</li> <li>polynomial func. of degree 3 or 10<br/> e.g. STGS<br/> (degree 10을 쓰면 numerical instability 때문에 noisier optimization으로 quality도 더 안 좋고, latency도 증가함)</li> <li>Bezier curve<br/> (성능 비슷하게 좋지만, recursive 계산 때문에 MAS보다 latency 큼)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/8.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/8.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/8.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/8.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Motion-Adaptive Control Points Pruning (MACP) : <ul> <li>baseline : fixed number of control points \(N_{c} = 4\) or \(N_{c} = N_{f}\)</li> <li>MAS with MACP :<br/> good trade-off (latency 조금 증가하지만 rendering quality 많이 증가)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/9.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/9.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/9.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/9.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Skating scene처럼 simple motion인 경우에는 MACP 덕분에 최소한의 N_c로도 대부분의 dynamic 3DGS 표현 가능 </div> <h2 id="conclusion">Conclusion</h2> <ul> <li>Limitation : <ul> <li>Prior 필요 <ul> <li>depthmap (for all)</li> <li>2D track (for all)</li> <li>motion mask (for \(L_{M}\))</li> </ul> </li> <li>in-the-wild video에서 camera 또는 object가 매우 빠르게 움직이는 경우 input frames 자체가 blurry한데,<br/> 이러한 input frames의 흐림 자체가 rendering quality를 낮춤 <ul> <li>현재 가능한 solution :<br/> SOTA 2D deblurring methods를 pre-processing으로 먼저 input frames에 적용한 뒤 training에 사용</li> <li>future work :<br/> 따로 pre-processing하지 않고,<br/> deblurring method와 recon. pipeline을 통합하여<br/> joint deblurring and rendering optimization framework 구축</li> </ul> </li> <li>per-Scene model이라 feed-forward model로 확장 가능 <code class="language-plaintext highlighter-rouge">???</code></li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/10.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/10.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/10.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/10.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br/> photometric consistency loss에서 motion mask 값이 static region에 대해서만 0이라는데,<br/> dynamic object를 exclude한다는 문구로 미루어보아 (static region에만 loss를 걸어주기 위해)<br/> static region에 대해서 1이어야 하는 거 아닌가요?</p> </li> <li> <p>A1 :<br/> code implementation 한 번 보자</p> </li> <li> <p>Q2 :<br/> colmap을 안 쓰면 Gaussian mean initialization은 어떻게 하나요?</p> </li> <li> <p>A2 :<br/> per-Gaussian parameter로 Gaussian마다 a set of control points \(\boldsymbol P = \{ \boldsymbol p_{k} | \boldsymbol p_{k} \in R^{3} \}\) where \(k \in [0, N_{c}-1]\) 를 가지고 있고,<br/> depth prior와 coarsely-estimated camera param.로 2D track prior를 unproject한 뒤 LS approx.로 a set of control points \(\boldsymbol P\) 를 initialize (그러면 arbitrary timestep \(t\) 에서의 Gaussian mean은 해당 spline function 위에 있게 됨)</p> </li> <li> <p>Q3 :<br/> MACP로 control point를 하나 뺄 대 어떤 control point를 빼나요?</p> </li> <li> <p>A3 :<br/> 하나 적은 control points’ set으로 new spline function \(S(t, \boldsymbol P)\) 를 다시 만들고, control point들은 time에 대해 균등하게 배치</p> </li> <li> <p>Q4 :<br/> colmap-free라서 camera parameter를 추정한다고 할 때 요즘 논문들은 보편적으로 depth prior를 활용하여 학습을 통해 추정하나요?</p> </li> <li> <p>A4 :<br/> 아니요, NoPoSplat 같은 논문을 보면 depth prior 없어도 transformer가 photometric loss만으로 camera parameter 추정하기도 합니다</p> </li> <li> <p>Q5 :<br/> two-stage로 나눠서 학습을 통해 먼저 camera parameter를 추정하려면 training time이 더 오래 걸릴텐데, COLMAP pre-processing 쓰지 않고 직접 camera parameter를 추정하는 이유가 뭔가요?</p> </li> <li> <p>A5 :<br/> COLMAP으로 camera pose를 pre-compute 해놓는 건 in-the-wild video에서 별로 성능 안 좋음 (특히 원형 호수 같은 images에서는 COLMAP 성능 꽝). 그래서 차라리 직접 학습을 통해 camera parameter를 추정하는 게 더 성능 좋음</p> </li> <li> <p>Q6 :<br/> dynamic NVS modeling 기법 중에 Spline-based representation이 갖는 장점은?</p> </li> <li>A6 : <ul> <li>grid-based modeling - 단점 : struggle to fully capture the fine details<br/> e.g. HexPlane, 4DGS</li> <li>polynomial modeling - 단점 : fixed degree restricts flexibility for complex motions</li> <li>spline-based modeling - 장점 : explicit spline function이 continuous trajectory를 보장해서 unseen novel timestamp에 대해서도 생각보다 퀄리티 괜찮음</li> </ul> </li> <li> <p>Q7 :<br/> camera parameter를 추정할 때 unproject함으로써 photometric consistency와 geometric consistency를 이용하는데,<br/> 만약에 카메라(extrinsic)가 고정된 채 scene content만 dynamic하게 움직이는 경우에는 해당 두 가지 loss가 잘 작동하지 않을 것 같습니다</p> </li> <li>A7 :<br/> 맞는 말입니다. 하지만 dataset 중에 dynamic scene을 찍는 카메라가 움직이지 않는 경우는 거의 없고, camera movement가 작은 DAVIS data가 있긴 한데 거기서도 해당 모델이 잘 작동하긴 했습니다.</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="dynamic"/><category term="colmap"/><category term="free"/><category term="motion"/><category term="adaptive"/><category term="monocular"/><summary type="html"><![CDATA[Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video (CVPR 2025)]]></summary></entry><entry><title type="html">NoPoSplat</title><link href="https://semyeong-yu.github.io/blog/2025/NoPoSplat/" rel="alternate" type="text/html" title="NoPoSplat"/><published>2025-02-03T10:00:00+00:00</published><updated>2025-02-03T10:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/NoPoSplat</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/NoPoSplat/"><![CDATA[<h2 id="no-pose-no-problem---surprisingly-simple-3d-gaussian-splats-from-sparse-unposed-images">No Pose, No Problem - Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images</h2> <h4 id="botao-ye-sifei-liu-haofei-xu-xueting-li-marc-pollefeys-ming-hsuan-yang-songyou-peng">Botao Ye, Sifei Liu, Haofei Xu, Xueting Li, Marc Pollefeys, Ming-Hsuan Yang, Songyou Peng</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2410.24207">https://arxiv.org/abs/2410.24207</a><br/> project website :<br/> <a href="https://noposplat.github.io/">https://noposplat.github.io/</a><br/> code :<br/> <a href="https://github.com/cvg/NoPoSplat">https://github.com/cvg/NoPoSplat</a></p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/4.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/4.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="contribution">Contribution</h2> <p><code class="language-plaintext highlighter-rouge">pose-free generalizable sparse-view 3D recon. model in canonical Gaussian space!</code></p> <ul> <li>model : <ul> <li><code class="language-plaintext highlighter-rouge">unposed</code> (no extrinsic) <code class="language-plaintext highlighter-rouge">sparse-view</code> images로부터 3DGS를 통해 3D scene recon.하는 feed-forward network 제시</li> <li><code class="language-plaintext highlighter-rouge">photometric loss만으로</code> train 가능<br/> (<code class="language-plaintext highlighter-rouge">GT depth 사용 X</code>, explicit matching loss 사용 X)</li> <li>본 논문은 intrinsic의 영향을 받는 image appearance에만 의존하여 recon.을 수행하므로<br/> <code class="language-plaintext highlighter-rouge">scale ambiguity</code> 문제 해결을 위해 <code class="language-plaintext highlighter-rouge">intrinsic embedding method</code> 사용<br/> (intrinsic은 input으로 사용)</li> <li>covariance, opacity, color를 예측하는 Gaussian Param. Head에서 fine texture detail 주기 위해 <code class="language-plaintext highlighter-rouge">RGB shortcut</code> 사용</li> </ul> </li> <li>downstream tasks : <ul> <li>recon.된 3DGS를 이용하여 novel-view-synthesis 및 pose-estimation task 수행 가능 <ul> <li>특히 limited input image overlap (sparse) 상황에서는 pose-required methods보다 더 좋은 성능</li> <li>정확히 pose-estimation 수행하는 two-stage coarse-to-fine pipeline 제시</li> </ul> </li> <li>generalize well to out-of-distribution data</li> </ul> </li> <li>Gaussian Space : <ul> <li><code class="language-plaintext highlighter-rouge">first input view의 local camera coordinate</code>을 <code class="language-plaintext highlighter-rouge">canonical space</code>로 고정하고 모든 input view의 3DGS들을 해당 space에서 directly 예측</li> <li>기존에는 transform-then-fuse pipeline이었는데,<br/> 본 논문은 global coordinate으로의 <code class="language-plaintext highlighter-rouge">explicit transform 없이</code> canonical space 내에서의 different views의 fusion 자체를 직접 network로 학습</li> <li>local coordinate에서 global coordinate으로 3DGS를 explicitly transform할 필요가 없으므로<br/> explicitly transform하면서 생기는 per-frame Gaussians의 misalignment를 방지할 수 있고, extrinsic pose 없이도 (pose-free) 3D recon. 가능</li> </ul> </li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li>SfM : <ul> <li>bundle adjustment 등 최적화 과정을 거치는데,<br/> off-the-shelf pose estimation method 사용하는 것 자체가 많은 연산을 필요로 하고 runtime 늘림</li> <li>3D recon.에 only two frames만 input으로 사용하더라도<br/> SfM을 통해 해당 two frames의 camera pose를 구하려면 many poses from dense videos 필요 (impractical)</li> <li>textureless area (원형 호수 등) 또는 image가 sparse한 영역에서는 부정확한 pose 내놓음</li> </ul> </li> <li>Pose-Free Method : <ul> <li>pose-estimation과 3D recon.을 single pipeline으로 통합하자! : <d-cite key="DBARF">[1]</d-cite>, <d-cite key="Flowcam">[2]</d-cite>, <d-cite key="Unifying">[3]</d-cite> <ul> <li>pose-estimation과 scene-recon.을 번갈아가며 수행하는 sequential process 에서 error가 쌓이기 때문에<br/> SOTA novel-view-synthesis methods보다 성능 bad</li> </ul> </li> <li>DUSt3R, MASt3R 계열</li> </ul> </li> <li>DUSt3R, MASt3R : <ul> <li>공통점 1)<br/> pose-free method</li> <li>공통점 2)<br/> directly predict in canonical space</li> <li>차이점 1)<br/> DUSt3R, MASt3R는 transformer output이 3D pointmap (point cloud)인데,<br/> NoPoSplat은 mean, covariance, opacity, color를 가진 3DGS (rasterization) 사용</li> <li>차이점 2)<br/> NoPoSplat은 DUSt3R, MASt3R 계열과 달리 <code class="language-plaintext highlighter-rouge">GT depth 필요 없고 photometric loss만으로</code> 훈련 가능</li> </ul> </li> <li>pixelSplat, MVSplat : <ul> <li>차이점 1) (아래 그림 참고)<br/> pixelSplat, MVSplat은 먼저 intrinsic을 이용해 2D-to-3D로 unproject(lift)하여 each local-coordinate에서 3DGS를 예측한 뒤 extrinsic을 이용해 world-coordinate으로 transform한 뒤 fuse했는데,<br/> NoPoSplat은 canonical space 내에서의 different views의 fusion 자체를 directly network로 학습하기 때문에 <code class="language-plaintext highlighter-rouge">global coordinate으로 transform할 필요가 없으므로</code> 이에 따른 <code class="language-plaintext highlighter-rouge">misalignment를 방지</code>할 수 있고 <code class="language-plaintext highlighter-rouge">camera pose (extrinsic)도 필요 없음</code></li> <li>차이점 2)<br/> pixelSplat에선 epipolar constraint, MVSplat에선 cost volume이라는 geometry prior를 사용하였는데<br/> image view overlap이 적을 때는 geometry prior가 정확하지 않음.<br/> NoPoSplat은 (image overlap이 클 때 유리한) <code class="language-plaintext highlighter-rouge">geometry prior들을 사용하지 않음</code></li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/2.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/2.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="method">Method</h2> <h3 id="architecture">Architecture</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/3.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/3.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>I/O :<br/> \(f_{\theta} : \left\{ (I^{v}, k^{v}) \right\}_{v=1}^{V} \mapsto \left\{ \bigcup (\mu_{j}^{v}, \alpha_{j}^{v}, r_{j}^{v}, s_{j}^{v}, c_{j}^{v}) \right\}_{j=1, \ldots, H \times W}^{v=1, \ldots, V}\) <ul> <li>input : <ul> <li>sparse unposed multi-view images \(I\) (image 개수 \(V\))</li> <li>camera intrinsics \(k\) (available from modern devices <d-cite key="intrinsic">[4]</d-cite>)</li> </ul> </li> <li>output : <ul> <li>mean \(\mu \in R^{3}\), opacity \(\alpha \in R\), rotation \(r \in R^{4}\), scale \(s \in R^{3}\), SH \(c \in R^{k}\) (\(k\) degrees of freedom)</li> </ul> </li> </ul> </li> <li>Pipeline : <ul> <li><code class="language-plaintext highlighter-rouge">Encoder, Decoder</code> : <ul> <li>특히 input views끼리 content overlap이 적은 상황 (sparse) 에서는<br/> epipolar constraint나 cost volume 같은 geometry prior가 없더라도<br/> simple ViT 구조만으로도 좋은 성능 달성 가능</li> <li>RGB images를 image tokens로 patchify, flatten한 뒤<br/> intrinsic token과 concatenate한 뒤<br/> Encoder and Decoder에 feed-forward</li> </ul> </li> <li>Gaussian Parameter Prediction Head :<br/> DPT 구조 <ul> <li><code class="language-plaintext highlighter-rouge">Gaussian Center Head</code> :<br/> Decoder feature 사용</li> <li><code class="language-plaintext highlighter-rouge">Gaussian Param Head</code> :<br/> RGB image와 Decoder feature 사용 <ul> <li><code class="language-plaintext highlighter-rouge">RGB shortcut</code> :<br/> 3D recon.에서 fine texture detail을 잡는 것이 중요하기 때문에 사용</li> <li>Decoder feature :<br/> high-level semantic info.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <h3 id="gaussian-space">Gaussian Space</h3> <ul> <li>baseline: <code class="language-plaintext highlighter-rouge">Local-to-Global Gaussian Space</code> <ul> <li>pixelSplat, MVSplat 등</li> <li>how :<br/> 먼저 each pixel의 depth를 network로 예측한 뒤<br/> predicted depth와 intrinsic을 이용해 2D-to-3D로 unproject(lift)하여 each local-coordinate에서 3DGS 예측한 뒤<br/> extrinsic을 이용해 world-coordinate으로 transform한 뒤<br/> 모든 transformed 3DGS들을 fuse</li> <li>issue : <ul> <li>local-coordinate에서 world-coordinate으로 transform할 때 <code class="language-plaintext highlighter-rouge">accurate camera pose</code> (extrinsic) 필요한데, 이는 input view가 sparse한 real-world 상황에서 얻기 어렵</li> <li>특히 input view가 sparse할 때 또는 out-of-distribution data로 일반화할 때는<br/> <code class="language-plaintext highlighter-rouge">each transformed 3DGS들을 조화롭게 combine</code>하는 게 어렵</li> </ul> </li> </ul> </li> <li>NoPoSplat: <code class="language-plaintext highlighter-rouge">Canonical Gaussian Space</code> <ul> <li>how :<br/> first input view를 global referecne coordinate으로 고정한 뒤 (\([R | t] = [\boldsymbol I | \boldsymbol 0]\))<br/> 해당 coordinate 내에서 each input view \(v\) 마다 set \(\left\{ \mu_{j}^{v \rightarrow 1}, r_{j}^{v \rightarrow 1}, c_{j}^{v \rightarrow 1}, \alpha_{j}, s_{j} \right\}\) 을 예측<br/> where view \(1\) : canonical Gaussian space</li> <li>benefit : <ul> <li>global coordinate으로 explicitly transform할 필요가 없으므로 camera pose (extrinsic) 필요 없음</li> <li>explicitly transform-then-fuse하는 게 아니라 fuse 자체를 network로 학습하는 것이기 때문에<br/> 조화로운 global representation 가능</li> </ul> </li> </ul> </li> </ul> <h3 id="camera-intrinsic-embedding">Camera Intrinsic Embedding</h3> <ul> <li>Camera Intrinsic Embedding : <ul> <li>issue :<br/> only appearance에만 의존하여 3D recon.을 수행함<br/> <code class="language-plaintext highlighter-rouge">scale ambiguity</code> (scale misalignment) 문제 해결 필요!<br/> 필요한 geometric info.를 제공하기 위해!<br/> intrinsic \(k = [f_{x}, f_{y}, c_{x}, c_{y}]\)</li> <li>solve : <ul> <li>Trial 1) Global Intrinsic Embedding by Addition :<br/> intrinsic \(k\) 을 linear layer에 통과시킨 뒤 RGB image token에 add</li> <li>Trial 2) Global Intrinsic Embedding by Concat :<br/> intrinsic \(k\) 을 linear layer에 통과시킨 뒤 RGB image token에 concat</li> <li>Trial 3) Pixel-wise (Dense) Intrinsic Embedding :<br/> each pixel \(p_{j}\)에 대해 ray direction \(K^{-1} p_{j}\) 구한 뒤<br/> SH 이용해서 high-dim. feature로 변환한 뒤<br/> RGB image와 concat</li> </ul> </li> </ul> </li> </ul> <h3 id="training-and-inference">Training and Inference</h3> <ul> <li> <p>Loss :<br/> only photometric loss<br/> (linear comb. of MSE and LPIPS)</p> </li> <li>Relative Pose Estimation :<br/> canonical space에 3DGS들이 있다는 전제 하에<br/> <code class="language-plaintext highlighter-rouge">two-stage coarse-to-fine pipeline</code> <ul> <li>Coarse Stage :<br/> Gaussian center에 <code class="language-plaintext highlighter-rouge">PnP algorithm with RANSAC</code> (efficient as done in ms) 적용하여<br/> <code class="language-plaintext highlighter-rouge">initial rough pose estimate</code> 구하기</li> <li>Fine Stage :<br/> <code class="language-plaintext highlighter-rouge">3DGS param.을 freeze</code>한 채<br/> training에 사용했던 <code class="language-plaintext highlighter-rouge">photometric loss</code>를 이용해<br/> target view와 align되도록 rough <code class="language-plaintext highlighter-rouge">target camera pose를 optimize</code>(refine) <ul> <li>automatic diff.에서의 overhead를 줄이기 위해<br/> camera Jacobian을 계산 <d-cite key="GSslam">[5]</d-cite></li> </ul> </li> </ul> </li> <li>Evaluation-Time Pose Alignment : <ul> <li>unposed input images의 경우<br/> scene은 다른데 rendered two images는 같을 수 있으므로<br/> just two input views로 3D scene recon. 수행하는 건 사실 ambiguous</li> <li>GT camera pose를 이용하는 other baseline들 <d-cite key="pose1">[6]</d-cite>, <a href="https://semyeong-yu.github.io/blog/2024/pixelSplat/">7</a>과 비교하기 위해 (evaluation purpose)<br/> pose-free methods <d-cite key="nopose1">[8]</d-cite>, <d-cite key="nopose2">[9]</d-cite>의 경우 target view에 대한 camera pose를 optimize한 뒤 비교에 사용</li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <h3 id="implementation">Implementation</h3> <ul> <li>Experiment : <ul> <li>Dataset : <ul> <li>training :<br/> RE10K (RealEstate10k) : indoor real estate<br/> DL3DV : outdoor (camera motion pattern 더 다양)</li> <li>zero-shot generalization :<br/> ACID : nature scene by drone<br/> DTU<br/> ScanNet<br/> ScanNet++<br/> in-the-whild mobile phone capture<br/> SORA-generated images</li> </ul> </li> <li>camera overlap :<br/> SOTA dense feature matching method <d-cite key="ROMA">[10]</d-cite> 로<br/> input images’ camera overlap 정도를 측정하여<br/> small (0.05%-0.3%), medium (0.3%-0.55%), large (0.55%-0.8%)로 나눔</li> <li>Baseline : <ul> <li>pose-required novel-view-synthesis :<br/> pixelNeRF, AttnRend, pixelSplat, MVSplat</li> <li>pose-free novel-view-synthesis and relative pose estimation :<br/> DUSt3R, MASt3R, Splatt3R, CoPoNeRF, RoMa</li> </ul> </li> <li>Implementation :<br/> encoder, decoder, Gaussian center head는 MASt3R의 weights로 initialize하고<br/> (사실 scratch부터 training해도 성능 비슷하긴 함)<br/> Gaussian param head는 randomly initialize</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/5.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/5.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/5.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/5.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="result">Result</h3> <ul> <li>Novel View Synthesis : <ul> <li>SOTA pose-free (DUSt3R, MASt3R, Splatt3R) : <ul> <li>DUSt3R 계열은 <code class="language-plaintext highlighter-rouge">per-pixel depth loss</code>에 의존하기 때문에 each views를 <code class="language-plaintext highlighter-rouge">fuse하는 게 어렵</code><br/> 그래서 대부분 상황에서 NoPoSplat이 훨씬 더 좋음</li> </ul> </li> <li>SOTA pose-required (pixelSplat, MVSplat) : <ul> <li>pixelSplat, MVSplat은 <code class="language-plaintext highlighter-rouge">input view overlap이 작을 때 부정확한 geometry prior</code> (epipolar constraint, cost volume)을 사용하기 때문에<br/> image view overlap이 작은 상황에서는 NoPoSplat이 더 좋음</li> <li>pixelSplat, MVSplat은 <code class="language-plaintext highlighter-rouge">transform-then-fuse strategy</code>를 사용하는데 <code class="language-plaintext highlighter-rouge">misalignment</code>로 부정확할 수 있기 때문에<br/> canonical space에서 directly 예측하는 NoPoSplat이 더 좋을 수 있음</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/6.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/6.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/6.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/6.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/7.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/7.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/7.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/7.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Relative Pose Estimation :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/8.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/8.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/8.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/8.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Geometry Reconstruction : <ul> <li>SOTA pose-required (pixelSplat, MVSplat) : <ul> <li>pixelSplat, MVSplat은 explicitly transform-then-fuse하는 과정에서 두 input images의 경계 영역에서 misalignment (아래 그림에서 파란색 화살표로 표기) 가 있고,<br/> input views’ overlap이 적을 때는 geometry prior가 부정확해서 distortion (아래 그림에서 분홍색 화살표로 표기) 있는데,<br/> NoPoSplat은 canonical space에서 directly 예측하므로 해결</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/9.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/9.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/9.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/9.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Cross-Dataset Generalization :<br/> NoPoSplat은 geometry prior를 사용하지 않으므로 다양한 scene type에 adapt 가능<br/> 심지어 ScanNet++로의 zero-shot generalization에 대해 RE10K로 훈련시킨 NoPoSplat과 ScanNet++로 훈련시킨 pose-required Splatt3R을 비교했을 때 NoPoSplat이 더 좋음!</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/10.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/10.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/10.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/10.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Model Efficiency :<br/> NoPoSplat은 0.015초만에 (66 FPS) 3DGS 예측 가능<br/> (additional geometry prior 안 쓰니까 speed 빠름!)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/11.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/11.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/11.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/11.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> inference on RTX 4090 GPU </div> <ul> <li>In-the-Wild Unposed Images :<br/> 3D Generation task에 적용 가능!<br/> 먼저 text/image to multi-image/video model 이용해서 sparse scene-level multi-view images 얻은 뒤<br/> Ours (NoPoSplat) 이용해서 3D model 얻음</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/13.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/13.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/13.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/13.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="ablation-study">Ablation Study</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/12.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/12.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/12.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/12.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Ablation Study : <ul> <li><code class="language-plaintext highlighter-rouge">Output Canonical Gaussian Space</code> :<br/> transform-then-fuse pipeline of pose-required methods has <code class="language-plaintext highlighter-rouge">ghosting artifacts</code></li> <li><code class="language-plaintext highlighter-rouge">Camera Intrinsic Embedding</code> :<br/> no intrinsic leads to <code class="language-plaintext highlighter-rouge">blurry</code> results due to <code class="language-plaintext highlighter-rouge">scale ambiguity</code><br/> 실험적으로 intrinsic token concat. 방식이 best</li> <li><code class="language-plaintext highlighter-rouge">RGB Shortcut</code> :<br/> no RGB Shortcut leads to <code class="language-plaintext highlighter-rouge">blurry</code> results in texture-rich areas<br/> (위 그림의 quilt in row 1 and chair in row 3)</li> <li><code class="language-plaintext highlighter-rouge">3 Input Views</code> instead of 2 :<br/> baselines과의 공평한 비교를 위해 NoPoSplat은 two input-views setting을 사용했는데<br/> three input-views를 사용할 경우 성능이 훨씬 좋아졌음!</li> </ul> </li> </ul> <h2 id="conclusion">Conclusion</h2> <ul> <li> <p>Future Work :<br/> NoPoSplat은 static scene에만 적용했는데, dynamic scene에 NoPoSplat의 pipeline을 확장 적용!</p> </li> <li> <p>Limitation :</p> <ul> <li><code class="language-plaintext highlighter-rouge">camera intrinsic은 known</code>이라는 걸 가정!</li> <li>feed-forward model은 <code class="language-plaintext highlighter-rouge">non-generative</code>하므로 <code class="language-plaintext highlighter-rouge">unseen region</code>에는 대응 못 함</li> <li><code class="language-plaintext highlighter-rouge">static scene</code>에 적용</li> </ul> </li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br/> 사실 NoPoSplat은 camera pose 이용한 global coordinate으로의 explicit transform이나 geometry prior (epopiolar constraint, cost volume 등)나 GT depth 없이<br/> 오로지 implicit network의 학습에 의존하여 scene recon. 능력을 학습하겠다는 건데<br/> photometric loss만으로도 잘 학습이 되나? two input images 경계면의 smoothness 등 추가 regularization loss 추가해주는 게 낫지 않음?</p> </li> <li> <p>A1 :<br/> TBD</p> </li> <li> <p>Q2 :<br/> photometric loss에만 의존하기 때문에 ViT semantic info. 말고도 more info. 주기 위해 intrinsic과 RGB shortcut을 사용하는데<br/> 둘 말고 또 추가하면 좋은 거 있을까?</p> </li> <li> <p>A2 :<br/> TBD</p> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="dynamic"/><category term="GS"/><category term="SfMfree"/><summary type="html"><![CDATA[Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images (ICLR 2025)]]></summary></entry><entry><title type="html">MonST3R</title><link href="https://semyeong-yu.github.io/blog/2025/MonST3R/" rel="alternate" type="text/html" title="MonST3R"/><published>2025-01-22T10:00:00+00:00</published><updated>2025-01-22T10:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/MonST3R</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/MonST3R/"><![CDATA[<h2 id="monst3r---a-simple-approach-for-estimating-geometry-in-the-presence-of-motion">MonST3R - A Simple Approach for Estimating Geometry in the Presence of Motion</h2> <h4 id="junyi-zhang-charles-herrmann-junhwa-hur-varun-jampani-trevor-darrell-forrester-cole-deqing-sun-ming-hsuan-yang">Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, Ming-Hsuan Yang</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2410.03825">https://arxiv.org/abs/2410.03825</a><br/> project website :<br/> <a href="https://monst3r-project.github.io/">https://monst3r-project.github.io/</a></p> </blockquote> <h2 id="contribution">Contribution</h2> <p><code class="language-plaintext highlighter-rouge">static scene에 사용됐던 DUSt3R를 dynamic scene에 확장한 버전!</code></p> <ul> <li>geometry-first approach that <code class="language-plaintext highlighter-rouge">directly</code> estimates <code class="language-plaintext highlighter-rouge">per-timestep geometry (pointmap)</code> of <code class="language-plaintext highlighter-rouge">dynamic</code> scene <ul> <li>이전까지의 논문들은 <d-cite key="GaussianMarbles">[1]</d-cite>, <d-cite key="TrackRecon">[2]</d-cite>, <d-cite key="Kumar">[3]</d-cite>, <d-cite key="Barsan">[4]</d-cite>, <d-cite key="Mustafa">[5]</d-cite>, <d-cite key="Lei">[6]</d-cite>, <d-cite key="Chu">[7]</d-cite>, <d-cite key="Wangb">[8]</d-cite>, <d-cite key="Wanga">[9]</d-cite>, <d-cite key="Liu">[10]</d-cite> 처럼<br/> depth, optical flow, trajectory estimation을 사용하는 subtasks로 쪼갠 뒤<br/> global optimization 또는 multi-stage pipeline 등으로 합치는<br/> complex system을 쓰는데,<br/> 이는 보통 느리고, 다루기 힘들고, prone-to-error at each step</li> <li>이전까지의 논문들은 motion과 geometry를 함께 사용하여 dynamic scene을 다뤘는데,<br/> motion, depth label, camera pose 정보가 있는 GT dynamic video data는 거의 없다<br/> (그래서 다른 model(prior)를 쓰는데, 이는 부정확성이 쌓일 수 있음)</li> <li>대신 본 논문은<br/> limited data로 only DUSt3R의 decoder and head만 fine-tuning하여 (<code class="language-plaintext highlighter-rouge">small-scale fine-tuning</code>)<br/> <code class="language-plaintext highlighter-rouge">explicit motion representation 없이</code><br/> only <code class="language-plaintext highlighter-rouge">geometry</code> (pointmap)를 <code class="language-plaintext highlighter-rouge">directly</code> 예측하는 pipeline 제시!</li> <li>each timestep마다 DUSt3R 방식으로 pointmap (geometry) 예측한 뒤<br/> 같은 camera coordinate frame (global pointmap)에 대해 <code class="language-plaintext highlighter-rouge">3D align</code></li> <li>downstream tasks :<br/> 예측한 pointmap (geometry) 를 바탕으로<br/> feed-forward 4D reconstruction 뿐만 아니라<br/> video depth estimation, camera pose estimation, video segmentation 등<br/> 여러 downstream video-specific tasks에 적용</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Final Loss</code> :<br/> \(\hat X = \text{argmin}_{X, P_{W}, \sigma} L_{align} (X, \sigma, P_{W}) + w_{smooth} L_{smooth} (X) + w_{flow} L_{flow} (X)\) <ul> <li>세 가지 loss :<br/> 3D alignment loss \(L_{align}\), camera trajectory smoothness loss \(L_{smooth}\), flow projection loss \(L_{flow}\)</li> <li>learnable param. :<br/> <code class="language-plaintext highlighter-rouge">global pointmap</code> \(X\), global pointmap으로의 <code class="language-plaintext highlighter-rouge">3D alignment transformation</code> \(P_{W}\), <code class="language-plaintext highlighter-rouge">scale factor</code> \(\sigma\) 를 업데이트하는데,<br/> 얘네는 본질적으로 re-parameterization에 의해 <code class="language-plaintext highlighter-rouge">depthmap</code> \(\hat D\), <code class="language-plaintext highlighter-rouge">extrinsic</code> \(\hat P\), <code class="language-plaintext highlighter-rouge">intrinsic</code> \(\hat K\) 로 구성되어 있음<br/> 즉, MonST3R는 결국 jointly optimize video depthmap and camera pose (extrinsic, intrinsic)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/2.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/2.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="related-works">Related Works</h2> <ul> <li>DUSt3R :<br/> DUSt3R를 바로 dynamic scene에 적용할 경우 두 가지 한계 발생 <ul> <li>문제 1) (static scene인 것처럼) fg object에 align하여 bg가 misaligned<br/> DUSt3R는 static scene으로만 학습됐기 때문에<br/> dynamic scene의 pointmaps를 알맞게 align하지 못하여<br/> moving fg object가 가만히 있는 것처럼 align되고<br/> static bg element는 misaligned</li> <li>문제 2) fg object의 geometry(depth)를 잘 예측하지 못하여 fb object를 bg에 둠</li> <li>해결)<br/> domain mismatch이므로 다시 train!<br/> 본 논문은 limited data를 최대한 사용하여 small-scale fine-tuning하는 training strategy 제시</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/3.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/3.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>motion mask :<br/> DUSt3R는 static scene으로 훈련되었기 때문에 dynamic scene에 적용하기 위해<br/> GT motion mask를 사용할 수도 있다 <ul> <li>inference할 때<br/> image의 dynamic region은 black pixels로 대체하고<br/> corresponding tokens는 mask tokens로 대체하여<br/> dynamic objects를 masking out 할 수도 있는데,<br/> black pixels와 mask tokens는 out-of-distribution w.r.t training 이므로<br/> pose estimation 결과가 안 좋아짐</li> <li>본 논문은 그렇게 무작정 dynamic region을 mask out 하지 않고 이 문제 해결!</li> </ul> </li> </ul> <h2 id="architecture">Architecture</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/1.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/1.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/1.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/1.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="method">Method</h2> <h3 id="main-idea">Main Idea</h3> <p>DUSt3R의 아이디어를 그대로 가져오고,<br/> DUSt3R의 각 output pointmap \(X^{t} \in R^{W \times H \times 3}\) 이 time 정보 \(t\) 를 가지고 있음</p> <h3 id="training-dataset">Training Dataset</h3> <p>real-world dynamic scene은 보통 GT camera pose를 가지고 있지 않으므로<br/> SfM 등 sensor measurement 또는 post-processing을 통해 추정하는데<br/> 이는 부정확할 수 있고 costly하므로<br/> 본 논문은 GT camera pose, depth 정보를 알 수 있는 synthetic datasets를<br/> dynamic fine-tuning을 위한 training dataset으로 사용</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/4.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/4.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Training Dataset for Dynamic Fine-Tuning :<br/> PointOdyssey는 dynamic objects 많아서 많이 사용하고<br/> TartanAir는 static scene이라서 적게 사용하고<br/> Waymo는 specialized domain이라서 적게 사용 <ul> <li>3 synthetic datasets : <ul> <li>PointOdyssey (Zheng et al.)</li> <li>TartanAir (Wang et al.)</li> <li>Spring (Mehl et al.)</li> </ul> </li> <li>1 real-world dataset : <ul> <li>Waymo (Sun et al.) with LiDAR</li> </ul> </li> </ul> </li> </ul> <h3 id="training-strategy">Training Strategy</h3> <p>dataset이 small-scale이므로<br/> data efficiency를 극대화시키기 위해<br/> 다양한 training techniques 사용</p> <ul> <li>Training Strategies : <ul> <li>전략 1)<br/> encoder는 freeze한 뒤<br/> network의 decoder와 prediction head만 fine-tune<br/> (encoder(CroCo)의 geometric knowledge는 유지)</li> <li>전략 2)<br/> each video마다 temporal stride 1~9 만큼 떨어진 two frames를 sampling하여 input pair로 사용하는데,<br/> stride가 클수록 sampling prob.도 linearly 큼<br/> \(\rightarrow\)<br/> 서로 더 멀리 떨어진 frame pair, 즉 large motion에 more weights 부여</li> <li>전략 3)<br/> Field-of-View augmentation (center crop with various image scales) 사용하여<br/> 다양한 camera intrinsics에도 일반화 가능하도록!<br/> (training videos에는 해당 variation이 흔하지 않음)</li> </ul> </li> </ul> <h3 id="dynamic-global-point-clouds-and-camera-pose">Dynamic Global Point Clouds and Camera Pose</h3> <p>frame 수가 많기 때문에<br/> pairwise pointmap 들로부터 직접 하나의 dynamic global point cloud를 추출하는 건 어렵.<br/> 지금부터 pairwise model을 이용해서<br/> <code class="language-plaintext highlighter-rouge">dynamic global pcd</code> \(\hat X\) 와 <code class="language-plaintext highlighter-rouge">camera pose</code> \(\hat K, \hat P = [\hat R | \hat T]\) 를 <code class="language-plaintext highlighter-rouge">동시에</code> optimize하는 방법을 소개하겠다</p> <ul> <li>Video Graph : <ul> <li>DUSt3R의 경우<br/> global alignment를 위해 모든 frame pair에 대해 connectivity graph를 만드는데,<br/> dynamic scene video에 대해 이렇게 graph 만드려면 too expensive</li> <li>계산량 줄이기 위해<br/> 전체 frames에 대해 graph를 만드는 게 아니라<br/> video의 <code class="language-plaintext highlighter-rouge">sliding temporal window</code> 내에 있는 frames에 대해 <code class="language-plaintext highlighter-rouge">국소적인 graph</code> 만듦</li> <li>sliding temporal window 내에 있는 모든 each frame pair<br/> \((t, t') \in W^{t} = {(a, b) | a, b \in [t, \ldots, t + w], a \neq b}\) 에 대해<br/> (\(w\) : temporal window size)<br/> MonST3R로 pairwise pointmap을 구하고,<br/> off-the-shelf method로 optical flow 구함</li> <li>runtime 줄이기 위해 strided sampling 적용</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/5.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/5.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/5.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/5.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Dynamic Global Point Cloud and Pose Optimization : <ul> <li>goal :<br/> 모든 pairwise pointmaps를 <code class="language-plaintext highlighter-rouge">같은 global coordinate frame</code>에 모아서 <code class="language-plaintext highlighter-rouge">world-coordinate pointmap</code> \(X^{t} \in R^{H \times W \times 3}\) 만들기</li> <li>re-parameterization : <ul> <li>notation :<br/> \(P^{t} = [R^{t} | T^{t}]\) : extrinsic camera pose<br/> \(K^{t}\) : intrinsic<br/> \(D^{t}\) : per-frame depthmap</li> <li>global pointmap \(X^{t}\) :<br/> <code class="language-plaintext highlighter-rouge">depthmap, intrinsic, extrinsic</code> 을 이용하여 parameterize <code class="language-plaintext highlighter-rouge">global pointmap</code><br/> \(X_{i,j}^{t} = P^{t^{-1}} h (K^{t^{-1}} [i D_{i,j}^{t} ; j D_{i,j}^{t} ; D_{i,j}^{t}])\) <ul> <li>intrinsic \(K^{t^{-1}}\) :<br/> depthmap 정보를 2D pixel-coordinate \((i, j)\) 에서 3D camera-coordinate으로 변환한 뒤</li> <li>homogeneous mapping \(h(\cdot)\) :<br/> homogeneous-coordinate으로 변환한 뒤<br/> (\(R^{t}, T^{t}\) 를 하나의 행렬로 표현 가능하도록 하여 just 행렬 곱셈을 통해 변환 가능)</li> <li>extrinsic \(P^{t^{-1}}\) :<br/> world-coordinate으로 변환</li> </ul> </li> </ul> </li> <li>loss :<br/> dynamic scene video이기 때문에 DUSt3R의 3D alignment loss 뿐만 아니라 두 가지 video-specific loss 추가 <ul> <li>Loss 1) DUSt3R의 <code class="language-plaintext highlighter-rouge">3D alignment</code> loss : <ul> <li>goal :<br/> <code class="language-plaintext highlighter-rouge">각 pairwise pointmap</code> \(X^{t; t \leftarrow t'}\), \(X^{t'; t \leftarrow t'}\) 을<br/> <code class="language-plaintext highlighter-rouge">world-coordinate</code>의 <code class="language-plaintext highlighter-rouge">global pointmap</code> \(X^{t}\) 에 <code class="language-plaintext highlighter-rouge">align</code>시키는<br/> single rigid transformation \(P^{t;e}\)<br/> (\(X^{t; t \leftarrow t'}\) 와 \(X^{t'; t \leftarrow t'}\) 는 둘 다 <code class="language-plaintext highlighter-rouge">이미 같은 camera-coordinate</code> (\(t\) 의 frame) 에 align되어 있으므로<br/> \(X^{t; t \leftarrow t'}\) 을 global pointmap에 align시키는 \(P\) 와<br/> \(X^{t'; t \leftarrow t'}\) 을 global pointmap에 align시키는 \(P\) 는 같음)</li> <li>how :<br/> \(L_{align}(X, \sigma, P_{W}) = \sum_{W^{i} \in W} \sum_{e \in W} \sum_{t \in e} \| C^{t; e} \cdot (X^{t} - \sigma^{e} P^{t;e} X^{t;e}) \|_{1}\) <ul> <li>notation :<br/> \(W^{i} \in W\) : each sliding temporal window<br/> \(e = (t, t') \in W^{i}\) : each frame pair within the window<br/> \(t \in e\) : each frame<br/> \(\sigma^{e}\) : frame 크기 차이를 보정하는 per-(frame pair) scale factor<br/> \(P_{W}\) : sliding temporal window 내의 여러 frame pair에 대한 3D alignment transformation 집합</li> </ul> </li> </ul> </li> <li>Loss 2) <code class="language-plaintext highlighter-rouge">camera trajectory smoothness</code> loss : <ul> <li>goal :<br/> nearby timestep에 대해 \(R, T\) 가 크게 변하지 않도록 하여<br/> <code class="language-plaintext highlighter-rouge">시간에 따라 camera motion (extrinsic) 이 smooth</code>하도록</li> <li>how :<br/> \(L_{smooth}(X) = \sum_{t=0}^{N} (\| R^{t^{T}} R^{t+1} - I \|_{f} + \| R^{t^{T}} (T^{t+1} - T^{t}) \|_{2})\)</li> </ul> </li> <li>Loss 3) <code class="language-plaintext highlighter-rouge">flow projection</code> loss : <ul> <li>goal :<br/> confident <code class="language-plaintext highlighter-rouge">static region</code>에 대해<br/> global pointmaps \(X^{t}\) 와 camera poses \(K^{t}, R^{t}, T^{T}\), 즉 <code class="language-plaintext highlighter-rouge">camera motion만으로 계산한 optical flow</code>가<br/> <code class="language-plaintext highlighter-rouge">off-the-shelf method가 내놓은 optical flow</code>와 consistent하도록</li> <li>how :<br/> \(L_{flow}(X) = \sum_{W^{i} \in W} \sum_{t \rightarrow t' \in W^{i}} \| S^{global; t \rightarrow t'} \cdot (F_{cam}^{global; t \rightarrow t'} - F_{est}^{t \rightarrow t'}) \|_{1}\) <ul> <li>\(S^{global; t \rightarrow t'}\) : static region에 대해 loss term 걸어줌<br/> (static mask 구하는 방법 : 아래의 Confident Static Regions 섹션에서 설명!)<br/> (\(X^{t}\) 가 learnable 하므로 학습 중에 계속 updated)</li> <li>\(F_{cam}^{global; t \rightarrow t'}\) : global pointmap \(X^{t}\) 에 camera motion (intrinsic, extrinsic)을 적용하여 계산한 optical flow field</li> <li>\(F_{est}^{t \rightarrow t'}\) : off-the-shelf method가 내놓은 optical flow field</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Final Loss</code> :<br/> \(\hat X = \text{argmin}_{X, P_{W}, \sigma} L_{align} (X, \sigma, P_{W}) + w_{smooth} L_{smooth} (X) + w_{flow} L_{flow} (X)\) <ul> <li>learnable param. :<br/> <code class="language-plaintext highlighter-rouge">global pointmap</code> \(X\), global pointmap으로의 <code class="language-plaintext highlighter-rouge">3D alignment transformation</code> \(P_{W}\), <code class="language-plaintext highlighter-rouge">scale factor</code> \(\sigma\) 를 업데이트하는데,<br/> 얘네는 본질적으로 re-parameterization에 의해 <code class="language-plaintext highlighter-rouge">depthmap</code> \(\hat D\), <code class="language-plaintext highlighter-rouge">extrinsic</code> \(\hat P\), <code class="language-plaintext highlighter-rouge">intrinsic</code> \(\hat K\) 로 구성되어 있음<br/> 즉, MonST3R는 결국 jointly optimize video depthmap and camera pose (extrinsic, intrinsic)</li> <li>\(w_{smooth} = 0.01, w_{flow} = 0.01\)<br/> (\(L_{flow} \lt 20\) 일 때, 즉 camera poses를 roughly align한 뒤에 \(L_{flow}\) 활성화함)<br/> (\(L_{flow} \gt 50\) 일 때, 즉 초기에 motion mask is updated)</li> </ul> </li> </ul> <h2 id="downstream-applications">Downstream Applications</h2> <h3 id="intrinsics-and-relative-pose-estimation">Intrinsics and Relative Pose Estimation</h3> <ul> <li> <p>Intrinsic Pose Estimation :<br/> time \(t\) 에서의 pointmap \(X^{t}\) 을 이용해서<br/> 2D image와 3D pointmap이 align되도록 하는<br/> focal length \(f^{t}\) 를 추정함으로써<br/> camera intrinsic \(K^{t}\) 추정</p> </li> <li> <p>Relative Pose Estimation :<br/> DUSt3R와 달리 dynamic objects는<br/> epipolar matrix 또는 Procrustes alignment를 위한 가정(<code class="language-plaintext highlighter-rouge">???</code>)들에 위배<br/> \(\rightarrow\)<br/> 대신 주어진 3D point와 corresponding 2D point를 바탕으로 추정하는 PnP algorithm(<code class="language-plaintext highlighter-rouge">???</code>)과<br/> random sampling 방식의 RANSAC algorithm(<code class="language-plaintext highlighter-rouge">???</code>) 사용<br/> (dynamic scene이어도 대부분의 pixels는 static할 것이므로<br/> randomly-sampled points는 static elements에 더 가중치를 두기 때문에<br/> relative pose는 inliers(static)로 robustly estimate 가능)</p> </li> </ul> <h3 id="confident-static-regions">Confident Static Regions</h3> <ul> <li>Static Mask :<br/> 단순하게 <code class="language-plaintext highlighter-rouge">두 optical flow field가 일치하는 (차이가 적은) 영역</code>을 <code class="language-plaintext highlighter-rouge">Static Region</code>으로 간주!<br/> static mask \(S^{t \rightarrow t'} = [\alpha \gt \| F_{cam}^{t \rightarrow t'} - F_{est}^{t \rightarrow t'} \|_{1}]\)<br/> (이 confident static mask를 나중에 global pose optimization에도 사용할 거임!) <ul> <li>\(F_{cam}^{t \rightarrow t'}\) :<br/> <code class="language-plaintext highlighter-rouge">camera motion</code>만으로 optical flow 추정 <ul> <li>Step 1)<br/> frame pair \(I^{t}\), \(I^{t'}\) 로부터<br/> pointmaps \(X^{t; t \leftarrow t'}\), \(X^{t'; t \leftarrow t'}\) 와 pointmaps \(X^{t; t' \leftarrow t}\), \(X^{t'; t' \leftarrow t}\) 추정</li> <li>Step 2)<br/> 위에서 언급한 방법으로 Intrinsics \(K^{t}, K^{t'}\) 와 Relative Pose \(R^{t \rightarrow t'}, T^{t \rightarrow t'}\) 추정</li> <li>Step 3)<br/> only camera motion \(t \rightarrow t'\) 이용해서<br/> optical flow field \(F_{cam}^{t \rightarrow t'}\) 추정<br/> \(F_{cam}^{t \rightarrow t'} = \pi (D^{t; t \leftarrow t'} K^{t'} R^{t \rightarrow t'} K^{t^{-1}} \hat x + K^{t'} T^{t \rightarrow t'}) - x\) <ul> <li>notation :<br/> \(x\) : pixel-coordinate<br/> \(\hat x\) : homogeneous coordinate<br/> \(\pi(\cdot)\) : projection operation \((x,y,z) \rightarrow (\frac{x}{z}, \frac{y}{z})\)<br/> \(D^{t; t \leftarrow t'}\) : estimated depth from pointmap \(X^{t; t \leftarrow t'}\)</li> <li>Step 3-1)<br/> intrinsic 이용하여 frame \(t\) 에 대해 2D에서 3D로 backproject</li> <li>Step 3-2)<br/> camera motion (relative camera pose) \(t \rightarrow t'\) 적용<br/> (\(R^{t \rightarrow t'} X + T^{t \rightarrow t'}\))</li> <li>Step 3-3)<br/> intrinsic 이용하여 frame \(t'\) 에 대해 다시 3D에서 2D image coordinate으로 project</li> </ul> </li> </ul> </li> <li>\(F_{est}^{t \rightarrow t'}\) :<br/> <code class="language-plaintext highlighter-rouge">off-the-shelf method</code> <d-cite key="SEARAFT">[11]</d-cite> 이용해서 optical flow 추정</li> </ul> </li> </ul> <h3 id="video-depth">Video Depth</h3> <p>optimal global pointmap \(\hat X\) 자체가 re-parameterization에 의해<br/> per-frame depthmap \(\hat D\) 로 이루어져 있고,<br/> just \(\hat D\) 자체가 video depth</p> <h2 id="experiment">Experiment</h2> <h3 id="results">Results</h3> <ul> <li>Single-Frame and Video <code class="language-plaintext highlighter-rouge">Depth Estimation</code> : <ul> <li>baseline : <ul> <li>video depth method :<br/> NVDS <d-cite key="NVDS">[12]</d-cite><br/> ChronoDepth <d-cite key="Chrono">[13]</d-cite><br/> DepthCrafter <d-cite key="DepthCrafter">[14]</d-cite></li> <li>single-frame depth method :<br/> Depth-Anything-V2 <d-cite key="DepthAnything">[15]</d-cite><br/> Marigold <d-cite key="Marigold">[16]</d-cite><br/> DUSt3R <a href="https://semyeong-yu.github.io/blog/2024/DUSt3R/">blog</a></li> <li>joint video depth and pose estimation method :<br/> CasualSAM <d-cite key="CasualSAM">[17]</d-cite><br/> Robust-CVD <d-cite key="RobustCVD">[18]</d-cite></li> </ul> </li> <li>benchmark dataset : <ul> <li>video depth :<br/> KITTI<br/> Sintel<br/> Bonn</li> <li>monocular single-frame depth :<br/> NYU-v2</li> </ul> </li> <li>metric : <d-cite key="DepthCrafter">[14]</d-cite>, <d-cite key="DepthAnything">[15]</d-cite> 에서처럼<br/> Abs Rel : absolute relative error<br/> \(\sigma \lt 1.25\) : percentage of inlier points <code class="language-plaintext highlighter-rouge">???</code><br/> (All methods output scale- and/or shift- invariant depth estimates. For video depth evaluation, we align a single scale and/or shift factor per each sequence, whereas the single-frame evaluation adopts per-frame median scaling, following DUSt3R) <code class="language-plaintext highlighter-rouge">???</code></li> <li>results : <ul> <li>video depth :<br/> MonST3R는 specialized video depth estimation techniques와 유사한 성능</li> <li>single-frame depth :<br/> DUSt3R 구조를 dynamic scene’s video에 대해 fine-tuning했는데도<br/> single-frame depth estimation에 대해 여전히 DUSt3R와 유사한 성능</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/6.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/6.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/6.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/6.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Camera Pose Estimation</code> :<br/> obtain camera trajectories <ul> <li>baseline : <ul> <li>joint video depth and pose estimation method (경쟁자들) :<br/> CasualSAM <d-cite key="CasualSAM">[17]</d-cite><br/> Robust-CVD <d-cite key="RobustCVD">[18]</d-cite></li> <li>learning-based visual odometry method :<br/> DROID-SLAM (GT intrinsic 필요)<br/> Particle-SfM (Ours보다 5배 느림)<br/> DPVO (GT intrinsic 필요)<br/> LEAP-VO (GT intrinsic 필요)</li> <li>DUSt3R with GT motion mask :<br/> 단순히 dynamic region의 pixel과 token을 mask out<br/> (Related Works 섹션의 motion mask에서 설명함)</li> </ul> </li> <li>benchmark dataset :<br/> Sintel<br/> TUM-dynamics<br/> ScanNet</li> <li>metric : Particle-SfM, LEAP-VO에서처럼<br/> Sim(3) Umeyama alignment 적용한 뒤 <code class="language-plaintext highlighter-rouge">???</code><br/> Absolute Translation Error (ATE)<br/> Relative Translation Error (RPE trans)<br/> Relative Rotation Error (RPE rot)</li> <li>results :<br/> joint depth and pose estimation methods 중에 제일 성능 좋고<br/> GT intrinsic 필요 없는데도 pose-specific methods와 유사한 성능</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/7.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/7.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/7.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/7.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/8.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/8.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/8.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/8.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="ablation-study">Ablation Study</h3> <ul> <li>Ablation Study : <ul> <li>training dataset :<br/> datasets 섞어 쓰면 camera pose estimation에 도움!</li> <li>fine-tuning strategy :<br/> only decoder and head만 fine-tuning하는 게 다른 training strategies보다 나음!</li> <li>loss :<br/> 본 논문에서 언급한 세 가지 loss (\(L_{align}, L_{smooth}, L_{flow}\)) 는<br/> video depth accuracy를 크게 해치지 않으면서 pose estimation accuracy를 높임!</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/9.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/9.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/9.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/9.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="limitation">Limitation</h2> <ul> <li>Limitation : <ul> <li>이론적으로는 dynamic camera intrinsics를 estimate할 수 있지만,<br/> 사실상 이는 <code class="language-plaintext highlighter-rouge">careful hyperparameter tuning</code>과 <code class="language-plaintext highlighter-rouge">manual constraints</code>를 필요로 함</li> <li><code class="language-plaintext highlighter-rouge">out-of-distribution inputs</code>에 struggle<br/> e.g. 건물 내부 또는 도심 야외 등으로 훈련한 경우 넓은 공터 같은 새로운 scene에 대해서는 제대로 작동 안함 <ul> <li>해결법 :<br/> training set을 확장하면 MonST3R가 in-the-wild videos에 대해서도 더 robust해질 듯</li> </ul> </li> </ul> </li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br/> 왜 static region에 대해서만 optical flow가 consistent하도록 하는 flow projection loss 적용함?</p> </li> <li> <p>A1 :<br/> ddd</p> </li> <li> <p>Q2 :<br/> Gaussian Marbles에서는 frame끼리 divide-and-conquer로 merge하면서 frame 전후 관계를 trajectory로 연결하여 temporal info.를 이용함.<br/> MonST3R는 각 timestep의 pointmap을 global pointmap으로 align하는데 camera trajectory smoothness loss 말고 3D alignment loss에서 frame 전후 관계, 즉 temporal info.를 이용함?</p> </li> <li> <p>A2 :<br/> TBD</p> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="dynamic"/><category term="GS"/><category term="geometry"/><category term="SfMfree"/><summary type="html"><![CDATA[A Simple Approach for Estimating Geometry in the Presence of Motion (ICLR 2025)]]></summary></entry><entry><title type="html">Dynamic Gaussian Marbles</title><link href="https://semyeong-yu.github.io/blog/2025/GaussianMarbles/" rel="alternate" type="text/html" title="Dynamic Gaussian Marbles"/><published>2025-01-16T12:00:00+00:00</published><updated>2025-01-16T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/GaussianMarbles</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/GaussianMarbles/"><![CDATA[<h2 id="dynamic-gaussian-marbles-for-novel-view-synthesis-of-casual-monocular-videos">Dynamic Gaussian Marbles for Novel View Synthesis of Casual Monocular Videos</h2> <h4 id="colton-stearns-adam-harley-mikaela-uy-florian-dubost-federico-tombari-gordon-wetzstein-leonidas-guibas">Colton Stearns, Adam Harley, Mikaela Uy, Florian Dubost, Federico Tombari, Gordon Wetzstein, Leonidas Guibas</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2406.18717">https://arxiv.org/abs/2406.18717</a><br/> project website :<br/> <a href="https://geometry.stanford.edu/projects/dynamic-gaussian-marbles.github.io/">https://geometry.stanford.edu/projects/dynamic-gaussian-marbles.github.io/</a></p> </blockquote> <h2 id="contribution">Contribution</h2> <ul> <li>Dynamic 4D Gaussian Splatting : <ul> <li>이전까지는 multiple simultaneous viewpoints of a scene 세팅 (dense multi-camera setup)에서의 recon. 논문들이 많았음<br/> \(\rightarrow\)<br/> 평소의 casual <code class="language-plaintext highlighter-rouge">monocular video</code> (challenging)로 4D recon.을 수행해보자!</li> <li>input에 multi-view info.가 없는 underconstrained monocular video더라도<br/> prior (<code class="language-plaintext highlighter-rouge">careful optimization strategy</code> 및 <code class="language-plaintext highlighter-rouge">off-the-shelf depth and motion estimation</code> 및 <code class="language-plaintext highlighter-rouge">geometry-based regularization</code>) 이용해서<br/> 적절한 constraint를 복원할 수 있다!</li> </ul> </li> <li>Dynamic Gaussian Marbles :<br/> monocular setting의 어려움을 해결하기 위해 GS에서 세 가지 사항을 변경<br/> 이를 통해 Gaussian trajectories를 학습할 수 있음 <ul> <li>isotropic Gaussian Marbles :<br/> <code class="language-plaintext highlighter-rouge">isotropic</code> Gaussian을 사용함으로써<br/> Gaussian의 <code class="language-plaintext highlighter-rouge">degrees of freedom을 줄이고</code><br/> <code class="language-plaintext highlighter-rouge">local shape보다는 motion과 apperance</code> 표현하는 데 더 집중하도록 제한</li> <li>hierarchical divide-and-conquer learning strategy :<br/> time 길이가 어느 정도 짧아야 잘 포착할 수 있으므로 <br/> long video를 short <code class="language-plaintext highlighter-rouge">subsequences로 나누고 optimize by iteratively merging the subsequences</code><br/> (long-sequence tracking 대신 인접한 subsequences를 붙이는 task로!) <ul> <li>procedure :<br/> 아래의 과정을 반복하며 locality와 global coherence를 모두 챙김! <ul> <li><code class="language-plaintext highlighter-rouge">motion estimation</code> :<br/> \(G^{b}\) 의 frame을 \(G^{a}\) 의 trajectory에 하나씩 더해 가며 motion estimation을 수행하므로 <d-cite key="Dynamic3DGS">[1]</d-cite> 처럼 <code class="language-plaintext highlighter-rouge">locality</code>와 smoothness로부터 benefit</li> <li><code class="language-plaintext highlighter-rouge">merge</code></li> <li><code class="language-plaintext highlighter-rouge">global adjustment</code> : <d-cite key="4DGS">[2]</d-cite> 처럼 <code class="language-plaintext highlighter-rouge">global coherence</code>라는 benefit</li> </ul> </li> </ul> </li> <li>prior :<br/> monocular video로도 recon. 잘 수행하기 위해 prior 이용 <ul> <li><code class="language-plaintext highlighter-rouge">image(2D)-space prior</code> : SAM (Rendering loss-segmentation), CoTracker (Tracking loss), DepthAnything (Rendering loss-depthmap)</li> <li><code class="language-plaintext highlighter-rouge">geometry(3D)-space prior</code> : regularization of Gaussian trajectories with rigidity (Isometry loss) and Chamfer priors (3D Alignment loss)</li> </ul> </li> </ul> </li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li> <p>Gaussian Splatting :<br/> TBD</p> </li> <li> <p>Dynamic Gaussian Splatting :<br/> TBD</p> </li> <li> <p>Other Dynamic Nerual Scene Representations :<br/> TBD</p> </li> </ul> <h2 id="method">Method</h2> <h3 id="overview">Overview</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/1.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/1.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/1.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/1.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/3.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/3.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="dynamic-gaussian-marbles">Dynamic Gaussian Marbles</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/2.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/2.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> simpler Gaussian Marble의 경우에만 generalize well to novel view </div> <ul> <li>Gaussian marble : <ul> <li><code class="language-plaintext highlighter-rouge">isotropic</code> :<br/> \(R = I\) and \(S = s \in R^{1}\) <ul> <li>anisotropic Gaussian은 expensive할 뿐만 아니라<br/> underconstrained monocular cam. setting에서는 오히려 degrees of freedom 많으면 poor하다는 걸 실험적으로 발견</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">semantic instance</code> :<br/> assign each Gaussian marble to semantic instance \(y \in N\) by SAM-driven TrackAnything</li> <li><code class="language-plaintext highlighter-rouge">dynamic trajectory</code> :<br/> trajectory \(\Delta X \in R^{T \times 3}\) : a sequence of translations which maps marble’s position change at each timestep</li> </ul> </li> </ul> <h3 id="divide-and-conquer-motion-estimation">Divide-and-Conquer Motion Estimation</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/3.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/3.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Training Procedure : <ul> <li>Step 1) <code class="language-plaintext highlighter-rouge">initialization for each frame</code><br/> initialize Gaussian marbles \([G_{11}, G_{22}, \ldots, G_{TT}]\) for each frame<br/> (initial marbles \(G_{ii}\) have trajectory length 1) <ul> <li>Step 1-1) obtain prior (<code class="language-plaintext highlighter-rouge">depthmap</code> and <code class="language-plaintext highlighter-rouge">segmentation</code>)<br/> obtain monocular (LiDAR) depthmap and segmentation from SAM-driven TrackAnything <d-cite key="TrackAnything">[3]</d-cite></li> <li>Step 1-2) <code class="language-plaintext highlighter-rouge">unproject</code> from 2D to 3D<br/> unproject the depthmap into point cloud<br/> perform outlier removal and downsampling</li> <li>Step 1-3) initialize Gaussian marbles and trajectory <ul> <li>Gaussian marbles : <ul> <li>mean \(\mu \in R^{3}\) : Step 1-2)에서 얻은 pcd</li> <li>color \(c \in R^{3}\) : pixel color (pixel-aligned Gaussians)</li> <li><code class="language-plaintext highlighter-rouge">instance class</code> \(y \in R^{1}\) : Step 1-1)에서 얻은 segmentation</li> <li>scale \(s \in R^{1}\) and opacity \(\alpha \in R^{1}\) : 3DGS 논문에서 했던대로 초기화</li> </ul> </li> <li>trajectory : <ul> <li><code class="language-plaintext highlighter-rouge">trajectory</code> : \(\Delta X = [\boldsymbol 0] \in R^{T \times 3}\)</li> </ul> </li> </ul> </li> </ul> </li> <li>Step 2) bottom-up divide-and-conquer merge<br/> merge short-trajectories into longer trajectories<br/> e.g. \(G = [G_{12}, G_{34}, G_{56}, G_{78}] \rightarrow G = [G_{14}, G{58}]\) <ul> <li>Step 2-1) <code class="language-plaintext highlighter-rouge">motion estimation</code> <ul> <li>Step 2-1-1) make a pair b.w. adjacent marbles <ul> <li>adjacent Gaussian marble set끼리 a pair로 묶음<br/> e.g. \([(G_{12}^{a}, G_{34}^{b}), (G_{56}^{a}, G_{78}^{b})]\)</li> <li>\(G^{a}\) 는 merge할 prev. frames’ Gaussians이고,<br/> \(G^{b}\) 는 merge할 next frames’ Gaussians</li> </ul> </li> <li>Step 2-1-2) \(G^{a}\) 의 trajectory 확장 <ul> <li>goal :<br/> \(G_{12}^{a}\) 의 trajectory인 \(\Delta X = [\Delta X_{1}, \Delta X_{2}]\) 는 이미 학습되어 merge된 motion이고,<br/> \(G_{12}^{a}\) 의 trajectory와 \(G_{34}^{b}\) 의 frame \(3\) 을 잇는 motion \(\Delta X_{3}\) 을 학습해야 함!</li> <li>constant-velocity assumption에 따라 trajectory를 \(\Delta X = [\Delta X_{1}, \Delta X_{2}, \Delta X_{3}^{init}]\) 로 확장</li> </ul> </li> <li>Step 2-1-3) trajectory optimization <ul> <li>\(G_{12}^{a}\) 를 frame \(3\) 에 render한 뒤<br/> \(\Delta X_{3}\) 이 frame \(3\) 으로의 motion을 잘 반영하도록 \(\Delta X_{3}\) 을 업데이트<br/> (\(\eta\) 번 반복 by 아래에서 설명할 Loss)</li> </ul> </li> <li>Step 2-1-4) repeat <ul> <li>Step 2-1-2), Step 2-1-3)을 반복<br/> until \(G^{a}\) 의 trajectory가 \(G_{34}^{b}\) 내 모든 frames를 커버할 때까지</li> <li>e.g. \(G^{a}\) 의 trajectory를 \(\Delta X = [\Delta X_{1}, \Delta X_{2}, \Delta X_{3}, \Delta X_{4}^{init}]\) 로 확장한 뒤<br/> \(G^{a}\) 를 frame \(4\) 에 render한 뒤<br/> \(\Delta X_{4}\) 가 frame \(4\) 으로의 motion을 잘 반영하도록 \(\Delta X_{4}\) 을 업데이트</li> </ul> </li> </ul> </li> <li>Step 2-2) <code class="language-plaintext highlighter-rouge">merge</code> <ul> <li>motion estimation을 거치고 나면 \(G_{ij}^{a}\) 와 \(G_{ij}^{b}\) 가 같은 frame subsequence \([i, j]\) 를 recon.할 것이므로<br/> merge by just union \(G_{ij} = G_{ij}^{a} \cup G_{ij}^{b}\)<br/> (set size 2배 됨)</li> <li>computational load 줄이기 위해<br/> opacity 또는 scale이 너무 작은 Gaussians는 drop하고,<br/> random downsampling 수행하여<br/> set size를 constant하게 유지</li> </ul> </li> <li>Step 2-3) <code class="language-plaintext highlighter-rouge">global adjustment</code> <ul> <li>merge로 합치고 나서도 still optimized라는 보장이 없기 때문에<br/> newly merged Gaussians를 모두 jointly optimize</li> <li>merged set가 \(G_{ij}\) 라고 했을 때<br/> \([i, j]\) 내 a frame을 randomly sampling하고<br/> \(G_{ij}\) 의 모든 Gaussians를 해당 frame에 render한 뒤<br/> Gaussian \(c, s, \alpha, \Delta X\) 을 업데이트<br/> (\(\beta\) 번 반복)</li> <li>그럼 merged Gaussians \(G_{ij}\) 가 global Gaussians로 인정받을 수 있음!</li> </ul> </li> </ul> </li> </ul> </li> <li>Inference Procedure : <ul> <li>learned Gaussian trajectories 이용해서<br/> render (roll out) into specific timestep \(t\)</li> </ul> </li> </ul> <h3 id="loss">Loss</h3> <p>Motion Estimation 단계에서 아래의 Loss들 사용!</p> <ul> <li><code class="language-plaintext highlighter-rouge">Tracking</code> Loss :<br/> \(L_{track} = \sum_{p \in P} \sum_{g \in N(p_{i})} \alpha_{i}^{'} \| D_{i} \| \mu_{i}^{'} - p_{i} \| - D_{j} \| \mu_{j}^{'} - p_{j} \| \|\)<br/> where \(\mu_{i}^{'}\) and \(D_{i}\) : mean and depth of projected 2D Gaussian<br/> where \(P\) : tracked points by CoTracker <d-cite key="CoTracker">[4]</d-cite><br/> where \(N(p_{i})\) : tracked point \(p_{i}\) 와의 the nearest 3D Gaussians<br/> where \(\alpha_{i}^{'}\) : Gaussian’s opacity <ul> <li>goal :<br/> <code class="language-plaintext highlighter-rouge">2D point track</code>인 CoTracker <d-cite key="CoTracker">[4]</d-cite> (2D prior)를 사용하여<br/> <code class="language-plaintext highlighter-rouge">Gassian marble trajectories</code>를 regularize</li> <li>Step 1)<br/> \(G^{b}\) 의 frame \(j\) 로의 \(\Delta X_{j}\) 를 optimize하고자 할 때,<br/> CoTracker <d-cite key="CoTracker">[4]</d-cite> 를 이용하여 frames \([j - w , j + w]\) (\(w = 12\))에서의 point tracks \(P\) 를 estimate<br/> (from 2D frame to 2D frame)<br/> (일종의 GT로 사용)</li> <li>Step 2)<br/> a source frame \(i \in [j - w , j + w]\) 을 randomly sampling</li> <li>Step 3)<br/> Gaussian marble trajectory \(\Delta X\) 로부터 frame \(i\) 와 frame \(j\) 에서의 3DGS position을 sampling하고<br/> 3DGS를 2D Gaussian in image plane으로 project시켜 2D mean, depth, covariance 구함</li> <li>Step 4)<br/> Step 1)의 tracked point \(p_{i \rightarrow j}\) 와 가장 가까운 \(K = 32\) 개의 Step 3)의 2D Gaussians를 구한 뒤<br/> <code class="language-plaintext highlighter-rouge">2D tracked point와 2D Gaussian 사이의 거리</code>가 frame \(i\), \(j\) 에서 거의 <code class="language-plaintext highlighter-rouge">일정하게 유지되도록</code> loss term 걸어줌<br/> (for <code class="language-plaintext highlighter-rouge">temporal consistency</code>)</li> </ul> </li> <li>Rendering Loss : <ul> <li><code class="language-plaintext highlighter-rouge">image</code> rendering하여<br/> GT image와의 L1 loss 및 LPIPS loss 구함</li> <li><code class="language-plaintext highlighter-rouge">disparity map</code> rendering하여<br/> initial disparity estimation과의 L1 loss 구함</li> <li><code class="language-plaintext highlighter-rouge">segmentation map</code> rendering하여<br/> SAM(off-the-shelf instance segmentation)과의 L1 loss 구함</li> </ul> </li> <li>Geometry Loss : <ul> <li><code class="language-plaintext highlighter-rouge">Local Isometry</code> Loss :<br/> \(L_{iso-local} = \sum_{g^{a} \in G} \sum_{g^{b} \in N(g^{a})} | \| \mu_{i}^{a} - \mu_{i}^{b} \| - \| \mu_{j}^{a} - \mu_{j}^{b} \| |\) <ul> <li>goal :<br/> prev. works <d-cite key="Dynamic3DGS">[2]</d-cite>, <d-cite key="DynamicPointFields">[5]</d-cite> 에서처럼<br/> Gaussian marbles가 <code class="language-plaintext highlighter-rouge">locally rigid motion</code>을 따르도록 regularize</li> <li>Step 1)<br/> \(G^{b}\) 의 frame \(j\) 로의 \(\Delta X_{j}\) 를 optimize하고자 할 때,<br/> 우선 a source timestep \(i \in [j - 1 , j + 1]\) 을 randomly sampling</li> <li>Step 2)<br/> 3DGS \(g^{a} \in G\) 및 이와 가까운 3DGS들 \(g^{b} \in N(g^{a})\) 에 대해<br/> <code class="language-plaintext highlighter-rouge">nearest 3D Gaussians끼리의 거리</code>가 frame \(i\), \(j\) 에서 거의 <code class="language-plaintext highlighter-rouge">일정하게 유지되도록</code> loss term 걸어줌</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Instance Isometry</code> Loss :<br/> \(L_{iso-instance} = \sum_{g^{a} \in G} \sum_{g^{b} \in Y(g^{a})} | \| \mu_{i}^{a} - \mu_{i}^{b} \| - \| \mu_{j}^{a} - \mu_{j}^{b} \| |\) <ul> <li>goal :<br/> 각 semantic instance가 일관적으로 움직이도록 regularize</li> <li>Step 1)<br/> \(G^{b}\) 의 frame \(j\) 로의 \(\Delta X_{j}\) 를 optimize하고자 할 때,<br/> 우선 a source timestep \(i \in [j - 1 , j + 1]\) 을 randomly sampling</li> <li>Step 2)<br/> 3DGS \(g^{a} \in G\) 및 이와 semantic label이 같은 3DGS들 \(g^{b} \in Y(g^{a})\) 에 대해<br/> <code class="language-plaintext highlighter-rouge">semantic label이 같은 3D Gaussians끼리의 거리</code>가 frame \(i\), \(j\) 에서 거의 <code class="language-plaintext highlighter-rouge">일정하게 유지되도록</code> loss term 걸어줌</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">3D Alignment</code> Loss :<br/> \(L_{chamfer} = \sum_{g^{1} \in G^{1}} \text{min}_{g^{2} \in G^{2}} \| \mu^{1} - \mu^{2} \| + \sum_{g^{2} \in G^{2}} \text{min}_{g^{1} \in G^{1}} \| \mu^{1} - \mu^{2} \|\) <ul> <li>goal :<br/> merge하고나서 <code class="language-plaintext highlighter-rouge">Global Adjustment</code>할 때 a frame에 전부 rendering해서 optimize하므로 <code class="language-plaintext highlighter-rouge">projected 2D image plane 상에서는 align</code> 되어 있음<br/> 그런데 merge하고나서 <code class="language-plaintext highlighter-rouge">3D space 상에서도 align</code>할 필요 있음<br/> (3DGS를 align한다는 게 무슨 의미지? 모든 3DGS가 함께 scene recon.에 기여하도록 서로 가깝게 만든다는 건가 <code class="language-plaintext highlighter-rouge">???</code>)<br/> (만약에 3D alignment 하지 않으면 3D and novel-view 상에서 <code class="language-plaintext highlighter-rouge">cloudy artifacts</code> 생김)<br/> (off-the-shelf depth estimation이 time에 따라 inconsistent할 경우 이와 같은 상황 발생)</li> <li>Step 1)<br/> 두 pcd 집합을 서로 가깝게 만드는 Chamfer loss를 적용할 건데,<br/> merge할 Gaussian set \(G^{a}\) 와 \(G^{b}\) 는 scene의 명확히 서로 다른 부분을 관측하고 있으므로<br/> 둘 사이에 Chamfer loss를 바로 적용하면 안 됨</li> <li>Step 2)<br/> set \(G_{12}^{a}\) 와 \(G_{34}^{b}\) 를 single frame의 subsets \([G_{1}^{a}, G_{2}^{a}, G_{3}^{b}, G_{4}^{b}]\) 로 나눔<br/> where \(G_{1}^{a}\) contains Gaussians initialized from frame \(1\)</li> <li>Step 3)<br/> 해당 subsets list를 random shuffle한 뒤<br/> 맨 앞의 25%는 set \(G^{1}\) 으로 묶고, 다음 25%는 set \(G^{2}\) 로 묶음<br/> (\(G^{1}\) 과 \(G^{2}\) 가 <code class="language-plaintext highlighter-rouge">scene의 어떤 부분을 보고 있는지 명확히 정해지지 않도록 randomness 부여</code>)<br/> (만약 이렇게 randomness 부여하지 않는다면 observed scene content difference에 overfitting될 수 있음 <code class="language-plaintext highlighter-rouge">???</code>)</li> <li>Step 4)<br/> \(G^{1}\) 과 \(G^{2}\) 에 대해 2-way Chamfer distance 계산</li> </ul> </li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <h3 id="dataset">Dataset</h3> <ul> <li>training :<br/> 아래의 두 가지 datasets는 multi-view info.를 포함하고 있으므로<br/> monocular setting을 모방하기 위해<br/> training and evaluation protocol을 수정 <ul> <li>NVIDIA Dynamic Scenes Dataset : <ul> <li>구성 :<br/> 7 videos<br/> 12 calibrated cameras</li> <li>setting :<br/> prev. benchmarked evaluations는 각 timestep마다 different training camera를 사용하는데,<br/> (monocular teleporting camera 방식 <d-cite key="monocular">[6]</d-cite>)<br/> 이는 realistic monocular video setting이 아니므로<br/> 본 논문에서는 single camera 4를 training에 사용하고 single camera 3, 5, 6을 evaluation에 사용</li> </ul> </li> <li>DyCheck iPhone Dataset : <ul> <li>구성 :<br/> 7 videos</li> <li>setting :<br/> single camera로 구성되어 있는 monocular video setting이긴 하지만<br/> multi-view info.를 포함하도록 3D trajectory가 scene 전체를 돌기 때문에<br/> camera의 calculated motion은 일상 video와 다르다 <ul> <li>방법 1) official benchmark 그대로 사용</li> <li>방법 2) camera pose 제거<br/> We remove camera poses, offloading the camera motion into the learned 4D scene representation’s dynamics. We find this setting interesting because it simulates additional dynamic content, where previously “static” regions of the scene now have rigid dynamics equal to the inverse camera motion, which must be solved by the scene representation itself. <code class="language-plaintext highlighter-rouge">???</code></li> </ul> </li> </ul> </li> </ul> </li> <li>test : <ul> <li>Total-Recon Dataset :<br/> 2 time-synchronized and calibrated videos with LiDAR</li> <li>Davis Dataset</li> <li>YouTube-VOS Dataset</li> <li>real-world videos</li> </ul> </li> </ul> <h3 id="implementation">Implementation</h3> <ul> <li>Implementation : <ul> <li>NVIDIA Dynamic Scenes Dataset : <ul> <li>120,000 Gaussians per frame and upsample to 240,000 Gaussians during the last stage of global adjustment</li> <li>\(\eta = 128\) on motion estimation and \(\beta = 48\) on global adjustment</li> <li>stop divide-and-conquer after learning subsequences of length 32 on both fg and bg</li> </ul> </li> <li>DyCheck iPhone Dataset : <ul> <li>220,000 Gaussians per frame if camear pose exists else 180,000 Gaussians</li> <li>\(\eta = 80\) on motion estimation and \(\beta = 32\) on global adjustment</li> <li>stop divide-and-conquer after learning subsequences of length 8 on fg and length 32(512) on bg<br/> (when there exists camera pose, need to learn more a dynamic bg)</li> </ul> </li> <li>Total-Recon Dataset : <ul> <li>120,000 Gaussians per frame</li> <li>$\eta = 80\(on motion estimation and\)\beta = 32$$ on global adjustment</li> <li>stop divide-and-conquer after learning subsequences of length 8 on fg and length 32 on bg</li> </ul> </li> </ul> </li> </ul> <h3 id="results">Results</h3> <ul> <li>Dynamic Novel View Synthesis : <ul> <li>TBD</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/4.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/4.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/8.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/8.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/8.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/8.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/6.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/6.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/6.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/6.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Tracking and Editing : <ul> <li>TBD</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/5.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/5.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/5.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/5.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="ablation-study">Ablation Study</h3> <ul> <li>motion estimation : <ul> <li>frame 간의 motion 정보를 학습</li> <li>locality and smoothness 보장</li> </ul> </li> <li>global adjustment : <ul> <li>merge하고나서 global Gaussian이 specific frame을 잘 render하도록</li> <li>global coherence 보장</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/7.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/7.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/7.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/7.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="conclusion">Conclusion</h2> <ul> <li>Limitation :<br/> extremely challenging open-world dynamic and monocular novel-view-synthesis는 잘 못 함 <ul> <li><code class="language-plaintext highlighter-rouge">2D image prior에 의존</code>하기 때문에<br/> SAM (segmentation), CoTracker (tracking), DepthAnything (depth estimation) 가 부정확할 경우<br/> 결과 안 좋음</li> <li><code class="language-plaintext highlighter-rouge">3D geometric prior에도 의존</code>하는데,<br/> <code class="language-plaintext highlighter-rouge">rapid and non-rigid motion</code>을 포함한 scene의 경우<br/> 잘 대응 못 함</li> </ul> </li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br/> Gaussian의 motion을 학습하는 것이라면 없던 object가 등장하거나 원래 있던 object가 frame 밖으로 벗어나는 경우에도 잘 대응할 수 있는지?<br/> CoTracker 등 여러 prior들도 위의 상황에 잘 대응하는지?</p> </li> <li> <p>A1 :<br/> TBD</p> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="Gaussian"/><category term="Marble"/><category term="degree"/><category term="freedom"/><category term="dynamic"/><category term="view"/><category term="synthesis"/><summary type="html"><![CDATA[Dynamic Gaussian Marbles for Novel View Synthesis of Casual Monocular Videos (SIGGRAPH 2024)]]></summary></entry></feed>