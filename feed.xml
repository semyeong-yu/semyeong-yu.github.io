<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://semyeong-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://semyeong-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-02T07:47:38+00:00</updated><id>https://semyeong-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Difix3D+</title><link href="https://semyeong-yu.github.io/blog/2025/Difix3D/" rel="alternate" type="text/html" title="Difix3D+"/><published>2025-03-15T12:00:00+00:00</published><updated>2025-03-15T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/Difix3D</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/Difix3D/"><![CDATA[<h2 id="difix3d---improving-3d-reconstructions-with-single-step-diffusion-models-cvpr-2025">Difix3D+ - Improving 3D Reconstructions with Single-Step Diffusion Models (CVPR 2025)</h2> <h4 id="jay-zhangjie-wu-yuxuan-zhang-haithem-turki-xuanchi-ren-jun-gao-mike-zheng-shou-sanja-fidler-zan-gojcic-huan-ling">Jay Zhangjie Wu, Yuxuan Zhang, Haithem Turki, Xuanchi Ren, Jun Gao, Mike Zheng Shou, Sanja Fidler, Zan Gojcic, Huan Ling</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2503.01774">https://arxiv.org/abs/2503.01774</a><br/> project website :<br/> <a href="https://research.nvidia.com/labs/toronto-ai/difix3d/">https://research.nvidia.com/labs/toronto-ai/difix3d/</a></p> </blockquote> <blockquote> <p>핵심 :<br/> nearly real-time single-step 2D diffusion model을<br/> 3D artifacts removing task에 맞게 fine-tune한 뒤,<br/> 3D model에 distill하여 progressively update하거나<br/> post-processing으로 씀!</p> </blockquote> <h2 id="contribution">Contribution</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-15-Difix3D/1.PNG-480.webp 480w,/assets/img/2025-03-15-Difix3D/1.PNG-800.webp 800w,/assets/img/2025-03-15-Difix3D/1.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-15-Difix3D/1.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Difix3D+ 설명 : <ul> <li>Stage 1)<br/> sparse view로 학습된 못난 3D model로 rendering한 novel-view의 <code class="language-plaintext highlighter-rouge">artifacts를 제거</code>하도록<br/> <code class="language-plaintext highlighter-rouge">single-step 2D diffusion</code> model (Difix)을 <code class="language-plaintext highlighter-rouge">minimal fine-tuning</code></li> <li>Stage 2)<br/> fine-tuned Difix를 이용하여<br/> 3D model로 distill하거나 post-processing <ul> <li>Difix 역할 1) <code class="language-plaintext highlighter-rouge">reconstruction at training phase</code> :<br/> fine-tuned Difix를 적용하여 clean pseudo-training views (training set)을 iteratively <code class="language-plaintext highlighter-rouge">augment</code>함으로써<br/> fine-tuned Difix의 prior를 3D model로 <code class="language-plaintext highlighter-rouge">distill</code></li> <li>Difix 역할 2) <code class="language-plaintext highlighter-rouge">neural enhancer at inference phase</code> :<br/> single-step 2D diffusion model은 inference speed 빠르므로<br/> 남은 residual artifacts를 제거하기 위해 improved recon. output에 직접 Difix 적용<br/> (<code class="language-plaintext highlighter-rouge">near real-time post-processing</code>)</li> </ul> </li> </ul> </li> <li>Difix3D+ Contribution : <ul> <li><code class="language-plaintext highlighter-rouge">single-step diffusion model</code> :<br/> ADD (Adversarial Diffusion Distillation)으로 학습된 single-step diffusion model은<br/> single-step만 U-Net을 query하므로 inference speed 빠름 <ul> <li>ADD (Adversarial Diffusion Distillation) <a href="https://ostin.tistory.com/305">Link</a> <a href="https://velog.io/@sckim0430/Adversarial-Diffusion-Distillation">Link</a> :<br/> SDS loss + GAN loss</li> <li>DMD (Distribution Matching Distillation) <a href="https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/dmd/">Link</a></li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">minimal fine-tuning</code> :<br/> 그래서 fine-tuning하는 데 single GPU로 only a few hours만 필요</li> <li><code class="language-plaintext highlighter-rouge">general</code> model :<br/> 모든 3D models (NeRF, 3DGS 등)에 사용 가능한 single model</li> <li>metrics : <ul> <li>artifacts 제거(fix)하므로 3D consistency 유지한 채 FID score 2배 이상 및 PSNR 1dB 이상 향상</li> <li>single-step diffusion model을 사용하므로 multi-step standard diffusion model보다 10배 이상 빠름</li> </ul> </li> </ul> </li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li>현재 NVS methods의 한계 :<br/> per-scene optimization framework <ul> <li>input이 <code class="language-plaintext highlighter-rouge">sparse</code>하거나<br/> input camera poses로부터 먼 <code class="language-plaintext highlighter-rouge">extreme novel view</code>를 rendering하거나<br/> <code class="language-plaintext highlighter-rouge">varying lighting</code> conditions 또는 <code class="language-plaintext highlighter-rouge">imperfect camera poses</code> 상황에서<br/> spurious(가짜) geometry 또는 missing regions 등 artifacts 생김</li> <li>underlying geometry를 제대로 반영하지 못한 채 (incorrect shape)<br/> inherent smoothness에만 의존하는 3D representation으로도<br/> training images’ radiance에 overfitting되는<br/> <code class="language-plaintext highlighter-rouge">shape-radiance ambiguity</code> 문제</li> </ul> </li> <li>large 2D generative model : <ul> <li>2D diffusion model prior :<br/> large internet-scale data를 학습하여 real-world images의 distribution을 잘 이해하고 있으므로 diffusion priors는 여러 분야에 generalize 가능 <ul> <li>inpainting <d-cite key="11">[1]</d-cite> <d-cite key="64">[2]</d-cite> <d-cite key="85">[3]</d-cite></li> <li>outpainting <d-cite key="5">[4]</d-cite> <d-cite key="62">[5]</d-cite> <d-cite key="76">[6]</d-cite></li> </ul> </li> <li>2D diffusion model prior to 3D : <ul> <li><code class="language-plaintext highlighter-rouge">multi-step으로 U-Net을 query</code> <d-cite key="25">[7]</d-cite> <d-cite key="41">[8]</d-cite> <d-cite key="72">[9]</d-cite> <d-cite key="89">[10]</d-cite> :<br/> object-centric scenes를 optimize하고, more expansive camera trajectories를 가진 larger env.로 scale하는 데 사용<br/> But,, <code class="language-plaintext highlighter-rouge">time-consuming</code>!</li> <li><code class="language-plaintext highlighter-rouge">single-step만 U-Net을 query</code> <d-cite key="difix">[11]</d-cite> <d-cite key="22">[12]</d-cite> <d-cite key="32">[13]</d-cite> <d-cite key="49">[14]</d-cite> <d-cite key="77">[15]</d-cite> <d-cite key="78">[16]</d-cite> :<br/> inference speed 빠르고,<br/> minimal fine-tuning만으로도 extreme novel-view에서도 NeRF/3DGS rendering의 artifacts를 “fix”할 수 있음!</li> </ul> </li> </ul> </li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li>3D recon. 개선 :<br/> imperfect noisy input data에 대응하기 위해 <ul> <li><code class="language-plaintext highlighter-rouge">optimize camera poses</code> <d-cite key="6">[17]</d-cite> <d-cite key="21">[18]</d-cite> <d-cite key="35">[19]</d-cite> <d-cite key="39">[20]</d-cite> <d-cite key="59">[21]</d-cite> <d-cite key="69">[22]</d-cite></li> <li><code class="language-plaintext highlighter-rouge">lighting variations</code> 고려 <d-cite key="34">[23]</d-cite> <d-cite key="60">[24]</d-cite> <d-cite key="73">[25]</d-cite></li> <li><code class="language-plaintext highlighter-rouge">transient occlusions</code> 완화 <d-cite key="48">[26]</d-cite></li> <li>위 방법들의 한계 :<br/> 완전히 artifacts를 해결하진 못함</li> </ul> </li> <li>Priors for NVS :<br/> under-observed (잘 보지 못한) 영역들을 잘 recon.하지 못하는 문제를 해결하기 위해 <ul> <li><code class="language-plaintext highlighter-rouge">Geometric priors</code> :<br/> noise에 민감하고, dense input일 때만 미미한 개선 <ul> <li>by <code class="language-plaintext highlighter-rouge">regularization term</code> <d-cite key="38">[27]</d-cite> <d-cite key="55">[28]</d-cite> <d-cite key="75">[29]</d-cite></li> <li>by <code class="language-plaintext highlighter-rouge">pre-trained models</code> which provide <code class="language-plaintext highlighter-rouge">depth GT</code> <d-cite key="7">[30]</d-cite> <d-cite key="45">[31]</d-cite> <d-cite key="63">[32]</d-cite> <d-cite key="90">[33]</d-cite> and <code class="language-plaintext highlighter-rouge">normal GT</code> <d-cite key="82">[34]</d-cite></li> </ul> </li> <li>여러 scenes’ data로 <code class="language-plaintext highlighter-rouge">feed-forward neural network</code> 훈련 :<br/> 이웃한 reference views의 정보를 aggregate하여 reference views 근처에서는 잘 수행하지만, <d-cite key="88">[35]</d-cite> <d-cite key="4">[36]</d-cite> <d-cite key="31">[37]</d-cite> <d-cite key="44">[38]</d-cite> <d-cite key="79">[39]</d-cite><br/> rendering 분포가 inherently multi-mode를 가지는 ambiguous regions에서는 잘 못 함</li> </ul> </li> <li>Generative Priors for NVS : <ul> <li>GAN :<br/> NeRF 개선하기 위해 per-scene GAN 훈련 <d-cite key="46">[40]</d-cite></li> <li>Diffusion : <ul> <li>diffusion model이 <code class="language-plaintext highlighter-rouge">직접 novel view를 generate</code> by minimal fine-tuning <d-cite key="8">[41]</d-cite> <d-cite key="13">[42]</d-cite> <d-cite key="81">[43]</d-cite> <d-cite key="83">[44]</d-cite> : <ul> <li>단점 : 3D model을 사용하지 않으므로 generative nature leads to <code class="language-plaintext highlighter-rouge">multi-view geometric inconsistency</code> across different frames/poses, especially in under-observed and noisy regions</li> </ul> </li> <li>diffusion model이 <code class="language-plaintext highlighter-rouge">3D model의 optimization을 guide</code> <d-cite key="12">[45]</d-cite> <d-cite key="25">[46]</d-cite> <d-cite key="70">[47]</d-cite> <d-cite key="72">[48]</d-cite> <d-cite key="89">[49]</d-cite> <ul> <li>단점 : <code class="language-plaintext highlighter-rouge">multi-step</code>으로 U-Net을 query (denoise)해야 해서 훈련 많이 <code class="language-plaintext highlighter-rouge">느림</code></li> </ul> </li> <li>diffusion model로 training image set을 augment하여 3D model을 fine-tuning <d-cite key="difix">[11]</d-cite> <d-cite key="27">[50]</d-cite> <d-cite key="28">[51]</d-cite> :<br/> (위의 두 방법을 합친 느낌?!) <ul> <li>본 논문 : 어설픈 3D model의 output에 대해 2D diffusion model (U-Net)을 먼저 fine-tuning한 뒤<br/> diffusion model로 training image set을 augment하여 diffusion prior를 3D model로 distill (fine-tuning)하거나<br/> diffusion model로 post-processing</li> </ul> </li> </ul> </li> </ul> </li> </ul> <h2 id="overall-pipeline">Overall Pipeline</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-15-Difix3D/2.PNG-480.webp 480w,/assets/img/2025-03-15-Difix3D/2.PNG-800.webp 800w,/assets/img/2025-03-15-Difix3D/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-15-Difix3D/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Architecture : <ul> <li>Stage 1)<br/> single-step 2D diffusion model (Difix)을 minimal fine-tuning</li> <li>Stage 2) <ul> <li>Step 1)<br/> Difix로 clean pseudo-training views (training set) 얻음 (augment!)<br/> 이 때, novel camera pose는 reference view부터 target view까지의 경로를 따라 pose interpolation으로 얻음</li> <li>Step 2)<br/> cleaned novel view를 이용하여 diffusion prior를 3D model로 distill <ul> <li>Step 1, 2를 반복하여 progressively update 3D representation<br/> (recon.하는 공간 크기를 키우고 diffusion model의 strong conditioning을 보장하기 위해)</li> </ul> </li> <li>Step 3)<br/> final updated 3D representation으로 rendering한 output을<br/> Difix로 real-time post-processing</li> </ul> </li> </ul> </li> </ul> <h2 id="difix---from-a-pretrained-diffusion-model-to-a-3d-artifact-fixer">Difix - from a Pretrained Diffusion Model to a 3D Artifact Fixer</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-15-Difix3D/3.PNG-480.webp 480w,/assets/img/2025-03-15-Difix3D/3.PNG-800.webp 800w,/assets/img/2025-03-15-Difix3D/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-15-Difix3D/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>based on <code class="language-plaintext highlighter-rouge">SD-Turbo</code> <d-cite key="49">[14]</d-cite> :<br/> SD-Turbo는 <code class="language-plaintext highlighter-rouge">single-step diffusion</code> model이라서 image-to-image translation task <d-cite key="40">[52]</d-cite> 를 effectively 수행 가능</li> </ul> <h3 id="fine-tuning">Fine Tuning</h3> <p>위의 Overall Pipeline에서 Stage 1)에 해당되는 내용!</p> <ul> <li>Fine Tuning :<br/> single-step 2D diffusion model인 SD-Turbo <d-cite key="49">[14]</d-cite> 가 3D Artifact Fixer 역할을 할 수 있도록<br/> Pix2pix-Turbo <d-cite key="40">[52]</d-cite> 와 유사한 방식으로 fine-tune <ul> <li>I/O : <ul> <li>input : clean reference view \(I_{ref}\) and degraded rendered target view \(\tilde I\)</li> <li>output : clean reference view \(I_{ref}\) and clean target view \(\hat I\)<br/> (training 및 inference에는 clean reference view \(I_{ref}\) 만 사용)</li> </ul> </li> <li>architecture :<br/> frozen VAE encoder - U-Net - reference mixing layer (self-attention layer) - LoRA fine-tuned decoder <ul> <li>frozen VAE encoder :<br/> reference view \(I_{ref}\) 와 degraded target novel view \(\tilde I\) 를 latent space로 encode한 뒤 concat<br/> as \(\epsilon (\tilde I, I_{ref}) = \boldsymbol z \in R^{(B \ast V) \times H \times W \times C}\)<br/> where \(V\) : the number of views (reference views and target views)<br/> where \(C\) : the number of latent channels</li> <li>U-Net</li> <li>reference-view conditioning by reference mixing layer (self-attention layer) :<br/> \(\boldsymbol z \in R^{B \times (V \ast H \ast W) \times C}\) 로 reshape한 뒤<br/> \(V \ast H \ast W\) dimension (dim=1)에 대해 <code class="language-plaintext highlighter-rouge">self-attention layer</code>를 적용하여<br/> <code class="language-plaintext highlighter-rouge">reference view 간의 cross-view consistency</code>를 포착<br/> (특히 rendered target novel view의 퀄리티가 좋지 않을 때<br/> <code class="language-plaintext highlighter-rouge">함께 input으로 넣어주는 clean reference view로부터</code> objects, color, texture 등 key info.를 잘 포착할 수 있음!)</li> <li>LoRA fine-tuned decoder</li> </ul> </li> <li>디테일 :<br/> lower noise level (e.g. \(\tau = 200\) instead of \(\tau = 1000\)) 부여<br/> s.t. diffusion model generates less-hallucinated results (덜 상상하며 생성)</li> <li>아이디어 :<br/> a specific noise level \(\tau\) 에서<br/> <code class="language-plaintext highlighter-rouge">3D model이 rendering한, artifacts를 가진 images의 분포</code>는<br/> <code class="language-plaintext highlighter-rouge">원래 diffusion model을 train하는 데 사용한, Gaussian noise를 가진 images의 분포</code>와 <code class="language-plaintext highlighter-rouge">유사할 것</code>이다!</li> <li>loss :<br/> \(L = L_{Recon} + L_{LPIPS} + 0.5 L_{Gram}\) <ul> <li>\(L_{Recon}\) : L2 loss</li> <li>\(L_{LPIPS}\) : perceptual loss (VGG-16 features끼리 비교)</li> <li>\(L_{Gram} = \frac{1}{L} \sum_{l=1}^{L} \beta_{l} \| G_{l}(\hat I) - G_{l}(I) \|_{2}\) :<br/> style loss for sharper detail (VGG-16 features의 Gram matrix끼리 비교)<br/> where \(G_{l}(I) = \phi_{l}(I)^{T} \phi_{l}(I)\)<br/> where \(\hat I\) (rendered noisy image) - \(I\) (GT clean image) pair는 아래에서 설명할 Data Curation 방법으로 생성</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-15-Difix3D/4.PNG-480.webp 480w,/assets/img/2025-03-15-Difix3D/4.PNG-800.webp 800w,/assets/img/2025-03-15-Difix3D/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-15-Difix3D/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> noise level이 높으면 model은 artifacts를 잘 제거하지만 image context도 함께 바꿈,, noise level이 낮으면 model은 image를 거의 안 건드림 </div> <h3 id="data-curation">Data Curation</h3> <ul> <li>Data Curation :<br/> single-step 2D diffusion model SD-Turbo 를 fine-tuning하기 위해<br/> loss \(L = L_{Recon} + L_{LPIPS} + 0.5 L_{Gram}\) 를 사용하려면<br/> novel-view synthesis artifacts를 가진 image \(\tilde I\) 와<br/> 이에 대응되는 깨끗해진 GT image \(I\) 를<br/> pair로 가진 large dataset를 구축해야 함 <ul> <li>방법 1) sparse reconstruction strategy :<br/> every \(n\)-th frame을 3D model training에 사용한 뒤<br/> 나머지 frames를 GT로 삼고, 해당 pose에 대해 novel-view-synthesis 수행하여 \(I\) - \(\tilde I\) pair 구축 <ul> <li>DL3DV dataset 처럼 camera trajectory가 있어서 novel views를 띄엄띄엄 sampling한 경우에는 잘 적용됨</li> <li>MipNeRF360 또는 LLFF dataset 처럼 training에 사용할 every \(n\)-th frame이 거의 같은 영역을 보고 있는 경우에는 최적의 방법이 아님<br/> \(\rightarrow\) 그럼 training sample 수를 최대한 늘리려면 어떻게 할까?<br/> \(\rightarrow\) 아래의 방법들 사용</li> </ul> </li> <li>방법 2) cycle reconstruction :<br/> Internal RDS (real driving scene) dataset 처럼 거의 linear trajectory인 경우<br/> original trajectory를 \(T_{o}\)라 하고, 이를 horizontally 1-6 m 옮긴 trajectory를 \(T_{s}\) 라 하면<br/> NeRF-1을 \(I\) on \(T_{o}\) 에 대해 학습한 뒤 \(T_{s}\) 에 대해 NeRF-1을 rendering한 뒤<br/> these rendered views on \(T_{s}\) 에 대해 NeRF-2를 학습한 뒤 \(T_{o}\) 에 대해 NeRF-2를 rendering한 걸 \(\tilde I\) 로 사용</li> <li>방법 3) model underfitting :<br/> artifacts 더 많은 \(\tilde I\) 를 generate하기 위해<br/> 기존 training schedule epoch 수의 25%-75% 정도로만 훈련시켜서<br/> render \(\tilde I\) with underfitted recon.</li> <li>방법 4) cross reference :<br/> multi-camera dataset의 경우에는<br/> 그 중 하나의 camera로만 3D model을 학습시킨 뒤<br/> 나머지 camera view \(I\) 에 해당되는 pose에 대해 render \(\tilde I\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-15-Difix3D/5.PNG-480.webp 480w,/assets/img/2025-03-15-Difix3D/5.PNG-800.webp 800w,/assets/img/2025-03-15-Difix3D/5.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-15-Difix3D/5.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="difix3d---nvs-with-diffusion-priors">Difix3D+ - NVS with Diffusion Priors</h2> <h3 id="difix3d---progressive-3d-update">Difix3D - Progressive 3D Update</h3> <p>위의 Overall Pipeline에서 Stage 2)의 Step 1), 2)에 해당되는 내용!</p> <ul> <li>Progressive 3D Update and Reference-View Conditioning : <ul> <li>desired novel-view trajectory가 input views로부터 너무 멀다면<br/> diffusion model의 reference-view conditioning signal이 약해서<br/> diffusion model은 더 상상하며(hallucinate) degraded rendered novel view를 깨끗하게 만듦</li> <li>Instruct-NeRF2NeRF와 비슷하게 iterative training scheme을 사용하여<br/> the set of 3D cues를 progressively 늘려가면서<br/> diffusion model의 reference-view conditioning 을 증가시키면<br/> diffusion model은 self-attention layer에서 clean reference view로부터 key info.를 많이 얻을 수 있음</li> </ul> </li> <li>Strategy : <ul> <li>처음에 sparse reference views만으로 optimize 3D model</li> <li>1.5k iter.마다 GT novel camera pose를 조금씩 perturb 하여<br/> (by reference view부터 target view까지의 경로를 따라 pose interpolation)<br/> 3D model로 novel view를 rendering한 뒤<br/> fine-tuned 2D diffusion model로 refine <ul> <li>the refined clean novel-view images를 다음 1.5k iter.에서 training set에 더함</li> <li>sparse reference views 뿐만 아니라 the refined clean novel views까지 training set으로 사용하므로 3D model의 consistency와 quality가 좋아짐!<br/> (3D model로 distill)</li> </ul> </li> <li>위의 과정을 반복하면서 reference views와 target views 사이의 3D cues’ overlap을 progressively 증가시켜서<br/> target view에서의 artifact-free rendering을 보장할 수 있게 됨!</li> </ul> </li> </ul> <h3 id="difix3d---real-time-post-render-processing">Difix3D+ - Real time Post Render Processing</h3> <p>위의 Overall Pipeline에서 Stage 2)의 Step 3)에 해당되는 내용!</p> <ul> <li>diffusion prior를 3D model로 distill하더라도<br/> 3D recon. model의 limited capacity로 인해<br/> under-observed regions에서는 sharp detail을 살리지 못함<br/> \(\rightarrow\)<br/> fine-tuned diffusion model (Difix)을 적용하여 final post-processing at render time 함으로써<br/> consistency를 유지하면서 novel-view를 enhance <ul> <li>fine-tuned diffusion model (Difix)는 single-step diffusion model이므로<br/> 이로 인한 부가적인 rendering time은 only 76 ms on NVIDIA A100 GPU</li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <h3 id="in-the-wild-artifact-removal">In-the-wild Artifact Removal</h3> <ul> <li>In-the-wild Artifact Removal :<br/> DL3DV dataset and Nerfbusters dataset 사용 <ul> <li>Difix Training : <ul> <li>DL3DV dataset의 경우 scenes의 80% (112 scenes)를 randomly select</li> <li>Data Curation Strategy로 80,000개의 noisy-clean image pair 만들어서 Difix 훈련</li> </ul> </li> <li>Evaluation : <ul> <li>DL3DV dataset의 경우 나머지 20% (28 scenes) 이용<br/> reference view와 target view 간에 상당한 차이가 있도록<br/> camera position에 따라 reference view와 target view를 분류</li> <li>Nerfbusters dataset의 경우 12 captures 이용<br/> Nerfbusters <d-cite key="70">[47]</d-cite> 의 recommended protocol을 따라<br/> reference view와 target view를 분류</li> </ul> </li> <li>Baseline : <ul> <li>Nerfbusters <d-cite key="70">[47]</d-cite> :<br/> NeRF의 artifacts를 제거하기 위해 3D diffusion model 사용</li> <li>GANeRF <d-cite key="46">[40]</d-cite> :<br/> NeRF의 artifacts 제거하기 위해 per-scene GAN 훈련</li> <li>NeRFLiX <d-cite key="88">[35]</d-cite> :<br/> feed-forward network를 사용하여 근처 reference views의 정보를 aggregate하여 퀄리티 향상</li> <li>3DGS-based methods :<br/> gsplat library <a href="https://github.com/nerfstudio-project/gsplat">Link</a> 사용</li> </ul> </li> <li>Metric : PSNR, SSIM, LPIPS, FID</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-15-Difix3D/6.PNG-480.webp 480w,/assets/img/2025-03-15-Difix3D/6.PNG-800.webp 800w,/assets/img/2025-03-15-Difix3D/6.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-15-Difix3D/6.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-15-Difix3D/7.PNG-480.webp 480w,/assets/img/2025-03-15-Difix3D/7.PNG-800.webp 800w,/assets/img/2025-03-15-Difix3D/7.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-15-Difix3D/7.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-15-Difix3D/8.PNG-480.webp 480w,/assets/img/2025-03-15-Difix3D/8.PNG-800.webp 800w,/assets/img/2025-03-15-Difix3D/8.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-15-Difix3D/8.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="automotive-scene-enhancement">Automotive Scene Enhancement</h3> <ul> <li>Automotive Scene Enhancement :<br/> RDS (real driving scene) dataset 사용<br/> (multi-view dataset이라서 서로 40도의 overlaps를 가지고 있는 세 개의 cameras 있음) <ul> <li>Difix Training : <ul> <li>세 개의 cameras 중 center camera를 reference 및 target view로 사용하고,<br/> 40 scenes 사용하여 훈련</li> <li>Data Curation Strategy로 100,000개의 noisy-clean image pair 만들어서 Difix 훈련</li> </ul> </li> <li>Evaluation : <ul> <li>나머지 두 개의 cameras를 novel view로 사용하고,<br/> 20 scenes 사용하여 평가</li> </ul> </li> </ul> </li> </ul> <h3 id="ablation-study">Ablation Study</h3> <ul> <li>Ablation Study on Pipeline :<br/> 하나씩 요소를 추가해가며 진행! <ul> <li>(a) <code class="language-plaintext highlighter-rouge">3D model update 없이</code><br/> rendered views에 그냥 직접 Difix 적용 <ul> <li>reference views에서 먼 less observed regions에서는 별로고, consistency 유지하지 못해서 flickering 발생</li> </ul> </li> <li>(b) <code class="language-plaintext highlighter-rouge">non-incremental (not progressive) 3D model update</code><br/> (pseudo-views를 한 번에 모두 training set에 추가하여 3D model update(distill))</li> <li>(c) <code class="language-plaintext highlighter-rouge">progressive 3D model update</code></li> <li>(d) <code class="language-plaintext highlighter-rouge">post-rendering</code>으로도 Difix 적용</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-15-Difix3D/9.PNG-480.webp 480w,/assets/img/2025-03-15-Difix3D/9.PNG-800.webp 800w,/assets/img/2025-03-15-Difix3D/9.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-15-Difix3D/9.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-15-Difix3D/10.PNG-480.webp 480w,/assets/img/2025-03-15-Difix3D/10.PNG-800.webp 800w,/assets/img/2025-03-15-Difix3D/10.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-15-Difix3D/10.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-15-Difix3D/11.PNG-480.webp 480w,/assets/img/2025-03-15-Difix3D/11.PNG-800.webp 800w,/assets/img/2025-03-15-Difix3D/11.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-15-Difix3D/11.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Ablation Study on Difix Training : <ul> <li><code class="language-plaintext highlighter-rouge">low noise value</code> \(\tau = 200\) 을 사용한 Difix3D+<br/> versus high noise value \(\tau = 1000\) 을 사용한 Pix2pix-Turbo <d-cite key="40">[52]</d-cite> <ul> <li>high noise value 를 사용할 경우<br/> diffusion model이 more hallucinated (더 상상하며) GT와 다른 결과를 generate하기 때문에<br/> poorer generalization on the test dataset</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">reference view conditioning (self-attention layer)</code>의 유무</li> <li><code class="language-plaintext highlighter-rouge">Gram style loss</code>의 유무</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-03-15-Difix3D/12.PNG-480.webp 480w,/assets/img/2025-03-15-Difix3D/12.PNG-800.webp 800w,/assets/img/2025-03-15-Difix3D/12.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-03-15-Difix3D/12.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="conclusion">Conclusion</h2> <ul> <li>Limitation : <ul> <li>inherently limited by initial 3D model의 성능</li> </ul> </li> <li>Future Work : <ul> <li>initial 3D model의 성능에 의존하는 문제를<br/> modern diffusion priors 사용하여 극복</li> <li>Difix는 single-step 2D diffusion model을 fine-tuning한 model인데,<br/> single-step video diffusion model로 확장하여<br/> long-context 3D consistency까지도 향상</li> </ul> </li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br/> single-step 2D diffusion model인 Difix는<br/> novel-view rendering이나 camera pose selection에 관여하지 않고<br/> 그냥 더러운 image를 깨끗하게 만드는 역할일 뿐인건가요?</p> </li> <li>A1 :<br/> 맞는 말씀인데, 깨끗하게 만듦으로써 distillation으로 3D model param.를 업데이트하는 역할도 있습니다.<br/> 다시 한 번 설명해보도록 하겠습니다.<br/> Difix는 더러운 image를 깨끗하게 만들 수 있도록 fine-tuning 되었습니다.<br/> 근데, <code class="language-plaintext highlighter-rouge">더러운 image를 깨끗하게 만듦으로써 두 가지 역할</code>을 수행합니다. <ul> <li>첫 번째는 3D model이 render한 더러운 image를 깨끗하게 만들어서 이를 다시 training image로 사용하는 과정에서 diffusion prior를 3D model로 <code class="language-plaintext highlighter-rouge">distill하여 3D model을 update</code>하는 데 사용할 수 있습니다.</li> <li>두 번째는 final updated 3D model이 render한 image를 마지막으로 좀 더 깨끗하게 만드는 <code class="language-plaintext highlighter-rouge">post-processing</code>에 사용합니다.</li> </ul> </li> <li> <p>Q2 :<br/> 한 번에 3D model을 update하는 non-incremental 방법에 비해<br/> progressively 3D model을 update하는 방법의 성능이 더 좋은 이유는 무엇이라고 생각하시나요?</p> </li> <li>A2 :<br/> 핵심은<br/> reference view부터 target view까지의 경로를 따라 <code class="language-plaintext highlighter-rouge">novel camera pose를 조금씩 perturb</code>한다는 것과,<br/> Difix의 <code class="language-plaintext highlighter-rouge">self-attention layer</code>에 있다고 생각합니다. <ul> <li>progressive update의 경우에는<br/> 먼저 reference view와 가까이 있는 novel view에서 더러운 image를 render한 뒤 이를 Difix에 넣어서<br/> self-attention layer에 의해 가까운 clean reference view의 도움으로 더러운 novel view를 쉽게 깨끗하게 만들 수 있습니다.<br/> 그리고 깨끗해진 novel view를 다시 training set (reference view)에 넣고, novel view를 reference view부터 target view까지 조금씩만 이동시키므로<br/> 마찬가지로 그 다음 novel view에 대해서도 nearby clean reference view의 도움으로 계속해서 novel view를 쉽게 깨끗하게 만들 수 있습니다.</li> <li>non-incremental update의 경우에는<br/> reference view부터 target view까지의 경로에 있는 임의의 모든 novel views에 대해 더러운 image를 한꺼번에 render한 뒤 이를 한꺼번에 Difix에 넣어서 한꺼번에 깨끗하게 만들어야 하는데,<br/> 만약 <code class="language-plaintext highlighter-rouge">reference view로부터 멀리 있는 novel view의 경우</code><br/> 둘의 3D content 차이가 커서 self-attention layer가 둘의 관계를 잘 포착할 수 없고 clean reference view의 도움으로 novel view를 깨끗하게 만들기가 쉽지 않습니다.</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="postprocessing"/><category term="single"/><category term="step"/><category term="diffusion"/><summary type="html"><![CDATA[Improving 3D Reconstructions with Single-Step Diffusion Models (CVPR 2025)]]></summary></entry><entry><title type="html">Radiant Foam</title><link href="https://semyeong-yu.github.io/blog/2025/radfoam/" rel="alternate" type="text/html" title="Radiant Foam"/><published>2025-02-26T12:00:00+00:00</published><updated>2025-02-26T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/radfoam</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/radfoam/"><![CDATA[<h2 id="radiant-foam---real-time-differentiable-ray-tracing">Radiant Foam - Real-Time Differentiable Ray Tracing</h2> <h4 id="shrisudhan-govindarajan-daniel-rebain-kwang-moo-yi-andrea-tagliasacchi">Shrisudhan Govindarajan, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2502.01157">https://arxiv.org/abs/2502.01157</a><br/> project website :<br/> <a href="https://radfoam.github.io/">https://radfoam.github.io/</a><br/> code :<br/> <a href="https://github.com/theialab/radfoam">https://github.com/theialab/radfoam</a><br/> reference :<br/> Presentation of https://charlieppark.kr from 3D-Nerd Community</p> </blockquote> <h2 id="contribution">Contribution</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-26-radfoam/2.PNG-480.webp 480w,/assets/img/2025-02-26-radfoam/2.PNG-800.webp 800w,/assets/img/2025-02-26-radfoam/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-26-radfoam/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Ray Tracing</code> with <code class="language-plaintext highlighter-rouge">Voronoi Diagram</code> : <ul> <li>Voronoi Diagram : <ul> <li><code class="language-plaintext highlighter-rouge">foam model</code> 중 하나</li> <li>3D <code class="language-plaintext highlighter-rouge">공간 자체를 partitioning</code>하는 3D scene representation</li> <li>edge-flip 하는 순간에 두 외접원이 겹쳤다가 continuous하게 변하므로 <code class="language-plaintext highlighter-rouge">gradient-based optimization</code> 적용 가능!</li> </ul> </li> <li>Ray Tracing : <ul> <li>장점 : <ul> <li>ray 단위로 처리하므로 굴절, 반사 등 <code class="language-plaintext highlighter-rouge">빛 효과 반영</code> 가능</li> <li><code class="language-plaintext highlighter-rouge">non-pinhole</code> 실제 camera 지원 가능<br/> (image-centric rasterization 기법과 달리 object-centric ray-tracing 기법이라서)</li> </ul> </li> <li>단점 및 극복? : <ul> <li>rasterization에 비해서는 <code class="language-plaintext highlighter-rouge">속도가 느림</code><br/> 현재 많은 3D 논문들은 volume rendering을 기반으로 하여, 특히 3DGS tile-based rasterization은 parallelism 덕분에 빠른 recon. 가능<br/> 하지만 ray-tracing의 경우 mesh-based representation이 regular하지 않아서 recon.에 불리했고, rasterization에 비해 느렸음.<br/> 하지만 본 논문에서는 voronoi representation에 많은 정보를 저장함으로써 FPS를 높였는데, 이를 계기로 앞으로 더 발전할지도?</li> </ul> </li> </ul> </li> <li>Question <a href="https://semyeong-yu.github.io/blog/2025/radfoam/#question">Link</a> 참고!!</li> </ul> </li> </ul> <h2 id="background">Background</h2> <ul> <li>Ray Tracing : <ul> <li>How : <ul> <li>Step 1)<br/> camera로부터 ray extend</li> <li>Step 2)<br/> ray가 scene을 가로지르며 transmit, reflect, sub-surface scattered, etc.</li> <li>Step 3)<br/> ray가 light source에 도달하면 illumination equation으로부터 pixel 값 얻음</li> </ul> </li> <li>장점 : <ul> <li>rasterizer : primitive 단위로 처리하기 때문에 그림자 같은 빛 효과 반영 어렵</li> <li>ray tracer : ray 단위로 처리하는데, ray는 intersected info.를 모두 가지고 있으므로 그림자나 global illumination effects 반영 쉬움</li> </ul> </li> </ul> </li> <li>Ray Tracing with Bounding Volume Hierarchy (BVH) : <ul> <li>단점 : overlapping Gaussian 때문에 속도 느림 (비효율)</li> </ul> </li> <li>3D Representation : <ul> <li>3DGS는 rasterization을 위해, 2D projection이 가능하도록 3D covariance를 가진 learnable 3D pcd이다</li> <li>3DGS에 ray-tracing을 적용하기 위해, learnable 3D pcd를 이용한 또 다른 3D scene representation은 없을까?<br/> \(\rightarrow\)<br/> ray-tracing을 적용하기 위해, 공간 자체를 partitioning하는 voronoi diagram을 사용하자!!</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-26-radfoam/1.PNG-480.webp 480w,/assets/img/2025-02-26-radfoam/1.PNG-800.webp 800w,/assets/img/2025-02-26-radfoam/1.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-26-radfoam/1.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="method">Method</h2> <h3 id="delaunay-triangulation">Delaunay Triangulation</h3> <p>triangle mesh를 표현하는 데 있어서<br/> point D가 triangle ABC의 외접원의 내부에 있으면 implausible<br/> \(\rightarrow\)<br/> edge-flip으로 해결!</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-26-radfoam/3.PNG-480.webp 480w,/assets/img/2025-02-26-radfoam/3.PNG-800.webp 800w,/assets/img/2025-02-26-radfoam/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-26-radfoam/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> reference: https://charlieppark.kr </div> <ul> <li>Delaunay Triangulation : <ul> <li>문제 :<br/> triangle mesh의 vertex location을 continuously 바꿔도<br/> edge-flip 일어나면 triangle mesh의 connectivity (edge)는 <code class="language-plaintext highlighter-rouge">discrete</code>하게 변해서 gradient-based optimization 적용 불가능</li> </ul> </li> </ul> <h3 id="voronoi-diagram">Voronoi Diagram</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-26-radfoam/4.PNG-480.webp 480w,/assets/img/2025-02-26-radfoam/4.PNG-800.webp 800w,/assets/img/2025-02-26-radfoam/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-26-radfoam/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Voronoi Diagram : <ul> <li>Delaunay Triangulation의 dual graph</li> <li>How : <ul> <li>Step 1)<br/> Delaunay Triangulation의 each triangle mesh의 외접원을 그림</li> <li>Step 2)<br/> triangle mesh의 외접원의 중심들을 잇기</li> </ul> </li> <li>장점 : <ul> <li>triangle mesh의 learanble vertex location (아래 그림의 빨간 점)을 continuously 바꾸다가<br/> 두 외접원이 만나서 edge-flip 일어나면<br/> triangle mesh의 edge (아래 그림의 초록색 삼각형)는 discrete하게 변하지만,<br/> 두 외접원 (아래 그림의 파란 원)은 겹쳤다가 continuously 변하므로 voronoi diagram (아래 그림의 검은색 벌집모양)은 <code class="language-plaintext highlighter-rouge">continuously</code> 변함<br/> 그래서 <code class="language-plaintext highlighter-rouge">gradient-based optimization</code> 적용 가능!</li> <li>외접원을 3DGS처럼 간주하여 <code class="language-plaintext highlighter-rouge">3D 공간을 partitioning</code></li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-26-radfoam/5.PNG-480.webp 480w,/assets/img/2025-02-26-radfoam/5.PNG-800.webp 800w,/assets/img/2025-02-26-radfoam/5.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-26-radfoam/5.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="ray-tracing-algorithm">Ray Tracing Algorithm</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-26-radfoam/6.PNG-480.webp 480w,/assets/img/2025-02-26-radfoam/6.PNG-800.webp 800w,/assets/img/2025-02-26-radfoam/6.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-26-radfoam/6.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-26-radfoam/7.PNG-480.webp 480w,/assets/img/2025-02-26-radfoam/7.PNG-800.webp 800w,/assets/img/2025-02-26-radfoam/7.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-26-radfoam/7.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Ray Tracing Foams : <ul> <li>How : <ul> <li>a ray와 Voronoi Diagram’s cell을 이루는 면의 교점을 sampling point로 사용</li> <li>a ray가 Voronoi Diagram의 a cell에 진입했을 때<br/> ray direction과 normal vector가 90도 미만의 각도를 가지는 면 (위 그림에서 green) 중 가장 먼저 만나는 면에 exit intersection이 존재하고<br/> ray direction과 normal vector가 90도 이상의 각도를 가지는 back-facing 면 (위 그림에서 blue)는 무시!<br/> exit intersection이 존재하는 면에 인접한 cell에 a ray가 진입하여 위의 과정을 똑같이 반복</li> </ul> </li> <li>Ray Tracing은 reflection 등 빛 효과인데<br/> 위의 알고리즘에는 surface reflection에 대한 설명이 없어서<br/> surface reflection과 volume rendering을 어떻게 섞어서 구현했는지는 코드로 확인해야 할 듯!<br/> TBD <code class="language-plaintext highlighter-rouge">???</code></li> </ul> </li> </ul> <h3 id="loss">Loss</h3> <ul> <li>Loss :<br/> \(L = L_{rgb} + \lambda L_{quantile}\) <ul> <li>L2 photometric loss</li> <li>quatile loss<br/> \(L_{quantile} = E_{t_{1}, t_{2} \sim U[0, 1]} | W^{-1}(t_{1}) - W^{-1}(t_{2}) |\)<br/> where \(W^{-1}(\cdot)\) : quantile function (inverse CDF) of the volume rendering weight distribution along the ray <ul> <li>distortion loss of MipNeRF360 <a href="https://semyeong-yu.github.io/blog/2024/MipNeRF360/#regularization-for-interval-based-models">Blog</a> 와 비슷한데,<br/> expensive quadratic nested sum 항을 제거</li> <li>실제로 object가 있는 곳에 Gaussian’s weight on a ray 가 높도록 하여 floater artifacts 제거하는 regularization</li> </ul> </li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-26-radfoam/8.PNG-480.webp 480w,/assets/img/2025-02-26-radfoam/8.PNG-800.webp 800w,/assets/img/2025-02-26-radfoam/8.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-26-radfoam/8.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-26-radfoam/9.PNG-480.webp 480w,/assets/img/2025-02-26-radfoam/9.PNG-800.webp 800w,/assets/img/2025-02-26-radfoam/9.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-26-radfoam/9.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-26-radfoam/10.PNG-480.webp 480w,/assets/img/2025-02-26-radfoam/10.PNG-800.webp 800w,/assets/img/2025-02-26-radfoam/10.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-26-radfoam/10.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="conclusion">Conclusion</h2> <ul> <li>Voronoi-based 3D representation : <ul> <li>Limitation : <ul> <li><code class="language-plaintext highlighter-rouge">high memory consumption</code> : <ul> <li>Ours는 이웃한 점들 사이의 cell boundaries가 equidistant해야 한다는 가정 필요</li> <li>그래서 surface를 정의하기 위해서는 많고 작은 empty cells (high VRAM) 필요</li> <li>심지어 Voronoi param. size에 비해 foam model의 3D 공간이 너무 넓다</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">need SfM</code> : <ul> <li>need SfM to initialize voronoi diagram</li> </ul> </li> </ul> </li> <li>Future Work : <ul> <li>Voronoi diagram을 넘어 representation을 generalize함으로써 위의 가정 완화</li> <li>여러 foam models를 compose together efficiently</li> <li>illumination이 계속 변하는 경우에 대응</li> <li>static scenes 말고 dynamic content에 대응</li> <li>scene editing via Voronoi representation</li> <li>generative model을 결합하여 unseen scene에 대응</li> <li>without SfM</li> <li>현재 real-time ray tracing은 대부분 triangle mesh로 수행되어 왔는데<br/> 위의 Future Work를 통해 foam model-based ray tracing도 발전 가능!</li> </ul> </li> </ul> </li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br/> voronoi diagram은 여러 3D representation 중 하나인 거고, 사실 꼭 voronoi diagram일 필요는 없는 거잖아요? <code class="language-plaintext highlighter-rouge">triangle mesh 대신 voronoi diagram</code>을 썼을 때의 장단점이 있을까요?</p> </li> <li>A1 : <ul> <li>장점 : <ul> <li>공간을 삼각형 대신 다각형(cell)로 분할하므로 <code class="language-plaintext highlighter-rouge">더 복잡한 기하구조</code>(움푹 패인 부분, 부드러운 곡면 등)를 표현할 수 있습니다.</li> <li>기존 triangle mesh보다 저장해야 하는 정보(VRAM)가 많지만, 실시간 ray intersection 연산이 더 적기 때문에 <code class="language-plaintext highlighter-rouge">더 빠르게 rendering</code>할 수 있습니다. (trade-off b.w. VRAM and FPS)</li> <li>각 cell로부터 SDF(수학적인 함수)를 정의하면 vertex 및 edge로 표현되는 explicit mesh 대신 <code class="language-plaintext highlighter-rouge">수학 식(SDF)으로 표현되는 implicit surface</code>를 만들 수 있고 implicit surface ray marching 방식으로 rendering하여 부드러운 곡면을 표현할 수 있습니다.</li> </ul> </li> <li>단점 :<br/> 기존 ray-tracing rendering pipeline은 triangle mesh 기반으로 짜여져 있어서 새로운 acceleration 및 intersection algorithm 필요</li> </ul> </li> <li> <p>Q2 :<br/> ray-tracing model 중에서 FPS가 제일 빠른 건 사실 <code class="language-plaintext highlighter-rouge">memory에 voronoi foam representation 방식으로 3D scene info.를 미리 잘 사전 저장</code>해놓았기 때문에 실시간으로 ray-tracing했을 때 <code class="language-plaintext highlighter-rouge">실시간 ray intersection 연산이 더 적을 수 있어서 빠르게 rendering</code>할 수 있는 거라고 생각합니다. 그래서 <code class="language-plaintext highlighter-rouge">VRAM memory와 FPS의 trade-off</code>가 있는 거 같은데 이 점에 대해 어떻게 생각하시나요?</p> </li> <li>A2 : <ul> <li>rasterization 대신 ray-tracing 쓰려는 이유 :<br/> <code class="language-plaintext highlighter-rouge">memory</code>와 <code class="language-plaintext highlighter-rouge">FPS</code>를 포기하고 <code class="language-plaintext highlighter-rouge">illumination 효과</code> 반영 가능</li> <li>ray-tracing에서 implicit MLP 대신 explicit voronoi diagram 쓰려는 이유 :<br/> <code class="language-plaintext highlighter-rouge">memory</code>를 포기하고 <code class="language-plaintext highlighter-rouge">FPS</code> 높임 <ul> <li>memory 줄이는 방법? (by ChatGPT) : <ul> <li>hierarchical LOD-based voronoi compression <code class="language-plaintext highlighter-rouge">???</code></li> <li>필요한 부분만 GPU에 load하여 update하는 on-demand streaming 방식 <code class="language-plaintext highlighter-rouge">???</code></li> </ul> </li> </ul> </li> <li>ray-tracing에서 triangle mesh 대신 voronoid diagram 쓰려는 이유 :<br/> for continuous change of planes <code class="language-plaintext highlighter-rouge">???</code><br/> (But, triangle-mesh-based ray-tracing rendering pipeline is already well-established..)</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="3DGS"/><category term="ray"/><category term="tracing"/><category term="voronoi"/><category term="diagram"/><summary type="html"><![CDATA[Real-Time Differentiable Ray Tracing]]></summary></entry><entry><title type="html">Spacetime Gaussian</title><link href="https://semyeong-yu.github.io/blog/2025/STGS/" rel="alternate" type="text/html" title="Spacetime Gaussian"/><published>2025-02-08T10:00:00+00:00</published><updated>2025-02-08T10:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/STGS</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/STGS/"><![CDATA[<h2 id="spacetime-gaussian-feature-splatting-for-real-time-dynamic-view-synthesis-cvpr-2024">Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis (CVPR 2024)</h2> <h4 id="zhan-li-zhang-chen-zhong-li-yi-xu">Zhan Li, Zhang Chen, Zhong Li, Yi Xu</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2312.16812">https://arxiv.org/abs/2312.16812</a><br/> project website :<br/> <a href="https://oppo-us-research.github.io/SpacetimeGaussians-website/">https://oppo-us-research.github.io/SpacetimeGaussians-website/</a><br/> code :<br/> <a href="https://github.com/oppo-us-research/SpacetimeGaussians">https://github.com/oppo-us-research/SpacetimeGaussians</a><br/> blog reference :<br/> <a href="https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/spacetime-gaussian/">https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/spacetime-gaussian/</a></p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-08-STGS/1.PNG-480.webp 480w,/assets/img/2025-02-08-STGS/1.PNG-800.webp 800w,/assets/img/2025-02-08-STGS/1.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-08-STGS/1.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="contribution">Contribution</h2> <ul> <li>Spacetime Gaussian (STG) :<br/> 3DGS를 dynamic 4D scene으로 확장하기 위해<br/> <code class="language-plaintext highlighter-rouge">time-dependent opacity, motion trajectory(mean), rotation</code> 사용 <ul> <li>사실 time-dependent opacity, mean, rotation를 polynomial 등 특정 func.에 regression하는 task!</li> </ul> </li> <li> <p>Splatted Feature Rendering :<br/> spherical harmonics (<code class="language-plaintext highlighter-rouge">SH</code>) coeff. <code class="language-plaintext highlighter-rouge">대신</code><br/> base color, view direction info., time info.를 encoding하는 <code class="language-plaintext highlighter-rouge">feature</code> \(f_{i}(t) \in R^{9}\) 사용</p> </li> <li>Guided Sampling of Gaussians :<br/> initialization 할 때 Gaussian이 희박한 먼 영역은 흐릿해지는 경향이 있는데,<br/> 이를 해결하기 위해 <code class="language-plaintext highlighter-rouge">학습 오차와 coarse depth를 guidance</code>로 삼아<br/> 4D scene에서 <code class="language-plaintext highlighter-rouge">새로운 Gaussian을 sampling</code></li> </ul> <h2 id="method">Method</h2> <h3 id="spacetime-gaussians">Spacetime Gaussians</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-08-STGS/2.PNG-480.webp 480w,/assets/img/2025-02-08-STGS/2.PNG-800.webp 800w,/assets/img/2025-02-08-STGS/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-08-STGS/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p><code class="language-plaintext highlighter-rouge">temporal-and-spatial opacity</code> :<br/> \(\alpha_{i}(\boldsymbol x, t) = \sigma_{i}(t) \text{exp}(-\frac{1}{2} (\boldsymbol x - \mu_{i}(t))^{T} \Sigma_{i}(t)^{-1} (\boldsymbol x - \mu_{i}(t)))\)<br/> (temporal opacity \(\sigma_{i}(t)\) 가 위치 \(\boldsymbol x\) 에 따라 (spatial) 희석됨)<br/> where \(\sigma_{i}(t)\) : temporal opacity<br/> where \(\mu_{i}(t), \Sigma_{i}(t)\) : time-dependent mean, covariance</p> </li> <li><code class="language-plaintext highlighter-rouge">Temporal Radial Basis Function</code> (<code class="language-plaintext highlighter-rouge">temporal opacity</code>) :<br/> \(\sigma_{i}(t) = \sigma_{i}^{s} \text{exp}(-s_{i}^{\tau} | t - \mu_{i}^{\tau} |^{2})\)<br/> where <code class="language-plaintext highlighter-rouge">temporal center</code> \(\mu_{i}^{\tau}\) : \(i\)-th STG \(G_{i}\) 가 가장 잘 보이는 timestamp<br/> where <code class="language-plaintext highlighter-rouge">temporal scaling factor</code> \(s_{i}^{\tau}\) : valid 지속 기간 결정<br/> where temporal-independent <code class="language-plaintext highlighter-rouge">spatial opacity</code> \(\sigma_{i}^{s}\) <ul> <li>1D Gaussian으로 모델링!<br/> 즉, timestamp \(t\) 가 temporal center \(\mu_{i}^{\tau}\) 에서 멀어질수록 opacity \(\sigma_{i}^{s}\) 가 옅어짐!</li> <li>시간에 따라 변하는 <code class="language-plaintext highlighter-rouge">opacity</code> \(\sigma_{i}(t)\) 을 통해<br/> <code class="language-plaintext highlighter-rouge">새로 나타나거나 사라지는</code> object를 효과적으로 모델링할 수 있음!</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Polynomial Motion Trajectory</code> :<br/> \(\mu_{i}(t) = \sum_{k=0}^{n_{p}} b_{i,k}(t - \mu_{i}^{\tau})^{k}\) <ul> <li>polynomial로 모델링!</li> <li>polynomical coeff. \(b_{i,k} \in R\) 은 learnable param.</li> <li>시간에 따라 변하는 <code class="language-plaintext highlighter-rouge">mean</code> \(\mu_{i}(t)\) 을 통해<br/> object <code class="language-plaintext highlighter-rouge">motion</code>을 모델링할 수 있음!</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Polynomial Rotation</code> :<br/> \(q_{i}(t) = \sum_{k=0}^{n_{q}} c_{i,k}(t - \mu_{i}^{\tau})^{k}\) <ul> <li>polynomial로 모델링!</li> <li>polynomical coeff. \(c_{i,k} \in R\) 은 learnable param.</li> <li>시간에 따라 변하는 <code class="language-plaintext highlighter-rouge">quaternion</code> \(q_{i}(t)\) 을 통해<br/> object <code class="language-plaintext highlighter-rouge">변형</code>을 모델링할 수 있음!</li> </ul> </li> <li>time-independent Scale : <ul> <li>Scaling matrix \(S_{i}\) 는 시간에 독립적</li> </ul> </li> </ul> <h3 id="splatted-feature-rendering">Splatted Feature Rendering</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-08-STGS/3.PNG-480.webp 480w,/assets/img/2025-02-08-STGS/3.PNG-800.webp 800w,/assets/img/2025-02-08-STGS/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-08-STGS/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Feature Splatting</code> : <ul> <li><code class="language-plaintext highlighter-rouge">color</code>의 경우 view direction 뿐만 아니라 시간에 따라 변하므로<br/> <code class="language-plaintext highlighter-rouge">spherical harmonics (SH) coeff.</code> 대신<br/> <code class="language-plaintext highlighter-rouge">feature</code> \(f_{i}(t) = [f_{i}^{base}, f_{i}^{dir}, (t - \mu_{i}^{\tau}) f_{i}^{time}]^{T} \in R^{9}\) 로 대체! <ul> <li>RGB base color : \(f_{i}^{base} \in R^{3}\)</li> <li>view direction : \(f_{i}^{dir} \in R^{3}\)</li> <li>time : \(f_{i}^{time} \in R^{3}\)</li> </ul> </li> <li>RGB color (SH coeff.) 대신 feature \(f_{i}(t)\) 를 image-space로 splatting한 뒤<br/> 2-layer MLP \(\Phi\) 를 거쳐 최종 RGB color를 얻음<br/> \(I = F^{base} + \Phi(F^{dir}, F^{time}, \boldsymbol r)\) <ul> <li>feature \(f_{i}(t)\) 를 image-space로 splatting한 feature를 \(F^{base}, F^{dir}, F^{time}\) 으로 분할</li> <li>target view direction : \(\boldsymbol r\)</li> </ul> </li> <li>장점 : <ul> <li>less param. than SH coeff. encoding</li> <li>still fast rendering using shallow MLP \(\Phi\)</li> </ul> </li> <li>light 버전 :<br/> rendering speed를 최대화하기 위해 선택적으로 MLP \(\Phi\) 를 삭제하고 \(F^{base}\) 만 유지</li> </ul> </li> </ul> <h3 id="loss">Loss</h3> <ul> <li>learnable param. : <ul> <li>temporal <code class="language-plaintext highlighter-rouge">opacity</code> : <ul> <li>time-independent spatial opacity \(\sigma_{i}^{s}\)</li> <li>temporal scaling factor \(s_{i}^{\tau}\)</li> <li>temporal center \(\mu_{i}^{\tau}\)</li> </ul> </li> <li>time-dependent <code class="language-plaintext highlighter-rouge">motion</code> (trajectory) : <ul> <li>polynomial coeff. \(\{ b_{i,k} \}_{k=0}^{n_{p}}\)</li> </ul> </li> <li>time-dependent <code class="language-plaintext highlighter-rouge">rotation</code> (quaternion) : <ul> <li>polynomial coeff. \(\{ c_{i,k} \}_{k=0}^{n_{q}}\)</li> </ul> </li> <li>time-dependent <code class="language-plaintext highlighter-rouge">color</code> : <ul> <li>feature \(f_{i}^{base}, f_{i}^{dir}, f_{i}^{time}\)</li> </ul> </li> </ul> </li> <li>loss :<br/> photometric loss (L1, D-SSIM)</li> </ul> <h3 id="guided-sampling-of-gaussians">Guided Sampling of Gaussians</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-08-STGS/4.PNG-480.webp 480w,/assets/img/2025-02-08-STGS/4.PNG-800.webp 800w,/assets/img/2025-02-08-STGS/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-08-STGS/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>issue :<br/> initialization 할 때 <code class="language-plaintext highlighter-rouge">Gaussian이 희박한 먼 영역은 흐릿</code>해지는 경향이 있음</p> </li> <li> <p>solution :</p> <ul> <li>이를 해결하기 위해 <code class="language-plaintext highlighter-rouge">학습 오차</code>와 <code class="language-plaintext highlighter-rouge">coarse depth</code>를 guidance로 삼아<br/> 4D scene에서 새로운 Gaussian을 sampling</li> <li>sampling 효율성을 보장하기 위해<br/> <code class="language-plaintext highlighter-rouge">loss가 안정된 후</code>에 sampling 진행</li> <li>Procedure : <ul> <li>Step 1)<br/> 학습 오차에 noise가 있을 수 있으므로<br/> patch-wise로 학습 오차를 계산하여 상당한 오차가 있는 patch 찾기</li> <li>Step 2)<br/> 학습 오차가 큰 patch의 중앙 pixel의 ray를 따라 Gaussian들을 sampling<br/> (coarse depth map을 이용해 Gaussian들이 희박한 깊이 범위를 찾은 뒤 해당 범위에서 uniform sampling)<br/> (3회 이하로 수행) <ul> <li>feature sampling 중에 생성되는 coarse depth map을 이용하므로<br/> additional overhead 거의 없음</li> <li>새로 sampling된 Gaussian들의 mean에 작은 noise를 추가<br/> (불필요한 Gaussian들은 학습 중에 자연스레 opacity가 낮아져 remove됨)</li> </ul> </li> </ul> </li> <li>의의 :<br/> <code class="language-plaintext highlighter-rouge">3DGS density control을 보완</code><br/> (3DGS density control은 기존 Gaussian들 근처에서 점진적으로 Gaussian들을 증가시키는데,<br/> 본 논문의 Guided Sampling은 Gaussian들이 희박한 새로운 영역에서 Gaussians들을 sampling)</li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <ul> <li>Implementation :<br/> \(n_{p}=3, n_{q}=1\)<br/> Adam optimizer<br/> initialize Spacetime Gaussians using SfM pcd of all timestamps<br/> density control의 pruning을 3DGS보다 더 공격적으로 수행하여 Gaussian 수를 줄이고 모델 크기 작게 유지<br/> 40~60 min. for 50 frames on NVIDIA A6000 GPU</li> </ul> <h3 id="result">Result</h3> <ul> <li>Neural 3D Video Dataset :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-08-STGS/5.PNG-480.webp 480w,/assets/img/2025-02-08-STGS/5.PNG-800.webp 800w,/assets/img/2025-02-08-STGS/5.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-08-STGS/5.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-08-STGS/6.PNG-480.webp 480w,/assets/img/2025-02-08-STGS/6.PNG-800.webp 800w,/assets/img/2025-02-08-STGS/6.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-08-STGS/6.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Google Immersive Dataset :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-08-STGS/7.PNG-480.webp 480w,/assets/img/2025-02-08-STGS/7.PNG-800.webp 800w,/assets/img/2025-02-08-STGS/7.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-08-STGS/7.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-08-STGS/8.PNG-480.webp 480w,/assets/img/2025-02-08-STGS/8.PNG-800.webp 800w,/assets/img/2025-02-08-STGS/8.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-08-STGS/8.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Technicolor Dataset :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-08-STGS/9.PNG-480.webp 480w,/assets/img/2025-02-08-STGS/9.PNG-800.webp 800w,/assets/img/2025-02-08-STGS/9.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-08-STGS/9.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="ablation-study">Ablation Study</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-08-STGS/10.PNG-480.webp 480w,/assets/img/2025-02-08-STGS/10.PNG-800.webp 800w,/assets/img/2025-02-08-STGS/10.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-08-STGS/10.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-08-STGS/11.PNG-480.webp 480w,/assets/img/2025-02-08-STGS/11.PNG-800.webp 800w,/assets/img/2025-02-08-STGS/11.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-08-STGS/11.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="limitation">Limitation</h2> <ul> <li>Limitation : <ul> <li>need SfM for Spacetime Gaussians’ Initialization</li> <li>per-Scene model (<code class="language-plaintext highlighter-rouge">???</code> maybe)</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="dynamic"/><category term="3DGS"/><summary type="html"><![CDATA[Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis (CVPR 2024)]]></summary></entry><entry><title type="html">SplineGS</title><link href="https://semyeong-yu.github.io/blog/2025/SplineGS/" rel="alternate" type="text/html" title="SplineGS"/><published>2025-02-06T10:00:00+00:00</published><updated>2025-02-06T10:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/SplineGS</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/SplineGS/"><![CDATA[<h2 id="splinegs---robust-motion-adaptive-spline-for-real-time-dynamic-3d-gaussians-from-monocular-video-cvpr-2025">SplineGS - Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video (CVPR 2025)</h2> <h4 id="jongmin-park-minh-quan-viet-bui-juan-luis-gonzalez-bello-jaeho-moon-jihyong-oh-munchurl-kim">Jongmin Park, Minh-Quan Viet Bui, Juan Luis Gonzalez Bello, Jaeho Moon, Jihyong Oh, Munchurl Kim</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2412.09982">https://arxiv.org/abs/2412.09982</a><br/> project website :<br/> <a href="https://kaist-viclab.github.io/splinegs-site/">https://kaist-viclab.github.io/splinegs-site/</a></p> </blockquote> <blockquote> <p>핵심 :</p> <ol> <li>COLMAP-free :<br/> two-stage training strategy 사용<br/> 즉, camera param.을 먼저 roughly estimate한 뒤 jointly optimize camera param. and 3DGS param.</li> <li>dynamic scenes from in-the-wild monocular videos :<br/> static 3DGS와 dynamic 3DGS의 union</li> <li>dynamic 3DGS’s mean :<br/> apply spline-based model (MAS) to each dynamic 3DGS mean (trajectories)<br/> 이 때, depthmap과 camera param.를 이용해 2D track을 unproject하여 3D mean trajectories 초기화</li> <li>thousands time faster than SOTA :<br/> more efficient than MLP-based or grid-based</li> <li>loss :<br/> RGB image recon. loss<br/> depth recon. loss<br/> 2D projection alignment loss<br/> 3D alignment loss<br/> motion mask loss</li> </ol> </blockquote> <h2 id="contribution">Contribution</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/1.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/1.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/1.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/1.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>novelty : <ul> <li>Motion-Adaptive Spline (MAS) :<br/> continuous dynamic <code class="language-plaintext highlighter-rouge">3DGS trajectories</code> (deformation) 을 효율적으로 모델링하기 위해<br/> <code class="language-plaintext highlighter-rouge">cubic Hermite splines</code> with a small number of control points 사용 <ul> <li>control point : <ul> <li>learnable param.</li> <li>determines each piecewise cubic func.’s curvature and direction</li> </ul> </li> <li>initialization :<br/> <code class="language-plaintext highlighter-rouge">2D track</code>을 <code class="language-plaintext highlighter-rouge">depthmap</code> 이용하여 3D로 unproject</li> </ul> </li> <li>Motion-Adaptive Control points Pruning (MACP) :<br/> quality, efficiency 모두 챙기기 위해 계속 <code class="language-plaintext highlighter-rouge">control points를 prune</code>하여 수 조절</li> <li>joint optimization strategy :<br/> <code class="language-plaintext highlighter-rouge">photometric and geometric consistency</code> loss 이용해서<br/> (external estimators 필요 X)<br/> <code class="language-plaintext highlighter-rouge">camera param.</code> 와 <code class="language-plaintext highlighter-rouge">3DGS param.</code>를 jointly optimize<br/> (COLMAP-free!)</li> </ul> </li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li>dynamic novel-view-synthesis : <ul> <li>implicit representation (<code class="language-plaintext highlighter-rouge">MLP</code>) 이용하여 deformation 모델링 in canonical space <d-cite key="Deform1">[1]</d-cite>, <d-cite key="Deform2">[2]</d-cite>, <d-cite key="Deform3">[3]</d-cite>, <d-cite key="Deform4">[4]</d-cite>, <d-cite key="Deform5">[5]</d-cite> <ul> <li>단점 : 아무리 tiny MLP더라도 computational <code class="language-plaintext highlighter-rouge">overhead</code> and low speed</li> </ul> </li> <li>4D space-time domain을 <code class="language-plaintext highlighter-rouge">multiple 2D planes로 decompose</code>하는 grid-based model <d-cite key="Grid1">[6]</d-cite>, <a href="https://semyeong-yu.github.io/blog/2024/4DGS/">4DGS</a>, <d-cite key="Grid3">[7]</d-cite>, <d-cite key="Grid4">[8]</d-cite> <ul> <li>단점 : grid representation으로는 scene의 dynamic 특징의 <code class="language-plaintext highlighter-rouge">fine detail을 fully capture할 수 없음</code></li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">polynomial trajectories</code> 적용 <d-cite key="trajectory">[9]</d-cite> <ul> <li>장점 : efficient (low cost)</li> <li>단점 : polynomial trajectory의 <code class="language-plaintext highlighter-rouge">fixed degree</code>는 complex motion을 표현하는 flexibility 측면에서 제한적임</li> </ul> </li> </ul> </li> <li>spline : <ul> <li>minimal number of control points로 complex shape를 smooth and continuous representation으로 표현할 수 있음</li> </ul> </li> <li>SplineGS (본 논문) : <ul> <li>논문 <d-cite key="Mosca">[10]</d-cite>, <d-cite key="GauFRe">[11]</d-cite>에서처럼<br/> 각각 static bg와 moving object를 표현하기 위해<br/> 3DGS를 <code class="language-plaintext highlighter-rouge">static 3DGS와 dynamic 3DGS의 union</code>으로 확장 <ul> <li>static region :<br/> diffuse and specular features는 보존한 채<br/> time-encoded feature는 제거</li> <li>dynamic region :<br/> mean \(\mu_{i}\) 는 deformation modeling에 의해 결정되는 time-dependent var.<br/> rotation \(q_{i}\) 와 scale \(s_{i}\) 도 time-dependent var.</li> </ul> </li> <li>논문 <a href="https://semyeong-yu.github.io/blog/2025/STGS/">STGS</a>에서처럼<br/> final pixel <code class="language-plaintext highlighter-rouge">color</code>를 예측할 때 splatted feature rendering 사용 (<code class="language-plaintext highlighter-rouge">SH coeff. 대신 feature</code>!)</li> </ul> </li> </ul> <h2 id="method">Method</h2> <h3 id="architecture">Architecture</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/2.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/2.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>goal :<br/> jointly optimize 3DGS param. and camera param. <ul> <li>camera param. :<br/> extrinsic \([\hat R_{t} | \hat T_{t}] \in R^{3 \times 4}\) for each time \(t\)<br/> and shared intrinsic \(\hat K \in R^{3 \times 3}\) across all \(t\)</li> <li>how :<br/> two-stage optimization<br/> (warm-up stage and main traning stage) <ul> <li><code class="language-plaintext highlighter-rouge">warm-up stage</code> :<br/> optimize <code class="language-plaintext highlighter-rouge">coarse camera param.</code><br/> using photometric and geometric consistency<br/> (<code class="language-plaintext highlighter-rouge">SfM 사용하지 않기 위해!</code>)</li> <li><code class="language-plaintext highlighter-rouge">main training stage</code> :<br/> initialize 3DGS based on the estimated camera poses<br/> and<br/> jointly optimize 3DGS param. and camera param. with MAS and MACP</li> </ul> </li> </ul> </li> </ul> <h3 id="motion-adaptive-spline-for-3dgs">Motion-Adaptive Spline for 3DGS</h3> <p>time \(t\) 에서 each dynamic 3DGS의 mean \(\mu(t)\) (continuous trajectory)를 모델링하기 위해<br/> cubic Hermite spline function with a set of learnable control points 사용 (MAS)<br/> 즉, each dynamic Gaussian마다 a set of control points가 있고 얘네들의 spline curve로 Gaussian mean \(\mu(t)\) 을 결정!</p> <ul> <li>Motion-Adaptive Spline (<code class="language-plaintext highlighter-rouge">MAS</code>) :<br/> \(\mu(t) = S(t, \boldsymbol P)\) <ul> <li>input : <ul> <li>time \(t\)</li> <li>a set of \(N_{c}\) learnable control points<br/> \(\boldsymbol P = \{ \boldsymbol p_{k} | \boldsymbol p_{k} \in R^{3} \}\) where \(k \in [0, N_{c}-1]\)</li> </ul> </li> <li>piece-wise cubic Hermite spline function \(S(\cdot)\) :<br/> \(S(t, \boldsymbol P) = (2t_{r}^{3} - 3t_{r}^{2} + 1) \boldsymbol p_{\lfloor t_{s} \rfloor} + (t_{r}^{3} - 2t_{r}^{2} + t_{r}) \boldsymbol m_{\lfloor t_{s} \rfloor} + (-2t_{r}^{3} + 3t_{r}^{2}) \boldsymbol p_{\lfloor t_{s} \rfloor + 1} + (t_{r}^{3} - t_{r}^{2}) \boldsymbol m_{\lfloor t_{s} \rfloor + 1}\) <ul> <li>\(N_{f}\) : frame (timestamp) 개수</li> <li>\(N_{c}\) : control point 개수 (estimated by MACP)</li> <li> \[t \in [0, N_{f} - 1]\] </li> <li>\(t_{s} = t \frac{N_{c} - 1}{N_{f} - 1} \in [0, N_{c} - 1]\)<br/> e.g. 3.7</li> <li>\(t_{r} = t_{s} - \lfloor t_{s} \rfloor\)<br/> e.g. 0.7</li> <li>\(\boldsymbol m_{k} = (\boldsymbol p_{k+1} - \boldsymbol p_{k-1})/2\) : approx. tangent(기울기) of control point \(\boldsymbol p_{k}\)</li> <li><code class="language-plaintext highlighter-rouge">piece-wise cubic Hermite spline function</code> :<br/> \(\lfloor t_{s} \rfloor = 3\) 에서의 control point 및 tangent와<br/> \(\lfloor t_{s} \rfloor + 1 = 4\) 에서의 control point 및 tangent와<br/> 그 사이 어디쯤 있는지 \(t_{r} = 0.7\) 를 이용하여<br/> \(\lfloor t_{s} \rfloor = 3\) 과 \(\lfloor t_{s} \rfloor + 1 = 4\) 사이의 piece-wise cubic Hermite spline function을 그림</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/11m.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/11m.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/11m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/11m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/13m.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/13m.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/13m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/13m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Initialization of 3D Control Points</code> :<br/> intialization은 quality에 매우 중요!<br/> long-range <code class="language-plaintext highlighter-rouge">2D track</code> <d-cite key="cotracker">[12]</d-cite>과 <code class="language-plaintext highlighter-rouge">depth</code> <d-cite key="unidepth">[13]</d-cite> prior 사용 <ul> <li>notation : <ul> <li>2D track by <d-cite key="cotracker">[12]</d-cite> : \(\mathcal{T} = \left\{ \varphi_{t}^{tr} | \varphi_{t}^{tr} \in R^{2} \right\}_{t \in [0, N_{f} - 1]}\)<br/> where \(\varphi_{t}^{tr}\) : 2D track on pixel-coordinate at time \(t\)</li> <li>projection func. from 3D camera-space to 2D image-space by intrinsic \(K\) : \(\pi_{K}(\cdot)\)</li> </ul> </li> <li>Step 1)<br/> <code class="language-plaintext highlighter-rouge">unproject 2D track</code> \(\mathcal{T}\) on image-space into 3D track curve on world-space<br/> using <code class="language-plaintext highlighter-rouge">depth</code> \(d_{t}\) and coarsely-estimated <code class="language-plaintext highlighter-rouge">camera param.</code> \(\hat K, [\hat R_{t} | \hat T_{t}]\)<br/> \(W_{t}(\varphi_{t}^{tr}) = \hat R_{t}^{T} \pi_{\hat K}^{-1}(\varphi_{t}^{tr}, d_{t}(\varphi_{t}^{tr})) - \hat R_{t}^{T} \hat T_{t}\) <ul> <li>we estimate camera param. \(\hat K, \hat R, \hat T\) from only frames (without any GT)</li> </ul> </li> <li>Step 2)<br/> initialize per-Gaussian control points set \(\boldsymbol P\)<br/> by least-square approx. s.t. <code class="language-plaintext highlighter-rouge">spline curve</code> \(S(t, \boldsymbol P)\) fits the initial <code class="language-plaintext highlighter-rouge">tracker curve</code> \(W_{t}(\varphi_{t}^{tr})\)<br/> \(\text{min}_{\boldsymbol P} \sum_{t=0}^{N_{f} - 1} \| W_{t}(\varphi_{t}^{tr}) - S(t, \boldsymbol P) \|^{2}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/14m.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/14m.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/14m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/14m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Motion-Adaptive Control Points Pruning (<code class="language-plaintext highlighter-rouge">MACP</code>) : <ul> <li>issue : <ul> <li>control points 수가 너무 많으면<br/> spline curve가 over-fitting되고 speed가 느려짐</li> <li>scene마다 motion의 종류와 정도가 각기 다르므로<br/> control points 수 for each dynamic 3DGS 는 scene에 맞춰서 need to be adaptively adjusted</li> </ul> </li> <li>solution :<br/> sparser control points로 prune하기 위해<br/> <code class="language-plaintext highlighter-rouge">every 3DGS densification이 끝날 때마다</code> new spline function \(\mu(t) = S(t, \boldsymbol P')\) 계산<br/> where \(\boldsymbol P' = \left\{ \boldsymbol p_{l}' | \boldsymbol p_{l}' \in R^{3} \right\}_{l \in [0, N_{c} - 2]}\) : a set of \(N_{c} - 1\) control points<br/> (current set \(\boldsymbol P\) 보다 control point 1개 더 적음)</li> <li>Step 1)<br/> <code class="language-plaintext highlighter-rouge">1개 적은 control point set</code>으로도 최대한 비슷한 spline curve를 만들도록 least-square approx.<br/> \(\text{min}_{\boldsymbol P'} \sum_{t=0}^{N_{f}-1} \| S(t, \boldsymbol P) - S(t, \boldsymbol P') \|^{2}\)</li> <li>Step 2)<br/> \(S(t, \boldsymbol P)\) 와 \(S(t, \boldsymbol P')\) 간의 error \(E\) 가 작을 때만 a set of control points 업데이트<br/> \(\boldsymbol P = \begin{cases} \boldsymbol P' &amp; \text{if} &amp; E \lt \epsilon \\ \boldsymbol P &amp; O.W. \end{cases}\)<br/> where error \(E = \frac{1}{N_{f}} \sum_{t=0}^{N_{f} - 1} \| \pi_{\hat K}(\hat R_{t} S(t, \boldsymbol P) + \hat T_{t}) - \pi_{\hat K} (\hat R_{t} S(t, \boldsymbol P') + \hat T_{t} \|^{2}\)<br/> (각 timestamp \(t\) 에서 <code class="language-plaintext highlighter-rouge">3D mean on spline curve를 2D로 project시킨 뒤 차이</code> 비교)</li> <li>의의 :<br/> each dynamic 3DGS마다 a set of control points를 따로 가지고 있는데,<br/> MACP 덕분에 각 dynamic 3DGS가 각기 다른 수의 control points를 가질 수 있고,<br/> <code class="language-plaintext highlighter-rouge">motion이 복잡한 part는 control points 수가 많고</code><br/> <code class="language-plaintext highlighter-rouge">motion이 단순한 part는 control poitns 수가 적은</code> 방식으로<br/> scene에 adaptively adjust 가능</li> </ul> </li> </ul> <h3 id="camera-pose-estimation">Camera Pose Estimation</h3> <ul> <li>Camera Pose : <ul> <li><code class="language-plaintext highlighter-rouge">extrinsic</code> :<br/> \([\hat R_{t} | \hat T_{t}] = F_{\theta}(\gamma(t))\) <ul> <li>extrinsic 은 <code class="language-plaintext highlighter-rouge">time에 대한 function</code></li> <li>notation : <ul> <li>\(\gamma(\cdot)\) : positional encoding</li> <li>\(F_{\theta}\) : shallow MLP</li> </ul> </li> </ul> </li> <li>intrinsic (<code class="language-plaintext highlighter-rouge">focal length</code>) : <ul> <li>focal length \(\hat f\) 는 learnable param. <code class="language-plaintext highlighter-rouge">shared across all frames</code> in monocular video</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/12m.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/12m.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/12m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/12m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Loss for optimizing Camera Pose : <ul> <li>Loss 1) <code class="language-plaintext highlighter-rouge">photometric consistency</code> : <code class="language-plaintext highlighter-rouge">projection alignment</code> <ul> <li>목적 :<br/> target frame \(t\) 의 pixel \(i\) 가 reference frame \(t_{ref}\) 의 pixel \(j\) 로 projection 되었을 때<br/> reference frame’s pixel \(j\) 의 color \(I_{t_{ref}}(\varphi_{t \rightarrow t_{ref}})\) 가<br/> target frame’s pixel \(i\) 의 color \(I_{t}(\varphi_{t})\) 와 일치하도록</li> <li>notation : <ul> <li>\(\varphi_{t}\) : target frame’s pixel-coordinate</li> <li>\(\varphi_{t \rightarrow t_{ref}} = \pi_{\hat K} (\hat R_{t_{ref}} (\hat R_{t}^{T} \pi_{\hat K}^{-1} (\varphi_{t}, d_{t}(\varphi_{t})) - \hat R_{t}^{T} \hat T_{t}) + \hat T_{t_{ref}})\) : reference frame’s pixel-coordinate corresponding to \(\varphi_{t}\)<br/> (2D target frame \(t\)’s pixel-coordinate \(\rightarrow\) 3D location world-coordinate \(\rightarrow\) 2D reference frame \(t_{ref}\)’s pixel-coordinate)</li> </ul> </li> <li>loss :<br/> \(L_{pc} = \sum_{\varphi_{t}} \| M_{t, t_{ref}}(\varphi_{t}) \circledast (I_{t}(\varphi_{t}) - I_{t_{ref}}(\varphi_{t \rightarrow t_{ref}})) \|^{2}\) <ul> <li>\(M_{t, t_{ref}} = M_{t}(\varphi_{t}) M_{t_{ref}}(\varphi_{t \rightarrow t_{ref}})\) : union motion mask<br/> (dynamic objects는 color 변하는 게 당연하니까 제거하고, static region에 대해서만 loss 걸어줌)<br/> (\(M_{t}\) 와 \(M_{t_{ref}}\) 는 각각 \(I_{t}\) 와 \(I_{t_{ref}}\) 로부터 미리 계산한 motion mask <d-cite key="TrackAnything">[14]</d-cite>)</li> </ul> </li> </ul> </li> <li>Loss 2) <code class="language-plaintext highlighter-rouge">geometric consistency</code> : <code class="language-plaintext highlighter-rouge">3D alignment</code> <ul> <li>목적 :<br/> target frame \(t\) 의 pixel \(i\) 가 reference frame \(t_{ref}\) 의 pixel \(j\) 로 projection 되었을 때<br/> reference frame’s pixel \(j\) 를 3D location on world-coordinate으로 unproject시킨 \(W_{t_{ref}}(\varphi_{t \rightarrow t_{ref}})\) 가<br/> target frame’s pixel \(i\) 를 3D location on world-coordinate으로 unproject시킨 \(W_{t}(\varphi_{t})\) 와 일치하도록</li> <li>notation : <ul> <li>\(W_{t}(\varphi_{t}) = \hat R_{t}^{T} \pi_{\hat K}^{-1}(\varphi_{t}, d_{t}(\varphi_{t})) - \hat R_{t}^{T} \hat T_{t}\) : unproject from pixel-coordinate to 3D world-coordinate</li> </ul> </li> <li>loss :<br/> \(L_{gc} = \sum_{\varphi_{t}} \| M_{t, t_{ref}}(\varphi_{t}) \circledast (W_{t}(\varphi_{t}) - W_{t_{ref}}(\varphi_{t \rightarrow t_{ref}})) \|^{2}\)</li> </ul> </li> </ul> </li> </ul> <h3 id="loss">Loss</h3> <ul> <li>Two-stage Optimization : <ul> <li>Stage 1) warm-up stage <ul> <li>optimize <code class="language-plaintext highlighter-rouge">only camera param.</code></li> <li>loss :<br/> \(L_{total}^{warm} = \lambda_{pc} L_{pc} + \lambda_{gc} L_{gc}\) <ul> <li>\(L_{pc}\) : photometric consistency (projection alignment)</li> <li>\(L_{gc}\) : geometric consistency (3D alignment)</li> </ul> </li> </ul> </li> <li>Stage 2) main training stage <ul> <li>Step 2-1)<br/> Stage 1)에서 coarsely 예측한 camera param. \(\hat K, \hat R, \hat T\) 를 이용하여<br/> 각 dynamic 3DGS의 <code class="language-plaintext highlighter-rouge">a set of control points 초기화</code><br/> (how? : 위의 Motion-Adaptive Spline for 3DGS 섹션에서 설명함)</li> <li>Step 2-2)<br/> <code class="language-plaintext highlighter-rouge">jointly optimize 3DGS param. and camera param.</code></li> <li>loss :<br/> \(L_{total}^{main} = \lambda_{rgb} L_{rgb} + \lambda_{d} L_{d} + \lambda_{M} L_{M} + \lambda_{pc} L_{pc} + \lambda_{d-pc} L_{d-pc} + \lambda_{gc} L_{gc}\) <ul> <li><code class="language-plaintext highlighter-rouge">recon. loss</code> : <ul> <li>\(L_{rgb}\) : L1 recon. loss b.w. rendered frame and GT frame</li> <li>\(L_{d}\) : L1 recon. loss b.w. rendered depth and GT depth</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">alignment loss</code> : <ul> <li>\(L_{pc}\) : photometric consistency (projection alignment)</li> <li>\(L_{gc}\) : geometric consistency (3D alignment)</li> <li>\(L_{d-pc}\) : additional photometric consistency (projection alignment) <ul> <li><code class="language-plaintext highlighter-rouge">prior depth</code> <d-cite key="unidepth">[13]</d-cite> <code class="language-plaintext highlighter-rouge">대신</code> 3DGS를 이용한 <code class="language-plaintext highlighter-rouge">rendered depth</code> 사용하여<br/> photometric consistency 계산</li> <li>prior depth 대신 rendered depth를 사용하면<br/> estimated 3DGS geometry 의 도움을 받아<br/> joint optimization of camera param. and 3DGS param. 가능!</li> </ul> </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">motion mask loss</code> : <ul> <li>\(L_{M} = 1 - \text{f1-score} = 1 - \frac{2(\sum_{\varphi_{t}} M_{t}(\varphi_{t}) \hat M_{t}(\varphi_{t})) + \epsilon}{(\sum_{\varphi_{t}} M_{t}(\varphi_{t}) + \hat M_{t}(\varphi_{t})) + \epsilon}\) : binary dice loss<br/> b.w. <code class="language-plaintext highlighter-rouge">pre-computed GT motion mask</code> \(M_{t}\) from prior <d-cite key="TrackAnything">[14]</d-cite><br/> and <code class="language-plaintext highlighter-rouge">rendered motion mask</code> \(\hat M_{t}\) from dynamic 3D Gaussians <ul> <li>rendered motion mask :<br/> \(\hat M_{t}(\varphi_{t}) = \sum_{i \in N} m_{i} \alpha_{i} \prod_{j=1}^{i-1} (1 - \alpha_{j})\)<br/> where \(m_{i} = 0\) if \(i\)-th 3DGS is static 3DGS, and \(m_{i} = 1\) if \(i\)-th 3DGS is dynamic 3DGS<br/> (즉, \(i\)-th 3DGS가 static인지, dynamic인지에 따른 \(m_{i}\) 를 accumulate 하여 motion mask로 rendering!)</li> </ul> </li> <li>binary dice loss는 highly imbalanced segmentation을 위해 제안되었듯이<br/> dynamic 3DGS와 static 3DGS를 더 잘 분리할 수 있게 해줌</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <ul> <li>Dataset : <ul> <li>NVIDIA dataset <ul> <li>evaluation configuration : <d-cite key="RoDynRF">[RoDynRF]</d-cite> 를 따름</li> <li>dataset sampling : <d-cite key="NSFF">[NSFF]</d-cite> 를 따름 <ul> <li>sample 24 timestamps</li> <li>larger motion을 simulate하기 위해<br/> 홀수 frames 제외</li> <li>generalization을 위해<br/> test 시에 사용할 timestamps를 training할 때 제외</li> </ul> </li> </ul> </li> <li>DAVIS dataset (avg. 70 frames per video)</li> </ul> </li> </ul> <h3 id="result">Result</h3> <ul> <li>Novel-View-Synthesis : <ul> <li>SOTA baseline : <ul> <li>COLMAP-based : <d-cite key="colmap1">[DynNeRF]</d-cite>, <d-cite key="colmap2">[MonoNeRF]</d-cite>, <d-cite key="colmap3">[STGS]</d-cite>, <d-cite key="colmap4">[SCGS]</d-cite>, <d-cite key="Deform5">[D3DGS]</d-cite>, <d-cite key="4DGS">[4DGS]</d-cite>, <d-cite key="RoDynRF">[RoDynRF]</d-cite>, <d-cite key="CasualFVS">[CasualFVS]</d-cite>, <d-cite key="Ex4DGS">[Ex4DGS]</d-cite>, <d-cite key="Mosca">[Mosca]</d-cite> <ul> <li>RoDynRF, DynNeRF : 느림</li> <li>Ex4DGS, STGS : multi-view setting으로 설계되어 monocular video로 학습하면 시간에 따라 점점 inconsistent geometry alignment</li> <li>D3DGS, STGS : SfM(COLMAP)이 DAVIS dataset에서 camera param. 및 initial pcd 잘 추정 못함</li> </ul> </li> <li>COLMAP-free : <d-cite key="RoDynRF">[RoDynRF]</d-cite>, <d-cite key="Mosca">[Mosca]</d-cite></li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/3.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/3.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/4.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/4.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/5.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/5.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/5.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/5.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Novel View and Time Synthesis : <ul> <li>SOTA baseline : <ul> <li>NeRF-based : <d-cite key="colmap1">[DynNeRF]</d-cite>, <d-cite key="RoDynRF">[RoDynRF]</d-cite> <ul> <li>RoDynRF, DynNeRF : unseen timestamp에 대해 artifacts 및 blurriness 생김</li> </ul> </li> <li>3DGS-based : <d-cite key="colmap3">[STGS]</d-cite>, <d-cite key="Deform5">[D3DGS]</d-cite>, <d-cite key="4DGS">[4DGS]</d-cite> <ul> <li>STGS, D3DGS, 4DGS : unseen timestamp에 대해 더 심각한 degradation 생김</li> </ul> </li> </ul> </li> <li>본 논문 (SplineGS) :<br/> MAS(Motion-Adaptive Spline) 덕분에<br/> dynamic 3D Gaussian들을 효과적으로 deform시켜서<br/> 시간에 따라 움직이는 물체의 continuous trajectories를 정확히 캡처할 수 있음 <ul> <li>unseen timestamp에 대해서도 continuous trajectory로 잘 캡처 가능</li> <li>temporal consistency는 아래의 tOF score로 확인 가능</li> <li>continuous trajectories 모델링 능력을 확인하기 위해<br/> 아래 그림에 dynamic objects의 projected 2D motion tracking 결과도 있음</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/6.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/6.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/6.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/6.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/7.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/7.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/7.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/7.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="ablation-study">Ablation Study</h3> <ul> <li>Motion-Adaptive Spline (MAS) : <ul> <li>baseline : various deformation models <ul> <li>MLP<br/> e.g. D3DGS</li> <li>grid-based model<br/> e.g. 4DGS</li> <li>polynomial func. of degree 3 or 10<br/> e.g. STGS<br/> (degree 10을 쓰면 numerical instability 때문에 noisier optimization으로 quality도 더 안 좋고, latency도 증가함)</li> <li>Bezier curve<br/> (성능 비슷하게 좋지만, recursive 계산 때문에 MAS보다 latency 큼)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/8.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/8.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/8.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/8.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Motion-Adaptive Control Points Pruning (MACP) : <ul> <li>baseline : fixed number of control points \(N_{c} = 4\) or \(N_{c} = N_{f}\)</li> <li>MAS with MACP :<br/> good trade-off (latency 조금 증가하지만 rendering quality 많이 증가)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/9.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/9.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/9.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/9.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Skating scene처럼 simple motion인 경우에는 MACP 덕분에 최소한의 N_c로도 대부분의 dynamic 3DGS 표현 가능 </div> <h2 id="conclusion">Conclusion</h2> <ul> <li>Limitation : <ul> <li>Prior 필요 <ul> <li>depthmap (for all)</li> <li>2D track (for all)</li> <li>motion mask (for \(L_{M}\))</li> </ul> </li> <li>in-the-wild video에서 camera 또는 object가 매우 빠르게 움직이는 경우 input frames 자체가 blurry한데,<br/> 이러한 input frames의 흐림 자체가 rendering quality를 낮춤 <ul> <li>현재 가능한 solution :<br/> SOTA 2D deblurring methods를 pre-processing으로 먼저 input frames에 적용한 뒤 training에 사용</li> <li>future work :<br/> 따로 pre-processing하지 않고,<br/> deblurring method와 recon. pipeline을 통합하여<br/> joint deblurring and rendering optimization framework 구축</li> </ul> </li> <li>per-Scene model이라 feed-forward model로 확장 가능 <code class="language-plaintext highlighter-rouge">???</code></li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/10.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/10.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/10.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-06-SplineGS/10.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br/> photometric consistency loss에서 motion mask 값이 static region에 대해서만 0이라는데,<br/> dynamic object를 exclude한다는 문구로 미루어보아 (static region에만 loss를 걸어주기 위해)<br/> static region에 대해서 1이어야 하는 거 아닌가요?</p> </li> <li> <p>A1 :<br/> code implementation 한 번 보자</p> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="dynamic"/><category term="colmap"/><category term="free"/><category term="motion"/><category term="adaptive"/><category term="monocular"/><summary type="html"><![CDATA[Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video (CVPR 2025)]]></summary></entry><entry><title type="html">NoPoSplat</title><link href="https://semyeong-yu.github.io/blog/2025/NoPoSplat/" rel="alternate" type="text/html" title="NoPoSplat"/><published>2025-02-03T10:00:00+00:00</published><updated>2025-02-03T10:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/NoPoSplat</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/NoPoSplat/"><![CDATA[<h2 id="no-pose-no-problem---surprisingly-simple-3d-gaussian-splats-from-sparse-unposed-images">No Pose, No Problem - Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images</h2> <h4 id="botao-ye-sifei-liu-haofei-xu-xueting-li-marc-pollefeys-ming-hsuan-yang-songyou-peng">Botao Ye, Sifei Liu, Haofei Xu, Xueting Li, Marc Pollefeys, Ming-Hsuan Yang, Songyou Peng</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2410.24207">https://arxiv.org/abs/2410.24207</a><br/> project website :<br/> <a href="https://noposplat.github.io/">https://noposplat.github.io/</a><br/> code :<br/> <a href="https://github.com/cvg/NoPoSplat">https://github.com/cvg/NoPoSplat</a></p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/4.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/4.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="contribution">Contribution</h2> <p><code class="language-plaintext highlighter-rouge">pose-free generalizable sparse-view 3D recon. model in canonical Gaussian space!</code></p> <ul> <li>model : <ul> <li><code class="language-plaintext highlighter-rouge">unposed</code> (no extrinsic) <code class="language-plaintext highlighter-rouge">sparse-view</code> images로부터 3DGS를 통해 3D scene recon.하는 feed-forward network 제시</li> <li><code class="language-plaintext highlighter-rouge">photometric loss만으로</code> train 가능<br/> (<code class="language-plaintext highlighter-rouge">GT depth 사용 X</code>, explicit matching loss 사용 X)</li> <li>본 논문은 intrinsic의 영향을 받는 image appearance에만 의존하여 recon.을 수행하므로<br/> <code class="language-plaintext highlighter-rouge">scale ambiguity</code> 문제 해결을 위해 <code class="language-plaintext highlighter-rouge">intrinsic embedding method</code> 사용<br/> (intrinsic은 input으로 사용)</li> <li>covariance, opacity, color를 예측하는 Gaussian Param. Head에서 fine texture detail 주기 위해 <code class="language-plaintext highlighter-rouge">RGB shortcut</code> 사용</li> </ul> </li> <li>downstream tasks : <ul> <li>recon.된 3DGS를 이용하여 novel-view-synthesis 및 pose-estimation task 수행 가능 <ul> <li>특히 limited input image overlap (sparse) 상황에서는 pose-required methods보다 더 좋은 성능</li> <li>정확히 pose-estimation 수행하는 two-stage coarse-to-fine pipeline 제시</li> </ul> </li> <li>generalize well to out-of-distribution data</li> </ul> </li> <li>Gaussian Space : <ul> <li><code class="language-plaintext highlighter-rouge">first input view의 local camera coordinate</code>을 <code class="language-plaintext highlighter-rouge">canonical space</code>로 고정하고 모든 input view의 3DGS들을 해당 space에서 directly 예측</li> <li>기존에는 transform-then-fuse pipeline이었는데,<br/> 본 논문은 global coordinate으로의 <code class="language-plaintext highlighter-rouge">explicit transform 없이</code> canonical space 내에서의 different views의 fusion 자체를 직접 network로 학습</li> <li>local coordinate에서 global coordinate으로 3DGS를 explicitly transform할 필요가 없으므로<br/> explicitly transform하면서 생기는 per-frame Gaussians의 misalignment를 방지할 수 있고, extrinsic pose 없이도 (pose-free) 3D recon. 가능</li> </ul> </li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li>SfM : <ul> <li>bundle adjustment 등 최적화 과정을 거치는데,<br/> off-the-shelf pose estimation method 사용하는 것 자체가 많은 연산을 필요로 하고 runtime 늘림</li> <li>3D recon.에 only two frames만 input으로 사용하더라도<br/> SfM을 통해 해당 two frames의 camera pose를 구하려면 many poses from dense videos 필요 (impractical)</li> <li>textureless area (원형 호수 등) 또는 image가 sparse한 영역에서는 부정확한 pose 내놓음</li> </ul> </li> <li>Pose-Free Method : <ul> <li>pose-estimation과 3D recon.을 single pipeline으로 통합하자! : <d-cite key="DBARF">[1]</d-cite>, <d-cite key="Flowcam">[2]</d-cite>, <d-cite key="Unifying">[3]</d-cite> <ul> <li>pose-estimation과 scene-recon.을 번갈아가며 수행하는 sequential process 에서 error가 쌓이기 때문에<br/> SOTA novel-view-synthesis methods보다 성능 bad</li> </ul> </li> <li>DUSt3R, MASt3R 계열</li> </ul> </li> <li>DUSt3R, MASt3R : <ul> <li>공통점 1)<br/> pose-free method</li> <li>공통점 2)<br/> directly predict in canonical space</li> <li>차이점 1)<br/> DUSt3R, MASt3R는 transformer output이 3D pointmap (point cloud)인데,<br/> NoPoSplat은 mean, covariance, opacity, color를 가진 3DGS (rasterization) 사용</li> <li>차이점 2)<br/> NoPoSplat은 DUSt3R, MASt3R 계열과 달리 <code class="language-plaintext highlighter-rouge">GT depth 필요 없고 photometric loss만으로</code> 훈련 가능</li> </ul> </li> <li>pixelSplat, MVSplat : <ul> <li>차이점 1) (아래 그림 참고)<br/> pixelSplat, MVSplat은 먼저 intrinsic을 이용해 2D-to-3D로 unproject(lift)하여 each local-coordinate에서 3DGS를 예측한 뒤 extrinsic을 이용해 world-coordinate으로 transform한 뒤 fuse했는데,<br/> NoPoSplat은 canonical space 내에서의 different views의 fusion 자체를 directly network로 학습하기 때문에 <code class="language-plaintext highlighter-rouge">global coordinate으로 transform할 필요가 없으므로</code> 이에 따른 <code class="language-plaintext highlighter-rouge">misalignment를 방지</code>할 수 있고 <code class="language-plaintext highlighter-rouge">camera pose (extrinsic)도 필요 없음</code></li> <li>차이점 2)<br/> pixelSplat에선 epipolar constraint, MVSplat에선 cost volume이라는 geometry prior를 사용하였는데<br/> image view overlap이 적을 때는 geometry prior가 정확하지 않음.<br/> NoPoSplat은 (image overlap이 클 때 유리한) <code class="language-plaintext highlighter-rouge">geometry prior들을 사용하지 않음</code></li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/2.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/2.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="method">Method</h2> <h3 id="architecture">Architecture</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/3.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/3.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>I/O :<br/> \(f_{\theta} : \left\{ (I^{v}, k^{v}) \right\}_{v=1}^{V} \mapsto \left\{ \bigcup (\mu_{j}^{v}, \alpha_{j}^{v}, r_{j}^{v}, s_{j}^{v}, c_{j}^{v}) \right\}_{j=1, \ldots, H \times W}^{v=1, \ldots, V}\) <ul> <li>input : <ul> <li>sparse unposed multi-view images \(I\) (image 개수 \(V\))</li> <li>camera intrinsics \(k\) (available from modern devices <d-cite key="intrinsic">[4]</d-cite>)</li> </ul> </li> <li>output : <ul> <li>mean \(\mu \in R^{3}\), opacity \(\alpha \in R\), rotation \(r \in R^{4}\), scale \(s \in R^{3}\), SH \(c \in R^{k}\) (\(k\) degrees of freedom)</li> </ul> </li> </ul> </li> <li>Pipeline : <ul> <li><code class="language-plaintext highlighter-rouge">Encoder, Decoder</code> : <ul> <li>특히 input views끼리 content overlap이 적은 상황 (sparse) 에서는<br/> epipolar constraint나 cost volume 같은 geometry prior가 없더라도<br/> simple ViT 구조만으로도 좋은 성능 달성 가능</li> <li>RGB images를 image tokens로 patchify, flatten한 뒤<br/> intrinsic token과 concatenate한 뒤<br/> Encoder and Decoder에 feed-forward</li> </ul> </li> <li>Gaussian Parameter Prediction Head :<br/> DPT 구조 <ul> <li><code class="language-plaintext highlighter-rouge">Gaussian Center Head</code> :<br/> Decoder feature 사용</li> <li><code class="language-plaintext highlighter-rouge">Gaussian Param Head</code> :<br/> RGB image와 Decoder feature 사용 <ul> <li><code class="language-plaintext highlighter-rouge">RGB shortcut</code> :<br/> 3D recon.에서 fine texture detail을 잡는 것이 중요하기 때문에 사용</li> <li>Decoder feature :<br/> high-level semantic info.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <h3 id="gaussian-space">Gaussian Space</h3> <ul> <li>baseline: <code class="language-plaintext highlighter-rouge">Local-to-Global Gaussian Space</code> <ul> <li>pixelSplat, MVSplat 등</li> <li>how :<br/> 먼저 each pixel의 depth를 network로 예측한 뒤<br/> predicted depth와 intrinsic을 이용해 2D-to-3D로 unproject(lift)하여 each local-coordinate에서 3DGS 예측한 뒤<br/> extrinsic을 이용해 world-coordinate으로 transform한 뒤<br/> 모든 transformed 3DGS들을 fuse</li> <li>issue : <ul> <li>local-coordinate에서 world-coordinate으로 transform할 때 <code class="language-plaintext highlighter-rouge">accurate camera pose</code> (extrinsic) 필요한데, 이는 input view가 sparse한 real-world 상황에서 얻기 어렵</li> <li>특히 input view가 sparse할 때 또는 out-of-distribution data로 일반화할 때는<br/> <code class="language-plaintext highlighter-rouge">each transformed 3DGS들을 조화롭게 combine</code>하는 게 어렵</li> </ul> </li> </ul> </li> <li>NoPoSplat: <code class="language-plaintext highlighter-rouge">Canonical Gaussian Space</code> <ul> <li>how :<br/> first input view를 global referecne coordinate으로 고정한 뒤 (\([R | t] = [\boldsymbol I | \boldsymbol 0]\))<br/> 해당 coordinate 내에서 each input view \(v\) 마다 set \(\left\{ \mu_{j}^{v \rightarrow 1}, r_{j}^{v \rightarrow 1}, c_{j}^{v \rightarrow 1}, \alpha_{j}, s_{j} \right\}\) 을 예측<br/> where view \(1\) : canonical Gaussian space</li> <li>benefit : <ul> <li>global coordinate으로 explicitly transform할 필요가 없으므로 camera pose (extrinsic) 필요 없음</li> <li>explicitly transform-then-fuse하는 게 아니라 fuse 자체를 network로 학습하는 것이기 때문에<br/> 조화로운 global representation 가능</li> </ul> </li> </ul> </li> </ul> <h3 id="camera-intrinsic-embedding">Camera Intrinsic Embedding</h3> <ul> <li>Camera Intrinsic Embedding : <ul> <li>issue :<br/> only appearance에만 의존하여 3D recon.을 수행함<br/> <code class="language-plaintext highlighter-rouge">scale ambiguity</code> (scale misalignment) 문제 해결 필요!<br/> 필요한 geometric info.를 제공하기 위해!<br/> intrinsic \(k = [f_{x}, f_{y}, c_{x}, c_{y}]\)</li> <li>solve : <ul> <li>Trial 1) Global Intrinsic Embedding by Addition :<br/> intrinsic \(k\) 을 linear layer에 통과시킨 뒤 RGB image token에 add</li> <li>Trial 2) Global Intrinsic Embedding by Concat :<br/> intrinsic \(k\) 을 linear layer에 통과시킨 뒤 RGB image token에 concat</li> <li>Trial 3) Pixel-wise (Dense) Intrinsic Embedding :<br/> each pixel \(p_{j}\)에 대해 ray direction \(K^{-1} p_{j}\) 구한 뒤<br/> SH 이용해서 high-dim. feature로 변환한 뒤<br/> RGB image와 concat</li> </ul> </li> </ul> </li> </ul> <h3 id="training-and-inference">Training and Inference</h3> <ul> <li> <p>Loss :<br/> only photometric loss<br/> (linear comb. of MSE and LPIPS)</p> </li> <li>Relative Pose Estimation :<br/> canonical space에 3DGS들이 있다는 전제 하에<br/> <code class="language-plaintext highlighter-rouge">two-stage coarse-to-fine pipeline</code> <ul> <li>Coarse Stage :<br/> Gaussian center에 <code class="language-plaintext highlighter-rouge">PnP algorithm with RANSAC</code> (efficient as done in ms) 적용하여<br/> <code class="language-plaintext highlighter-rouge">initial rough pose estimate</code> 구하기</li> <li>Fine Stage :<br/> <code class="language-plaintext highlighter-rouge">3DGS param.을 freeze</code>한 채<br/> training에 사용했던 <code class="language-plaintext highlighter-rouge">photometric loss</code>를 이용해<br/> target view와 align되도록 rough <code class="language-plaintext highlighter-rouge">target camera pose를 optimize</code>(refine) <ul> <li>automatic diff.에서의 overhead를 줄이기 위해<br/> camera Jacobian을 계산 <d-cite key="GSslam">[5]</d-cite></li> </ul> </li> </ul> </li> <li>Evaluation-Time Pose Alignment : <ul> <li>unposed input images의 경우<br/> scene은 다른데 rendered two images는 같을 수 있으므로<br/> just two input views로 3D scene recon. 수행하는 건 사실 ambiguous</li> <li>GT camera pose를 이용하는 other baseline들 <d-cite key="pose1">[6]</d-cite>, <a href="https://semyeong-yu.github.io/blog/2024/pixelSplat/">7</a>과 비교하기 위해 (evaluation purpose)<br/> pose-free methods <d-cite key="nopose1">[8]</d-cite>, <d-cite key="nopose2">[9]</d-cite>의 경우 target view에 대한 camera pose를 optimize한 뒤 비교에 사용</li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <h3 id="implementation">Implementation</h3> <ul> <li>Experiment : <ul> <li>Dataset : <ul> <li>training :<br/> RE10K (RealEstate10k) : indoor real estate<br/> DL3DV : outdoor (camera motion pattern 더 다양)</li> <li>zero-shot generalization :<br/> ACID : nature scene by drone<br/> DTU<br/> ScanNet<br/> ScanNet++<br/> in-the-whild mobile phone capture<br/> SORA-generated images</li> </ul> </li> <li>camera overlap :<br/> SOTA dense feature matching method <d-cite key="ROMA">[10]</d-cite> 로<br/> input images’ camera overlap 정도를 측정하여<br/> small (0.05%-0.3%), medium (0.3%-0.55%), large (0.55%-0.8%)로 나눔</li> <li>Baseline : <ul> <li>pose-required novel-view-synthesis :<br/> pixelNeRF, AttnRend, pixelSplat, MVSplat</li> <li>pose-free novel-view-synthesis and relative pose estimation :<br/> DUSt3R, MASt3R, Splatt3R, CoPoNeRF, RoMa</li> </ul> </li> <li>Implementation :<br/> encoder, decoder, Gaussian center head는 MASt3R의 weights로 initialize하고<br/> (사실 scratch부터 training해도 성능 비슷하긴 함)<br/> Gaussian param head는 randomly initialize</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/5.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/5.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/5.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/5.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="result">Result</h3> <ul> <li>Novel View Synthesis : <ul> <li>SOTA pose-free (DUSt3R, MASt3R, Splatt3R) : <ul> <li>DUSt3R 계열은 <code class="language-plaintext highlighter-rouge">per-pixel depth loss</code>에 의존하기 때문에 each views를 <code class="language-plaintext highlighter-rouge">fuse하는 게 어렵</code><br/> 그래서 대부분 상황에서 NoPoSplat이 훨씬 더 좋음</li> </ul> </li> <li>SOTA pose-required (pixelSplat, MVSplat) : <ul> <li>pixelSplat, MVSplat은 <code class="language-plaintext highlighter-rouge">input view overlap이 작을 때 부정확한 geometry prior</code> (epipolar constraint, cost volume)을 사용하기 때문에<br/> image view overlap이 작은 상황에서는 NoPoSplat이 더 좋음</li> <li>pixelSplat, MVSplat은 <code class="language-plaintext highlighter-rouge">transform-then-fuse strategy</code>를 사용하는데 <code class="language-plaintext highlighter-rouge">misalignment</code>로 부정확할 수 있기 때문에<br/> canonical space에서 directly 예측하는 NoPoSplat이 더 좋을 수 있음</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/6.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/6.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/6.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/6.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/7.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/7.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/7.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/7.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Relative Pose Estimation :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/8.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/8.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/8.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/8.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Geometry Reconstruction : <ul> <li>SOTA pose-required (pixelSplat, MVSplat) : <ul> <li>pixelSplat, MVSplat은 explicitly transform-then-fuse하는 과정에서 두 input images의 경계 영역에서 misalignment (아래 그림에서 파란색 화살표로 표기) 가 있고,<br/> input views’ overlap이 적을 때는 geometry prior가 부정확해서 distortion (아래 그림에서 분홍색 화살표로 표기) 있는데,<br/> NoPoSplat은 canonical space에서 directly 예측하므로 해결</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/9.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/9.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/9.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/9.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Cross-Dataset Generalization :<br/> NoPoSplat은 geometry prior를 사용하지 않으므로 다양한 scene type에 adapt 가능<br/> 심지어 ScanNet++로의 zero-shot generalization에 대해 RE10K로 훈련시킨 NoPoSplat과 ScanNet++로 훈련시킨 pose-required Splatt3R을 비교했을 때 NoPoSplat이 더 좋음!</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/10.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/10.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/10.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/10.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Model Efficiency :<br/> NoPoSplat은 0.015초만에 (66 FPS) 3DGS 예측 가능<br/> (additional geometry prior 안 쓰니까 speed 빠름!)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/11.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/11.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/11.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/11.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> inference on RTX 4090 GPU </div> <ul> <li>In-the-Wild Unposed Images :<br/> 3D Generation task에 적용 가능!<br/> 먼저 text/image to multi-image/video model 이용해서 sparse scene-level multi-view images 얻은 뒤<br/> Ours (NoPoSplat) 이용해서 3D model 얻음</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/13.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/13.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/13.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/13.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="ablation-study">Ablation Study</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/12.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/12.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/12.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/12.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Ablation Study : <ul> <li><code class="language-plaintext highlighter-rouge">Output Canonical Gaussian Space</code> :<br/> transform-then-fuse pipeline of pose-required methods has <code class="language-plaintext highlighter-rouge">ghosting artifacts</code></li> <li><code class="language-plaintext highlighter-rouge">Camera Intrinsic Embedding</code> :<br/> no intrinsic leads to <code class="language-plaintext highlighter-rouge">blurry</code> results due to <code class="language-plaintext highlighter-rouge">scale ambiguity</code><br/> 실험적으로 intrinsic token concat. 방식이 best</li> <li><code class="language-plaintext highlighter-rouge">RGB Shortcut</code> :<br/> no RGB Shortcut leads to <code class="language-plaintext highlighter-rouge">blurry</code> results in texture-rich areas<br/> (위 그림의 quilt in row 1 and chair in row 3)</li> <li><code class="language-plaintext highlighter-rouge">3 Input Views</code> instead of 2 :<br/> baselines과의 공평한 비교를 위해 NoPoSplat은 two input-views setting을 사용했는데<br/> three input-views를 사용할 경우 성능이 훨씬 좋아졌음!</li> </ul> </li> </ul> <h2 id="conclusion">Conclusion</h2> <ul> <li> <p>Future Work :<br/> NoPoSplat은 static scene에만 적용했는데, dynamic scene에 NoPoSplat의 pipeline을 확장 적용!</p> </li> <li> <p>Limitation :</p> <ul> <li><code class="language-plaintext highlighter-rouge">camera intrinsic은 known</code>이라는 걸 가정!</li> <li>feed-forward model은 <code class="language-plaintext highlighter-rouge">non-generative</code>하므로 <code class="language-plaintext highlighter-rouge">unseen region</code>에는 대응 못 함</li> <li><code class="language-plaintext highlighter-rouge">static scene</code>에 적용</li> </ul> </li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br/> 사실 NoPoSplat은 camera pose 이용한 global coordinate으로의 explicit transform이나 geometry prior (epopiolar constraint, cost volume 등)나 GT depth 없이<br/> 오로지 implicit network의 학습에 의존하여 scene recon. 능력을 학습하겠다는 건데<br/> photometric loss만으로도 잘 학습이 되나? two input images 경계면의 smoothness 등 추가 regularization loss 추가해주는 게 낫지 않음?</p> </li> <li> <p>A1 :<br/> TBD</p> </li> <li> <p>Q2 :<br/> photometric loss에만 의존하기 때문에 ViT semantic info. 말고도 more info. 주기 위해 intrinsic과 RGB shortcut을 사용하는데<br/> 둘 말고 또 추가하면 좋은 거 있을까?</p> </li> <li> <p>A2 :<br/> TBD</p> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="dynamic"/><category term="GS"/><category term="SfMfree"/><summary type="html"><![CDATA[Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images (ICLR 2025)]]></summary></entry><entry><title type="html">MonST3R</title><link href="https://semyeong-yu.github.io/blog/2025/MonST3R/" rel="alternate" type="text/html" title="MonST3R"/><published>2025-01-22T10:00:00+00:00</published><updated>2025-01-22T10:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/MonST3R</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/MonST3R/"><![CDATA[<h2 id="monst3r---a-simple-approach-for-estimating-geometry-in-the-presence-of-motion">MonST3R - A Simple Approach for Estimating Geometry in the Presence of Motion</h2> <h4 id="junyi-zhang-charles-herrmann-junhwa-hur-varun-jampani-trevor-darrell-forrester-cole-deqing-sun-ming-hsuan-yang">Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, Ming-Hsuan Yang</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2410.03825">https://arxiv.org/abs/2410.03825</a><br/> project website :<br/> <a href="https://monst3r-project.github.io/">https://monst3r-project.github.io/</a></p> </blockquote> <h2 id="contribution">Contribution</h2> <p><code class="language-plaintext highlighter-rouge">static scene에 사용됐던 DUSt3R를 dynamic scene에 확장한 버전!</code></p> <ul> <li>geometry-first approach that <code class="language-plaintext highlighter-rouge">directly</code> estimates <code class="language-plaintext highlighter-rouge">per-timestep geometry (pointmap)</code> of <code class="language-plaintext highlighter-rouge">dynamic</code> scene <ul> <li>이전까지의 논문들은 <d-cite key="GaussianMarbles">[1]</d-cite>, <d-cite key="TrackRecon">[2]</d-cite>, <d-cite key="Kumar">[3]</d-cite>, <d-cite key="Barsan">[4]</d-cite>, <d-cite key="Mustafa">[5]</d-cite>, <d-cite key="Lei">[6]</d-cite>, <d-cite key="Chu">[7]</d-cite>, <d-cite key="Wangb">[8]</d-cite>, <d-cite key="Wanga">[9]</d-cite>, <d-cite key="Liu">[10]</d-cite> 처럼<br/> depth, optical flow, trajectory estimation을 사용하는 subtasks로 쪼갠 뒤<br/> global optimization 또는 multi-stage pipeline 등으로 합치는<br/> complex system을 쓰는데,<br/> 이는 보통 느리고, 다루기 힘들고, prone-to-error at each step</li> <li>이전까지의 논문들은 motion과 geometry를 함께 사용하여 dynamic scene을 다뤘는데,<br/> motion, depth label, camera pose 정보가 있는 GT dynamic video data는 거의 없다<br/> (그래서 다른 model(prior)를 쓰는데, 이는 부정확성이 쌓일 수 있음)</li> <li>대신 본 논문은<br/> limited data로 only DUSt3R의 decoder and head만 fine-tuning하여 (<code class="language-plaintext highlighter-rouge">small-scale fine-tuning</code>)<br/> <code class="language-plaintext highlighter-rouge">explicit motion representation 없이</code><br/> only <code class="language-plaintext highlighter-rouge">geometry</code> (pointmap)를 <code class="language-plaintext highlighter-rouge">directly</code> 예측하는 pipeline 제시!</li> <li>each timestep마다 DUSt3R 방식으로 pointmap (geometry) 예측한 뒤<br/> 같은 camera coordinate frame (global pointmap)에 대해 <code class="language-plaintext highlighter-rouge">3D align</code></li> <li>downstream tasks :<br/> 예측한 pointmap (geometry) 를 바탕으로<br/> feed-forward 4D reconstruction 뿐만 아니라<br/> video depth estimation, camera pose estimation, video segmentation 등<br/> 여러 downstream video-specific tasks에 적용</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Final Loss</code> :<br/> \(\hat X = \text{argmin}_{X, P_{W}, \sigma} L_{align} (X, \sigma, P_{W}) + w_{smooth} L_{smooth} (X) + w_{flow} L_{flow} (X)\) <ul> <li>세 가지 loss :<br/> 3D alignment loss \(L_{align}\), camera trajectory smoothness loss \(L_{smooth}\), flow projection loss \(L_{flow}\)</li> <li>learnable param. :<br/> <code class="language-plaintext highlighter-rouge">global pointmap</code> \(X\), global pointmap으로의 <code class="language-plaintext highlighter-rouge">3D alignment transformation</code> \(P_{W}\), <code class="language-plaintext highlighter-rouge">scale factor</code> \(\sigma\) 를 업데이트하는데,<br/> 얘네는 본질적으로 re-parameterization에 의해 <code class="language-plaintext highlighter-rouge">depthmap</code> \(\hat D\), <code class="language-plaintext highlighter-rouge">extrinsic</code> \(\hat P\), <code class="language-plaintext highlighter-rouge">intrinsic</code> \(\hat K\) 로 구성되어 있음<br/> 즉, MonST3R는 결국 jointly optimize video depthmap and camera pose (extrinsic, intrinsic)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/2.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/2.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="related-works">Related Works</h2> <ul> <li>DUSt3R :<br/> DUSt3R를 바로 dynamic scene에 적용할 경우 두 가지 한계 발생 <ul> <li>문제 1) (static scene인 것처럼) fg object에 align하여 bg가 misaligned<br/> DUSt3R는 static scene으로만 학습됐기 때문에<br/> dynamic scene의 pointmaps를 알맞게 align하지 못하여<br/> moving fg object가 가만히 있는 것처럼 align되고<br/> static bg element는 misaligned</li> <li>문제 2) fg object의 geometry(depth)를 잘 예측하지 못하여 fb object를 bg에 둠</li> <li>해결)<br/> domain mismatch이므로 다시 train!<br/> 본 논문은 limited data를 최대한 사용하여 small-scale fine-tuning하는 training strategy 제시</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/3.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/3.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>motion mask :<br/> DUSt3R는 static scene으로 훈련되었기 때문에 dynamic scene에 적용하기 위해<br/> GT motion mask를 사용할 수도 있다 <ul> <li>inference할 때<br/> image의 dynamic region은 black pixels로 대체하고<br/> corresponding tokens는 mask tokens로 대체하여<br/> dynamic objects를 masking out 할 수도 있는데,<br/> black pixels와 mask tokens는 out-of-distribution w.r.t training 이므로<br/> pose estimation 결과가 안 좋아짐</li> <li>본 논문은 그렇게 무작정 dynamic region을 mask out 하지 않고 이 문제 해결!</li> </ul> </li> </ul> <h2 id="architecture">Architecture</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/1.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/1.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/1.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/1.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="method">Method</h2> <h3 id="main-idea">Main Idea</h3> <p>DUSt3R의 아이디어를 그대로 가져오고,<br/> DUSt3R의 각 output pointmap \(X^{t} \in R^{W \times H \times 3}\) 이 time 정보 \(t\) 를 가지고 있음</p> <h3 id="training-dataset">Training Dataset</h3> <p>real-world dynamic scene은 보통 GT camera pose를 가지고 있지 않으므로<br/> SfM 등 sensor measurement 또는 post-processing을 통해 추정하는데<br/> 이는 부정확할 수 있고 costly하므로<br/> 본 논문은 GT camera pose, depth 정보를 알 수 있는 synthetic datasets를<br/> dynamic fine-tuning을 위한 training dataset으로 사용</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/4.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/4.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Training Dataset for Dynamic Fine-Tuning :<br/> PointOdyssey는 dynamic objects 많아서 많이 사용하고<br/> TartanAir는 static scene이라서 적게 사용하고<br/> Waymo는 specialized domain이라서 적게 사용 <ul> <li>3 synthetic datasets : <ul> <li>PointOdyssey (Zheng et al.)</li> <li>TartanAir (Wang et al.)</li> <li>Spring (Mehl et al.)</li> </ul> </li> <li>1 real-world dataset : <ul> <li>Waymo (Sun et al.) with LiDAR</li> </ul> </li> </ul> </li> </ul> <h3 id="training-strategy">Training Strategy</h3> <p>dataset이 small-scale이므로<br/> data efficiency를 극대화시키기 위해<br/> 다양한 training techniques 사용</p> <ul> <li>Training Strategies : <ul> <li>전략 1)<br/> encoder는 freeze한 뒤<br/> network의 decoder와 prediction head만 fine-tune<br/> (encoder(CroCo)의 geometric knowledge는 유지)</li> <li>전략 2)<br/> each video마다 temporal stride 1~9 만큼 떨어진 two frames를 sampling하여 input pair로 사용하는데,<br/> stride가 클수록 sampling prob.도 linearly 큼<br/> \(\rightarrow\)<br/> 서로 더 멀리 떨어진 frame pair, 즉 large motion에 more weights 부여</li> <li>전략 3)<br/> Field-of-View augmentation (center crop with various image scales) 사용하여<br/> 다양한 camera intrinsics에도 일반화 가능하도록!<br/> (training videos에는 해당 variation이 흔하지 않음)</li> </ul> </li> </ul> <h3 id="dynamic-global-point-clouds-and-camera-pose">Dynamic Global Point Clouds and Camera Pose</h3> <p>frame 수가 많기 때문에<br/> pairwise pointmap 들로부터 직접 하나의 dynamic global point cloud를 추출하는 건 어렵.<br/> 지금부터 pairwise model을 이용해서<br/> <code class="language-plaintext highlighter-rouge">dynamic global pcd</code> \(\hat X\) 와 <code class="language-plaintext highlighter-rouge">camera pose</code> \(\hat K, \hat P = [\hat R | \hat T]\) 를 <code class="language-plaintext highlighter-rouge">동시에</code> optimize하는 방법을 소개하겠다</p> <ul> <li>Video Graph : <ul> <li>DUSt3R의 경우<br/> global alignment를 위해 모든 frame pair에 대해 connectivity graph를 만드는데,<br/> dynamic scene video에 대해 이렇게 graph 만드려면 too expensive</li> <li>계산량 줄이기 위해<br/> 전체 frames에 대해 graph를 만드는 게 아니라<br/> video의 <code class="language-plaintext highlighter-rouge">sliding temporal window</code> 내에 있는 frames에 대해 <code class="language-plaintext highlighter-rouge">국소적인 graph</code> 만듦</li> <li>sliding temporal window 내에 있는 모든 each frame pair<br/> \((t, t') \in W^{t} = {(a, b) | a, b \in [t, \ldots, t + w], a \neq b}\) 에 대해<br/> (\(w\) : temporal window size)<br/> MonST3R로 pairwise pointmap을 구하고,<br/> off-the-shelf method로 optical flow 구함</li> <li>runtime 줄이기 위해 strided sampling 적용</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/5.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/5.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/5.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/5.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Dynamic Global Point Cloud and Pose Optimization : <ul> <li>goal :<br/> 모든 pairwise pointmaps를 <code class="language-plaintext highlighter-rouge">같은 global coordinate frame</code>에 모아서 <code class="language-plaintext highlighter-rouge">world-coordinate pointmap</code> \(X^{t} \in R^{H \times W \times 3}\) 만들기</li> <li>re-parameterization : <ul> <li>notation :<br/> \(P^{t} = [R^{t} | T^{t}]\) : extrinsic camera pose<br/> \(K^{t}\) : intrinsic<br/> \(D^{t}\) : per-frame depthmap</li> <li>global pointmap \(X^{t}\) :<br/> <code class="language-plaintext highlighter-rouge">depthmap, intrinsic, extrinsic</code> 을 이용하여 parameterize <code class="language-plaintext highlighter-rouge">global pointmap</code><br/> \(X_{i,j}^{t} = P^{t^{-1}} h (K^{t^{-1}} [i D_{i,j}^{t} ; j D_{i,j}^{t} ; D_{i,j}^{t}])\) <ul> <li>intrinsic \(K^{t^{-1}}\) :<br/> depthmap 정보를 2D pixel-coordinate \((i, j)\) 에서 3D camera-coordinate으로 변환한 뒤</li> <li>homogeneous mapping \(h(\cdot)\) :<br/> homogeneous-coordinate으로 변환한 뒤<br/> (\(R^{t}, T^{t}\) 를 하나의 행렬로 표현 가능하도록 하여 just 행렬 곱셈을 통해 변환 가능)</li> <li>extrinsic \(P^{t^{-1}}\) :<br/> world-coordinate으로 변환</li> </ul> </li> </ul> </li> <li>loss :<br/> dynamic scene video이기 때문에 DUSt3R의 3D alignment loss 뿐만 아니라 두 가지 video-specific loss 추가 <ul> <li>Loss 1) DUSt3R의 <code class="language-plaintext highlighter-rouge">3D alignment</code> loss : <ul> <li>goal :<br/> <code class="language-plaintext highlighter-rouge">각 pairwise pointmap</code> \(X^{t; t \leftarrow t'}\), \(X^{t'; t \leftarrow t'}\) 을<br/> <code class="language-plaintext highlighter-rouge">world-coordinate</code>의 <code class="language-plaintext highlighter-rouge">global pointmap</code> \(X^{t}\) 에 <code class="language-plaintext highlighter-rouge">align</code>시키는<br/> single rigid transformation \(P^{t;e}\)<br/> (\(X^{t; t \leftarrow t'}\) 와 \(X^{t'; t \leftarrow t'}\) 는 둘 다 <code class="language-plaintext highlighter-rouge">이미 같은 camera-coordinate</code> (\(t\) 의 frame) 에 align되어 있으므로<br/> \(X^{t; t \leftarrow t'}\) 을 global pointmap에 align시키는 \(P\) 와<br/> \(X^{t'; t \leftarrow t'}\) 을 global pointmap에 align시키는 \(P\) 는 같음)</li> <li>how :<br/> \(L_{align}(X, \sigma, P_{W}) = \sum_{W^{i} \in W} \sum_{e \in W} \sum_{t \in e} \| C^{t; e} \cdot (X^{t} - \sigma^{e} P^{t;e} X^{t;e}) \|_{1}\) <ul> <li>notation :<br/> \(W^{i} \in W\) : each sliding temporal window<br/> \(e = (t, t') \in W^{i}\) : each frame pair within the window<br/> \(t \in e\) : each frame<br/> \(\sigma^{e}\) : frame 크기 차이를 보정하는 per-(frame pair) scale factor<br/> \(P_{W}\) : sliding temporal window 내의 여러 frame pair에 대한 3D alignment transformation 집합</li> </ul> </li> </ul> </li> <li>Loss 2) <code class="language-plaintext highlighter-rouge">camera trajectory smoothness</code> loss : <ul> <li>goal :<br/> nearby timestep에 대해 \(R, T\) 가 크게 변하지 않도록 하여<br/> <code class="language-plaintext highlighter-rouge">시간에 따라 camera motion (extrinsic) 이 smooth</code>하도록</li> <li>how :<br/> \(L_{smooth}(X) = \sum_{t=0}^{N} (\| R^{t^{T}} R^{t+1} - I \|_{f} + \| R^{t^{T}} (T^{t+1} - T^{t}) \|_{2})\)</li> </ul> </li> <li>Loss 3) <code class="language-plaintext highlighter-rouge">flow projection</code> loss : <ul> <li>goal :<br/> confident <code class="language-plaintext highlighter-rouge">static region</code>에 대해<br/> global pointmaps \(X^{t}\) 와 camera poses \(K^{t}, R^{t}, T^{T}\), 즉 <code class="language-plaintext highlighter-rouge">camera motion만으로 계산한 optical flow</code>가<br/> <code class="language-plaintext highlighter-rouge">off-the-shelf method가 내놓은 optical flow</code>와 consistent하도록</li> <li>how :<br/> \(L_{flow}(X) = \sum_{W^{i} \in W} \sum_{t \rightarrow t' \in W^{i}} \| S^{global; t \rightarrow t'} \cdot (F_{cam}^{global; t \rightarrow t'} - F_{est}^{t \rightarrow t'}) \|_{1}\) <ul> <li>\(S^{global; t \rightarrow t'}\) : static region에 대해 loss term 걸어줌<br/> (static mask 구하는 방법 : 아래의 Confident Static Regions 섹션에서 설명!)<br/> (\(X^{t}\) 가 learnable 하므로 학습 중에 계속 updated)</li> <li>\(F_{cam}^{global; t \rightarrow t'}\) : global pointmap \(X^{t}\) 에 camera motion (intrinsic, extrinsic)을 적용하여 계산한 optical flow field</li> <li>\(F_{est}^{t \rightarrow t'}\) : off-the-shelf method가 내놓은 optical flow field</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Final Loss</code> :<br/> \(\hat X = \text{argmin}_{X, P_{W}, \sigma} L_{align} (X, \sigma, P_{W}) + w_{smooth} L_{smooth} (X) + w_{flow} L_{flow} (X)\) <ul> <li>learnable param. :<br/> <code class="language-plaintext highlighter-rouge">global pointmap</code> \(X\), global pointmap으로의 <code class="language-plaintext highlighter-rouge">3D alignment transformation</code> \(P_{W}\), <code class="language-plaintext highlighter-rouge">scale factor</code> \(\sigma\) 를 업데이트하는데,<br/> 얘네는 본질적으로 re-parameterization에 의해 <code class="language-plaintext highlighter-rouge">depthmap</code> \(\hat D\), <code class="language-plaintext highlighter-rouge">extrinsic</code> \(\hat P\), <code class="language-plaintext highlighter-rouge">intrinsic</code> \(\hat K\) 로 구성되어 있음<br/> 즉, MonST3R는 결국 jointly optimize video depthmap and camera pose (extrinsic, intrinsic)</li> <li>\(w_{smooth} = 0.01, w_{flow} = 0.01\)<br/> (\(L_{flow} \lt 20\) 일 때, 즉 camera poses를 roughly align한 뒤에 \(L_{flow}\) 활성화함)<br/> (\(L_{flow} \gt 50\) 일 때, 즉 초기에 motion mask is updated)</li> </ul> </li> </ul> <h2 id="downstream-applications">Downstream Applications</h2> <h3 id="intrinsics-and-relative-pose-estimation">Intrinsics and Relative Pose Estimation</h3> <ul> <li> <p>Intrinsic Pose Estimation :<br/> time \(t\) 에서의 pointmap \(X^{t}\) 을 이용해서<br/> 2D image와 3D pointmap이 align되도록 하는<br/> focal length \(f^{t}\) 를 추정함으로써<br/> camera intrinsic \(K^{t}\) 추정</p> </li> <li> <p>Relative Pose Estimation :<br/> DUSt3R와 달리 dynamic objects는<br/> epipolar matrix 또는 Procrustes alignment를 위한 가정(<code class="language-plaintext highlighter-rouge">???</code>)들에 위배<br/> \(\rightarrow\)<br/> 대신 주어진 3D point와 corresponding 2D point를 바탕으로 추정하는 PnP algorithm(<code class="language-plaintext highlighter-rouge">???</code>)과<br/> random sampling 방식의 RANSAC algorithm(<code class="language-plaintext highlighter-rouge">???</code>) 사용<br/> (dynamic scene이어도 대부분의 pixels는 static할 것이므로<br/> randomly-sampled points는 static elements에 더 가중치를 두기 때문에<br/> relative pose는 inliers(static)로 robustly estimate 가능)</p> </li> </ul> <h3 id="confident-static-regions">Confident Static Regions</h3> <ul> <li>Static Mask :<br/> 단순하게 <code class="language-plaintext highlighter-rouge">두 optical flow field가 일치하는 (차이가 적은) 영역</code>을 <code class="language-plaintext highlighter-rouge">Static Region</code>으로 간주!<br/> static mask \(S^{t \rightarrow t'} = [\alpha \gt \| F_{cam}^{t \rightarrow t'} - F_{est}^{t \rightarrow t'} \|_{1}]\)<br/> (이 confident static mask를 나중에 global pose optimization에도 사용할 거임!) <ul> <li>\(F_{cam}^{t \rightarrow t'}\) :<br/> <code class="language-plaintext highlighter-rouge">camera motion</code>만으로 optical flow 추정 <ul> <li>Step 1)<br/> frame pair \(I^{t}\), \(I^{t'}\) 로부터<br/> pointmaps \(X^{t; t \leftarrow t'}\), \(X^{t'; t \leftarrow t'}\) 와 pointmaps \(X^{t; t' \leftarrow t}\), \(X^{t'; t' \leftarrow t}\) 추정</li> <li>Step 2)<br/> 위에서 언급한 방법으로 Intrinsics \(K^{t}, K^{t'}\) 와 Relative Pose \(R^{t \rightarrow t'}, T^{t \rightarrow t'}\) 추정</li> <li>Step 3)<br/> only camera motion \(t \rightarrow t'\) 이용해서<br/> optical flow field \(F_{cam}^{t \rightarrow t'}\) 추정<br/> \(F_{cam}^{t \rightarrow t'} = \pi (D^{t; t \leftarrow t'} K^{t'} R^{t \rightarrow t'} K^{t^{-1}} \hat x + K^{t'} T^{t \rightarrow t'}) - x\) <ul> <li>notation :<br/> \(x\) : pixel-coordinate<br/> \(\hat x\) : homogeneous coordinate<br/> \(\pi(\cdot)\) : projection operation \((x,y,z) \rightarrow (\frac{x}{z}, \frac{y}{z})\)<br/> \(D^{t; t \leftarrow t'}\) : estimated depth from pointmap \(X^{t; t \leftarrow t'}\)</li> <li>Step 3-1)<br/> intrinsic 이용하여 frame \(t\) 에 대해 2D에서 3D로 backproject</li> <li>Step 3-2)<br/> camera motion (relative camera pose) \(t \rightarrow t'\) 적용<br/> (\(R^{t \rightarrow t'} X + T^{t \rightarrow t'}\))</li> <li>Step 3-3)<br/> intrinsic 이용하여 frame \(t'\) 에 대해 다시 3D에서 2D image coordinate으로 project</li> </ul> </li> </ul> </li> <li>\(F_{est}^{t \rightarrow t'}\) :<br/> <code class="language-plaintext highlighter-rouge">off-the-shelf method</code> <d-cite key="SEARAFT">[11]</d-cite> 이용해서 optical flow 추정</li> </ul> </li> </ul> <h3 id="video-depth">Video Depth</h3> <p>optimal global pointmap \(\hat X\) 자체가 re-parameterization에 의해<br/> per-frame depthmap \(\hat D\) 로 이루어져 있고,<br/> just \(\hat D\) 자체가 video depth</p> <h2 id="experiment">Experiment</h2> <h3 id="results">Results</h3> <ul> <li>Single-Frame and Video <code class="language-plaintext highlighter-rouge">Depth Estimation</code> : <ul> <li>baseline : <ul> <li>video depth method :<br/> NVDS <d-cite key="NVDS">[12]</d-cite><br/> ChronoDepth <d-cite key="Chrono">[13]</d-cite><br/> DepthCrafter <d-cite key="DepthCrafter">[14]</d-cite></li> <li>single-frame depth method :<br/> Depth-Anything-V2 <d-cite key="DepthAnything">[15]</d-cite><br/> Marigold <d-cite key="Marigold">[16]</d-cite><br/> DUSt3R <a href="https://semyeong-yu.github.io/blog/2024/DUSt3R/">blog</a></li> <li>joint video depth and pose estimation method :<br/> CasualSAM <d-cite key="CasualSAM">[17]</d-cite><br/> Robust-CVD <d-cite key="RobustCVD">[18]</d-cite></li> </ul> </li> <li>benchmark dataset : <ul> <li>video depth :<br/> KITTI<br/> Sintel<br/> Bonn</li> <li>monocular single-frame depth :<br/> NYU-v2</li> </ul> </li> <li>metric : <d-cite key="DepthCrafter">[14]</d-cite>, <d-cite key="DepthAnything">[15]</d-cite> 에서처럼<br/> Abs Rel : absolute relative error<br/> \(\sigma \lt 1.25\) : percentage of inlier points <code class="language-plaintext highlighter-rouge">???</code><br/> (All methods output scale- and/or shift- invariant depth estimates. For video depth evaluation, we align a single scale and/or shift factor per each sequence, whereas the single-frame evaluation adopts per-frame median scaling, following DUSt3R) <code class="language-plaintext highlighter-rouge">???</code></li> <li>results : <ul> <li>video depth :<br/> MonST3R는 specialized video depth estimation techniques와 유사한 성능</li> <li>single-frame depth :<br/> DUSt3R 구조를 dynamic scene’s video에 대해 fine-tuning했는데도<br/> single-frame depth estimation에 대해 여전히 DUSt3R와 유사한 성능</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/6.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/6.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/6.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/6.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Camera Pose Estimation</code> :<br/> obtain camera trajectories <ul> <li>baseline : <ul> <li>joint video depth and pose estimation method (경쟁자들) :<br/> CasualSAM <d-cite key="CasualSAM">[17]</d-cite><br/> Robust-CVD <d-cite key="RobustCVD">[18]</d-cite></li> <li>learning-based visual odometry method :<br/> DROID-SLAM (GT intrinsic 필요)<br/> Particle-SfM (Ours보다 5배 느림)<br/> DPVO (GT intrinsic 필요)<br/> LEAP-VO (GT intrinsic 필요)</li> <li>DUSt3R with GT motion mask :<br/> 단순히 dynamic region의 pixel과 token을 mask out<br/> (Related Works 섹션의 motion mask에서 설명함)</li> </ul> </li> <li>benchmark dataset :<br/> Sintel<br/> TUM-dynamics<br/> ScanNet</li> <li>metric : Particle-SfM, LEAP-VO에서처럼<br/> Sim(3) Umeyama alignment 적용한 뒤 <code class="language-plaintext highlighter-rouge">???</code><br/> Absolute Translation Error (ATE)<br/> Relative Translation Error (RPE trans)<br/> Relative Rotation Error (RPE rot)</li> <li>results :<br/> joint depth and pose estimation methods 중에 제일 성능 좋고<br/> GT intrinsic 필요 없는데도 pose-specific methods와 유사한 성능</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/7.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/7.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/7.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/7.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/8.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/8.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/8.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/8.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="ablation-study">Ablation Study</h3> <ul> <li>Ablation Study : <ul> <li>training dataset :<br/> datasets 섞어 쓰면 camera pose estimation에 도움!</li> <li>fine-tuning strategy :<br/> only decoder and head만 fine-tuning하는 게 다른 training strategies보다 나음!</li> <li>loss :<br/> 본 논문에서 언급한 세 가지 loss (\(L_{align}, L_{smooth}, L_{flow}\)) 는<br/> video depth accuracy를 크게 해치지 않으면서 pose estimation accuracy를 높임!</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/9.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/9.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/9.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/9.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="limitation">Limitation</h2> <ul> <li>Limitation : <ul> <li>이론적으로는 dynamic camera intrinsics를 estimate할 수 있지만,<br/> 사실상 이는 <code class="language-plaintext highlighter-rouge">careful hyperparameter tuning</code>과 <code class="language-plaintext highlighter-rouge">manual constraints</code>를 필요로 함</li> <li><code class="language-plaintext highlighter-rouge">out-of-distribution inputs</code>에 struggle<br/> e.g. 건물 내부 또는 도심 야외 등으로 훈련한 경우 넓은 공터 같은 새로운 scene에 대해서는 제대로 작동 안함 <ul> <li>해결법 :<br/> training set을 확장하면 MonST3R가 in-the-wild videos에 대해서도 더 robust해질 듯</li> </ul> </li> </ul> </li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br/> 왜 static region에 대해서만 optical flow가 consistent하도록 하는 flow projection loss 적용함?</p> </li> <li> <p>A1 :<br/> ddd</p> </li> <li> <p>Q2 :<br/> Gaussian Marbles에서는 frame끼리 divide-and-conquer로 merge하면서 frame 전후 관계를 trajectory로 연결하여 temporal info.를 이용함.<br/> MonST3R는 각 timestep의 pointmap을 global pointmap으로 align하는데 camera trajectory smoothness loss 말고 3D alignment loss에서 frame 전후 관계, 즉 temporal info.를 이용함?</p> </li> <li> <p>A2 :<br/> TBD</p> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="dynamic"/><category term="GS"/><category term="geometry"/><category term="SfMfree"/><summary type="html"><![CDATA[A Simple Approach for Estimating Geometry in the Presence of Motion (ICLR 2025)]]></summary></entry><entry><title type="html">Dynamic Gaussian Marbles</title><link href="https://semyeong-yu.github.io/blog/2025/GaussianMarbles/" rel="alternate" type="text/html" title="Dynamic Gaussian Marbles"/><published>2025-01-16T12:00:00+00:00</published><updated>2025-01-16T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/GaussianMarbles</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/GaussianMarbles/"><![CDATA[<h2 id="dynamic-gaussian-marbles-for-novel-view-synthesis-of-casual-monocular-videos">Dynamic Gaussian Marbles for Novel View Synthesis of Casual Monocular Videos</h2> <h4 id="colton-stearns-adam-harley-mikaela-uy-florian-dubost-federico-tombari-gordon-wetzstein-leonidas-guibas">Colton Stearns, Adam Harley, Mikaela Uy, Florian Dubost, Federico Tombari, Gordon Wetzstein, Leonidas Guibas</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2406.18717">https://arxiv.org/abs/2406.18717</a><br/> project website :<br/> <a href="https://geometry.stanford.edu/projects/dynamic-gaussian-marbles.github.io/">https://geometry.stanford.edu/projects/dynamic-gaussian-marbles.github.io/</a></p> </blockquote> <h2 id="contribution">Contribution</h2> <ul> <li>Dynamic 4D Gaussian Splatting : <ul> <li>이전까지는 multiple simultaneous viewpoints of a scene 세팅 (dense multi-camera setup)에서의 recon. 논문들이 많았음<br/> \(\rightarrow\)<br/> 평소의 casual <code class="language-plaintext highlighter-rouge">monocular video</code> (challenging)로 4D recon.을 수행해보자!</li> <li>input에 multi-view info.가 없는 underconstrained monocular video더라도<br/> prior (<code class="language-plaintext highlighter-rouge">careful optimization strategy</code> 및 <code class="language-plaintext highlighter-rouge">off-the-shelf depth and motion estimation</code> 및 <code class="language-plaintext highlighter-rouge">geometry-based regularization</code>) 이용해서<br/> 적절한 constraint를 복원할 수 있다!</li> </ul> </li> <li>Dynamic Gaussian Marbles :<br/> monocular setting의 어려움을 해결하기 위해 GS에서 세 가지 사항을 변경<br/> 이를 통해 Gaussian trajectories를 학습할 수 있음 <ul> <li>isotropic Gaussian Marbles :<br/> <code class="language-plaintext highlighter-rouge">isotropic</code> Gaussian을 사용함으로써<br/> Gaussian의 <code class="language-plaintext highlighter-rouge">degrees of freedom을 줄이고</code><br/> <code class="language-plaintext highlighter-rouge">local shape보다는 motion과 apperance</code> 표현하는 데 더 집중하도록 제한</li> <li>hierarchical divide-and-conquer learning strategy :<br/> time 길이가 어느 정도 짧아야 잘 포착할 수 있으므로 <br/> long video를 short <code class="language-plaintext highlighter-rouge">subsequences로 나누고 optimize by iteratively merging the subsequences</code><br/> (long-sequence tracking 대신 인접한 subsequences를 붙이는 task로!) <ul> <li>procedure :<br/> 아래의 과정을 반복하며 locality와 global coherence를 모두 챙김! <ul> <li><code class="language-plaintext highlighter-rouge">motion estimation</code> :<br/> \(G^{b}\) 의 frame을 \(G^{a}\) 의 trajectory에 하나씩 더해 가며 motion estimation을 수행하므로 <d-cite key="Dynamic3DGS">[1]</d-cite> 처럼 <code class="language-plaintext highlighter-rouge">locality</code>와 smoothness로부터 benefit</li> <li><code class="language-plaintext highlighter-rouge">merge</code></li> <li><code class="language-plaintext highlighter-rouge">global adjustment</code> : <d-cite key="4DGS">[2]</d-cite> 처럼 <code class="language-plaintext highlighter-rouge">global coherence</code>라는 benefit</li> </ul> </li> </ul> </li> <li>prior :<br/> monocular video로도 recon. 잘 수행하기 위해 prior 이용 <ul> <li><code class="language-plaintext highlighter-rouge">image(2D)-space prior</code> : SAM (Rendering loss-segmentation), CoTracker (Tracking loss), DepthAnything (Rendering loss-depthmap)</li> <li><code class="language-plaintext highlighter-rouge">geometry(3D)-space prior</code> : regularization of Gaussian trajectories with rigidity (Isometry loss) and Chamfer priors (3D Alignment loss)</li> </ul> </li> </ul> </li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li> <p>Gaussian Splatting :<br/> TBD</p> </li> <li> <p>Dynamic Gaussian Splatting :<br/> TBD</p> </li> <li> <p>Other Dynamic Nerual Scene Representations :<br/> TBD</p> </li> </ul> <h2 id="method">Method</h2> <h3 id="overview">Overview</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/1.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/1.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/1.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/1.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/3.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/3.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="dynamic-gaussian-marbles">Dynamic Gaussian Marbles</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/2.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/2.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> simpler Gaussian Marble의 경우에만 generalize well to novel view </div> <ul> <li>Gaussian marble : <ul> <li><code class="language-plaintext highlighter-rouge">isotropic</code> :<br/> \(R = I\) and \(S = s \in R^{1}\) <ul> <li>anisotropic Gaussian은 expensive할 뿐만 아니라<br/> underconstrained monocular cam. setting에서는 오히려 degrees of freedom 많으면 poor하다는 걸 실험적으로 발견</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">semantic instance</code> :<br/> assign each Gaussian marble to semantic instance \(y \in N\) by SAM-driven TrackAnything</li> <li><code class="language-plaintext highlighter-rouge">dynamic trajectory</code> :<br/> trajectory \(\Delta X \in R^{T \times 3}\) : a sequence of translations which maps marble’s position change at each timestep</li> </ul> </li> </ul> <h3 id="divide-and-conquer-motion-estimation">Divide-and-Conquer Motion Estimation</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/3.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/3.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Training Procedure : <ul> <li>Step 1) <code class="language-plaintext highlighter-rouge">initialization for each frame</code><br/> initialize Gaussian marbles \([G_{11}, G_{22}, \ldots, G_{TT}]\) for each frame<br/> (initial marbles \(G_{ii}\) have trajectory length 1) <ul> <li>Step 1-1) obtain prior (<code class="language-plaintext highlighter-rouge">depthmap</code> and <code class="language-plaintext highlighter-rouge">segmentation</code>)<br/> obtain monocular (LiDAR) depthmap and segmentation from SAM-driven TrackAnything <d-cite key="TrackAnything">[3]</d-cite></li> <li>Step 1-2) <code class="language-plaintext highlighter-rouge">unproject</code> from 2D to 3D<br/> unproject the depthmap into point cloud<br/> perform outlier removal and downsampling</li> <li>Step 1-3) initialize Gaussian marbles and trajectory <ul> <li>Gaussian marbles : <ul> <li>mean \(\mu \in R^{3}\) : Step 1-2)에서 얻은 pcd</li> <li>color \(c \in R^{3}\) : pixel color (pixel-aligned Gaussians)</li> <li><code class="language-plaintext highlighter-rouge">instance class</code> \(y \in R^{1}\) : Step 1-1)에서 얻은 segmentation</li> <li>scale \(s \in R^{1}\) and opacity \(\alpha \in R^{1}\) : 3DGS 논문에서 했던대로 초기화</li> </ul> </li> <li>trajectory : <ul> <li><code class="language-plaintext highlighter-rouge">trajectory</code> : \(\Delta X = [\boldsymbol 0] \in R^{T \times 3}\)</li> </ul> </li> </ul> </li> </ul> </li> <li>Step 2) bottom-up divide-and-conquer merge<br/> merge short-trajectories into longer trajectories<br/> e.g. \(G = [G_{12}, G_{34}, G_{56}, G_{78}] \rightarrow G = [G_{14}, G{58}]\) <ul> <li>Step 2-1) <code class="language-plaintext highlighter-rouge">motion estimation</code> <ul> <li>Step 2-1-1) make a pair b.w. adjacent marbles <ul> <li>adjacent Gaussian marble set끼리 a pair로 묶음<br/> e.g. \([(G_{12}^{a}, G_{34}^{b}), (G_{56}^{a}, G_{78}^{b})]\)</li> <li>\(G^{a}\) 는 merge할 prev. frames’ Gaussians이고,<br/> \(G^{b}\) 는 merge할 next frames’ Gaussians</li> </ul> </li> <li>Step 2-1-2) \(G^{a}\) 의 trajectory 확장 <ul> <li>goal :<br/> \(G_{12}^{a}\) 의 trajectory인 \(\Delta X = [\Delta X_{1}, \Delta X_{2}]\) 는 이미 학습되어 merge된 motion이고,<br/> \(G_{12}^{a}\) 의 trajectory와 \(G_{34}^{b}\) 의 frame \(3\) 을 잇는 motion \(\Delta X_{3}\) 을 학습해야 함!</li> <li>constant-velocity assumption에 따라 trajectory를 \(\Delta X = [\Delta X_{1}, \Delta X_{2}, \Delta X_{3}^{init}]\) 로 확장</li> </ul> </li> <li>Step 2-1-3) trajectory optimization <ul> <li>\(G_{12}^{a}\) 를 frame \(3\) 에 render한 뒤<br/> \(\Delta X_{3}\) 이 frame \(3\) 으로의 motion을 잘 반영하도록 \(\Delta X_{3}\) 을 업데이트<br/> (\(\eta\) 번 반복 by 아래에서 설명할 Loss)</li> </ul> </li> <li>Step 2-1-4) repeat <ul> <li>Step 2-1-2), Step 2-1-3)을 반복<br/> until \(G^{a}\) 의 trajectory가 \(G_{34}^{b}\) 내 모든 frames를 커버할 때까지</li> <li>e.g. \(G^{a}\) 의 trajectory를 \(\Delta X = [\Delta X_{1}, \Delta X_{2}, \Delta X_{3}, \Delta X_{4}^{init}]\) 로 확장한 뒤<br/> \(G^{a}\) 를 frame \(4\) 에 render한 뒤<br/> \(\Delta X_{4}\) 가 frame \(4\) 으로의 motion을 잘 반영하도록 \(\Delta X_{4}\) 을 업데이트</li> </ul> </li> </ul> </li> <li>Step 2-2) <code class="language-plaintext highlighter-rouge">merge</code> <ul> <li>motion estimation을 거치고 나면 \(G_{ij}^{a}\) 와 \(G_{ij}^{b}\) 가 같은 frame subsequence \([i, j]\) 를 recon.할 것이므로<br/> merge by just union \(G_{ij} = G_{ij}^{a} \cup G_{ij}^{b}\)<br/> (set size 2배 됨)</li> <li>computational load 줄이기 위해<br/> opacity 또는 scale이 너무 작은 Gaussians는 drop하고,<br/> random downsampling 수행하여<br/> set size를 constant하게 유지</li> </ul> </li> <li>Step 2-3) <code class="language-plaintext highlighter-rouge">global adjustment</code> <ul> <li>merge로 합치고 나서도 still optimized라는 보장이 없기 때문에<br/> newly merged Gaussians를 모두 jointly optimize</li> <li>merged set가 \(G_{ij}\) 라고 했을 때<br/> \([i, j]\) 내 a frame을 randomly sampling하고<br/> \(G_{ij}\) 의 모든 Gaussians를 해당 frame에 render한 뒤<br/> Gaussian \(c, s, \alpha, \Delta X\) 을 업데이트<br/> (\(\beta\) 번 반복)</li> <li>그럼 merged Gaussians \(G_{ij}\) 가 global Gaussians로 인정받을 수 있음!</li> </ul> </li> </ul> </li> </ul> </li> <li>Inference Procedure : <ul> <li>learned Gaussian trajectories 이용해서<br/> render (roll out) into specific timestep \(t\)</li> </ul> </li> </ul> <h3 id="loss">Loss</h3> <p>Motion Estimation 단계에서 아래의 Loss들 사용!</p> <ul> <li><code class="language-plaintext highlighter-rouge">Tracking</code> Loss :<br/> \(L_{track} = \sum_{p \in P} \sum_{g \in N(p_{i})} \alpha_{i}^{'} \| D_{i} \| \mu_{i}^{'} - p_{i} \| - D_{j} \| \mu_{j}^{'} - p_{j} \| \|\)<br/> where \(\mu_{i}^{'}\) and \(D_{i}\) : mean and depth of projected 2D Gaussian<br/> where \(P\) : tracked points by CoTracker <d-cite key="CoTracker">[4]</d-cite><br/> where \(N(p_{i})\) : tracked point \(p_{i}\) 와의 the nearest 3D Gaussians<br/> where \(\alpha_{i}^{'}\) : Gaussian’s opacity <ul> <li>goal :<br/> <code class="language-plaintext highlighter-rouge">2D point track</code>인 CoTracker <d-cite key="CoTracker">[4]</d-cite> (2D prior)를 사용하여<br/> <code class="language-plaintext highlighter-rouge">Gassian marble trajectories</code>를 regularize</li> <li>Step 1)<br/> \(G^{b}\) 의 frame \(j\) 로의 \(\Delta X_{j}\) 를 optimize하고자 할 때,<br/> CoTracker <d-cite key="CoTracker">[4]</d-cite> 를 이용하여 frames \([j - w , j + w]\) (\(w = 12\))에서의 point tracks \(P\) 를 estimate<br/> (from 2D frame to 2D frame)<br/> (일종의 GT로 사용)</li> <li>Step 2)<br/> a source frame \(i \in [j - w , j + w]\) 을 randomly sampling</li> <li>Step 3)<br/> Gaussian marble trajectory \(\Delta X\) 로부터 frame \(i\) 와 frame \(j\) 에서의 3DGS position을 sampling하고<br/> 3DGS를 2D Gaussian in image plane으로 project시켜 2D mean, depth, covariance 구함</li> <li>Step 4)<br/> Step 1)의 tracked point \(p_{i \rightarrow j}\) 와 가장 가까운 \(K = 32\) 개의 Step 3)의 2D Gaussians를 구한 뒤<br/> <code class="language-plaintext highlighter-rouge">2D tracked point와 2D Gaussian 사이의 거리</code>가 frame \(i\), \(j\) 에서 거의 <code class="language-plaintext highlighter-rouge">일정하게 유지되도록</code> loss term 걸어줌<br/> (for <code class="language-plaintext highlighter-rouge">temporal consistency</code>)</li> </ul> </li> <li>Rendering Loss : <ul> <li><code class="language-plaintext highlighter-rouge">image</code> rendering하여<br/> GT image와의 L1 loss 및 LPIPS loss 구함</li> <li><code class="language-plaintext highlighter-rouge">disparity map</code> rendering하여<br/> initial disparity estimation과의 L1 loss 구함</li> <li><code class="language-plaintext highlighter-rouge">segmentation map</code> rendering하여<br/> SAM(off-the-shelf instance segmentation)과의 L1 loss 구함</li> </ul> </li> <li>Geometry Loss : <ul> <li><code class="language-plaintext highlighter-rouge">Local Isometry</code> Loss :<br/> \(L_{iso-local} = \sum_{g^{a} \in G} \sum_{g^{b} \in N(g^{a})} | \| \mu_{i}^{a} - \mu_{i}^{b} \| - \| \mu_{j}^{a} - \mu_{j}^{b} \| |\) <ul> <li>goal :<br/> prev. works <d-cite key="Dynamic3DGS">[2]</d-cite>, <d-cite key="DynamicPointFields">[5]</d-cite> 에서처럼<br/> Gaussian marbles가 <code class="language-plaintext highlighter-rouge">locally rigid motion</code>을 따르도록 regularize</li> <li>Step 1)<br/> \(G^{b}\) 의 frame \(j\) 로의 \(\Delta X_{j}\) 를 optimize하고자 할 때,<br/> 우선 a source timestep \(i \in [j - 1 , j + 1]\) 을 randomly sampling</li> <li>Step 2)<br/> 3DGS \(g^{a} \in G\) 및 이와 가까운 3DGS들 \(g^{b} \in N(g^{a})\) 에 대해<br/> <code class="language-plaintext highlighter-rouge">nearest 3D Gaussians끼리의 거리</code>가 frame \(i\), \(j\) 에서 거의 <code class="language-plaintext highlighter-rouge">일정하게 유지되도록</code> loss term 걸어줌</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Instance Isometry</code> Loss :<br/> \(L_{iso-instance} = \sum_{g^{a} \in G} \sum_{g^{b} \in Y(g^{a})} | \| \mu_{i}^{a} - \mu_{i}^{b} \| - \| \mu_{j}^{a} - \mu_{j}^{b} \| |\) <ul> <li>goal :<br/> 각 semantic instance가 일관적으로 움직이도록 regularize</li> <li>Step 1)<br/> \(G^{b}\) 의 frame \(j\) 로의 \(\Delta X_{j}\) 를 optimize하고자 할 때,<br/> 우선 a source timestep \(i \in [j - 1 , j + 1]\) 을 randomly sampling</li> <li>Step 2)<br/> 3DGS \(g^{a} \in G\) 및 이와 semantic label이 같은 3DGS들 \(g^{b} \in Y(g^{a})\) 에 대해<br/> <code class="language-plaintext highlighter-rouge">semantic label이 같은 3D Gaussians끼리의 거리</code>가 frame \(i\), \(j\) 에서 거의 <code class="language-plaintext highlighter-rouge">일정하게 유지되도록</code> loss term 걸어줌</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">3D Alignment</code> Loss :<br/> \(L_{chamfer} = \sum_{g^{1} \in G^{1}} \text{min}_{g^{2} \in G^{2}} \| \mu^{1} - \mu^{2} \| + \sum_{g^{2} \in G^{2}} \text{min}_{g^{1} \in G^{1}} \| \mu^{1} - \mu^{2} \|\) <ul> <li>goal :<br/> merge하고나서 <code class="language-plaintext highlighter-rouge">Global Adjustment</code>할 때 a frame에 전부 rendering해서 optimize하므로 <code class="language-plaintext highlighter-rouge">projected 2D image plane 상에서는 align</code> 되어 있음<br/> 그런데 merge하고나서 <code class="language-plaintext highlighter-rouge">3D space 상에서도 align</code>할 필요 있음<br/> (3DGS를 align한다는 게 무슨 의미지? 모든 3DGS가 함께 scene recon.에 기여하도록 서로 가깝게 만든다는 건가 <code class="language-plaintext highlighter-rouge">???</code>)<br/> (만약에 3D alignment 하지 않으면 3D and novel-view 상에서 <code class="language-plaintext highlighter-rouge">cloudy artifacts</code> 생김)<br/> (off-the-shelf depth estimation이 time에 따라 inconsistent할 경우 이와 같은 상황 발생)</li> <li>Step 1)<br/> 두 pcd 집합을 서로 가깝게 만드는 Chamfer loss를 적용할 건데,<br/> merge할 Gaussian set \(G^{a}\) 와 \(G^{b}\) 는 scene의 명확히 서로 다른 부분을 관측하고 있으므로<br/> 둘 사이에 Chamfer loss를 바로 적용하면 안 됨</li> <li>Step 2)<br/> set \(G_{12}^{a}\) 와 \(G_{34}^{b}\) 를 single frame의 subsets \([G_{1}^{a}, G_{2}^{a}, G_{3}^{b}, G_{4}^{b}]\) 로 나눔<br/> where \(G_{1}^{a}\) contains Gaussians initialized from frame \(1\)</li> <li>Step 3)<br/> 해당 subsets list를 random shuffle한 뒤<br/> 맨 앞의 25%는 set \(G^{1}\) 으로 묶고, 다음 25%는 set \(G^{2}\) 로 묶음<br/> (\(G^{1}\) 과 \(G^{2}\) 가 <code class="language-plaintext highlighter-rouge">scene의 어떤 부분을 보고 있는지 명확히 정해지지 않도록 randomness 부여</code>)<br/> (만약 이렇게 randomness 부여하지 않는다면 observed scene content difference에 overfitting될 수 있음 <code class="language-plaintext highlighter-rouge">???</code>)</li> <li>Step 4)<br/> \(G^{1}\) 과 \(G^{2}\) 에 대해 2-way Chamfer distance 계산</li> </ul> </li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <h3 id="dataset">Dataset</h3> <ul> <li>training :<br/> 아래의 두 가지 datasets는 multi-view info.를 포함하고 있으므로<br/> monocular setting을 모방하기 위해<br/> training and evaluation protocol을 수정 <ul> <li>NVIDIA Dynamic Scenes Dataset : <ul> <li>구성 :<br/> 7 videos<br/> 12 calibrated cameras</li> <li>setting :<br/> prev. benchmarked evaluations는 각 timestep마다 different training camera를 사용하는데,<br/> (monocular teleporting camera 방식 <d-cite key="monocular">[6]</d-cite>)<br/> 이는 realistic monocular video setting이 아니므로<br/> 본 논문에서는 single camera 4를 training에 사용하고 single camera 3, 5, 6을 evaluation에 사용</li> </ul> </li> <li>DyCheck iPhone Dataset : <ul> <li>구성 :<br/> 7 videos</li> <li>setting :<br/> single camera로 구성되어 있는 monocular video setting이긴 하지만<br/> multi-view info.를 포함하도록 3D trajectory가 scene 전체를 돌기 때문에<br/> camera의 calculated motion은 일상 video와 다르다 <ul> <li>방법 1) official benchmark 그대로 사용</li> <li>방법 2) camera pose 제거<br/> We remove camera poses, offloading the camera motion into the learned 4D scene representation’s dynamics. We find this setting interesting because it simulates additional dynamic content, where previously “static” regions of the scene now have rigid dynamics equal to the inverse camera motion, which must be solved by the scene representation itself. <code class="language-plaintext highlighter-rouge">???</code></li> </ul> </li> </ul> </li> </ul> </li> <li>test : <ul> <li>Total-Recon Dataset :<br/> 2 time-synchronized and calibrated videos with LiDAR</li> <li>Davis Dataset</li> <li>YouTube-VOS Dataset</li> <li>real-world videos</li> </ul> </li> </ul> <h3 id="implementation">Implementation</h3> <ul> <li>Implementation : <ul> <li>NVIDIA Dynamic Scenes Dataset : <ul> <li>120,000 Gaussians per frame and upsample to 240,000 Gaussians during the last stage of global adjustment</li> <li>\(\eta = 128\) on motion estimation and \(\beta = 48\) on global adjustment</li> <li>stop divide-and-conquer after learning subsequences of length 32 on both fg and bg</li> </ul> </li> <li>DyCheck iPhone Dataset : <ul> <li>220,000 Gaussians per frame if camear pose exists else 180,000 Gaussians</li> <li>\(\eta = 80\) on motion estimation and \(\beta = 32\) on global adjustment</li> <li>stop divide-and-conquer after learning subsequences of length 8 on fg and length 32(512) on bg<br/> (when there exists camera pose, need to learn more a dynamic bg)</li> </ul> </li> <li>Total-Recon Dataset : <ul> <li>120,000 Gaussians per frame</li> <li>$\eta = 80\(on motion estimation and\)\beta = 32$$ on global adjustment</li> <li>stop divide-and-conquer after learning subsequences of length 8 on fg and length 32 on bg</li> </ul> </li> </ul> </li> </ul> <h3 id="results">Results</h3> <ul> <li>Dynamic Novel View Synthesis : <ul> <li>TBD</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/4.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/4.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/8.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/8.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/8.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/8.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/6.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/6.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/6.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/6.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Tracking and Editing : <ul> <li>TBD</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/5.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/5.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/5.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/5.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="ablation-study">Ablation Study</h3> <ul> <li>motion estimation : <ul> <li>frame 간의 motion 정보를 학습</li> <li>locality and smoothness 보장</li> </ul> </li> <li>global adjustment : <ul> <li>merge하고나서 global Gaussian이 specific frame을 잘 render하도록</li> <li>global coherence 보장</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/7.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/7.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/7.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/7.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="conclusion">Conclusion</h2> <ul> <li>Limitation :<br/> extremely challenging open-world dynamic and monocular novel-view-synthesis는 잘 못 함 <ul> <li><code class="language-plaintext highlighter-rouge">2D image prior에 의존</code>하기 때문에<br/> SAM (segmentation), CoTracker (tracking), DepthAnything (depth estimation) 가 부정확할 경우<br/> 결과 안 좋음</li> <li><code class="language-plaintext highlighter-rouge">3D geometric prior에도 의존</code>하는데,<br/> <code class="language-plaintext highlighter-rouge">rapid and non-rigid motion</code>을 포함한 scene의 경우<br/> 잘 대응 못 함</li> </ul> </li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br/> Gaussian의 motion을 학습하는 것이라면 없던 object가 등장하거나 원래 있던 object가 frame 밖으로 벗어나는 경우에도 잘 대응할 수 있는지?<br/> CoTracker 등 여러 prior들도 위의 상황에 잘 대응하는지?</p> </li> <li> <p>A1 :<br/> TBD</p> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="Gaussian"/><category term="Marble"/><category term="degree"/><category term="freedom"/><category term="dynamic"/><category term="view"/><category term="synthesis"/><summary type="html"><![CDATA[Dynamic Gaussian Marbles for Novel View Synthesis of Casual Monocular Videos (SIGGRAPH 2024)]]></summary></entry><entry><title type="html">Feed Forward Bullet Time Reconstruction</title><link href="https://semyeong-yu.github.io/blog/2025/BTimer/" rel="alternate" type="text/html" title="Feed Forward Bullet Time Reconstruction"/><published>2025-01-10T12:00:00+00:00</published><updated>2025-01-10T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/BTimer</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/BTimer/"><![CDATA[<h2 id="feed-forward-bullet-time-reconstruction-of-dynamic-scenes-from-monocular-videos">Feed Forward Bullet Time Reconstruction of Dynamic Scenes from Monocular Videos</h2> <h4 id="hanxue-liang-jiawei-ren-ashkan-mirzaei-antonio-torralba-ziwei-liu-igor-gilitschenski-sanja-fidler-cengiz-oztireli-huan-ling-zan-gojcic-jiahui-huang">Hanxue Liang, Jiawei Ren, Ashkan Mirzaei, Antonio Torralba, Ziwei Liu, Igor Gilitschenski, Sanja Fidler, Cengiz Oztireli, Huan Ling, Zan Gojcic, Jiahui Huang</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2412.03526">https://arxiv.org/abs/2412.03526</a><br/> project website :<br/> <a href="https://research.nvidia.com/labs/toronto-ai/bullet-timer/">https://research.nvidia.com/labs/toronto-ai/bullet-timer/</a></p> </blockquote> <h2 id="contribution">Contribution</h2> <ul> <li>model : <ul> <li><code class="language-plaintext highlighter-rouge">motion-aware</code> <code class="language-plaintext highlighter-rouge">feed-forward</code> model for real-time recon. and novel-view-synthesis of <code class="language-plaintext highlighter-rouge">dynamic</code> scenes</li> <li>obtain <code class="language-plaintext highlighter-rouge">scalability</code> and <code class="language-plaintext highlighter-rouge">generalization</code> by using both static and dynamic scene datasets<br/> (static and dynamic recon.에 모두 사용 가능)</li> <li>Procedure : <ul> <li>Step 1) pre-train on large static scene dataset</li> <li>Step 2) video duration or FPS에 구애받지 않고 scale effectively across datasets</li> <li>Step 3) output multi-view volumetric video representation</li> </ul> </li> <li>recon. a bullet-time scene within 150ms with SOTA performance on a single GPU<br/> from 12 context frames of \(256 \times 256\) resolution</li> <li>bullet (target) timestamp \(t_{b}\) 를 원하는 each timestamp in video로 설정하면<br/> full video를 recon.할 수 있으므로<br/> 각 frame을 <code class="language-plaintext highlighter-rouge">parallel</code>하게 recon. 가능</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/2m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/2m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/2m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/2m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">BulletTimer</code> (Novelty 1.) :<br/> main model <ul> <li>recon. at <code class="language-plaintext highlighter-rouge">arbitrary</code> target (bullet) timestamp and <code class="language-plaintext highlighter-rouge">arbitrary</code> novel-view<br/> by adding <code class="language-plaintext highlighter-rouge">bullet-time embedding</code> to all the context (input) frames<br/> and <code class="language-plaintext highlighter-rouge">aggregating</code> pred. from all the context frames</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">NTE Module</code> (Novelty 2.) :<br/> <code class="language-plaintext highlighter-rouge">pre-processing</code> (FPI) <ul> <li><code class="language-plaintext highlighter-rouge">fast motion</code>에 대응하기 위해<br/> model에 feed하기 전에<br/> <code class="language-plaintext highlighter-rouge">intermediate (interpolated) frames를 predict</code></li> <li>inference할 때<br/> arbitrary target (bullet) timestamp에 대해<br/> recon.할 수 있도록 도움</li> </ul> </li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li> <p>Dynamic scene recon. from monocular video :<br/> still challenging<br/> due to inherently ill-posed (해가 무수히 많음) nature of dynamic recon. from limited observations</p> </li> <li> <p>Static scene recon. :</p> <ul> <li>optimization-based (per-scene) :<br/> NeRF, HyperNeRF</li> <li>learning-based (feed-forward) :<br/> MonoNeRF, GS-LRM</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/5m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/5m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/5m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/5m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Dynamic scene recon. :<br/> dynamic scene은 complex motion 때문에 ambiguity 존재<br/> 이를 해소하는 데 도움될 data prior 필요 <ul> <li>optimization-based (per-scene) : <ul> <li>use contraint (data prior) like depth and optical flow <d-cite key="33">[1]</d-cite>, <d-cite key="36">[2]</d-cite>, <d-cite key="37">[3]</d-cite>, <d-cite key="68">[4]</d-cite><br/> \(\rightarrow\)<br/> given data와 위의 data prior 간의 싱크를 맞추는 게 challenging <d-cite key="34">[5]</d-cite>, <d-cite key="63">[6]</d-cite></li> <li>per-scene approach는 time-consuming and thus scale 어렵</li> </ul> </li> <li>learning-based (<code class="language-plaintext highlighter-rouge">feed-forward</code>) : <ul> <li>directly predict recon. in feed-forward manner<br/> so, can <code class="language-plaintext highlighter-rouge">learn strong inherent prior directly from data</code> <d-cite key="7">[7]</d-cite>, <d-cite key="10">[8]</d-cite>, <d-cite key="12">[9]</d-cite>, <d-cite key="25">[10]</d-cite>, <d-cite key="53">[11]</d-cite></li> <li>근데 dynamic scene 모델링하기 복잡하고 4D supervision data 부족해서 적용하는데 한계 있었음</li> <li>지금 시점 기준 L4GM <d-cite key="53">[11]</d-cite> 이 유일한 feed-forward dynamic recon. model인데,<br/> synthetic object-centric dataset으로 훈련돼서<br/> fixed camera view-point와 multi-view supervision을 필요로 한다는 한계와<br/> real-world scene에 generalize하기 어렵다는 한계가 있었음</li> </ul> </li> </ul> </li> <li>Feed-Forward Dynamic scene recon. : <ul> <li>본 논문은<br/> <code class="language-plaintext highlighter-rouge">pixel-aligned 3DGS</code> <d-cite key="79">[12]</d-cite> 를 기반으로<br/> novel BulletTimer and NTE module 제안</li> </ul> </li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li>Dynamic 3D Representation : <ul> <li>TBD</li> </ul> </li> <li>Novel-View-Synthesis : <ul> <li>TBD</li> </ul> </li> <li>Feed-Forward Reconstruction : <ul> <li>TBD</li> </ul> </li> </ul> <h2 id="overview">Overview</h2> <ul> <li> <p>notation :<br/> context frames \(I_{c} \subset I\)<br/> camera poses \(P_{c} \subset P\)<br/> context timestamps \(T_{c} \subset T\)<br/> bullet timestamp \(t_{b} \in [\text{min}(T_{c}), \text{max}(T_{c})]\)<br/> recon. at timestamp \(t \notin T\) by NTE module</p> </li> <li> <p>Architecture :</p> <ul> <li>Training :<br/> BTimer와 NTE Module을 별도로 각각 train<br/> (not end-to-end)</li> <li>Inference :<br/> NTE Module로 pre-process한 뒤<br/> BTimer 사용</li> </ul> </li> </ul> <h2 id="method">Method</h2> <h3 id="btimer-bullet-timer">BTimer (Bullet Timer)</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/3m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/3m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/3m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/3m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Model Design : <ul> <li>encode (input) :<br/> \(i\)-th frame \(I_{i} \in I_{c}\) 을 \(8 \times 8\) 짜리 patches로 나눈 뒤<br/> \(j\)-th patch에 대해<br/> per-patch input token \(f_{ij} |_{j=1}^{HW / 64} = f_{ij}^{rgb} + f_{ij}^{pose} + f_{i}^{time}\) 만든 뒤<br/> concatenate input tokens from all context frames<br/> and feed into Transformer <ul> <li><code class="language-plaintext highlighter-rouge">image</code> encoder :<br/> GS-LRM <d-cite key="79">[12]</d-cite> 에서 영감을 받아,<br/> <code class="language-plaintext highlighter-rouge">ViT</code> model을 backbone으로 사용</li> <li><code class="language-plaintext highlighter-rouge">camera pose</code> encoder :<br/> <code class="language-plaintext highlighter-rouge">camera Plucker embedding</code> <d-cite key="70">[13]</d-cite></li> <li><code class="language-plaintext highlighter-rouge">time</code> encoder :<br/> PE (Positional Encoding) 및 linear layer를 거쳐<br/> \(t_{i}\) 와 \(t_{b}\) 를 각각 \(f_{i}^{ctx}\) 와 \(f_{i}^{bullet}\) 으로 encode한 뒤<br/> \(f_{i}^{time} = f_{i}^{ctx} + f_{i}^{bullet}\) <ul> <li><code class="language-plaintext highlighter-rouge">context (input)</code> timestamp \(t_{i}\) from context (input) frame \(I_{i}\)</li> <li><code class="language-plaintext highlighter-rouge">bullet (target)</code> timestamp \(t_{b}\) that is <code class="language-plaintext highlighter-rouge">shared</code> across context (input) frames</li> </ul> </li> </ul> </li> <li>decode (output) :<br/> transformer의 per-patch output token \(f_{ij}^{out}\) 을<br/> <code class="language-plaintext highlighter-rouge">per-patch 3DGS param. at bullet timestamp</code> \(G_{ij} \in R^{8 \times 8 \times 12}\) 로 regression <ul> <li>each Gaussian has 12 param. as color \(c \in R^{3}\), scale \(s \in R^{3}\), rotation unit quaternion \(q \in R^{4}\), opacity \(\sigma \in R\), and ray distance \(\tau \in R\)</li> <li>3D position is obtained by pixel-aligned unprojection \(\mu = o + \tau d\)<br/> (\(o\) and \(d\) are obtained from camera pose \(P_{i}\))</li> </ul> </li> </ul> </li> <li> <p>Loss :<br/> \(L_{RGB} = L_{MSE} + \lambda L_{LPIPS}\) with \(\lambda = 0.5\)</p> </li> <li>Timestamp :<br/> context (input) frames와 bullet (target supervision) frame 을 잘 고르는 게 중요 <ul> <li><code class="language-plaintext highlighter-rouge">In-context Supervision</code> : <ul> <li>bullet timestamp is randomly selected from context frames<br/> \(t_{b} \in T_{c}\)</li> <li>model이 context timestamp에 대해 정확히 recon. 가능하도록</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Interpolation Supervision</code> : <ul> <li>bullet timestamp lies between two adjacent context frames<br/> \(t_{b} \notin T_{c}\)</li> <li>model이 dynamic parts를 interpolate할 수 있도록</li> <li>pixel-aligned 3DGS의 inductive bias 때문에<br/> motion이 복잡하고 빠를 때 intermediate timestamp에 대해 예측 잘 못 함<br/> \(\rightarrow\)<br/> 먼저 NTE module의 도움을 받아 pre-process한 뒤<br/> BTimer 사용</li> <li>local minimum 방지 및 view 간 consistency 상승</li> </ul> </li> </ul> </li> <li>Inference : <ul> <li>bullet (target) timestamp \(t_{b}\) 를 원하는 each timestamp in video로 설정하면<br/> full video를 recon.할 수 있으므로<br/> 각 frame을 <code class="language-plaintext highlighter-rouge">parallel</code>하게 recon. 가능</li> <li><code class="language-plaintext highlighter-rouge">???</code><br/> For a video longer than the number of training context views \(| I_{c} |\),<br/> at timestamp \(t\), apart from including this exact timestamp and setting \(t_{b} = t\),<br/> we uniformly distribute the remaining \(| I_{c} | − 1\) required context frames across the whole duration of the video<br/> to form the input batch with \(| I_{c} |\) frames</li> </ul> </li> </ul> <h3 id="nte-module-novel-time-enhancer">NTE Module (Novel Time Enhancer)</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/4m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/4m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/4m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/4m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>NTE Module Design :<br/> decoder-only LVSM <d-cite key="27">[13]</d-cite> 에서 영감을 받아,<br/> BTimer model과 구조 똑같지만,<br/> I/O가 다름 <ul> <li>input : <ul> <li>context frame : <ul> <li>context (input) timestamp embedding<br/> (BTimer model과 달리 bullet timestamp embedding은 안 넣음)</li> <li>camera pose Plucker embedding</li> <li>context (input) frame</li> </ul> </li> <li>intermediate frame : <ul> <li>bullet (target) timestamp embedding</li> <li>target camera pose Plucker embedding</li> </ul> </li> </ul> </li> <li>output : <ul> <li>Transformer의 per-patch output tokens 중 target token을<br/> unpatchify and project directly to RGB values by linear layer<br/> \(\rightarrow\)<br/> RGB frame for any bullet (target) timestamp<br/> (this RGB frame은 NTE module network의 direct output이고, 3DGS로 rendering한 게 아님!!)<br/> \(\rightarrow\)<br/> NTE Module의 output은<br/> BTimer에서 bullet timestamp의 image로 쓰임</li> </ul> </li> <li>Implementation : <ul> <li>LVSM <d-cite key="27">[13]</d-cite> 에서처럼<br/> 안정적인 훈련을 위해 <code class="language-plaintext highlighter-rouge">QK-norm</code> 사용<br/> (Q와 K의 내적 과정에서 값이 너무 크거나 작으면 gradient explode or vanish 발생할 수 있으므로<br/> Q와 K를 normalize)</li> <li>target token에 attention할 수 있도록<br/> <code class="language-plaintext highlighter-rouge">masked attention</code> 사용</li> <li> <d-cite key="50">[14]</d-cite> <p>에서처럼<br/> 빠른 inference를 위해 <code class="language-plaintext highlighter-rouge">KV-Cache</code> 사용<br/> (training할 때는 전체 input sequence에 대해 K, Q, V를 계산하지만,<br/> inference할 때는 prev. token에서 계산한 K, V를 cache에 저장한 채 매번 Q만 새로 계산함으로써 input sequence 전체에 대해 K, V를 매번 계산할 필요 없어 계산 비용 감소)</p> </li> <li>NTE Module has negligible overhead on runtime</li> </ul> </li> </ul> </li> <li> <p>Loss :<br/> \(L_{RGB} = L_{MSE} + \lambda L_{LPIPS}\) with \(\lambda = 0.5\)</p> </li> <li>BTimer and NTE Module : <ul> <li>NTE Module로 직접 RGB image 예측하여<br/> novel-view-synthesis 할 수 있긴 한데, 그럼 성능 안 좋음<br/> (Ablation Study에 있음)</li> <li>feed-forward transformer (NTE Module)로 FPI pre-process한 뒤<br/> <code class="language-plaintext highlighter-rouge">feed-forward transformer</code> (BTimer)로 data info. 포착하여<br/> <code class="language-plaintext highlighter-rouge">3DGS param. 예측</code>한 뒤<br/> 3DGS rasterization으로 novel-view-synthesis</li> </ul> </li> </ul> <h3 id="curriculum-training-at-scale">Curriculum Training at Scale</h3> <ul> <li>Generalizability : <ul> <li>data 다양성이 많을수록 model generalizability가 높아짐<br/> static dataset은 많이 존재하고<br/> dynamic dataset은 적게 존재하지만 motion awareness 및 temporal consistency 확보 가능</li> <li>본 논문의 model인 BTimer는<br/> generalizable to both static and dynamic scenes <ul> <li>static scene : equalize all \(t_{b}\)</li> <li>dynamic scene : recon. at arbitrary bullet \(t_{b}\)</li> <li>different domain에서는 different model 필요로 하는<br/> GS-LRM <d-cite key="79">[12]</d-cite> or MVSplat <d-cite key="10">[8]</d-cite> 과는 다름</li> </ul> </li> </ul> </li> <li>Curriculum Training : <ul> <li>Stage 1) <code class="language-plaintext highlighter-rouge">Low-res to High-res Static Pretraining</code> <ul> <li>static dataset으로 pre-train <ul> <li>both synthetic and real-world</li> <li>390K training samples</li> <li>normalize datasets to be bounded in \(10^{3}\) cube</li> <li>종류 : <ul> <li>Objaverse</li> <li>RE10K</li> <li>MVImgNet</li> <li>DL3DV</li> </ul> </li> </ul> </li> <li>no time embedding<br/> (static scene이니까)</li> <li>data distribution이 복잡하기 때문에<br/> coarse 세팅 (low-resol.(\(128 \times 128\)) and few-view(\(| I_{c} | = 4\)))에서 시작해서<br/> 점점 fine 세팅 (high-resol.(\(256 \times 256 \rightarrow 512 \times 512\)))으로 train</li> </ul> </li> <li>Stage 2) <code class="language-plaintext highlighter-rouge">Dynamic Scene Co-training</code> <ul> <li>dynamic dataset으로 fine-tuning <ul> <li>종류 : <ul> <li>Kubric</li> <li>PointOdyssey</li> <li>DynamicReplica</li> <li>Spring</li> </ul> </li> </ul> </li> <li>4D dynamic dataset이 부족하기 때문에<br/> 안정적인 훈련을 위해<br/> static dataset을 함께 사용하여 co-training</li> <li>Internet video로부터 camera pose를 매기는 customized pipeline 구축하여<br/> real-world data에 대한 robustness 향상 <ul> <li>먼저 PANDA-70M dataset에서 random select한 video를 20s 길이의 clips로 자르기</li> <li>SAM으로 video의 dynamic objects를 mask out</li> <li>DROID-SLAM으로 video camera pose를 estimate</li> <li>reprojection error 측정하여 low-quality의 video 및 pose는 필터링</li> <li>최종적으로 obtain 40K clips with high-quality camera trajectories</li> </ul> </li> </ul> </li> <li>Stage 3) <code class="language-plaintext highlighter-rouge">Long-context Window Fine-tuning</code> <ul> <li>NTE Module 말고 BTimer model에만 적용</li> <li>context (input) image 수를<br/> \(| I_{c} | = 4\) 에서 \(| I_{c} | = 12\) 로 늘려서<br/> long video recon.하는 데 도움</li> </ul> </li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <h3 id="implementation">Implementation</h3> <ul> <li>Implementation : <ul> <li>Backbone Transformer :<br/> FlashAttention-3 <d-cite key="13">[15]</d-cite> and FlexAttention <d-cite key="24">[16]</d-cite></li> <li>3DGS Rasterization :<br/> gsplat library <d-cite key="74">[17]</d-cite></li> <li>Training Schedule : <ul> <li>BTimer :<br/> totally 4 days on 64 A100 GPUs <ul> <li>Stage 1)<br/> \(128^{2}\) resol. 90K iter. init lr \(4 \times 10^{-4}\)<br/> \(\rightarrow\)<br/> \(256^{2}\) resol. 90K iter. init lr \(2 \times 10^{-4}\)<br/> \(\rightarrow\)<br/> \(512^{2}\) resol. 50K iter. init lr \(1 \times 10^{-4}\)</li> <li>Stage 2)<br/> 10K iter.</li> <li>Stage 3)<br/> 5K iter.</li> </ul> </li> <li>NTE Module : <ul> <li>Stage 1)<br/> \(128^{2}\) resol. 140K iter. init lr \(4 \times 10^{-4}\)<br/> \(\rightarrow\)<br/> \(256^{2}\) resol. 60K iter. init lr \(2 \times 10^{-4}\)<br/> \(\rightarrow\)<br/> \(512^{2}\) resol. 30K iter. init lr \(1 \times 10^{-4}\)</li> <li>Stage 2)<br/> 20K iter.</li> </ul> </li> </ul> </li> <li>Inference time : <ul> <li>BTimer : <ul> <li>20 ms for 4-view \(256^{2}\) recon.</li> <li>150 ms for 12-view \(256^{2}\) recon.</li> <li>4.2 s for 12-view \(512 \times 896\) recon.</li> </ul> </li> <li>NTE : <ul> <li>0.44 s for 4-view \(512 \times 896\) recon. w/o KV cache</li> </ul> </li> </ul> </li> </ul> </li> </ul> <h3 id="results">Results</h3> <ul> <li>Dynamic Novel-View-Synthesis (Quantitative) : <ul> <li>DyCheck Benchmark <d-cite key="22">[18]</d-cite> : <ul> <li>dataset :<br/> DyCheck iPhone dataset (7 dynamic scenes by 3 synchronized cameras)</li> <li>baseline :<br/> TiNeuVox, NSFF, T-NeRF, Nerfies, HyperNeRF, PGDVS, direct depth warp <ul> <li>BTimer는 per-scene optimization method에 competitive performance 달성</li> <li>BTimer는 consistent depth estimate 없이도 PGDVS보다 성능 좋음</li> </ul> </li> </ul> </li> <li>NVIDIA Dynamic Scene Benchmark <d-cite key="75">[19]</d-cite> : <ul> <li>dataset :<br/> NVIDIA Dynamic Scene dataset (9 dynamic scenes by 12 forward-facing synchronized cameras)</li> <li>baseline :<br/> HyperNeRF, DynNeRF, NSFF, RoDynRF, MonoNeRF, 4D-GS, Casual-FVS <ul> <li>feed-forward 방식이므로 optimization time 필요 없음</li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/6m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/6m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/6m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/6m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Dynamic Novel-View-Synthesis (Qualitative) : <ul> <li>test on real-world scene 위해<br/> DAVIS dataset의 monocular videos 이용하고,<br/> customized pipeline으로 camera pose estimate해서 사용</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/8m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/8m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/8m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/8m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/7m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/7m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/7m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/7m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Static Novel-View-Synthesis : <ul> <li>RealEstate10K Benchmark : <ul> <li>baseline : pixelSplat, MVSplat, GPNR, GS-LRM</li> </ul> </li> <li>Tanks &amp; Temples Benchmark :<br/> from InstantSplat Benchmark <ul> <li>baseline : GS-LRM (SOTA)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/10m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/10m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/10m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/10m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> single dataset(Ours-Static)보다 mixed-dataset(Ours-Full) 사용하는 게 generalization 및 성능 훨씬 좋음 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/9m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/9m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/9m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/9m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="ablation-study">Ablation Study</h3> <ul> <li>Ablation 1) Context Frames : <ul> <li>train할 때 context frames 더 많이 쓰면 3DGS prediction이 progressively 많아지므로 more complete scene recon. 가능</li> <li>inference할 때 서로 멀리 떨어진 context frames를 arbitrarily 골라서 커버하는 view 범위 넓힘</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/12m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/12m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/12m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/12m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Ablation 2) Curriculum Training : <ul> <li>Stage 1)<br/> single dataset 말고 multiple dataset 써야<br/> geometry와 sharp detail 잡는 데 도움</li> <li>Stage 2)<br/> static scene을 섞어서 co-train해야<br/> geometry 및 rich detail 잡는 데 도움</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/11m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/11m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/11m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/11m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Ablation 3) Interpolation Supervision : <ul> <li>temporal and multi-view consistency 챙기는 데 도움</li> <li>bullet timestamp가 input frames에 없을 때를 훈련하지 않으면<br/> white-edge artifacts 생김<br/> (interpolation loss를 cheat하려고 camera에 너무 가까운 3DGS를 만들기 때문)</li> </ul> </li> <li>Ablation 4) NTE Module : <ul> <li>motion이 빠르고 복잡할 때 도움<br/> (ghosting artifacts 해소)</li> <li>BTimer 없이<br/> 3D info. 쓰지 않는 NTE Module만으로 novel-view-synthesis 수행하면<br/> input camera trajectory와 먼 novel-view에 대해서는 잘 recon. 못 함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/13m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/13m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/13m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/13m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="conclusion">Conclusion</h2> <ul> <li>Limitation : <ul> <li><code class="language-plaintext highlighter-rouge">geometry</code> : <ul> <li>SOTA depth prediction model <d-cite key="71">[20]</d-cite> 만큼 정확하게<br/> geometry (depth map)을 recover하지는 않음</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">memory issue</code> :<br/> transformer를 사용하다보니 memory 많이 소요 <ul> <li>need 3 days on 64 A100 GPU (40GB VRAM)</li> <li>up to \(512 \times 904\) spatial resol.</li> <li>up to 12 context frames</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">pose</code> : <ul> <li>need camera pose param.</li> <li>future work :<br/> DUSt3R, NoPoSplat처럼 pose-free일 순 없을까?</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">non-generative</code> : <ul> <li>본 논문은 feed-forward Transformer 모델이고<br/> generative model이 아니기 때문에<br/> cannot generate unseen region<br/> (unseen view를 예측하는 view extrapolation 불가능)</li> <li>future work :<br/> generative prior 사용하여 view extrapolation 수행</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">novelty</code> : <ul> <li>사실 arbitrary bullet timestamp를 input token에 추가한 뒤<br/> 모든 input frames를 transformer에 때려넣고 원하는 bullet timestamp에서의 frame을 뽑아내는 video interpolation 방식으로 보이고,<br/> 다만 차이점은 각 frame image를 transformer output으로 구하는 게 아니라 각 frame의 3DGS param.를 transformer output으로 구하는 것이고..<br/> 모델 자체의 novelty보다는 implementation을 잘 해서 결과 좋게 낸 것 같다..</li> <li>(static, dynamic) data를 많이 쓰고 stage 별 training을 통해 높은 performance를 달성할 수 있었고<br/> feed-forward 방식을 통해 빠른 속도를 달성</li> </ul> </li> </ul> </li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br/> NTE Module이 마지막에 linear layer로 RGB value를 예측함으로써<br/> pixel-space에서 RGB image at bullet timestamp 를 interpolate하고<br/> 이를 BTimer에 사용하는데,<br/> latent-space에서 interpolation 다룬 뒤 BTimer에 넘기면 더 성능 좋아질 수 있지 않을까?</p> </li> <li> <p>A1 :<br/> TBD</p> </li> <li> <p>Q2 :<br/> NTE Module이 예측한 pixel-space RGB image가 BTimer의 input으로 들어가는데,<br/> NTE Module output이 부정확하면 drift 연쇄적으로 BTimer의 결과에도 악영향 미칠 거 같아.<br/> refinement, uncertainty(confidence) 등으로 NTE Module output의 부정확성을 감소시켜 성능 높일 수 있을까?</p> </li> <li> <p>A2 :<br/> TBD</p> </li> <li> <p>Q3 :<br/> limitation 중에 unseen view는 recon.하지 못한다는 게 있는데 (view extrapolation 불가능)<br/> 본 논문이 generalizability를 가진다는 말은<br/> static and dynamic unseen dataset (scene)에 대응할 수 있어서인거지?</p> </li> <li> <p>A3 :<br/> TB</p> </li> <li> <p>Q4 :<br/> BTimer와 NTE Module을 각각 별도로 train하므로 not end-to-end인데<br/> end-to-end training할 수는 없을까?</p> </li> <li> <p>A4 :<br/> TBD</p> </li> <li> <p>Q5 :<br/> pixelSplat에서는 a pair of images를 transformer의 input으로 넣어 둘의 관계를 파악하여 static scene recon.하는데<br/> dynamic scene recon.에서는 motion 정보를 캡처해야 하기 때문에<br/> transformer의 input으로 두 장이 아니라 여러 장의 image를 넣어주어야 하는거야?</p> </li> <li> <p>A5 :<br/> TBD</p> </li> <li> <p>Q6 :<br/> interpolation supervision으로 context (input) frame이 아닌 그 사이의 frame에 대해 rendering할 때 GT는 무엇으로 두나요?</p> </li> <li> <p>A6 :<br/> context (input) frame의 image와 camera pose를 직접 interpolate하여 사용 <code class="language-plaintext highlighter-rouge">???</code></p> </li> <li> <p>Q7 :<br/> 다른 논문들을 보면 4DGS처럼 canonical time에 대한 시간에 따른 Gaussian 변화량을 MLP로 학습하거나,<br/> 또는 Dynamic Gaussian Marbles처럼 prev. frame의 GS가 next frame의 GS에 미치는 영향을 학습하기 위해 global adjustment해서 gaussian trajectory를 학습함으로써<br/> GS끼리 정보를 주고받습니다.<br/> 본 논문에서는 모든 input frames를 BTimer에 parallel하게 넣어준 뒤 bullet (target) timestamp마다 3DGS param.를 따로 뽑아내는데<br/> 그럼 3DGS끼리는 정보를 공유하지 않는 건가요?</p> </li> <li> <p>A7 :<br/> 네, 일단 BTimer 이 논문에서는 모든 input frames를 BTimer에 parallel하게 때려넣은 뒤 self-attention에 의존해서 t를 포함한 frames 간의 관계를 학습하는 것 같습니다.</p> </li> <li> <p>Q8 :<br/> 어차피 3DGS끼리 정보를 공유하지 않는 거면 굳이 3DGS를 사용한 이유가 있나요?</p> </li> <li> <p>A8 :<br/> novel-view-synthesis task에서 novel camera pose에 대한 image를 뽑아내려면 3D info.를 이용해야 recon.이 잘 될 것이기 때문에 3DGS를 이용합니다.<br/> 논문에서 언급되어 있듯이 NTE Module만을 이용해서 from 2D to 2D로 novel-view-synthesis task를 수행하면 quality가 좋지 않았다고 합니다.</p> </li> <li> <p>Q9 :<br/> camera pose의 영향을 많이 받을 것 같아요. 만약에 input frame 3에서 보였던 물체가 frame 밖을 벗어나거나 occlusion 때문에 input frame 4에서 안 보이게 되었을 때에도 잘 recon.하려면 prev. frame의 3D info. 정보를 결합해서 반영하는 식이어야 할 것 같은데, 각 bullet timestamp의 3D info.끼리 어떻게 relate되는지에 대한 내용이 없으니까 이와 같은 상황에 잘 대응할 수 있는지 궁금합니다.</p> </li> <li> <p>A9 :<br/> TBD</p> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="general"/><category term="dynamic"/><category term="GS"/><category term="view"/><category term="synthesis"/><summary type="html"><![CDATA[Feed Forward Bullet Time Reconstruction of Dynamic Scenes from Monocular Videos (CVPR 2025)]]></summary></entry><entry><title type="html">3DGS Code Review</title><link href="https://semyeong-yu.github.io/blog/2024/3DGScode/" rel="alternate" type="text/html" title="3DGS Code Review"/><published>2024-12-29T12:00:00+00:00</published><updated>2024-12-29T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/3DGScode</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/3DGScode/"><![CDATA[<blockquote> <p>code :<br/> <a href="https://github.com/graphdeco-inria/gaussian-splatting">https://github.com/graphdeco-inria/gaussian-splatting</a><br/> reference :<br/> https://charlieppark.kr<br/> NeRF and 3DGS Study<br/> https://arxiv.org/abs/2401.03890</p> </blockquote> <h2 id="code-flow-diagram">Code Flow Diagram</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/17m.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/17m.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/17m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/17m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> reference : https://charlieppark.kr </div> <h2 id="trainpy">train.py</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/1m.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/1m.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/1m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/1m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 1. train.py Algorithm </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gaussians</span> <span class="o">=</span> <span class="nc">GaussianModel</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">sh_degree</span><span class="p">)</span>
<span class="n">scene</span> <span class="o">=</span> <span class="nc">Scene</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">gaussians</span><span class="p">)</span>
</code></pre></div></div> <h2 id="gaussian-initialize">Gaussian Initialize</h2> <p>Fig 1.의 빨간 박스 : SfM pcd로부터 Gaussian param. 초기화</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Scene</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(...):</span>
    <span class="bp">...</span>
    <span class="n">self</span><span class="p">.</span><span class="n">gaussians</span><span class="p">.</span><span class="nf">create_from_pcd</span><span class="p">(</span><span class="n">scene_info</span><span class="p">.</span><span class="n">point_cloud</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">cameras_extent</span><span class="p">)</span>  
</code></pre></div></div> <ul> <li>scene 폴더의 \(\text{__init__.py}\) 에서 \(\text{class Scene.__init__()}\) : <ul> <li>scene_info :<br/> Colmap 또는 Blender의 pcd, camera info.를 받아옴 <ul> <li>pcd : scene_info.point_cloud</li> <li>camera : scene_info.train_cameras, scene_info.test_cameras</li> </ul> </li> <li>self.gaussians.create_from_pcd() :<br/> SfM pcd로부터 Gaussian param.들을 initialize</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/9m.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/9m.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/9m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/9m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> class Scene.__init__() </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/10m.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/10m.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/10m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/10m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> sceneLoadTypeCallbacks(dict) &gt; readColmapSceneInfo </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/2m.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/2m.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/2m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/2m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> GaussianModel.create_from_pcd() </div> <h2 id="densification">Densification</h2> <p>Fig 1.의 초록 박스 : densification (clone and split)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/3m.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/3m.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/3m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/3m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> train.py </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/4m.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/4m.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/4m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/4m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> GaussianModel.add_densification_stats() </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/5m.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/5m.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/5m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/5m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> GaussianModel.densify_and_prune() </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/6m.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/6m.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/6m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/6m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> GaussianModel.densify_and_clone(), GaussianModel.densify_and_split() </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/7m.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/7m.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/7m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/7m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> GaussianModel.densification_postfix() </div> <h2 id="gs-rasterize">GS Rasterize</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/8m.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/8m.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/8m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/8m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 2. GS Rasterize Algorithm </div> <p>Fig 2.의 노란 박스 : GS rasterization by CUDA</p> <ul> <li>gaussian_renderer 폴더의 \(\text{__init__.py}\) 에서 \(\text{render()}\) :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/11.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/11.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/11.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/11.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> render() </div> <ul> <li>GaussianRasterizer(nn.Module) : <ul> <li>C++/CUDA rasterization routine을 invoke</li> <li>nn.Module을 override하는 class의 forward()를 호출하려면<br/> model(…)<br/> vs.<br/> torch.autograd.Function를 override하는 _RasterizeGaussians의 forward()를 호출하려면<br/> _RasterizeGaussians.apply(…)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/12.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/12.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/12.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/12.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> diff_gaussian_rasterization.__init__.py.GaussianRasterizer(nn.Module) </div> <ul> <li>_RasterizeGaussians(torch.autograd.Function) : <ul> <li>goal : <ul> <li>MLP weight가 아니라 3DGS param.를 gradient-based back-progapagation으로 update해야 함</li> <li><code class="language-plaintext highlighter-rouge">torch.autograd.Function</code> 함수를 override하여<br/> <code class="language-plaintext highlighter-rouge">@staticmethod</code>로 <code class="language-plaintext highlighter-rouge">forward()</code>와 <code class="language-plaintext highlighter-rouge">backward()</code> 정의</li> <li><code class="language-plaintext highlighter-rouge">ctx.save_for_backward()</code>와 <code class="language-plaintext highlighter-rouge">ctx.saved_tensors</code>를 이용해서<br/> forward()에서 backward()로 정보 전달</li> <li>backward()에서 자동 미분(autograd) 말고 gradient를 manually 지정함으로써 효율적인 계산 가능</li> </ul> </li> <li>torch.autograd.Function.forward()<br/> _C.rasterize_gaussians() 호출해서 rasterization 결과 반환</li> <li>torch.autograd.Function.backward()<br/> _C.rasterize_gaussians_backward() 호출해서 gradient 반환</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/13.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/13.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/13.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/13.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> diff_gaussian_rasterization.__init__.py._RasterizeGaussians(torch.autograd.Function).forward() </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/14.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/14.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/14.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/14.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> diff_gaussian_rasterization.__init__.py._RasterizeGaussians(torch.autograd.Function).backward() </div> <ul> <li>setup.py 에서 setup() : C++/CUDA 확장 <ul> <li>ext_modules : pytorch용 CUDA Extension 정의 <ul> <li><code class="language-plaintext highlighter-rouge">CUDAExtension()</code> : pytorch에서 CUDA Extension 패키지를 빌드하는 module</li> <li>name : compiled CUDA Extension 패키지 이름</li> <li>sources : 컴파일할 C++/CUDA source codes</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/15.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/15.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/15.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/15.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> C++/CUDA code를 컴파일하여 diff_gaussian_rasterization._C 패키지를 만들기 위한 setup.py </div> <ul> <li>ext.cpp 컴파일 :<br/> <code class="language-plaintext highlighter-rouge">PYBIND11_MODULE()</code>로 python 함수와 rasterize_points.cu 의 CUDA 함수를 연결</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/16.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/16.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/16.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/16.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> python 함수와 rasterize_points.cu 의 CUDA 함수를 PYBIND11_MODULE()로 연결 </div> <ul> <li>rasterize_points.cu : <ul> <li>RasterizeGaussiansCUDA()<br/> color, radii 저장을 위한 tensor 만들고 geometry, bin, image 저장을 위한 buffer 만든 뒤<br/> CudaRasterizer::Rasterizer::forward() 호출<br/> (<code class="language-plaintext highlighter-rouge">namespace::class::static function</code> 형식)</li> <li>RasterizeGaussiansBackwardCUDA()<br/> each param. gradient 저장을 위한 tensor 만든 뒤<br/> CudaRasterizer::Rasterizer::backward() 호출</li> <li>markVisible()<br/> present tensor 만든 뒤<br/> CudaRasterizer::Rasterizer::markVisible() 호출</li> </ul> </li> <li>buffer 만들기 : obtain() 함수 사용</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/25.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/25.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/25.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/25.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>rasterizer_impl.cu : <ul> <li>int CudaRasterizer::Rasterizer::forward()<br/> (아래의 ### FORWARD section에서 설명) <ul> <li>FORWARD::preprocess() 호출<br/> \(\rightarrow\) \(\text{preprocessCUDA&lt;&gt; &lt;&lt;&lt;,&gt;&gt;&gt;()}\) 호출</li> <li>cub::DeviceScan::InclusiveSum() 호출</li> <li>\(\text{duplicatedWithKeys&lt;&lt;&lt;,&gt;&gt;&gt;()}\) 호출</li> <li>cub::DeviceRadixSort::SortPairs() 호출</li> <li>\(\text{identifyTileRanges&lt;&lt;&lt;,&gt;&gt;&gt;()}\) 호출</li> <li>FORWARD::render() 호출<br/> \(\rightarrow\) \(\text{renderCUDA&lt;&gt; &lt;&lt;&lt;,&gt;&gt;&gt;()}\) 호출</li> </ul> </li> </ul> </li> </ul> <pre><code class="language-C++">// create a tile grid (a group of blocks)
dim3 tile_grid((width + BLOCK_X - 1) / BLOCK_X, (height + BLOCK_Y - 1) / BLOCK_Y, 1);
// create a block (a group of threads(pixels))
dim3 block(BLOCK_X, BLOCK_Y, 1);
</code></pre> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/19.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/19.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/19.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/19.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/20.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/20.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/20.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/20.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/21.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/21.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/21.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/21.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>rasterizer_impl.cu : <ul> <li>void CudaRasterizer::Rasterizer::backward()<br/> (아래의 ### BACKWARD section에서 설명) <ul> <li>BACKWARD::render() 호출</li> <li>BACKWARD::preprocess() 호출</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/22.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/22.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/22.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/22.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/23.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/23.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/23.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/23.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>rasterizer_impl.cu : <ul> <li>void CudaRasterizer::Rasterizer::markVisible() <ul> <li>\(\text{checkFrustum&lt;&lt;&lt;,&gt;&gt;&gt;()}\) 호출<br/> \(\rightarrow\) \(\text{in_frustum()}\) 호출</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/24.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/24.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/24.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/24.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="forward">FORWARD</h3> <ul> <li>\(\text{preprocessCUDA&lt;NUMCHANNELS&gt; &lt;&lt;&lt;(P + 255) / 256, 256 &gt;&gt;&gt;()}\)<br/> where P : # of Gaussians<br/> where one thread per each Gaussian</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/34.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/34.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/34.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/34.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-C++">/* preprocessCUDA()의 main code flow */

// global rank of current thread (Gaussian)
auto idx = cg::this_grid().thread_rank();

in_frustum(/*...*/) // near culling

// compute 3D covariance in world-coord.
computeCov3D(/*...*/)
// compute 2D covariance in image-coord.(z=1) by EWA Splatting
computeCov2D(/*...*/)

// compute inverse of 2D covariance
float3 conic = { cov.z * det_inv, -cov.y * det_inv, cov.x * det_inv }; 

// image from ndc to pixel-coord.
float2 point_image = { ndc2Pix(p_proj.x, W), ndc2Pix(p_proj.y, H) };

// overlap 기준 : 반지름 = 3 * sigma
float my_radius = ceil(3.f * sqrt(max(lambda1, lambda2)));

// idx-th Gaussian과 겹치는 가장 왼/오른쪽, 위/아래쪽 tile의 index를 계산
getRect(point_image, my_radius, rect_min, rect_max, grid);

// idx-th Gaussian의 depth, radius 저장
depths[idx] = p_view.z; radii[idx] = my_radius;
// pixel-coord. image 저장
points_xy_image[idx] = point_image; 
// idx-th Gaussian의 inverse covariance, opacity 저장
conic_opacity[idx] = { conic.x, conic.y, conic.z, opacities[idx] }; 
// idx-th Gaussian과 겹치는 tile 개수 저장
tiles_touched[idx] = (rect_max.y - rect_min.y) * (rect_max.x - rect_min.x); 
</code></pre> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/26.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/26.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/26.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/26.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/27.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/27.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/27.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/27.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> __global__ void preprocessCUDA() </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/28.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/28.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/28.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/28.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> __device__ bool in_frustum() </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/29.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/29.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/29.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/29.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> __device__ void computeCov3D() </div> <pre><code class="language-C++">// world-to-camera of 3D mean to obtain J
float3 t = transformPoint4x3(mean, viewmatrix);
</code></pre> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/30.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/30.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/30.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/30.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> __device__ float3 computeCov2D() </div> <ul> <li>\(\text{__device__ void getRect()}\) : <ul> <li>uint2 rect_min 에서는 idx-th Gaussian의 가장 왼쪽 (p.x - \(3 \sigma\))과 겹치는 tile의 index를 계산 <ul> <li>\(3 \sigma\) : 99.7% confidence (Gaussian의 max. 반지름으로 간주)</li> <li>p.x - max_radius 는 pixel-coord.인데<br/> tile 너비 당 pixel 수인 BLOCK_X 로 나누면<br/> tile index가 됨</li> <li>grid.x 는 x축 상의 grid 내 tile(block) 개수</li> </ul> </li> <li>uint2 rect_max 에서는 idx-th Gaussian의 가장 오른쪽 (p.x + \(3 \sigma\))과 겹치는 tile의 index를 계산 <ul> <li>BLOCK_X - 1 을 분자에 더해주는 이유는<br/> C++에서 / BLOCK_X 계산하는 게 내림이기 때문</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/31.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/31.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/31.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/31.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> __device__ float ndc2Pix() and __device__ void getRect() </div> <ul> <li>cub::DeviceScan::InclusiveSum() <ul> <li>Gaussian instance를 몇 개 만들어야 하는지 계산하기 위해<br/> inclusive(prefix) sum 수행</li> <li>GPU parallel computing을 지원하는 CUDA library인 CUB 사용</li> </ul> </li> </ul> <pre><code class="language-C++">/*
// 각 Gaussian이 touch한 tile 개수
geomState.tiles_touched : [2, 3, 0, 2, 1]의 주소

// inclusive sum 수행한 후의 output array
geomState.point_offsets : [2, 5, 5, 7, 8]의 주소

// Gaussian이 touch한 총 tile 개수 또는 duplicated Gaussian instance 수
num_rendered = *(geomState.point_offsets + P-1) : 8
*/
CHECK_CUDA(cub::DeviceScan::InclusiveSum(geomState.scanning_space, geomState.scan_size, geomState.tiles_touched, geomState.point_offsets, P), debug)
</code></pre> <ul> <li>\(\text{duplicatedWithKeys&lt;&lt;&lt;(P+255)/256, 256&gt;&gt;&gt;()}\)<br/> one thread per each Gaussian <ul> <li>idx-th Gaussian과 겹치는 tile 개수만큼 (idx: current thread rank)<br/> duplicated Gaussian instance의 key-value pair 만들기 <ul> <li>64-bit key : tileID-depth <ul> <li>tileID : y * grid.x + x</li> <li>depth : depths[idx]</li> </ul> </li> <li>value : GaussianID <ul> <li>GaussianID : idx</li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/35.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/35.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/35.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/35.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/37.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/37.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/37.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/37.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>cub::DeviceRadixSort::SortPairs() <ul> <li>key를 기준으로 정렬 (in parallel)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/35.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/35.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/35.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/35.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">from collections import deque

# bit sequence 말고 자연수를 정렬한다고 할 때
# 1의 자릿수 기준으로 정렬한 뒤
# 10의 자릿수 기준으로 정렬한 뒤
# ...
def radixSort():
    nums = list(map(int, input().split(' ')))
    buckets = [deque() for _ in range(10)] # 각 자릿수(0~9)에 대응되는 10개의 empty deque()
    
    max_val = max(nums)
    queue = deque(nums) # 정렬할 숫자들
    digit = 1 # 정렬 기준이 되는 자릿수
    
    while (max_val &gt;= digit): # 가장 큰 수의 자릿수일 때까지만 실행
        while queue:
            num = queue.popleft() # 정렬할 숫자
            buckets[(num // digit) % 10].append(num) # 각 자릿수(0~9)에 따라 buckets에 num을 넣는다.
        
        # 해당 정렬 기준 자릿수에서 buckets에 다 넣었으면, buckets에 담겨있는 순서대로 꺼내와서 queue에 넣음
        for bucket in buckets:
            while bucket:
                queue.append(bucket.popleft())

        digit *= 10 # 정렬 기준이 되는 자릿수 증가시키기
    
    print(list(queue))
</code></pre> <ul> <li>\(\text{identifyTileRanges&lt;&lt;&lt;,&gt;&gt;&gt;()}\)<br/> one thread per each duplicated Gaussian instance <ul> <li>아래 코드 설명 :<br/> tile1-depth1 (Gaussian0), tile1-depth2 (Gaussian1), tile2-depth1 (Gaussian2), tile2-depth3 (Gaussian3) 으로 sort되었고<br/> idx = 2 (Gaussian2) 일 때 <ul> <li>key : tile2-depth1</li> <li>currtile : tile2</li> <li>prevtile : tile1</li> <li>currtile과 prevtile이 다르므로, 즉 idx는 처음으로 tileID가 바뀐 Gaussian instance에 해당하므로<br/> prevtile의 TileRange의 끝 지점과<br/> currtile의 TileRange의 시작 지점을<br/> GaussianID인 idx로 설정</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/38.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/38.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/38.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/38.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>\(\text{renderCUDA&lt;NUMCHANNELS&gt; &lt;&lt;&lt;grid, block&gt;&gt;&gt;()}\)<br/> where one block for each tile<br/> where one thread for each pixel</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/36.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/36.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/36.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/36.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-C++">/* renderCUDA()의 main code flow */

// current block
// block.group_index() : current block의 (x, y) index
auto block = cg::this_thread_block();

// current block's first pixel
uint2 pix_min = { block.group_index().x * BLOCK_X, block.group_index().y * BLOCK_Y };
// current thread
// pixf : current thread의 (x, y) index
float2 pixf = { (float)(pix_min.x + block.thread_index().x), (float)(pix_min.y + block.thread_index().y) };

// check if current thread corresponds to a valid pixel
bool inside = pix.x &lt; W&amp;&amp; pix.y &lt; H;

// current tile의 Gaussians 수 (range.y - range.x)를 처리하기 위한 batch 수
const int rounds = ((range.y - range.x + BLOCK_SIZE - 1) / BLOCK_SIZE);

// current block 내 모든 threads가 __syncthreads_count()라는 barrier에 도달해서
// done==True를 만족하는 threads 수 반환
int num_done = __syncthreads_count(done);

for (int i = 0; i &lt; rounds; i++, toDo -= BLOCK_SIZE)
{
    /* fetch step */
    // current block 내 모든 threads가 각자 progress-th Gaussian을 병렬적으로 fetch하여
    // BLOCK_SIZE=256개(batch)만큼씩 fetch into shared memory
    // block.thread_rank() : current thread의 local rank
    int progress = i * BLOCK_SIZE + block.thread_rank();
    if (range.x + progress &lt; range.y){
        int coll_id = point_list[range.x + progress];
        collected_id[block.thread_rank()] = coll_id;
        collected_xy[block.thread_rank()] = points_xy_image[coll_id];
        collected_conic_opacity[block.thread_rank()] = conic_opacity[coll_id];
    }
    // current block 내 모든 threads가 여기 barrier에 도달할 때까지 대기
    block.sync();

    /* rasterization step */
    // thread(pixel)마다 병렬적으로 BLOCK_SIZE=256개(batch)의 Gaussians를 앞에서부터 alpha-compositing
    // accumulated opacity T 가 너무 작아지면 해당 threads는 alpha-compositing 종료
    for (int ch = 0; ch &lt; CHANNELS; ch++)
	    C[ch] += features[collected_id[j] * CHANNELS + ch] * alpha * T;
    for (int ch = 0; ch &lt; CHANNELS; ch++)
		// 마지막에 bg color까지 alpha-compositing
        out_color[ch * H * W + pix_id] = C[ch] + T * bg_color[ch];
}
</code></pre> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/32.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/32.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/32.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/32.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-3DGScode/33.PNG-480.webp 480w,/assets/img/2024-12-29-3DGScode/33.PNG-800.webp 800w,/assets/img/2024-12-29-3DGScode/33.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-3DGScode/33.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> __global__ void __launch_bounds__(BLOCK_X * BLOCK_Y) renderCUDA() </div> <h3 id="backward">BACKWARD</h3> <ul> <li> <p>BACKWARD::render()</p> </li> <li> <p>BACKWARD::preprocess()</p> </li> <li> <p>nn.Module의 MLP weight가 아니라 3DGS param.를 gradient-based back-progapagation으로 update해야 하므로<br/> autograd(자동미분)에 의존하지 않고 각 3DGS param.의 gradient를 계산하는 과정을 직접 implement</p> <ul> <li>자세한 과정 설명은 생략하겠음!<br/> 공부하고 싶다면 <a href="https://github.com/graphdeco-inria/diff-gaussian-rasterization/blob/9c5c2028f6fbee2be239bc4c9421ff894fe4fbe0/cuda_rasterizer/backward.cu">backward.cu</a> 와 <a href="https://semyeong-yu.github.io/blog/2024/GS/">blog</a> 의 param. gradient 직접 유도 (Appendix A.) 부분 참고!!</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="3DGS"/><category term="code"/><summary type="html"><![CDATA[3D Gaussian Splatting for Real-Time Radiance Field Rendering (SIGGRAPH 2023)]]></summary></entry><entry><title type="html">CUDA Programming</title><link href="https://semyeong-yu.github.io/blog/2024/CUDA/" rel="alternate" type="text/html" title="CUDA Programming"/><published>2024-12-29T12:00:00+00:00</published><updated>2024-12-29T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/CUDA</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/CUDA/"><![CDATA[<blockquote> <p>Lecture :<br/> 24F EE514 Parallel Computing<br/> by KAIST Minsoo Rhu <a href="https://sites.google.com/view/kaist-via">VIA Research Group</a></p> </blockquote> <h2 id="spmd-programming">SPMD Programming</h2> <ul> <li>GPGPU Programming : <ul> <li>serial part : in CPU C code (host)</li> <li>parallel part : in GPU SPMD kernel code (device)</li> </ul> </li> <li>SPMD (Single Program Multiple Data) : <ul> <li>grid (kernel) \(\supset\) block (SM) \(\supset\) warp \(\supset\) thread <ul> <li>gridDim : grid 내 block 개수</li> <li>blockIdx : block index</li> <li>blockDim : block 내 thread 개수</li> <li>threadIdx : thread index</li> </ul> </li> <li>보통<br/> 1 warp = 32 threads<br/> 1 block = 256 threads</li> <li>block 내 threads끼리 shared memory를 공유</li> </ul> </li> <li>memory address space : <ul> <li>1 address에 1 Byte를 저장하므로<br/> memory address가 32-bit일 때<br/> \(2^{32}\) Byte 저장 가능</li> <li>linear memory address space를 implement하는 것은 복잡</li> </ul> </li> <li>Shared Memory Model <ul> <li>shared var. in shared address space에 저장함으로써 threads끼리 communicate</li> <li>atomicity : threads끼리 겹치지 않도록 mutual exclusion <ul> <li>semaphore</li> <li>mutex : \(\text{LOCK(mylock); //critical section UNLOCK(mylock);}\)</li> <li>atomic : \(\text{atomic{//critical section}}\) 또는 \(\text{atomicAdd(x, 10);}\)</li> </ul> </li> <li>efficient implementation을 위해 hardware support 필요<br/> processor 수가 많으면 costly할 수 있음</li> <li>e.g. OpenMP</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-CUDA/2m.PNG-480.webp 480w,/assets/img/2024-12-29-CUDA/2m.PNG-800.webp 800w,/assets/img/2024-12-29-CUDA/2m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-CUDA/2m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Message Passing Model <ul> <li>thread는 각자 private address space를 가지고 있고<br/> threads끼리 message 주고받음으로써 communicate</li> <li>system-wide load/store를 위한 hardware implementation 필요 없음</li> <li>e.g. Open MPI</li> </ul> </li> </ul> <h2 id="cuda-programming">CUDA Programming</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-CUDA/3m.PNG-480.webp 480w,/assets/img/2024-12-29-CUDA/3m.PNG-800.webp 800w,/assets/img/2024-12-29-CUDA/3m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-CUDA/3m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>CUDA APIs : <ul> <li>\(\text{cudaMalloc()}\) : device(GPU) global memory에 allocate</li> <li>\(\text{cudaFree()}\) : device(GPU) global memory free</li> <li>\(\text{cudaMemcpy()}\) : data transfer between host(CPU) and device(GPU)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-CUDA/4m.PNG-480.webp 480w,/assets/img/2024-12-29-CUDA/4m.PNG-800.webp 800w,/assets/img/2024-12-29-CUDA/4m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-CUDA/4m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>CUDA Function : <ul> <li>\(\text{__global__}\) : <ul> <li>kernel function 정의할 때 사용 (return void)</li> <li>host에서 call해서 device에서 execute</li> </ul> </li> <li>\(\text{__device__}\) : <ul> <li>device에서 call해서 device에서 execute</li> </ul> </li> <li>\(\text{__host__}\) : <ul> <li>host에서 call해서 host에서 execute</li> </ul> </li> <li>\(\text{__global__}\) and \(\text{__device__}\) :<br/> device(GPU)에서 execute해야 하므로 <ul> <li><code class="language-plaintext highlighter-rouge">정의</code> :<br/> e.g. \(\text{template&lt;uint32_t C&gt; __global__ void __launch_bounds__(BLOCK_X * BLOCK_Y) renderCUDA()}\) <ul> <li>\(\text{template&lt;uint32_t C&gt;}\) :<br/> 최적화를 위해 runtime 말고 compile-time에 미리 값이 결정되는 param.</li> <li>\(\text{__launch_bounds__(BLOCK_X * BLOCK_Y)}\) :<br/> 최적화를 위해 CUDA kernel의 block 당 thread 개수(e.g. 256)를 명확하게 지정</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">호출</code> :<br/> e.g. \(\text{vecAddKernel&lt;C&gt; &lt;&lt;&lt;grid, block&gt;&gt;&gt; (param.)}\) <ul> <li>\(\text{uint32_t C}\) : template var.</li> <li>\(\text{dim3 grid}\) : grid 내 block 개수</li> <li>\(\text{dim3 block}\) : block 내 thread 개수</li> </ul> </li> </ul> </li> <li>function 정의할 때 param.를 <code class="language-plaintext highlighter-rouge">copy-by-reference</code> 하려면 (변수를 참조해서 수정하려면)<br/> \(\text{float3&amp; p}\) 처럼 param. 자료형 뒤에 &amp; 사용</li> </ul> </li> </ul> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// define kernel func.</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">vecAddKernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span>
<span class="p">{</span>
  <span class="c1">// global rank</span>
  <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>

<span class="c1">// n : global rank</span>
<span class="n">dim3</span> <span class="nf">DimGrid</span><span class="p">((</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mi">256</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="c1">// 256 threads per block</span>
<span class="c1">// DimBlock.x = 256</span>
<span class="n">dim3</span> <span class="nf">DimBlock</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>

<span class="c1">// call kernel func.</span>
<span class="c1">// kernel func.&lt;&lt;&lt;#block, #thread&gt;&gt;&gt;(param.)</span>
<span class="n">vecAddKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">DimGrid</span><span class="p">,</span> <span class="n">DimBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_A</span><span class="p">,</span> <span class="n">d_B</span><span class="p">,</span> <span class="n">d_C</span><span class="p">,</span> <span class="n">n</span><span class="p">);</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-CUDA/5m.PNG-480.webp 480w,/assets/img/2024-12-29-CUDA/5m.PNG-800.webp 800w,/assets/img/2024-12-29-CUDA/5m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-CUDA/5m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>limitation : bottlenecked by global memory bandwidth<br/> \(\rightarrow\)<br/> solution : scratchpad memory (shared memory) <ul> <li>cache : transparent to programmer (it just works)</li> <li>scratchpad : programmer has to manually manage data movement</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-CUDA/6m.PNG-480.webp 480w,/assets/img/2024-12-29-CUDA/6m.PNG-800.webp 800w,/assets/img/2024-12-29-CUDA/6m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-CUDA/6m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>CUDA Variable : <ul> <li>\(\text{int LocalVar;}\) :<br/> <code class="language-plaintext highlighter-rouge">register</code>에 저장하여 thread 혼자서 사용</li> <li>\(\text{(__device__) __shared__ int SharedVar;}\) :<br/> <code class="language-plaintext highlighter-rouge">shared memory</code>에 저장하여 block 내 threads끼리 공유</li> <li>\(\text{__device__ int GlobalVar;}\) :<br/> <code class="language-plaintext highlighter-rouge">global memory</code>에 저장하여 grid 내 모든 threads가 공유</li> <li>\(\text{(__device__) __constant__ int SharedVar;}\) :<br/> <code class="language-plaintext highlighter-rouge">constant memory</code>에 저장하여 grid 내 모든 threads가 공유</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-CUDA/7m.PNG-480.webp 480w,/assets/img/2024-12-29-CUDA/7m.PNG-800.webp 800w,/assets/img/2024-12-29-CUDA/7m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-CUDA/7m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="tiled-matrix-multiplication">Tiled Matrix Multiplication</h2> <ul> <li>Matrix Multiplication without shared memory :<br/> each thread has to access global memory,<br/> so performance is bottlenecked by global memory bandwidth</li> </ul> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">MatrixMulKernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">M</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">P</span><span class="p">,</span> <span class="kt">int</span> <span class="n">Width</span><span class="p">){</span>
  <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>

  <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

  <span class="k">if</span> <span class="p">((</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">Width</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">col</span> <span class="o">&lt;</span> <span class="n">Width</span><span class="p">)){</span>
    <span class="kt">float</span> <span class="n">value</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="c1">// each thread computes one element of output matrix</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">Width</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">){</span>
      <span class="n">value</span> <span class="o">+=</span> <span class="n">M</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">Width</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">N</span><span class="p">[</span><span class="n">k</span> <span class="o">*</span> <span class="n">Width</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>
    <span class="p">}</span>
    
    <span class="n">P</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">Width</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span><span class="p">;</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <ul> <li>Tiling Algorithm : <ul> <li>17p TBD</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="others"/><category term="CUDA"/><category term="GPU"/><category term="kernel"/><category term="parallel"/><summary type="html"><![CDATA[.cu coding]]></summary></entry></feed>