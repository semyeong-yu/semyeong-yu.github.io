<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://semyeong-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://semyeong-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-02T07:00:43+00:00</updated><id>https://semyeong-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">DreamFusion</title><link href="https://semyeong-yu.github.io/blog/2024/Dreamfusion/" rel="alternate" type="text/html" title="DreamFusion"/><published>2024-08-29T11:00:00+00:00</published><updated>2024-08-29T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Dreamfusion</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Dreamfusion/"><![CDATA[<h2 id="dreamfusion-text-to-3d-using-2d-diffusion-iclr-2023">DreamFusion: Text-to-3D using 2D Diffusion (ICLR 2023)</h2> <h4 id="ben-poole-ajay-jain-jonathan-t-barron-ben-mildenhall">Ben Poole, Ajay Jain, Jonathan T. Barron, Ben Mildenhall</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2209.14988">https://arxiv.org/abs/2209.14988</a><br/> project website :<br/> <a href="https://dreamfusion3d.github.io/">https://dreamfusion3d.github.io/</a><br/> pytorch code :<br/> <a href="https://github.com/ashawkey/stable-dreamfusion">https://github.com/ashawkey/stable-dreamfusion</a></p> </blockquote> <h2 id="contribution">Contribution</h2> <ul> <li><code class="language-plaintext highlighter-rouge">SDS(Score Distillation) Loss</code> 처음 제시 <ul> <li>scalable, high-quality 2D diffusion model의 능력을 3D domain renderer로 distill</li> <li>3D 또는 multi-view training data 필요없고, pre-trained 2D diffusion model만 있으면, 3D synthesis 수행 가능!</li> </ul> </li> <li>NeRF가 Diffusion(Imagen) model with text에서 내놓을 만한 그럴 듯한 image를 합성하도록 함</li> <li><code class="language-plaintext highlighter-rouge">text-to-3D</code> synthesis 발전 시작</li> </ul> <h2 id="overview">Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/1-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/1-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Overview <ul> <li>initialize NeRF with random weight</li> <li>for each iter. <ul> <li>camera 위치와 각도, light 위치와 색상을 randomly sampling<br/> \(P(camera), P(light)\)</li> <li>NeRF로 image rendering</li> <li>NeRF param. \(\theta\) 와 text embedding \(\tau\) 이용해서 SDS loss 계산</li> <li>update NeRF weight</li> </ul> </li> </ul> </li> </ul> <h3 id="random-camera-light-sampling">Random camera, light sampling</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/2-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/2-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">camera</code> : <ul> <li>3D model을 <code class="language-plaintext highlighter-rouge">bounded sphere</code> 내부로 제한하고,<br/> spherical coordinate(구 표면)에서 camera 위치를 sampling하여<br/> 구의 원점을 바라보도록 camera 각도 설정</li> <li>width(64)에 0.7 ~ 1.35의 상수값을 곱하여 focal length 설정</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">light</code> : <ul> <li>camera 위치를 중심으로 한 확률분포로부터 light의 위치를 sampling하고<br/> (어떤 확률분포 <code class="language-plaintext highlighter-rouge">????</code>)<br/> light 색상도 sampling</li> </ul> </li> </ul> <h3 id="nerf-rendering-with-shading">NeRF Rendering with shading</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/3-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/3-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> albedo : NeRF가 예측한 color </div> <ul> <li>rendering 방법 : <ol> <li>albedo \(\rho\) 만으로 rendering<br/> (기존 NeRF와 동일)</li> <li>albedo \(\rho\) 뿐만 아니라 shading하여 rendering</li> <li>albedo \(\rho\) 를 white \((1, 1, 1)\) 로 바꾼 뒤 shading하여 rendering</li> </ol> </li> <li><code class="language-plaintext highlighter-rouge">Shading</code>의 역할 : <ul> <li>shading 없이 \(\rho\) 만으로 rendering하면<br/> 평평한 3D model이 나와도 점수 높게 나옴</li> <li>shading으로 (빛 반사에 따른) shape 정보까지 고려해서 rendering하면<br/> <code class="language-plaintext highlighter-rouge">volume 있는</code> 3D model이 되도록 촉구</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">NeRF MLP</code> \(\theta\) : <ul> <li>MLP output : volume density \(\tau\) 와 albedo \(\rho\)</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Normal</code> \(n\) :<br/> \(n = - \frac{\nabla_{\mu} \tau}{\| \nabla_{\mu} \tau \|}\)<br/> where \(n\) 은 물체 표면의 법선벡터 <ul> <li>normal vector의 방향은<br/> volume density \(\tau\) 가 가장 급격하게 변하는 방향, 즉 \(\nabla_{\mu} \tau\) 의<br/> 반대 방향</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Shading</code> \(s\) :<br/> \(s = (l_p \circ \text{max}(0, n \cdot \frac{l - \mu}{\| l - \mu \|})) + l_a\)<br/> where \(l_p\) 는 light 좌표 \(l\) 에서 나오는 light(광원) 색상<br/> where \(l_a\) 는 ambient light(환경 조명) 색상<br/> where \(\mu\) 는 shading 값을 계산할 surface 위 point 좌표<br/> where \(\circ\) 는 element-wise multiplication <ul> <li>\(n \cdot (l - \mu)\) 는 표면에서의 normal vector와 표면에서 광원까지의 vector 간의 내적이며,<br/> 이는 Lambertian(diffuse) reflectance(난반사)에 의해 광원의 빛이 반사되는 정도를 나타냄<br/> 왜냐하면, 빛이 표면에 수직으로 들어올수록 많이 반사됨</li> <li>만약 빛이 표면 반대쪽에 있어서 또는 back-facing normal로 잘못 예측해서<br/> 내적 값 \(n \cdot (l - \mu)\) 이 음수일 경우<br/> 난반사에 의해 광원의 빛이 반사되는 정도는 0</li> <li>\(l_p \circ \text{난반사 정도} + l_a\) 에 의해<br/> <code class="language-plaintext highlighter-rouge">광원</code>의 색상 \(l_p\) 는 물체 <code class="language-plaintext highlighter-rouge">표면의 난반사 정도에 따라</code> 반영되고<br/> <code class="language-plaintext highlighter-rouge">환경 조명</code>의 색상 \(l_a\) 는 물체의 <code class="language-plaintext highlighter-rouge">모든 표면에 일정하게</code> 반영됨</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Color</code> \(c\) :<br/> \(c = \rho \circ s\) 또는 \(c = \rho\)</li> </ul> <h3 id="diffusion-loss-with-conditioning">Diffusion loss with conditioning</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/5-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/5-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Latent Diffusion</code> model : <ul> <li>image \(x\) 가 아니라 encoder를 거친 image latent vector \(z\) 에 대해 noising, denoising 수행</li> <li>noisy \(z_T\) 와 text embedding vector \(\tau_{\theta}\) (conditioning)을 concat한 뒤<br/> denoising하여 input image와 유사한 확률 분포를 갖도록 학습</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/4-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/4-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>text embedding vector \(\tau_{\theta}\) :<br/> T5-XXL text embedding을 거치기 전에<br/> text prompt engineering 수행 <ul> <li>Elevation angle(고각)이 60도 이상일 때 “overhead view”</li> <li>azimuth angle(방위각)에 따라 “front view”, “side view”, “back view”</li> <li>text prompt engineering은 원래 좀 투박하게 하나?</li> </ul> </li> <li>Imagen : <ul> <li>latent diffusion model with \(64 \times 64\) resolution<br/> (for fast training)</li> </ul> </li> </ul> <h3 id="sample-in-parmater-space-not-pixel-space">Sample in Parmater Space, not Pixel Space</h3> <ul> <li>loss optimization으로 tractable sample 만들기 위해 diffusion model의 힘을 이용해서<br/> \(x\) in pixel space 가 아니라, \(\theta\) in parameter space 를 optimize<br/> s.t. \(x=g(\theta)\) 가 그럴 듯한 diffusion model sample처럼 보이도록</li> </ul> <h3 id="optimization">Optimization</h3> <ul> <li>실험적인 implementation : <ul> <li>noise level (time) sampling \(t\) :<br/> \(z_t, t \sim U[0, 1]\) 에서 noise level이 너무 크거나(\(t=1\)) 너무 작을 경우(\(t=0\)) instability 생기므로<br/> noise level \(t \sim U[0.02, 0.98]\) 로 sampling</li> <li>guidance weight \(w\) :<br/> Imagen이 NeRF에 얼만큼 영향을 미칠지(guide할지)인데,<br/> high-quality 3D model을 학습하기 위해서는<br/> classifier-free guidance weight \(w\) 를 큰 값(100)으로 설정<br/> (NeRF MLP output color가 sigmoid에 의해 [0, 1]로 bounded되어있으므로 constrained optimization 문제라서 guidance weight 커도 딱히 artifacts 없음)<br/> (만약 너무 작은 guidance weight 값을 사용할 경우 object를 표현하는 중간값을 찾고자 하여 over-smoothing됨 <code class="language-plaintext highlighter-rouge">????</code>)</li> <li>seed :<br/> noise level이 높을 때 smoothed density는 distinct modes를 많이 가지지 않고<br/> SDS Loss는 mode-seeking property를 가지고 있으므로<br/> random seed 바꿔도 실험 결과는 큰 차이 없음</li> </ul> </li> <li>implementation : <ul> <li>train : TPUv4, 15000 iter., 1.5h with Distributed Shampoo optimizer</li> <li>rendering : 각 cpu는 개별 view를 rendering하는데 사용</li> </ul> </li> </ul> <h2 id="rendering">Rendering</h2> <h3 id="structure">Structure</h3> <ul> <li>Mip-NeRF 360 구조 사용</li> <li>entire scene 대신 single object를 generate할 때<br/> <code class="language-plaintext highlighter-rouge">bounded sphere</code> 내에서 NeRF view-synthesis 하면 빠르게 수렴 및 좋은 성능</li> <li>\(\gamma(d)\) 를 input으로 받아 배경 색상을 계산하는 별도의 MLP로 <code class="language-plaintext highlighter-rouge">environment map</code>을 생성한 뒤 그 위에 ray rendering하면 좋은 성능 <ul> <li>배경이 보이는 부분은 배경에서의 누적 \(\alpha\) 값이 1이도록</li> <li>물체 때문에 배경이 안 보이는 부분은 배경에서의 누적 \(\alpha\) 값이 0이도록</li> </ul> </li> </ul> <h3 id="geometry-regularizer">Geometry Regularizer</h3> <ul> <li>DreamField의 regularization : <ul> <li><code class="language-plaintext highlighter-rouge">empty space가 불필요하게 채워지는</code> 것을 방지</li> <li>\(L_T = - \text{min} (\tau, \text{mean}(T(\theta, p)))\) :<br/> 평균 <code class="language-plaintext highlighter-rouge">transmittance가 클수록</code> loss가 작음<br/> where \(T(\theta, p)\) : transmittance with NeRF parameter \(\theta\) and camera pose \(p\)<br/> where \(\tau\) : 최대값 상수</li> </ul> </li> <li>Ref-NeRF의 regularization : <ul> <li>normal vector \(n_i\) 의 back-facing (<code class="language-plaintext highlighter-rouge">물체 안쪽을 향하는</code>) 문제를 방지</li> <li>orientation loss \(L = \Sigma_{i} w_i max(0, n_i \cdot d)^2\) :<br/> ray를 쏘면 물체의 앞면만 보이니까<br/> 물체 표면의 normal vector 방향과 ray 방향의 내적이 음수여야 한다<br/> 따라서 \(n_i\) 와 \(d\) 의 <code class="language-plaintext highlighter-rouge">내적이 양수일 경우</code> back-facing normal vector이므로 penalize <ul> <li>textureless shading을 쓸 때 해당 regularization이 중요<br/> 만약 해당 regularization 안 쓰면<br/> density field로 구한 normal 방향이 camera 반대쪽을 향하게 되어 shading이 더 어두워짐</li> </ul> </li> </ul> </li> </ul> <h2 id="sds-loss">SDS Loss</h2> <ul> <li> <p>NeRF로 rendering한 image \(x\) 에 noise를 더한 것을 \(z_t\) 로 두고<br/> U-Net \(\hat \epsilon_{\phi}(z_t | y, t)\) 을 빼서 denoising하여 얻은 image의 확률분포가<br/> 2D diffusion prior가 내놓는 image의 확률분포와 비슷하도록 하는 loss이며,<br/> 그 차이만큼 NeRF \(\theta\) 로 back-propagation</p> </li> <li> <p>배경지식 :</p> <ul> <li>DDPM Loss : \(E_{t, x_0, \epsilon} [\| \epsilon - \hat \epsilon_{\phi}(\alpha_{t}x_0 + \sigma_{t} \epsilon, t) \|^{2}]\)<br/> where \(\epsilon \sim N(0, I)\)<br/> where \(\alpha_{t} = \sqrt{\bar \alpha_{t}}\)<br/> where \(\sigma_{t} = \sqrt{1-\bar \alpha_{t}}\)</li> <li>위의 DDPM Loss는 denoising U-Net param.을 업데이트하기 위함<br/> 우리는 fixed denoising U-Net을 이용하여<br/> NeRF param. \(\theta\) 를 업데이트하기 위한 SDS Loss 필요!</li> </ul> </li> </ul> <h3 id="simple-derivation-of-sds-loss">Simple Derivation of SDS Loss</h3> <ul> <li>DDPM Loss를 \(\phi\) 말고 \(\theta\) 에 대해 미분하고<br/> constant \(\frac{dz_t}{dx} = \alpha_{t} \boldsymbol I\) 를 \(w(t)\) 에 넣으면</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/6-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/6-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> x는 NeRF가 생성한 image이고, y는 text embedding vector </div> <ul> <li>위의 U-Net Jacobian은 상당한 연산량을 가지는 데 비해<br/> 작은 noise만 줄 뿐 큰 영향이 없으므로<br/> SDS Loss에서 U-Net Jacobian term은 생략</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/7-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/7-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="derivation-of-sds-loss">Derivation of SDS Loss</h3> <ul> <li>SDS Loss gradient : <ul> <li>inspired by gradient of weighted probability density distillation loss <d-cite key="WaveNet">[1]</d-cite></li> <li> \[\nabla_{\theta} L_{SDS}(\phi, x=g(\theta)) = \nabla_{\theta} E_{t, z_t|x}[w(t)\frac{\sigma_{t}}{\alpha_{t}}\text{KL}(q(z_t|g(\theta)) \| p_{\phi}(z_t | y, t))]\] </li> </ul> </li> <li>KL-divergence : <ul> <li><a href="https://semyeong-yu.github.io/blog/2024/Diffusion/">Diffusion</a> 의 KL-divergence 부분에 따르면<br/> 모르는 분포 \(q(x)\) ( \(\epsilon\) ) 을 N개 sampling하여 trained \(p(x | \theta)\)로 근사하고자 할 때,<br/> \(KL(q \| p) \simeq \frac{1}{N} \sum_{n=1}^{N} {log q(x_n) - log p(x_n | \theta)}\) 이므로<br/> \(\text{KL}(q(z_t|g(\theta)) \| p_{\phi}(z_t | y, t)) = E_{\epsilon}[\text{log} q(z_t | x = g(\theta)) - \text{log} p_{\phi}(z_t | y)]\)<br/> \(\rightarrow\)<br/> \(\nabla_{\theta}\text{KL}(q(z_t|g(\theta)) \| p_{\phi}(z_t; y, t)) = E_{\epsilon}[\nabla_{\theta}\text{log} q(z_t | x = g(\theta)) - \nabla_{\theta}\text{log} p_{\phi}(z_t | y)]\)</li> </ul> </li> <li>\(\theta\) 에 대한 \(\text{log}q\) 의 미분 : <ul> <li>gradient of <code class="language-plaintext highlighter-rouge">forward process entropy</code> w.r.t mean param. \(\theta\)<br/> (variance는 고정)</li> <li>아래 수식을 \(\nabla_{\theta}log q(z_t | x = g(\theta))\) 계산에 이용<br/> \(z_t = \alpha_{t} x + \sigma_{t} \epsilon \sim N(\alpha_{t} x, \sigma_{t}^2)\)<br/> \(\rightarrow \text{log} q(z_t|x=g(\theta)) = -\frac{1}{2\sigma_{t}^2} \| z_t - \alpha_{t} x \|^2 + \text{constant}\)<br/> \(\rightarrow \frac{d\text{log}q(z_t | x)}{dx} = \frac{\alpha_{t}}{\sigma_{t}^2}(z_t - \alpha_{t} x) = \frac{\alpha_{t}}{\sigma_{t}^2}\sigma_{t}\epsilon = \frac{\alpha_{t}}{\sigma_{t}}\epsilon\)<br/> and \(\frac{d\text{log}q(z_t | x)}{dz_t} = -\frac{1}{\sigma_{t}^2}(z_t - \alpha_{t} x) = -\frac{1}{\sigma_{t}^2}\sigma_{t}\epsilon = -\frac{1}{\sigma_{t}}\epsilon\)<br/> and \(\frac{dz_t}{dx} = \alpha_{t}\)</li> <li>\(\nabla_{\theta}log q(z_t | x = g(\theta)) = (\frac{d\text{log}q(z_t | x)}{dx} + \frac{d\text{log}q(z_t | x)}{dz_t}\frac{dz_t}{dx})\frac{dx}{d\theta}\)<br/> \(= (\frac{\alpha_{t}}{\sigma_{t}}\epsilon - \frac{1}{\sigma_{t}}\epsilon \alpha_{t})\frac{dx}{d\theta}\)<br/> \(= 0\)<br/> (\(q\) 는 <code class="language-plaintext highlighter-rouge">고정된 variance의 noise</code>를 사용하므로 \(\theta\) 에 대한 entropy \(\text{log}q\) 의 미분 값은 0) <ul> <li>위의 식에서 \(\frac{d\text{log}q(z_t | x)}{dx}\) :<br/> <code class="language-plaintext highlighter-rouge">parameter score function</code><br/> gradient of log probability w.r.t parameter \(x\)<br/> (\(x\) 에 대한 \(\text{log}q\) 의 gradient 계산)</li> <li>\(\frac{d\text{log}q(z_t | x)}{dz_t}\frac{dz_t}{dx}\) :<br/> <code class="language-plaintext highlighter-rouge">path derivative</code><br/> gradient of log probability w.r.t sample \(z_t\)<br/> (\(q\) 를 따르는 sample \(z_t\) 를 통해 \(x\) 에 대한 \(\text{log}q\) 의 gradient 계산)</li> </ul> </li> <li>Sticking-the-Landing <d-cite key="vargrad">[2]</d-cite> 에 따르면<br/> path derivative term은 냅두고<br/> parameter score function term을 제거하여<br/> SDS loss gradient에 \(\epsilon\) 항을 포함할 경우<br/> <code class="language-plaintext highlighter-rouge">control-variates</code> 기법 <a href="https://en.wikipedia.org/wiki/Control_variates">Wikipedia</a>에 의해<br/> \(E[\cdot]\) 으로 gradient 구할 때 <code class="language-plaintext highlighter-rouge">variance를 줄일 수</code> 있음!<br/> (자세한 설명은 아래의 SDS Loss gradient Summary 부분 참고)<br/> (variance가 작으면 optimization이 빨라지고 더 나은 결과를 도출할 수 있음)</li> </ul> </li> <li>\(\theta\) 에 대한 \(\text{log}p_{\phi}\) 의 미분 : <ul> <li>gradient of <code class="language-plaintext highlighter-rouge">backward process entropy</code> (denoising U-Net) w.r.t mean param. \(\theta\)</li> <li>아래 수식을 \(\nabla_{\theta}log p_{\phi}(z_t | y)\) 계산에 이용<br/> \(\frac{d\text{log}q(z_t | x)}{dz_t}\) 구했듯이 \(\epsilon\) 대신 \(\epsilon_{\phi}\) 넣으면<br/> \(\nabla_{z_t} \text{log}p_{\phi}(z_t | y) = \frac{d\text{log}p_{\phi}(z_t | y)}{dz_t} = -\frac{1}{\sigma_{t}}\hat \epsilon_{\phi}\)<br/> and \(\frac{dz_t}{dx} = \alpha_{t}\)</li> <li> \[\nabla_{\theta}\text{log} p_{\phi}(z_t | y) = \nabla_{z_t} \text{log}p_{\phi}(z_t | y) \frac{dz_t}{dx} \frac{dx}{d\theta} = - \frac{\alpha_{t}}{\sigma_{t}} \hat \epsilon_{\phi}(z_t | y) \frac{dx}{d\theta}\] </li> </ul> </li> <li>SDS Loss gradient <code class="language-plaintext highlighter-rouge">Summary</code> : <ul> <li>SDS Loss gradient :<br/> \(\nabla_{\theta} L_{SDS}(\phi, x=g(\theta)) = E_{t, z_t|x}[w(t)\frac{\sigma_{t}}{\alpha_{t}}\nabla_{\theta}\text{KL}(q(z_t|g(\theta)) \| p_{\phi}(z_t | y, t))]\)<br/> \(= E_{t, \epsilon}[w(t)\frac{\sigma_{t}}{\alpha_{t}}E_{\epsilon}[\nabla_{\theta}\text{log} q(z_t | x = g(\theta)) - \nabla_{\theta}\text{log} p_{\phi}(z_t | y)]]\)<br/> \(= E_{t, \epsilon}[w(t)\frac{\sigma_{t}}{\alpha_{t}} (-\frac{\alpha_{t}}{\sigma_{t}}\epsilon \frac{dx}{d\theta} + \frac{\alpha_{t}}{\sigma_{t}} \hat \epsilon_{\phi}(z_t | y) \frac{dx}{d\theta})]\)<br/> \(= E_{t, \epsilon}[w(t)(\hat \epsilon_{\phi}(z_t | y) - \epsilon)\frac{dx}{d\theta}]\)</li> <li>\(\nabla_{\theta}log q(z_t | x = g(\theta))\) 의 path derivative term은 \(\epsilon\) 과 관련 있고!<br/> \(\nabla_{\theta}\text{log} p_{\phi}(z_t | y)\) 은 \(\epsilon\) 의 예측, 즉 \(\hat \epsilon_{\phi}\) 와 관련 있고!<br/> 둘의 KL-divergence를 loss term으로 사용한다!<br/> (\(\epsilon\) 을 \(\hat \epsilon\) 의 control-variate로 생각하여 <d-cite key="vargrad">[2]</d-cite> 방식처럼 SDS Loss gradient 만들 수 있음!)</li> </ul> </li> </ul> <h2 id="pseudo-code">Pseudo Code</h2> <pre><code class="language-Python">params = generator.init() # NeRF param.
opt_state = optimizer.init(params) # optimizer
diffusion_model = diffusion.load_model() # Imagen diffusion model
for iter in iterations:
  t = random.uniform(0., 1.) # noise level (time step)
  alpha_t, sigma_t = diffusion_model.get_coeffs(t) # determine noisy z_t's mean, std.
  eps = random.normal(img_shape) # gaussian noise (epsilon)
  x = generator(params, ...) # NeRF rendered image
  z_t = alpha_t * x + sigma_t * eps # noisy NeRF image
  epshat_t = diffusion_model.epshat(z_t, y, t) # denoising U-Net
  g = grad(weight(t) * dot(stopgradient[epshat_t - eps], x), params) # derivative of SDS loss; stopgradient since do not update diffusion model
  params, opt_state = optimizer.update(g, opt_state) # update NeRF param.
return params
</code></pre> <h2 id="experiment">Experiment</h2> <h3 id="metric">Metric</h3> <ul> <li><code class="language-plaintext highlighter-rouge">CLIP R-Precision</code> <d-cite key="dreamfield">[3]</d-cite> : <ul> <li><code class="language-plaintext highlighter-rouge">rendered image의 text 일관성</code>을 측정<br/> (rendered image가 주어졌을 때 CLIP이 오답 texts 중 적절한 text를 찾는 accuracy로 계산)</li> <li>기존 CLIP R-Precision은 geometry quality는 측정할 수 없으므로<br/> 평평한 flat geometry에 대해서도 높은 점수가 나올 수 있음</li> <li>textureless render의 R-Precision(Geo)도 추가로 측정!</li> </ul> </li> <li>PSNR :<br/> zero-shot text-to-3D generation에서는<br/> text에 대한 3D Ground-Truth를 만들 수 없으므로<br/> GT를 필요로 하는 PSNR 같은 metric은 사용하지 못함</li> </ul> <h3 id="result">Result</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/8-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/8-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Geo(metry)의 CLIP R-Precision 점수가 높다는 것은 평평한 3D model이 아니라 shape 정보까지 고려했다는 것! </div> <ul> <li>위의 표 설명 : <ul> <li>GT Images : oracle (CLIP training에 사용된 dataset)</li> <li>CLIP-Mesh : CLIP으로 mesh를 optimize한 연구</li> </ul> </li> <li> <p>DreamFusion은 training할 때 <code class="language-plaintext highlighter-rouge">Imagen</code>을 썼고,<br/> Dream Fields와 CLIP-Mesh는 training할 때 <code class="language-plaintext highlighter-rouge">CLIP</code>을 썼으므로<br/> Dream Fields와 CLIP-Mesh를 사용하는 게<br/> DreamFusion보다 성능이 더 좋아야 하는데,<br/> 위의 표를 보면 Color와 Geometry 평가에서 DreamFusion이 높은 성능(text 일관성)을 보인다는 것을 확인할 수 있다</p> </li> <li>아쉬운 점 :<br/> 비슷한 다른 모델이 있다면 PSNR, SSIM 등으로 비교할 수 있었을텐데<br/> 비교군이 없어서 R-Precision으로 consistency 측정만 했음</li> </ul> <h3 id="ablation-study">Ablation Study</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/9-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/9-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-29-Dreamfusion/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>어떤 기법이 얼마나 성능에 기여했는지 파악하기 위해<br/> 4가지 기법을 점진적으로 추가 <ul> <li>(i) <code class="language-plaintext highlighter-rouge">ViewAug</code> : view-points의 범위를 넓힘</li> <li>(ii) <code class="language-plaintext highlighter-rouge">ViewDep</code> : view-dependent text prompt-engineering 사용<br/> (e.g. “overhead view”, “side view”)</li> <li>(iii) <code class="language-plaintext highlighter-rouge">Lighting</code> : 조명 사용</li> <li>(iv) <code class="language-plaintext highlighter-rouge">Textureless</code> : albedo를 white로 만들어서 (color 없이) shading</li> </ul> </li> <li>geometry quality를 확인하기 위해<br/> 3가지 rendering 기법을 비교 <ul> <li>(Top) <code class="language-plaintext highlighter-rouge">Albedo</code> : albedo \(\rho\) 만으로 rendering<br/> (기존 NeRF와 동일)</li> <li>(Middle) <code class="language-plaintext highlighter-rouge">Full Shaded</code> : albedo \(\rho\) 뿐만 아니라 shading하여 rendering</li> <li>(Bottom) <code class="language-plaintext highlighter-rouge">Textureless</code> : albedo \(\rho\) 를 white \((1, 1, 1)\) 로 바꾼 뒤 shading</li> </ul> </li> <li>결과 설명 : <ul> <li>기법 추가 없이 Albedo rendering 하면 R-Precision은 높게 나오는데<br/> Geometry가 엄청 이상함 (e.g. 머리 2개 가진 개)</li> <li>ViewDep, Lighting, Textureless 기법 사용해야 정확한 <code class="language-plaintext highlighter-rouge">geometry</code>까지 recon할 수 있음</li> <li>(ii) ViewDep의 영향 :<br/> geometry 개선되지만, surface가 non-smooth하고 Shaded rendering 결과가 bad</li> <li>(iii) Lighting의 영향 :<br/> geometry 개선되지만, 어두운 부분은 (e.g. 해적 모자) 여전히 non-smooth</li> <li>(iv) Textureless의 영향 :<br/> geometry smooth하게 만드는 데 도움 되지만, color detail (e.g. 해골 뼈)이 geometry에 carved 되는 문제 발생</li> </ul> </li> </ul> <h2 id="limitation">Limitation</h2> <ul> <li>SDS를 적용하여 만든 2D image sample은 <code class="language-plaintext highlighter-rouge">over-saturated</code> 혹은 <code class="language-plaintext highlighter-rouge">over-smoothed</code> result <ul> <li>dynamic thresholding <d-cite key="dynathres">[4]</d-cite> 을 사용하면 SDS를 image에 적용할 때의 문제를 완화시킬 수 있다고 알려져 있긴 하지만, NeRF context에 대해서는 해결하지 못함<br/> <code class="language-plaintext highlighter-rouge">?????</code></li> </ul> </li> <li>SDS를 적용하여 만든 2D image sample은 <code class="language-plaintext highlighter-rouge">diversity</code> 부족<br/> (random seed 바꿔도 3D result에 큰 차이 없음)</li> </ul> <p>This may be fundamental to our use of reverse KL divergence, which has been previously noted to have mode-seeking properties in the context of variational inference and probability density distillation <code class="language-plaintext highlighter-rouge">?????</code></p> <ul> <li>\(64 \times 64\) Imagen (<code class="language-plaintext highlighter-rouge">low resol.</code>)을 사용하여 3D model의 fine-detail이 부족할 수 있음 <ul> <li>diffusion model 또는 NeRF를 더 큰 걸 사용하면 문제 해결할 수 있지만, 그만큼 겁나 느려지지…</li> </ul> </li> <li>2D image로부터 3D recon.하는 게 원래 어려운 task임<br/> e.g. inverse rendering, dreamfusion <ul> <li>같은 2D images로부터 무수히 많은 3D worlds가 존재할 수 있으니까</li> <li>optimization landscape가 highly non-convex하므로 local minima에 빠지지 않기 위한 기법들 필요<br/> (local minima : e.g. 모든 scene content가 하나의 flat surface에 painted된 경우)</li> <li>more <code class="language-plaintext highlighter-rouge">robust 3D prior</code>가 도움 될 것임</li> </ul> </li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 : reverse KL-divergence를 최소화하는 과정의 경우 mode-seeking property (확률 높은 중요한 부분 찾는 경향)가 있다는데,<br/> reverse KL-divergence와 mode-seeking property가 무슨 관계인가요?</p> </li> <li> <p>A1 : TBD</p> </li> <li> <p>Q2 : SDS loss로 image rendering한 samples의 경우 diversity가 부족하고 그 이유가 mode-seeking property라는 거 같은데,<br/> 오히려 diversity가 부족한 게 단점이 아니라,<br/> mode-seeking property로 중요한 부분을 잘 캐치해서 consistent하게 그려내는 게 장점이 될 수 있지 않나요?</p> </li> <li> <p>A2 : TBD</p> </li> </ul>]]></content><author><name></name></author><category term="generative"/><category term="sds"/><category term="diffusion"/><category term="nerf"/><category term="3d"/><category term="rendering"/><summary type="html"><![CDATA[Text-to-3D using 2D Diffusion (ICLR 2023)]]></summary></entry><entry><title type="html">GaussianEditor</title><link href="https://semyeong-yu.github.io/blog/2024/GSeditor/" rel="alternate" type="text/html" title="GaussianEditor"/><published>2024-08-25T11:00:00+00:00</published><updated>2024-08-25T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/GSeditor</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/GSeditor/"><![CDATA[<h2 id="gaussianeditor-swift-and-controllable-3d-editing-with-gaussian-splatting">GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting</h2> <h4 id="yiwen-chen-zilong-chen-chi-zhang-feng-wang-xiaofeng-yang-yikai-wang-zhongang-cai-lei-yang-huaping-liu-guosheng-lin">Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, Guosheng Lin</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2311.14521">https://arxiv.org/abs/2311.14521</a><br/> project website :<br/> <a href="https://gaussianeditor.github.io/">https://gaussianeditor.github.io/</a><br/> code :<br/> <a href="https://github.com/buaacyw/GaussianEditor">https://github.com/buaacyw/GaussianEditor</a></p> </blockquote> <h2 id="paper-review-후기">Paper Review 후기</h2> <ul> <li>novelty : <ul> <li>SAM mask를 GS로 inverse rendering해서 target GS identify</li> <li>기존 GS에서 크게 벗어나지 않도록(stability) anchor loss</li> </ul> </li> <li>3DGS 나오고나서 3DGS 이용한 Editing에 대해 잽싸게 낸 논문이라<br/> 비교 대상도 없고<br/> Editing loss도 기존 기법을 그대로 써서 novelty 흐음…?</li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li>NeRF-based 3D Editing : <ul> <li>Instruct-nerf2nerf: Editing 3D scenes with instructions</li> <li>Ed-nerf: Efficient text-guided editing of 3D scene using latent space nerf</li> <li>Clip-nerf: Text-and-Image driven manipulation of neural radiance fields</li> <li>Nerf-art: Text-driven neural radiance fields stylization</li> <li>Dreameditor: Text-driven 3D scene editing with neural fields</li> </ul> </li> <li>NeRF-based 3D Editing by MLP의 문제점 : <ul> <li><code class="language-plaintext highlighter-rouge">specific</code> scene parts를 직접 수정하는 데 제한</li> <li>inpainting 및 scene composition 과정이 <code class="language-plaintext highlighter-rouge">복잡</code></li> <li>strictly <code class="language-plaintext highlighter-rouge">masked area</code> 내에서만 editing 가능</li> </ul> </li> <li>3DGS-based 3D Editing의 문제점 : <ul> <li>Editing할 Gaussian을 <code class="language-plaintext highlighter-rouge">identify</code>(분류)해야 함</li> <li>SDS처럼 Diffusion model로 얻은 <code class="language-plaintext highlighter-rouge">random generative guidance</code>를 3DGS에 적용할 때<br/> randomness in loss로 인해<br/> view마다 non-consistent(random)한 image를 합성(Editing)하므로<br/> GS is directly affected by randomness,<br/> so 업데이트 불안정</li> <li><code class="language-plaintext highlighter-rouge">수많은</code> Gaussian points를 업데이트해야 함<br/> NeRF-based에서처럼 MLP NN buffering이 불가능하므로 불안정하여<br/> finely detailed result로 수렴하는 걸 방해</li> </ul> </li> </ul> <h2 id="method">Method</h2> <h3 id="gaussian-semantic-tracing">Gaussian Semantic Tracing</h3> <ul> <li> <p>전제 : 3DGS가 이미 잘 구성되어 있다고 가정하고, 특정 scene part를 제거 또는 추가하거나 inpainting하는 등 3D Editing 수행</p> </li> <li> <p>Gaussian Semantic Tracing :<br/> 훈련하는 동안 3D Editing할 target을 trace하기 위해 semantic label(mask) 생성</p> </li> </ul> <h4 id="parameters">Parameters</h4> <ul> <li> <p>\(x, s, q, \alpha , c\) (position, covariance(scale, quaternion), opacity, color) 뿐만 아니라<br/> \(m_{ij}\) (<code class="language-plaintext highlighter-rouge">semantic Gaussian mask</code> for i-th Gaussian and j-th semantic label) 추가</p> </li> <li> <p>densification할 때 clone/split된 points는 parent point의 semantic label를 그대로 물려받음</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-25-GSeditor/1-480.webp 480w,/assets/img/2024-08-25-GSeditor/1-800.webp 800w,/assets/img/2024-08-25-GSeditor/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-25-GSeditor/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 처음에 inaccurate segmentation mask에서 출발했더라도 Gaussian semantic tracing하는 동안 3DGS 업데이트하면서 semantic Gaussian mask도 알맞게 업데이트됨 </div> <h4 id="initial-labeling-process">Initial Labeling Process</h4> <ul> <li> <p>camera pose 하나 골라서 <code class="language-plaintext highlighter-rouge">SAM</code>(Segment Anything)으로 2D segmentation 수행한 뒤<br/> <code class="language-plaintext highlighter-rouge">inverse rendering</code>으로 2D mask를 3D Gaussian으로 unproject<br/> \(w_i^j = \Sigma o_i (p) \ast T_i^j (p) \ast M^j (p)\)<br/> where \(w_i^j\) : weight of i-th Gaussian for j-th semantic label<br/> where \(o, T, M, p\) : opacity, transmittance, mask, pixel</p> </li> <li> <p>average weight가 threshold를 넘는 경우에만 해당 i-th Gaussian이 j-th semantic class를 갖는다고 선별</p> </li> </ul> <h3 id="hierarchical-gaussian-splatting">Hierarchical Gaussian Splatting</h3> <ul> <li> <p>Hierarchical Gaussian Splatting :<br/> stabilized and fine results 만들기 위해 anchor loss 사용</p> </li> <li> <p>3D Editing 위해 densification할 때<br/> threshold를 manually 정하는 게 아니라,<br/> 3D position gradients가 top k% 안에 드는 3DGS들만 선택적으로 densify<br/> (k값이 점점 증가)</p> </li> <li><code class="language-plaintext highlighter-rouge">anchor loss</code> : <ul> <li>3D Editing 때문에 densification할 때마다 <code class="language-plaintext highlighter-rouge">기존</code>의 Gaussian param.을 anchor에 record</li> <li>3D Editing에 따라 변형되는 Gaussian param.가 각 anchor로부터 크게 벗어나지 않도록 함<br/> \(L_{anchor}^P = \Sigma_{i=0}^n \lambda_{i} (P_i - \hat P_i)^2\)<br/> where \(P : x, s, q, \alpha , c, m_{ij}\)<br/> where \(\lambda_{i}\) 값이 점점 증가 (새로 만들어지는 Gaussian param.의 영향이 크도록)</li> <li><code class="language-plaintext highlighter-rouge">stable</code> geometry formation under stochastic loss 보장</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Edit loss</code> : <ul> <li>3DGS model로 rendering한 image와 diffusion model 간의 차이<br/> \(L_{Edit} = D(\theta ; p, e)\)<br/> where \(D, \theta , p, e\) : Diffusion model, 3D model, camera pose, prompt</li> <li>2D diffusion model로 3D Editing하는 방법 :<br/> 1) <code class="language-plaintext highlighter-rouge">DreamFusion</code> <d-cite key="Dreamfusion">[1]</d-cite> 의 <code class="language-plaintext highlighter-rouge">SDS loss</code>처럼 3D model의 rendering과 other conditions를 2D diffusion model에 넣어준 뒤, noise 넣고 <code class="language-plaintext highlighter-rouge">denoising하는 과정에서 내놓은 score</code>가 3D model의 업데이트 방향을 guide<br/> 즉, 3D model로 만든 image가 2D diffusion에서의 그럴 듯한 image distribution에 부합하도록 함<br/> 2) <code class="language-plaintext highlighter-rouge">Instruct-nerf2nerf</code>처럼 3D model의 rendering과 prompts 이용해서 <code class="language-plaintext highlighter-rouge">2D Editing</code> 수행하는 데 초점을 두고, Edited 2D multi-view images를 training target으로 사용하여 3D model에게 guidance 줌</li> </ul> </li> <li>total loss :<br/> \(L = L_{Edit} + \Sigma_{P \in [x, s, q, \alpha , c]} \lambda_{P} L_{anchor}^P\)</li> </ul> <h3 id="3d-inpainting">3D Inpainting</h3> <ul> <li> <p>외부 모델들 사용해서 Efficient 3D Editing 구현</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Object Removal</code> (object 제거) :</p> <ul> <li>semantic label(mask) 가지는 3D Gaussian만 삭제하면 target object와 scene 사이의 interface에서 artifacts 생김</li> <li><code class="language-plaintext highlighter-rouge">precise mask</code>를 생성할 필요가 있음 <ul> <li>삭제한 3DGS와 가장 가까운 Gaussian을 KNN으로 identify</li> <li>이를 다양한 view-points로 project하여 mask를 <code class="language-plaintext highlighter-rouge">확장</code> (dilate)</li> <li>hole을 메꿔서 interface area를 정확하게 표현하도록 refined mask를 생성</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Diffusion model</code>을 이용해서 해당 area를 <code class="language-plaintext highlighter-rouge">2D inpainting</code> (object 삭제)</li> <li>inpainted image를 기반으로 <code class="language-plaintext highlighter-rouge">3DGS 업데이트</code></li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-25-GSeditor/4-480.webp 480w,/assets/img/2024-08-25-GSeditor/4-800.webp 800w,/assets/img/2024-08-25-GSeditor/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-25-GSeditor/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Object Incorporation by text</code> (object 추가 혹은 수정) : <ul> <li>editing area에 BB 만듦</li> <li><code class="language-plaintext highlighter-rouge">Stable Diffusion XL</code> model <d-cite key="SDXL">[2]</d-cite> 을 이용해서 해당 area에 <code class="language-plaintext highlighter-rouge">넣을 image</code>를 생성하고<br/> fg object is segmented</li> <li><code class="language-plaintext highlighter-rouge">Wonder3D</code> model <d-cite key="Wonder3D">[3]</d-cite> 을 이용해서 fg-segmented image를 3D textured <code class="language-plaintext highlighter-rouge">mesh</code>로 변환</li> <li>Hierarchical Gaussian Splatting을 이용해서 mesh를 새로운 <code class="language-plaintext highlighter-rouge">3DGS</code>로 변환</li> <li><code class="language-plaintext highlighter-rouge">DPT</code> <d-cite key="DPT">[4]</d-cite> 로 depth estimation해서 기존의 3DGS와 생성된 3DGS의 <code class="language-plaintext highlighter-rouge">depth를 align</code>해주고 기존의 3DGS와 생성된 3DGS를 <code class="language-plaintext highlighter-rouge">concatenate</code>(결합)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-25-GSeditor/5-480.webp 480w,/assets/img/2024-08-25-GSeditor/5-800.webp 800w,/assets/img/2024-08-25-GSeditor/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-25-GSeditor/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-25-GSeditor/3-480.webp 480w,/assets/img/2024-08-25-GSeditor/3-800.webp 800w,/assets/img/2024-08-25-GSeditor/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-25-GSeditor/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="experiments">Experiments</h2> <ul> <li>Implementation : <ul> <li>view-point (camera-pose) 개수 : 24-96개</li> <li>optimization : 3DGS가 이미 구성되었다는 전제 하에<br/> 3D Editing하는 데만 500-1000 steps, 5-10 min.<br/> (3 min. for Wonder3D mesh 생성 + 2 min. for 3DGS로 변환 및 refine)</li> </ul> </li> <li>Ablation Study : <ul> <li>w/o Semantic Tracing :<br/> target object만 Editing되는 게 아니라 image 전체 Editing</li> <li>w/o Hierarchical GS :<br/> uncontrolled densification 및 image blurring 초래</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-25-GSeditor/6-480.webp 480w,/assets/img/2024-08-25-GSeditor/6-800.webp 800w,/assets/img/2024-08-25-GSeditor/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-25-GSeditor/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-25-GSeditor/7-480.webp 480w,/assets/img/2024-08-25-GSeditor/7-800.webp 800w,/assets/img/2024-08-25-GSeditor/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-25-GSeditor/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="conclusion">Conclusion</h2> <ul> <li>2 strategies <ul> <li><code class="language-plaintext highlighter-rouge">Gaussian Semantic Tracing</code><br/> for precise Gaussian <code class="language-plaintext highlighter-rouge">identification</code> of editing areas</li> <li><code class="language-plaintext highlighter-rouge">Hierarchical GS</code><br/> for balance b.w. fluidity and <code class="language-plaintext highlighter-rouge">stability</code><br/> to achieve <code class="language-plaintext highlighter-rouge">detailed(fine) results</code> under stochastic guidance</li> </ul> </li> </ul> <h2 id="limitation">Limitation</h2> <ul> <li>supervision을 위해 <code class="language-plaintext highlighter-rouge">2D diffusion model에 의존</code>하여 3D editing을 수행하는데<br/> 현재 2D diffusion model은 특정 복잡한 prompts에 대해서는 effective guidance를 제공하는 데 어려움이 있어 3D editing에도 한계 있음</li> </ul> <h2 id="question">Question</h2> <ul> <li>Q1 : Edit loss에서 사용하는 SDS loss나 Instruct-nerf2nerf 기법은 이미 있는 내용이고,<br/> 본 논문에서 볼 건 아래의 두 가지 정도인데 (SAM mask를 GS로 inverse rendering해서 target GS identify하고<br/> 기존 GS에서 크게 벗어나지 않도록(stability) anchor loss)<br/> 별로 novelty가 없는 것 같다</li> <li>A1 : ㅇㅈ</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="GS"/><category term="3d"/><category term="editing"/><summary type="html"><![CDATA[Swift and Controllable 3D Editing with Gaussian Splatting (CVPR 2024)]]></summary></entry><entry><title type="html">COLMAP-Free 3D Gaussian Splatting</title><link href="https://semyeong-yu.github.io/blog/2024/Colmapfree/" rel="alternate" type="text/html" title="COLMAP-Free 3D Gaussian Splatting"/><published>2024-08-24T11:00:00+00:00</published><updated>2024-08-24T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Colmapfree</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Colmapfree/"><![CDATA[<h2 id="colmap-free-3d-gaussian-splatting">COLMAP-Free 3D Gaussian Splatting</h2> <h4 id="yang-fu-sifei-liu-amey-kulkarni-jan-kautz-alexei-a-efros-xiaolong-wang">Yang Fu, Sifei Liu, Amey Kulkarni, Jan Kautz, Alexei A. Efros, Xiaolong Wang</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2312.07504">https://arxiv.org/abs/2312.07504</a><br/> project website :<br/> <a href="https://oasisyang.github.io/colmap-free-3dgs/">https://oasisyang.github.io/colmap-free-3dgs/</a><br/> pytorch code :<br/> <a href="https://github.com/NVlabs/CF-3DGS">https://github.com/NVlabs/CF-3DGS</a></p> </blockquote> <h2 id="introduction">Introduction</h2> <ul> <li>기존 novel-view-synthesis : <ul> <li>input images<br/> \(\rightarrow\) COLMAP library for SfM <code class="language-plaintext highlighter-rouge">pcd, camera pose</code> 계산<br/> \(\rightarrow\) NeRF or 3DGS</li> <li>단점 : 시간 많이 걸리고, feature 추출 오차에 대해 민감성이 있고, 반복적인 영역을 처리하는 데 어려움</li> </ul> </li> <li> <p>Motivation :<br/> <code class="language-plaintext highlighter-rouge">pose estimation</code>과 <code class="language-plaintext highlighter-rouge">novel-view-synthesis</code>를<br/> COLMAP과 3DGS로 나눠서 하지 말고<br/> <code class="language-plaintext highlighter-rouge">end-to-end로 동시에</code> 할 수는 없을까?</p> </li> <li>Related Work :<br/> 사전에 COLMAP library 사용하지 않기 위해<br/> <code class="language-plaintext highlighter-rouge">BARF, Nope-NeRF, L2G-NeRF</code> 등<br/> 여러 방법들이 제안되어 왔지만<br/> 여러 한계 있음 <ul> <li>perturbation이 적어야 함</li> <li><code class="language-plaintext highlighter-rouge">camera motion의 범위</code>가 너무 넓으면 안 됨<br/> (Nope-NeRF 등은 pose를 직접 optimize하는 게 아니라 ray casting process를 optimize하는 간접적인 방법이라서 camera 이동이 커지면 optimize 난이도가 복잡해짐)</li> <li><code class="language-plaintext highlighter-rouge">training time</code>이 너무 오래 걸림</li> <li>NeRF-based 기법들은 MLP-based implicit method이므로<br/> 3DGS처럼 explicit pcd를 요구하는 method에 적용하기 어렵</li> <li>regularization term이 많아져서 복잡하고 geometric prior를 요구하기도 함</li> </ul> </li> <li>COMALP-Free 3D GS : <ul> <li>3DGS가 <code class="language-plaintext highlighter-rouge">explicit</code> representation (pcd 등) 을 활용할 수 있기 때문에 새로운 접근이 가능해졌다 <ul> <li>temporal continuity data (video sequence)와<br/> explicit representation data (3DGS)를 이용해서<br/> pose estimation과 novel view synthesis를 동시에 수행</li> </ul> </li> <li>Local 3DGS : <ul> <li><code class="language-plaintext highlighter-rouge">initialization</code> 위해 Nope-Nerf 랑 비슷하게 monocular <code class="language-plaintext highlighter-rouge">depth-map</code> 사용</li> <li>목표 :<br/> 주어진 frame \(t-1\) 에서의 local 3D Gaussian 집합을 구성하고,<br/> frame \(t\) 에서의 local 3D Gaussian 집합으로 변환할 수 있는<br/> <code class="language-plaintext highlighter-rouge">relative camera pose (affine transformation)</code> 학습</li> </ul> </li> <li>Global 3DGS : <ul> <li>목표 :<br/> Local 3DGS에서 구한 relative camera pose를 기반으로<br/> Global 3DGS를 순차적으로 점진적으로 계속 업데이트해서<br/> entire scene <code class="language-plaintext highlighter-rouge">reconstruction</code> 결과가 깔끔하게 나타나도록 하자</li> </ul> </li> </ul> </li> <li>COLMAP vs 본 논문 : <ul> <li>COLMAP : 100장의 images를 <code class="language-plaintext highlighter-rouge">한 번에</code> 넣고 camera pose를 optimize</li> <li>본 논문 : video sequence를 <code class="language-plaintext highlighter-rouge">순차적으로</code> 실시간으로 받으며 점진적으로 optimize</li> </ul> </li> </ul> <h2 id="overview">Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-24-Colmapfree/1-480.webp 480w,/assets/img/2024-08-24-Colmapfree/1-800.webp 800w,/assets/img/2024-08-24-Colmapfree/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-24-Colmapfree/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="local-3dgs-for-relative-pose-estimation">Local 3DGS for Relative Pose Estimation</h2> <ul> <li>Initialization from a Single View : <ul> <li>initial frame을 monocular depth network (DPT)에 넣어 depth map \(D_1\) 생성</li> <li>3D mean :<br/> <code class="language-plaintext highlighter-rouge">initial frame</code> (2D 정보)과<br/> <code class="language-plaintext highlighter-rouge">initial depth map</code> \(D_1\) (3D 정보)와<br/> <code class="language-plaintext highlighter-rouge">intrinsic</code> param. 이용해서<br/> 3D pcd로 투영하고, 이를 initial 3DGS mean point로 사용</li> <li>opacity, SH-coeff., covariance(rotation, scale) :<br/> L1, D-SSIM photometric loss로 <code class="language-plaintext highlighter-rouge">optimal (initial) Local 3DGS</code>를 5초 정도만에 구함<br/> initial frame \(t = 1\) 에 대해<br/> \(G_t^{\ast} = \text{argmin}_{\alpha_{t}, c_t, r_t, s_t} L_{rgb}(R(G_t), I_t) = (1 - \lambda) L_1 + \lambda L_{D-SSIM}\)</li> </ul> </li> <li>Pose Estimation by 3D Gaussian Transformation : <ul> <li>Gaussian 집합 \(G_t\) 를 \(G_{t+1}\) 로 올바르게 변환할 수 있는 learnable SE-3 affine transformation \(T_t\) 를 찾아야 함</li> <li>전제 : video로 찍은 연속적인 frame이므로 \(T_t\) 의 값이 크지 않음</li> <li>photometric loss로 <code class="language-plaintext highlighter-rouge">optimal relative camera pose(affine transformation)</code>을 10초 안에 구함<br/> \(T_t^{\ast} = \text{argmin}_{T_t} L_{rbg} (R(T_t \odot G_t), I_{t+1})\)<br/> where \(G_t\) is <code class="language-plaintext highlighter-rouge">freezed</code> (self-rotation 등 방지)<br/> (geometric transformation(camera movement)에만 집중)</li> </ul> </li> </ul> <h2 id="global-3dgs-with-progressively-growing">Global 3DGS with Progressively Growing</h2> <ul> <li>Local 3DGS를 통해 optimal relative camera pose를 구했다 <ul> <li>한계 : frame \(F\) 와 frame \(F+t\) 간의 relative camera pose를 단순히 \(\prod_{k=F}^{F+t} T_k\) 처럼 곱으로 두면 오차가 점점 커져서<br/> entire <code class="language-plaintext highlighter-rouge">scene reconstruction 결과가 noisy</code></li> </ul> </li> <li>Global 3DGS : <ul> <li>frame이 들어올 때마다 relative camera pose \(T_t\) 와 frame \(t, t+1\) 이용해서 <code class="language-plaintext highlighter-rouge">optimal Global 3DGS</code> 업데이트 (progressively growing)</li> <li>어떻게 업데이트? :<br/> frame \(t+1\) 에는 frame \(t\) 에서 <code class="language-plaintext highlighter-rouge">보지 못한 일부 영역</code> 들이 있으므로<br/> 새로운 frame에 대한 <code class="language-plaintext highlighter-rouge">under-reconstruction densification</code>에 초점을 두어<br/> last frame까지 계속해서 점진적으로 scene reconstruction 수행<br/> (last frame까지 계속 under-reconstruction 상황(보지 못했던 영역)이 발생할 것이라는 전제)<br/> (새로운 테크닉은 아니고 3DGS에서의 adaptive density control과 동일)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-24-Colmapfree/2-480.webp 480w,/assets/img/2024-08-24-Colmapfree/2-800.webp 800w,/assets/img/2024-08-24-Colmapfree/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-24-Colmapfree/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Local 3DGS와 Global 3DGS는 iteratively optimized</li> </ul> <h2 id="experiment">Experiment</h2> <ul> <li>GS 말고 pose-free NeRF methods와 비교했을 때<br/> pose trajectory와 scene reconstruction 측면에서<br/> 본 논문이 훨씬 더 좋은 성능</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-24-Colmapfree/3-480.webp 480w,/assets/img/2024-08-24-Colmapfree/3-800.webp 800w,/assets/img/2024-08-24-Colmapfree/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-24-Colmapfree/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-24-Colmapfree/4-480.webp 480w,/assets/img/2024-08-24-Colmapfree/4-800.webp 800w,/assets/img/2024-08-24-Colmapfree/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-24-Colmapfree/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-24-Colmapfree/5-480.webp 480w,/assets/img/2024-08-24-Colmapfree/5-800.webp 800w,/assets/img/2024-08-24-Colmapfree/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-24-Colmapfree/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>COLMAP + 3DGS와 비교했을 때<br/> 본 논문과 동일한 성능</p> </li> <li>우리는 pose estimation을 할 때 photometric loss에만 의존했음 <ul> <li>photometric loss에만 의존해서 relative camera pose를 구했기 때문에 \(RPE_r, RPE_t\) 값이 Nope-NeRF보다 조금 높게 나타날 수 있음</li> <li>Nope-NeRF에서는 chamfer distance(point cloud 집합인 \(P_i\) 와 \(P_j\) 가 서로 가까워지도록 하는 point cloud loss) 추가하여 pose accuracy 높임</li> </ul> </li> <li>Nope-NeRF에서와 달리 본 논문에서 depth loss를 쓰면 pose accuracy는 비슷하고 novel view synthesis performance는 오히려 떨어지므로 depth loss는 안 씀</li> </ul> <h2 id="limitation">Limitation</h2> <ul> <li>camera pose와 3DGS를 동시에 순차적으로 optimize하므로<br/> video stream 혹은 ordered image 집합에만 적용 가능<br/> \(\rightarrow\) unordered image 집합에도 적용하는 future work 필요</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="COLMAP"/><category term="SfM"/><category term="GS"/><category term="depth"/><category term="pose"/><category term="rendering"/><category term="3d"/><summary type="html"><![CDATA[COLMAP-Free 3D Gaussian Splatting (CVPR 2024)]]></summary></entry><entry><title type="html">Mip-NeRF 360</title><link href="https://semyeong-yu.github.io/blog/2024/MipNeRF360/" rel="alternate" type="text/html" title="Mip-NeRF 360"/><published>2024-08-11T01:03:00+00:00</published><updated>2024-08-11T01:03:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/MipNeRF360</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/MipNeRF360/"><![CDATA[<h2 id="mip-nerf-360-unbounded-anti-aliased-neural-radiance-fields">Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields</h2> <h4 id="jonathan-t-barron-ben-mildenhall-dor-verbin-pratul-p-srinivasan-peter-hedman">Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, Peter Hedman</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2111.12077">https://arxiv.org/abs/2111.12077</a><br/> project website :<br/> <a href="https://jonbarron.info/mipnerf360/">https://jonbarron.info/mipnerf360/</a><br/> pytorch code :<br/> <a href="https://github.com/google-research/multinerf">https://github.com/google-research/multinerf</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>sampling 기법 개선하고, bounded scene으로 warp</li> <li>non-linear scene, ray parameterization :<br/> disparity에 비례하도록 sampling 개선하고<br/> bounded space로 mapping하여<br/> 임의의 방향과 깊이에 대한 unbounded scene 다룸</li> <li>efficient proposal-based online-distillation :<br/> higher capacity MLP을 조금만 evaluate해서<br/> 효율적으로 large scene 다룸</li> <li>interval-distortion-based regularizer :<br/> artifacts 줄이기 위해<br/> step-function을 delta-function에 가깝게 regularize</li> </ol> </blockquote> <h2 id="introduction">Introduction</h2> <ul> <li>임의의 <code class="language-plaintext highlighter-rouge">direction</code>(360 degrees)과 임의의 <code class="language-plaintext highlighter-rouge">depth</code>로 <code class="language-plaintext highlighter-rouge">unbounded</code> 되어있는 scene 문제 해결 <ul> <li>non-linear scene parameterization :<br/> sampling 개선하고 bounded space로 mapping하여<br/> 임의의 방향과 깊이에 대한 unbounded scene 다룰 수 있음</li> <li>online-distillation :<br/> higher capacity MLP을 조금만 evaluate해서 효율적으로 large scene 다룰 수 있음</li> <li>distortion-based regularizer :<br/> artifacts 줄이기 위한 regularization</li> </ul> </li> <li>NeRF model을 large unbounded scene에 적용하는 데 3가지 문제가 있다<br/> (자세한 내용은 스킵했는데 나중에 읽어보자) <ul> <li>Parametrization : Mip-NeRF는 3D coordinate가 bounded domain 안에 있는 경우만 처리 가능</li> <li>Efficiency : large-and-detailed scene은 large MLP를 필요로 해서 expensive</li> <li>Ambiguity : scene content가 임의의 distance에 있고 이는 only 적은 수의 ray로 관찰되기 때문에 inherent ambiguity 발생</li> </ul> </li> </ul> <h2 id="scene-and-ray-parameterization">Scene and Ray Parameterization</h2> <h3 id="ray-interval-parameterization">Ray Interval Parameterization</h3> <ul> <li> <p>Ray Interval Parameterization :<br/> samples의 경우 distance가 아니라 그의 역수인 <code class="language-plaintext highlighter-rouge">disparity에 비례</code>하여 분포하도록 하면<br/> 가까이 있는 content는 많이 sampling하고 멀리 있는 content는 덜 sampling함으로써<br/> <code class="language-plaintext highlighter-rouge">임의의 scale의 unbounded scene</code>을 잘 다룰 수 있음</p> </li> <li>NeRF : <ul> <li>NeRF에서는 distance에 비례하여 stratified uniform sampling 했음</li> <li>만약 NDC parameterization을 쓴다면<br/> NDC-space에서 distance에 비례하여 stratified uniform sampling 하면<br/> disparity (distance의 역수)에 비례하여 uniform sampling 한 것과 같은 효과를 가짐<br/> 그 이유는 <a href="https://semyeong-yu.github.io/blog/2024/NDC/">Normalized-Device-Coordinates</a> 의 Linear in Disparity 파트 참고</li> <li>그런데 NDC는 single direction으로만 unbounded된 scene (front-facing camera)에 대해서만 적합하고<br/> 모든 방향으로 unbounded된 scene에 대해서는 적합하지 않음</li> </ul> </li> <li>Mip-NeRF 360 : <ul> <li>처음부터 ray interval을 disparity (distance의 역수)에 비례하도록 <d-cite key="LLFF">[2]</d-cite> parameterize 한다</li> </ul> </li> </ul> <h3 id="ray-interval-parameterization-in-disparity">Ray Interval Parameterization in Disparity</h3> <ul> <li>distance along ray를 t-space 또는 s-space에서 나타내자 <ul> <li>t-space :<br/> Euclidean ray distance \(t \in [t_n, t_f]\)<br/> \(t = g^{-1}(s \cdot g(t_f) + (1-s) \cdot g(t_n))\)</li> <li>s-space :<br/> normalized ray distance \(s \in [0, 1]\)<br/> \(s = \frac{g(t)-g(t_n)}{g(t_f)-g(t_n)}\)</li> </ul> </li> <li>사용 예시 : <ul> <li>\(g(x) = \frac{1}{x}\) 로 설정할 경우<br/> <code class="language-plaintext highlighter-rouge">s-space에서 uniform sampling</code>하면<br/> <code class="language-plaintext highlighter-rouge">t-space에서 disparity에 비례</code>하여 distributed</li> <li>\(g(x) = log(x)\) 로 설정할 경우<br/> s-space에서 uniform sampling하면<br/> t-space에서는 logarithmic spacing <d-cite key="DONeRF">[3]</d-cite> 으로 distributed</li> </ul> </li> <li>기존 NeRF 모델에서는 t-distance를 따라 uniform sampling했지만<br/> 본 논문에서는 <code class="language-plaintext highlighter-rouge">s-distance</code>를 따라 uniform sampling하여 나타낸다</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/4-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/4-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="scene-parameterization">Scene Parameterization</h3> <ul> <li> <p>Scene Parameterization :<br/> unbounded scene을 radius-2 내부의 <code class="language-plaintext highlighter-rouge">bounded space</code>로 mapping하기 위해 <code class="language-plaintext highlighter-rouge">contract 함수</code>를 사용<br/> ray parameterization을 할 때 disparity에 비례하게 sampling 했으므로<br/> contract 함수도 consistently <code class="language-plaintext highlighter-rouge">disparity에 비례</code>하게 bounded space로 mapping<br/> \(\rightarrow\) scene origin에서 cast된 ray의 경우 contract 함수를 적용한 후에는 아래 그림의 주황색 영역에서 일정한 길이의 interval을 가진다</p> </li> <li>Define smooth coordinate-transformation function as \(f(x) : R^3 \rightarrow R^3\) <ul> <li>\(\mu, \Sigma\) 를 갖는 Gaussian에 non-linear \(f\) 를 적용하여 \(\mu_{c}, \Sigma_{c}\) 를 갖는 Gaussian으로 변형하려면<br/> \((\mu_{c}, \Sigma_{c}) = f(\mu, \Sigma) = (f(\mu), J_{f}(\mu) \Sigma J_{f}(\mu)^T)\)<br/> where \(f(x) \approx f(\mu) + J_{f}(\mu)(x-\mu)\) (linear approx.)</li> <li>이는 state transition model \(f = \text{contract}(x) = \begin{cases} x &amp; \text{if} \| x \| \leq 1 \\ (2 - \frac{1}{\| x \|})(\frac{x}{\| x \|}) &amp; \text{if} \| x \| \gt 1 \end{cases}\) 을 사용했을 때<br/> classic Extended Kalman filter <d-cite key="kalman">[1]</d-cite> 와 수학적으로 동일</li> <li>MipNeRF360에서는 contract 함수를<br/> <code class="language-plaintext highlighter-rouge">point가 아니라</code> Euclidean 3D-space에 있는 <code class="language-plaintext highlighter-rouge">Gaussian</code>에 적용!<br/> 또한<br/> <code class="language-plaintext highlighter-rouge">모든 방향</code> (360 degress)에 대해 적용!</li> </ul> </li> <li>IPE (integrated positional encoding) : <ul> <li>Mip-NeRF :<br/> \(\gamma (\mu, \Sigma) = \left[ \begin{bmatrix} sin(2^l \mu) \circledast exp(-\frac{1}{2} 4^l diag(\Sigma)) \\ cos(2^l \mu) \circledast exp(-\frac{1}{2} 4^l diag(\Sigma)) \end{bmatrix} \right]_{l=0}^{l=L-1}\)</li> <li>Mip-NeRF 360 :<br/> \(\gamma (\text{contract}(\mu, \Sigma))\)<br/> where<br/> \(f(x) = \text{contract}(x) = \begin{cases} x &amp; \text{if} \| x \| \leq 1 \\ (2 - \frac{1}{\| x \|})(\frac{x}{\| x \|}) &amp; \text{if} \| x \| \gt 1 \end{cases}\)<br/> and \(f(x) \approx f(\mu) + J_{f}(\mu)(x-\mu)\)<br/> and \(f(\mu, \Sigma) = (f(\mu), J_{f}(\mu) \Sigma J_{f}(\mu)^T)\)</li> <li>Mip-NeRF 360 procedure :<br/> casting cone<br/> \(\rightarrow\) uniform sampling in s-space<br/> \(\rightarrow\) contract 3D Gaussians in t-space into bounded sphere<br/> \(\rightarrow\) IPE \(\gamma\)<br/> \(\rightarrow\) MLP</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/1-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/1-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> contract 함수는 파란색 구(radius 1) 외부의 Gaussian(회색)을 주황색 영역(radius 1 ~ 2)의 Gaussian(빨간색)으로 mapping </div> <h2 id="coarse-to-fine-online-distillation">Coarse-to-Fine Online Distillation</h2> <ul> <li>기존 NeRF :<br/> coarse-MLP와 fine-MLP</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/2-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/2-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 위의 그림은 Mip-NeRF, 아래의 그림은 Mip-NeRF 360 </div> <ul> <li>Mip-NeRF 360 :<br/> proposal-MLP와 NeRF-MLP <ul> <li><code class="language-plaintext highlighter-rouge">small</code> proposal-MLP는 <code class="language-plaintext highlighter-rouge">many</code> samples로 <code class="language-plaintext highlighter-rouge">여러 번</code> evaluate하고,<br/> <code class="language-plaintext highlighter-rouge">large</code> NeRF-MLP는 <code class="language-plaintext highlighter-rouge">less</code> samples로 <code class="language-plaintext highlighter-rouge">딱 한 번</code> evaluate함으로써<br/> Mip-NeRF보다 조금만 더 costly하지만 훨씬 더 <code class="language-plaintext highlighter-rouge">higher capacity</code>를 가진 것과 같은 효과<br/> \(\rightarrow\) 효율적으로 <code class="language-plaintext highlighter-rouge">large unbounded scene</code>을 표현하기에 적절<br/> distill 효과가 좋아서 proposal-MLP의 경우 크기 줄이더라도 accuracy 감소하지 않음</li> <li>small proposal-MLP : <ul> <li>color 말고 volume density만 예측하여 weight \(\hat w\) 구함</li> </ul> </li> <li>large NeRF-MLP : <ul> <li>color, volume density 예측하여 weight \(w\) 구하고 rendering</li> </ul> </li> </ul> </li> <li>Loss :<br/> 아래 두 가지 loss로 각 MLP를 jointly train <ul> <li><code class="language-plaintext highlighter-rouge">reconstruction loss</code> : <ul> <li>large NeRF-MLP에서 rendering해서 구함<br/> 기존 NeRF 방식과 동일</li> <li><code class="language-plaintext highlighter-rouge">GT-image를 supervision</code>으로 하여 <code class="language-plaintext highlighter-rouge">NeRF-MLP만 업데이트</code></li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">proposal loss</code> : <ul> <li>두 MLP의 <code class="language-plaintext highlighter-rouge">weight histogram이 consistent</code>하도록 함<br/> 즉, proposal-MLP의 weight histogram이 NeRF-MLP의 weight histogram을 따라잡도록 함<br/> (Mip-NeRF 계열은 point가 아니라 interval 별로 weight를 구하므로 histogram을 만들 수 있음)</li> <li><code class="language-plaintext highlighter-rouge">NeRF-MLP의 weight를 supervision</code>으로 하여 <code class="language-plaintext highlighter-rouge">proposal-MLP만 업데이트</code><br/> (<code class="language-plaintext highlighter-rouge">online distillation</code> of NeRF-MLP’s knowledge into proposal-MLP)</li> <li>문제 :<br/> 하나의 histogram bin의 distribution에 대해 아무 것도 가정할 수 없음<br/> (하나의 bin의 distribution이 uniform일 수도 있고 특정 지점에 몰빵된 delta function일 수도 있음…)<br/> coarse \(\hat t\) 와 fine \(t\) (bins)가 매우 다를 수 있음</li> <li>가정 :<br/> 두 개의 histogram이 매우 달라보이더라도<br/> 둘 다 <code class="language-plaintext highlighter-rouge">어떤 하나의 동일한 (underlying continuous) true mass distribution으로부터 유래되었다고 설명할 수 있다면</code> 둘의 차이인 loss는 0 이다</li> <li>위의 가정에 따라<br/> NeRF-MLP (\(t\), \(w\))의 구간 \(T\) 와 겹치는 모든 proposal-MLP의 weight \(\hat w_{j}\) 를 더해서 아래와 같이 NeRF-MLP weight \(w\) 의 <code class="language-plaintext highlighter-rouge">upper bound</code>를 구하자<br/> \(\text{bound}(\hat t, \hat w, T) = \sum_{j: T \cap \hat T_{j} \neq \emptyset} \hat w_{j}\)<br/> (\(t\) 와 \(\hat t\) 가 정렬되어 있으므로 summed-area table로 효율적으로 계산 가능)</li> <li>만약 두 개의 histogram이 consistent하다면,<br/> NeRF-MLP (\(t\), \(w\))의 모든 구간 (\(T_i, w_i\))에 대해<br/> \(w_i \leq \text{bound}(\hat t, \hat w, T_i)\) 이어야 한다<br/> \(\rightarrow\)<br/> 아래와 같이 <code class="language-plaintext highlighter-rouge">proposal loss는 이를 위반하는 경우</code>에 해당한다<br/> \(L_{prop}(t, w, \hat t, \hat w) = \sum_{i}\frac{1}{w_i} \text{max}(0, w_i - \text{bound}(\hat t, \hat w, T_i))^2\)</li> <li>즉, <code class="language-plaintext highlighter-rouge">proposal-MLP가 NeRF-MLP의 upper-bound를 형성</code>하도록 한다는 것은<br/> proposal-MLP가 NeRF-MLP histogram의 개형을 <code class="language-plaintext highlighter-rouge">따라잡도록</code> 하는 효과!</li> <li>proposal loss가 asymmetirc loss인 이유 :<br/> proposal-MLP가 NeRF-MLP보다 coarse하기 때문에<br/> proposal-MLP weight가 NeRF-MLP weight의 upper bound를 형성하는 게 (overestimate) 당연하고,<br/> proposal-MLP weight가 NeRF-MLP weight를 underestimate (\(\text{bound}(\hat t, \hat w, T_i) &lt; w_i\)) 하는 경우에만 penalize</li> <li>proposal loss term에서 \(w_i\) 로 나누는 이유 :<br/> bound가 0일 때 \(\frac{dL_{prop}}{d\text{bound}} = \sum_{i} \frac{1}{w_i} \cdot 2 \cdot \text{max}(0, w_i - \text{bound}) \cdot (-1) = -2\sum_{i}1\) 와 같이<br/> \(w_i\) 크기와 상관없이 <code class="language-plaintext highlighter-rouge">gradient 값이 상수값</code>이 되어 균등하게 penalize하여 optimization에 도움됨</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/8-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/8-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 위의 histogram이 NeRF-MLP, 아래의 histogram이 proposal-MLP, 보통 proposal-MLP가 coarse하고 NeRF-MLP가 fine한데 여기선 반대로 그려져 있음 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/3-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/3-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> fine NeRF-MLP는 점점 scene content의 surface 쪽으로 weight가 집중되고, coarse proposal-MLP는 이를 따라잡으며 upper bound를 형성 </div> <h2 id="regularization-for-interval-based-models">Regularization for Interval-Based Models</h2> <ul> <li>Artifacts :<br/> NeRF 계열은 pose 문제 때문에 두 가지 주된 artifacts가 나타난다 <ul> <li><code class="language-plaintext highlighter-rouge">floater</code> :<br/> 특정 view를 너무 잘 설명하려던 나머지<br/> 실제로 물체가 존재하지 않는 small disconnected regions of dense volume에서 불필요하게 opacity를 예측하여<br/> 다른 view에서 보면 반투명한 blurry cloud처럼 보이는 부분</li> <li><code class="language-plaintext highlighter-rouge">background collapse</code> :<br/> 멀리 있는 surface가<br/> 반투명한 가까운 content로 잘못 모델링된 경우</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/5-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/5-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 반투명하게 떠다니는 게 floater, 좌하단에서 background surface가 가깝게 보이는 게 background collapse </div> <ul> <li>Artifacts 완화 : <ul> <li>기존 NeRF : <code class="language-plaintext highlighter-rouge">random noise</code><br/> <a href="https://semyeong-yu.github.io/blog/2024/NeRFcode/">NeRF-Code</a> 의 raw2outputs()에서 볼 수 있듯이<br/> raw-opacity에 random noise 더해서 \(\sigma_{i}\) 구함<br/> noise 덕분에 <code class="language-plaintext highlighter-rouge">불필요한 특정 지점에 overfit 되는 게 아니라 일관성 있게</code> 학습<br/> But, 부분적으로 artifacts 완화하고 reconstruction quality를 떨어뜨림</li> <li>Mip-NeRF 360 : <code class="language-plaintext highlighter-rouge">regularize</code><br/> ray-sampling은 이미 했고 weight를 구할 때<br/> <code class="language-plaintext highlighter-rouge">물체가 있을만한 정확한 지점에서 집중적으로 예측</code>하여<br/> <code class="language-plaintext highlighter-rouge">부정확한 지점에서의 불필요한 예측을 억제</code></li> </ul> </li> <li>Regularization for Interval-Based Models : <ul> <li><code class="language-plaintext highlighter-rouge">distortion loss</code> :<br/> step-function인 weight-histogram \(s, w\) 을 regularize하기 위해<br/> \(L_{dist}(s, w) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} w_s(u)w_s(v)|u-v|d_ud_v\) <ul> <li><code class="language-plaintext highlighter-rouge">NeRF-MLP 업데이트</code>할 때 artifacts 완화(<code class="language-plaintext highlighter-rouge">regularization</code>)하는 역할</li> </ul> </li> <li>위의 loss를 최소화하기 위해선<br/> \(w\) 를 매우 작은 \(|u-v|\) 에 몰빵하면 된다<br/> 즉, 위의 loss term만 있을 경우 histogram(step-function)이 <code class="language-plaintext highlighter-rouge">delta-function</code>에 가까워지면 된다</li> <li>t-distance 대신 <code class="language-plaintext highlighter-rouge">s-distance</code> 사용 :<br/> t-distance 사용하면 먼 거리에 있는 interval 길이가 길기 때문에 무조건 먼 거리의 interval 쪽으로 distortion 됨<br/> s-distance 기준으로 weight-histogram 만들어서 distortion loss 구하자!</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/6-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/6-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">distortion loss</code> : <ul> <li>weight \(w\) 는 step-function (각 interval 안에선 constant) 이므로<br/> 아래와 같이 계산하기 쉬운 꼴로 유도할 수 있음<br/> \(L_{dist}(s, w) = \sum_{i, j} w_i w_j |\frac{s_i + s_{i+1}}{2} - \frac{s_j + s_{j+1}}{2}| + \frac{1}{3} \sum_{i} w_i^2 (s_{i+1} - s_i)\)</li> <li>유도 과정 :<br/> 출처 : https://charlieppark.kr/ <ul> <li>\(L_{dist}(s, w) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} w_s(u)w_s(v)|u-v|d_ud_v\)<br/> where \(w_s(u) = w_i\) for \(u \in [s_i, s_{i+1})\)</li> <li>case 1. \(u, v\) are in the same interval : \(u, v \in [s_i, s_{i+1})\)<br/> \(\int_{s_i}^{s_{i+1}}\int_{s_i}^{s_{i+1}}w_i^2|u-v|d_ud_v\)<br/> \(= w_i^2 \frac{(s_{i+1}-s_i)^3}{3}\)</li> <li>case 2. \(u, v\) are in different intervals : \(u \in [s_i, s_{i+1}), v \in [s_j, s_{j+1})\) where \(i \neq j\)<br/> \(\int_{s_i}^{s_{i+1}}\int_{s_j}^{s_{j+1}}w_iw_j|u-v|d_ud_v\)<br/> \(\simeq \int_{s_i}^{s_{i+1}}\int_{s_j}^{s_{j+1}}w_iw_j|\frac{s_i + s_{i+1}}{2} - \frac{s_j + s_{j+1}}{2}|d_ud_v\)<br/> \(= w_iw_j|\frac{s_i + s_{i+1}}{2} - \frac{s_j + s_{j+1}}{2}|\cdot (s_{i+1}-s_i)(s_{j+1}-s_j)\)</li> <li> \[L_{dist}(s, w) = \sum_{i} w_i^2 \frac{(s_{i+1}-s_i)^3}{3} + \sum_{i, j} w_iw_j|\frac{s_i + s_{i+1}}{2} - \frac{s_j + s_{j+1}}{2}|\cdot (s_{i+1}-s_i)(s_{j+1}-s_j)\] </li> <li>\((s_{i+1} - s_i)^2\) 항과 \((s_{i+1}-s_i)(s_{j+1}-s_j)\) 항을 제거하여 학습의 안정성을 높임<br/> \(L_{dist}(s, w) = \frac{1}{3} \sum_{i} w_i^2 (s_{i+1} - s_i) + \sum_{i, j} w_i w_j |\frac{s_i + s_{i+1}}{2} - \frac{s_j + s_{j+1}}{2}|\)</li> <li>\(u, v\) 가 <code class="language-plaintext highlighter-rouge">same interval</code>에 있을 경우에는 \((s_{i+1}-s_i)\) 항으로 <code class="language-plaintext highlighter-rouge">각 구간의 (weighted) 너비</code>를 줄이고,<br/> \(u, v\) 가 <code class="language-plaintext highlighter-rouge">different interval</code>에 있을 경우에는 \(|\frac{s_i + s_{i+1}}{2} - \frac{s_j + s_{j+1}}{2}|\) 항으로 <code class="language-plaintext highlighter-rouge">두 구간 사이의 (weighted) 중심 거리</code>를 줄임<br/> 이 원리를 통해<br/> entire ray is unoccupied이 가능하다면 모든 weight가 0에 가까워지려 하고<br/> 불가능하다면 <code class="language-plaintext highlighter-rouge">weight를 few interval에 몰빵</code>하려 해서<br/> weight-histogram이 delta-function에 가까워질 수 있음</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/7-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/7-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="optimization">Optimization</h2> <ul> <li>Setting : <ul> <li>proposal-MLPs with 4 layers and 256 hidden_dim<br/> two proposal-MLP \((\hat s^0, \hat w^0)\) and \((\hat s^1, \hat w^1)\) each using 64 samples</li> <li>NeRF-MLP \((s, w)\) with 8 layers and 1024 hidden_dim<br/> one NeRF-MLP using 32 samples<br/> (NeRF와 Mip-NeRF에서는 MLP with 8 layers and 256 hidden_dim 사용했었음)</li> <li>total loss :<br/> \(L_{tot} = L_{recon}(C(t), C^{\ast}) + \lambda L_{dist}(s, w) + \sum_{k=0}^{1}L_{prop}(s, w, \hat s^k, \hat w^k)\)<br/> averaged over all rays in batch<br/> where author sets \(\lambda = 0.01\)</li> <li>\(L_{recon}\) and \(L_{dist}\) for NeRF-MLP <ul> <li>\(L_{recon}(x, x^{\ast}) = \sqrt{(x - x^{\ast})^2 + \epsilon^{2}}\) : Charbonnier loss<br/> slightly more stable than MSE</li> </ul> </li> <li>\(L_{prop}\) for proposal-MLP</li> <li>learning schedule :<br/> 250k iter. with batch size \(2^{14}\)<br/> Adam optimizer with \(\beta_{1} = 0.9, \beta_{2} = 0.999, \epsilon = 10^{-6}\)<br/> lr is annealed log-linearly from \(2 \times 10^{-3}\) to \(2 \times 10^{-5}\)<br/> warm-up phase of 512 iter.<br/> gradient clipping to norm of \(10^{-3}\)</li> </ul> </li> </ul> <h2 id="conclusion">Conclusion</h2> <ul> <li>Mip-NeRF extension for real-world unbounded scenes with unconstrained camera depth and orientations</li> <li>(Kalman-like) scene and ray parameterization</li> <li>efficient proposal-based coarse-to-fine distillation framework</li> <li>interval-distortion-based regularizer</li> <li>Mip-NeRF에 비해 57% reduction in MSE</li> </ul> <h2 id="question">Question</h2> <ul> <li>Q&amp;A reference : 3DGS online study</li> <li>Q1 : 아래의 문구가 이해되지 않습니다<br/> recall that the “bins” of those histograms \(t\) and \(\hat t\) need not be similar; indeed, if the proposal MLP successfully culls the set of distances where scene content exists, \(\hat t\) and \(t\) will be highly dissimilar</li> <li>A1 : 아래 사진의 (c)에서처럼 충분히 optimize되어 만약 coarse proposal-MLP가 이미 scene content가 있는 곳을 성공적으로 예측하고 있다면 이를 이용한 fine NeRF-MLP의 fine-samples는 그 곳에 더 촘촘히 존재할 것이므로 bin 간격이 달라져서 두 histogram이 크게 달라보인다<br/> 달라보이더라도 두 개의 histogram이 어떤 하나의 (true continuous underlying) mass distribution에서 유래되었다고 설명할 수 있으면 둘의 차이가 0이라고 가정하여 upper bound를 이용해서 proposal loss 만듬</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/3-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/3-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Q2 : 갑자기 든 생각인데 Mip-NeRF 360의 sampling 기법과 contract 함수가 background collapse의 원인이 될 수도 있지 않을까요?<br/> disparity에 비례하게 sampling하므로 먼 거리에 대해서는 덜 sampling한 채로 bounded space로 warp하는데,<br/> 먼 거리의 content에 대해 정보가 부족한 채로 warp하는 과정에서 왜곡이 일어날 수 있을 것 같다</li> <li>A2 : 그럴 수 있을 것 같습니다</li> </ul> <h2 id="code-review">Code Review</h2> <p>TBD</p> <h2 id="appendix">Appendix</h2> <h3 id="off-axis-positional-encoding">Off-Axis Positional Encoding</h3> <ul> <li> <p>Mip-NeRF의 IPE 식 :<br/> PE-basis를 identity matrix로 설정하였으므로<br/> \(P = \begin{pmatrix} \begin{matrix} 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2\end{matrix} &amp; \cdots &amp; \begin{matrix} 2^{L-1} &amp; 0 &amp; 0 \\ 0 &amp; 2^{L-1} &amp; 0 \\ 0 &amp; 0 &amp; 2^{L-1}\end{matrix} \end{pmatrix}^T\)<br/> \(diag(\Sigma_{r})\) 계산을 위해 \(diag(\Sigma)\) 만 알면 됨</p> </li> <li> <p>Mip-NeRF 360의 IPE 식 :<br/> IPE를 하기 전에 우선 Gaussian을 radius-2의 구 안으로 contract 해야 해서<br/> <code class="language-plaintext highlighter-rouge">어차피 full covariance matrix</code> \(\Sigma\) 가 필요하므로<br/> PE-basis를 아래와 같이 둔다<br/> (\(P\) 의 각 column이 twice-tessellated icosahedron(두 번 테셀레이션 된 정이십면체)의 unit-norm vertex이고, 음의 같은 방향으로 중복된 vertex는 제거)<br/> \(P = \begin{bmatrix} 0.8506508 &amp; 0 &amp; 0.5257311 \\ 0.809017 &amp; 0.5 &amp; 0.309017 \\ 0.5257311 &amp; 0.8506508 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0.809017 &amp; 0.5 &amp; -0.309017 \\ 0.8506508 &amp; 0 &amp; -0.5257311 \\ 0.309017 &amp; 0.809017 &amp; -0.5 \\ 0 &amp; 0.5257311 &amp; -0.8506508 \\ 0.5 &amp; 0.309017 &amp; -0.809017 \\ 0 &amp; 1 &amp; 0 \\ -0.5257311 &amp; 0.8506508 &amp; 0 \\ -0.309017 &amp; 0.809017 &amp; -0.5 \\ 0 &amp; 0.5257311 &amp; 0.8506508 \\ -0.309017 &amp; 0.809017 &amp; 0.5 \\ 0.309017 &amp; 0.809017 &amp; 0.5 \\ 0.5 &amp; 0.309017 &amp; 0.809017 \\ 0.5 &amp; -0.309017 &amp; 0.809017 \\ 0 &amp; 1 &amp; 0 \\ -0.5 &amp; 0.309017 &amp; 0.809017 \\ -0.809017 &amp; 0.5 &amp; 0.309017 \\ -0.809017 &amp; 0.5 &amp; -0.309017 \end{bmatrix}\)</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/9-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/9-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> column of PE-basis P </div> <ul> <li>Mip-NeRF 360의 IPE 식 : <ul> <li><code class="language-plaintext highlighter-rouge">off-axis feature</code>를 쓰면 <code class="language-plaintext highlighter-rouge">anisotropic Gaussian의 모양</code>까지 잘 encode할 수 있어서 rendering quality 향상</li> <li>IPE 식에서 \(diag(\Sigma_{r}) = diag(P \Sigma P^T)\) 를 계산할 때<br/> large \(P\) matrix에 대해 행렬곱을 하려면 너무 비싸므로<br/> \(diag(P \Sigma P^T)\) 대신 \(\text{sum}(P^T \circledast (\Sigma P^T), \text{dim}=0, \text{keepdim=False})\) 로 계산하면<br/> Mip-NeRF의 Axis-Aligned IPE보다 Mip-NeRF 360의 Off-Axis IPE가 살짝만 더 expensive</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/10-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/10-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 왼쪽은 Mip-NeRF의 Axis-Aligned IPE이고, 오른쪽은 Mip-NeRF 360의 Off-Axis IPE, 각 encoding basis로 Gaussian을 projection해서 marginal distribution을 구했을 때 Off-Axis Projection을 해야 Gaussian shape를 더 잘 구분할 수 있음 </div> <h3 id="annealing-weight">Annealing Weight</h3> <ul> <li> <p>proposal-weight PDF로 fine-sampling할 때<br/> proposal-weight \(\hat w\) 대신 \(\hat w_n \propto \hat w^{\frac{bn/N}{(b-1)n/N+1}}\)<br/> where \(n\) iter. out of \(N\) steps<br/> where 지수부분 \(\frac{bn/N}{(b-1)n/N+1}\) 은 0부터 1까지 빠르게 증가<br/> where bias hyper-param. \(b=10\)</p> </li> <li> <p>\(n=0\) 일 때는 \(\hat w_0 \propto 1\) 을 따라 sampling하고<br/> \(n=N\) 일 때는 \(\hat w_N \propto \hat w\) 을 따라 sampling하므로<br/> <code class="language-plaintext highlighter-rouge">훈련 초기</code> 단계에서 NeRF-MLP가 더 다양한 proposal-interval 범위를 <code class="language-plaintext highlighter-rouge">모험</code>(exploration)할 수 있도록 함</p> </li> </ul> <h3 id="dilation">Dilation</h3> <ul> <li> <p>proposal-weight PDF로 fine-sampling할 때<br/> aliasing-artifacts 줄이기 위해 먼저 proposal-histogram \((\hat t, \hat w)\) 을 dilate</p> </li> <li> <p>이유 :<br/> proposal-MLP는 오직 input ray로만 supervised되므로<br/> 특정 각도에 대해서만 예측 가능할 수 있다<br/> (<code class="language-plaintext highlighter-rouge">proposal-MLP는 rotationally aliased</code>)</p> </li> <li> <p>해결 : dilation<br/> proposal-MLP의 <code class="language-plaintext highlighter-rouge">interval을 넓힘</code>으로써 anti-aliasing<br/> <code class="language-plaintext highlighter-rouge">왜 interval 넓히면 anti-aliasing??? weight 변화가 급격하지 않아서???</code></p> <ul> <li>Step 1) proposal-MLP의 histogram \((\hat s, \hat w)\) 에서 weight를 구간 길이로 나눈 뒤 normalize해서 probability density \(\hat p\) 로 만듬<br/> \(\hat p_i = \frac{\hat w_i}{(\hat s_{i+1} - \hat s_{i})}\)</li> <li>Step 2) dilation factor (얼만큼 각 구간을 넓힐지) 계산<br/> fine intervals 개수가 많을수록 평균적인 구간 길이가 짧아지고 dilation factor가 작아짐<br/> \(\epsilon = \frac{a}{\prod_{k'=1}^{k-1} n_{k'}} + b\)<br/> where \(k\) 번째 coarse-to-fine resampling 단계에서 \(n_k\) 개의 fine intervals을 resample</li> <li>Step 3) 각 구간 \([\hat s_{i}, \hat s_{i+1}]\) 를 \([\hat s_{i} - \epsilon, \hat s_{i+1} + \epsilon]\) 로 확장</li> <li>Step 4) 확장한 각 구간마다 probability density 최대값을 구함<br/> \(\text{max}_{\hat s - \epsilon \leq s \lt \hat s + \epsilon} \hat p_{\hat s} (s)\)<br/> where \(\hat p_{\hat s} (s)\) is interpolation into the step function defined by \(\hat s, \hat p\) at \(s\)</li> <li>Step 5) 다시 구간 길이로 곱한 뒤 normalize해서 weight-histogram으로 만듬</li> </ul> </li> </ul> <h3 id="sampling">Sampling</h3> <ul> <li>Mip-NeRF : <ul> <li>sampling 방식 :<br/> coarse-histogram으로부터 \(n+1\) 개의 t-distance를 sampling한 뒤<br/> 각 fine-samples를 끝점으로 하여 \(n\) 개의 fine-intervals을 얻음</li> <li>단점 :<br/> 아래 그림과 같이 samples가 coarse-histogram 각 구간의 전범위를 전부 span하지 못하여<br/> 일부 구간이 sampling에서 제외되므로(충분히 다뤄지지 않으므로)<br/> coarse-histogram의 영향을 서서히 약화시킴</li> </ul> </li> <li>Mip-NeRF 360 : <ul> <li>sampling 방식 :<br/> coarse-histogram으로부터 \(n\) 개의 s-distance를 sampling한 뒤<br/> 각 구간의 mid-points \(n-1\) 개와 <code class="language-plaintext highlighter-rouge">coarse-sample의 양끝 점 2개</code>를 끝점으로 하여 \(n\) 개의 fine-intervals를 얻음</li> <li>효과 :<br/> samples가 coarse-histogram의 처음과 끝 구간도 전부 span하고<br/> 불규칙한 resampling도 감소하여<br/> rendering quality는 큰 변화 없지만 aliasing 줄이는 데 도움됨</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/11-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/11-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> sampling한 건 막대로 표시하였고, 이로부터 8개의 fine-intervals를 얻음 </div> <h3 id="background-colors">Background Colors</h3> <ul> <li>NeRF와 Mip-NeRF : <ul> <li>훈련시킬 때 background color를 <code class="language-plaintext highlighter-rouge">black or white</code>로 설정하는데<br/> scene recon.할 때 background가 <code class="language-plaintext highlighter-rouge">반투명</code>하게 reconstruct될 수도 있다</li> <li>문제 :<br/> 반투명한 background는<br/> view synthesis 자체에는 문제가 없지만<br/> ray-termination-distance가 부정확하여<br/> <code class="language-plaintext highlighter-rouge">부정확한 depth-map</code>을 생성할 수 있다</li> </ul> </li> <li>Mip-NeRF 360 : <ul> <li>Blender dataset :<br/> 이전과 똑같이 black or white로 background color 설정</li> <li>360 and LLFF dataset :<br/> 훈련시킬 때 background color를 \([0, 1]^3\) 사이의 <code class="language-plaintext highlighter-rouge">random color</code>로 설정하여<br/> scene recon.할 때 fully-opaque background이도록 함</li> </ul> </li> </ul> <h3 id="implementation-details">Implementation Details</h3> <ul> <li>\(\mu, \Sigma\) 를 갖는 Gaussian에 non-linear \(f\) 를 적용하여 \(\mu_{c}, \Sigma_{c}\) 를 갖는 Gaussian으로 변형하기 위해<br/> \((\mu_{c}, \Sigma_{c}) = (f(\mu), J_{f}(\mu) \Sigma J_{f}(\mu)^T)\)<br/> where \(f(x) \approx f(\mu) + J_{f}(\mu)(x-\mu)\) (linear approx.)</li> <li>이 때, \(J_{f}(\mu)\) matrix는 autodiff framework로 직접 계산할 수 있지만<br/> 사실 우리는 직접 explicitly matrix를 만들 필요가 없음</li> <li><code class="language-plaintext highlighter-rouge">less expensive 계산</code> 위해<br/> \(J_{f}(\mu)\)와 행렬곱하는 것과 똑같은 역할을 하는 함수를 이용<br/> e.g. \(J_{f}(\mu) \Sigma J_{f}(\mu)^T\) 계산 위해<br/> <code class="language-plaintext highlighter-rouge">Jax의 linearize</code> operator <d-cite key="Jax">[4]</d-cite>를 두 번 적용</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="rendering"/><category term="3d"/><category term="multiscale"/><category term="antialiasing"/><summary type="html"><![CDATA[Unbounded Anti-Aliased Neural Radiance Fields]]></summary></entry><entry><title type="html">Multi-Modal Study</title><link href="https://semyeong-yu.github.io/blog/2024/Multimodal/" rel="alternate" type="text/html" title="Multi-Modal Study"/><published>2024-08-05T15:00:00+00:00</published><updated>2024-08-05T15:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Multimodal</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Multimodal/"><![CDATA[<h2 id="multi-modal-study">Multi-Modal Study</h2> <h2 id="storyimager---a-unified-and-efficient-framework-for-coherent-story-visualization-and-completion">StoryImager - A Unified and Efficient Framework for Coherent Story Visualization and Completion</h2> <h4 id="ming-tao-bing-kun-bao-hao-tang-yaowei-wang-changsheng-xu">Ming Tao, Bing-Kun Bao, Hao Tang, Yaowei Wang, Changsheng Xu</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2404.05979">StoryImager</a></p> </blockquote> <ul> <li><code class="language-plaintext highlighter-rouge">Story Visualization</code> Task : <ul> <li>input : prince image / cat image / prompts</li> <li>output : story images</li> <li>video generation과는 다른 게, story visualization은 웹툰 같다고 생각하면 됨<br/> story visualization은 image 간의 consistency를 유지하긴 하지만, video generation처럼 frame끼리 연속성을 보장할 필요는 없음</li> </ul> </li> <li>StoryImager: <ul> <li>task : coherent story visualization and completion</li> <li>기존 모델은 visualization과 continuation을 위한 model을 별도로 필요했는데,<br/> 본 논문은 single model (<code class="language-plaintext highlighter-rouge">통합적인 framework</code>) 제시<br/> by <code class="language-plaintext highlighter-rouge">global consistency</code> 반영!</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/StoryImager/1-480.webp 480w,/assets/img/2024-08-05-Multimodal/StoryImager/1-800.webp 800w,/assets/img/2024-08-05-Multimodal/StoryImager/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/StoryImager/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Architecture :<br/> maintain <code class="language-plaintext highlighter-rouge">global consistency</code><br/> by context-feature-extractor<br/> and FS-CAM (frame-story cross-attention module)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/StoryImager/2-480.webp 480w,/assets/img/2024-08-05-Multimodal/StoryImager/2-800.webp 800w,/assets/img/2024-08-05-Multimodal/StoryImager/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/StoryImager/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p><code class="language-plaintext highlighter-rouge">random masking</code> :<br/> make a story board from story images<br/> \(\rightarrow\) VAE<br/> \(\rightarrow\) random masking to VAE latent space</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">context-feature-extractor</code> :</p> <ul> <li>word-embeddings \(\rightarrow\) transformer<br/> \(\rightarrow\) prior embeddings<br/> \(\rightarrow\) MLP<br/> \(\rightarrow\) frame-aware latent prior<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">story board images가 story prompts를 반영하도록</code> 하기 위해 masked VAE latent space와 concat해서 이를 FS-CAM에서 story board로 사용</li> <li>word-embeddings \(\rightarrow\) transformer<br/> \(\rightarrow\) context embeddings<br/> \(\rightarrow\) transformer<br/> \(\rightarrow\) global context<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">global 정보</code> 주기 위해 FS-CAM에서 text prompts로 사용</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/StoryImager/4-480.webp 480w,/assets/img/2024-08-05-Multimodal/StoryImager/4-800.webp 800w,/assets/img/2024-08-05-Multimodal/StoryImager/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/StoryImager/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>FS-CAM (<code class="language-plaintext highlighter-rouge">frame-story cross-attention module</code>) :<br/> 개별 story board - 개별 text prompts 를 locally cross-attention한 것과,<br/> 전체 story board - 전체 text prompts 를 globally cross-attention한 것을<br/> concat</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/StoryImager/3-480.webp 480w,/assets/img/2024-08-05-Multimodal/StoryImager/3-800.webp 800w,/assets/img/2024-08-05-Multimodal/StoryImager/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/StoryImager/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="intelligent-grimm---open-ended-visual-storytelling-via-latent-diffusion-models">Intelligent Grimm - Open-ended Visual Storytelling via Latent Diffusion Models</h2> <h4 id="chang-liu-haoning-wu-yujie-zhong-xiaoyun-zhang-yanfeng-wang-weidi-xie">Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, Yanfeng Wang, Weidi Xie</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2306.00973">Intelligent Grimm</a></p> </blockquote> <ul> <li>Intelligent Grimm : <ul> <li>NIPS 2023에서 novelty 부족으로 reject 당했다가 보완해서 CVPR 2024에 accept</li> <li>task : open-ended visual storytelling</li> <li>StoryGen : unseen characters에 대해서도 추가적인 character-specific-optimization 없이 story visualization 가능</li> <li>StorySalon : online-video, open-source e-book 등 소싱해서 만든 dataset</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">context encoding</code> : <ul> <li>SDM : pre-trained stable diffusion model<br/> CLIP : pre-trained CLIP encoder<br/> VAE : pre-trained VAE</li> <li>visual condition feature = [SDM(image1, CLIP(text1)), SDM(image2, CLIP(text2)), …, SDM(imagek-1, CLIP(textk-1))]<br/> k-th frame image 만들기 위해 cross-attention 하는 데 사용</li> </ul> </li> <li>visual-language contextual fusion :<br/> <code class="language-plaintext highlighter-rouge">cross-attention</code> 사용<br/> 아래 Fig. (b)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/IntelligentGrimm/1-480.webp 480w,/assets/img/2024-08-05-Multimodal/IntelligentGrimm/1-800.webp 800w,/assets/img/2024-08-05-Multimodal/IntelligentGrimm/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/IntelligentGrimm/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">conditional generation</code> : <ul> <li>prev. frame t-1 \(\rightarrow\) add noise<br/> \(\rightarrow\) denoising one step<br/> \(\rightarrow\) diffusion U-Net</li> <li>prev. text \(\rightarrow\) text encoder<br/> \(\rightarrow\) diffusion U-Net</li> <li>diffusion U-Net (self-attention, text-attention)<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">denoising one step에 해당되는 feature</code>를 추출</li> <li>extracted image-diffusion-denoising-feature<br/> &amp; random noise<br/> &amp; current text \(\rightarrow\) text encoder<br/> \(\rightarrow\) StoryGen model (self-attention, image-attention, text-attention)<br/> \(\rightarrow\) current frame t</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">multi-frame conditioning</code> :<br/> story의 경우 frame t에 영향을 주는 image들이 frame t-1, frame t-2, … 일 수 있음<br/> 이 때, 과거 frames에 모두 같은 random noise를 줄 경우 성능 좋지 않아서<br/> <code class="language-plaintext highlighter-rouge">현재에 가까운 과거 frame일수록 noise를 덜 주는 식으로 temporal order를 부여</code>하면 성능 좋음</li> </ul> <h2 id="generating-realistic-images-from-in-the-wild-sounds">Generating Realistic Images from In-the-wild Sounds</h2> <h4 id="taegyeong-lee-jeonghun-kang-hyeonyu-kim-taehwan-kim">Taegyeong Lee, Jeonghun Kang, Hyeonyu Kim, Taehwan Kim</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2309.02405">Image from in-the-wild Sounds</a></p> </blockquote> <ul> <li> <p>novelty :<br/> 이전까지는 wild sound와 image 간의 pair가 없어서 limited categories와 music의 sound로부터 image를 생성하는 연구만 진행되었음<br/> 본 논문은 sound와 image 간의 large paired dataset이 없더라도<br/> wild sound로부터 image를 생성하는 task를 최초로 제시</p> </li> <li> <p>method :</p> <ul> <li>stage (a) :<br/> <code class="language-plaintext highlighter-rouge">audio captioning</code>을 통해 sound를 text로 변환한 audio caption과<br/> sound의 dynamic 특성을 반영하기 위한 <code class="language-plaintext highlighter-rouge">audio attention</code>과<br/> 제대로 image visualization하기 위한 <code class="language-plaintext highlighter-rouge">sentence attention</code>을<br/> 함께 사용하여 positional encoding을 거친 뒤 vector w를 <code class="language-plaintext highlighter-rouge">initialize</code><br/> (이 때, Audio-Captioning-Transformer model의 decoder에서 나오는 확률값을 audio attention이라고 정의함)</li> <li>stage (b) :<br/> audio caption으로부터 만든 vector z와<br/> stage (a)의 vector w로부터<br/> new latent vector z’를 만들고,<br/> <code class="language-plaintext highlighter-rouge">stable-diffusion model</code>을 이용하여 이로부터 image를 생성한다<br/> 여기서 image와 vector z 간의 <code class="language-plaintext highlighter-rouge">CLIPscore similarity</code>를 이용해서 audio caption으로부터 만든 vector z를 optimize하고<br/> image와 audio 간의 <code class="language-plaintext highlighter-rouge">AudioCLIP similarity</code>를 이용해서 <code class="language-plaintext highlighter-rouge">audio를 직접 optimize</code>한다<br/> (image가 text에 맞게 생성되도록 image를 점점 변화시키면서 생성하는 Style-CLIP에서 영감을 받아 이를 diffusion model에 적용)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/ImageSound/1-480.webp 480w,/assets/img/2024-08-05-Multimodal/ImageSound/1-800.webp 800w,/assets/img/2024-08-05-Multimodal/ImageSound/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/ImageSound/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>local minimum에 빠지지 않기 위해 audio attention과 sentence attention을 이용한 stage (a)의 initialization이 매우 중요</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/ImageSound/2-480.webp 480w,/assets/img/2024-08-05-Multimodal/ImageSound/2-800.webp 800w,/assets/img/2024-08-05-Multimodal/ImageSound/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/ImageSound/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Q1 :<br/> image는 pixel 단위로 값이 있어서 feature map을 통해 vector로 만들 수 있고, text는 word 단위로 embedding을 통해 vector를 만들 수 있습니다. AudioCLIP을 통해 audio를 직접 optimize했다는데 audio는 무엇을 기준으로 vector로 만들어서 optimize 가능한 건가요?</p> </li> <li> <p>A1 :<br/> audio는 melspectrogram을 만든 뒤 ViT에서 image 다루듯이 똑같이 patch로 쪼개서 vector로 만든다<br/> AudioCLIP similarity의 경우 audio encoding과 image encoding과 text encoding 간의 contrastive learning을 통해 구할 수 있다</p> </li> </ul> <h2 id="vivit---a-video-vision-transformer">ViViT - A Video Vision Transformer</h2> <h4 id="anurag-arnab-mostafa-dehghani-georg-heigold-chen-sun-mario-lucic-cordelia-schmid">Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, Cordelia Schmid</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2103.15691">ViViT</a></p> </blockquote> <p><strong>ViViT는 아직 정리 완료 못했음 TBD…</strong></p> <ul> <li> <p>video of (T, H, W, C)를 sampling하여<br/> token sequence of (n_t, n_h, n_w, C) 을 만들고<br/> positional embedding을 더한 뒤 (N, d)로 reshape해서 transformer의 input으로 넣어줌</p> </li> <li> <p>uniform frame sampling :<br/> ViT에서처럼 각 2D frame을 독립적으로 embedding 후 모든 token을 concat</p> </li> <li> <p>Tubelet sampling :<br/> temporal info.를 반영하기 위해 tokenization 단계에서 spatial, temporal info.를 fuse</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/ViViT/1-480.webp 480w,/assets/img/2024-08-05-Multimodal/ViViT/1-800.webp 800w,/assets/img/2024-08-05-Multimodal/ViViT/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/ViViT/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/ViViT/2-480.webp 480w,/assets/img/2024-08-05-Multimodal/ViViT/2-800.webp 800w,/assets/img/2024-08-05-Multimodal/ViViT/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/ViViT/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Model 1 :<br/> CNN과 달리 transformer layer는 token 수에 비례하게 quadratic complexity를 가지므로 input frame에 linearly 필요</p> </li> <li> <p>Model 2 (factorized encoder) :<br/> spatial과 temporal을 두 개의 transformer encoder로 구성하여 많은 연산량 필요</p> </li> <li> <p>Model 3 (factorized self-attention) :<br/> 여전히 두 개의 encoder로 특정 dim만 뽑아서 attention 연산</p> </li> <li> <p>Model 4 (factorized dot-product attention) :<br/> spatial head의 경우 spatial-dim.에 대해서만 attention 수행</p> </li> </ul> <h2 id="llama-vid---an-image-is-worth-2-tokens-in-large-language-models">LLaMA-VID - An Image is Worth 2 Tokens in Large Language Models</h2> <ul> <li> <p>task : 주로 Video-QA</p> </li> <li> <p>VLM :</p> <ul> <li>영화 같은 long video understanding</li> <li>token 수가 너무 많아서 문제</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/llamaVID/1-480.webp 480w,/assets/img/2024-08-05-Multimodal/llamaVID/1-800.webp 800w,/assets/img/2024-08-05-Multimodal/llamaVID/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/llamaVID/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>architecture <ul> <li>context attention :<br/> \(E_t = mean(softmax(Q_t X_t^T) X_t)\)</li> <li>context token : from video frame and user question</li> <li>content token : from video frame</li> </ul> </li> <li>contribution <ul> <li>각 video frame을 두 가지의 token으로 나타냄 <ul> <li>context token (one token)</li> <li>content token (one token으로 compressed될 수도 있고 아닐 수도 있음)</li> </ul> </li> <li>hour-long video understanding을 위한 instruction dataset 만듦</li> </ul> </li> </ul> <h2 id="peekaboo---interactive-video-generation-via-masked-diffusion">PEEKABOO - Interactive Video Generation via Masked-Diffusion</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/PEEKABOO/3-480.webp 480w,/assets/img/2024-08-05-Multimodal/PEEKABOO/3-800.webp 800w,/assets/img/2024-08-05-Multimodal/PEEKABOO/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/PEEKABOO/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Video Diffusion Model </div> <ul> <li>기존 video generation diffusion model : <ul> <li>성능 꽤 좋은데 아직 user가 control하기 어려움</li> <li>이전에 spatial control을 적용하려면 전체 network를 training시키거나 adapter로 training시키는 과정이 필요했다</li> <li>본 논문은 추가적인 training 없이 masked attention module을 사용하여<br/> diffusion의 3D UNet을 사용하는 다양한 model에 적용할 수 있는 방법을 제시</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/PEEKABOO/1-480.webp 480w,/assets/img/2024-08-05-Multimodal/PEEKABOO/1-800.webp 800w,/assets/img/2024-08-05-Multimodal/PEEKABOO/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/PEEKABOO/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>contribution : <ul> <li><code class="language-plaintext highlighter-rouge">UNet</code> 기반의 video generation model이라면 <code class="language-plaintext highlighter-rouge">spatio-temporal control</code> 가능<br/> (spatio-temporal control : video가 generated될 때 object size, location, and trajectory 등을 user가 control하는 것)</li> <li><code class="language-plaintext highlighter-rouge">training-free</code></li> <li>no additial latency at inference time</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/PEEKABOO/2-480.webp 480w,/assets/img/2024-08-05-Multimodal/PEEKABOO/2-800.webp 800w,/assets/img/2024-08-05-Multimodal/PEEKABOO/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/PEEKABOO/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Masked attention</code> : <ul> <li>fg에만 attention하도록 만들기 위해</li> <li>\(\text{MaskedAttention}(Q, K, V, M) = \text{softmax}(\frac{QK^T}{d}+M)V\)<br/> where \(M[i, j] = - \infty\) if bg(0)<br/> where \(M[i, j] = 0\) if fg(1)</li> </ul> </li> <li>binary mask : <ul> <li>image :<br/> input BB를 입력으로 받아서 BB object 있는 부분만 fg = 1이 되도록 binary mask \(M_{input}^f[i]\) 를 만들어서 latent size로 downsample<br/> where size of \(n_{frame} \times n_{latents}\)</li> <li>text :<br/> text embedding을 받아서 object 나타내는 단어만 fg = 1이 되도록 mask \(T[j]\)<br/> where size of \(n_{text}\)</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Masked cross attention</code> : <ul> <li>text와 image 간의 attention</li> <li>cross-attention mask :<br/> \(M_{CA}^f[i, j] = fg(M_{input}^f[i]) \ast fg(T[j]) + (1-fg(M_{input}^f[i])) \ast (1-fg(T[j]))\)<br/> where \(fg\) : pixel 또는 text token이 fg이면 1을 반환하고, bg이면 0을 반환<br/> where size of \(n_{latents} \times n_{text}\)</li> <li>cross-attention mask : <ul> <li>image와 text가 둘 다 fg(1)이거나 둘 다 bg(0)이면 1을 반환하고<br/> 둘 중 하나가 fg(1)이고 둘 중 하나가 bg(0)이면 0을 반환</li> <li>즉, <code class="language-plaintext highlighter-rouge">fg와 bg가 서로 attention하지 않도록</code>!<br/> <code class="language-plaintext highlighter-rouge">fg는 fg끼리, bg는 bg끼리 attention하도록</code>!</li> </ul> </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Masked spatial attention</code> : <ul> <li>image self-attention for spatial</li> <li>spatial-attention mask :<br/> \(M_{SA}^f[i, j] = fg(M_{input}^f[i]) \ast fg(M_{input}^f[j]) + (1-fg(M_{input}^f[i])) \ast (1-fg(M_{input}^f[j]))\)<br/> where size of \(n_{latents} \times n_{latents}\)</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Masked temporal attention</code> : <ul> <li>image self-attention for temporal</li> <li>temporal-attention mask :<br/> \(M_{TA}^i[f, k] = fg(M_{input}^f[i]) \ast fg(M_{input}^k[i]) + (1-fg(M_{input}^f[i])) \ast (1-fg(M_{input}^k[i]))\)<br/> where size of \(n_{frame} \times n_{frame}\)</li> </ul> </li> <li>Extension :<br/> image binary mask를 input BB 받아서 manually 만들지 않고<br/> text prompt 넣어주면 LLM이 대신 만들어줄 수 있음 (VideoDirectorGPT와 유사)<br/> \(\rightarrow\)<br/> 그럼 text prompt만 입력으로 넣어주면 user control이 가능한 video를 생성할 수 있음!</li> </ul> <h2 id="controlnet---adding-conditional-control-to-text-to-image-diffusion-models">ControlNet - Adding Conditional Control to Text-to-Image Diffusion Models</h2> <h2 id="instructpix2pix---learning-to-follow-image-editing-instructions">InstructPix2Pix - Learning to Follow Image Editing Instructions</h2>]]></content><author><name></name></author><category term="generative"/><category term="multi-modal"/><category term="generative"/><summary type="html"><![CDATA[Paper Review]]></summary></entry><entry><title type="html">NeRF-Code</title><link href="https://semyeong-yu.github.io/blog/2024/NeRFcode/" rel="alternate" type="text/html" title="NeRF-Code"/><published>2024-08-05T15:00:00+00:00</published><updated>2024-08-05T15:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/NeRFcode</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/NeRFcode/"><![CDATA[<h2 id="nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h2> <h4 id="ben-mildenhall-pratul-psrinivasan-matthew-tancik">Ben Mildenhall, Pratul P.Srinivasan, Matthew Tancik</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2003.08934">https://arxiv.org/abs/2003.08934</a><br/> project website :<br/> <a href="https://www.matthewtancik.com/nerf">https://www.matthewtancik.com/nerf</a><br/> pytorch code :<br/> <a href="https://github.com/yenchenlin/nerf-pytorch">https://github.com/yenchenlin/nerf-pytorch</a><br/> <a href="https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file">https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file</a><br/> tiny tensorflow code :<br/> <a href="https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb">https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb</a><br/> Overview image reference :<br/> <a href="https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#dataflow">https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#dataflow</a></p> </blockquote> <p>NeRF code는 빠른 실행을 위해 lower-level framework인 jax와 jit compile로 짜여진 버전도 있는데,<br/> 본 포스팅에서는 좀 더 익숙한 numpy, Pytorch framework로 코드 리뷰를 진행하였다</p> <h2 id="train-code-flow-overview">Train Code Flow Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/1-480.webp 480w,/assets/img/2024-08-05-NeRFcode/1-800.webp 800w,/assets/img/2024-08-05-NeRFcode/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="load-data">Load Data</h2> <ul> <li>load data : <ul> <li>load_llff.py</li> <li>load_blender.py</li> <li>load_LINEMOD.py</li> <li>load_deepvoxels.py</li> </ul> </li> </ul> <h3 id="load_llff_data">load_llff_data()</h3> <ul> <li>LLFF dataset : real dataset<br/> return images, poses, bds, render_poses, i_test <ul> <li>images : np (N, H, W, C)</li> <li>poses : np (N, 3, 5)<br/> camera poses<br/> poses[:, 0:3, 0:3] : 3-by-3 rotation matrix<br/> poses[:, 0:3, 3:4] : 3-by-1 translation matrix<br/> poses[:, 0:3, 4:5] : H, W, focal-length for intrinsic matrix</li> <li>bds : np (N, 2)<br/> scene bounds<br/> dim=1 : 2 = 1(near bound) + 1(far bound)</li> <li>render_poses : np (M, 3, 5)<br/> dim=0 : the number of generated poses for novel view synthesis<br/> generate new pose along sphere or spiral path</li> <li>i_test : int<br/> index of holdout-view (avg pose랑 가장 비슷한 pose를 갖는 view)<br/> training에서 제외하여 test할 때 사용</li> <li>near, far = 0., 1. if ndc is true else near, far = 0.9 * bds.min(), 1. * bds.max()</li> </ul> </li> </ul> <h3 id="load_blender_data">load_blender_data()</h3> <ul> <li>Blender dataset : synthetic dataset<br/> return images, poses, render_poses, hwf, i_split <ul> <li>images : np (N, H, W, C)<br/> blender dataset은 RGB-A channel을 가지고 있어 C = 4</li> <li>i_train, i_val, i_test = i_split</li> <li>near, far = 2., 6.<br/> (blender synthetic dataset은 통제된 환경에서 수집된 data이므로 ndc 사용하지 않고 frustum의 near, far plane 고정)</li> <li>투명한 배경을 흰 배경으로 만들려면<br/> RGB * opacity + (1 - opacity) 를 통해<br/> RGB 값을 opacity만큼 반영하고 opacity가 작을수록(투명할수록) 색상이 흰색(1.)에 가까워지도록 함<br/> images = images[…,:3]*images[…,-1:] + (1.-images[…,-1:])</li> <li>그냥 투명한 배경 그대로 쓰려면<br/> RGB-A channel에서 RGB channel만 가져와서 씀<br/> images = images[…,:3]</li> </ul> </li> </ul> <h3 id="load_linemod_data">load_LINEMOD_data()</h3> <ul> <li>LINEMOD dataset : real dataset<br/> return images, poses, render_poses, hwf, K, i_split, near, far</li> </ul> <h3 id="load_dv_data">load_dv_data()</h3> <ul> <li>Deepvoxels dataset : synthetic dataset<br/> return images, poses, render_poses, hwf, i_split <ul> <li>near, far = hemi_R - 1., hemi_R + 1.<br/> where hemi_R = np.mean(np.linalg.norm(poses[:,:3,-1], axis=-1))<br/> camera center들로 이루어진 반구의 평균 반지름</li> </ul> </li> </ul> <h2 id="create-nerf-model">Create NeRF Model</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/4-480.webp 480w,/assets/img/2024-08-05-NeRFcode/4-800.webp 800w,/assets/img/2024-08-05-NeRFcode/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>args.N_importance : fine-MLP에서 추가적으로 사용할 fine-sample 개수 <ul> <li>args.N_importance &gt; 0 : fine-MLP 사용함</li> <li>args.N_importance &lt;= 0 : fine-MLP 사용 안함</li> </ul> </li> <li>network_query_fn : 추후에 run_network() 사용하기 위한 함수 <ul> <li>input : position info., view-direction info., model</li> <li>output : model output</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/10-480.webp 480w,/assets/img/2024-08-05-NeRFcode/10-800.webp 800w,/assets/img/2024-08-05-NeRFcode/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>render_kwargs_train : dict for rendering <ul> <li>network_query_fn : 추후에 run_network() 사용하기 위한 함수</li> <li>perturb : 일반화 위해 stratified ray-sampling할 때 randomness 추가할지 여부<br/> (test할 때는 False)</li> <li>network_fine, network_fn : fine-MLP, coarse-MLP</li> <li>N_importance, N_samples : number of fine-sampling, coarse-sampling</li> <li>white_bkgd : rendering에서 alpha-channel 사용할 때 투명한 부분이 흰색으로 채워지도록 할지 여부</li> <li>raw_noise_std : regularize(artifacts 완화) 위해 raw2ouputs()에서 model output 중 opacity에 추가할 noise의 std값<br/> (test할 때는 0.)</li> <li>lindisp : <ul> <li>NDC를 사용하는 front-unbounded llff dataset의 경우 lindisp = False로 설정하여<br/> linearly sampling in depth, 즉 depth를 균등하게 sampling하여<br/> 먼 거리의 scene도 적절히 표현</li> <li>NDC를 사용하지 않는 나머지 dataset의 경우 lindisp = True로 설정하여<br/> linearly sampling in inverse-depth, 즉 가까운 depth를 더 많이 sampling하여<br/> 가까운 scene의 디테일을 잘 포착</li> </ul> </li> </ul> </li> </ul> <h3 id="positional-encoding">Positional Encoding</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/2-480.webp 480w,/assets/img/2024-08-05-NeRFcode/2-800.webp 800w,/assets/img/2024-08-05-NeRFcode/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>get_embedder() input :<br/> PE freq. 개수 \(L\) 과 PE 쓸지말지 여부</li> <li>get_embedder() output :<br/> PE-function과 PE 결과의 dim.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/3-480.webp 480w,/assets/img/2024-08-05-NeRFcode/3-800.webp 800w,/assets/img/2024-08-05-NeRFcode/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>self.embed_fns :<br/> 각 frequency(\(0 \sim 2^{L-1}\))와 각 period function(\(sin, cos\))에 대한<br/> list of lambda functions<br/> \([sin(2^0x), cos(2^0x), \ldots sin(2^{L-1}x), cos(2^{L-1}x)]\)</li> <li>Embedder.embed(x) :<br/> self.embed_fns의 각 PE-function을 input x에 적용하여 dim=-1에 대해 concat</li> </ul> <h3 id="nerf-model">NeRF model</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/5-480.webp 480w,/assets/img/2024-08-05-NeRFcode/5-800.webp 800w,/assets/img/2024-08-05-NeRFcode/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>input_ch : position info. dim. : 3</li> <li>input_ch_views : view-direction info. dim. : 3</li> <li>use_viewdirs : MLP input으로 view-direction info.를 사용할지 말지 여부<br/> (view-direction info.를 사용하면 RGB color 계산에 도움됨)</li> <li>output_ch : output(RGB, opacity) dim. : 4 use_viewdirs가 False일 때만 사용하는 값</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/6-480.webp 480w,/assets/img/2024-08-05-NeRFcode/6-800.webp 800w,/assets/img/2024-08-05-NeRFcode/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>input x를 position info.와 view-direction info.로 쪼갬</li> <li>self.use_viewdirs가 True일 때(view-direction info. 사용할 때) :<br/> position info.만 넣어서 opacity를 뽑은 뒤<br/> view-direction info.를 추가로 넣어서 RGB 뽑고<br/> dim=-1에 대해 concat</li> <li>self.use_viewdirs가 False일 때(view-direction info. 사용 안 할 때) :<br/> position info.만 넣어서 output_ch만큼 한 번에 뽑음</li> </ul> <h3 id="run_network">run_network</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/9-480.webp 480w,/assets/img/2024-08-05-NeRFcode/9-800.webp 800w,/assets/img/2024-08-05-NeRFcode/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/7-480.webp 480w,/assets/img/2024-08-05-NeRFcode/7-800.webp 800w,/assets/img/2024-08-05-NeRFcode/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>flatten position and flatten view-direction \(\rightarrow\) each positional encoding and concat \(\rightarrow\) batchify model and apply model \(\rightarrow\) reshape again output</li> </ul> <h3 id="batchify">batchify</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/8-480.webp 480w,/assets/img/2024-08-05-NeRFcode/8-800.webp 800w,/assets/img/2024-08-05-NeRFcode/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>input이 주어지면 chunk만큼씩 쪼개서 적용하는 model 반환</li> </ul> <h2 id="get-ray-with-batch">Get Ray with batch</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/11-480.webp 480w,/assets/img/2024-08-05-NeRFcode/11-800.webp 800w,/assets/img/2024-08-05-NeRFcode/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>rays : shape (N, 2, H, W, 3) <ul> <li>dim=1 : rays_o, rays_d</li> <li>dim=2, 3 : for H*W개의 pixels</li> <li>dim=4 : 3d</li> </ul> </li> <li>rays_rgb : shape (N, 3, H, W, 3) after concat with images <ul> <li>dim=1 : rays_o, rays_d, images</li> </ul> </li> <li>rays_rgb : shape (N, H, W, 3, 3) \(\rightarrow\) (N_train, H, W, 3, 3) \(\rightarrow\) (N_train * H * W, 3, 3) \(\rightarrow\) shuffle along dim=0 <ul> <li>dim=0 : the number of rays(pixels)</li> <li>dim=1 : rays_o, rays_d, images</li> <li>dim=2 : 3d for rays and rgb for images</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/14-480.webp 480w,/assets/img/2024-08-05-NeRFcode/14-800.webp 800w,/assets/img/2024-08-05-NeRFcode/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>batch : N_train * H * W 개의 ray를 batch size = N_rand-개씩 묶어서 전부 사용<br/> shape (N_train * H * W, 3, 3) \(\rightarrow\) shape (N_rand, 3, 3) \(\rightarrow\) (3, N_rand, 3)</li> <li>batch_rays : shape (2, N_rand, 3) <ul> <li>dim=0 : rays_o, rays_d</li> <li>dim=1 : the number of rays</li> </ul> </li> <li>target_s : shape (N_rand, 3) <ul> <li>dim=0 : the number of pixels</li> <li>dim=1 : target pixel RGB</li> </ul> </li> <li>shuffle rays_rgb by torch.randperm() for every epoch</li> </ul> <h3 id="get_rays_np">get_rays_np</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/12-480.webp 480w,/assets/img/2024-08-05-NeRFcode/12-800.webp 800w,/assets/img/2024-08-05-NeRFcode/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>parameter :<br/> K : intrinsic matrix of shape (3, 3)<br/> c2w : extrinsic matrix of shape (3, 4)</li> <li>line 1 :<br/> np.meshgrid([0, …, W-1], [0, …, H-1], indexing=’xy’) <ul> <li>indexing=’xy’ : 첫 번째 array를 row-방향으로 반복하고, 두 번째 array를 column-방향으로 반복</li> <li>i, j : both shape (H, W) : 2D-pixel-coordinate (x, y)</li> </ul> </li> <li>line 2 : <ul> <li>apply intrinsic matrix<br/> <a href="https://semyeong-yu.github.io/blog/2024/NeRF/">NeRF-Blog</a> 의 Ray from input image (pre-processing) 참고</li> <li>dirs : shape (H, W, 3) : 2D-normalized-coordinate</li> </ul> </li> <li>line 4 : <ul> <li>apply extrinsic matrix to calculate ray-direction</li> <li>dirs[…, np.newaxis, :] : shape (H, W, 1, 3) \(\rightarrow\) (H, W, 3, 3) by broad-casting</li> <li>c2w[:3, :3] : shape (3, 3) \(\rightarrow\) (H, W, 3, 3) by broad-casting</li> <li>ray_d : shape (H, W, 3)<br/> “elementwise-multiplication 후 sum”은 “matrix-multiplication”과 동일한 계산</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/13-480.webp 480w,/assets/img/2024-08-05-NeRFcode/13-800.webp 800w,/assets/img/2024-08-05-NeRFcode/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> * 오타 정정 : 2. matrix multiplication에서 [u, v, 1; u, v, 1; u, v, 1] 대신 [u; v; 1] </div> <ul> <li>line 6 : <ul> <li>apply extrinsic matrix to calculate ray-origin</li> <li>rays_o : shape (3,) \(\rightarrow\) (H, W, 3) by broad-casting</li> </ul> </li> </ul> <h2 id="get-ray-without-batch">Get Ray without batch</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/15-480.webp 480w,/assets/img/2024-08-05-NeRFcode/15-800.webp 800w,/assets/img/2024-08-05-NeRFcode/15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>차이점 :<br/> Get Ray with batch에서는 N_train * H * W 개의 ray를 batch size = N_rand-개씩 묶어서 전부 사용했다면<br/> Get Ray without batch에서는 N_train 중 training view 하나를 randomly 고른 뒤 H * W 개의 ray 중 N_rand-개를 randomly 골라서 사용</p> </li> <li>target : shape (N, H, W, C) \(\rightarrow\) (H, W, C)<br/> \(\rightarrow\) target_s : shape (N_rand, C)</li> <li>coords : H * W 개의 ray를 H-axis와 W-axis에서 인덱싱하기 위해 meshgrid of shape (H, W, 2) 생성 <ul> <li>초반부 iter. : 중심부 crop해서 meshgrid of shape (2 * dH, 2 * dW, 2) 생성</li> <li>후반부 iter. : meshgrid of shape (H, W, 2) 생성</li> <li>dim=2 : coords[:, :, 0]은 H-coord이고, coords[:, :, 1]은 W-coord</li> </ul> </li> <li>select_coords : shape (N_rand, 2)<br/> H * W 개의 ray 중 N_rand-개를 randomly 고름</li> <li>batch_rays : shape (2, N_rand, 3)</li> <li>target_s : shape (N_rand, 3)</li> </ul> <h2 id="render">Render</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/16-480.webp 480w,/assets/img/2024-08-05-NeRFcode/16-800.webp 800w,/assets/img/2024-08-05-NeRFcode/16-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/17-480.webp 480w,/assets/img/2024-08-05-NeRFcode/17-800.webp 800w,/assets/img/2024-08-05-NeRFcode/17-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/17.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>input : <ul> <li>chunk : 동시에 처리할 수 있는 최대 ray 수 (due to maximum memory usage)</li> <li>c2w_staticcam : view-direction의 영향을 확인하고자 할 때 사용<br/> 기존 c2w는 view-direction MLP input 만드는 데만 사용하고<br/> c2w_staticcam으로 rendering 위한 rays_o, rays_d 다시 계산</li> </ul> </li> <li>output : <ul> <li>rgb_map : shape (B, 3)<br/> predicted RGB values for B개의 rays</li> <li>disp_map : shape (B,)<br/> disparity map (inverse of depth)</li> <li>acc_map : shape (B,)<br/> sum of sample weights along each ray</li> <li>extras : 나머지 dict from render_rays()<br/> fine-MLP를 사용하는 경우에만 존재 <ul> <li>rgb0, disp0, acc0 : from coarse-MLP</li> <li>z_std : shape (B,)<br/> std of distances (\(t\) 값) of fine samples for each ray</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/19-480.webp 480w,/assets/img/2024-08-05-NeRFcode/19-800.webp 800w,/assets/img/2024-08-05-NeRFcode/19-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/19.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>rays : <ul> <li>if use_viewdirs = True : shape (N_rand, 8)<br/> dim=1 : 3(rays_o) + 3(rays_d) + 1(near) + 1(far)</li> <li>if use_viewdirs = False : shape (N_rand, 11)<br/> dim=1 : 3(rays_o) + 3(rays_d) + 1(near) + 1(far) + 3(viewdirs)</li> </ul> </li> <li>all_ret : dict <ul> <li>rgb_map : shape (N_rand, 3)</li> <li>disp_map : shape (N_rand,)</li> <li>acc_map : shape (N_rand,)</li> <li>raw : MLP raw output (raw2outputs() 안 한 것)</li> <li>rgb0, disp0, acc0 : from coarse-MLP</li> <li>z_std : shape (N_rand,)</li> </ul> </li> <li>render() output :<br/> rgb_map, disp_map, acc_map, (나머지 모아놓은)-dict</li> </ul> <h3 id="ndc_rays">ndc_rays</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/18-480.webp 480w,/assets/img/2024-08-05-NeRFcode/18-800.webp 800w,/assets/img/2024-08-05-NeRFcode/18-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/18.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>shift ray origin to near plane :<br/> NDC를 적용하기 전에 3D ray origin \(o\) 을 near plane 위 \(o_n\) 으로 옮긴다<br/> (world-coordinate에서 ray가 near plane에서 출발하도록)<br/> by \(o_n = o + t_nd\)<br/> where z-axis에서는 \(-n = o_z + t_nd_z\) 이므로 \(t_n = \frac{-(n+o_z)}{d_z}\)<br/> where n은 argument(near)</p> </li> <li> <p>project ray to NDC-space :<br/> ray \(r = o_n + td\) 를 NDC로 projection했을 때<br/> projected ray \(r^{\ast} = o^{\ast} + t^{\ast} d^{\ast}\) 에서<br/> \(o^{\ast} = \begin{bmatrix} -\frac{f_{cam}}{\frac{W}{2}}\frac{o_{n_x}}{o_{n_z}} \\ -\frac{f_{cam}}{\frac{H}{2}}\frac{o_{n_y}}{o_{n_z}} \\ 1 + \frac{2n}{o_{n_z}} \end{bmatrix}\) where n은 argument(near)<br/> and<br/> \(t^{\ast} = \frac{td_z}{o_{n_z} + td_z} = 1 - \frac{o_{n_z}}{o_{n_z} + td_z}\)<br/> and<br/> \(d^{\ast} = \begin{bmatrix} -\frac{f_{cam}}{\frac{W}{2}}(\frac{d_x}{d_z} - \frac{o_{n_x}}{o_{n_z}}) \\ -\frac{f_{cam}}{\frac{H}{2}}(\frac{d_y}{d_z} - \frac{o_{n_y}}{o_{n_z}}) \\ -2n\frac{1}{o_{n_z}} \end{bmatrix}\) where n은 argument(near)</p> </li> </ul> <h3 id="batchify_rays">batchify_rays</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/20-480.webp 480w,/assets/img/2024-08-05-NeRFcode/20-800.webp 800w,/assets/img/2024-08-05-NeRFcode/20-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/20.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Out-of-Memory를 방지하기 위해 N_rand-개의 rays를 더 작은 chunk (B개)로 쪼개서 rendering</p> </li> <li>ret : render_rays()의 output<br/> dict <ul> <li>rgb_map : shape (B, 3)<br/> predicted RGB values by alpha-compositing</li> <li>disp_map : shape (B,)<br/> disparity map (inverse of depth)</li> <li>acc_map : shape (B,)<br/> sum of sample weights along each ray</li> <li>raw : MLP raw output (raw2outputs() 안 한 것)</li> <li>rgb0, disp0, acc0 : from coarse-MLP</li> <li>z_std : shape (B,)<br/> std of distances (\(t\) 값) of fine-samples for each ray</li> </ul> </li> <li>all_ret : B-개씩 쪼개서 rendering한 걸 다시 N_rand-개로 합침<br/> dict <ul> <li>rgb_map : shape (N_rand, 3)</li> <li>disp_map : shape (N_rand,)</li> <li>acc_map : shape (N_rand,)</li> <li>raw : MLP raw output (raw2outputs() 안 한 것)</li> <li>rgb0, disp0, acc0 : from coarse-MLP</li> <li>z_std : shape (N_rand,)</li> </ul> </li> </ul> <h3 id="render_rays">render_rays</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/21-480.webp 480w,/assets/img/2024-08-05-NeRFcode/21-800.webp 800w,/assets/img/2024-08-05-NeRFcode/21-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/21.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>ray_batch of shape (B, 8) or (B, 11)로부터<br/> rays_o, rays_d, near, far, viewdirs 분리</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/22-480.webp 480w,/assets/img/2024-08-05-NeRFcode/22-800.webp 800w,/assets/img/2024-08-05-NeRFcode/22-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/22.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Stratified Sampling of distance \(t\) for coarse-MLP :<br/> z_vals : shape (B, N_samples) = (N_rays, N_samples)<br/> stratified sampled distance \(t\) <ul> <li>Let 균등한 간격을 나타내는 \(t_{vals} \in [0, 1]\) has shape (N_samples,)</li> <li>if lindisp = False:<br/> sample linearly in depth<br/> \(z_{vals} = near \cdot (1-t_{vals}) + far \cdot (t_{vals})\)</li> <li>if lindisp = True:<br/> sample linearly in inverse-depth<br/> \(z_{vals} = \frac{1}{\frac{1}{near} \cdot (1-t_{vals}) + \frac{1}{far} \cdot (t_{vals})}\)</li> <li>if perturb = True:<br/> add randomness</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/25-480.webp 480w,/assets/img/2024-08-05-NeRFcode/25-800.webp 800w,/assets/img/2024-08-05-NeRFcode/25-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/25.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> perturb=False이면 맨 윗줄을 coarse-samples로 쓰고, perturb=True이면 맨 아랫줄을 coarse-samples로 쓴다 </div> <ul> <li>pts, viewdirs : coarse-MLP input <ul> <li>pts : position info. \(r = o + td\) of shape (B, N_samples, 3)</li> <li>viewdirs : view-direction info. of shape (B, 3)</li> </ul> </li> <li>raw : coarse-MLP output<br/> shape (B, N_samples, 4) where 4 : for RGB, opacity</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/23-480.webp 480w,/assets/img/2024-08-05-NeRFcode/23-800.webp 800w,/assets/img/2024-08-05-NeRFcode/23-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/23.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Inverse-transform Sampling of distance \(t\) for fine-MLP : <ul> <li>coarse-samples :<br/> z_vals : shape (B, N_samples) = (N_rays, N_samples)</li> <li>fine-samples :<br/> coarse-MLP의 MLP output raw에 대해 raw2outputs()로 구한 weights 값을 Fine-Sampling에 사용<br/> z_samples : shape (B, N_importance)</li> <li>total sorted samples for fine-MLP :<br/> z_vals : shape (B, N_samples + N_importance)</li> </ul> </li> <li>pts, viewdirs : fine-MLP input <ul> <li>pts : position info. \(r = o + td\) of shape (B, N_samples + N_importance, 3)</li> <li>viewdirs : view-direction info. of shape (B, 3)</li> </ul> </li> <li>raw : fine-MLP output<br/> shape (B, N_samples + N_importance, 4) where 4 : RGB, opacity</li> </ul> <h3 id="sample_pdf">sample_pdf</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/29-480.webp 480w,/assets/img/2024-08-05-NeRFcode/29-800.webp 800w,/assets/img/2024-08-05-NeRFcode/29-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/29.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>input : <ul> <li>z_vals_mid : shape (B, N_samples - 1)<br/> stratified samples 사이의 중점</li> <li>weights[…, 1:-1] : shape (B, N_samples - 2)<br/> 시작점, 끝점 빼고 weight of each stratified sample</li> <li>det : stratified samples에 randomness 부여했다면 False</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/27-480.webp 480w,/assets/img/2024-08-05-NeRFcode/27-800.webp 800w,/assets/img/2024-08-05-NeRFcode/27-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/27.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/28-480.webp 480w,/assets/img/2024-08-05-NeRFcode/28-800.webp 800w,/assets/img/2024-08-05-NeRFcode/28-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/28.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>pdf : shape (B, N_samples - 2)<br/> \(\frac{w_i}{\sum_{j=1}^{num_{N_samples - 2}} w_j}\)</li> <li>cdf : shape (B, N_samples - 1)<br/> \(F_i = \sum_{j=1}^{i-1} f_j\)<br/> by torch.cumsum()<br/> 각 row는 0 ~ 1 에서 점점 증가하는 수로 이루어져 있음</li> <li>u : shape (B, N_importance) <ul> <li>det가 True (no randomness)일 경우 :<br/> \(\begin{bmatrix} 0 &amp; \frac{1}{N_{importance}-1} &amp; \cdots &amp; 1 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \end{bmatrix}\)</li> <li>det가 False (randomness)일 경우 :<br/> 0 ~ 1 사이의 random float로 이루어져 있음</li> </ul> </li> <li>inds : shape (B, N_importance)<br/> u를 cdf의 어디에 끼워넣을 수 있는지에 대한 index<br/> by torch.searchsorted()</li> <li>below : shape (B, N_importance)<br/> max(0, inds - 1)</li> <li>above : shape (B, N_importance)<br/> min(N_samples - 2, inds)</li> <li>inds_g : shape (B, N_importance, 2) and range [0, N_samples - 1)<br/> u가 위치할 수 있는 cdf의 두 경계의 index를 의미</li> <li>cdf_g : shape (B, N_importance, 2)<br/> torch.gather(cdf.expand(), 2, inds_g)<br/> inds_g에 따라 cdf의 값(확률값)을 추출해옴<br/> where cdf.expand() : shape (B, N_importance, N_samples - 1)<br/> where inds_g : shape (B, N_importance, 2) and range [0, N_samples - 1)</li> <li>bins_g : shape (B, N_importance, 2)<br/> torch.gather(bins.expand(), 2, inds_g)<br/> inds_g에 따라 bins의 값(coarse-samples 사이의 중점 \(t\) 값)을 추출해옴<br/> where bins.expand() : shape (B, N_importance, N_samples - 1)<br/> where inds_g : shape (B, N_importance, 2) and range [0, N_samples - 1)</li> <li>denom : shape (B, N_importance)<br/> u가 위치할 수 있는 구간의 cdf 값 차이</li> <li>t : shape (B, N_importance)<br/> u가 구간 내에서 차지하는 상대적인 위치</li> <li>samples : shape (B, N_importance)<br/> fine samples의 \(t\) 값<br/> bins_g[…, 0]과 bins_g[…, 1] 사이의 값</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/31-480.webp 480w,/assets/img/2024-08-05-NeRFcode/31-800.webp 800w,/assets/img/2024-08-05-NeRFcode/31-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/31.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> CDF 가로축의 empty circles는 coarse(stratified) samples 사이의 중점(mid-point)의 t값 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/24-480.webp 480w,/assets/img/2024-08-05-NeRFcode/24-800.webp 800w,/assets/img/2024-08-05-NeRFcode/24-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/24.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>render_rays() output : dict <ul> <li>rgb_map : shape (B, 3)<br/> predicted RGB values by alpha-compositing</li> <li>disp_map : shape (B,)<br/> disparity map (inverse of depth)</li> <li>acc_map : shape (B,)<br/> sum of sample weights along each ray</li> <li>raw : MLP raw output (raw2outputs() 안 한 것)</li> <li>rgb0, disp0, acc0 : from coarse-MLP</li> <li>z_std : shape (B,)<br/> std of distances (\(t\) 값) of fine-samples for each ray</li> </ul> </li> </ul> <h3 id="raw2outputs">raw2outputs</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/26-480.webp 480w,/assets/img/2024-08-05-NeRFcode/26-800.webp 800w,/assets/img/2024-08-05-NeRFcode/26-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/26.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>input : <ul> <li>raw : shape (B, num_samples, 4)</li> </ul> </li> <li> <p>dists : shape (B, num_samples)<br/> \(\delta_{i}\) : sample 간의 간격 in world-coordinate<br/> sample 간의 간격 in t-coordinate 에 \(\| d \|\) 곱해서 구함<br/> (dists[:, -1]은 마지막 sample부터 inf까지의 간격을 의미하는 매우 큰 수 1e10)</p> </li> <li> <p>rgb : shape (B, num_sample, 3)<br/> \(c_i\) : raw-RGB에 sigmoid 씌운 값<br/> sigmoid(raw[…, :3])<br/> so that \(c_i \in (0, 1)\)</p> </li> <li> <p>alpha : shape (B, num_samples)<br/> \(\alpha_{i} = 1 - \exp(- \sigma_{i} \delta_{i})\)<br/> where \(\sigma_{i}\) : raw-opacity에 noise 더하고 relu 씌운 값<br/> so that \(\sigma_{i} \in [0, \infty)\)</p> </li> <li> <p>weights : shape (B, num_samples)<br/> \(w_i = \alpha_{i} \times T_i\)<br/> where \(T_i = \prod_{j=1}^{i-1} (1-\alpha_{j}+1e-10)\) is obtained by torch.cumprod()</p> </li> <li>output : <ul> <li>rgb_map : shape (B, 3)<br/> predicted RGB values<br/> by volume rendering \(\hat{C}(r) = \sum_{i=1}^{num_{samples}} T_i \alpha_{i} c_i = \sum_{i=1}^{num_{samples}} w_i c_i\) <ul> <li>if white_bkgd (투명한 배경 대신 흰색) :<br/> \(\hat{C}(r) = \sum_{i=1}^{num_{samples}} w_i c_i + (1 - \sum_{i=1}^{num_{samples}} w_i)\)<br/> so that 투명해서 \(\sum_{i=1}^{num_{samples}} w_i\) 가 작을 때 RGB-color가 흰색(1.)에 가깝도록</li> </ul> </li> <li>disp_map : shape (B,)<br/> disparity map (inverse of depth)<br/> by \(\frac{1}{max(1e-10, \frac{\sum_{i=1}^{num_{samples}} w_i t_i}{\sum_{i=1}^{num_{samples}} w_i})}\)</li> <li>acc_map : shape (B,)<br/> sum of sample weights along each ray<br/> by \(\sum_{i=1}^{num_{samples}} w_i\)</li> <li>weights : shape (B, num_samples)<br/> weight of each sample<br/> \(w_i = \alpha_{i} \times T_i\)</li> <li>depth_map : shape (B,)<br/> depth map (estimated distance to object)<br/> by \(\sum_{i=1}^{num_{samples}} w_i t_i\)<br/> (weight가 높은 깊이 값 \(t_i\) 을 더 많이 반영하는 식으로 weighted sum)</li> </ul> </li> </ul> <h2 id="evaluation">Evaluation</h2> <h3 id="img2mse-for-loss-and-mse2psnr-for-psnr">img2mse for loss and mse2psnr for psnr</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/32-480.webp 480w,/assets/img/2024-08-05-NeRFcode/32-800.webp 800w,/assets/img/2024-08-05-NeRFcode/32-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/32.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/30-480.webp 480w,/assets/img/2024-08-05-NeRFcode/30-800.webp 800w,/assets/img/2024-08-05-NeRFcode/30-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/30.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>loss = coarse-MLP-loss + fine-MLP-loss<br/> where each is MSE loss b.w. predicted RGB and GT RGB of shape (N_rand, 3)</li> <li>PSNR : \(PSNR = -10 * log_{10}(loss)\)</li> <li>to8b : 0. ~ 1.에서 0 ~ 255 (8-bit)로 변환</li> </ul> <h3 id="test">test</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/34-480.webp 480w,/assets/img/2024-08-05-NeRFcode/34-800.webp 800w,/assets/img/2024-08-05-NeRFcode/34-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/34.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>args.i_video iter.마다<br/> novel view(render_poses)에 대해 rendering해서<br/> 여러 장의 rgb_map과 disp_map을 동영상으로 저장</li> <li>args.i_testset iter.마다<br/> test view에 대해 rendering해서<br/> 한 장의 rgb_map을 사진으로 저장</li> </ul> <h3 id="render_path">render_path</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/33-480.webp 480w,/assets/img/2024-08-05-NeRFcode/33-800.webp 800w,/assets/img/2024-08-05-NeRFcode/33-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/33.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>inference rendering (한 장씩)</li> <li>빠른 rendering을 위해 H, W, focal을 downsample</li> </ul> <h2 id="question">Question</h2> <ul> <li>Q1 : 왜 ndc_rays() 호출할 때 near bound n 값에 near = 1.으로 하드코딩해서 넣어주지?</li> <li>A1 : <code class="language-plaintext highlighter-rouge">????</code></li> <li>Q2 : 왜 blender dataset에서 render_poses 만들 때 phi=-30. 으로 하드코딩해서 넣어주지?</li> <li>A2 : <code class="language-plaintext highlighter-rouge">????</code></li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="rendering"/><category term="3d"/><summary type="html"><![CDATA[NeRF Code Review]]></summary></entry><entry><title type="html">MipNeRF</title><link href="https://semyeong-yu.github.io/blog/2024/MipNeRF/" rel="alternate" type="text/html" title="MipNeRF"/><published>2024-08-03T01:03:00+00:00</published><updated>2024-08-03T01:03:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/MipNeRF</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/MipNeRF/"><![CDATA[<h2 id="mip-nerf-a-multiscale-representation-for-anti-aliasing-neural-radiance-fields">Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</h2> <h4 id="jonathan-t-barron-ben-mildenhall-matthew-tancik-peter-hedman-ricardo-martin-brualla-pratul-p-srinivasan">Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2103.13415">https://arxiv.org/abs/2103.13415</a><br/> project website :<br/> <a href="https://jonbarron.info/mipnerf/">https://jonbarron.info/mipnerf/</a><br/> pytorch code :<br/> <a href="https://github.com/bebeal/mipnerf-pytorch">https://github.com/bebeal/mipnerf-pytorch</a><br/> <a href="https://github.com/google/mipnerf">https://github.com/google/mipnerf</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>scale 잘 반영하도록 sample(region)을 pre-filtering!!</li> <li>ray-tracing하여 point-encoding 대신<br/> cone-tracing하여 region-encoding 이므로<br/> frustum의 모양과 크기 정보를 encode할 수 있어서 scale 반영 가능</li> <li>IPE 단계에서<br/> <code class="language-plaintext highlighter-rouge">high variance (구간 길이가 일정하다면 distant view)</code>일 때<br/> <code class="language-plaintext highlighter-rouge">high freq.를 attenuate</code> (pre-filtering) 하여<br/> <code class="language-plaintext highlighter-rouge">임의의 continuous-space scale</code>을 가지는 scene에 대해 <code class="language-plaintext highlighter-rouge">anti-aliased</code> representation 학습 가능<br/> \(\rightarrow\) multi-resolution dataset에 대해 성능 대폭 향상<br/> \(\rightarrow\) scale-aware하므로 <code class="language-plaintext highlighter-rouge">single MLP</code> 하나만으로 충분하여 빠르고 가벼움</li> <li>camera center로부터 각 pixel로 3D cone을 쏜 다음,<br/> <code class="language-plaintext highlighter-rouge">frustum을 multi-variate Gaussian으로 근사</code>한 뒤,<br/> Gaussian 내 좌표를 positional encoding한 것 (PE-basis-lifted Gaussian)에 대해<br/> expected value \(E \left[ \gamma (x) \right]\) 계산<br/> 주의 : frustum이 Gaussian 분포를 따르는 게 아니라, frustum 내부의 mean, variance 값을 먼저 구한 뒤 해당 mean, variance 값을 갖는 Gaussian으로 frustum을 대신(근사)할 수 있다고 생각!</li> </ol> </blockquote> <h2 id="introduction">Introduction</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/2-480.webp 480w,/assets/img/2024-08-03-MipNeRF/2-800.webp 800w,/assets/img/2024-08-03-MipNeRF/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>기존 NeRF의 문제점 : <ul> <li>rendering 위해 sampling할 때 <code class="language-plaintext highlighter-rouge">single ray</code> per pixel 쏴서 composite 하므로<br/> dataset에 있는 물체의 크기(resolution)가 일정하지 않을 때<br/> multi-scales images에 대해 학습하더라도</li> <li><code class="language-plaintext highlighter-rouge">blurry</code> rendering in <code class="language-plaintext highlighter-rouge">close-up</code> views<br/> (because 가까이서 찍어서 zoom-out하면 물체 in <code class="language-plaintext highlighter-rouge">high resolution</code>)</li> <li><code class="language-plaintext highlighter-rouge">aliased</code>(계단) rendering in <code class="language-plaintext highlighter-rouge">distant</code> views<br/> (because 멀리서 찍어서 zoom-in하면 물체 in <code class="language-plaintext highlighter-rouge">low resolution</code>)</li> <li>그렇다고 multiple rays per pixel through its footprint로 brute-force super-sampling(offline rendering)하는 것은 정확하긴 하겠지만 too costly 비현실적</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Minmap</code> Approach :<br/> classic 컴퓨터 그래픽스 분야에서 rendering할 때 aliasing을 없애기 위한 <code class="language-plaintext highlighter-rouge">pre-filtering</code> 기법<br/> 본 논문인 Mip-NeRF가 여기서 영감을 얻음<br/> signal(e.g. image)을 diff. <code class="language-plaintext highlighter-rouge">downsampling scales</code>로 나타낸 뒤 pixel footprint를 근거로 ray에 사용하기 위한 <code class="language-plaintext highlighter-rouge">적절한 scale을 고른다</code><br/> render time에 할 복잡할 일을 pre-computation phase에 미리 하는 것일 뿐이긴 하지만, 주어진 texture마다 한 번만 minmap을 만들면 된다는 장점이 있다</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/4-480.webp 480w,/assets/img/2024-08-03-MipNeRF/4-800.webp 800w,/assets/img/2024-08-03-MipNeRF/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Mip-NeRF : <ul> <li>represent pre-filtered scene at <code class="language-plaintext highlighter-rouge">continuous space of scales</code></li> <li>ray 대신 <code class="language-plaintext highlighter-rouge">conical frustum</code> 사용해서 <code class="language-plaintext highlighter-rouge">anti-aliased</code> rendering with fine details</li> <li>multi-scale variant of dataset에 대해 평균 error rate 60% 감소</li> <li>NeRF가 hierarchical sampling을 위해 coarse and fine MLP를 분리했다면, Mip-NeRF는 <code class="language-plaintext highlighter-rouge">scale-aware</code>하므로 <code class="language-plaintext highlighter-rouge">single multi-scale MLP만으로 충분</code><br/> 따라서 NeRF보다 7% 빠르고, param. 수는 절반이고, sampling도 더 효율적</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/1-480.webp 480w,/assets/img/2024-08-03-MipNeRF/1-800.webp 800w,/assets/img/2024-08-03-MipNeRF/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> ray 대신 cone을 쏘고, point-encoding 대신 frustum region-encoding </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/3-480.webp 480w,/assets/img/2024-08-03-MipNeRF/3-800.webp 800w,/assets/img/2024-08-03-MipNeRF/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>차이 : 기존 NeRF는 <code class="language-plaintext highlighter-rouge">a single point</code>를 encode하고, Mip-NeRF는 <code class="language-plaintext highlighter-rouge">a region of space</code>를 encode</p> </li> <li> <p>기존 NeRF :<br/> camera center로부터 각 pixel로 ray를 하나 쏜 다음 point sampling한 뒤 positional encoding<br/> point-sampled feature는 ray가 보는 <code class="language-plaintext highlighter-rouge">volume의 모양과 크기를 무시</code>하는 것임<br/> 예를 들어 training할 때 camera1로부터 t 사이의 간격이 평균 10cm로 학습된 scene에 대해<br/> camera2로 inference를 할 때 t 사이의 간격이 평균 1cm로 sampling된다면<br/> 10개의 점은 같은 point-based feature를 갖게 되어 scale을 고려하지 못함<br/> 이러한 ambiguity가 기존 NeRF의 성능 하락의 요인</p> </li> <li> <p>Mip-NeRF :<br/> volume 정보를 반영하기 위해 camera center로부터 각 pixel로 3D cone을 쏜 다음, 3D point 및 그 주위의 Gaussian region을 encode하기 위해 IPE</p> </li> <li> <p>IPE (<code class="language-plaintext highlighter-rouge">integrated positional encoding</code>) :<br/> region을 encode하기 위한 방식<br/> <code class="language-plaintext highlighter-rouge">frustum을 multi-variate Gaussian으로 근사</code>한 뒤,<br/> Gaussian 내 좌표를 positional encoding한 것 (PE-basis-lifted Gaussian)에 대해 expected value \(E \left[ \gamma (x) \right]\) 계산</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/5-480.webp 480w,/assets/img/2024-08-03-MipNeRF/5-800.webp 800w,/assets/img/2024-08-03-MipNeRF/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 사진 출처 : https://xoft.tistory.com/16 </div> <h2 id="related-work">Related Work</h2> <h3 id="anti-aliasing-in-rendering">Anti-aliasing in Rendering</h3> <blockquote> <p><code class="language-plaintext highlighter-rouge">anti-aliasing</code>을 위한 고전적인 방법으로는 두 가지가 있다.</p> </blockquote> <ol> <li><code class="language-plaintext highlighter-rouge">supersampling</code> : <ul> <li>rendering할 때 <code class="language-plaintext highlighter-rouge">multiple rays per pixel</code>을 쏴서 Nyquist frequency에 가깝게 sampling rate를 높임 (super-sampling)</li> <li><code class="language-plaintext highlighter-rouge">expensive</code> as runtime increases linearly with the super-sampling rate, so used only in <code class="language-plaintext highlighter-rouge">offline</code> rendering</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">pre-filtering</code> : <ul> <li>target sampling rate에 맞춰서 Nyquist frequency를 줄이기 위해 scene에 <code class="language-plaintext highlighter-rouge">lowpass-filter</code>를 씌운 버전 사용</li> <li>scene을 미리 <code class="language-plaintext highlighter-rouge">downsampling multi-scales</code> (sparse voxel octree 또는 minmap)로 나타낸 뒤, <code class="language-plaintext highlighter-rouge">ray 대신 cone</code>을 추적하여 cone과 scene이 만나는 곳의 cone’s footprint에 대응되는 적절한 scale을 골라서 사용 (<code class="language-plaintext highlighter-rouge">target sampling rate에 맞는 적절한 scale</code>)</li> <li>scene에 filter 씌운 버전을 한 번만 미리 계산하면 되므로, better for <code class="language-plaintext highlighter-rouge">real-time</code> rendering</li> </ul> </li> </ol> <blockquote> <p>그런데 아래의 이유로 고전적인 multi-scale representation은 적용 불가능</p> <ul> <li>input scene의 <code class="language-plaintext highlighter-rouge">geometry를 미리 알 수 없으므로</code> pre-filtering 할 수가 없어서<br/> 대신 pre-filtering 방식 자체를 training할 때 학습해야 한다</li> <li>input scene의 <code class="language-plaintext highlighter-rouge">scale이 continuous</code>하므로 a fixed number of scales (discrete)과 상황이 다르다</li> </ul> </blockquote> <p>\(\rightarrow\) 결론 : 따라서 Mip-NeRF는 training하는 동안, <code class="language-plaintext highlighter-rouge">임의의 scale</code>에 대해 query 받을 수 있는, scene의 <code class="language-plaintext highlighter-rouge">pre-filtered representation</code>을 학습한다.</p> <h3 id="scene-representations-for-view-synthesis">Scene Representations for View Synthesis</h3> <ul> <li> <p>만약 images of scene are captured <code class="language-plaintext highlighter-rouge">densely</code> 라면, novel view synthesis 위해, intermediate representation of scene을 reconstruct하지 않고 <code class="language-plaintext highlighter-rouge">light field interpolation</code> 기법 사용</p> </li> <li>만약 images of scene are captured <code class="language-plaintext highlighter-rouge">sparsely</code> 라면, novel view synthesis 위해, <code class="language-plaintext highlighter-rouge">explicit representation</code> of the scene’s 3D geometry and appearance를 reconstruct <ul> <li><code class="language-plaintext highlighter-rouge">polygon-mesh-based</code> representation (<code class="language-plaintext highlighter-rouge">discrete, classic</code>) :<br/> with either diffuse or view-dependent textures<br/> can be stored efficiently, but mesh geometry optimization에 gradient descent를 이용하는 것은 불연속점 및 극소점 때문에 어렵</li> <li><code class="language-plaintext highlighter-rouge">volumetric</code> representation : <ul> <li><code class="language-plaintext highlighter-rouge">voxel-grid-based</code> representation (<code class="language-plaintext highlighter-rouge">discrete, classic</code>) : deep learning으로 학습 가능, but 고해상도의 scene에는 부적합</li> <li><code class="language-plaintext highlighter-rouge">coordinate-based</code> neural representation (<code class="language-plaintext highlighter-rouge">continuous, recent</code>) : 3D 좌표를 그 위치에서의 scene의 특징(e.g. volume density, radiance)으로 mapping하는 continuous function을 MLP로 예측 <ul> <li>implicit surface representation</li> <li><code class="language-plaintext highlighter-rouge">implicit volumetric NeRF representation</code></li> </ul> </li> </ul> </li> </ul> </li> <li>문제점 :<br/> 그동안 view synthesis를 할 때 sampling 및 aliasing에는 덜 주목했다<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">classic discrete</code> representation의 경우 앞에서 소개한 minmap, octree와 같은 고전적인 multi-scale pre-filtering 기법을 쓰면 aliasing 없이 rendering 가능하다<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">coordinate-based</code> representation의 경우 scale이 continuous하므로 고전적인 anti-aliasing 기법은 사용할 수 없다<br/> \(\rightarrow\) sparse voxel octree 기반의 multi-scale representation으로 implicit surface의 continuous neural representation을 구현한 논문 <d-cite key="continuouspriori">[1]</d-cite> 있긴 하지만, scene geometry의 priori를 알아야 한다는 제약이 있다<br/> \(\rightarrow\) Mip-NeRF는 training하는 동안, <code class="language-plaintext highlighter-rouge">임의의 scale</code>에 대해 query 받을 수 있는, scene의 <code class="language-plaintext highlighter-rouge">anti-aliased (pre-filtered)</code> representation을 학습한다.</li> </ul> <h2 id="method">Method</h2> <h3 id="cone-tracing-and-positional-encoding">Cone Tracing and Positional Encoding</h3> <ul> <li>Cone Tracing : <ul> <li>Let \(d\) is cone direction vector from \(o\) to image plane</li> <li>Let \(\hat r =\) radius at image plane \(o + d\) \(= \frac{2}{\sqrt{12}}\) of pixel-width<br/> so that image plane에서의 cone의 \(x, y\) 축 variance가 pixel’s footprint의 variance와 같아지도록<br/> \(\hat r\)은 ray의 radius 변화율, 즉 frustum의 넓이를 결정</li> <li>\(t \in [t_0, t_1]\) 일 때 conical frustum 내의 \(x\)는 아래 범위의 값을 가질 때 indicator function \(F(x, o, d, \hat r, t_0, t_1)=1\)이다<br/> \(F(x, o, d, \hat r, t_0, t_1) = 1 \left\{ (t_0 \lt \frac{d^T(x-o)}{\| d \|^2} \lt t_1) \land (\frac{d^T(x-o)}{\| d \| \| x-o \|} \gt \frac{1}{\sqrt{1+(\frac{\hat r}{\| d \|})^2}}) \right\}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/6-480.webp 480w,/assets/img/2024-08-03-MipNeRF/6-800.webp 800w,/assets/img/2024-08-03-MipNeRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Region Encoding :<br/> conical frustum 내에 있는 모든 좌표 \(x\)에 대해 직접<br/> expected value \(E \left[ \gamma (x) \right] = \frac{\int \gamma (x) F(x, o, d, \hat r, t_0, t_1) dx}{\int F(x, o, d, \hat r, t_0, t_1) dx}\) 계산하면<br/> region을 encode할 수 있는데<br/> 여기서 분자의 적분식은 closed-form solution이 없음<br/> \(\rightarrow\) 직접 계산하지 말고<br/> <code class="language-plaintext highlighter-rouge">conical-frustum을 multi-variate Gaussian으로 근사</code>한 뒤<br/> Gaussian 내에 있는 모든 좌표 \(x\)에 대해<br/> expected value \(E \left[ \gamma (x) \right]\) 계산</p> </li> <li>frustum을 multi-variate Gaussian으로 근사 : <ul> <li>conical-frustum 단면은 대칭적인 원이기 때문에<br/> \(o, d\) 뿐만 아니라 아래의 3가지 정보만 알면 Gaussian을 특정할 수 있다 <ul> <li><code class="language-plaintext highlighter-rouge">mean distance along ray from o</code> \(\mu_{t}\)</li> <li><code class="language-plaintext highlighter-rouge">variance along ray</code> \(\sigma_{t}^2\)</li> <li><code class="language-plaintext highlighter-rouge">variance perpendicular to ray</code> \(\sigma_{r}^2\)</li> </ul> </li> <li>Let mid-point \(t_{\mu} = \frac{t_0+t_1}{2}\)<br/> Let half-width \(t_{\sigma}=\frac{t_1-t_0}{2}\)</li> <li>아래 수식의 유도과정은 하위에 별도로 정리함<br/> \(\mu_{t} = t_{\mu} + \frac{2t_{\mu}t_{\sigma}^2}{3t_{\mu}^2+t_{\sigma}^2}\)<br/> \(\sigma_{t}^2 = \frac{t_{\sigma}^2}{3} - \frac{4t_{\sigma}^4(12t_{\mu}^2-t_{\sigma}^2)}{15(3t_{\mu}^2+t_{\sigma}^2)^2}\)<br/> \(\sigma_{r}^2 = \hat r^2 (\frac{t_{\mu}^2}{4} + \frac{5t_{\sigma}^2}{12} - \frac{4t_{\sigma}^4}{15(3t_{\mu}^2+t_{\sigma}^2)})\)</li> <li>위의 3가지 param.를 가지는 Gaussian은 <code class="language-plaintext highlighter-rouge">t-coordinate</code>에서 정의했는데<br/> 아래 수식에 의해 <code class="language-plaintext highlighter-rouge">world-coordinate</code>으로 변환할 수 있다<br/> \(\mu = o + \mu_{t}d\)<br/> \(\Sigma = \sigma_{t}^2(dd^T) + \sigma_{r}^2(I-\frac{dd^T}{\| d \|^2})\)<br/> where \(dd^T =\) \(d\) 의 outer product은 \(d\) 방향으로의 투영을 의미하는 rank-1 matrix<br/> where \(I-\frac{dd^T}{\| d \|^2}\) 는 \(\frac{d}{\| d\ \|}\) 와 수직인 subspace로의 투영을 의미하는 rank-2 matrix</li> </ul> </li> <li>Integrated Positional Encoding (IPE) : <ul> <li>목표 : 위에서 계산한 \(\mu, \Sigma\) 의 Gaussian 내에 있는 모든 좌표 \(x\)에 대해 expected value \(E \left[ \gamma (x) \right]\) 계산</li> <li>우선 <code class="language-plaintext highlighter-rouge">PE (positional-encoding) basis</code> P를 재정의<br/> \(P = \begin{pmatrix} \begin{matrix} 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2\end{matrix} &amp; \cdots &amp; \begin{matrix} 2^{L-1} &amp; 0 &amp; 0 \\ 0 &amp; 2^{L-1} &amp; 0 \\ 0 &amp; 0 &amp; 2^{L-1}\end{matrix} \end{pmatrix}^T\)<br/> \(\gamma (x) = \begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}\)</li> <li>\(E \left[ \gamma (x) \right]\) 는 expectation over \(\gamma (x) = \begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}\) 이므로<br/> \(x\) in Gaussian of \(\mu, \Sigma\) \(\rightarrow\) \(\gamma (x)\) in Gaussian of \(\mu_{r}, \Sigma_{r}\) 로 변환해야 한다<br/> 즉, <code class="language-plaintext highlighter-rouge">PE basis P로 lift</code>한 뒤의 mean과 covariance를 구해야 한다<br/> Since \(Cov[Ax, By] = A Cov[x, y] B^T\),<br/> \(\mu_{r} = P \mu\)<br/> \(\Sigma_{r} = P \Sigma P^T\)</li> <li>최종적으로 \(E \left[ \gamma (x) \right]\) , 즉 <code class="language-plaintext highlighter-rouge">expectation over lifted multi-variate Gaussian</code> of \(\mu_{r}, \Sigma_{r}\) 을 구하면 된다<br/> Since \(E_{k \sim N(\mu, \sigma^2)}[e^{itk}] = exp(i \mu t - \frac{1}{2} \sigma^2 t^2)\) and \(sin(k) = \frac{e^{ik}-e^{-ik}}{2i}\) and \(cos(k) = \frac{e^{ik}+e^{-ik}}{2}\),<br/> \(E_{k \sim N(\mu, \sigma^2)}[sin(k)] = sin(\mu)exp(-\frac{1}{2}\sigma^2)\) and \(E_{k \sim N(\mu, \sigma^2)}[cos(k)] = cos(\mu)exp(-\frac{1}{2}\sigma^2)\) for each axis-k<br/> (positional-encoding은 각 dim.을 independently encode하므로 marginal distribution of \(\gamma (x)\) 에 의존)<br/> \(\rightarrow\)<br/> \(\gamma (\mu, \Sigma) = E_{x \sim N(\mu, \Sigma)} [\gamma (x)] = E_{Px \sim N(\mu_{r}, \Sigma_{r})} [\begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}]\)<br/> \(= \begin{bmatrix} sin(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \\ cos(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \end{bmatrix}\)<br/> where \(\circledast\) is element-wise multiplication</li> <li>\(diag(\Sigma_{r})\) 만 필요하므로 \(\Sigma_{r}\) 전부 계산하지 말고 <code class="language-plaintext highlighter-rouge">efficiently diagonal만 계산</code><br/> PE-basis \(P\) 가 identity matrix이므로 \(diag(\Sigma)\) 만 필요<br/> \(diag(\Sigma_{r}) = diag(P \Sigma P^T) = \left[ diag(\Sigma), 4 diag(\Sigma), \ldots , 4^{L-1}diag(\Sigma) \right]^T\)<br/> where 3d-vector \(diag(\Sigma) = \sigma_{t}^2(d \circledast d) + \sigma_{r}^2(1-\frac{d \circledast d}{\| d \|^2})\)<br/> diagonal만 직접 계산하면, IPE feature는 PE feature랑 비슷하게 cost 소모</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/7-480.webp 480w,/assets/img/2024-08-03-MipNeRF/7-800.webp 800w,/assets/img/2024-08-03-MipNeRF/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>IPE vs PE : <ul> <li>PE :<br/> point를 encode<br/> 0~L까지의 <code class="language-plaintext highlighter-rouge">모든 frequencies에 대해 동일하게</code> encode<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">high-freq.</code> PE features are <code class="language-plaintext highlighter-rouge">aliased</code><br/> (PE period가 interval width보다 작은 경우 PE over interval oscillates repeatedly)</li> <li>IPE :<br/> interval region을 integrate하여 encode<br/> IPE feature를 만드는 데 사용된 interval \(t \in [t_0, t_1]\) width보다 period가 작은 <code class="language-plaintext highlighter-rouge">high freq.</code>의 경우 <code class="language-plaintext highlighter-rouge">attenuate</code>하여 <code class="language-plaintext highlighter-rouge">anti-aliasing</code><br/> by \(exp(-\frac{1}{2}(\omega \sigma)^2)\) term</li> <li>위와 같은 특성 덕분에 IPE는 interval 내 공간의 모양과 크기를 smoothly encode할 수 있는 anti-aliased PE 기법이다!</li> <li>high freq.는 IPE 단계 자체에서 attenuate되므로 <code class="language-plaintext highlighter-rouge">L을 hyper-param.로 두지 않고 extremely large fixed-value</code>로 두면 된다<br/> 본 논문에서는 IPE feature의 last dim.이 numerical epsilon보다 작아지는 값인 \(L=16\) 으로 둠</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/16-480.webp 480w,/assets/img/2024-08-03-MipNeRF/16-800.webp 800w,/assets/img/2024-08-03-MipNeRF/16-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> NeRF에서는 L이 너무 크면 overfitting, Mip-NeRF에서는 IPE 단계 자체에서 high freq.를 attenuate하므로 L 커도 상관 없음 </div> <ul> <li>IPE의 의미 :<br/> 이게 Mip-NeRF의 핵심!! <ul> <li>수식 :<br/> PE-basis P 는 다양한 frequency \(\omega\) 로 구성되어 있고<br/> 각 element는 \(E_{x \sim N(\mu, \Sigma)} [\gamma_{\omega} (x)] = sin(\omega \mu) exp(-\frac{1}{2}(\omega \sigma)^2)\)</li> <li>distant view :<br/> <code class="language-plaintext highlighter-rouge">distant views (low-resolution)</code>, 즉 멀리 있는 <code class="language-plaintext highlighter-rouge">wide frustum</code> (high variance \(\sigma\))의 경우에는<br/> <code class="language-plaintext highlighter-rouge">detail-info.</code> (high freq. \(\omega\))는 <code class="language-plaintext highlighter-rouge">training에 사용하지 않겠다</code><br/> \(\rightarrow\) more attenuation for high \(\sigma\) and high \(\omega\)</li> <li>close view :<br/> <code class="language-plaintext highlighter-rouge">close views (high-resolution)</code>, 즉 가까이 있는 <code class="language-plaintext highlighter-rouge">narrow frustum</code> (low variance \(\sigma\))의 경우에는<br/> <code class="language-plaintext highlighter-rouge">detail-info.</code> (high freq. \(\omega\))를 training할 때 좀 더 <code class="language-plaintext highlighter-rouge">허용</code></li> <li>위와 같이 scale을 반영할 수 있으므로 blurry 및 aliased rendering 문제 해결 가능!</li> </ul> </li> <li>수식 <code class="language-plaintext highlighter-rouge">Summary</code> : <ul> <li>frustum을 근사하는 multi-variate Gaussian의 mean, variance \(\mu, \sigma\) 를 구한다</li> <li>PE-basis P로 lift한 Gaussian의 mean, variance \(\mu_{r}, \Sigma_{r}\) 를 구한다<br/> \(P = \begin{pmatrix} \begin{matrix} 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2\end{matrix} &amp; \cdots &amp; \begin{matrix} 2^{L-1} &amp; 0 &amp; 0 \\ 0 &amp; 2^{L-1} &amp; 0 \\ 0 &amp; 0 &amp; 2^{L-1}\end{matrix} \end{pmatrix}^T\)<br/> \(\gamma (x) = \begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}\)<br/> \(\mu_{r} = P \mu\) and \(\Sigma_{r} = P \Sigma P^T\)</li> <li>\(E_{x \sim N(\mu_{r}, \Sigma_{r})} [\gamma (x)] = \begin{bmatrix} sin(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \\ cos(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \end{bmatrix}\)<br/> (efficiently \(\Sigma_{r}\) 의 diagonal만 직접 계산)<br/> \(diag(\Sigma_{r}) = diag(P \Sigma P^T) = \left[ diag(\Sigma), 4 diag(\Sigma), \ldots , 4^{L-1}diag(\Sigma) \right]^T\)</li> </ul> </li> </ul> <h3 id="conical-frustum-integral-derivation">Conical Frustum Integral Derivation</h3> <ul> <li> <p>우선 <code class="language-plaintext highlighter-rouge">Cartesian-coordinate</code>에서 <code class="language-plaintext highlighter-rouge">conical-coordinate</code>으로 변환<br/> \((x, y, z) = \varphi (r, t, \theta) = t \cdot (r cos \theta , r sin \theta , 1)\)<br/> where \(\theta \in [0, 2 \pi)\) and \(t \geq 0\) and \(\| r \| \leq \hat r\)<br/> Then,<br/> \(dx dy dz = | det(D \varphi) | dr dt d\theta\)<br/> \(= \begin{vmatrix} t cos\theta &amp; t sin\theta &amp; 0 \\ r cos\theta &amp; r sin\theta &amp; 1 \\ - rt sin\theta &amp; rt cos\theta &amp; 0 \end{vmatrix} dr dt d\theta\)<br/> \(= (rt^2cos^2\theta + rt^2sin\theta) dr dt d\theta\)<br/> \(= rt^2 dr dt d\theta\)</p> </li> <li> <p>conical frustum의 volume \(V = \int \int \int dx dy dz = \int_{0}^{2\pi} \int_{t_0}^{t_1} \int_{0}^{\hat r} r t^2 dr dt d\theta = \pi \hat r^2 \frac{t_1^3 - t_0^3}{3}\) 에 대해<br/> conical frustum에서 uniformly-sampling한 points의 <code class="language-plaintext highlighter-rouge">probability density function</code>은 \(\frac{rt^2}{V}\) 이다</p> </li> <li><code class="language-plaintext highlighter-rouge">t-axis</code> : <ul> <li> \[E \left[ t \right] = \int_{0}^{2\pi} \int_{t_0}^{t_1} \int_{0}^{\hat r} t \cdot \frac{rt^2}{V} dr dt d\theta = \frac{3(t_1^4 - t_0^4)}{4(t_1^3 - t_0^3)}\] </li> <li> \[E \left[ t^2 \right] = \int_{0}^{2\pi} \int_{t_0}^{t_1} \int_{0}^{\hat r} t^2 \cdot \frac{rt^2}{V} dr dt d\theta = \frac{3(t_1^5- t_0^5)}{5(t_1^3 - t_0^3)}\] </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">x-axis</code> (\(x = t r cos \theta\)) : <ul> <li> \[E \left[ t r cos\theta \right] = \int_{0}^{2\pi} \int_{t_0}^{t_1} \int_{0}^{\hat r} t r cos\theta \cdot \frac{rt^2}{V} dr dt d\theta = 0\] </li> <li> \[E \left[ (t r cos \theta)^2 \right] = \int_{0}^{2\pi} \int_{t_0}^{t_1} \int_{0}^{\hat r} t^2 r^2 cos^2 \theta \cdot \frac{rt^2}{V} dr dt d\theta = \frac{\hat r^2}{4} \frac{3(t_1^5 - t_0^5)}{5(t_1^3 - t_0^3)}\] </li> </ul> </li> <li> <p><code class="language-plaintext highlighter-rouge">y-axis</code> (\(y = t r sin \theta\)) :<br/> conical frustum은 x, y에 대해 symmetric하므로 위에서 구한 x-axis에서의 값과 동일</p> </li> <li>이제 conical frustum 내부에 있는 random point에 대한 mean, covariance 값을 구할 수 있다 <ul> <li><code class="language-plaintext highlighter-rouge">mean distance along ray from o</code> :<br/> \(\mu_{t} = E \left[ t \right] = \frac{3(t_1^4 - t_0^4)}{4(t_1^3 - t_0^3)}\)</li> <li><code class="language-plaintext highlighter-rouge">variance along ray</code> :<br/> \(\sigma_{t}^2 = E \left[ t^2 \right] - (E \left[ t \right])^2 = \frac{3(t_1^5- t_0^5)}{5(t_1^3 - t_0^3)} - \mu_{t}^2\)</li> <li><code class="language-plaintext highlighter-rouge">variance perpendicular to ray</code> :<br/> \(\sigma_{r}^2 = E \left[ x^2 \right] - (E \left[ x \right])^2 = \hat r^2 \frac{3(t_1^5 - t_0^5)}{20(t_1^3 - t_0^3)}\)</li> </ul> </li> <li>그런데 \(t_0, t_1\) 이 서로 가까우면 \(\frac{(t_1^5- t_0^5)}{(t_1^3 - t_0^3)}\) 과 같은 꼴은 numerically unstable as 0 or NaN instead of accurate values 이므로 training fail<br/> \(\rightarrow\)<br/> \(t_{\mu} = \frac{t_0+t_1}{2}\) and \(t_{\sigma}=\frac{t_1-t_0}{2}\) 로 re-parameterize하면<br/> <code class="language-plaintext highlighter-rouge">first-order term + correct(higher-order) term 꼴</code>로 정리 가능하고<br/> \(t_{\sigma}\) 가 작을 때에도 stable and accurate values 가짐 <ul> <li> \[\mu_{t} = t_{\mu} + \frac{2t_{\mu}t_{\sigma}^2}{3t_{\mu}^2+t_{\sigma}^2}\] </li> <li> \[\sigma_{t}^2 = \frac{t_{\sigma}^2}{3} - \frac{4t_{\sigma}^4(12t_{\mu}^2-t_{\sigma}^2)}{15(3t_{\mu}^2+t_{\sigma}^2)^2}\] </li> <li> \[\sigma_{r}^2 = \hat r^2 (\frac{t_{\mu}^2}{4} + \frac{5t_{\sigma}^2}{12} - \frac{4t_{\sigma}^4}{15(3t_{\mu}^2+t_{\sigma}^2)})\] </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">limitation</code> :<br/> frustum의 base 반지름과 top 반지름 차이가 클수록<br/> conical-frustum을 multi-variate Gaussian으로 approx.하는 건 inaccurate<br/> (예를 들어, camera FOV가 클 때 camera center와 매우 가까운 frustum)<br/> 대부분의 dataset에서는 흔하지 않은 case이긴 하지만,<br/> macro photography with fisheye lens와 같은 특별한 case에서 MipNeRF를 쓸 때는 frustum을 multi-variate Gaussian으로 approx.하는 게 문제가 될 수 있음</li> </ul> <h3 id="architecture">Architecture</h3> <ul> <li>아래 내용들을 제외하고는 NeRF의 Architecture와 동일 <ul> <li>ray-tracing 대신 cone-tracing</li> <li>PE 대신 IPE</li> <li>point-encoding 이므로 \(n\)개의 구간에 대해 \(n\)개의 point sampling<br/> \(\rightarrow\)<br/> interval(region)-encoding 이므로 \(n\)개의 구간을 위해 \(n+1\)개의 point sampling</li> <li>PE feature로는 scale을 반영할 수 없으므로 두 가지 MLP (coarse-MLP, fine-MLP) 이용해서 hierarchical sampling<br/> (coarse-MLP에서는 \(N_c=64\) points per ray, fine-MLP에서는 \(N_c+N_f=64+128\) points per ray)<br/> \(\rightarrow\)<br/> IPE feature 자체가 scale을 반영할 수 있으므로 MLP 하나를 반복해서 써서 hierarchical sampling<br/> (한 번은 \(N_c=128\) points per ray, 그 다음은 \(N_f=128\) points per ray)<br/> NeRF와 MipNeRF의 공정한 비교를 위해 같은 수(총 256개)의 point를 사용</li> <li>hierarchical sampling에서 piecewise-constant PDF of normalized \(w\) 에 따라 fine-sampling 하기 전에<br/> weight \(w_k\) 를 바로 사용하지 않고<br/> 2-tap MaxBlur filter 를 적용하여 weight의 wide and smooth upper bound 를 사용<br/> \(w_k^{\ast} = \frac{1}{2}(max(w_{k-1}, w_k) + max(w_k, w_{k+1})) + \alpha\)<br/> where 빈 공간에서도 일부 samples 추출되도록 보장하기 위해 \(\alpha=0.01\) 설정</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/8-480.webp 480w,/assets/img/2024-08-03-MipNeRF/8-800.webp 800w,/assets/img/2024-08-03-MipNeRF/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> single MLP 쓰니까 coarse loss와 fine loss 간의 balance 맞추기 위해 hyperparam. gamma = 0.1로 설정 </div> <ul> <li>MaxBlur filter : <ul> <li>MaxPool 대신 MaxBlurPool 쓰면 aliasing 감소 효과</li> <li>MipNeRF에서 weight에 MaxBlur filter 쓰는 이유 :<br/> scene content는 아무래도 연속적으로 존재하니까<br/> 인접한 samples 간의 weight \(w\) 가 갑작스럽게 변하거나 불연속적인 outlier 를 제외하여 smoothing 해주는 역할</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/10-480.webp 480w,/assets/img/2024-08-03-MipNeRF/10-800.webp 800w,/assets/img/2024-08-03-MipNeRF/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> MaxBlur on sample weight / plot reference : https://charlieppark.kr/ </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/9-480.webp 480w,/assets/img/2024-08-03-MipNeRF/9-800.webp 800w,/assets/img/2024-08-03-MipNeRF/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Setting :<br/> implementation on JaxNeRF<br/> 1 million iter., Adam optimizer, batch_size = 4096, lr from \(5 \cdot 10^{-4}\) to \(5 \cdot 10^{-6}\)</li> </ul> <h2 id="result">Result</h2> <ul> <li>multi-scale dataset에 대해 NeRF보다 error rate 60% 감소</li> <li>single-scale dataset에 대해 NeRF보다 error rate 17% 감소</li> <li>NeRF의 param.의 절반이고, NeRF보다 7% 빠름</li> <li>brute-force super-sampling한 버전보다 22배 빠른데 accuracy 비슷</li> </ul> <h2 id="question">Question</h2> <ul> <li>Q&amp;A reference : 3DGS online study</li> <li>Q1 : distant view (scene content in low-resolution)일 때 IPE의 \(exp(-\frac{1}{2}(\omega \sigma)^2)\) term에 의해 high freq.를 attenuate하여 anti-aliasing 가능한 건 이해했는데,<br/> close view (scene content in high-resolution)일 때 blurry rendering은 어떻게 해결??</li> <li>A1 : 위에서 “Method - Cone Tracing and Positional Encoding - IPE의 의미”에 설명해둠</li> <li>Q2 : image plane에서의 cone의 \(x, y\) 축 variance가 pixel’s footprint의 variance와 같아지도록<br/> \(\hat r =\) radius at image plane \(o + d\) \(= \frac{2}{\sqrt{12}}\) of pixel-width 로 설정한다는데 이 부분이 이해가 되지 않습니다</li> <li>A2 : uniform distribution을 가정했을 때 pixel의 square variance는 \(\frac{w^2}{12}\) 이고, cone at image plane의 circle variance는 \(\frac{\hat r^2}{4}\) 이므로 variance 값이 같으려면 \(\hat r = \frac{2}{\sqrt{12}} \times w\)</li> <li>Q3 : \(E \left[ \gamma (x) \right] = \frac{\int \gamma (x) F(x, o, d, \hat r, t_0, t_1) dx}{\int F(x, o, d, \hat r, t_0, t_1) dx}\) 에서 분자의 적분식을 closed-form으로 계산할 수 없어서 conical frustum을 multi-variate Gaussian으로 근사했다는데,<br/> conical frustum의 모양과 크기 범위에 대한 parameter가 주어진다면 frustum 내부의 점 \(x\) 에 sin 및 cos을 씌운 \(\gamma (x)\) 의 경우 \(x\) 에 대해 공간 적분할 수 있지 않나요?</li> <li>A3 : frustum 내에 있는 모든 좌표에 \(\gamma\) 를 씌워서 공간 적분하는 것 자체가 말도 안 되게 복잡한 식이라 closed-form solution이 없기 때문에 frustum의 mean과 variance를 구해서 Gaussian으로 근사해서 expected value 구합니다</li> <li>Q4 : 논문을 보면 frustum을 multi-variate Gaussian으로 근사하기 위해서는 먼저 \(F(x, o, d, \hat r, t_0, t_1)\) 의 mean, covariance를 계산해야 한다고 쓰여있던데<br/> appendix를 보면 indicator function인 F의 mean과 covariance가 아니라 conical frustum의 \(r, t, \theta\) 범위를 이용해서 공간 적분해서 \(t, x, y\) 축의 mean과 variance를 계산하지 않나요?</li> <li>A4 : 맞습니다. 논문에서 \(F(x, o, d, \hat r, t_0, t_1)\) 의 mean, covariance를 계산해야 한다고 언급되어 있는 것은 단순히 frustum 내부 범위에 속해있는 지점에 대해 적분을 통해 mean, variance를 구해야 한다는 뜻인 것 같습니다.</li> <li>Q5 : NeRF에서 rendering할 때는 EWA volume splatting과 같은 좌표계 변환을 고려하지 않아도 되나요?</li> <li>A5 : NeRF에서는 ray를 따라 MLP의 output을 alpha-compositing하여 직접 pixel 값을 얻어내므로 ray를 쓰기 위해 cam-to-world coordinate 변환만 필요하고, projection에 의한 non-linear 좌표계 변환과는 관련이 없다.<br/> 반면, Gaussian Splatting에서는 rendering할 때 3D Gaussian 자체를 직접 projection해서 쓰기 때문에 3D Gaussian covariance matrix on world-coordinate을 2D Gaussian covariance matrix on image-coordinate (ray-space)으로 projection해야 하므로 non-linear 좌표계 변환이 필요하다. 이를 위해 EWA volume splatting에 따라 non-linear transformation을 Taylor approx.하여 local affine transformation으로서 Jacobian을 사용한다</li> <li>Q6 : camera origin과 pixel 중심을 잇는 ray가 image plane에 수직이 아닌 pixel의 경우 \(\hat r\) 과 \(d\) 를 어떻게 정의하지?</li> <li>A6 : \(d\) 는 camera origin부터 pixel 중심까지의 거리 vector이고,<br/> cone 단면의 \(\hat r\)은 \(d\) 와 수직인 방향으로 \(\frac{2}{\sqrt{12}}\) of pixel-width 이므로<br/> cone 단면이 image plane 위에 있지 않은 꼴이 됨</li> </ul> <h2 id="appendix">Appendix</h2> <p>TBD</p> <h2 id="code-review">Code Review</h2> <h3 id="ipe-integrated-positional-encoding">IPE (integrated positional encoding)</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/11-480.webp 480w,/assets/img/2024-08-03-MipNeRF/11-800.webp 800w,/assets/img/2024-08-03-MipNeRF/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>self.scales : \([2^l, \ldots, 2^{L-1}]\) of shape (\(L-l\),)</li> <li>Let B : the number of cones<br/> Let S : the number of samples (regions) (Gaussians)</li> <li>case 1: Point-IPE<br/> position은 Gaussian으로 근사해서 IPE 씀 <ul> <li>x : \(\mu\) of shape (B, S, 3) and y : \(diag(\Sigma)\) of shape (B, S, 3)</li> <li>x_enc : \(\mu_{r}\) , 즉 PE-basis-lifted mean of shape (B, S, (\(L-l\)) * 6)<br/> where 6 = 3 (3d-vector) * 2 (sin and cos)<br/> \(\mu_{r} = P \mu = \begin{pmatrix} \begin{matrix} 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2\end{matrix} &amp; \cdots &amp; \begin{matrix} 2^{L-1} &amp; 0 &amp; 0 \\ 0 &amp; 2^{L-1} &amp; 0 \\ 0 &amp; 0 &amp; 2^{L-1}\end{matrix} \end{pmatrix}^T \begin{bmatrix} \mu_{x} \\ \mu_{y} \\ \mu_{z} \end{bmatrix}\)<br/> \(( \begin{bmatrix} \mu_{x} &amp; \mu_{y} &amp; \mu_{z} \\ \vdots &amp; \vdots &amp; \vdots \\ \mu_{x} &amp; \mu_{y} &amp; \mu_{z} \end{bmatrix} \circledast \begin{bmatrix} 2^l &amp; 2^l &amp; 2^l \\ \vdots &amp; \vdots &amp; \vdots \\ 2^{L-1} &amp; 2^{L-1} &amp; 2^{L-1} \end{bmatrix})\) .reshape(B, S, -1) \(= [\mu_{x}2^l, \mu_{y}2^l, \mu_{z}2^l, \ldots, \mu_{x}2^{L-1}, \mu_{y}2^{L-1}, \mu_{z}2^{L-1}]\) of shape ((\(L-l\)) * 3,)<br/> where \(\circledast\) is element-wise multiplication<br/> \(\cos{Px} = \sin{(Px + \frac{\pi}{2})}\)</li> <li>y_enc : \(diag(\Sigma_{r})\) , 즉 diagonal of PE-basis-lifted covariance of shape (B, S, (\(L-l\)) * 6)<br/> where 6 = 3 (3d-vector) * 2 (sin and cos)<br/> \(diag(\Sigma_{r}) = diag(P \Sigma P^T) = \left[ diag(\Sigma), 4 diag(\Sigma), \ldots , 4^{L-1}diag(\Sigma) \right]^T\)<br/> \(( \begin{bmatrix} \Sigma_{00} &amp; \Sigma_{11} &amp; \Sigma_{22} \\ \vdots &amp; \vdots &amp; \vdots \\ \Sigma_{00} &amp; \Sigma_{11} &amp; \Sigma_{22} \end{bmatrix} \circledast \begin{bmatrix} 4^l &amp; 4^l &amp; 4^l \\ \vdots &amp; \vdots &amp; \vdots \\ 4^{L-1} &amp; 4^{L-1} &amp; 4^{L-1} \end{bmatrix})\) .reshape(B, S, -1) \(= [\Sigma_{00}4^l, \Sigma_{11}4^l, \Sigma_{22}4^l, \ldots, \Sigma_{00}4^{L-1}, \Sigma_{11}4^{L-1}, \Sigma_{22}4^{L-1}]\)</li> <li>x_ret : \(\gamma (\mu, \Sigma)\) of shape (B, S, (\(L-l\)) * 6)<br/> \(\gamma (\mu, \Sigma) = E_{Px \sim N(\mu_{r}, \Sigma_{r})} [\begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}] = \begin{bmatrix} sin(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \\ cos(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \end{bmatrix}\)</li> <li>y_ret (covariance of \(\sin{z}\) where \(z \sim N(x_{enc}, y_{enc})\) ) 은 안 씀</li> </ul> </li> <li>case 2: View-Direction-PE<br/> view-direction은 IPE 말고 그냥 PE 씀 <ul> <li>x : view-direction of shape (B, S, 3) and y : None</li> <li>x_enc : \(Pd\) , 즉 PE-basis-lifted view-direction of shape (B, S, (\(L-l\)) * 6)</li> <li>x_ret : \(\gamma (d)\) of shape (B, S, (\(L-l\)) * 6)<br/> \(\gamma (d) = \begin{bmatrix} sin(Pd) \\ cos(Pd) \end{bmatrix}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/13-480.webp 480w,/assets/img/2024-08-03-MipNeRF/13-800.webp 800w,/assets/img/2024-08-03-MipNeRF/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/14-480.webp 480w,/assets/img/2024-08-03-MipNeRF/14-800.webp 800w,/assets/img/2024-08-03-MipNeRF/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> jax ver. </div> <h3 id="maxblur-filter">MaxBlur filter</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/12-480.webp 480w,/assets/img/2024-08-03-MipNeRF/12-800.webp 800w,/assets/img/2024-08-03-MipNeRF/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>\(w_k^{\ast} = \frac{1}{2}(max(w_{k-1}, w_k) + max(w_k, w_{k+1})) + \alpha\)<br/> where 빈 공간에서도 일부 samples 추출되도록 보장하기 위해 constant \(\alpha=0.01\) 설정</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/15-480.webp 480w,/assets/img/2024-08-03-MipNeRF/15-800.webp 800w,/assets/img/2024-08-03-MipNeRF/15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> jax ver. </div>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="rendering"/><category term="3d"/><category term="multiscale"/><category term="antialiasing"/><summary type="html"><![CDATA[A Multiscale Representation for Anti-Aliasing Neural Radiance Fields]]></summary></entry><entry><title type="html">Vim, Pycharm Debug Shortcut</title><link href="https://semyeong-yu.github.io/blog/2024/vim/" rel="alternate" type="text/html" title="Vim, Pycharm Debug Shortcut"/><published>2024-08-01T11:00:00+00:00</published><updated>2024-08-01T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/vim</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/vim/"><![CDATA[<h2 id="vim">Vim</h2> <p>vimtutor : 튜토리얼<br/> vim 파일명 : 노말모드 실행</p> <h3 id="mode">Mode</h3> <ul> <li> <p>입력모드 :<br/> i : 노말모드 &gt; 입력모드 (커서 앞)<br/> I : 노말모드 &gt; 입력모드 (문장 시작)<br/> a : 노말모드 &gt; 입력모드 (커서 뒤)<br/> A : 노말모드 &gt; 입력모드 (문장 끝)</p> </li> <li> <p>노말모드 :<br/> esc : 입력모드 &gt; 노말모드</p> </li> <li> <p>명령모드 :<br/> ‘:’ : 노말모드 &gt; 명령모드</p> </li> </ul> <h3 id="입력모드">입력모드</h3> <p>파일 수정</p> <h3 id="명령모드">명령모드</h3> <p>:q : 종료<br/> :q! : 강제 종료<br/> :w : 저장<br/> :w! : 강제 저장<br/> :wq : 저장 후 종료<br/> :wq! : 강제 저장 후 종료</p> <h3 id="노말모드">노말모드</h3> <h4 id="navigation">Navigation</h4> <ul> <li> <p>커서 :<br/> hjkl : 커서 이동</p> </li> <li> <p>line :<br/> 0 : line 앞<br/> $ : line 뒤<br/> 20G : 20번째 line 앞</p> </li> <li> <p>문단 :<br/> { : 문단 시작<br/> } : 문단 끝</p> </li> <li> <p>단어 :<br/> w : 다음 단어 앞<br/> 3w : 3번째 다음 단어 앞<br/> e : 다음 단어 뒤<br/> b : 이전 단어 앞<br/> 3b : 3번째 이전 단어 앞</p> </li> <li> <p>화면, 파일 :<br/> H : 화면 위<br/> M : 화면 중간<br/> L : 화면 끝<br/> gg : 파일 앞<br/> G : 파일 끝</p> </li> <li> <p>스크롤링 :<br/> Ctrl+u : 위로 스크롤링<br/> Ctrl+d : 아래로 스크롤링</p> </li> </ul> <h4 id="비주얼선택-잘라내기-복사-붙여넣기">비주얼(선택), 잘라내기, 복사, 붙여넣기</h4> <ul> <li> <p>비주얼(선택) 모드 :<br/> v : 비주얼(선택) 모드<br/> Ctrl+v : 블럭 단위 비주얼(선택) 모드<br/> v + hjkl : 드래그 선택<br/> v aw : 단어 1개 선택</p> </li> <li> <p>잘라내기 :<br/> x : 글자 잘라내기<br/> dd : line 잘라내기</p> </li> <li> <p>복사 :<br/> y : 복사<br/> yy : line 복사</p> </li> <li> <p>붙여넣기 :<br/> p : 붙여넣기<br/> “p 혹은 *p : 클립보드 붙여넣기</p> </li> </ul> <h4 id="반복-되감기-앞감기">반복, 되감기, 앞감기</h4> <ul> <li>. : 이전 명령 반복</li> <li>u : undo (되돌리기)</li> <li>Ctrl+r : redo</li> </ul> <h4 id="command--object-조합">Command + Object 조합</h4> <ul> <li> <p>예시 :<br/> d 3w : 다음 단어 3개 잘라내기<br/> d 2j : 아래 2줄 잘라내기<br/> c i[ : 대괄호 안에 있는 것을 변경</p> </li> <li> <p>Command :<br/> d : 잘라내기 (delete)<br/> y : 복사 (yank)<br/> c : 변경 (change)<br/> v : 선택 (visual)<br/> Ctrl+v : 블럭 단위 선택</p> </li> <li> <p>Object :<br/> 3w : 다음 단어 3개<br/> 3b : 이전 단어 3개<br/> aw : 단어 1개<br/> ap : 문단 1개<br/> as : line 1개<br/> i” : “ “ 안에 있는 것<br/> ip : 문단 안에 있는 것<br/> i{ : 중괄호 안에 있는 것<br/> i( : 소괄호 안에 있는 것<br/> a( : 소괄호 포함 모든 것<br/> a[ : 대괄포 포함 모든 것<br/> f( : 현재부터 소괄호(포함)까지<br/> t( : 현재부터 소괄호(미포함)까지<br/> /abc : 현재부터 abc(미포함)까지 (드래그 표시로 확인 가능)</p> </li> </ul> <h4 id="검색">검색</h4> <ul> <li>/<단어> : <단어> 검색 후 n 누르면 밑으로 계속 검색</단어></단어></li> <li>?<단어> : <단어> 검색 후 n 누르면 위로 계속 검색</단어></단어></li> <li>n : 계속 검색</li> </ul> <h2 id="pycharm-debug">Pycharm Debug</h2> <ul> <li> <p>실행 :<br/> Ctrl+F5 : 그냥 실행<br/> F9 : break point 설정<br/> F5 또는 우상단 벌레 버튼 : 디버깅 모드 실행 (첫 번째 break point 직전에서 멈춤)</p> </li> <li> <p>디버깅 모드 :<br/> F10 : 코드 한 줄 실행<br/> F11 : 함수 안으로 이동<br/> Shift+F11 : 함수 밖(호출 위치)로 이동<br/> F5 : 다음 breakpoint 직전에서 멈춤<br/> Shift+F5 또는 우상단 정지 버튼 : 디버깅 모드 해제</p> </li> </ul>]]></content><author><name></name></author><category term="cv-tasks"/><category term="vim"/><category term="pycharm"/><category term="debug"/><summary type="html"><![CDATA[vim, pycharm debug shortcut]]></summary></entry><entry><title type="html">Normalized Device Coordinates</title><link href="https://semyeong-yu.github.io/blog/2024/NDC/" rel="alternate" type="text/html" title="Normalized Device Coordinates"/><published>2024-07-30T15:00:00+00:00</published><updated>2024-07-30T15:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/NDC</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/NDC/"><![CDATA[<h2 id="ndc-normalized-device-coordinates">NDC: Normalized Device Coordinates</h2> <blockquote> <p>referenced blog :<br/> <a href="https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#background">https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#background</a></p> </blockquote> <h3 id="motivation">Motivation</h3> <p>NeRF에서<br/> MLP의 input은 3D world-coordinate이고,<br/> MLP의 output인 \(c, \sigma\) 를 accumulate해서 2D pixel-coordinate을 채운다<br/> 이 때, LLFF (Local Light Field Fusion) dataset 에 있는<br/> <code class="language-plaintext highlighter-rouge">unbounded (in single direction) 3D world-coordinate</code>의 scene 정보를<br/> <code class="language-plaintext highlighter-rouge">bounded 3D NDC space</code>로 project하면<br/> <code class="language-plaintext highlighter-rouge">MLP를 효율적으로 쓸 수 있다</code><br/> NDC space로의 projection 과정을 수식적으로 알아보고자 한다.</p> <h3 id="from-world-coordinate-to-ndc-to-pixel-coordinate">From world-coordinate To NDC To pixel-coordinate</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-30-NDC/1-480.webp 480w,/assets/img/2024-07-30-NDC/1-800.webp 800w,/assets/img/2024-07-30-NDC/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-30-NDC/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>camera transformation : <ul> <li><code class="language-plaintext highlighter-rouge">3D world-coordinate</code> (canonical-coordinate)<br/> \(\rightarrow\)<br/> <code class="language-plaintext highlighter-rouge">3D camera-coordinate</code></li> <li>extrinsic matrix \(\begin{bmatrix} R &amp; t \\ 0 &amp; 1 \end{bmatrix}\)</li> </ul> </li> <li>projection transformation : <ul> <li><code class="language-plaintext highlighter-rouge">3D camera-coordinate</code><br/> \(\rightarrow\)<br/> <code class="language-plaintext highlighter-rouge">3D NDC (normalized-device-coordinate)</code> (canonical view volume)</li> <li>normalized-device-coordinate (NDC) :<br/> <code class="language-plaintext highlighter-rouge">camera 원점이 중앙에 있는</code> \([-1, 1]^3\) cube (<code class="language-plaintext highlighter-rouge">정육면체</code>)</li> <li>frustum \(\rightarrow\) 직육면체 \(\rightarrow\) 정육면체<br/> consists of perspective projection and then orthographic projection<br/> z-axis 방향 바꾸기 포함</li> </ul> </li> <li>viewport transformation : <ul> <li><code class="language-plaintext highlighter-rouge">3D NDC</code><br/> \(\rightarrow\)<br/> <code class="language-plaintext highlighter-rouge">2D pixel-coordinate</code></li> <li>\([-1, 1]^3\) 의 NDC를 flatten하여 2 \(\times\) 2 square를 raster image로 mapping</li> <li>intrinsic matrix \(\begin{bmatrix} f_x &amp; s &amp; W/2 \\ 0 &amp; f_y &amp; H/2 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\) (초점거리 곱하고 원점 좌상단 이동)<br/> y-axis 방향 바꾸기 포함</li> </ul> </li> </ul> <h3 id="projection-transformation">Projection Transformation</h3> <blockquote> <p>Step 1. <code class="language-plaintext highlighter-rouge">Perspective Projection</code></p> </blockquote> <ul> <li>frustum을 bounded cuboid로 변환<br/> bound :<br/> \(x \in [l, r]\) where \(l \lt 0\), \(r \gt 0\)<br/> \(y \in [b, t]\) where \(b \lt 0\), \(t \gt 0\)<br/> \(z \in [f, n]\) where \(f \lt 0\), \(n \lt 0\)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-30-NDC/2-480.webp 480w,/assets/img/2024-07-30-NDC/2-800.webp 800w,/assets/img/2024-07-30-NDC/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-30-NDC/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-30-NDC/3-480.webp 480w,/assets/img/2024-07-30-NDC/3-800.webp 800w,/assets/img/2024-07-30-NDC/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-30-NDC/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 3D camera-coordinate </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-30-NDC/4-480.webp 480w,/assets/img/2024-07-30-NDC/4-800.webp 800w,/assets/img/2024-07-30-NDC/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-30-NDC/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>\(z = n\) plane은 그대로 냅두고, 직육면체 꼴이 되도록 그 뒤 plane 변환<br/> camera를 통과하는 any line은 z-axis에 평행한 line이 됨</p> </li> <li> <p>perspective projection matrix :<br/> \(P_{per} = \begin{bmatrix} n &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; n &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; n+f &amp; -nf \\ 0 &amp; 0 &amp; 1 &amp; 0 \end{bmatrix}\)<br/> \(P_{per} \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix} = \begin{bmatrix} nX \\ nY \\ (n+f)Z - nf \\ Z \end{bmatrix}\)<br/> <code class="language-plaintext highlighter-rouge">?????</code></p> </li> </ul> <blockquote> <p>Step 2. <code class="language-plaintext highlighter-rouge">Orthographic Projection</code></p> </blockquote> <ul> <li> <p>corner (l, b, n)이 원점이 되도록 shift한 뒤,<br/> \([0, r-l] \times [0, t-b] \times [f-n, 0]\) 의 직육면체를 \([0, 2] \times [0, 2] \times [-2, 0]\) 의 정육면체로 scale한 뒤,<br/> center (1, 1, -1)이 원점이 되도록 \([-1, 1]^3\) 으로 shift</p> </li> <li> <p>orthographic projection matrix :<br/> \(M_{orth} = \begin{pmatrix} I_{3 \times 3} &amp; \begin{matrix} -1 \\ -1 \\ 1 \end{matrix} \\ 0_{1 \times 3} &amp; 1 \end{pmatrix} \begin{pmatrix} \begin{matrix} \frac{2}{r-l} &amp; 0 &amp; 0 \\ 0 &amp; \frac{2}{t-b} &amp; 0 \\ 0 &amp; 0 &amp; \frac{2}{n-f} \end{matrix} &amp; 0_{3 \times 1} \\ 0_{1 \times 3} &amp; 1 \end{pmatrix} \begin{pmatrix} I_{3 \times 3} &amp; \begin{matrix} -l \\ -b \\ -n \end{matrix} \\ 0_{1 \times 3} &amp; 1 \end{pmatrix}\)<br/> \(= \begin{bmatrix} \frac{2}{r-l} &amp; 0 &amp; 0 &amp; -\frac{r+l}{r-l} \\ 0 &amp; \frac{2}{t-b} &amp; 0 &amp; -\frac{t+b}{t-b} \\ 0 &amp; 0 &amp; \frac{2}{n-f} &amp; -\frac{n+f}{n-f} \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix}\)</p> </li> </ul> <blockquote> <p>Step 3. <code class="language-plaintext highlighter-rouge">Projection Matrix</code></p> </blockquote> <p>Since perspective projection matrix is scalable,<br/> \(M_{proj} = M_{orth} (- P_{per})\)<br/> \(= \begin{bmatrix} \frac{2}{r-l} &amp; 0 &amp; 0 &amp; -\frac{r+l}{r-l} \\ 0 &amp; \frac{2}{t-b} &amp; 0 &amp; -\frac{t+b}{t-b} \\ 0 &amp; 0 &amp; \frac{2}{n-f} &amp; -\frac{n+f}{n-f} \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix} \begin{bmatrix} -n &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; -n &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; -n-f &amp; nf \\ 0 &amp; 0 &amp; -1 &amp; 0 \end{bmatrix}\)<br/> \(= \begin{bmatrix} -\frac{2n}{r-l} &amp; 0 &amp; \frac{r+l}{r-l} &amp; 0 \\ 0 &amp; -\frac{2n}{t-b} &amp; \frac{t+b}{t-b} &amp; 0 \\ 0 &amp; 0 &amp; -\frac{n+f}{n-f} &amp; \frac{2nf}{n-f} \\ 0 &amp; 0 &amp; -1 &amp; 0 \end{bmatrix}\)</p> <p>camera-coordinate에서 \(z \in [f, n]\) where \(f \lt 0\), \(n \lt 0\) 이었는데,<br/> NDC에서는 z-axis의 방향이 반대이므로<br/> \(f \lt 0\), \(n \lt 0\) 대신 \(f = -f \gt 0\), \(n = -n \gt 0\) 를 대입하면,<br/> \(M_{proj} = \begin{bmatrix} \frac{2n}{r-l} &amp; 0 &amp; \frac{r+l}{r-l} &amp; 0 \\ 0 &amp; \frac{2n}{t-b} &amp; \frac{t+b}{t-b} &amp; 0 \\ 0 &amp; 0 &amp; -\frac{n+f}{n-f} &amp; -\frac{2nf}{n-f} \\ 0 &amp; 0 &amp; -1 &amp; 0 \end{bmatrix}\)</p> <p>OpenGL과 같은 graphics frameworks에서는 보통<br/> \(M_{proj}X\) 를 \(M_{proj}X\) 의 fourth entry로 나눴을 때 \(M_{proj}X\) 의 third entry(Z 값)이 양수가 되도록 하기 때문에 (아래 Step 4의 NDC 참고)<br/> 조금 수정하면<br/> 최종적인 projection matrix는<br/> \(M_{proj} = \begin{bmatrix} \frac{2n}{r-l} &amp; 0 &amp; \frac{r+l}{r-l} &amp; 0 \\ 0 &amp; \frac{2n}{t-b} &amp; \frac{t+b}{t-b} &amp; 0 \\ 0 &amp; 0 &amp; -\frac{n+f}{f-n} &amp; -\frac{2nf}{f-n} \\ 0 &amp; 0 &amp; -1 &amp; 0 \end{bmatrix}\)</p> <p>camera frustum은 보통 symmetric하므로 \(l = -r\), \(b = -t\) 라 했을 때<br/> projection matrix는<br/> \(M_{proj} = \begin{bmatrix} \frac{n}{r} &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; \frac{n}{t} &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; -\frac{f+n}{f-n} &amp; -\frac{2nf}{f-n} \\ 0 &amp; 0 &amp; -1 &amp; 0 \end{bmatrix}\)</p> <blockquote> <p>Step 4. from <code class="language-plaintext highlighter-rouge">camera-coordinate</code> to <code class="language-plaintext highlighter-rouge">NDC</code></p> </blockquote> <ul> <li> <p>camera-coordinate :<br/> \(\boldsymbol X = \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}\)<br/> where \(Z \lt 0\)</p> </li> <li> <p>NDC :</p> <ul> <li>\(\begin{bmatrix} -\frac{n}{r}\frac{X}{Z} \\ -\frac{n}{t}\frac{Y}{Z} \\ \frac{f+n}{f-n} + \frac{2nf}{f-n}\frac{1}{Z} \\ 1 \end{bmatrix} = \boldsymbol x \sim M_{proj} \boldsymbol X = \begin{bmatrix} \frac{n}{r}X \\ \frac{n}{t}Y \\ -\frac{f+n}{f-n}Z -\frac{2nf}{f-n} \\ -Z \end{bmatrix}\)<br/> where \(Z \lt 0\) and \(n, f \gt 0\)</li> <li>검토해보면, \(Z = -n\) 은 \(x_Z = -1\) 로 mapping되고, \(Z = -f\) 는 \(x_Z = 1\) 로 잘 mapping되네~</li> <li>Let<br/> \(a_x = -\frac{n}{r}\)<br/> \(a_y = -\frac{n}{t}\)<br/> \(a_z = \frac{f+n}{f-n}\)<br/> \(b_z = \frac{2nf}{f-n}\)<br/> Then \(\boldsymbol x = \begin{bmatrix} a_x\frac{X}{Z} \\ a_y\frac{Y}{Z} \\ a_z + \frac{b_z}{Z} \\ 1 \end{bmatrix} = \begin{bmatrix} a_x\frac{X}{Z} \\ a_y\frac{Y}{Z} \\ a_z + \frac{b_z}{Z} \end{bmatrix}\)</li> </ul> </li> </ul> <h3 id="linear-in-disparity">Linear in Disparity</h3> <ul> <li>출처 : https://charlieppark.kr</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-30-NDC/5-480.webp 480w,/assets/img/2024-07-30-NDC/5-800.webp 800w,/assets/img/2024-07-30-NDC/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-30-NDC/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>LLFF (Local Light Field Fusion) : <ul> <li>Nyquist-rate에 따르면 카메라가 \(\frac{1}{2K_xf(\frac{1}{z_{min}}-\frac{1}{z_{max}})}\) 보다 촘촘히 있어야 한다<br/> \(\Delta_{u} \leq \frac{1}{2K_xf(\frac{1}{z_{min}}-\frac{1}{z_{max}})}\)<br/> where \(K_x = \text{min}(B_x, \frac{1}{2\Delta_{x})\)</li> <li>LLFF dataset에서 다루는 scene은 unbounded in single direction (front-facing) 이므로 \(z_{max} = \infty\) 이므로<br/> \(\Delta_{u} \leq \frac{1}{2K_xf\frac{1}{z_{min}}}\)</li> <li>즉, \(z_{min}\) 이 작을수록 (<code class="language-plaintext highlighter-rouge">물체가 가까이 있을수록</code>)<br/> 더 <code class="language-plaintext highlighter-rouge">촘촘한 view-sampling이 필요</code>하며<br/> <code class="language-plaintext highlighter-rouge">high-freq.</code> detail을 많이 가지고 있다는 의미이다</li> </ul> </li> <li>\(\begin{bmatrix} -\frac{n}{r}\frac{X}{Z} \\ -\frac{n}{t}\frac{Y}{Z} \\ \frac{f+n}{f-n} + \frac{2nf}{f-n}\frac{1}{Z} \\ 1 \end{bmatrix} = \boldsymbol x \sim M_{proj} \boldsymbol X = \begin{bmatrix} \frac{n}{r}X \\ \frac{n}{t}Y \\ -\frac{f+n}{f-n}Z -\frac{2nf}{f-n} \\ -Z \end{bmatrix}\)<br/> where \(Z \lt 0\) and \(n, f \gt 0\) 에서<br/> \(Z\) 축에 해당하는 \(\frac{f+n}{f-n} + \frac{2nf}{f-n}\frac{1}{Z}\) 만 보면<br/> <code class="language-plaintext highlighter-rouge">NDC space의 깊이</code> 값은 원래 camera coordinate의 깊이 값의 역수, 즉 <code class="language-plaintext highlighter-rouge">camera-coordinate의 disparity에 비례</code>한다는 것을 알 수 있다<br/> 즉, NDC space에서의 depth distance에 따라 stratified uniform sampling한다면<br/> 원래 camera coordinate의 disparity에 비례하여 sampling하는 효과를 가진다</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-30-NDC/6-480.webp 480w,/assets/img/2024-07-30-NDC/6-800.webp 800w,/assets/img/2024-07-30-NDC/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-30-NDC/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="projection-in-nerf-ray">Projection in NeRF ray</h3> <p>any 3D points on ray \(r = o + td\)를<br/> NDC space (camera 원점이 중앙에 있는 \([-1, 1]^3\) cube) 로 projection하면<br/> 3D points on projected ray \(r^{\ast} = o^{\ast} + t^{\ast} d^{\ast}\) 가 된다<br/> 위에서 유도한 Projection Matrix 를 사용하면<br/> \(\boldsymbol x = \begin{bmatrix} a_x\frac{o_x + td_x}{o_z + td_z} \\ a_y\frac{o_y + td_y}{o_z + td_z} \\ a_z + \frac{b_z}{o_z + td_z} \end{bmatrix} = \begin{bmatrix} o_x^{\ast} + t^{\ast} d_x^{\ast} \\ o_y^{\ast} + t^{\ast} d_y^{\ast} \\ o_z^{\ast} + t^{\ast} d_z^{\ast} \end{bmatrix}\)</p> <p>먼저 projected 원점 좌표를 구해보자<br/> \(t = t^{\ast} = 0\) 를 대입하면<br/> \(o^{\ast} = \begin{bmatrix} o_x^{\ast} \\ o_y^{\ast} \\ o_z^{\ast} \end{bmatrix} = \begin{bmatrix} a_x\frac{o_x}{o_z} \\ a_y\frac{o_y}{o_z} \\ a_z + \frac{b_z}{o_z} \end{bmatrix}\)</p> <p>다음으로 projected t와 d를 구해보자<br/> \(\begin{bmatrix} t^{\ast} d_x^{\ast} \\ t^{\ast} d_y^{\ast} \\ t^{\ast} d_z^{\ast} \end{bmatrix} = \begin{bmatrix} a_x\frac{o_x + td_x}{o_z + td_z} \\ a_y\frac{o_y + td_y}{o_z + td_z} \\ a_z + \frac{b_z}{o_z + td_z} \end{bmatrix} - \begin{bmatrix} o_x^{\ast} \\ o_y^{\ast} \\ o_z^{\ast} \end{bmatrix}\)<br/> \(= \begin{bmatrix} a_x\frac{o_x + td_x}{o_z + td_z} - a_x\frac{o_x}{o_z} \\ a_y\frac{o_y + td_y}{o_z + td_z} - a_y\frac{o_y}{o_z} \\ a_z + \frac{b_z}{o_z + td_z} - (a_z + \frac{b_z}{o_z}) \end{bmatrix}\)<br/> \(= \begin{bmatrix} a_x\frac{td_z}{o_z + td_z}(\frac{d_x}{d_z} - \frac{o_x}{o_z}) \\ a_y\frac{td_z}{o_z + td_z}(\frac{d_y}{d_z} - \frac{o_y}{o_z}) \\ -b_z\frac{td_z}{o_z + td_z}\frac{1}{o_z} \end{bmatrix}\)<br/> \(= \frac{td_z}{o_z + td_z} \begin{bmatrix} a_x(\frac{d_x}{d_z} - \frac{o_x}{o_z}) \\ a_y(\frac{d_y}{d_z} - \frac{o_y}{o_z}) \\ -b_z\frac{1}{o_z} \end{bmatrix}\)</p> <blockquote> <p>Result</p> </blockquote> <p>ray \(r = o + td\)를<br/> NDC space (camera 원점이 중앙에 있는 \([-1, 1]^3\) cube) 로 projection 했을 때<br/> projected ray \(r^{\ast} = o^{\ast} + t^{\ast} d^{\ast}\) 는 아래와 같이 구할 수 있다</p> <p>\(o^{\ast} = \begin{bmatrix} o_x^{\ast} \\ o_y^{\ast} \\ o_z^{\ast} \end{bmatrix} = \begin{bmatrix} a_x\frac{o_x}{o_z} \\ a_y\frac{o_y}{o_z} \\ a_z + \frac{b_z}{o_z} \end{bmatrix}\)<br/> and<br/> \(t^{\ast} = \frac{td_z}{o_z + td_z} = 1 - \frac{o_z}{o_z + td_z}\)<br/> and<br/> \(d^{\ast} = \begin{bmatrix} d_x^{\ast} \\ d_y^{\ast} \\ d_z^{\ast} \end{bmatrix} = \begin{bmatrix} a_x(\frac{d_x}{d_z} - \frac{o_x}{o_z}) \\ a_y(\frac{d_y}{d_z} - \frac{o_y}{o_z}) \\ -b_z\frac{1}{o_z} \end{bmatrix}\)</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">Ray Projection to NDC 장점</code></p> </blockquote> <ul> <li> <p>ray에서 \(t \in [0, \infty)\) 였다면 projected ray에서 \(t^{\ast} \in [0, 1)\)<br/> LLFF dataset에서<br/> camera에서 출발한 ray가 아무 object도 “hit”하지 않는다면 \(t = \infty\)일텐데,<br/> NDC (bounded cube)로 warp한다면 \(t^{\ast} \in [0, 1)\) 이므로<br/> MLP 효율적으로 쓸 수 있음</p> </li> <li> <p>single-direction이긴 하지만<br/> NDC space에서의 depth distance에 따라 stratified uniform sampling한다면<br/> 원래 camera coordinate의 disparity에 비례하여 sampling하는 효과를 가지므로<br/> 가까이 있는 content는 많이 sampling하고 멀리 있는 content는 덜 sampling함으로써<br/> 임의의 scale의 unbounded scene을 잘 다룰 수 있음</p> </li> </ul> <blockquote> <p>Projection transformation 한계</p> </blockquote> <p>LLFF dataset과 같이 <code class="language-plaintext highlighter-rouge">single</code> direction으로만 unbounded된 camera frustum, 즉 <code class="language-plaintext highlighter-rouge">front-facing scene</code>에 대해서만 적용 가능하고<br/> unbounded 360 scene에 대해서는 기본 NeRF가 잘 수행 못함<br/> \(\rightarrow\) MipNeRF360 등 NeRF 후속 연구에서 해결됨</p> <blockquote> <p>특정 case</p> </blockquote> <p>\(f_{cam}\)이 camera의 focal length이고,<br/> \(W, H\)가 image plane의 width, height in pix 일 때<br/> <code class="language-plaintext highlighter-rouge">image plane이 정확히 camera frustum의 near plane</code>에 있고<br/> <code class="language-plaintext highlighter-rouge">camera frustum의 far plane을 infinity로 확장</code>하도록<br/> camera를 설정하면,<br/> \(z = -n = -f_{cam} \lt 0\), \(r = \frac{W}{2}\), \(t = \frac{H}{2}\), \(z = -f \rightarrow -\infty\) 이므로</p> <p>\(a_x = -\frac{n}{r} = -\frac{f_{cam}}{\frac{W}{2}}\)<br/> \(a_y = -\frac{n}{t} = -\frac{f_{cam}}{\frac{H}{2}}\)<br/> \(\lim_{f \rightarrow \infty} a_z = \lim_{f \rightarrow \infty} \frac{f+n}{f-n} = 1\)<br/> \(\lim_{f \rightarrow \infty} b_z = \lim_{f \rightarrow \infty} \frac{2nf}{f-n} = 2n\)<br/> 이므로</p> <p>ray \(r = o + td\) 를 NDC로 projection했을 때<br/> projected ray \(r^{\ast} = o^{\ast} + t^{\ast} d^{\ast}\) 에서<br/> \(o^{\ast} = \begin{bmatrix} -\frac{f_{cam}}{\frac{W}{2}}\frac{o_x}{o_z} \\ -\frac{f_{cam}}{\frac{H}{2}}\frac{o_y}{o_z} \\ 1 + \frac{2n}{o_z} \end{bmatrix}\)<br/> and<br/> \(t^{\ast} = \frac{td_z}{o_z + td_z} = 1 - \frac{o_z}{o_z + td_z}\)<br/> and<br/> \(d^{\ast} = \begin{bmatrix} -\frac{f_{cam}}{\frac{W}{2}}(\frac{d_x}{d_z} - \frac{o_x}{o_z}) \\ -\frac{f_{cam}}{\frac{H}{2}}(\frac{d_y}{d_z} - \frac{o_y}{o_z}) \\ -2n\frac{1}{o_z} \end{bmatrix}\)</p> <h3 id="ndc-projection-in-nerf-pytorch-code">NDC projection in NeRF Pytorch code</h3> <p><a href="https://github.com/yenchenlin/nerf-pytorch">NeRF-Pytorch</a> 기준으로<br/> run_nerf_helpers.py의 ndc_rays()에 구현되어 있으며<br/> 자세한 설명은 <a href="https://semyeong-yu.github.io/blog/2024/NeRFcode/">NeRF-Code-Review</a>에 있음</p>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="NDC"/><category term="3d"/><summary type="html"><![CDATA[How NDC Works for Ray]]></summary></entry><entry><title type="html">State Space Model</title><link href="https://semyeong-yu.github.io/blog/2024/SSM/" rel="alternate" type="text/html" title="State Space Model"/><published>2024-07-18T15:00:00+00:00</published><updated>2024-07-18T15:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/SSM</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/SSM/"><![CDATA[<h2 id="state-space-model">State Space Model</h2> <blockquote> <p>참고 논문 :<br/> <a href="https://arxiv.org/abs/2406.07887">https://arxiv.org/abs/2406.07887</a><br/> 참고 강연 :<br/> by NVIDIA Wonmin Byeon</p> </blockquote> <h3 id="abstract">Abstract</h3> <ul> <li>Large Language Models (LLMs) are usually based on <code class="language-plaintext highlighter-rouge">Transformer</code> architectures. <ul> <li>Transformer-based models 장점 :<br/> highly <code class="language-plaintext highlighter-rouge">parallelizable</code><br/> can model <code class="language-plaintext highlighter-rouge">massive amounts of data</code></li> <li>Transformer-based models 단점 :<br/> significant <code class="language-plaintext highlighter-rouge">computational overhead</code> due to the <code class="language-plaintext highlighter-rouge">quadratic self-attention</code> calculations, especially on longer sequences<br/> large inference-time <code class="language-plaintext highlighter-rouge">memory requirements</code> from the <code class="language-plaintext highlighter-rouge">key-value cache</code></li> </ul> </li> <li>More recently, <code class="language-plaintext highlighter-rouge">State Space Models (SSM)</code> like Mamba have been shown to have fast parallelizable training and inference as an alternative of Transformer.<br/> In this talk, I present the strengths and weaknesses of <code class="language-plaintext highlighter-rouge">Mamba, Mamba-2, and Transformer models</code> at larger scales. I also introduce a <code class="language-plaintext highlighter-rouge">hybrid architecture consisting of Mamba-2, attention, and MLP layers</code>.<br/> While pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks that require <code class="language-plaintext highlighter-rouge">strong copying</code> or <code class="language-plaintext highlighter-rouge">in-context learning</code> abilities.<br/> In contrast, the hybrid model closely matches or exceeds the Transformer on all standard and long-context tasks and is predicted to be up to 8x faster when generating tokens at inference time.</li> </ul> <h3 id="is-attention-all-we-need">Is Attention All We Need?</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/4-480.webp 480w,/assets/img/2024-07-18-SSM/4-800.webp 800w,/assets/img/2024-07-18-SSM/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Transformer : <ul> <li>fast training due to parallelization</li> <li>slow inference for long sequence(context) <ul> <li>key-value cache can improve speed, but increase GPU memory</li> </ul> </li> </ul> </li> <li>RNN : <ul> <li>slow training due to no parallelization</li> <li>fast inference because scale linearly with sequence length</li> </ul> </li> <li>Mamba : <ul> <li>fast training</li> <li>fast inference because scale linearly with sequence length and can deal with unbounded context</li> </ul> </li> <li> <p>SSM or RNN :<br/> state = fixed-sized vector (compression)<br/> high efficiency, but low performance</p> </li> <li>Transformer :<br/> cache of entire history (no compression)<br/> high performance, but low efficiency</li> </ul> <h3 id="mamba-linear-time-sequence-modeling-with-selective-state-spaces">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</h3> <ul> <li>SSM</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/1-480.webp 480w,/assets/img/2024-07-18-SSM/1-800.webp 800w,/assets/img/2024-07-18-SSM/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Selective SSM :<br/> matrix B, C and step size are dependent on the input</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/2-480.webp 480w,/assets/img/2024-07-18-SSM/2-800.webp 800w,/assets/img/2024-07-18-SSM/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Parallel scan :<br/> The order does not matter through the associative property, so can calculate sequences in part and iteratively combine them</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/3-480.webp 480w,/assets/img/2024-07-18-SSM/3-800.webp 800w,/assets/img/2024-07-18-SSM/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Hardware-aware implementation :<br/> minimize copying between RAMs</li> </ul> <h3 id="mamba-2">Mamba-2</h3> <ul> <li>Mamba에서 Main Bottleneck이 Parallel scan 부분이었는데,<br/> Mamba-2는 divide input into chunks 등 architecture 개선으로 이를 해결하고자 했음</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/13-480.webp 480w,/assets/img/2024-07-18-SSM/13-800.webp 800w,/assets/img/2024-07-18-SSM/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="limitations-of-mamba">Limitations of Mamba</h3> <ul> <li>Poor at MMLU and Phonebook task<br/> 아래를 요구하는 task에 대해서는 Mamba가 잘 못함 <ul> <li>in-context learning</li> <li>info. routing between tokens</li> <li>copying from the context (bad on long-context tasks)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/5-480.webp 480w,/assets/img/2024-07-18-SSM/5-800.webp 800w,/assets/img/2024-07-18-SSM/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/6-480.webp 480w,/assets/img/2024-07-18-SSM/6-800.webp 800w,/assets/img/2024-07-18-SSM/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="hybrid-architecture-of-mamba-and-transformer">Hybrid Architecture of Mamba and Transformer</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/7-480.webp 480w,/assets/img/2024-07-18-SSM/7-800.webp 800w,/assets/img/2024-07-18-SSM/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Our Hybrid Mamba-Transformer Model <ul> <li>Minimize the number of Attention Layers and Maximize the number of MLPs</li> <li>Does not necessarily need Rotary Position Embedding (RoPE)</li> <li>evenly spread attention and MLP layers</li> <li>Place Mamba layer at the beginning, so has no position embedding</li> <li>Group-Query Attention (GQA) makes more efficient</li> <li>Global Attention makes better performance</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/11-480.webp 480w,/assets/img/2024-07-18-SSM/11-800.webp 800w,/assets/img/2024-07-18-SSM/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Mamba-2 Hybrid<br/> Inference Speed is fast<br/> Now, states in Mamba can understand longer history!</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/12-480.webp 480w,/assets/img/2024-07-18-SSM/12-800.webp 800w,/assets/img/2024-07-18-SSM/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Attention Layer is bottleneck at Hybrid model,<br/> so Context Length가 길어질수록 Speedup 증가율은 줄어듬</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/8-480.webp 480w,/assets/img/2024-07-18-SSM/8-800.webp 800w,/assets/img/2024-07-18-SSM/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/9-480.webp 480w,/assets/img/2024-07-18-SSM/9-800.webp 800w,/assets/img/2024-07-18-SSM/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="summary">Summary</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/10-480.webp 480w,/assets/img/2024-07-18-SSM/10-800.webp 800w,/assets/img/2024-07-18-SSM/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 왼쪽부터 4K, 16K, 32K-based models </div> <p>Mamba-2 Hybrid는 Transformer와 달리 Quadratic calculation까지 필요 없고 inference 빠름<br/> but, Attention Layer가 Bottleneck이듯이 해결해야 할 사항들이 남아 있어 앞으로도 발전 가능성 있음</p>]]></content><author><name></name></author><category term="cv-tasks"/><category term="SSM"/><category term="Mamba"/><summary type="html"><![CDATA[SSM]]></summary></entry></feed>