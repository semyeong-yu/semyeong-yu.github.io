<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://semyeong-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://semyeong-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-07-25T14:44:09+00:00</updated><id>https://semyeong-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">State Space Model</title><link href="https://semyeong-yu.github.io/blog/2024/SSM/" rel="alternate" type="text/html" title="State Space Model"/><published>2024-07-18T15:00:00+00:00</published><updated>2024-07-18T15:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/SSM</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/SSM/"><![CDATA[<h2 id="state-space-model">State Space Model</h2> <blockquote> <p>참고 논문 :<br/> <a href="https://arxiv.org/abs/2406.07887">https://arxiv.org/abs/2406.07887</a><br/> 참고 강연 :<br/> by NVIDIA Wonmin Byeon</p> </blockquote> <h3 id="abstract">Abstract</h3> <ul> <li>Large Language Models (LLMs) are usually based on <code class="language-plaintext highlighter-rouge">Transformer</code> architectures. <ul> <li>Transformer-based models 장점 :<br/> highly <code class="language-plaintext highlighter-rouge">parallelizable</code><br/> can model <code class="language-plaintext highlighter-rouge">massive amounts of data</code></li> <li>Transformer-based models 단점 :<br/> significant <code class="language-plaintext highlighter-rouge">computational overhead</code> due to the <code class="language-plaintext highlighter-rouge">quadratic self-attention</code> calculations, especially on longer sequences<br/> large inference-time <code class="language-plaintext highlighter-rouge">memory requirements</code> from the <code class="language-plaintext highlighter-rouge">key-value cache</code></li> </ul> </li> <li>More recently, <code class="language-plaintext highlighter-rouge">State Space Models (SSM)</code> like Mamba have been shown to have fast parallelizable training and inference as an alternative of Transformer.<br/> In this talk, I present the strengths and weaknesses of <code class="language-plaintext highlighter-rouge">Mamba, Mamba-2, and Transformer models</code> at larger scales. I also introduce a <code class="language-plaintext highlighter-rouge">hybrid architecture consisting of Mamba-2, attention, and MLP layers</code>.<br/> While pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks that require <code class="language-plaintext highlighter-rouge">strong copying</code> or <code class="language-plaintext highlighter-rouge">in-context learning</code> abilities.<br/> In contrast, the hybrid model closely matches or exceeds the Transformer on all standard and long-context tasks and is predicted to be up to 8x faster when generating tokens at inference time.</li> </ul> <h3 id="is-attention-all-we-need">Is Attention All We Need?</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/4-480.webp 480w,/assets/img/2024-07-18-SSM/4-800.webp 800w,/assets/img/2024-07-18-SSM/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Transformer : <ul> <li>fast training due to parallelization</li> <li>slow inference for long sequence(context) <ul> <li>key-value cache can improve speed, but increase GPU memory</li> </ul> </li> </ul> </li> <li>RNN : <ul> <li>slow training due to no parallelization</li> <li>fast inference because scale linearly with sequence length</li> </ul> </li> <li>Mamba : <ul> <li>fast training</li> <li>fast inference because scale linearly with sequence length and can deal with unbounded context</li> </ul> </li> <li> <p>SSM or RNN :<br/> state = fixed-sized vector (compression)<br/> high efficiency, but low performance</p> </li> <li>Transformer :<br/> cache of entire history (no compression)<br/> high performance, but low efficiency</li> </ul> <h3 id="mamba-linear-time-sequence-modeling-with-selective-state-spaces">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</h3> <ul> <li>SSM</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/1-480.webp 480w,/assets/img/2024-07-18-SSM/1-800.webp 800w,/assets/img/2024-07-18-SSM/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Selective SSM :<br/> matrix B, C and step size are dependent on the input</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/2-480.webp 480w,/assets/img/2024-07-18-SSM/2-800.webp 800w,/assets/img/2024-07-18-SSM/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Parallel scan :<br/> The order does not matter through the associative property, so can calculate sequences in part and iteratively combine them</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/3-480.webp 480w,/assets/img/2024-07-18-SSM/3-800.webp 800w,/assets/img/2024-07-18-SSM/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Hardware-aware implementation :<br/> minimize copying between RAMs</li> </ul> <h3 id="mamba-2">Mamba-2</h3> <ul> <li>Mamba에서 Main Bottleneck이 Parallel scan 부분이었는데,<br/> Mamba-2는 divide input into chunks 등 architecture 개선으로 이를 해결하고자 했음</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/13-480.webp 480w,/assets/img/2024-07-18-SSM/13-800.webp 800w,/assets/img/2024-07-18-SSM/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="limitations-of-mamba">Limitations of Mamba</h3> <ul> <li>Poor at MMLU and Phonebook task<br/> 아래를 요구하는 task에 대해서는 Mamba가 잘 못함 <ul> <li>in-context learning</li> <li>info. routing between tokens</li> <li>copying from the context (bad on long-context tasks)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/5-480.webp 480w,/assets/img/2024-07-18-SSM/5-800.webp 800w,/assets/img/2024-07-18-SSM/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/6-480.webp 480w,/assets/img/2024-07-18-SSM/6-800.webp 800w,/assets/img/2024-07-18-SSM/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="hybrid-architecture-of-mamba-and-transformer">Hybrid Architecture of Mamba and Transformer</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/7-480.webp 480w,/assets/img/2024-07-18-SSM/7-800.webp 800w,/assets/img/2024-07-18-SSM/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Our Hybrid Mamba-Transformer Model <ul> <li>Minimize the number of Attention Layers and Maximize the number of MLPs</li> <li>Does not necessarily need Rotary Position Embedding (RoPE)</li> <li>evenly spread attention and MLP layers</li> <li>Place Mamba layer at the beginning, so has no position embedding</li> <li>Group-Query Attention (GQA) makes more efficient</li> <li>Global Attention makes better performance</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/11-480.webp 480w,/assets/img/2024-07-18-SSM/11-800.webp 800w,/assets/img/2024-07-18-SSM/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Mamba-2 Hybrid<br/> Inference Speed is fast<br/> Now, states in Mamba can understand longer history!</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/12-480.webp 480w,/assets/img/2024-07-18-SSM/12-800.webp 800w,/assets/img/2024-07-18-SSM/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Attention Layer is bottleneck at Hybrid model,<br/> so Context Length가 길어질수록 Speedup 증가율은 줄어듬</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/8-480.webp 480w,/assets/img/2024-07-18-SSM/8-800.webp 800w,/assets/img/2024-07-18-SSM/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/9-480.webp 480w,/assets/img/2024-07-18-SSM/9-800.webp 800w,/assets/img/2024-07-18-SSM/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="summary">Summary</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/10-480.webp 480w,/assets/img/2024-07-18-SSM/10-800.webp 800w,/assets/img/2024-07-18-SSM/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 왼쪽부터 4K, 16K, 32K-based models </div> <p>Mamba-2 Hybrid는 Transformer와 달리 Quadratic calculation까지 필요 없고 inference 빠름<br/> but, Attention Layer가 Bottleneck이듯이 해결해야 할 사항들이 남아 있어 앞으로도 발전 가능성 있음</p>]]></content><author><name></name></author><category term="cv-tasks"/><category term="SSM"/><category term="Mamba"/><summary type="html"><![CDATA[SSM]]></summary></entry><entry><title type="html">3D Gaussian Splatting</title><link href="https://semyeong-yu.github.io/blog/2024/GS/" rel="alternate" type="text/html" title="3D Gaussian Splatting"/><published>2024-07-11T10:00:00+00:00</published><updated>2024-07-11T10:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/GS</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/GS/"><![CDATA[<h2 id="3d-gaussian-splatting-for-real-time-radiance-field-rendering">3D Gaussian Splatting for Real-Time Radiance Field Rendering</h2> <h4 id="bernhard-kerbl-georgios-kopanas-thomas-leimkühler-george-drettakis">Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, George Drettakis</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2308.04079">https://arxiv.org/abs/2308.04079</a><br/> project website :<br/> <a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/</a><br/> code :<br/> <a href="https://github.com/graphdeco-inria/gaussian-splatting">https://github.com/graphdeco-inria/gaussian-splatting</a><br/> referenced blog :<br/> <a href="https://xoft.tistory.com/51">https://xoft.tistory.com/51</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>DD</li> </ol> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/2-480.webp 480w,/assets/img/2024-07-11-GS/2-800.webp 800w,/assets/img/2024-07-11-GS/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="abstract">Abstract</h2> <ul> <li>novel 3D Gaussian scene representation with real-time differentiable renderer<br/> <code class="language-plaintext highlighter-rouge">수많은 3D Gaussian이 모여 scene을 구성</code>하고 있다!</li> <li>Very Fast rendering (\(\geq\) 100 FPS) :<br/> real-time as \(\geq\) 30 FPS<br/> rasterization이 optimization의 main bottleneck인데, 3DGS는 fast rasterization 가짐</li> <li>Higher Quality than SOTA Mip-NeRF360(2022)</li> <li>Faster Training than SOTA InstantNGP(2022)</li> </ul> <h2 id="introduction">Introduction</h2> <h3 id="why-3d-gaussian">Why 3D Gaussian?</h3> <p>3D scene representation 방법</p> <ol> <li><code class="language-plaintext highlighter-rouge">Mesh or Point</code> <ul> <li>explicit</li> <li>good for fast GPU/CUDA-based rasterization(3D \(\rightarrow\) 2D)</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">NeRF</code> method <ul> <li>implicit (MLP로 geometry 및 appearance를 표현)</li> <li>ray marching</li> <li>continuous coordinate-based representation</li> <li>interpolate values stored in voxels, hash grids, or points</li> <li>But,,, continuous ray로부터 discrete points를 뽑아 내는 <code class="language-plaintext highlighter-rouge">stochastic sampling</code> for rendering 때문에 <code class="language-plaintext highlighter-rouge">연산량이 많고 noise</code> 생김</li> <li>MLP는 dot product 및 더하기(kernel regression)의 특성상 <code class="language-plaintext highlighter-rouge">orthogonality</code>를 흐리기 때문에 high-freq. output을 잘 표현할 수 없어서 따로 미리 positional encoding을 수행</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">3D Gaussian</code> method <ul> <li>explicit (3D Gaussian으로 geometry를, SH coeff.로 appearance를 표현)</li> <li>differentiable volumetric representation</li> <li>efficient rasterization(projection and \(\alpha\)-blending)</li> <li>3D Gaussian(ellipsoid)이나 SH coeff.라는 explicit 표현 자체가 <code class="language-plaintext highlighter-rouge">orthogonality</code>를 잘 살리기 때문에 high-freq. output 잘 표현 가능</li> </ul> </li> </ol> <h3 id="rendering-nerf-vs-3dgs">Rendering (NeRF vs 3DGS)</h3> <ul> <li>NeRF : <ul> <li>ray per pixel 쏴서 coarse(stratified) and fine(PDF) sampling하고,</li> <li>MLP로 sampled points의 color 및 volume density를 구하고,</li> <li>이 값들을 volume rendering 식으로 summation</li> </ul> </li> <li>3DGS : <ul> <li>image를 tile(14 \(\times\) 14 pixel)들로 나누고,</li> <li>tile마다 Gaussian을 Depth에 따라 정렬한 뒤</li> <li>앞에서부터 뒤로 \(\alpha\)-blending</li> </ul> </li> </ul> <h2 id="related-work">Related Work</h2> <p>생략 (추후에 다시 볼 수도)</p> <h2 id="overview">Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/1-480.webp 480w,/assets/img/2024-07-11-GS/1-800.webp 800w,/assets/img/2024-07-11-GS/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For unbounded and complete scenes,<br/> For 1080p high resolution and real-time(\(\geq\) 30 fps) rendering,</p> <ol> <li><code class="language-plaintext highlighter-rouge">input</code> : <ul> <li>Most point-based methods require <code class="language-plaintext highlighter-rouge">MVS</code>(Multi-View Stereo) data,<br/> but 3DGS only needs <code class="language-plaintext highlighter-rouge">SfM points</code> for initialization</li> <li>COLMAP 등 SfM(Structure-from-Motion) camera calibration으로 얻은 <code class="language-plaintext highlighter-rouge">sparse point cloud</code>에서 시작해서<br/> scene을 3D Gaussians로 나타냄으로써<br/> <code class="language-plaintext highlighter-rouge">empty space에서의 불필요한 계산을 하지 않도록</code> continuous volumetric radiance fields 정보를 저장</li> <li>NeRF-synthetic dataset의 경우 3DGS 는 random initialization으로도 좋은 퀄리티 달성</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">optimization</code> interleaved with <code class="language-plaintext highlighter-rouge">adaptive density control</code> : <ul> <li>optimize 4 parameters :<br/> 3D position(mean), anisotropic covariance, opacity, and spherical harmonic coeff.(color)<br/> <code class="language-plaintext highlighter-rouge">highly anisotropic volumetric splats</code>는 <code class="language-plaintext highlighter-rouge">fine structures</code>를 compact하게 나타낼 수 있음!!<br/> <code class="language-plaintext highlighter-rouge">spherical harmonics</code>를 통해 <code class="language-plaintext highlighter-rouge">directional appearance(color)</code>를 잘 나타낼 수 있음!!<d-cite key="Plenoxels">[1]</d-cite>, <d-cite key="InstantNGP">[2]</d-cite></li> <li>adaptive density control :<br/> gradient 기반으로 Gaussian 형태를 변화시키기 위해, add and occasionally remove 3D Gaussians during optimization</li> </ul> </li> <li>differentiable visibility-aware <code class="language-plaintext highlighter-rouge">real-time rendering</code> :<br/> perform \(\alpha\)-blending of <code class="language-plaintext highlighter-rouge">anisotropic splats</code> respecting visibility order<br/> by fast <code class="language-plaintext highlighter-rouge">GPU sorting</code> algorithm and <code class="language-plaintext highlighter-rouge">tile-based rasterization</code>(projection and \(\alpha\)-blending)<br/> 한편, accumulated \(\alpha\) values를 tracking함으로써 <code class="language-plaintext highlighter-rouge">Gaussians 수에 제약 없이</code> 빠른 backward pass도 가능</li> </ol> <hr/> <h3 id="pseudo-code">Pseudo-Code</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/3-480.webp 480w,/assets/img/2024-07-11-GS/3-800.webp 800w,/assets/img/2024-07-11-GS/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>빨간 박스 : initialization<br/> 파란 박스 : optimization<br/> 초록 박스 : 특정 iter.마다 Gaussian을 clone, split, remove</p> <h2 id="differentiable-3d-gaussian-splatting">Differentiable 3D Gaussian Splatting</h2> <h3 id="3d-gaussian">3D Gaussian</h3> <ul> <li> <p><code class="language-plaintext highlighter-rouge">differentiable</code> volumetric representation의 특성을 가지고 있으면서도 빠른 rendering을 위해 <code class="language-plaintext highlighter-rouge">unstructured and explicit</code>한 게 무엇이 있을까?<br/> \(\rightarrow\) 3D Gaussian !!</p> </li> <li> <p>a point를 a small planar circle with a normal이라고 가정하는 이전 Point-based rendering 논문들 <d-cite key="Point1">[3]</d-cite> <d-cite key="Point2">[4]</d-cite> 과 달리<br/> <code class="language-plaintext highlighter-rouge">SfM points는 sparse해서 normals(법선)를 estimate하기 어려울</code> 뿐만 아니라, estimate 한다 해도 very noisy normals를 optimize하는 것은 매우 어렵<br/> \(\rightarrow\) normals 필요 없는 3D Gaussians !!<br/> k-dim. Gaussian : \(G(\boldsymbol x) = (2\pi)^{-\frac{k}{2}}det(\Sigma)^{-\frac{1}{2}}e^{-\frac{1}{2}(\boldsymbol x - \boldsymbol \mu)^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu)}\)</p> </li> </ul> <h2 id="parameters-to-train">Parameters to train</h2> <ol> <li><code class="language-plaintext highlighter-rouge">scale vector</code> \(s\) and <code class="language-plaintext highlighter-rouge">quaternion</code> \(q\) for <code class="language-plaintext highlighter-rouge">covariance matrix</code></li> <li><code class="language-plaintext highlighter-rouge">spherical harmonics</code>(SH) coeff. for <code class="language-plaintext highlighter-rouge">color</code></li> <li><code class="language-plaintext highlighter-rouge">opacity</code> \(\alpha\)</li> <li><code class="language-plaintext highlighter-rouge">3D position</code> for <code class="language-plaintext highlighter-rouge">mean</code></li> </ol> <h3 id="parameter-1-covariance-matrix">Parameter 1. Covariance matrix</h3> <blockquote> <p>covariance matrix and scale vector(scale) and quaternion(rotation)</p> </blockquote> <ul> <li>covariance matrix는 positive semi-definite \(x^T M x \geq 0\) for all \(x \in R^n\)이어야만 physical meaning을 가지는데,<br/> \(\Sigma\) 를 직접 바로 optimize하면 invalid covariance matrix가 될 수 있음<br/> 그렇다면!!</li> </ul> <p>\(\Sigma\) 가 <code class="language-plaintext highlighter-rouge">symmetric</code> and <code class="language-plaintext highlighter-rouge">positive semi-definite</code>이도록 \(\Sigma = R S S^T R^T\) 로 정의해서<br/> \(\Sigma\) 대신 <code class="language-plaintext highlighter-rouge">x,y,z-axis scale</code>을 나타내는 <code class="language-plaintext highlighter-rouge">3D vector</code> \(s\) 와 <code class="language-plaintext highlighter-rouge">rotation</code>을 나타내는 <code class="language-plaintext highlighter-rouge">4D quaternion</code> \(q\) 를 optimize 하자!!<br/> quaternion에 대한 설명은 <a href="https://semyeong-yu.github.io/blog/2024/Quaternion">Quaternion</a> 블로그 참고!!</p> <ul> <li><code class="language-plaintext highlighter-rouge">scale</code> 3D vector \(s\) <code class="language-plaintext highlighter-rouge">초기값</code> :<br/> <a href="https://github.com/graphdeco-inria/gaussian-splatting/blob/b2ada78a779ba0455dfdc2b718bdf1726b05a1b6/scene/gaussian_model.py#L134C1-L134C1">GaussianModel().create_from_pcd()</a><br/> SfM sparse point cloud의 각 점에 대해 가장 가까운 점 3개까지의 거리의 평균을 각 axis(\(x, y, z\))별로 구한 것을 3 \(\times\) 1 \(s\)라 할 때<br/> normalize 효과를 위해 log, sqrt 씌운 뒤<br/> 3 \(\times\) 1 \(log(\sqrt{s})\) 의 값을 3번 복사하여 3 \(\times\) 3 scale matrix \(S\)를 초기화 <pre><code class="language-Python">dist2 = torch.clamp_min(distCUDA2(torch.from_numpy(np.asarray(pcd.points)).float().cuda()), 0.0000001)
scales = torch.log(torch.sqrt(dist2))[...,None].repeat(1, 3)
</code></pre> </li> <li> <p><code class="language-plaintext highlighter-rouge">scale</code> 3D vector \(s\) <code class="language-plaintext highlighter-rouge">activation function</code> :<br/> smooth gradient 얻기 위해 exponential activation function을 씌움</p> </li> <li><code class="language-plaintext highlighter-rouge">quaternion</code> \(q\) <code class="language-plaintext highlighter-rouge">초기값</code> :<br/> 각 점에 대해 \(\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}\) 으로 quaternion을 초기화하고<br/> 이를 이용하여 rotation matrix \(R\) 초기화 <pre><code class="language-Python">rots = torch.zeros((fused_point_cloud.shape[0], 4), device="cuda")
rots[:, 0] = 1
</code></pre> </li> <li><code class="language-plaintext highlighter-rouge">anisotropic covariance</code>는 다양한 모양의 geometry를 나타내기 위해 optimize하기에 적합!</li> </ul> <blockquote> <p>param. gradient 직접 유도</p> </blockquote> <p>training할 때 automatic differentiation으로 인한 <code class="language-plaintext highlighter-rouge">overhead를 방지</code>하기 위해 <code class="language-plaintext highlighter-rouge">param. gradient를 직접 유도</code>함!</p> <p>Appendix A. <code class="language-plaintext highlighter-rouge">?????</code></p> <blockquote> <p>EWA volume splatting (2001) :<br/> world-to-camera 는 linear transformation 이지만,<br/> <code class="language-plaintext highlighter-rouge">camera-to-image (projection)</code> 는 <code class="language-plaintext highlighter-rouge">non-linear transformation</code> 이다!!</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/4-480.webp 480w,/assets/img/2024-07-11-GS/4-800.webp 800w,/assets/img/2024-07-11-GS/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 위 그림 : camera coordinate / 아래 그림 : image coordinate (ray space) </div> <ul> <li><code class="language-plaintext highlighter-rouge">world</code> coordinate (3D) : <ul> <li> \[\boldsymbol u = \begin{bmatrix} u_0 \\ u_1 \\ u_2 \end{bmatrix}\] </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">camera</code> coordinate (3D) : <ul> <li>\(\boldsymbol t = \begin{bmatrix} t_0 \\ t_1 \\ t_2 \end{bmatrix}\)<br/> \(= W \boldsymbol u + d\)<br/> where \(W\) : <code class="language-plaintext highlighter-rouge">viewing transformation</code> affine matrix from world coordinate to camera coordinate</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">image</code> coordinate (2D) : <ul> <li>\(\boldsymbol x = \begin{bmatrix} x_0 \\ x_1 \\ x_2 \end{bmatrix}\)<br/> \(= \phi(\boldsymbol t) = \begin{bmatrix} \frac{t_0}{t_2} \\ \frac{t_1}{t_2} \\ \| (t_0, t_1, t_2)^T \| \end{bmatrix}\)</li> <li>function \(\phi\) 는 non-linear하므로 Affine transformation이 불가능하다.</li> <li><code class="language-plaintext highlighter-rouge">Local Affine (Linear) transform으로 Approx.</code>하기 위해 \(\boldsymbol t = \boldsymbol t_{k}\) 에서의 <code class="language-plaintext highlighter-rouge">Taylor Approx.</code>를 이용하면,<br/> \(\phi_{k}(\boldsymbol t) = \phi(\boldsymbol t_{k}) + \boldsymbol J_{k} \cdot (\boldsymbol t - \boldsymbol t_{k})\)<br/> where<br/> \(\boldsymbol J_{k} = \frac{d\phi}{d \boldsymbol t}(\boldsymbol t_{k}) = \begin{bmatrix} \frac{d\phi}{d \boldsymbol t_{0}}(\boldsymbol t_{k}) &amp; \frac{d\phi}{d \boldsymbol t_{1}}(\boldsymbol t_{k}) &amp; \frac{d\phi}{d \boldsymbol t_{2}}(\boldsymbol t_{k}) \end{bmatrix} = \begin{bmatrix} \frac{1}{t_{k, 2}} &amp; 0 &amp; -\frac{t_{k, 0}}{t_{k, 2}^2} \\ 0 &amp; \frac{1}{t_{k, 2}} &amp; -\frac{t_{k, 1}}{t_{k, 2}^2} \\ \frac{t_{k, 0}}{l} &amp; \frac{t_{k, 1}}{l} &amp; \frac{t_{k, 2}}{l} \end{bmatrix}\)<br/> and ray distance \(l = \| (t_{k, 0}, t_{k, 1}, t_{k, 2})^T \|\)<br/> Here, \(J\) : <code class="language-plaintext highlighter-rouge">Jacobian</code>(각 axis로 편미분한 matrix) of the <code class="language-plaintext highlighter-rouge">affine approx.</code> of the <code class="language-plaintext highlighter-rouge">projective transformation</code> from camera coordinate to image coordinate</li> <li>즉, camera coordinate에서 임의의 좌표 \(\boldsymbol t_{k}\) 주변에 존재하는 입력 좌표 \(\boldsymbol t\)에 대해서는 image coordinate으로의 affine(linear) transformation이 충족된다.</li> <li>Gaussian Splatting 논문의 경우 <code class="language-plaintext highlighter-rouge">Gaussian의 중심점</code>을 \(\boldsymbol t_{k}\) 로 두면 그 주변의 \(\boldsymbol t\)에 대해서는 Jacobian을 이용한 affine(linear) transformation 가능!</li> </ul> </li> </ul> <blockquote> <p><code class="language-plaintext highlighter-rouge">Projection</code> of 3D Gaussian <code class="language-plaintext highlighter-rouge">covariance</code> to 2D</p> </blockquote> <ul> <li> <p><code class="language-plaintext highlighter-rouge">world coordinate</code> :<br/> \(\Sigma\) : 3 \(\times\) 3 covariance matrix of 3D Gaussian</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">image coordiante</code> (z=1) :<br/> \(\Sigma^{\ast} = J W \Sigma W^T J^T\) : covariance matrix of 2D splat</p> <ul> <li>Step 1. world-to-camera (<code class="language-plaintext highlighter-rouge">affine</code>) :<br/> \(\boldsymbol u \rightarrow W \boldsymbol u + d\)</li> <li>Step 2. camera-to-image (<code class="language-plaintext highlighter-rouge">local affine approx.</code>) :<br/> Projection<br/> \(W \boldsymbol u + d \rightarrow \phi_{k}(W \boldsymbol u + d) = x_k + \boldsymbol J_{k} W \boldsymbol u + \boldsymbol J_{k} (d - \boldsymbol t_{k})\)<br/> 상수 부분을 제외하면 \(\boldsymbol x = \boldsymbol J_{k} W \boldsymbol u\)</li> <li>Step 3. covariance 특성 :<br/> \(Cov[Ax] = E[(Ax - E[Ax])(Ax - E[Ax])^T]\)<br/> \(= E[A(x - E[x])(x - E[x])^TA^T] = A Cov[x] A^T\)</li> <li>Step 4. <code class="language-plaintext highlighter-rouge">world-to-image covariance</code> :<br/> \(\boldsymbol u \rightarrow \boldsymbol J_{k} W \boldsymbol u\) 이므로<br/> \(\Sigma \rightarrow \boldsymbol J \boldsymbol W \Sigma \boldsymbol W^T \boldsymbol J^T\)</li> <li>Step 5. <code class="language-plaintext highlighter-rouge">covariance dimension reduction</code> :<br/> 추가로, \(\boldsymbol J \boldsymbol W \Sigma \boldsymbol W^T \boldsymbol J^T\) 로 계산한 \(\Sigma^{\ast}\) 는 3-by-3 matrix 인데,<br/> 3D Gaussian을 한쪽 축으로 적분하면 2D Gaussian과 동일한 값을 가지게 되므로<br/> 3-by-3 covariance matrix의 3번째 행과 열의 값을 버린<br/> 2-by-2 matrix를 projected 2D covariance matrix 로 사용하면 됨!</li> </ul> </li> </ul> <h3 id="parameter-2-spherical-harmonicssh-coeff">Parameter 2. Spherical Harmonics(SH) coeff.</h3> <ul> <li><code class="language-plaintext highlighter-rouge">Spherical Harmonics</code> (SH) :<br/> spherical coordinate 에서 <code class="language-plaintext highlighter-rouge">각도</code> (\(\theta, \phi\))를 입력받아 <code class="language-plaintext highlighter-rouge">구의 표면 위치에서의 값</code>을 출력하는 함수<br/> spherical coordinate 에서 라플라스 방정식을 풀면 아래 수식과 같음</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/5-480.webp 480w,/assets/img/2024-07-11-GS/5-800.webp 800w,/assets/img/2024-07-11-GS/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/6-480.webp 480w,/assets/img/2024-07-11-GS/6-800.webp 800w,/assets/img/2024-07-11-GS/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/7-480.webp 480w,/assets/img/2024-07-11-GS/7-800.webp 800w,/assets/img/2024-07-11-GS/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> l이 같은 함수들은 same band l에 있다고 말함 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/8-480.webp 480w,/assets/img/2024-07-11-GS/8-800.webp 800w,/assets/img/2024-07-11-GS/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 가로축 : theta, 세로축 : phi, 채도 : SH magnitude, 색상 : SH phase </div> <ul> <li> <p>SH coeff. <code class="language-plaintext highlighter-rouge">초기값</code> :<br/> 0-band SH (\(\theta, \phi\) 와 관계없는 view-independent color) 의 경우 SfM으로 얻은 point cloud의 RGB color값과 RGB2SH 이용하여 초기화<br/> 다른 band의 경우 0으로 초기화</p> </li> <li>SH 의 역할 : <ul> <li>SH에서 band 수를 제한해서 쓴다는 것은 높은 band (high freq. 또는 detail info.)는 자른다는 의미이므로 <code class="language-plaintext highlighter-rouge">smoothing</code> 역할</li> <li>적은 비용(coeff. 몇 개만 사용)으로 SH function을 <code class="language-plaintext highlighter-rouge">approx.</code></li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">SH coeff.</code>로 <code class="language-plaintext highlighter-rouge">color</code> 나타내는 법 :<br/> Fourier Series 에서처럼,<br/> SH coeff. \(k_{l}^{m}\) 의 optimal 값을 구해서<br/> \(k_{l}^{m}\) 와 \(Y_l^m(\theta, \phi)\) 의 weighted sum!<br/> \(C = \Sigma_{l=0}^{l_{max}} \Sigma_{m=-l}^{l} k_l^m Y_l^m(\theta, \phi)\)<br/> 즉, <code class="language-plaintext highlighter-rouge">trainable parameter</code> : SH coeff.인 \(k_{l}^{m}\)<br/> (<code class="language-plaintext highlighter-rouge">light source</code>마다 SH coeff. \(k_{l}^{m}\) 다르므로 find optimal value)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/9-480.webp 480w,/assets/img/2024-07-11-GS/9-800.webp 800w,/assets/img/2024-07-11-GS/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="parameter-3-opacity">Parameter 3. opacity</h3> <ul> <li> <p>opacity \(\alpha\) <code class="language-plaintext highlighter-rouge">초기값</code> :<br/> 임의의 실수값으로 초기화<br/> inverse_sigmoid(0.1 * torch.ones(…))</p> </li> <li> <p>opacity \(\alpha\) <code class="language-plaintext highlighter-rouge">range</code> :<br/> \(\alpha \in [0, 1)\) 이므로<br/> 마지막에 sigmoid activation function을 씌워서 smooth gradient를 얻음</p> </li> </ul> <h3 id="parameter-4-3d-positionmean">Parameter 4. 3D position(mean)</h3> <h2 id="fast-differentiable-rasterizer-for-gaussians">Fast Differentiable Rasterizer for Gaussians</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/10-480.webp 480w,/assets/img/2024-07-11-GS/10-800.webp 800w,/assets/img/2024-07-11-GS/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Tile Rasterizer</p> </blockquote> <ul> <li> <p>기능 : 3D Gaussians로 구성된 3D model을 특정 camera pose에 대해 2D rendering</p> </li> <li><code class="language-plaintext highlighter-rouge">input</code> : <ul> <li>image의 rendering할 width, height</li> <li>3D Gaussian의 xyz-mean, covariance in world-coordinate</li> <li>3D Gaussian의 color, opacity</li> <li>current camera pose</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Frustum Culling</code> :<br/> 주어진 camera pose에서 view frustum을 그려서<br/> view frustum과 교차하는 확률이 99% confidence interval 범위 밖에 있는 3D Gaussians는 제거(culling)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/16-480.webp 480w,/assets/img/2024-07-11-GS/16-800.webp 800w,/assets/img/2024-07-11-GS/16-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Guard Band</code> :<br/> 아래의 경우 projected 2D covariance 계산이 불안정하기 때문에 개별적으로 제거 <ul> <li><code class="language-plaintext highlighter-rouge">view frustum의 near plane에 가까이 있는</code> Gaussian의 경우,<br/> EWA Volume Splatting에서 언급된 cam-to-img projection <code class="language-plaintext highlighter-rouge">nonlinearity</code>가 심하기 때문에<br/> projection matrix를 Jacobian으로 approx.한 값에 더 큰 artifact가 생김</li> <li>view frustum 밖에 멀리 떨어진 경우 <code class="language-plaintext highlighter-rouge">?????</code><br/> 코드에서는 이 경우는 빼버렸음 (주석 처리)<br/> (diff-gaussian-rasterization/cuda_rasterizer/auxiliary.h)</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Create Tiles</code> :<br/> <code class="language-plaintext highlighter-rouge">CUDA 병렬 처리</code>를 위해<br/> \(w \times h\)의 image를 \(16 \times 16\) pixel의 tiles로 쪼갬</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/11-480.webp 480w,/assets/img/2024-07-11-GS/11-800.webp 800w,/assets/img/2024-07-11-GS/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p><code class="language-plaintext highlighter-rouge">Parallelism</code> :<br/> <code class="language-plaintext highlighter-rouge">tile마다</code> 개별 <code class="language-plaintext highlighter-rouge">CUDA thread</code> block으로 실행하여<br/> forward/backward processing, data loading/sharing을 병렬처리<br/> (여러 threads가 Gaussian points를 shared memory에 collaboratively load)<br/> (VRAM과 DRAM 사이의 이동은 overhead 발생하기 때문에 <code class="language-plaintext highlighter-rouge">VRAM</code>에서 모두 처리해버릴 수 있도록 <code class="language-plaintext highlighter-rouge">CUDA Functions</code>(.cu)를 직접 짬!)</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Duplicate with Keys</code> :</p> <ul> <li><code class="language-plaintext highlighter-rouge">view-space-depth</code>와 <code class="language-plaintext highlighter-rouge">tile-ID</code>를 이용하여 tile마다 각 Gaussian의 key를 생성<br/> tile-ID 쪽이 MSB<br/> view-space-depth 쪽이 LSB<br/> 각 Gaussian의 value는 Gaussian’s index</li> <li><code class="language-plaintext highlighter-rouge">CUDA 병렬처리</code> 덕분에 2D Gaussian 하나가 3개의 tiles에 걸쳐 있다면, 3개의 2D Gaussians로 복제(<code class="language-plaintext highlighter-rouge">instance화</code>)되는 것처럼 작동</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/12-480.webp 480w,/assets/img/2024-07-11-GS/12-800.webp 800w,/assets/img/2024-07-11-GS/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Sort by Keys</code> : <ul> <li>tile마다 Depth 기준으로 <code class="language-plaintext highlighter-rouge">Radix Sort</code></li> <li>여기서 한 번 sort 하고 나면 끝!! 추가로 per-pixel sorting 할 필요 없음</li> <li>pixel-wise sorting이 아니라 Gaussians sort라서 \(\alpha\)-blending approx.이긴 한데, <code class="language-plaintext highlighter-rouge">splats가 각 pixel size 정도가 되기 때문에</code> 해당 approx. 오차는 무시 가능!</li> <li>쨌든 이 덕분에 visible artifacts 없이 training, rendering performance 베리베리 굳</li> </ul> </li> </ul> <pre><code class="language-Python">from collections import deque
# 양방향에서 삽입/삭제 가능한 queue형 자료구조

# 1의 자릿수 기준으로 정렬한 뒤
# 10의 자릿수 기준으로 정렬한 뒤
# ...
def radixSort():
    nums = list(map(int, input().split(' ')))
    buckets = [deque() for _ in range(10)] # 각 자릿수(0~9)에 대응되는 10개의 empty deque()
    
    max_val = max(nums)
    queue = deque(nums) # 정렬할 숫자들
    digit = 1 # 정렬 기준이 되는 자릿수
    
    while (max_val &gt;= digit): # 가장 큰 수의 자릿수일 때까지만 실행
        while queue:
            num = queue.popleft() # 정렬할 숫자
            buckets[(num // digit) % 10].append(num) # 각 자릿수(0~9)에 따라 buckets에 num을 넣는다.
        
        # 해당 정렬 기준 자릿수에서 buckets에 다 넣었으면, buckets에 담겨있는 순서대로 꺼내와서 정렬한다.
        for bucket in buckets:
            while bucket:
                queue.append(bucket.popleft())

        digit *= 10 # 정렬 기준이 되는 자릿수 증가시키기
    
    print(list(queue))
</code></pre> <ul> <li> <p><code class="language-plaintext highlighter-rouge">Identify Tile Ranges</code> :<br/> tile별 Gaussian list를 효율적으로 관리하기 위해<br/> tile마다 Gaussian list 범위 식별</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Get Tile Ranges</code> :<br/> 모든 tile에 대해 Gaussian list 범위 읽어옴</p> </li> <li>\(\alpha\)-Blending in Order (<code class="language-plaintext highlighter-rouge">forward process</code>) : <ul> <li>tile별 CUDA 병렬처리에 의해 각 pixel에 대해 <code class="language-plaintext highlighter-rouge">color</code> 및 <code class="language-plaintext highlighter-rouge">opacity</code> \(\alpha\) 값을 Gaussian list의 <code class="language-plaintext highlighter-rouge">앞에서 뒤로</code> accumulate</li> <li>i-th tile에 있는 pixels 중 a pixel’s accumulated opacity 값이 target saturation threshold를 넘어서면, 해당 i-th thread STOP (유일한 STOP 조건)</li> <li><code class="language-plaintext highlighter-rouge">Gaussian의 개수를 제한하지 않음</code>으로써 scene-specific hyper-param. tuning 없이 arbitrary depth complexity를 가지는 scene을 커버 가능<br/> (GPU Radix Sort 덕분에 parallelism(병렬) 및 amortized(분할상환) 가능하여 Gaussian 개수 늘릴 수 있었음)</li> <li><code class="language-plaintext highlighter-rouge">기존 기법들은 pixel마다 정렬이 필요</code>해서 inefficient했지만<br/> 본 논문은 tile별 CUDA 병렬처리 덕분에 efficient<br/> (e.g. NeRF : ray per pixel 쏴서 accumulate per pixel for volume rendering <code class="language-plaintext highlighter-rouge">???</code>)</li> </ul> <p>\(c = \alpha_{1}c_{1} + (1 - \alpha_{1})(\alpha_{2}c_{2} + (1 - \alpha_{2})(\cdots)) = \alpha_{1}c_{1} + (1 - \alpha_{1})\alpha_{2}c_{2} + (1 - \alpha_{1})(1 - \alpha_{2})(\cdots) = \cdots = \sum_{i=1}^{N}(\alpha_{i}c_{i}\prod_{j=1}^{i-1}(1-\alpha_{j}))\) where \(\alpha_{0} = 0\)</p> </li> <li><code class="language-plaintext highlighter-rouge">Backward process</code> : <ul> <li>각 tile의 Gaussian list에 대해 Gaussian의 <code class="language-plaintext highlighter-rouge">opacity 비율에 따라</code> <code class="language-plaintext highlighter-rouge">뒤에서 앞으로</code> gradient update</li> <li>직접 backward gradient update 식을 구해서 이용</li> <li>backward process를 위해 <d-cite key="Point1">[3]</d-cite>처럼 <code class="language-plaintext highlighter-rouge">pixel마다</code> global memory에 blended points list를 저장할 수도 있지만<br/> dynamic memory management overhead가 생기기 때문에<br/> forward process에서 <code class="language-plaintext highlighter-rouge">tile마다</code> 구했던 range 및 sorted Gaussian list를 <code class="language-plaintext highlighter-rouge">재사용</code></li> <li>\(\alpha\)-blending으로 합쳤던 각 Gaussian으로 gradient backpropagation을 해주려면<br/> \(\alpha\)-blending 각 step에서의 accumulated opacity 값이 필요한데,<br/> 이를 따로 list에 저장해두고 훑는 게 아니라,<br/> \(\alpha\)-blending을 할 때 그때그때 각 Gaussian point에 지금까지의 accumulated opacity 값인 \(\alpha_{l}\)을 저장해두고,<br/> \(\alpha\)-blending final step에서의 total accumulated opacity 값 \(\alpha_{N}\)만 backward process에 넘겨 주면<br/> backward process할 때 \(\alpha_{N}\)를 \(\alpha_{l}\)로 나눈 값을 gradient 계산에 사용 <code class="language-plaintext highlighter-rouge">?????</code></li> <li>0으로 나눠지는 경우를 방지하기 위해 \(\alpha\) 값이 \(\frac{1}{255}\)보다 작다면 blending update 안 함</li> <li>rasterization 중에 blending 값이 0.9999를 초과하기 전에 STOP<br/> <code class="language-plaintext highlighter-rouge">?????</code></li> </ul> </li> <li> <p>Primitives :<br/> 본 논문의 Gaussians는 <code class="language-plaintext highlighter-rouge">Euclidean space</code>에 <code class="language-plaintext highlighter-rouge">primitives</code>를 남김 <code class="language-plaintext highlighter-rouge">?????</code><br/> \(\rightarrow\) <d-cite key="Plenoxels">[1]</d-cite>, <d-cite key="MipNeRF360">[10]</d-cite>과 달리 distant or large Gaussians 처리를 위해 space compaction, warping, or projection 할 필요가 없음 <code class="language-plaintext highlighter-rouge">?????</code></p> </li> <li>Efficient Rasterization : <ul> <li>Pulsar 논문<d-cite key="Pulsar">[5]</d-cite> 에서처럼<br/> an entire image에 대해 가장 작은 원소(<code class="language-plaintext highlighter-rouge">primitives</code>)를 미리 정렬(<code class="language-plaintext highlighter-rouge">pre-sort</code>)하여 <code class="language-plaintext highlighter-rouge">primitives = Gaussians ?????</code><br/> pixel-wise sorting 비용을 절감</li> <li>differentiable</li> <li>arbitrary number of Gaussians에 대해 backpropagation 가능<br/> with low additional memory : O(1) per pixel</li> <li>2D projection 가능</li> </ul> </li> </ul> <h2 id="optimization-with-adaptive-density-control-of-3d-gaussians">Optimization with Adaptive Density Control of 3D Gaussians</h2> <h3 id="optimization">Optimization</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/13-480.webp 480w,/assets/img/2024-07-11-GS/13-800.webp 800w,/assets/img/2024-07-11-GS/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Loss :<br/> predicted image와 GT image를 비교하는<br/> <code class="language-plaintext highlighter-rouge">L1 loss</code> 및 <code class="language-plaintext highlighter-rouge">D-SSIM loss</code><br/> D-SSIM : Directional Structural Similarity Index Measure</p> </li> <li> <p>3D Gaussian의 xyz-mean에 대해서만 <d-cite key="Plenoxels">[1]</d-cite>에서처럼 <code class="language-plaintext highlighter-rouge">standard exponential decay scheduling</code> 사용</p> </li> <li>Adam optimizer로 네 가지 param. 업데이트 <ul> <li>3D xyz-mean</li> <li>3D covariance</li> <li>color</li> <li>opacity</li> </ul> </li> <li>optimization 세부 사항 : <ul> <li>연산을 <code class="language-plaintext highlighter-rouge">low resol.부터 warm-up</code> :<br/> 목적 : model이 효율적으로 coarse info.부터 학습하도록 하여 <code class="language-plaintext highlighter-rouge">stability</code> 향상<br/> 초기에 4배 작은 image로 optimization 진행하고 250, 500 iter.에서 2배씩 upsampling</li> <li>Spherical Harmonics <code class="language-plaintext highlighter-rouge">low band부터 warm-up</code> :<br/> 목적 : 처음부터 high band로 detail까지 학습하려고 하면<br/> scene의 corner를 촬영하거나 inside-out 방식(카메라가 촬영 대상의 내부에 위치하여 바깥쪽을 촬영) 때문에<br/> <code class="language-plaintext highlighter-rouge">놓친 angular 영역이 있을 경우 SH의 0-band coeff. (base or diffuse color)가 부적절</code>하게 만들어질 수 있어서<br/> 처음에는 0-band coeff.를 optimize하고 매 1000 iter.마다 band 수 늘려서 4-band coeff.까지 optimization</li> </ul> </li> </ul> <h3 id="adaptive-density-control-of-gaussians">Adaptive Density Control of Gaussians</h3> <p>optimization of 4 param.의 경우 매 iter.마다 update하지만,<br/> Adaptive Density Control of Gaussians의 경우 <code class="language-plaintext highlighter-rouge">100 iter.마다</code> update</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/14-480.webp 480w,/assets/img/2024-07-11-GS/14-800.webp 800w,/assets/img/2024-07-11-GS/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Remove</code> :<br/> \(\alpha\) 값이 threshold보다 작거나<br/> world-space에서 크기가 매우 크거나<br/> view-space에서 footprint가 매우 큰 경우<br/> 3D Gaussians 제거</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/15-480.webp 480w,/assets/img/2024-07-11-GS/15-800.webp 800w,/assets/img/2024-07-11-GS/15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Gaussians가 scene을 제대로 표현 못 하는 중<br/> \(\rightarrow\) scene을 제대로 표현하기 위해선 Gaussian position을 크게 옮겨야 함<br/> \(\rightarrow\) view-space positional gradient \(\Delta_{p} L\)가 큼<br/> \(\rightarrow\) under/over-reconstruction 상황이므로 clone/split을 통해 정확한 위치에 Gaussian이 분포하도록 하자</p> </li> <li><code class="language-plaintext highlighter-rouge">Split</code> :<br/> <code class="language-plaintext highlighter-rouge">over-reconstruction</code>의 경우 3D Gaussians split <ul> <li>split : 1개의 Gaussian을 <code class="language-plaintext highlighter-rouge">2개로 분리</code>하고 각 scale을 줄인 후 <code class="language-plaintext highlighter-rouge">기존 3D Gaussian의 PDF</code>에 따라 sampling하여 배치<br/> Gaussians의 수는 증가하지만, total volume은 유지</li> <li>조건 1. <code class="language-plaintext highlighter-rouge">view-space positional gradient</code> \(\Delta_{p} L\)의 avg. magnitude \(\geq\) threshold \(\tau_{pos}\)</li> <li>조건 2. <code class="language-plaintext highlighter-rouge">covariance</code>가 큼</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Clone</code> :<br/> <code class="language-plaintext highlighter-rouge">under-reconstruction</code>의 경우 3D Gaussians clone <ul> <li>clone : <code class="language-plaintext highlighter-rouge">같은 크기로 copy</code> 후 <code class="language-plaintext highlighter-rouge">positional gradient 방향</code>에 배치<br/> total volume 및 Gaussians의 수 모두 증가</li> <li>조건 1. <code class="language-plaintext highlighter-rouge">view-space positional gradient</code> \(\Delta_{p} L\)의 avg. magnitude \(\geq\) threshold \(\tau_{pos}\)</li> <li>조건 2. <code class="language-plaintext highlighter-rouge">covariance</code>가 작음</li> </ul> </li> <li>3000 iter.마다 \(\alpha\) <code class="language-plaintext highlighter-rouge">알파 값을 주기적으로 0으로 초기화</code> 하면 전체 Gaussian 조절에 큰 도움이 됨! <ul> <li>효과 1. volumetric 기법의 특성상 <code class="language-plaintext highlighter-rouge">camera와 가까운 영역</code>에서 많은 <code class="language-plaintext highlighter-rouge">floater</code>들이 생겨서 Gaussian density가 증가하는데, 이를 제거해주는 역할<br/> floater 해결 관련 논문 : <d-cite key="floater1">[6]</d-cite> <d-cite key="floater2">[7]</d-cite> <d-cite key="floater3">[8]</d-cite></li> <li>효과 2. <code class="language-plaintext highlighter-rouge">큰 Gaussian들이 중첩</code>되어 있는 case를 제거해주는 역할</li> </ul> </li> </ul> <h2 id="results">Results</h2> <h3 id="implementation">Implementation</h3> <ul> <li> <p>custom CUDA kernel :<br/> tile-based rasterization을 위해<br/> custom CUDA kernel를 추가하여 사용 like <d-cite key="Point1">[3]</d-cite>, <d-cite key="Plenoxels">[1]</d-cite>, <d-cite key="superfast">[9]</d-cite></p> </li> <li> <p>Radix Sort :<br/> fast Radix Sort를 위해 NVIDIA CUB sorting routines <d-cite key="radixsort">[11]</d-cite> 사용</p> </li> <li> <p>interactive image viewer :<br/> open-source SIBR <a href="https://gitlab.inria.fr/sibr/sibr_core">SIBR</a> 이용해서<br/> interactive image-rendering viewer 만듬 (frame rate 측정에 사용)</p> </li> </ul> <h3 id="evaluation">Evaluation</h3> <ul> <li>Dataset :<br/> bounded indoor scenes와 unbounded outdoor scenes 전부 커버 <ul> <li>synthetic Blender dataset (Nerf) :<br/> have exhaustive set of bounded views with exact camera param.<br/> \(\rightarrow\) SOTA result even with 100K uniformly random initialization</li> <li>Mip-Nerf360 dataset</li> <li>Tanks&amp;Temples dataset</li> <li>Hedman et al. dataset</li> </ul> </li> <li>Metrics : <ul> <li>PSNR</li> <li>L-PIPS</li> <li>SSIM (D-SSIM)</li> </ul> </li> <li>Comparison : <ul> <li><code class="language-plaintext highlighter-rouge">Quality</code> : NeRF 계열 중 SOTA인 <code class="language-plaintext highlighter-rouge">Mip-Nerf360</code> <d-cite key="MipNeRF360">[10]</d-cite>과 비교 <ul> <li>끝까지 훈련시켰을 때 비슷한 quality 보이고,</li> <li>training speed는 35-45 min. versus 48 hours</li> </ul> </li> <li>Traning/Rendering <code class="language-plaintext highlighter-rouge">Speed</code> : NeRF 계열 중 SOTA인 <code class="language-plaintext highlighter-rouge">InstantNGP</code> <d-cite key="InstantNGP">[2]</d-cite>, <code class="language-plaintext highlighter-rouge">Plenoxels</code> <d-cite key="Plenoxels">[1]</d-cite> 과 비교 <ul> <li>speed SOTA인 <d-cite key="InstantNGP">[2]</d-cite> , <d-cite key="Plenoxels">[1]</d-cite> 과 비슷한 quality 가질 때까지 training 5-10 min.밖에 안 걸리고,</li> <li>훈련 더 하면 <d-cite key="InstantNGP">[2]</d-cite>, <d-cite key="Plenoxels">[1]</d-cite>보다 더 좋은 quality 가짐</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/19-480.webp 480w,/assets/img/2024-07-11-GS/19-800.webp 800w,/assets/img/2024-07-11-GS/19-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/19.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/17-480.webp 480w,/assets/img/2024-07-11-GS/17-800.webp 800w,/assets/img/2024-07-11-GS/17-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/17.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 7K iter.으로도 꽤 좋은 결과 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/18-480.webp 480w,/assets/img/2024-07-11-GS/18-800.webp 800w,/assets/img/2024-07-11-GS/18-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/18.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Comparison : <ul> <li>Compactness :<br/> anisotropic 3D Gaussians는<br/> scene representation 뿐만 아니라<br/> complex shape with a lower number of param.을 모델링하는 데도 쓰일 수 있음 <ul> <li>space carving으로 얻은 <d-cite key="Point3">[12]</d-cite> 의 initial point cloud에서 시작했을 때 <d-cite key="Point3">[12]</d-cite> 의 PSNR 값은 2-4 min.만에 넘겨버림</li> <li>또한, <d-cite key="Point3">[12]</d-cite> 의 point cloud의 4분의 1만큼만 써도 작은 model size로도 <d-cite key="Point3">[12]</d-cite> 의 PSNR 넘겨버림</li> </ul> </li> </ul> </li> </ul> <blockquote> <p>Space Carving :</p> <ul> <li>설명 : 여러 camera에 대해 voxel-space에서 object 있는 부분만 남기고 깎아내는 기법</li> <li>이유 : 3D reconstruction을 할 때 color 정보만으로 segmentation 가능할 정도로 background는 simple할수록 좋기 때문</li> <li>한계 : 빛, 그림자 같은 정보는 사용하지 않기 때문에 fg/bg 판단만 가능하다. 따라서 lidar처럼 camera에 depth-detection 메커니즘이 없을 경우 물체 내부의 구멍 같은 건 reconstruct 불가능</li> </ul> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/20-480.webp 480w,/assets/img/2024-07-11-GS/20-800.webp 800w,/assets/img/2024-07-11-GS/20-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/20.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="ablation-study">Ablation Study</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/21-480.webp 480w,/assets/img/2024-07-11-GS/21-800.webp 800w,/assets/img/2024-07-11-GS/21-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/21.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> PSNR score for Ablation Study </div> <ul> <li> <p>Intialization (SfM) :<br/> We also assess the importance of initializing the 3D Gaussians from the SfM point cloud. 할 차례</p> </li> <li> <p>Densification (clone, split) :<br/> ddd</p> </li> <li> <p>Unlimited depth complexity of splats with gradients :<br/> ddd</p> </li> <li> <p>Anisotropic Covariance :<br/> ddd</p> </li> <li> <p>Spherical Harmonics :<br/> ddd</p> </li> </ul> <h2 id="discussion">Discussion</h2> <h3 id="limitations">Limitations</h3> <h3 id="conclusion">Conclusion</h3>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="gaussian"/><category term="splatting"/><category term="rendering"/><category term="3d"/><category term="view"/><category term="synthesis"/><summary type="html"><![CDATA[3D GS for Real-Time Radiance Field Rendering]]></summary></entry><entry><title type="html">Pytorch Basic Code (DDP)</title><link href="https://semyeong-yu.github.io/blog/2024/PytorchBasic/" rel="alternate" type="text/html" title="Pytorch Basic Code (DDP)"/><published>2024-07-08T11:00:00+00:00</published><updated>2024-07-08T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/PytorchBasic</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/PytorchBasic/"><![CDATA[<h2 id="pytorch-basic-code-distributeddataparallel-ver">Pytorch Basic Code (DistributedDataParallel ver.)</h2> <h3 id="deal-with-json-image-csv">Deal with json, image, csv</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/3-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/3-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">import os
import json
import pandas as pd
import csv
from PIL import Image
import cv2

# read json
if os.path.exists(json_path):
    with open(json_path, "r") as f:
        data = json.load(f)
    f.close()

# read image 방법 1.
img = Image.open(image_path).convert('RGB') # PIL image object in range [0, 255]
img.show()

# read image 방법 2.
img = cv2.imread(image_path) # np.ndarray of shape (H, W, C) in range [0, 255] in BGR mode
cv2.imshow('Image', img)
cv2.waitKey(0)
cv2.destroyAllWindows()

# read csv 방법 1. general case
data = pd.read_csv(csv_path, sep="|", index_col=0, skiprows=[1], na_values=['?', 'nan']).values # 0-th column (1-th row는 제외) ('?'와 'nan'은 결측값으로 인식)

# read csv 방법 2. special case : csv가 row별로 dictionary 형태일 때
if os.path.exists(csv_path):
    with open(csv_path, "r") as f:
        reader = csv.DictReader(f, delimiter=",")
        data = [{key : value for key, value in row.items()} for row in reader] # row별로 읽음

# write to csv
dataset = [] # list of dictionaries
dataset.append({"id":id, "w":w, "h":h, "class":i})
pd.DataFrame(dataset).to_csv(output_path, index=False) # output_path : ".../dataset.csv"
</code></pre> <h3 id="convert-to-tensor">Convert to Tensor</h3> <p>data.py의 CustomDataset(torch.utils.data.Dataset)에서 image는 <code class="language-plaintext highlighter-rouge">shape (C, H, W) tensor</code>여야 하기 때문에<br/> PIL.Image.open() 또는 cv2.imread()로 얻은<br/> PIL image object 또는 np.ndarray를 적절한 shape 및 range의 tensor로 변환해주어야 한다</p> <ul> <li>PIL image object \(\rightarrow\) Tensor <ul> <li>torchvision.transforms.ToTensor()</li> <li>np.array(), torch.tensor()</li> <li>getdata(), torch.tensor()</li> </ul> </li> <li>np.ndarray \(\rightarrow\) Tensor <ul> <li>torch.tensor()</li> </ul> </li> </ul> <pre><code class="language-Python">PIL_img = Image.open(image_path).convert('RGB') # PIL image object of size (W, H) in range [0, 255]

# 방법 1. torchvision.transforms.ToTensor()
transform = torchvision.transforms.Compose([
    torchvision.transforms.Resize((256, 256)),
    torchvision.transforms.ToTensor() # convert to tensor in range [0., 1.]
])
img = transform(PIL_img) # tensor of shape (C, H, W) in range [0., 1.]

# 방법 2. np.array(), torch.tensor()
img = np.array(PIL_img) # np.ndarray of shape (H, W, C) in range [0., 255.]
img = torch.tensor(img.transpose((2, 0, 1)).astype(float)).mul_(1.0) / 255.0 # tensor of shape (C, H, W) in range [0., 1.]

# 방법 3. getdata(), torch.tensor()
img_data = PIL_img.getdata()
img = torch.tensor(img_data, dtype=torch.float32) # tensor of shape (H*W*C,) in range [0, 255]
img = img.view(PIL_img.size[1], PIL_img.size[0], 3).permute(2, 0, 1) / 255.0 # tensor of shape (C, H, W) in range [0., 1.]
</code></pre> <pre><code class="language-Python">img = cv2.imread(image_path) # np.ndarray of shape (H, W, C) in range [0., 255.]

img = torch.tensor(img.transpose((2, 0, 1)).astype(float)).mul_(1.0) / 255.0 # tensor of shape (C, H, W) in range [0., 1.]
</code></pre> <h3 id="create-dataset">Create Dataset</h3> <ul> <li><code class="language-plaintext highlighter-rouge">data augmentation</code> : <ul> <li>Resize</li> <li>ToTensor</li> <li>RandomHorizontalFlip</li> <li>RandomVerticalFlip</li> <li>Normalize</li> <li>RandomRotation</li> <li>RandomAffine <ul> <li>shear</li> <li>scale (zoom-in/out)</li> </ul> </li> <li>RandomResizedCrop</li> <li>ColorJitter</li> <li>GaussianBlur</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/4-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/4-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">import os
import torch
from torch.utils.data import Dataset
import cv2
import numpy as np
import glob
import random

# Create Dataset
class CustomDataset(Dataset):
    def __init__(self, args, mode):
        # load할 data가 너무 크다면 __init__()에서는 load할 파일명만 저장해놓고 __getitem__()에서 필요할 때마다 load
        self.args = args
        self.mode = mode
        
        if mode == 'train':
            self.data_path = os.path.join(args.data_path, 'train_blur')
        elif mode == 'val':
            self.data_path = os.path.join(args.data_path, 'val_blur')
        elif mode == 'test':
            self.data_path = os.path.join(args.data_path, 'test_blur')
        
        # a list of data/train_blur/*.png
        self.blur_path_list = sorted(glob.glob(os.path.join(self.data_path, '*.png')))
        
        # a list of data/train_sharp/*.png
        self.sharp_path_list = [os.path.normpath(path.replace('blur', 'sharp') for path in self.blur_path_list)]

    def __getitem__(self, idx):
        # should return float tensor!!
        blur_path = self.blur_path_list[idx]
        # np.ndarray of shape (H, W, C) in range [0, 255]
        blur_img = cv2.imread(blur_path) 

        if self.mode == 'train':
            sharp_path = self.sharp_path_list[idx]
            sharp_img = cv2.imread(sharp_path)
            
            # np.ndarray of shape (pat, pat, C) where pat is patch_size
            blur_img, sharp_img = self.augment(self.get_random_patch(blur_img, sharp_img)) 
            
            # tensor of shape (C, pat, pat) in range [0, 1]
            return self.np2tensor(blur_img), self.np2tensor(sharp_img) 
        
        elif self.mode == 'val':
            sharp_path = self.sharp_path_list[idx]
            sharp_img = cv2.imread(sharp_path)
            return self.np2tensor(blur_img), self.np2tensor(sharp_img)
        
        elif self.mode == 'test':
            return self.np2tensor(blur_img), blur_path

    def np2tensor(self, x):
        # input : shape (H, W, C) / range [0, 255]
        # output : shape (C, H, W) / range [0, 1]
        ts = (2, 0, 1)
        x = torch.tensor(x.transpose(ts).astype(float)).mul_(1.0) # _ : in-place
        x = x / 255.0 # normalize
        return x

    def get_random_patch(self, blur_img, sharp_img):
        H, W, C = blur_img.shape # shape (H, W, C)

        pat = self.args.patch_size # pat : patch size
        iw = random.randrange(0, W - pat + 1) # iw : range [0, W - pat]
        ih = random.randrange(0, H - pat + 1) # ih : range [0, H - pat]

        blur_img = blur_img[ih:ih + pat, iw:iw + pat, :] # shape (pat, pat, C)
        sharp_img = sharp_img[ih:ih + pat, iw:iw + pat, :]

        return blur_img, sharp_img # shape (pat, pat, C)

    def augment(self, blur_img, sharp_img):
        # random horizontal flip
        if random.random() &lt; 0.5:
            blur_img = blur_img[:, ::-1, :] # Width-axis를 flip
            sharp_img = sharp_img[:, ::-1, :]
            '''
            flow-mask pair의 경우 C-dim.이 3 = 2(optical flow x, y) + 1(occlusion mask) 이므로
            shape (T, H, W, 3)의 flow-mask pair를 horizontal flip을 하려면
            flow = flow[:, :, ::-1, :]
            flow[:, :, :, 0] *= -1
            '''
            
        # random vertical flip
        if random.random() &lt; 0.5:
            blur_img = blur_img[::-1, :, :] # Height-axis를 flip
            sharp_img = sharp_img[::-1, :, :]
            '''
            flow = flow[:, ::-1, :, :]
            flow[:, :, :, 1] *= -1
            '''

        return blur_img, sharp_img

    def __len__(self):
        return len(self.path_list)
</code></pre> <h3 id="dataloader">DataLoader</h3> <ul> <li>DataParallel(DP) vs DistributedDataParallel(DDP) :<br/> <a href="https://tkayyoo.tistory.com/27#tktag2">Pytorch DP and DDP</a> <ul> <li>DataParallel(DP) :<br/> single-process<br/> multi-thread<br/> single-machine</li> <li>DistributedDataParallel(DDP) :<br/> multi-process<br/> single-machine과 multi-machine 모두 가능</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">rank</code> : <ul> <li>전체 distributed system에서 process 순서</li> <li>4-CPU system이 2개 있을 경우<br/> rank = machine 번호(0~1) * machine 당 process 개수(4) + process 번호(0~3)</li> <li>rank = 0인 process에 대해서만 wandb로 train log 출력</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">world_size</code> : <ul> <li>전체 distributed system에서 총 process 개수</li> <li>4-GPU system이 2개 있을 경우<br/> world_size = machine 개수(2) * machine 당 process 개수(4)</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">torch.distributed.init_process_group()</code> : <ul> <li>분산 학습 환경 초기화 : 분산 학습하는 각 process 간의 통신을 설정</li> <li>backend : ‘gloo’ for CPU, ‘nccl’ for GPU, ‘mpi’ for 고성능</li> <li>init_method : 각 process가 서로 탐색하는 방법(url)<br/> 예시 : ‘env://’, f’tcp://127.0.0.1:11203’</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">torch.utils.data.distributed.DistributedSampler()</code> : <ul> <li>world_size(총 process 개수)만큼 dataset을 분할하여 모든 process가 동일한 양의 dataset을 갖도록 함</li> <li>DistributedSampler는 각 epoch마다 dataset을 무작위로 분할</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">torch.utils.data.DataLoader()</code> : <ul> <li>shuffle=False :<br/> 보통 training일 때는 일반화를 위해 shuffle=True로 두지만<br/> 분산 학습을 할 때는 같은 epoch 내에서<br/> 각 process가 서로 다른 dataset을 처리하기 위해 (중복 방지)<br/> shuffle=False로 설정</li> <li>num_workers : cpu data load할 때 multi-processing core 개수</li> <li>pin_memory=True :<br/> data load한 장치(CPU)에서 GPU로 data를 옮길 때<br/> host memory가 아닌 CPU의 page-locked memory로 할당하고<br/> GPU는 이를 참조하여 복사하므로 전송 시간을 단축<br/> pin_memory=True와 non_blocking=True는 함께 사용</li> <li>drop_last=True : 나눠떨어지지 않는 마지막 batch를 버림</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">_collate_fn(input)</code> : <ul> <li>DataLoader()에서 1개의 batch로 묶을 때 사용하는 custom 전처리 함수</li> <li>input : <ul> <li>1개의 batch에 해당하는 입력</li> <li>Dataset(torch.utils.data.Dataset)의 <strong>getitem</strong>(self, idx)이 return img, target 형태일 때<br/> [(img1, target1), (img2, target2), …]의 형태</li> </ul> </li> <li>output : <ul> <li>for iter, (x, y) in enumerate(dataloader): 의 x, y</li> </ul> </li> <li>예 : 길이가 다른 input들을 batch로 묶기 위해 padding, tokenization<br/> img의 경우 CustomDataset()에서 augmentation으로 shape (C, H, W)로 통일해줬다면 (N, C, H, W)로 묶을 수 있지만,<br/> object detection task에서 target의 경우 n_box가 image마다 다르므로 N batch로 묶기 위해 padding해주어야 함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/5-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/5-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
import torch.distributed as dist
from datetime import timedelta

def _collate_fn(samples):
    # ...

# main_worker : 각 process가 실행하는 함수
def main_worker(process_id, args):

    rank = args.machine_id * args.num_processes + process_id
    
    world_size = args.num_machines * args.num_processes
    
    dist.init_process_group(backend='nccl', init_method='env://', world_size=world_size, rank=rank, timeout=timedelta(300))
    
    ###################################################################
    
    # for epoch in range ... 밖에서
    train_dataset = CustomDataset(args, 'train')
    '''
    train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transforms.Compose(
        [
        transforms.ToTensor(), 
        transforms.Normalize((0.1302,), (0.3069,))
        ]))  
    '''

    # machine 당 process 수로 나눔
    batch_size = int(args.batch_size / args.num_processes) 
    num_workers = int(args.num_workers / args.num_processes)

    # for epoch in range ... 안에서
    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)

    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=_collate_fn, pin_memory=True, drop_last=True, sampler=train_sampler)
</code></pre> <h3 id="train">Train</h3> <ul> <li><code class="language-plaintext highlighter-rouge">torch.multiprocessing.spawn()</code> : <ul> <li>main_worker : 각 process가 실행하는 함수</li> <li>nprocs : machine 당 process 개수인 4로 설정<br/> main_worker()의 첫 번째 argument는 process_id인 0~3이 됨</li> <li>args : main_worker()에 추가로 전달할 tuple 형태의 argument</li> <li>join</li> <li>daemon</li> <li>start_method</li> </ul> </li> </ul> <ol> <li><code class="language-plaintext highlighter-rouge">model</code> initialize, and set cuda, and parallelize <ul> <li>nn.parallel.DistributedDataParallel :<br/> 각 model 복사본은 각자의 optimizer를 이용해 gradient를 구하고<br/> rank=0의 process와 통신하여 gradient의 평균을 구해서 backpropagation 진행<br/> GIL(global interpreter lock)의 제약을 해결</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">wandb</code> init <ul> <li>wandb.init() : wandb 초기화<br/> vars(args)는 args 객체의 <strong>dict</strong> 속성을 반환<br/> {‘transforms’ : ‘BaseTransform’, ‘crop_size’ : 224}과 같이 반환</li> <li>wandb.watch() : wandb 기록<br/> 모든 param.의 gradient를 기록<br/> arg.log_interval-번째 batch마다 log 기록</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">optimizer, scheduler</code> initialize</li> <li>load <code class="language-plaintext highlighter-rouge">checkpoint</code></li> <li><code class="language-plaintext highlighter-rouge">train</code> with <code class="language-plaintext highlighter-rouge">barrier</code> <ul> <li>torch.distributed.barrier() :<br/> 분산 학습 환경에서<br/> 모든 process가 이 장벽에 도달할 때까지 대기하여<br/> 모든 process가 synchronize된 상태에서 훈련이 진행되도록 함</li> <li>torch.cuda.empty_cache() :<br/> 더 이상 사용하지 않는 tensor들을 GPU cached memory에서 해제<br/> 장점 : GPU memory 확보<br/> 단점 : 너무 자주 호출하면 메모리 할당/해제에 따른 성능 저하 발생</li> <li>train :<br/> args.accumulation_steps만큼 loss를 누적한 뒤 backward<br/> args.accumulation_steps마다 gradient 및 measurement 초기화, rank=0 logging</li> <li>validation :<br/> with torch.no_grad(): 로 gradient 누적 안 함!</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">wandb</code> and <code class="language-plaintext highlighter-rouge">distributed</code> finish</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/6-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/6-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">from importlib import import_module
import torch
import torch.nn as nn
import torch.multiprocessing as mp
from utils import *
from tqdm import tqdm
import wandb

def main():
    args = arg_parse()
    fix_seed(args.random_seed)
    
    # rank=0인 process를 실행하는 system의 IP 주소
    # rank=0인 system이 모든 backend 통신을 설정!
    os.environ['MASTER_ADDR'] = '127.0.0.1'
    # 해당 system에서 사용 가능한 PORT           
    os.environ['MASTER_PORT'] = '8892'

    mp.spawn(main_worker, nprocs=args.num_processes, args=(args,))
    # DDP가 아니라면, main.py에 def main_worker()의 내용을 넣고, train.py에 class Runner 만들자
    '''
    class Runner:
        def __init__(self, args, model):
            self.args = args
            self.model = model
            pass
        def train(self, dataloader, epoch):
            pass
        def validate(self, dataloader, epoch):
            pass
        def test(self, dataloader):
            pass
    '''

def main_worker(process_id, args):
    global best_acc
    best_acc = 0.0

    # 1. model initialize, and set cuda, and parallelize
    model = MyFMANet()

    torch.cuda.set_device(process_id)
    model.cuda(process_id)
    criterion = nn.NLLLoss().cuda(process_id) # criterion = nn.CrossEntropyLoss(reduction='mean').cuda(process_id)

    model = nn.parallel.DistributedDataParallel(model, device_ids=[process_id])

    # 2. wandb init
    if rank == 0:
        wandb.init(project=args.prj_name, name=f"{args.exp_name}", entity="semyeongyu", config=vars(args))
        wandb.watch(model, log='all', log_freq=args.log_interval)

    # 3. optimizer, scheduler initialize
    optimizer = getattr(import_module("torch.optim"), args.optimizer)(model.parameters(), lr=args.lr, betas=(args.b1, args.b2), weight_decay=args.weight_decay)
    scheduler = getattr(import_module("torch.optim.lr_scheduler"), args.lr_scheduler)(optimizer, T_max=args.period, eta_min=0, last_epoch=-1, verbose=True)
    # T_max : 주기 1번 도는 데 걸리는 최대 iter. 수 / eta_min : lr의 최솟값 / last_epoch : 학습 시작할 때의 epoch

    # 4. load checkpoint
    if args.resume_from:
        start_epoch, model, optimizer, scheduler = load_checkpoint(args.checkpoint_path, model, optimizer, scheduler, rank)
    else:
        start_epoch = 0

    # 5. train with barrier
    dist.barrier()

    for epoch in range(start_epoch, args.n_epochs):
        train_sampler.set_epoch(epoch) # train_sampler가 epoch끼리 동일하게 data 분할하는 것을 방지하기 위해

        optimizer.zero_grad() # epoch마다 gradient 초기화

        train_loss = train(train_loader, model, criterion, optimizer, scheduler, epoch, args)

        dist.barrier()

        if rank == 0:
            val_acc, val_loss = validate(val_loader, model, criterion, epoch, args)
            
            # best acc일 때마다 save checkpoint
            if (best_top1 &lt; val_acc):
                best_top1 = val_acc # best_top1은 global var.
                save_checkpoint(
                    {
                        'epoch': epoch,
                        'model': model.state_dict(),
                        'best_top1': best_top1,
                        'optimizer': optimizer.state_dict(),
                        'scheduler': scheduler.state_dict()
                    }, os.path.join(args.checkpoint_dir, args.exp_name), f"{epoch}_{round(best_top1, 4)}.pt"
                )
        
        torch.cuda.empty_cache() 
    
    # 6. wandb and distributed finish
    if rank == 0:
        wandb.run.finish()

    dist.destroy_process_group()
</code></pre> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/7-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/7-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">def train(train_loader, model, criterion, optimizer, scheduler, epoch, args):
    model.train()
    train_acc, train_loss = AverageMeter(), AverageMeter() 
    # measurement of acc and loss

    pbar = tqdm(enumerate(train_loader), total=len(train_loader))
    for step, (x, y_gt) in pbar:
        x, y_gt = x.cuda(non_blocking=True), y_gt.cuda(non_blocking=True) 
        # cuda device에 올려야 함
        # pin_memory=True와 non_blocking=True는 함께 사용

        # forward
        y_pred = model(x)

        # loss divided by accumulation_steps
        loss = criterion(y_pred, y_gt) / args.accumulation_steps

        # gradient 누적
        loss.backward()
        
        # measurement
        train_acc.update(
            topk_accuracy(y_pred.clone().detach(), y_gt).item(), x.size(0))
        train_loss.update(loss.item() * args.accumulation_steps, x.size(0))

        # args.accumulation_steps만큼 loss를 누적한 뒤 평균값으로 backward
        if (step+1) % args.accumulation_steps == 0:
            # gradient clipping
            nn.utils.clip_grad_norm_(model.parameters(), max_norm=args.max_norm)

            # backward
            optimizer.step()
            scheduler.step()

            # gradient 초기화
            optimizer.zero_grad()

            # logging
            dist.barrier()
            if rank == 0:
                # wandb log
                wandb.log(
                    {
                        "Training Loss": round(train_loss.avg, 4),
                        "Training Accuracy": round(train_acc.avg, 4),
                        "Learning Rate": optimizer.param_groups[0]['lr']
                    }
                )

                # tqdm log
                description = f'Epoch: {epoch+1}/{args.n_epochs} || Step: {(step+1)//args.accumulation_steps}/{len(train_loader)//args.accumulation_steps} || Training Loss: {round(train_loss.avg, 4)}'
                pbar.set_description(description)

                # measurement 초기화
                train_loss.init()
                train_acc.init()
    
    return train_loss.avg
</code></pre> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/8-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/8-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">def validate(val_loader, model, criterion, epoch, args):
    model.eval()
    val_acc, val_loss = AverageMeter(), AverageMeter() # measurement of acc and loss

    pbar = tqdm(enumerate(val_loader), total=len(val_loader))
    with torch.no_grad(): # validation은 gradient 누적 안 함!!
        for step, (x, y_gt) in pbar:
            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)
            
            # forward
            y_pred = model(x)
            loss = criterion(y_pred, y_gt)
            
            # measurement
            val_acc.update(topk_accuracy(y_pred.clone().detach(), y_gt).item(), x.size(0)) 
            val_loss.update(loss.item(), x.size(0))

            # tqdm log
            description = f'Epoch: {epoch+1}/{args.n_epochs} || Step: {step+1}/{len(val_loader)} || Validation Loss: {round(loss.item(), 4)} || Validation Accuracy: {round(val_acc.avg, 4)}'
            pbar.set_description(description)

    # wandb log
    wandb.log(
        {
            'Validation Loss': round(val_loss.avg, 4),
            'Validation Accuracy': round(val_acc.avg, 4)
        }
    )

    return val_acc.avg, val_loss.avg
</code></pre> <h3 id="utils">Utils</h3> <ul> <li><code class="language-plaintext highlighter-rouge">argument parser</code></li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/9-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/9-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">import argparse

def arg_parse():
    parser = argparse.ArgumentParser()

    parser.add_argument("--transforms", type=str, default="BaseTransform")
    parser.add_argument("--crop_size", type=int, default=224)

    args = parser.parse_args()

    return args
</code></pre> <ul> <li><code class="language-plaintext highlighter-rouge">seed</code></li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/10-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/10-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">import random
import torch
import numpy as np

def fix_seed(random_seed):
    torch.manual_seed(random_seed)
    torch.cuda.manual_seed(random_seed)
    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(random_seed)
    random.seed(random_seed)
</code></pre> <ul> <li><code class="language-plaintext highlighter-rouge">checkpoint</code> : dictionary of elements below <ul> <li>epoch</li> <li>model.state_dict()</li> <li>best_acc</li> <li>optimizer.state_dict()</li> <li>scheduler.state_dict()</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/11-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/11-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">def save_checkpoint(checkpoint, saved_dir, file_name):
    os.makedirs(saved_dir, exist_ok=True)
    output_path = os.path.join(saved_dir, file_name)
    torch.save(checkpoint, output_path) 
    # checkpoint : dictionary

def load_checkpoint(checkpoint_path, model, optimizer, scheduler, rank=-1):
    # checkpoint_path : ".../240325.pt"
    if rank != -1: # 분산학습 yes
        map_location = {"cuda:%d" % 0 : "cuda:%d" % rank}
        checkpoint = torch.load(checkpoint_path, map_location=map_location)
    else: # 분산학습 no
        checkpoint = torch.load(checkpoint_path)

    start_epoch = checkpoint['epoch']

    model.load_state_dict(checkpoint['model'])
    
    if optimizer is not None:
        optimizer.load_state_dict(checkpoint['optimizer'])
    
    if scheduler is not None:
        scheduler.load_state_dict(checkpoint['scheduler'])

    return start_epoch, model, optimizer, scheduler
</code></pre> <ul> <li><code class="language-plaintext highlighter-rouge">augmentation</code></li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/12-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/12-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">import albumentations as A
from albumentations.pytorch.transforms import ToTensorV2

class BaseTransform(object):
    def __init__(self, crop_size = 224):
        self.transform = A.Compose(
            [   
                A.RandomResizedCrop(crop_size, crop_size),
                A.HorizontalFlip(),
                A.Normalize(),
                ToTensorV2() 
# albumentations에서는 normalize 이후에 ToTensorV2를 사용해줘야 함 (여기서 어차피 shape (C,H,W)로 변경)
            ]
        )

    def __call__(self, img):
# BaseTransform()은 nn.Module을 상속한 게 아니므로 forward를 구현해도 __call__과 연결되어 있지 않음
# 따라서 __call__()을 직접 구현해줘야 함
        return self.transform(image=img)
</code></pre> <ul> <li><code class="language-plaintext highlighter-rouge">measurement</code></li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/13-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/13-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">class AverageMeter(object):
    def __init__(self):
        self.init()
    
    def init(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val*n
        self.count += n
        self.avg = self.sum / self.count

def topk_accuracy(pred, gt, k=1):
    # pred : shape (N, class)
    # gt : shape (N,)
    _, pred_topk = pred.topk(k, dim=1)
    n_correct = torch.sum(pred_topk.squeeze() == gt)

    return n_correct / len(gt)
</code></pre> <h3 id="multi-attention">Multi-Attention</h3> <ul> <li>FMA-Net (2024) <d-cite key="FMANet">[1]</d-cite>의 Multi-Attention 구현<br/> 출처 : <a href="https://github.com/KAIST-VICLab/FMA-Net">FMA-Net Code</a></li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/2-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/2-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Multi-Attention :</p> <ul> <li><code class="language-plaintext highlighter-rouge">CO(center-oriented)</code> attention :<br/> better align \(\tilde F_{w}^{i}\) to \(F_{c}^{0}\) (center feature map of initial temporally-anchored feature)</li> <li><code class="language-plaintext highlighter-rouge">DA(degradation-aware)</code> attention :<br/> \(\tilde F_{w}^{i}\) becomes globally adaptive to spatio-temporally variant degradation by using degradation kernels \(K^{D}\)</li> </ul> </blockquote> <ul> <li> <p>CO attention :<br/> \(Q=W_{q} F_{c}^{0}\)<br/> \(K=W_{k} \tilde F_{w}^{i}\)<br/> \(V=W_{v} \tilde F_{w}^{i}\)<br/> \(COAttn(Q, K, V) = softmax(\frac{QK^{T}}{\sqrt{d}})V\)<br/> 실험 결과, \(\tilde F_{w}^{i}\)가 자기 자신(self-attention)이 아니라 \(F_{c}^{0}\)과의 relation에 집중할 때 better performance</p> </li> <li> <p>DA attention :<br/> CO attention과 비슷하지만,<br/> Query 만들 때 \(F_{c}^{0}\) 대신 \(k^{D, i}\) 사용<br/> \(\tilde F_{w}^{i}\) becomes globally adaptive to spatio-temporally-variant degradation<br/> \(k^{D, i} \in R^{H \times W \times C}\) : degradation features adjusted by conv. with \(K^{D}\) (motion-aware spatio-temporally-variant degradation kernels) 에 대해<br/> \(Q=W_{q} k^{D, i}\)<br/> DA attention은 \(Net^{D}\) 말고 \(Net^{R}\) 에서만 사용</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">nn.Conv2d()</code> :</p> <ul> <li> \[H_{out} = \lfloor 1 + \frac{H_{in} + 2 \times pad - dilation \times (K-1) - 1}{stride} \rfloor\] </li> <li>argument : <ul> <li>groups :<br/> shape (\(C_{in}\), \(C_{out}\), K, K) 대신 \(C_{in}\), \(C_{out}\) 을 groups-개로 쪼개서<br/> shape (\(\frac{C_{in}}{groups}\), \(\frac{C_{out}}{groups}\), K, K)를 groups-번 실행하여 concat</li> </ul> </li> <li>variable : <ul> <li>weight : shape (\(C_{out}\), \(\frac{C_{in}}{groups}\), K, K)</li> <li>bias : shape (\(C_{out}\),)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/14-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/14-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">import torch
import torch.nn as nn

class Attention(nn.Module):
    # Restormer (CVPR 2022) transposed-attention block
    # original source code: https://github.com/swz30/Restormer
    def __init__(self, dim, n_head, bias):
        super(Attention, self).__init__()
        self.n_head = n_head # multi-head for channel dim.
        self.temperature = nn.Parameter(torch.ones(n_head, 1, 1)) 
        # multi-head 별로 scale factor를 parameterize

        # W_q
        self.q = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)
        self.q_dwconv = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, groups=dim, bias=bias)

        # W_kv
        self.kv_conv = nn.Conv2d(dim, dim*2, kernel_size=1, bias=bias)
        self.kv_dwconv = nn.Conv2d(dim*2, dim*2, kernel_size=3, stride=1, padding=1, groups=dim*2, bias=bias)

        # W_o
        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)

    def forward(self, x, f):
        # first input x : shape (N, C, H, W) -&gt; makes key and value
        # second input f : shape (N, C, H, W) -&gt; makes query
        N, C, H, W = x.shape

        # Apply W_q and W_kv
        q = self.q_dwconv(self.q(f)) # query q : shape (N, C, H, W)
        kv = self.kv_dwconv(self.kv_conv(x)) # kv : shape (N, 2*C, H, W)
        k, v = kv.chunk(2, dim=1) # key k and value v : shape (N, C, H, W)

        # Multi-Head Attention
        q = einops.rearrange(q, 'b (head c) h w -&gt; b head c (h w)', head=self.n_head)
        # query q : shape (N, C, H, W) -&gt; shape (N, M, C/M, H * W)
        k = einops.rearrange(k, 'b (head c) h w -&gt; b head c (h w)', head=self.n_head)
        # key k : shape (N, C, H, W) -&gt; shape (N, M, C/M, H * W)
        v = einops.rearrange(v, 'b (head c) h w -&gt; b head c (h w)', head=self.n_head)
        # value v : shape (N, C, H, W) -&gt; shape (N, M, C/M, H * W)

        # matrix mul.을 할 spatial dim.을 normalize
        q = torch.nn.functional.normalize(q, dim=-1)
        k = torch.nn.functional.normalize(k, dim=-1)

        '''
        - q @ k.transpose(-2, -1) = similarity :
          shape (N, M, C/M, C/M)
        - self.temperature = scale factor for each head :
          shape (M, 1, 1) -&gt; shape (N, M, C/M, C/M) 
        '''
        attn = (q @ k.transpose(-2, -1)) * self.temperature 
        attn = attn.softmax(dim=-1) # convert to probability distribution

        out = (attn @ v) # shape (N, M, C/M, H*W)
        
        # Multi-Head Attention - concatenation
        out = einops.rearrange(out, 'b head c (h w) -&gt; b (head c) h w', head=self.n_head, h=H, w=W) 
        # shape (N, C, H, W)

        # Apply W_o
        out = self.project_out(out) # shape (N, C, H, W)

        return out

class LayerNorm(nn.Module):
    def __init__(self, normalized_shape):
        super(LayerNorm, self).__init__()
        
        # learnable param.
        self.weight = nn.Parameter(torch.ones(normalized_shape)) # shape (C,)
        self.bias = nn.Parameter(torch.zeros(normalized_shape)) # shape (C,)
    
    def forward(self, x):
        # x : shape (N, C, H, W)
        # LayerNorm : dim. C에 대해 normalize
        mu = x.mean(1, keepdim=True)
        sigma = x.var(1, keepdim=True, unbiased=False)
        return (x - mu) / torch.sqrt(sigma + 1e-5) * self.weight + self.bias

class MultiAttentionBlock(nn.Module):
    def __init__(self, dim, n_head, ffn_expansion_factor, bias, is_DA):
        super(MultiAttentionBlock, self).__init__()
        self.norm1 = LayerNorm(dim)
        # center-oriented attention
        self.co_attn = Attention(dim, n_head, bias) 
        self.norm2 = LayerNorm(dim)
        self.ffn1 = FeedForward(dim, bias)

        if is_DA:
            self.norm3 = LayerNorm(dim)
            # degradation-aware attention
            self.da_attn = Attention(dim, n_head, bias) 
            self.norm4 = LayerNorm(dim)
            self.ffn2 = FeedForward(dim, bias)

    def forward(self, Fw, F0_c, Kd):
        Fw = Fw + self.co_attn(self.norm1(Fw), F0_c)
        Fw = Fw + self.ffn1(self.norm2(Fw))

        if Kd is not None:
            Fw = Fw + self.da_attn(self.norm3(Fw), Kd)
            Fw = Fw + self.ffn2(self.norm4(Fw))

        return Fw
</code></pre>]]></content><author><name></name></author><category term="cv-tasks"/><category term="pytorch"/><summary type="html"><![CDATA[Dataset, DataLoader, Train, Attention, ...]]></summary></entry><entry><title type="html">Solar LLM with Langchain</title><link href="https://semyeong-yu.github.io/blog/2024/LLM/" rel="alternate" type="text/html" title="Solar LLM with Langchain"/><published>2024-07-03T14:00:00+00:00</published><updated>2024-07-03T14:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/LLM</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/LLM/"><![CDATA[<h2 id="solar-llm-by-upstage">Solar LLM by Upstage</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-03-LLM/1-480.webp 480w,/assets/img/2024-07-03-LLM/1-800.webp 800w,/assets/img/2024-07-03-LLM/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-03-LLM/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="solar-llm-as-personalized-llm">Solar LLM as Personalized LLM</h3> <ul> <li> <p>Referenced Github :<br/> <a href="https://github.com/UpstageAI/cookbook/tree/main">UpstageAI-cookbook</a><br/> <a href="https://github.com/semyeong-yu/LLM-cookbook">UpstageAI-cookbook</a><br/> <a href="https://github.com/iamtaewan/solarllm-oracle-cookbook">OracleDB-cookbook</a></p> </li> <li> <p>Solar mini LLM :<br/> small-size<br/> best LLM for fine-tuning<br/> can be used as personalized LLM</p> </li> <li> <p>Future of AI Ecosystem Hierarchy :<br/> Domain-specific and self-fine-tuned LLMs<br/> Solar LLM O/S<br/> O/S<br/> AI chips</p> </li> <li> <p>Langchain :<br/> LLM과 application의 통합을 간소화하는 SDK</p> </li> <li> <p>핵심 기능 : 앞으로 아래에서 배울 예정!!</p> <ul> <li>LLM 사용 (query, context)</li> <li>Groundedness Check (팩트체크)</li> <li>Layout Analyzer (PDF 또는 img에서 정보 추출)</li> <li>Embedding and DB vector store (embedding vector를 DB에 저장)</li> <li>Define Custom Tools (img 생성, 뉴스 검색, 스케쥴 관리 등)</li> </ul> </li> </ul> <h3 id="chat">Chat</h3> <pre><code class="language-Python">from langchain_upstage import ChatUpstage

llm = ChatUpstage()
llm.invoke("What's the best season to get to Korean?") # invoke llm

llm = ChatUpstage(model="solar-1-mini-chat-ja")
llm.invoke("ソーラーLLM、こんにちは。ソーラーLLM、こんにちは。")
</code></pre> <h3 id="few-shot-learning---chain">Few-shot Learning - Chain</h3> <pre><code class="language-Python"># 1. use Chat Prompt Template
from langchain_core.prompts import ChatPromptTemplate

# 2. 농담조로 말하도록 Few-shot Learning
chat_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant."),
        ("human", "What is the capital of France?"),
        ("ai", "I know of it. It's Paris!!"),
        ("human", "What about Korea?"),
    ]
)

# 3. define and invoke chain
from langchain_core.output_parsers import StrOutputParser

chain = chat_prompt | llm | StrOutputParser()
chain.invoke({})
</code></pre> <pre><code class="language-Python"># 1. use Prompt Template
from langchain_core.prompts import PromptTemplate

# 2. 한 번에 답을 내도록 (Standard Prompting) Few-shot Learning
# 한 번에 답을 내려다보니 llm이 답 틀리게 내놓음
prompt_template = PromptTemplate.from_template(
    """
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?

A: The answer is 11.

Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?

A: the answer is
"""
)

# 3. define and invoke chain
chain = prompt_template | llm | StrOutputParser()
chain.invoke({})
</code></pre> <pre><code class="language-Python"># 1. use Prompt Template
from langchain_core.prompts import PromptTemplate

# 2. 설명하면서 답을 내도록 (Chain-of-Thought Prompting) Few-shot Learning
# 설명하면서 답을 내니 llm이 알맞게 답을 내놓음
prompt_template = PromptTemplate.from_template(
    """
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?

A: Roger started with 5 balls. 2 cans of 3 tennis balls
each is 6 tennis balls. 5 + 6 = 11. The answer is 11.

Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?"""
)

# 3. define and invoke chain
chain = prompt_template | llm | StrOutputParser()
chain.invoke({})
</code></pre> <h3 id="zero-shot-learning---chain">Zero-shot Learning - Chain</h3> <pre><code class="language-Python">from langchain_core.prompts import PromptTemplate

# Zero-shot, 즉 예시를 주지 않고
# "Let's think step by step"이라는 마법의 한 문장만 써줬는데도
# 답 잘 내놓음
prompt_template = PromptTemplate.from_template(
    """
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?

A: The answer is 11.

Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?

A: Let's think step by step.
"""
)
chain = prompt_template | llm | StrOutputParser()
chain.invoke({})
</code></pre> <h3 id="divide-and-conquer">Divide-and-Conquer</h3> <ul> <li>Please provide three questions from the following text</li> </ul> <p>보다는</p> <ul> <li>Please extract three keywords from the following text 한 다음<br/> Please provide one question from the following text regarding “Depth up-scaling (DUS)”</li> </ul> <h3 id="prompt-반복">Prompt 반복</h3> <p>python f-string과 비슷한 원리</p> <pre><code class="language-Python">from langchain_core.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template(
    "Tell me a {adjective} joke about {content}."
)
# prompt_template.format(adjective="funny", content="chickens")

chain = prompt_template | llm | StrOutputParser()
chain.invoke({"adjective": "funny", "content": "chickens"})
</code></pre> <h3 id="keep-message-history-in-langchain-prompts">Keep Message History in LangChain Prompts</h3> <p>MessagesPlaceholder</p> <pre><code class="language-Python">from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# General Chat form with Message History
rag_with_history_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant."),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{input}"),
    ]
)

from langchain_core.messages import AIMessage, HumanMessage

# Message History argument
history = [
    HumanMessage("What is the capital of France?"),
    AIMessage("It's Paris!!"),
]

chain = rag_with_history_prompt | llm | StrOutputParser()
chain_result = chain.invoke({"history": history, "input": "What about Korea?"})
print(chain_result)
</code></pre> <h3 id="groundedness-check-with-langchain">Groundedness Check with LangChain</h3> <p>Groundedness Check :<br/> 답(answer)이 주어진 문맥(context)과 일맥상통하는지 (구라가 아닌지) <code class="language-plaintext highlighter-rouge">팩트 체크</code>!</p> <pre><code class="language-Python">from langchain_upstage import UpstageGroundednessCheck

groundedness_check = UpstageGroundednessCheck()

answer = chain.invoke(
    {
        "question": "What is DUS?",
        "Context": context,
    }
)
print("Potential answer: ", answer)

gc_result = groundedness_check.invoke({"context": context, "answer": answer})
print("GC check result: ", gc_result)
if gc_result.lower().startswith("grounded"):
    print("Groundedness check passed")
else:
    print("Groundedness check failed")
</code></pre> <h3 id="pdf-loader-context로-사용">PDF Loader (Context로 사용)</h3> <p>PDF에 있는 내용을 읽어와서 Context로 사용!</p> <pre><code class="language-Python">from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("pdfs/solar_sample.pdf")
docs = loader.load()  # or layzer.lazy_load()
print(docs[0].page_content[:1000])
</code></pre> <h3 id="layout-analyzer-context로-사용">Layout Analyzer (Context로 사용)</h3> <p>LLM이 받아들이기 좋은 형태로 문서를 읽기 위해<br/> Extract layouts, tables, and figures from any document to .html file<br/> Maximize RAG performance (RAG는 이후에 설명 예정)</p> <pre><code class="language-Python">! pip3 install -qU  markdownify  langchain-upstage  requests
import os
from langchain_upstage import UpstageLayoutAnalysisLoader
from IPython.display import display, HTML

os.environ["UPSTAGE_API_KEY"] = "UPSTAGE_API_KEY"

loader = UpstageLayoutAnalysisLoader("invoice.png", split="page", use_ocr=True)
# For improved memory efficiency, consider using the lazy_load method to load documents page by page.
pages = loader.load()  # or loader.lazy_load()
for page in pages:
    print(page)

loader = UpstageLayoutAnalysisLoader("pdfs/solar_sample.pdf", output_type="html")
docs = layzer.load() # or loader.lazy_load()
display(HTML(docs[0].page_content[:5000]))
</code></pre> <h3 id="rag-retrieval-augmented-generation">RAG: Retrieval Augmented Generation</h3> <ul> <li>RAG (Retrieval Augmented Generation) : <ul> <li><code class="language-plaintext highlighter-rouge">pdf, html 등 주어진 파일에서 query와 관련 있는 부분만 검색해서 context로서 사용</code>!</li> <li>Large language models (LLMs) have a limited context size</li> <li>Not all context is relevant to a given question</li> <li><code class="language-plaintext highlighter-rouge">Relevant context is retrieved(검색) from external data sources</code> and added to the prompt</li> <li>LLM generates a response based on this augmented context prompt</li> <li>RAG is particularly useful for Question Answering on custom datasets</li> <li>Query \(\rightarrow\) Retrieve (Search) \(\rightarrow\) Augmented Prompt \(\rightarrow\) LLM \(\rightarrow\) Answer</li> </ul> </li> <li>Chunking, Splitting : <ul> <li>Fixed-size chunking : split text into equal-sized chunks based on character or token count</li> <li>Semantic chunking : split text based on semantic boundaries like sentences, paragraphs, or sections</li> <li>Hierarchical chunking : create chunks at multiple levels of granularity (The ideal chunk size depends on the embedding model, retrieval use-case, and downstream task)</li> </ul> </li> </ul> <pre><code class="language-Python">from langchain_upstage import UpstageLayoutAnalysisLoader
from langchain_community.retrievers import BM25Retriever
from langchain_text_splitters import (
    Language,
    RecursiveCharacterTextSplitter,
)

layzer = UpstageLayoutAnalysisLoader(
    "pdfs/kim-tse-2008.pdf", use_ocr=True, output_type="html"
)
docs = layzer.load()

text_splitter = RecursiveCharacterTextSplitter.from_language(
    chunk_size=1000, chunk_overlap=100, language=Language.HTML
)
splits = text_splitter.split_documents(docs)

retriever = BM25Retriever.from_documents(splits)

query = "What is bug classficiation?"
context_docs = retriever.invoke("bug") # keyword search
chain.invoke({"question": query, "Context": context_docs})
</code></pre> <h3 id="keyword-search-대신-semantic-search-with-embedding-space">Keyword Search 대신 Semantic Search with Embedding space</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-03-LLM/2-480.webp 480w,/assets/img/2024-07-03-LLM/2-800.webp 800w,/assets/img/2024-07-03-LLM/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-03-LLM/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Solar-Embedding-1-Large (v1.0)<br/> Convert unstructured text data into embedding vectors</p> <pre><code class="language-Python">from langchain_upstage import UpstageEmbeddings
from langchain_chroma import Chroma
from langchain_upstage import UpstageEmbeddings
from langchain.docstore.document import Document

embeddings_model = UpstageEmbeddings(
  api_key="UPSTAGE_API_KEY", 
  model="solar-embedding-1-large"
)
embeddings = embeddings_model.embed_documents(
    [
        "What is the best season to visit Korea?",
    ]
)
query_result = embeddings.embed_query("What does Sam do?") # vector

sample_text_list = [
    "Korea is a beautiful country to visit in the spring.",
    "The best time to visit Korea is in the fall.",
    "Best way to find bug is using unit test.",
    "Python is a great programming language for beginners.",
    "Sung Kim is a great teacher.",
    "맛있는 좋은 과일을 많이 먹어 볼까?"
]

sample_docs = [Document(page_content=text) for text in sample_text_list]

vectorstore = Chroma.from_documents(
    documents=sample_docs,
    embedding=UpstageEmbeddings(model="solar-embedding-1-large"),
)

retriever = vectorstore.as_retriever()
result_docs = retriever.invoke("When to visit Korea?")
print(result_docs[0].page_content[:100])
</code></pre> <h3 id="rag-summary">RAG Summary</h3> <ol> <li>load doc</li> <li>chunking, splits</li> <li>embedding, indexing (vector store)</li> <li>retrieval, augmenting context (find Top-k most similar doc chunks in vector store with the query embedding)</li> <li>invoke chain based on the augmented context by retrieval</li> </ol> <pre><code class="language-Python">from langchain_upstage import UpstageLayoutAnalysisLoader
from langchain_text_splitters import (
    Language,
    RecursiveCharacterTextSplitter,
)
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_upstage import ChatUpstage

# 1. load doc
layzer = UpstageLayoutAnalysisLoader("pdfs/kim-tse-2008.pdf", output_type="html")
docs = layzer.load()

# 2. chunking &amp; splits
text_splitter = RecursiveCharacterTextSplitter.from_language(
    chunk_size=1000, chunk_overlap=100, language=Language.HTML
)
splits = text_splitter.split_documents(docs)

# 3. Embedding &amp; indexing
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=UpstageEmbeddings(model="solar-embedding-1-large"),
)

# 4. retrieve
retriever = vectorstore.as_retriever()
result_docs = retriever.invoke("What is Bug Classification?")
print(len(result_docs))
print(result_docs[0].page_content[:100])

# 5. invoke chain based on the augmented context by retrieval 
llm = ChatUpstage()

prompt_template = PromptTemplate.from_template(
    """
    Please provide most correct answer from the following context. 
    If the answer is not present in the context, please write "The information is not present in the context."
    ---
    Question: {question}
    ---
    Context: {Context}
    """
)

chain = prompt_template | llm | StrOutputParser()
chain.invoke({"question": "What is bug classficiation?", "Context": result_docs})
</code></pre> <h3 id="oracle-db를-persistent-memory로-쓰기">Oracle DB를 persistent memory로 쓰기</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-03-LLM/3-480.webp 480w,/assets/img/2024-07-03-LLM/3-800.webp 800w,/assets/img/2024-07-03-LLM/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-03-LLM/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>매번 100만 개의 pdf를 load해서 index할 수는 없는 일!</p> <ol> <li>load doc</li> <li>chunking, splits</li> <li>embedding, indexing (<code class="language-plaintext highlighter-rouge">vector store with Oracle DB</code>)</li> <li>retrieval, augmenting context</li> <li>split한 text가 이미 Oracle DB vector store에 있는지 체크</li> <li>없다면 embedding, indexing again</li> <li>invoke chain based on the augmented context by retrieval</li> </ol> <pre><code class="language-Python">! pip3 install -qU  markdownify  langchain-upstage rank_bm25

import oracledb
from langchain_upstage import UpstageLayoutAnalysisLoader

# 0. connect to Oracle DB
username=os.environ["DB_USER"]
password=os.environ["DB_PASSWORD"]
dsn=os.environ["DSN"]

con = oracledb.connect(user=username, password=password, dsn=dsn)

try: 
    conn23c = oracledb.connect(user=username, password=password, dsn=dsn)
    print("Connection successful!", conn23c.version)
except Exception as e:
    print("Connection failed!")

# 1. load doc
layzer = UpstageLayoutAnalysisLoader("pdfs/kim-tse-2008.pdf", output_type="html")
docs = layzer.load()

# 2. chunking &amp; splits
text_splitter = RecursiveCharacterTextSplitter.from_language(
    chunk_size=1000, chunk_overlap=100, language=Language.HTML
)
splits = text_splitter.split_documents(docs)

# check if text is in the vector store
def is_in_vectorstore(vectorstore, text):
    search_results = vectorstore.get(ids=[text])
    if search_results and search_results["ids"]:
        return True
    else:
        return False

# 3. Embedding &amp; indexing 방법 1.
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=UpstageEmbeddings(model="solar-embedding-1-large"),
)

# 3. Embedding &amp; indexing 방법 2.

knowledge_base = OracleVS.from_documents(docs, upstage_embeddings, client=conn23c, table_name="text_embeddings2", distance_strategy=DistanceStrategy.DOT_PRODUCT)

# result_chunks = knowledge_base.similarity_search(user_question)

vectorstore = OracleVS(client=conn23c, embedding_function=upstage_embeddings, table_name="text_embeddings2", distance_strategy=DistanceStrategy.DOT_PRODUCT) # create vector store

oraclevs.create_index(
    client=conn23c,
    vector_store=vectorstore,
    params={
        "idx_name": "ivf_idx1",
        "idx_type": "IVF",
    },
) # index 추가

# 4. retrieve
retriever = vectorstore.as_retriever()

unique_splits = [
    split for split in splits if not is_in_vectorstore(vectorstore, split.page_content)
]
print(len(unique_splits))

# 5. split한 text가 이미 Oracle DB vector store에 있는지 체크
# 6. 없다면 embedding, indexing again
if len(unique_splits) &gt; 0:
    vectorstore = Chroma.from_documents(
        ids=[split.page_content for split in unique_splits],
        persist_directory="./chroma_db",
        documents=unique_splits,
        embedding=UpstageEmbeddings(model="solar-embedding-1-large"),
)

# 7. invoke chain based on the augmented context by retrieval 
llm = ChatUpstage()

prompt_template = PromptTemplate.from_template(
    """
    Please provide most correct answer from the following context. 
    If the answer is not present in the context, please write "The information is not present in the context."
    ---
    Question: {question}
    ---
    Context: {Context}
    ---
    Output: please, response in Korean
    """
)

chain = ({"context": retriever, "question": RunnablePassthrough()} | prompt_template | llm | StrOutputParser())
response = chain.invoke("What is bug classficiation?")
</code></pre> <h3 id="smart-rag">Smart RAG</h3> <p>local vector store에 검색했을 때</p> <ul> <li>내가 아는 건 <code class="language-plaintext highlighter-rouge">local RAG</code> 로 처리</li> <li>내가 모르는 건 <code class="language-plaintext highlighter-rouge">external search</code> 로 처리</li> </ul> <pre><code class="language-Python"># 주어진 context만으로 주어진 question에 답변할 수 있는지 판단
# RAG or Search?
def is_in(question, context):
    is_in_conetxt = """As a helpful assistant, 
please use your best judgment to determine if the answer to the question is within the given context. 
If the answer is present in the context, please respond with "yes". 
If not, please respond with "no". 
Only provide "yes" or "no" and avoid including any additional information. 
Please do your best. Here is the question and the context:
---
CONTEXT: {context}
---
QUESTION: {question}
---
OUTPUT (yes or no):"""

    is_in_prompt = PromptTemplate.from_template(is_in_conetxt)
    chain = is_in_prompt | ChatUpstage() | StrOutputParser()

    response = chain.invoke({"context": context, "question": question})
    print(response)
    return response.lower().startswith("yes")
</code></pre> <pre><code class="language-Python"># Smart RAG, Self-Improving RAG
import os
from tavily import TavilyClient

def smart_rag(question, context):
    if not is_in(question, context):
        print("Searching in tavily")
        tavily = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])
        context = tavily.search(query=question)

    chain = prompt_template | llm | StrOutputParser()
    return chain.invoke({"context": context, "question": question})
</code></pre> <pre><code class="language-Python">smart_rag("What is DUS?", solar_summary)
# 질문에 대한 답변이 solar_summary에 있는 내용이므로 RAG  
# yes  
# 'The answer to the question "What is DUS?" is:\n\nDepth Up-Scaling (DUS)'
</code></pre> <pre><code class="language-Python">smart_rag("How to get to Seoul from SF?", solar_summary)
# solar_summary에 없는 내용이므로 Search  
# no  
# Searching in tavily
# 'The answer to "How to get to Seoul from SF?" is:\n\n1. Fly from San Francisco (SFO) to Seoul (ICN) with airlines such as ANA, Japan Airlines, Asiana Airlines, Korean Air, and United Airlines.\n2. Take a train from Incheon Int\'l Airport T1 to Seoul Station.\n3. Take the BART from Civic Center / UN Plaza to Milpitas and then fly from San Jose (SJC) to Incheon (ICN).\n\nPlease note that the cheapest flights from San Francisco to Seoul start at $453 with AIR PREMIA.'
</code></pre> <h3 id="smart-rag-with-tools">Smart RAG with Tools</h3> <ol> <li>Define <code class="language-plaintext highlighter-rouge">Custom Tools</code></li> <li>Create a list of tools</li> <li>Bind the tools to LLM</li> </ol> <p>특정 task (산수 계산 혹은 뉴스기사 검색 등) 맞춤형으로<br/> custom tools를 정의함으로써<br/> LLM 답변의 질을 높일 수 있음!</p> <pre><code class="language-Python">! pip3 install -qU  markdownify  langchain-upstage rank_bm25

from langchain_core.tools import tool
import requests
import os
from tavily import TavilyClient

# external API to search
tavily = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])

# 1. Define Custom Tools
@tool
def add(a: int, b: int) -&gt; int:
    """Adds a and b."""
    return a + b

@tool
def solar_paper_search(query: str) -&gt; str:
    """Query for research paper about solarllm, dus, llm and general AI.
    If the query is about DUS, Upstage, AI related topics, use this.
    """
    return solar_summary

@tool
def internet_search(query: str) -&gt; str:
    """This is for query for internet search engine like Google.
    Query for general topics.
    """
    return tavily.search(query=query)

@tool
def get_news(topic: str) -&gt; str:
    """Get latest news about a topic.
    If users are more like recent news, use this.
    """
    # https://newsapi.org/v2/everything?q=tesla&amp;from=2024-04-01&amp;sortBy=publishedAt&amp;apiKey=API_KEY
    # change this to request news from a real API
    news_url = f"https://newsapi.org/v2/everything?q={topic}&amp;apiKey={os.environ['NEWS_API_KEY']}"
    respnse = requests.get(news_url)
    return respnse.json()

# 2. Create a list of tools 
tools = [add, solar_paper_search, internet_search, get_news]

# 3. Bind the tools to LLM
llm_with_tools = llm.bind_tools(tools)

llm_with_tools.invoke("What is Solar LLM?").tool_calls
# 출력 : [{'name': 'solar_paper_search', 'args': {'query': 'Solar LLM'}, 'id': 'cb1687d2-7c6a-45dc-8287-19376c335cd4'}]
llm_with_tools.invoke("What's best place in Seoul?").tool_calls
# 출력 : [{'name': 'internet_search', 'args': {'query': 'best place in Seoul'}, 'id': '1f86d563-de15-460a-abc0-0e644e284518'}]
</code></pre> <pre><code class="language-Python"># Smart RAG, Self-Improving RAG
import os
from tavily import TavilyClient

def call_tool_func(tool_call):
    tool_name = tool_call["name"].lower()
    if tool_name not in globals():
        print("Tool not found", tool_name)
        return None
    selected_tool = globals()[tool_name]
    return selected_tool.invoke(tool_call["args"])

def tool_rag(question):
    for _ in range(3): # try 3 times
        tool_calls = llm_with_tools.invoke(question).tool_calls
        if tool_calls:
            break
        else:
            print("try again")

    if not tool_calls:
        return "I'm sorry, I don't have an answer for that."
    
    print(tool_calls)
    context = ""
    for tool_call in tool_calls:
        context += str(call_tool_func(tool_call))

    chain = prompt_template | llm | StrOutputParser()
    return chain.invoke({"context": context, "question": question})

tool_rag("What is Solar llm?")
# 출력 : [{'name': 'solar_paper_search', 'args': {'query': 'What is Solar llm?'}, 'id': 'cb291b01-a1aa-4839-84a8-a473f4eb0920'}] 'Solar llm is a large language model (LLM) with 10.7 billion parameters.'
tool_rag("What is news about Tesla?")
# 출력 : [{'name': 'get_news', 'args': {'topic': 'Tesla'}, 'id': 'aade5002-b9e2-4a23-92d7-fd66f12cfeb6'}] "The news about Tesla is that the company has issued a voluntary recall for nearly 4,000 Cybertrucks due to a fault with the accelerator pedal that could get trapped, pushing the car to full speed."
</code></pre> <h3 id="fine-tuning-with-predibase">Fine-tuning with Predibase</h3> <p>CFT (Continued Fine-Tuning) : feedback database에 기반하여 계속 fine-tuning</p> <pre><code class="language-Python">adapter = pb.adapters.create(
  config=FinetuningConfig(
    base_model = "solar-1-mini-chat-240612",
    epochs = 1, # default: 3
    rank = 1, # default: 16
  ),
  dataset = pb_dataset, # also accepts the dataset name as str
  repo = repo,
  description = "initial model with defaults"
)
</code></pre>]]></content><author><name></name></author><category term="generative"/><category term="generative"/><category term="LLM"/><summary type="html"><![CDATA[Upstage Solar LLM as Personalized LLM]]></summary></entry><entry><title type="html">3D Rotation-Quaternion</title><link href="https://semyeong-yu.github.io/blog/2024/Quaternion/" rel="alternate" type="text/html" title="3D Rotation-Quaternion"/><published>2024-07-01T14:00:00+00:00</published><updated>2024-07-01T14:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Quaternion</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Quaternion/"><![CDATA[<h2 id="lecture-06-3d-rotations-and-complex-representations-cmu-15-462662">Lecture 06: 3D Rotations and Complex Representations (CMU 15-462/662)</h2> <blockquote> <p>referenced video :<br/> <a href="https://www.youtube.com/watch?v=YF5ZUlKxSgE&amp;list=PL9_jI1bdZmz2emSh0UQ5iOdT2xRHFHL7E&amp;index=7">3D Rotations and Quaternion</a><br/> referenced blog :<br/> <a href="https://blog.naver.com/hblee4119/223188806834">Quaternion</a></p> </blockquote> <h2 id="3d-rotation">3D Rotation</h2> <ul> <li>2D rotation에서는 order of rotations 노상관, but<br/> 3D rotation에서는 <code class="language-plaintext highlighter-rouge">order of rotations 중요</code></li> </ul> <h2 id="gimbal-lock">Gimbal Lock</h2> <ul> <li>Gimbal Lock :<br/> Euler angles \(\theta_{x}, \theta_{y}, \theta_{z}\) 로 회전시킬 때 두 축이 맞물려서 <code class="language-plaintext highlighter-rouge">한 축이 소실</code>되는 상황</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-Quaternion/2-480.webp 480w,/assets/img/2024-07-01-Quaternion/2-800.webp 800w,/assets/img/2024-07-01-Quaternion/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-Quaternion/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 1 -&gt; 2번째 그림 : x축(초록) 회전 / 2 -&gt; 3번째 그림 : z축(파랑) 회전 / 3 -&gt; 4번째 그림 : y축(빨강) 회전 </div> <ul> <li>위의 그림에 따르면 Euler angles는 \(x\)(초록), \(y\)(빨강), \(z\)(파랑) 순으로 <code class="language-plaintext highlighter-rouge">상속관계</code>여서<br/> \(x\)축(초록)을 회전시키면 그의 자식들인 \(y, z\)축(빨강, 파랑)도 같이 회전한다.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-Quaternion/3-480.webp 480w,/assets/img/2024-07-01-Quaternion/3-800.webp 800w,/assets/img/2024-07-01-Quaternion/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-Quaternion/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>이 때, <code class="language-plaintext highlighter-rouge">Gimbal Lock</code>은 위의 그림과 같이<br/> <code class="language-plaintext highlighter-rouge">상속관계에서의 2번째 축(빨강)이 -90도 혹은 90도 회전</code>했을 때<br/> <code class="language-plaintext highlighter-rouge">상속관계에서의 1번째 축(초록)과 3번째 축(파랑)이 겹쳐서</code> 같은 방향으로 회전하기 때문에 발생한다.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-Quaternion/1-480.webp 480w,/assets/img/2024-07-01-Quaternion/1-800.webp 800w,/assets/img/2024-07-01-Quaternion/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-Quaternion/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>예를 들어, 만약 \(\theta_{y} = \frac{\pi}{2}\) 로 고정한다면<br/> \(R_x R_y R_z = \begin{bmatrix} 0 &amp; 0 &amp; 1 \\ sin(\theta_{x}+\theta_{z}) &amp; cos(\theta_{x}+\theta_{z}) &amp; 0 \\ - cos(\theta_{x}+\theta_{z}) &amp; sin(\theta_{x}+\theta_{z}) &amp; 0 \end{bmatrix}\)<br/> 이므로 \(\theta_{x}, \theta_{z}\) 값(자유도=2)과 관계없이 <code class="language-plaintext highlighter-rouge">특정 하나의 axis에 대한 회전(자유도=1)으로 제약 생겨버림</code>!</li> </ul> <h2 id="quaternion">Quaternion</h2> <ul> <li> <p>Euler angles vs Quaternion :<br/> Euler angles는 상속관계이므로 한 번에 계산이 불가능하여 순서대로 회전시켜야 하고, 짐벌락 현상이 발생할 수 있지만<br/> Quaternion은 <code class="language-plaintext highlighter-rouge">한 번에 계산 가능</code>하여 <code class="language-plaintext highlighter-rouge">동시에 회전</code>시킬 수 있으며, 짐벌락 현상이 없다!</p> </li> <li>2D rotation : <ul> <li>real, rectangular form : 2D rotation matrix 복잡</li> <li>complex, polar form : 단순히 크기 곱하고, 각도 더하고!</li> </ul> </li> <li>3D rotation : <ul> <li>real, xyz form : 3D rotation matrix 복잡</li> <li>quaternion : only need <code class="language-plaintext highlighter-rouge">FOUR</code> coordinates!(one real, three imaginary)<br/> \(H\) = span(\(\{1, i, j, k\}\))<br/> \(q = a + bi + cj + dk \in H\)<br/> \(i^2 = j^2 = k^2 = ijk = -1\) \(\leftarrow\) <code class="language-plaintext highlighter-rouge">new property!</code><br/> \(ij = k\), \(ji = -k\)<br/> \(jk = i\), \(kj = -i\)<br/> \(ki = j\), , \(ik = -j\)</li> </ul> </li> <li>Quaternion : <ul> <li>distributive and associative</li> <li><code class="language-plaintext highlighter-rouge">not commutative</code> : \(qp \neq pq\) for \(q, p \in H\)</li> <li>quaternion is <code class="language-plaintext highlighter-rouge">a pair of scalar and vector</code><br/> \(q = a + bi + cj + dk\)<br/> \(= (a, \boldsymbol u) = (a, (b, c, d)) \in H\)<br/> where \(a \in Re(H) = R\) and \(\boldsymbol u \in Im(H) = R^3\)</li> <li><code class="language-plaintext highlighter-rouge">quaternion product</code> :<br/> \((a, \boldsymbol u)(b, \boldsymbol v) = (ab - \boldsymbol u \cdot \boldsymbol v, a \boldsymbol v + b \boldsymbol u + \boldsymbol u \times \boldsymbol v)\)<br/> \(\boldsymbol u \boldsymbol v = \boldsymbol u \times \boldsymbol v - \boldsymbol u \cdot \boldsymbol v\)</li> <li><code class="language-plaintext highlighter-rouge">quaternion conjugate</code> :<br/> \(q = (w, x, y, z)\)<br/> \(q^{\ast} = (w, -x, -y, -z)\)<br/> \(\| q \| = \sqrt{w^2 + x^2 + y^2 + z^2}\)<br/> \(q \cdot q^{\ast} = (w, x, y, z) \cdot (w, -x, -y, -z) = w^2 + x^2 + y^2 + z^2 = \| q \|^2\)<br/> \(q^{-1} = \frac{q^{\ast}}{\| q \|^2} = \frac{q^{\ast}}{q \cdot q^{\ast}} = \frac{1}{q}\)<br/> \((q_1 q_2)^{\ast} = q_2^{\ast} q_1^{\ast}\)</li> </ul> </li> <li>3D Transformations via Quaternions : <ul> <li><code class="language-plaintext highlighter-rouge">3D Rotation</code> : \(q x \bar q\) \(\leftrightarrow\) \(x\)를 \(u\)에 대해 \(\theta\)만큼 회전<br/> for \(q = cos(\frac{\theta}{2}) + sin(\frac{\theta}{2})u\)<br/> where pure imaginary 3D vector \(x, u \in Im(H) = R^3\)<br/> where unit quaternion \(q \in H = (R, R^3)\) where \(\| q \|^2 = 1\)<br/> where \(\bar q\) 는 \(q\)의 conjugate</li> <li><code class="language-plaintext highlighter-rouge">Interpolating Rotation</code> :<br/> interpolating Euler angles는 strange-looking paths 및 non-uniform rotation speed를 야기할 수 있음<br/> 대신 Quaternion으로 나타내면,<br/> <code class="language-plaintext highlighter-rouge">spherical linear interpolation (SLERP)</code> :<br/> Slerp(\(q_0, q_1, t\)) = \(q_0(q_0^{-1} q_1)^t\)<br/> where \(t \in [0, 1]\)</li> <li>Generating Coordinates for <code class="language-plaintext highlighter-rouge">Texture Maps</code> :<br/> (hyper)complex numbers는 <code class="language-plaintext highlighter-rouge">angle-preserving(conformal)</code> maps에 쓰임!<br/> texture에서 angle-preserving 특성은 사람 눈으로 보기에 매우 그럴 듯하게 보이게 함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-Quaternion/4-480.webp 480w,/assets/img/2024-07-01-Quaternion/4-800.webp 800w,/assets/img/2024-07-01-Quaternion/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-Quaternion/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Beyond Quaternion … :<br/> <code class="language-plaintext highlighter-rouge">Lie algebras</code> and <code class="language-plaintext highlighter-rouge">Lie Groups</code> 으로도 3D rotations를 나타낼 수 있으며,<br/> 특히 <code class="language-plaintext highlighter-rouge">statistics(averages) of rotations</code> 를 구할 때 매우 유용!<br/> <code class="language-plaintext highlighter-rouge">exponential map</code> : axis/angle \(\rightarrow\) rotation matrix<br/> <code class="language-plaintext highlighter-rouge">logarithmic map</code> : rotation matrix \(\rightarrow\) axis/angle</p> </li> <li> <p>4 \(\times\) 1 <code class="language-plaintext highlighter-rouge">quaternion</code> \(q\) 으로 3 \(\times\) 3 <code class="language-plaintext highlighter-rouge">rotation matrix</code> 만드는 방법 : <a href="https://github.com/graphdeco-inria/gaussian-splatting/blob/b2ada78a779ba0455dfdc2b718bdf1726b05a1b6/utils/general_utils.py#L78">build_rotation(r)</a></p> <pre><code class="language-Python">def build_rotation(r):
  norm = torch.sqrt(r[:,0]*r[:,0] + r[:,1]*r[:,1] + r[:,2]*r[:,2] + r[:,3]*r[:,3])
  q = r / norm[:, None] # use normalized quaternion

  R = torch.zeros((q.size(0), 3, 3), device='cuda')

  r = q[:, 0]
  x = q[:, 1]
  y = q[:, 2]
  z = q[:, 3]

  R[:, 0, 0] = 1 - 2 * (y*y + z*z)
  R[:, 0, 1] = 2 * (x*y - r*z)
  R[:, 0, 2] = 2 * (x*z + r*y)
  R[:, 1, 0] = 2 * (x*y + r*z)
  R[:, 1, 1] = 1 - 2 * (x*x + z*z)
  R[:, 1, 2] = 2 * (y*z - r*x)
  R[:, 2, 0] = 2 * (x*z - r*y)
  R[:, 2, 1] = 2 * (y*z + r*x)
  R[:, 2, 2] = 1 - 2 * (x*x + y*y)
  return R
</code></pre> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="quaternion"/><category term="rotation"/><summary type="html"><![CDATA[Quaternion for Rotation Matrix]]></summary></entry><entry><title type="html">Diffusion-DDPM</title><link href="https://semyeong-yu.github.io/blog/2024/Diffusion/" rel="alternate" type="text/html" title="Diffusion-DDPM"/><published>2024-06-25T15:00:00+00:00</published><updated>2024-06-25T15:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Diffusion</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Diffusion/"><![CDATA[<h2 id="diffusion">Diffusion</h2> <h3 id="diffusion-model">Diffusion Model</h3> <ul> <li>forward process : <code class="language-plaintext highlighter-rouge">fixed</code> Gaussian noise 더함</li> <li>reverse process : <code class="language-plaintext highlighter-rouge">learned</code> Gaussian noise 뺌 (mean, std를 학습)</li> </ul> <h3 id="likelihood">Likelihood</h3> <p>아래 둘 다 관측값 \(x\)가 나올 확률인데,</p> <ul> <li><code class="language-plaintext highlighter-rouge">probability</code> \(P(x | \theta)\) : <code class="language-plaintext highlighter-rouge">확률 분포가 고정</code>된 상태에서, <code class="language-plaintext highlighter-rouge">관측되는 사건이 변화</code>할 때의 확률<br/> 예: 선택 가능한 정수를 1~5로 제한(확률 분포 고정)했을 때, 관측 목표값이 1~5 중 한 개의 숫자(관측 사건 변화)일 경우</li> <li><code class="language-plaintext highlighter-rouge">likelihood</code> \(L(\theta | x)\) : <code class="language-plaintext highlighter-rouge">관측된 사건이 고정</code>된 상태에서, <code class="language-plaintext highlighter-rouge">확률 분포 몰라서 가정</code>할 때의 확률<br/> 예: 선택 가능한 정수를 1~5가 아니라 1~10 또는 4~50으로 바꾸면서(확률 분포 모름), 2가 관측될 확률을 계산(관측 사건 고정)할 경우<br/> 예: 어떤(모르는) 확률 분포를 따르는 task를 n회 반복 수행하여 관측했을 때 random var. 종류를 가정할 수도 있고 특정 random var.의 parameter를 가정할 수도 있다</li> </ul> <h3 id="markov-process">Markov process</h3> <ul> <li><code class="language-plaintext highlighter-rouge">Markov</code> process (= Markov chain = <code class="language-plaintext highlighter-rouge">memoryless</code> process) : Markov property를 가지는 discrete stochastic process<br/> \(P[s_{t+1}|s_t] = P[s_{t+1}|s_1, \ldots, s_t]\)</li> </ul> <h3 id="kl-divergence">KL-divergence</h3> <ul> <li>\(H(p, q) = - \sum p_i log q_i\) : 두 확률분포 p, q의 cross entropy<br/> (보통 \(p\)는 GT, \(q\)는 predicted)</li> <li>\(H(p) = - \sum p_i log p_i\) : p’s entropy (상수값)</li> <li>\(KL(p \| q) = H(p, q) - H(p) = \sum p_i log \frac{p_i}{q_i}\) : 두 확률분포 p, q의 차이<br/> \(H(p)\)는 상수값이므로 <code class="language-plaintext highlighter-rouge">KL-divergence minimize = cross entropy minimize</code></li> <li>모르는 분포 \(p(x)\)를 N개 sampling하여 trained \(q(x | \theta)\)로 근사하고자 할 때,<br/> \(KL(p \| q) \simeq \frac{1}{N} \sum_{n=1}^{N} {-log q(x_n | \theta) + log p(x_n)}\) :<br/> \(log p(x_n)\)은 \(\theta\)에 독립이므로 <code class="language-plaintext highlighter-rouge">KL-divergence minimize = negative log likelihood minimize = MLE</code></li> </ul> <p>KL-diverence 특성 :</p> <ol> <li>\(KL(p \| q) \geq 0\) : 확률분포 p = q일 때 최소</li> <li>\(KL(p \| q) \neq KL(q \| p)\) (asymmetric) : 거리 개념이 아님<br/> 거리 개념으로 쓰는 방법 : 2가지 KL-divergence를 평균내는 방식의 \(JSD(p \| q)\)</li> </ol> <h3 id="diffusion-algorithm">Diffusion Algorithm</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-06-25-Diffusion/1-480.webp 480w,/assets/img/2024-06-25-Diffusion/1-800.webp 800w,/assets/img/2024-06-25-Diffusion/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-06-25-Diffusion/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>forward process (\(q\)) :<br/> \(q(X_t | X_{t-1}) = N(X_t ; \mu_{X_{t-1}}, \Sigma_{X_{t-1}}) = N(X_t ; \sqrt{1-\beta_t} \cdot X_{t-1}, \beta_t \cdot I)\)<br/> where \(\beta_t\) : noise 주입 정도 (상수값)<br/> t가 증가할수록 \(\beta_t\)가 증가하여 다른 pixel(\(I\))을 선택하므로 noise가 강해진다</p> </li> <li> <p>backward process (\(p_{\theta}\)) :<br/> inference 할 때 image prior \(q(X_t)\)를 모르기 때문에 \(q(X_{t-1} | X_t)\)를 계산할 수 없다. (intractable)<br/> 하지만 train 할 때는 forward process에서 더해지는 noise 정보 (GT) 를 알고 있으므로 \(q(X_{t-1} | X_t, X_0)\)를 계산할 수 있다. (tractable)<br/> 목표 : \(q(X_{t-1} | X_t, X_0)\)를 근사하는 \(p_{\theta}(X_{t-1} | X_t)\)를 학습하여 inference할 때 사용하자!<br/> 즉, 확률분포 \(q\)에서 관측한 값 \(x\)로 \(p_{\theta | x}\)의 likelihood (그럴 듯한 정도) 를 구했을 때 그 값이 최대가 되도록 하는 <code class="language-plaintext highlighter-rouge">MLE Problem</code><br/> 즉, minimize \(E_q [- log p_{\theta} (x_0)]\)</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Diffusion Model</code> Naive Loss 수식 :<br/> 확률분포 \(q\)로 sampling했을 때,<br/> \(E_{x_T \sim q(x_T|x_0)}[- log p_{\theta}(x_0)] \leq\)<br/> \(E_q [D_{KL}(q(x_T | x_0) \| p_{\theta} (x_T)) + \sum_{t \gt 1} D_{KL}(q(x_{t-1} | x_t, x_0) \| p_{\theta} (x_{t-1} | x_t)) - log p_{\theta} (x_0 | x_1)]\)</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">DDPM</code>(Denoising Diffusion Probabilistic Model)(2020) Loss 수식 :<br/> \(E_{t, x_0, \epsilon} [\| \epsilon - \epsilon_{\theta}(\sqrt{\bar \alpha_{t}}x_0 + \sqrt{1-\bar \alpha_{t}} \epsilon, t) \|^{2}]\)<br/> where \(\epsilon \sim N(0, I)\)<br/> 즉, \(\epsilon_{\theta}\)가 Standard Gaussian 분포 \(\epsilon\)를 따르도록!<br/> 이 때, \(\epsilon_{\theta}\)의 input은 \(q(x_t | x_0)\)와 \(t\) !</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Distribution Summary</code> :</p> </li> </ul> <p>Let \(\alpha_t = 1 - \beta_t\) and \(\bar \alpha_t = \prod_{s=1}^t \alpha_s\) and \(\epsilon \sim N(0, I)\)</p> <ol> <li> \[x_t \sim q(x_t | x_{t-1}, x_0) = q(x_t | x_{t-1}) = N(x_t ; \sqrt{\alpha_{t}} \cdot x_{t-1}, \beta_{t} \cdot \boldsymbol I)\] </li> <li> \[x_t \sim q(x_t | x_0) = N(x_t; \sqrt{\bar \alpha_{t}} x_{0}, (1-\bar \alpha_{t}) \boldsymbol I) = \sqrt{\bar \alpha_{t}}x_0 + \sqrt{1-\bar \alpha_{t}} \epsilon\] </li> <li>\(x_{t-1} \sim q(x_{t-1} | x_t, x_0) = N(x_{t-1}; \tilde \mu_{t}(x_t), \tilde \beta_{t})\)<br/> where \(\tilde \mu_{t}(x_t) = \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{t})\)<br/> and \(\tilde \beta_{t} = \frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_{t}} \beta_{t}\)</li> <li>\(x_{t-1} \sim p_{\theta}(x_{t-1} | x_t) = N(x_{t-1}; \mu_{\theta}(x_t, t), \tilde \beta_{t})\)<br/> where \(\mu_{\theta}(x_t, t) = \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{\theta}(x_t, t))\)<br/> and \(\tilde \beta_{t} = \frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_{t}} \beta_{t}\)<br/> (training param.로 학습하는 부분은 \(\epsilon_{\theta}(x_t, t)\) 뿐!!)</li> </ol> <p>(위의 수식 유도과정은 지금부터 아래에서 다룰 예정)</p> <h3 id="diffusion-model-및-ddpm-loss-수식-유도">Diffusion Model 및 DDPM Loss 수식 유도</h3> <blockquote> <p>Step 1. ELBO (<code class="language-plaintext highlighter-rouge">Evidence Lower Bound</code>) 꼴로 변환</p> </blockquote> <p>\(log p_{\theta}(x_0)\)<br/> \(= E_{x_T \sim q(x_T|x_0)}[log p_{\theta}(x_0)]\)<br/> \(= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[log \frac{p_{\theta}(x_{0:T})}{p_{\theta}(x_{1:T}|x_0)}]\)<br/> \(= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[log \frac{p_{\theta}(x_{0:T})}{q(x_{1:T}|x_0)}] + E_{x_{1:T} \sim q(x_{1:T}|x_0)}[log \frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{1:T}|x_0)}] \cdots (\ast)\)<br/> (\(p_{\theta}(x_{1:T}|x_0)\)은 <code class="language-plaintext highlighter-rouge">intractable</code>하므로 KL divergence 항에 넣어서 제거!)</p> <p>마지막 식의 오른쪽 항은 아래와 같이 <code class="language-plaintext highlighter-rouge">KL divergence</code> 꼴이다.<br/> \(E_{x_{1:T} \sim q(x_{1:T}|x_0)}[log \frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{1:T}|x_0)}] = \sum q(x_{1:T}|x_0) log \frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{1:T}|x_0)} = D_{KL}(q(x_{1:T}|x_0) \| p_{\theta}(x_{1:T}|x_0))\)<br/> \(q(x_{1:T}|x_0)\)는 계산할 수 있지만 \(p_{\theta}(x_{1:T}|x_0)\)는 계산할 수 없으므로 KL divergence의 특성 \(KL(p \| q) \geq 0\)을 이용하면<br/> \((\ast)\) 으로부터<br/> \(log p_{\theta}(x_0) \geq E_{x_{1:T} \sim q(x_{1:T}|x_0)}[log \frac{p_{\theta}(x_{0:T})}{q(x_{1:T}|x_0)}]\)<br/> 즉, \(E_{x_T \sim q(x_T|x_0)}[- log p_{\theta}(x_0)] \leq E_{x_{1:T} \sim q(x_{1:T}|x_0)}[- log \frac{p_{\theta}(x_{0:T})}{q(x_{1:T}|x_0)}]\)</p> <blockquote> <p>Step 2. <code class="language-plaintext highlighter-rouge">Markov property</code> 이용하여 <code class="language-plaintext highlighter-rouge">Diffusion Model Naive Loss</code> 유도</p> </blockquote> \[E_{x_{1:T} \sim q(x_{1:T}|x_0)}[- log \frac{p_{\theta}(x_{0:T})}{q(x_{1:T}|x_0)}]\] \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[log \frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{0:T})}]\] \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[log \frac{\prod_{t=1}^{T} q(x_t|x_{t-1})}{p_{\theta}(x_T) \prod_{t=1}^T p_{\theta}(x_{t-1}|x_t)}]\] <p>by memoryless <code class="language-plaintext highlighter-rouge">Markov chain property</code></p> \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[- log p_{\theta}(x_T) + \sum_{t=1}^{T} log \frac{q(x_t|x_{t-1})}{p_{\theta}(x_{t-1}|x_t)}]\] \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[- log p_{\theta}(x_T) + \sum_{t=2}^{T} log \frac{q(x_t|x_{t-1})}{p_{\theta}(x_{t-1}|x_t)} + log \frac{q(x_1|x_0)}{p_{\theta}(x_0|x_1)}]\] \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[- log p_{\theta}(x_T) + \sum_{t=2}^{T} log \frac{q(x_t|x_{t-1}, x_0)}{p_{\theta}(x_{t-1}|x_t)} + log \frac{q(x_1|x_0)}{p_{\theta}(x_0|x_1)}]\] <p>by memoryless <code class="language-plaintext highlighter-rouge">Markov property</code><br/> 어떻게든 계산 가능하도록 (<code class="language-plaintext highlighter-rouge">tractable</code>하도록) 만들기 위해 \(q(x_t|x_{t-1})\) 의 조건부에 \(x_0\) 추가</p> \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[- log p_{\theta}(x_T) + \sum_{t=2}^{T} log (\frac{q(x_{t-1}|x_t, x_0)}{p_{\theta}(x_{t-1}|x_t)} \cdot \frac{q(x_t|x_0)}{q(x_{t-1}|x_0)}) + log \frac{q(x_1|x_0)}{p_{\theta}(x_0|x_1)}]\] <p>by <code class="language-plaintext highlighter-rouge">Bayes</code> 정리<br/> \(P(A|B \bigcap C) = \frac{P(B|A \bigcap C) \cdot P(A|C)}{P(B|C)}\)</p> <p>\(= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[- log p_{\theta}(x_T) + \sum_{t=2}^{T} log \frac{q(x_{t-1}|x_t, x_0)}{p_{\theta}(x_{t-1}|x_t)} + log \frac{q(x_T|x_0)}{q(x_1|x_0)} + log \frac{q(x_1|x_0)}{p_{\theta}(x_0|x_1)}]\)<br/> by log 곱셈으로 소거</p> \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[log \frac{q(x_T|x_0)}{p_{\theta}(x_T)} + \sum_{t=2}^{T} log \frac{q(x_{t-1}|x_t, x_0)}{p_{\theta}(x_{t-1}|x_t)} - log p_{\theta}(x_0|x_1)]\] \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[D_{KL}(q(x_T|x_0) \| p_{\theta}(x_T)) + \sum_{t=2}^{T} D_{KL}(q(x_{t-1}|x_t, x_0) \| p_{\theta}(x_{t-1}|x_t)) - log p_{\theta}(x_0|x_1)]\] <p>by <code class="language-plaintext highlighter-rouge">KL divergence</code> 식 \(D_{KL}(P \| Q) = \sum P(x) log (\frac{P(x)}{Q(x)})\)</p> <blockquote> <p>Step 3. <code class="language-plaintext highlighter-rouge">DDPM Loss</code> 유도</p> </blockquote> <ol> <li> <p>\(L_T = D_{KL}(q(x_T | x_0) \| p(x_T))\) : <code class="language-plaintext highlighter-rouge">regularization</code> loss<br/> <code class="language-plaintext highlighter-rouge">마지막 상태</code> \(x_T\)에서 확률분포 q, p의 차이를 최소화<br/> noise 주입 정도인 \(\beta_t\)는 미리 정해둔 schedule에 따른 상수값(fixed)이므로<br/> \(q(x_T | x_0)\)는 training과 관계없이 \(x_T\)가 항상 Gaussian 분포를 따르도록 한다.<br/> \(x_T\)가 <code class="language-plaintext highlighter-rouge">Gaussian 분포</code>를 따르므로 \(q(x_T | x_0)\)와 \(p(x_T)\)는 거의 유사하고,<br/> 결과적으로 둘의 KL divergence인 \(L_T\)는 항상 0에 가까운 값을 가지므로 training에서 \(L_T\) term은 <code class="language-plaintext highlighter-rouge">제외</code></p> </li> <li> <p>\(L_{t-1} = D_{KL}(q(x_{t-1} | x_t, x_0) \| p(x_{t-1} | x_t))\) : <code class="language-plaintext highlighter-rouge">denoising process</code> loss<br/> <code class="language-plaintext highlighter-rouge">현재 상태</code> \(x_t\)가 주어질 때 <code class="language-plaintext highlighter-rouge">이전 상태</code> \(x_{t-1}\)가 나올 확률 분포 q, p의 차이를 최소화</p> </li> <li> <p>\(L_0 = - log p_{\theta} (x_0 | x_1)\) : <code class="language-plaintext highlighter-rouge">reconstruction</code> loss<br/> q를 sampling했을 때 \(p_{\theta} (x_0 | x_1)\)를 최대화하여 (MLE) 확률분포 q, p의 차이를 최소화<br/> 전체적으로 봤을 때 \(L_0\)는 무수히 많은 time step \(T \sim 1000\) 중 단일 시점에서의 log likelihood 값이므로<br/> <code class="language-plaintext highlighter-rouge">값이 너무 작아서</code> training에서 \(L_0\) term은 <code class="language-plaintext highlighter-rouge">제외</code></p> </li> </ol> <p>Let’s only minimize the second term<br/> \(E_{x_{1:T} \sim q(x_{1:T}|x_0)}[\sum_{t=2}^{T} L_{t-1}] = E_{x_{1:T} \sim q(x_{1:T}|x_0)}[\sum_{t=2}^{T} D_{KL}(q(x_{t-1} | x_t, x_0) \| p(x_{t-1} | x_t))]\)</p> <blockquote> <p>Step 4. Gaussian param. \(\mu, \sigma\)로 KL-divergence 나타내기</p> </blockquote> <ul> <li> <p><code class="language-plaintext highlighter-rouge">Gaussian Integral</code> :<br/> \(\int_{-\infty}^{\infty} e^{-x^2}dx = \sqrt{\pi}\) and \(\int_{-\infty}^{\infty} x^2 e^{-ax^2}dx = \frac{1}{2}\sqrt{\pi}a^{-\frac{3}{2}}\)</p> </li> <li> <p>Integral of \(p(x)logp(x)\) for Gaussian \(p(x)\) :<br/> For \(p(x) = \frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} e^{-\frac{(x-\mu_{1})^2}{2 \sigma_{1}^2}} \sim N(\mu_{1}, \sigma_{1})\),<br/> \(\int p(x) log p(x) dx\)<br/> \(= \int \frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} e^{-\frac{(x-\mu_{1})^2}{2 \sigma_{1}^2}} log (\frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} e^{-\frac{(x-\mu_{1})^2}{2 \sigma_{1}^2}}) dx\)<br/> \(= \int \frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} e^{-t^2} (log \frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} - t^2) \sqrt{2} \sigma_{1} dt\)<br/> by 치환 \(t = \frac{x-\mu_{1}}{\sqrt{2}\sigma_{1}}\)<br/> \(= \frac{1}{\sqrt{\pi}} \int e^{-t^2} (log \frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} - t^2) dt\)<br/> \(= - \frac{log(2\pi \sigma_{1}^{2})}{2 \sqrt{\pi}} \int e^{-t^2} dt - \frac{1}{\sqrt{\pi}} \int t^2 e^{-t^2} dt\)<br/> \(= - \frac{log(2\pi \sigma_{1}^{2})}{2 \sqrt{\pi}} \cdot \sqrt{\pi} - \frac{1}{\sqrt{\pi}} \cdot \frac{\sqrt{\pi}}{2}\) by <code class="language-plaintext highlighter-rouge">Gaussian integral</code><br/> \(= - \frac{log(2\pi \sigma_{1}^{2})}{2} - \frac{1}{2}\)<br/> \(= - \frac{1}{2} (1 + log(2\pi \sigma_{1}^{2}))\)</p> </li> <li> <p>Integral of \(p(x)logq(x)\) for Gaussian \(p(x)\) and \(q(x)\) :<br/> For \(p(x) = \frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} e^{-\frac{(x-\mu_{1})^2}{2 \sigma_{1}^2}} \sim N(\mu_{1}, \sigma_{1})\)<br/> and \(q(x) = \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} e^{-\frac{(x-\mu_{2})^2}{2 \sigma_{2}^2}} \sim N(\mu_{2}, \sigma_{2})\),<br/> \(\int p(x) log q(x) dx\)<br/> \(= \int p(x) log \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} e^{-\frac{(x-\mu_{2})^2}{2 \sigma_{2}^2}} dx\)<br/> \(= \int p(x) log \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} dx - \int p(x) \frac{(x-\mu_{2})^2}{2 \sigma_{2}^2} dx\)<br/> \(= log \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} - \int p(x) \frac{(x-\mu_{2})^2}{2 \sigma_{2}^2} dx\)<br/> \(= log \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} - \frac{\int p(x) x^2 dx - \int 2 \mu_{2} x p(x) dx + \mu_{2}^2 \int p(x) dx}{2 \sigma_{2}^2}\)<br/> \(= log \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} - \frac{E_{1}(x^2) - 2 \mu_{2} E_{1}(x) + \mu_{2}^2}{2 \sigma_{2}^2}\)<br/> \(= log \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} - \frac{\sigma_{1}^2 + \mu_{1}^2 - 2 \mu_{2} \mu_{1} + \mu_{2}^2}{2 \sigma_{2}^2}\)<br/> by \(Var(X) = E[X^2] - (E[X])^2\)<br/> \(= - \frac{1}{2} log (2 \pi \sigma_{2}^{2}) - \frac{\sigma_{1}^2 + (\mu_{1} - \mu_{2})^2}{2 \sigma_{2}^2}\)</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">KL divergence</code> for Gaussian \(p(x)\) and \(q(x)\) :<br/> For \(p(x) = \frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} e^{-\frac{(x-\mu_{1})^2}{2 \sigma_{1}^2}} \sim N(\mu_{1}, \sigma_{1})\)<br/> and \(q(x) = \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} e^{-\frac{(x-\mu_{2})^2}{2 \sigma_{2}^2}} \sim N(\mu_{2}, \sigma_{2})\),<br/> \(D_{KL}(p \| q)\)<br/> \(= \int p(x) log \frac{p(x)}{q(x)} dx\)<br/> \(= \int p(x) logp(x) dx - \int p(x) log q(x)dx\)<br/> \(= - \frac{1}{2} (1 + log(2\pi \sigma_{1}^{2})) - (- \frac{1}{2} log (2 \pi \sigma_{2}^{2}) - \frac{\sigma_{1}^2 + (\mu_{1} - \mu_{2})^2}{2 \sigma_{2}^2})\)<br/> \(= - \frac{1}{2} + \frac{1}{2} log (\frac{2 \pi \sigma_{2}^2}{2 \pi \sigma_{1}^2}) + \frac{\sigma_{1}^2 + (\mu_{1} - \mu_{2})^2}{2 \sigma_{2}^2}\)<br/> \(= - \frac{1}{2} + log (\frac{\sigma_{2}}{\sigma_{1}}) + \frac{\sigma_{1}^2 + (\mu_{1} - \mu_{2})^2}{2 \sigma_{2}^2}\)</p> </li> </ul> <blockquote> <p>Step 5. Only Minimize the second term in Diffusion Loss</p> </blockquote> \[E_{x_{1:T} \sim q(x_{1:T}|x_0)}[\sum_{t=2}^{T} L_{t-1}]\] \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[\sum_{t=2}^{T} D_{KL}(q(x_{t-1} | x_t, x_0) \| p(x_{t-1} | x_t))]\] <p>Let \(\sigma\) <code class="language-plaintext highlighter-rouge">std have no learning param. (상수값)</code><br/> Let \(q(x_{t-1} | x_t, x_0)\) have Gaussian mean \(\tilde \mu_{t}\)<br/> Let \(p_{\theta}(x_{t-1} | x_t)\) have Gaussian mean \(\mu_{\theta}\)</p> <p>By Step 4., since \(\sigma\) is fixed, we have to minimize</p> \[E_{x_{1:T} \sim q(x_{1:T}|x_0)}[\frac{1}{2 \sigma_{t}^2} \| \tilde \mu_{t} (x_t, x_0) - \mu_{\theta} (x_t, t) \|^2] + C\] <p><code class="language-plaintext highlighter-rouge">Now we have to know</code> \(\tilde \mu_{t}\) and \(\mu_{\theta}\)</p> <blockquote> <p>Step 6. Obtain \(q(x_t \| x_0)\) from \(q(x_t \| x_{t-1})\)</p> </blockquote> <ol> <li> <p>Let’s define \(q(x_t | x_{t-1}) = N(x_t ; \sqrt{1-\beta_{t}} \cdot x_{t-1}, \beta_{t} \cdot \boldsymbol I)\)<br/> where noise 주입 비율인 \(\beta_{t}\)는 t에 따라 증가하는 상수값이고,<br/> noise 주입 비율이 커질수록 분산이 커지는 건 reasonable</p> </li> <li> <p>Let’s define \(\alpha_{t} = 1 - \beta_{t}\) and \(\bar \alpha_{t} = \prod_{s=1}^t \alpha_{s}\)<br/> where \(\bar \alpha_{t}\)는 \(s=1\)부터 \(s=t\)까지 \(\alpha_{s} = 1 - \beta_{s}\)의 누적곱</p> </li> </ol> <p>When \(\epsilon_{t-1}, \epsilon_{t-2}, \cdots, \epsilon_0 \sim N(0, I)\),<br/> \(x_t = \mu + \sigma \cdot \epsilon = \sqrt{\alpha_{t}} x_{t-1} + \sqrt{1-\alpha_{t}} \cdot \epsilon_{t-1}\)<br/> \(\cdots\)<br/> \(x_t = \sqrt{\bar \alpha_{t}} x_{0} + \sqrt{1-\bar \alpha_{t}} \cdot \epsilon\)<br/> where \(\epsilon \sim N(0, I)\)<br/> by merging two Gaussians \(N(0, \sigma_{1}^2 I), N(0, \sigma_{2}^2 I) \rightarrow N(0, (\sigma_{1}^2 + \sigma_{2}^2) I)\)</p> <p>Therefore,<br/> \(q(x_t | x_0) = N(x_t; \sqrt{\bar \alpha_{t}} x_{0}, (1-\bar \alpha_{t}) \boldsymbol I)\)</p> <p>즉, 우리가 정의한 Gaussian \(q(x_t \| x_{t-1})\) 으로부터 Gaussian \(q(x_t\|x_0)\) 를 얻어냈다!</p> <blockquote> <p>Step 7. Obtain \(q(x_{t-1} \| x_t, x_0)\)</p> </blockquote> <p><code class="language-plaintext highlighter-rouge">Remind that</code></p> <ol> <li> \[q(x_t | x_{t-1}, x_0) = q(x_t | x_{t-1}) = N(x_t ; \sqrt{\alpha_{t}} \cdot x_{t-1}, \beta_{t} \cdot \boldsymbol I)\] </li> <li> \[q(x_t | x_0) = N(x_t; \sqrt{\bar \alpha_{t}} x_{0}, (1-\bar \alpha_{t}) \boldsymbol I)\] </li> </ol> <p>\(q(x_{t-1} | x_t, x_0)\)<br/> \(= q(x_t | x_{t-1}, x_0) \frac{q(x_{t-1} | x_0)}{q(x_t | x_0)}\)<br/> \(\propto \exp (- \frac{(x_t - \sqrt{\alpha_{t}} x_{t-1})^2}{2 \beta_{t}} - \frac{(x_{t-1} - \sqrt{\bar \alpha_{t-1}} x_{0})^2}{2 (1-\bar \alpha_{t-1})} + \frac{(x_{t} - \sqrt{\bar \alpha_{t}} x_{0})^2}{2 (1-\bar \alpha_{t})})\)<br/> \(= \exp (- \frac{1}{2} ((\frac{\alpha_{t}}{\beta_{t}} + \frac{1}{1 - \bar \alpha_{t-1}})x_{t-1}^2 - (\frac{2 \sqrt{\alpha_t}}{\beta_{t}} x_t + \frac{2\sqrt{\bar \alpha_{t-1}}}{1 - \bar \alpha_{t-1}} x_0) x_{t-1} + C(x_t, x_0)))\)</p> <p>\(q(x_{t-1} | x_t, x_0)\) 또한 Gaussian이라서<br/> \(\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x-\mu)^2}{2 \sigma^2}}\) 꼴이므로<br/> \(q(x_{t-1} | x_t, x_0)\)의 지수부분을 \(x_{t-1}\)에 대한 이차식 꼴로 정리하면<br/> 계수 비교를 통해 \(q(x_{t-1} | x_t, x_0)\)의 mean, variance를 알 수 있음!</p> <ul> <li>\(q(x_{t-1} | x_t, x_0)\) 의 variance :<br/> \(\frac{1}{\sigma^{2}}\)<br/> \(= \frac{\alpha_{t}}{\beta_{t}} + \frac{1}{1 - \bar \alpha_{t-1}}\)<br/> \(= \frac{\alpha_{t} - \alpha_{t} \bar \alpha_{t-1} + \beta_{t}}{\beta_{t}(1 - \bar \alpha_{t-1})}\)<br/> \(= \frac{\alpha_{t} - \bar \alpha_{t} + 1 - \alpha_{t}}{\beta_{t}(1 - \bar \alpha_{t-1})}\)<br/> \(= \frac{1 - \bar \alpha_{t}}{\beta_{t}(1 - \bar \alpha_{t-1})}\)</li> </ul> <p>따라서 \(\sigma^{2} = \frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_{t}} \beta_{t}\)</p> <ul> <li>\(q(x_{t-1} | x_t, x_0)\) 의 mean :<br/> \(- \frac{2 \mu}{\sigma^{2}}\)<br/> \(= - (\frac{2 \sqrt{\alpha_t}}{\beta_{t}} x_t + \frac{2\sqrt{\bar \alpha_{t-1}}}{1 - \bar \alpha_{t-1}} x_0)\)<br/> \(\rightarrow \mu = (\frac{\sqrt{\alpha_t}}{\beta_{t}} x_t + \frac{\sqrt{\bar \alpha_{t-1}}}{1 - \bar \alpha_{t-1}} x_0) \cdot \sigma^{2}\)<br/> \(= (\frac{\sqrt{\alpha_t}}{\beta_{t}} x_t + \frac{\sqrt{\bar \alpha_{t-1}}}{1 - \bar \alpha_{t-1}} x_0) \cdot (\frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_{t}} \beta_{t})\)<br/> \(= \frac{\sqrt{\alpha_t} x_t (1 - \bar \alpha_{t-1}) + \beta_{t} x_0 \sqrt{\bar \alpha_{t-1}}}{\beta_{t}(1 - \bar \alpha_{t-1})} \cdot (\frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_{t}} \beta_{t})\)<br/> \(= \frac{\sqrt{\alpha_t} x_t (1 - \bar \alpha_{t-1}) + \beta_{t} x_0 \sqrt{\bar \alpha_{t-1}}}{1 - \bar \alpha_{t}}\)</li> </ul> <p>따라서<br/> \(\mu = \frac{\sqrt{\bar \alpha_{t-1}} \beta_{t}}{1 - \bar \alpha_{t}} x_0 + \frac{\sqrt{\alpha_t} (1 - \bar \alpha_{t-1})}{1 - \bar \alpha_{t}} x_t\)</p> <p>\(q(x_t | x_0) = N(x_t; \sqrt{\bar \alpha_{t}} x_{0}, (1-\bar \alpha_{t}) \boldsymbol I)\)이므로<br/> 방금 구한 \(\mu\) 식에서 \(x_0\) <code class="language-plaintext highlighter-rouge">소거</code>하기 위해<br/> \(x_t = \sqrt{\bar \alpha_{t}} x_{0} + \sqrt{1-\bar \alpha_{t}} \epsilon\) 대입하면</p> <p>\(\mu_{t} = \frac{\sqrt{\bar \alpha_{t-1}} \beta_{t}}{1 - \bar \alpha_{t}} x_0 + \frac{\sqrt{\alpha_t} (1 - \bar \alpha_{t-1})}{1 - \bar \alpha_{t}} x_t\)<br/> \(= \frac{\sqrt{\bar \alpha_{t-1}} \beta_{t}}{1 - \bar \alpha_{t}} (\frac{1}{\sqrt{\bar \alpha_{t}}}(x_t - \sqrt{1 - \bar \alpha_{t}} \epsilon_{t})) + \frac{\sqrt{\alpha_t} (1 - \bar \alpha_{t-1})}{1 - \bar \alpha_{t}} x_t\)<br/> \(= \frac{\sqrt{\bar \alpha_{t-1}} (1 - \alpha_{t})}{1 - \bar \alpha_{t}} (\frac{1}{\sqrt{\bar \alpha_{t}}}(x_t - \sqrt{1 - \bar \alpha_{t}} \epsilon_{t})) + \frac{\sqrt{\alpha_t} (1 - \bar \alpha_{t-1})}{1 - \bar \alpha_{t}} x_t\)<br/> \(= \frac{\sqrt{k} (1 - \alpha_{t})}{1 - \alpha_{t} k} (\frac{1}{\sqrt{\alpha_{t} k}}(x_t - \sqrt{1 - \alpha_{t} k} \epsilon_{t})) + \frac{\sqrt{\alpha_t} (1 - k)}{1 - \alpha_{t} k} x_t\)<br/> by \(k = \bar \alpha_{t-1}\) 및 \(\alpha_{t}k = \bar \alpha_{t}\)로 치환<br/> \(= \frac{1}{\sqrt{\alpha_{t}}} x_t - \frac{1 - \alpha_{t}}{\sqrt{\alpha_{t}}\sqrt{1 - \alpha_{t}k}} \epsilon_{t}\)<br/> \(= \frac{1}{\sqrt{\alpha_{t}}} x_t - \frac{1 - \alpha_{t}}{\sqrt{\alpha_{t}}\sqrt{1 - \bar \alpha_{t}}} \epsilon_{t}\)<br/> \(= \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{t})\)</p> <ul> <li>\(q(x_{t-1} | x_t, x_0)\) <code class="language-plaintext highlighter-rouge">결과</code> :<br/> \(q(x_{t-1} | x_t, x_0) = N(x_{t-1}; \tilde \mu_{t}(x_t), \tilde \beta_{t})\)<br/> where \(\tilde \mu_{t}(x_t) = \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{t})\)<br/> and \(\tilde \beta_{t} = \frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_{t}} \beta_{t}\)</li> </ul> <blockquote> <p>Step 8. Obtain \(p_{\theta}(x_{t-1} \| x_t)\)</p> </blockquote> <p>우리의 목적은<br/> \(D_{KL}(q(x_{t-1} | x_t, x_0) \| p_{\theta}(x_{t-1} | x_t))\) 최소화<br/> 즉, \(p\)의 분포를 \(q\)의 분포에 approx.하는 것이다</p> <p>\(q(x_{t-1} | x_t, x_0)\) 의 mean, variance 인<br/> \(\tilde \mu_{t}(x_t) = \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{t})\)<br/> \(\tilde \beta_{t} = \frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_{t}} \beta_{t}\) 에서<br/> \(x_t, \alpha_{t}, \beta_{t}\)는 입력값 및 미리 정해놓는 상수값이라서<br/> deep learning network인 \(\epsilon_{\theta}\) 가 시간 t에 따라 \(\epsilon_{t} \sim N(0, I)\) 을 학습하도록 하기 위해서<br/> <code class="language-plaintext highlighter-rouge">training param.로 학습할 수 있는 부분</code>은 \(\epsilon_{t} \sim N(0, I)\) 뿐이다<br/> 즉, <code class="language-plaintext highlighter-rouge">p와 q의 분포에서 차이가 날 수 있는 부분은 epsilon 뿐!</code></p> <p>따라서 \(p_{\theta}(x_{t-1} | x_t)\)의 평균인 \(\mu_{\theta}(x_t, t)\) 는<br/> \(\tilde \mu_{t}(x_t) = \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{t})\) 에서<br/> \(\epsilon_{t}\) 만 \(\epsilon_{\theta}(x_t, t)\) 로 바꾼 값이다<br/> \(\mu_{\theta}(x_t, t) = \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{\theta}(x_t, t))\)</p> <blockquote> <p>Step 9. Final <code class="language-plaintext highlighter-rouge">DDPM Loss</code></p> </blockquote> <p>Step 5.에 따르면 we have to minimize<br/> \(E_{x_{1:T} \sim q(x_{1:T}|x_0)}[\frac{1}{2 \sigma_{t}^2} \| \tilde \mu_{t} (x_t, x_0) - \mu_{\theta} (x_t, t) \|^2] + C\)</p> <p>\(E_{x_0, \epsilon}[\frac{1}{2 \|\Sigma(x_t, t) \|^2} \| \tilde \mu_{t} (x_t, x_0) - \mu_{\theta} (x_t, t) \|^2]\)<br/> \(= E_{x_0, \epsilon}[\frac{1}{2 \|\Sigma \|^2} \| \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{t}) - \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{\theta}(x_t, t)) \|^2]\)<br/> \(= E_{x_0, \epsilon}[\frac{(1 - \alpha_{t})^2}{2 \|\Sigma \|^2 \alpha_{t} (1 - \bar \alpha_{t})} \| \epsilon_{t} - \epsilon_{\theta}(x_t, t) \|^2]\)<br/> \(= E_{x_0, \epsilon}[\frac{(1 - \alpha_{t})^2}{2 \|\Sigma \|^2 \alpha_{t} (1 - \bar \alpha_{t})} \| \epsilon_{t} - \epsilon_{\theta}(\sqrt{\bar \alpha_{t}} x_{0} + \sqrt{1-\bar \alpha_{t}} \epsilon, t) \|^2]\)<br/> since \(x_t = \sqrt{\bar \alpha_{t}} x_{0} + \sqrt{1-\bar \alpha_{t}} \epsilon\)</p> <p>앞의 weight term을 제거하면<br/> <code class="language-plaintext highlighter-rouge">최종 Loss 값</code>은 드디어!!!<br/> \(= E_{x_0, \epsilon}[\| \epsilon_{t} - \epsilon_{\theta}(\sqrt{\bar \alpha_{t}} x_{0} + \sqrt{1-\bar \alpha_{t}} \epsilon, t) \|^2]\)<br/> where \(x_t = q(x_t | x_0) = \sqrt{\bar \alpha_{t}} x_{0} + \sqrt{1-\bar \alpha_{t}} \epsilon\) and \(\epsilon \sim N(0, I)\)</p> <h3 id="ddpm-pseudo-code">DDPM Pseudo-Code</h3> <ul> <li><code class="language-plaintext highlighter-rouge">forward</code> process : <code class="language-plaintext highlighter-rouge">Training</code> \(\epsilon_{\theta}\) for given input image \(x_0\)</li> </ul> <pre><code class="language-Python">while (converge){
  x_0 ~ q(x_0) # input image
  t ~ Uniform({1, ..., T}) # time step (integer)
  epsilon ~ N(0, I) # Gaussian target epsilon

  # gradient descent by DDPM loss
}
</code></pre> <p>DDPM loss :<br/> \(E_{x_0, \epsilon}[\| \epsilon_{t} - \epsilon_{\theta}(\sqrt{\bar \alpha_{t}} x_{0} + \sqrt{1-\bar \alpha_{t}} \epsilon, t) \|^2]\)</p> <ul> <li><code class="language-plaintext highlighter-rouge">backward</code> process : <code class="language-plaintext highlighter-rouge">Sampling</code> from Gaussian noise img to new img by trained \(\epsilon_{\theta}\)</li> </ul> <pre><code class="language-Python">x_T ~ N(0, I) # start with Gaussian noise image

for (t = T, ..., 1){
  z ~ N(0, I) if t &gt; 1 else z = 0  
  # sampling x_{t-1} from x_t by p_{theta}(x_{t-1} | x_t)
}
</code></pre> <p>Sampling :<br/> \(x_{t-1} = \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{\theta}(x_t, t)) + \sigma_{t} z\)</p> <h3 id="discussion">Discussion</h3> <ul> <li> <p>Q1 :<br/> image prior \(q(X_t)\)를 모르기 때문에 \(q(X_{t-1} | X_t)\)를 계산할 수 없으므로 \(q(X_{t-1} | X_t)\)를 근사하는 \(p_{\theta}(X_{t-1} | X_t)\) 학습한다고 처음에 얘기했는데, 수식 유도를 보면 \(q(X_{t-1} | X_t, X_0)\) 의 분포를 알고 있지 않나?</p> </li> <li> <p>A1 :<br/> <code class="language-plaintext highlighter-rouge">training dataset</code> \(X_0\) 에 대한 \(q(X_{t-1} | X_t, X_0)\) 는 알 수 있다! (tractable)<br/> forward process에서 \(X_{t-1}\)에서 \(X_t\)로 더하는 noise 정보 (GT) 를 알고 있기 때문에 backward process에서의 \(q(X_{t-1} | X_t, X_0)\) 분포를 수식으로 구할 수 있다.<br/> 이 때, 실제 분포 \(q(X_{t-1} | X_t, X_0)\)를 근사하는 \(p_{\theta}(X_{t-1} | X_t)\)를 학습한다면<br/> <code class="language-plaintext highlighter-rouge">inference</code>를 할 때에도 <code class="language-plaintext highlighter-rouge">임의로 Gaussian sampling한 noise</code>에 대해서도 \(p_{\theta}(X_{t-1} | X_t)\) 에 의해 image \(X_0\) 을 생성할 수 있다.<br/> 수식 유도 과정을 보면 Step 2 에서 intractable \(q\) 를 계산 가능하도록 (tractable 하도록) 만들기 위해 \(q\) 분포의 조건부에 \(X_0\)을 추가하는 것을 확인할 수 있다.</p> </li> </ul> <blockquote> <p>출처 블로그 :<br/> <a href="https://xoft.tistory.com/32">Diffusion Model</a><br/> <a href="https://xoft.tistory.com/33?category=1156151">DDPM 수식 유도</a><br/> <a href="https://woongchan789.tistory.com/12">DDPM 수식 유도</a><br/> <a href="https://metamath1.github.io/blog/posts/diffusion/ddpm_part1.html?fbclid=IwAR3_AboIfJ-KbHuDhLRGAUuOYRvfG8SkMCyFtt9O7UneWIaMdfePh-jSCJg">DDPM 코드 실습</a></p> </blockquote>]]></content><author><name></name></author><category term="generative"/><category term="diffusion"/><category term="generative"/><summary type="html"><![CDATA[Diffusion Study]]></summary></entry><entry><title type="html">MipNeRF</title><link href="https://semyeong-yu.github.io/blog/2024/MipNeRF/" rel="alternate" type="text/html" title="MipNeRF"/><published>2024-06-17T21:00:00+00:00</published><updated>2024-06-17T21:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/MipNeRF</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/MipNeRF/"><![CDATA[<h2 id="mip-nerf-a-multiscale-representation-for-anti-aliasing-neural-radiance-fields">Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</h2> <h4 id="jonathan-t-barron-ben-mildenhall-matthew-tancik-peter-hedman-ricardo-martin-brualla-pratul-p-srinivasan">Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2103.13415">https://arxiv.org/abs/2103.13415</a><br/> project website :<br/> <a href="https://jonbarron.info/mipnerf/">https://jonbarron.info/mipnerf/</a><br/> pytorch code :<br/> <a href="https://github.com/bebeal/mipnerf-pytorch">https://github.com/bebeal/mipnerf-pytorch</a><br/> <a href="https://github.com/google/mipnerf">https://github.com/google/mipnerf</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>Mip-NeRF는 training하는 동안, <code class="language-plaintext highlighter-rouge">임의의 scale</code>에 대해 query 받을 수 있는, scene의 <code class="language-plaintext highlighter-rouge">anti-aliased</code> <code class="language-plaintext highlighter-rouge">pre-filtered representation</code>을 학습한다.</li> <li>camera center로부터 각 pixel로 3D conical frustum을 쏜 다음, the <code class="language-plaintext highlighter-rouge">frustum을 multi-variate Gaussian으로 근사</code>한 뒤, Gaussian 내 좌표를 positional encoding한 것에 대해 <code class="language-plaintext highlighter-rouge">integral</code> \(E \left[ \gamma (x) \right]\) 계산</li> </ol> </blockquote> <h2 id="introduction">Introduction</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-06-17-MipNeRF/2-480.webp 480w,/assets/img/2024-06-17-MipNeRF/2-800.webp 800w,/assets/img/2024-06-17-MipNeRF/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-06-17-MipNeRF/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>기존 NeRF의 문제점 :<br/> rendering 위해 sampling할 때 <code class="language-plaintext highlighter-rouge">single ray</code> per pixel 쏴서 composite 하므로<br/> dataset images에 있는 물체의 크기가 일정하지 않을 때 (multiple resolutions) multi-scales images에 대해 학습하더라도<br/> high-resolution은 <code class="language-plaintext highlighter-rouge">blurry</code> rendering<br/> low-resolution은 <code class="language-plaintext highlighter-rouge">aliased</code> rendering<br/> 그렇다고 multiple rays per pixel through its footprint로 brute-force supersampling하는 것은 정확하긴 하겠지만 too costly 비현실적</p> </li> <li> <p>Minmapping Approach :<br/> classic 컴퓨터 그래픽스 분야에서 rendering할 때 aliasing을 없애기 위한 <code class="language-plaintext highlighter-rouge">pre-filtering</code> 기법<br/> 본 논문인 Mip-NeRF가 여기서 영감을 얻음<br/> signal(e.g. image)을 diff. downsampling scales로 나타낸 뒤 pixel footprint를 근거로 ray에 사용하기 위한 <code class="language-plaintext highlighter-rouge">적절한 scale을 고른다</code><br/> render time에 할 복잡할 일을 precomputation phase에 미리 하는 것일 뿐이긴 하지만, 주어진 texture마다 한 번만 minmap을 만들면 된다는 장점이 있다</p> </li> <li> <p>Mip-NeRF :</p> <ul> <li>represent pre-filtered scene at <code class="language-plaintext highlighter-rouge">continuous space of scales</code></li> <li>ray 대신 <code class="language-plaintext highlighter-rouge">conical frustum</code> 사용해서 <code class="language-plaintext highlighter-rouge">anti-aliased</code> rendering with fine details</li> <li>multiscale variant of dataset에 대해 평균 error rate 60% 감소</li> <li>NeRF가 hierarchical sampling을 위해 coarse and fine MLP를 분리했다면, Mip-NeRF는 <code class="language-plaintext highlighter-rouge">scale-aware</code>하므로 <code class="language-plaintext highlighter-rouge">single MLP만으로 충분</code><br/> 따라서 NeRF보다 7% 빠르고, param. 수는 절반</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-06-17-MipNeRF/1-480.webp 480w,/assets/img/2024-06-17-MipNeRF/1-800.webp 800w,/assets/img/2024-06-17-MipNeRF/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-06-17-MipNeRF/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>차이 : 기존 NeRF는 <code class="language-plaintext highlighter-rouge">a single point</code>를 encode하고, Mip-NeRF는 <code class="language-plaintext highlighter-rouge">a region of space</code>를 encode</p> </li> <li> <p>기존 NeRF :<br/> camera center로부터 각 pixel로 ray를 하나 쏜 다음 point sampling한 뒤 positional encoding</p> </li> <li> <p>Mip-NeRF :<br/> camera center로부터 각 pixel로 3D conical frustum을 쏜 다음, 3D point 및 그 주위의 Gaussian region을 encode하기 위해 <code class="language-plaintext highlighter-rouge">IPE(integrated positional encoding)</code><br/> IPE : the <code class="language-plaintext highlighter-rouge">frustum을 multi-variate Gaussian으로 근사</code>한 뒤, Gaussian 내 좌표를 positional encoding한 것에 대해 <code class="language-plaintext highlighter-rouge">integral</code> \(E \left[ \gamma (x) \right]\) 계산</p> </li> </ul> <h2 id="related-work">Related Work</h2> <h4 id="anti-aliasing-in-rendering">Anti-aliasing in Rendering</h4> <blockquote> <p><code class="language-plaintext highlighter-rouge">anti-aliasing</code>을 위한 고전적인 방법으로는 두 가지가 있다.</p> </blockquote> <ol> <li><code class="language-plaintext highlighter-rouge">supersampling</code> : <ul> <li>rendering할 때 <code class="language-plaintext highlighter-rouge">multiple rays per pixel</code>을 쏴서 Nyquist frequency에 가깝게 supersampling</li> <li><code class="language-plaintext highlighter-rouge">expensive</code> as runtime increases linearly with the supersampling rate, so used only in <code class="language-plaintext highlighter-rouge">offline</code> rendering</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">pre-filtering</code> : <ul> <li>target sampling rate에 맞춰서 Nyquist frequency를 줄이기 위해 scene에 <code class="language-plaintext highlighter-rouge">lowpass-filter</code>를 씌운 버전 사용</li> <li>scene을 미리 downsampling <code class="language-plaintext highlighter-rouge">multi-scales</code> (sparse voxel octree 또는 minmap)로 나타낸 뒤, <code class="language-plaintext highlighter-rouge">ray 대신 cone</code>을 추적하여 cone과 scene이 만나는 곳의 cone’s footprint에 대응되는 <code class="language-plaintext highlighter-rouge">적절한 scale</code>을 골라서 사용 (target sampling rate에 맞는 적절한 scale)</li> <li>scene에 filter 씌운 버전을 한 번만 미리 계산하면 되므로, better for <code class="language-plaintext highlighter-rouge">real-time</code> rendering</li> </ul> </li> </ol> <blockquote> <p>그런데 아래의 이유로 고전적인 multi-scale representation은 적용 불가능<br/> input scene의 geometry를 미리 알 수 없음<br/> input scene의 <code class="language-plaintext highlighter-rouge">scale이 continuous</code>하므로 a fixed number of scales (discrete)과 상황이 다름</p> </blockquote> <p>\(\rightarrow\) 결론 : 따라서 Mip-NeRF는 training하는 동안, <code class="language-plaintext highlighter-rouge">임의의 scale</code>에 대해 query 받을 수 있는, scene의 <code class="language-plaintext highlighter-rouge">pre-filtered representation</code>을 학습한다.</p> <h4 id="scene-representations-for-view-synthesis">Scene Representations for View Synthesis</h4> <ul> <li> <p>만약 images of scene are captured <code class="language-plaintext highlighter-rouge">densely</code> 라면, novel view synthesis 위해, intermediate representation of scene을 reconstruct하지 않고 <code class="language-plaintext highlighter-rouge">light field interpolation</code> 기법 사용</p> </li> <li>만약 images of scene are captured <code class="language-plaintext highlighter-rouge">sparsely</code> 라면, novel view synthesis 위해, <code class="language-plaintext highlighter-rouge">explicit representation</code> of the scene’s 3D geometry and appearance를 reconstruct <ul> <li><code class="language-plaintext highlighter-rouge">polygon-mesh-based</code> representation (<code class="language-plaintext highlighter-rouge">discrete, classic</code>) :<br/> with either diffuse or view-dependent textures<br/> can be stored efficiently, but mesh geometry optimization에 gradient descent를 이용하는 것은 불연속점 및 극소점 때문에 어렵</li> <li><code class="language-plaintext highlighter-rouge">volumetric</code> representation : <ul> <li><code class="language-plaintext highlighter-rouge">voxel-grid-based</code> representation (<code class="language-plaintext highlighter-rouge">discrete, classic</code>) : deep learning으로 학습 가능, but 고해상도의 scene에는 부적합</li> <li><code class="language-plaintext highlighter-rouge">coordinate-based</code> neural representation (<code class="language-plaintext highlighter-rouge">continuous, recent</code>) : 3D 좌표를 그 위치에서의 scene의 특징(e.g. volume density, radiance)으로 mapping하는 continuous function을 MLP로 예측 <ul> <li>implicit surface representation</li> <li><code class="language-plaintext highlighter-rouge">implicit volumetric NeRF representation</code></li> </ul> </li> </ul> </li> </ul> </li> <li>문제점 :<br/> 그동안 view synthesis를 할 때 sampling 및 aliasing에는 덜 주목했다<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">classic discrete</code> representation의 경우 앞에서 소개한 minmap, octree와 같은 고전적인 multi-scale pre-filtering 기법을 쓰면 aliasing 없이 rendering 가능하다<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">coordinate-based</code> representation의 경우 scale이 continuous하므로 고전적인 anti-aliasing 기법은 사용할 수 없다<br/> \(\rightarrow\) sparse voxel octree 기반의 multi-scale representation으로 implicit surface의 continuous neural representation을 구현한 논문 <d-cite key="continuouspriori">[1]</d-cite> 있긴 하지만, scene geometry의 priori를 알아야 한다는 제약이 있다<br/> \(\rightarrow\) Mip-NeRF는 training하는 동안, <code class="language-plaintext highlighter-rouge">임의의 scale</code>에 대해 query 받을 수 있는, scene의 anti-aliased <code class="language-plaintext highlighter-rouge">pre-filtered representation</code>을 학습한다.</li> </ul> <h2 id="method">Method</h2> <h4 id="cone-tracing-and-positional-encoding">Cone Tracing and Positional Encoding</h4> <p>TBD<br/> (<code class="language-plaintext highlighter-rouge">작성 중 ...</code>)</p> <h4 id="architecture">Architecture</h4> <p>TBD</p> <h2 id="conclusion">Conclusion</h2> <p>TBD</p>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="rendering"/><category term="3d"/><category term="multiscale"/><category term="antialiasing"/><summary type="html"><![CDATA[A Multiscale Representation for Anti-Aliasing Neural Radiance Fields]]></summary></entry><entry><title type="html">SegmentAnything</title><link href="https://semyeong-yu.github.io/blog/2024/SegmentAnything/" rel="alternate" type="text/html" title="SegmentAnything"/><published>2024-05-29T14:00:00+00:00</published><updated>2024-05-29T14:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/SegmentAnything</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/SegmentAnything/"><![CDATA[<h3 id="segmentanything">SegmentAnything</h3> <h4 id="alexander-kirillov-eric-mintun-nikhila-ravi-hanzi-mao-chloe-rolland-laura-gustafson-tete-xiao-spencer-whitehead-alexander-c-berg-wan-yen-lo-piotr-dollár-ross-girshick">Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2304.02643">https://arxiv.org/abs/2304.02643</a><br/> 출처 : Vision study mkd님</p> </blockquote> <hr/> <h2 id="abstract">Abstract</h2> <ul> <li> <p>Task<br/> Promptable Image Segmentation</p> </li> <li> <p>Model Architecture<br/> image encoder + prompt encoder + mask decoder</p> </li> <li> <p>Generate Data (Data Engine)<br/> assisted-manual stage \(\rightarrow\) semi-automatic stage \(\rightarrow\) fully-automatic stage<br/> data ‘SA-1B’ : 1B masks with 11M images</p> </li> <li> <p>Enable Zero-Shot Generalization<br/> Zero-Shot transfer to various tasks</p> </li> <li> <p>Code Review</p> </li> </ul> <h2 id="overview">Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/2-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/2-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>prompt : mask를 생성할 대상을 지정<br/> point, BB, mask(rough area), text(preliminary) 중 하나</p> </li> <li> <p>valid masks : segmented mask를 하나가 아닌 3개 (whole, part, sub-part) 생성<br/> ambiguous prompt에 대응하기 위해, zero shot을 위해<br/> 3개의 masks 중 GT와 가장 유사한(confidence score가 가장 높은) mask의 loss만 사용</p> </li> </ul> <h2 id="model">Model</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/3-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/3-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Image Encoder : MAE (Masked AutoEncoder) 방식의 ViT<br/> MAE 요약 : 이미지를 grid로 나누고 patches 중 일부를 가린 뒤 원본을 복원하도록 학습하고, 학습이 끝난 후에는 encoder embedding만 사용<br/> ViT-H/16 : 14 \(\times\) 14 windowed attention and 4 global attention blocks</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/4-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/4-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Prompt Encoder :<br/> Mask (dense prompt) : conv. 거친 후 image embedding에 pixel-wise sum (mask가 없는 pixel의 경우 ‘no mask’ prompt 사용)<br/> Point (sparse prompt) : positional encoding + learned embedding(fg or bg)<br/> BB (sparse prompt) : positional encoding + learned embedding(top-left or bottom-right)<br/> text (sparse prompt) : by CLIP text encoder</p> </li> <li> <p>Loss :</p> <ol> <li>Mask loss : related to mask prediction<br/> 1-1. focal loss : \(L(p_{t}) = - (1-p_{t})^{r}log(p_{t})\) where \((1-p_{t})^{r}\) gives more weight to few hard examples (\(p_{t} \sim 0\))<br/> 1-2. dice loss : 1 - dice score where dice score = \(\frac{2 \times Area(A \cap B)}{Area(A) + Area(B)}\)</li> </ol> </li> </ul> <ol> <li>IoU loss : related to confidence score<br/> MSE loss</li> </ol> <h2 id="data--develop-data-engine-by-curriculum-learning">Data : Develop Data Engine by Curriculum Learning</h2> <ul> <li> <p>Assisted-manual stage :<br/> public segmentation dataset \(\rightarrow\) SAM \(\rightarrow\) pixel-wise manual augmentation \(\rightarrow\) re-train<br/> After re-training, the number of masks per image increased from 20 to 44 in average<br/> Collect 4.3M masks from 0.12M images</p> </li> <li> <p>Semi-automatic stage :<br/> dataset from previous stage (4.3M masks) \(\rightarrow\) SAM \(\rightarrow\) mask predict 실패한(제외된) object를 annotate \(\rightarrow\) re-train<br/> After re-training, the number of masks per image increased from 44 to 72 in average<br/> Collect 5.9M masks from 0.18M images (totally 4.3M + 5.9M = 10.2M masks)</p> </li> <li> <p>Fully-automatic stage :<br/> dataset from previous stage (10.2M masks) : image에 32 \(\times\) 32 grid points 찍음 \(\rightarrow\) SAM<br/> ambiguity-aware training (whole, part, sub-part 구분 가능)<br/> After filtering masks with high confidence score,<br/> Collect SA-1B dataset : 1.1B masks from 11M images (various HR masks)<br/> 99.1% is fully-automatically generated<br/> follow RAI (Responsible AI) : no bias and blur human faces</p> </li> </ul> <h2 id="task">Task</h2> <p>generalizable (zero-shot transfer to various tasks)</p> <ul> <li>Zero-Shot Transfer Tasks : <ol> <li>Zero-Shot Single Point Valid Mask Evaluation</li> <li>Zero-Shot Edge Detection</li> <li>Zero-Shot Object Proposals</li> <li>Zero-Shot Instance Segmentation</li> <li>Zero-Shot Text-to-Mask (CLIP)</li> </ol> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/4-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/4-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Zero-Shot Single Point Valid Mask Evaluation :<br/> point 찍었을 때 그에 해당하는 mask를 얼마나 잘 생성하는가<br/> use one most-confident mask<br/> compare with RITM model on 23 datasets</p> </li> <li> <p>Zero-Shot Edge Detection :</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/6-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/6-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>About filter : 블로그 맨 아랫 부분에 설명해놓음</p> <ul> <li> <p>Zero-Shot Object Proposals :<br/> mask 예측 후 object의 identity(class)를 얼마나 잘 맞추는가</p> </li> <li> <p>Zero-Shot Instance Segmentation :</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/7-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/7-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Zero-Shot Text-to-Mask :<br/> image \(\rightarrow\) CLIP \(\rightarrow\) image embedding as input<br/> text \(\rightarrow\) CLIP \(\rightarrow\) text embedding as SAM prompt</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/8-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/8-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>SAM’s latent space에서 similar mask embedding vectors within threshold를 추출한 결과 실제로도 semantically similar</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/9-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/9-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A query is indicated by magenta box : top row shows matches at a low threshold and bottom row shows matches at a high threshold </div> <h2 id="code-review">Code Review</h2> <p>다음에 해야지… 라고 미뤄둠..ㅎㅎ</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/10-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/10-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/11-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/11-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/11.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/12-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/12-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/13-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/13-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/13.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/14-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/14-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/14.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/15-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/15-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/15.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/16-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/16-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/16-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/16.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/17-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/17-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/17-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/17.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/18-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/18-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/18-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/18.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/19-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/19-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/19-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/19.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/20-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/20-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/20-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/20.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/21-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/21-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/21-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/21.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/22-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/22-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/22-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/22.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/23-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/23-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/23-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/23.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/24-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/24-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/24-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/24.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/25-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/25-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/25-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/25.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/26-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/26-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/26-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/26.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/27-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/27-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/27-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/27.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/28-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/28-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/28-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/28.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/29-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/29-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/29-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/29.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/30-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/30-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/30-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/30.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/31-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/31-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/31-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/31.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/32-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/32-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/32-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/32.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/33-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/33-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/33-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/33.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/34-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/34-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/34-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/34.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/35-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/35-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/35-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/35.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/36-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/36-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/36-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/36.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/37-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/37-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/37-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/37.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/38-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/38-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/38-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/38.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/39-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/39-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/39-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/39.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/40-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/40-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/40-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/40.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/41-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/41-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/41-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/41.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container>]]></content><author><name></name></author><category term="cv-tasks"/><category term="image"/><category term="segmentation"/><summary type="html"><![CDATA[Promptable Image Segmentation]]></summary></entry><entry><title type="html">FMANet</title><link href="https://semyeong-yu.github.io/blog/2024/FMANet/" rel="alternate" type="text/html" title="FMANet"/><published>2024-05-02T14:00:00+00:00</published><updated>2024-05-02T14:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/FMANet</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/FMANet/"><![CDATA[<h3 id="fma-net--flow-guided-dynamic-filtering-and-iterative-feature-refinement-with-multi-attention-for-joint-video-super-resolution-and-deblurring">FMA-Net : Flow-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring</h3> <h4 id="geunhyuk-youk-jihyong-oh-munchurl-kim">Geunhyuk Youk, Jihyong Oh, Munchurl Kim</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2401.03707">https://arxiv.org/abs/2401.03707</a><br/> project website :<br/> <a href="https://kaist-viclab.github.io/fmanet-site/">https://kaist-viclab.github.io/fmanet-site/</a><br/> pytorch code :<br/> <a href="https://github.com/KAIST-VICLab/FMA-Net">https://github.com/KAIST-VICLab/FMA-Net</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> </blockquote> <hr/> <img-comparison-slider> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/9-480.webp 480w,/assets/img/2024-05-02-FMANet/9-800.webp 800w,/assets/img/2024-05-02-FMANet/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/10-480.webp 480w,/assets/img/2024-05-02-FMANet/10-800.webp 800w,/assets/img/2024-05-02-FMANet/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> <img-comparison-slider> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/11-480.webp 480w,/assets/img/2024-05-02-FMANet/11-800.webp 800w,/assets/img/2024-05-02-FMANet/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/12-480.webp 480w,/assets/img/2024-05-02-FMANet/12-800.webp 800w,/assets/img/2024-05-02-FMANet/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> <h2 id="abstract">Abstract</h2> <p><strong>Task : Joint learning of VSRDB (<code class="language-plaintext highlighter-rouge">video super-resolution and deblurring</code>)</strong></p> <ul> <li>restore HR video from blurry LR video<br/> challenging because should handle two types of degradation (SR and deblurring) simultaneously</li> <li>super-resolution : LR vs HR</li> <li>deblurring : blurry vs sharp</li> </ul> <p><strong>FGDF (<code class="language-plaintext highlighter-rouge">flow-guided dynamic filtering</code>)</strong></p> <ul> <li>precise estimation of both <code class="language-plaintext highlighter-rouge">spatio-temporally-variant</code> <code class="language-plaintext highlighter-rouge">degradation</code> and <code class="language-plaintext highlighter-rouge">restoration</code> kernels that are aware of motion trajectories (not stick to fixed positions)</li> <li>effectively <code class="language-plaintext highlighter-rouge">handle large motions with small-sized kernels</code> (naive dynamic filtering의 한계 극복)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/2-480.webp 480w,/assets/img/2024-05-02-FMANet/2-800.webp 800w,/assets/img/2024-05-02-FMANet/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>DCN (Deformable Conv.) : learn position-invariant \(n \times n\) filter coeff.<br/> vs<br/> DF (Dynamic filtering) : learn position-wise \(n \times n\) dynamic filter coeff.</p> <p>DF (Dynamic Filtering) : fixed surroundings<br/> vs<br/> FGDF (Flow Guided DF) : variable surroundings by learned optical flow</p> <p><strong>FRMA (<code class="language-plaintext highlighter-rouge">iterative feature refinement with multi-attention</code>)</strong></p> <p>refine features by iterative updates<br/> loss : TA (temporal anchor)<br/> multi-attention :</p> <ul> <li><code class="language-plaintext highlighter-rouge">center-oriented</code> attention (focus on target frame)</li> <li><code class="language-plaintext highlighter-rouge">degradation-aware</code> attention (use degradation kernels in globally adaptive manner)</li> </ul> <hr/> <h2 id="related-work">Related Work</h2> <p><strong>VSR (Video Super-Resolution)</strong></p> <p>Based on the number of input frames,</p> <ol> <li><code class="language-plaintext highlighter-rouge">sliding window</code>-based method : recover HR frames by using neighboring frames within a sliding window<br/> use CNN, optical flow estimation, deformable conv., or transformer focusing on temporal alignment<br/> vs</li> <li><code class="language-plaintext highlighter-rouge">recurrent</code>-based method : sequentially propagate the latent features of one frame to the next frame<br/> Chan et al. <d-cite key="vsr">[1]</d-cite> BasicVSR++ : combine bidirectional propagation of past and future frames into current frame features<br/> limit : gradient vanishing</li> </ol> <p><strong>DB (Video Deblurring)</strong></p> <p>Zhang et al. <d-cite key="adversarial">[2]</d-cite> 3D CNN<br/> Li et al. <d-cite key="groupshift">[3]</d-cite> grouped spatial-temporal shifts<br/> transformer-based : Restormer <d-cite key="restormer">[4]</d-cite>, Stripformer <d-cite key="stripformer">[5]</d-cite>, RVRT <d-cite key="rvrt">[6]</d-cite></p> <p><strong>Joint learning of VSRDB (not sequential cascade of VSR and DB)</strong></p> <p>Previous works are mostly designed for ISRDB</p> <p>Fang et al. <d-cite key="HOFFR">[7]</d-cite> HOFFR : the first deep-learning-based VSRDB<br/> limit : struggle to deblur spatially-variant motion blur because 2D CNN has spatially-equivariant and input-independent filters</p> <p><strong>Dynamic Filter Network</strong></p> <p>predict spatially-variant degradation or restoration kernels</p> <p>Zhou et al. <d-cite key="adaptivefilter">[8]</d-cite> :<br/> spatially adaptive deblurring filter for recurrent video deblurring<br/> Kim et al. <d-cite key="koalanet">[9]</d-cite> KOALAnet :<br/> blind SR predicts spatially-variant degradation and upsampling filters</p> <ul> <li>limit : apply dynamic filtering only to the reference frame (target position and its fixed surrounding neighbors), so cannot accurately exploit spatio-temporally-variant-motion info. from adjacent frames</li> <li>limit : if apply dynamic filtering to adjacent frames \(\rightarrow\) large-sized filters are required to capture large motions \(\rightarrow\) high computational complexity</li> <li>limit : <d-cite key="separableconv">[10]</d-cite> suggested two separable large 1D kernels to approximate a large 2D kernel \(\rightarrow\) does not capture fine detail, so inappropriate for video</li> </ul> <hr/> <h2 id="method">Method</h2> <p><strong>Overview</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/1-480.webp 480w,/assets/img/2024-05-02-FMANet/1-800.webp 800w,/assets/img/2024-05-02-FMANet/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>FMA-Net : VSRDB framework based on FGDF and FRMA<br/> allow for small-to-large motion representation learning</p> <ul> <li>input : <code class="language-plaintext highlighter-rouge">blurry LR sequence</code> \(X = \left\lbrace X_{c-N}:X_{c+N} \right\rbrace \in R^{T \times H \times W \times 3}\) where \(T=2N+1\) and \(c\) is a center frame index</li> <li>goal : predict <code class="language-plaintext highlighter-rouge">sharp HR center frame</code> \(\hat Y_{c} \in R^{sH \times sW \times 3}\) where \(s\) is SR scale factor</li> </ul> <ol> <li><code class="language-plaintext highlighter-rouge">degradation</code> learning network \(Net^{D}\) : learn <code class="language-plaintext highlighter-rouge">motion-aware</code> <code class="language-plaintext highlighter-rouge">spatio-temporally-variant</code> degradation kernels</li> <li><code class="language-plaintext highlighter-rouge">restoration</code> network \(Net^{R}\) : utilize these degradation kernels in a globally adaptive manner to restore center frame \(X_c\)</li> <li>\(Net^{D}\) and \(Net^{R}\) consist of FRMA blocks and FGDF module</li> </ol> <p><strong>FRMA block</strong></p> <p>pre-trained optical flow network : unstable for blurry frames and computationally expensive</p> <p>vs</p> <blockquote> <p>FRMA block :<br/> learn <code class="language-plaintext highlighter-rouge">self-induced</code> optical flow in a residual learning manner<br/> learn <code class="language-plaintext highlighter-rouge">multiple</code> optical flows with corresponding occlusion masks<br/> \(\rightarrow\) flow diversity enables to learn one-to-many relations b.w. pixels in a target frame and its neighbor frames<br/> \(\rightarrow\) beneficial since <code class="language-plaintext highlighter-rouge">blurry frame's pixel info. is spread due to light accumulation</code></p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/3-480.webp 480w,/assets/img/2024-05-02-FMANet/3-800.webp 800w,/assets/img/2024-05-02-FMANet/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Three features</p> <ol> <li>\(F \in R^{T \times H \times W \times C}\) :<br/> <code class="language-plaintext highlighter-rouge">temporally-anchored (unwarped)</code> feature at each frame index \(0 \sim T-1\)<br/> dim. T에 걸친 전체 feature</li> <li>\(F_w \in R^{H \times W \times C}\) :<br/> <code class="language-plaintext highlighter-rouge">warped</code> feature<br/> target frame feature 관련</li> <li>\(\boldsymbol f = \left \lbrace f_{c \rightarrow c+t}^{j}, o_{c \rightarrow c+t}^{j} \right \rbrace _{j=1:n}^{t=-N:N} \in R^{T \times H \times W \times (2+1)n}\) :<br/> multi-<code class="language-plaintext highlighter-rouge">flow-mask</code> pairs<br/> \(f_{c \rightarrow c+t}^{j}\) : learnable optical flow<br/> \(o_{c \rightarrow c+t}^{j}\) : learnable occlusion mask (sigmoid for stability)<br/> \(n\) is the number of multi-flow-mask pairs from the center frame index \(c\) to each frame index<br/> <code class="language-plaintext highlighter-rouge">왜 dim. (2+1)???</code> \(\rightarrow\) optical flow \(R^2\) and occlusion mask \(R^1\)</li> </ol> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/16-480.webp 480w,/assets/img/2024-05-02-FMANet/16-800.webp 800w,/assets/img/2024-05-02-FMANet/16-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>(i+1)-th Feature Refinement : 위첨자로 표기<br/> <code class="language-plaintext highlighter-rouge">feature refine 식 기원??</code> \(\rightarrow\) BasicVSR++에서 아이디어 따와서 iterative하게 변형</p> <ol> <li>\(F^{i+1}\)=RDB(\(F^{i}\)) :<br/> RDB <d-cite key="RDB">[11]</d-cite></li> <li>\(\boldsymbol f^{i+1}\) = \(\boldsymbol f^{i}\) + Conv3d(concat(\(\boldsymbol f^{i}\), \(W\)(\(F^{i+1}\), \(\boldsymbol f^{i}\)), \(F_{c}^{0}\)))<br/> \(W\)(\(F^{i+1}\), \(\boldsymbol f^{i}\)) : warp \(F^{i+1}\) to center frame index \(c\) based on \(f^{i}\)<br/> \(W\) : occlusion-aware backward warping<br/> concat : along channel dim.<br/> \(F_{c}^{0} \in R^{H \times W \times C}\) : feature map at center frame index \(c\) of the initial feature \(F^{0} \in R^{T \times H \times W \times C}\)</li> <li>\(\tilde F_{w}^{i}\) = Conv2d(concat(\(F_{w}^{i}\), \(r_{4 \rightarrow 3}\)(\(W\)(\(F^{i+1}\), \(\boldsymbol f^{i+1}\)))))<br/> \(r_{4 \rightarrow 3}\) : reshape from \(R^{T \times H \times W \times C}\) to \(R^{H \times W \times TC}\) for feature aggregation</li> <li>\(F_w^{i+1}\) = Multi-Attn(\(\tilde F_{w}^{i}\), \(F_{c}^{0}\)(, \(k^{D, i}\)))</li> </ol> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/15-480.webp 480w,/assets/img/2024-05-02-FMANet/15-800.webp 800w,/assets/img/2024-05-02-FMANet/15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>RDB Network <d-cite key="RDB">[11]</d-cite> :<br/> TBD</p> </blockquote> <blockquote> <p>RRDB Network <d-cite key="rrdb">[15]</d-cite> :<br/> TBD</p> </blockquote> <blockquote> <p>Occlusion-Aware Backward Warping <d-cite key="warp">[12]</d-cite> <d-cite key="warpp">[13]</d-cite> <d-cite key="warppp">[14]</d-cite> :<br/> TBD</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/3-480.webp 480w,/assets/img/2024-05-02-FMANet/3-800.webp 800w,/assets/img/2024-05-02-FMANet/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Multi-Attention :</p> <ul> <li><code class="language-plaintext highlighter-rouge">CO(center-oriented)</code> attention :<br/> better align \(\tilde F_{w}^{i}\) to \(F_{c}^{0}\) (center feature map of initial temporally-anchored feature)</li> <li><code class="language-plaintext highlighter-rouge">DA(degradation-aware)</code> attention :<br/> \(\tilde F_{w}^{i}\) becomes globally adaptive to spatio-temporally variant degradation by using degradation kernels \(K^{D}\)</li> </ul> </blockquote> <ul> <li> <p>CO attention :<br/> \(Q=W_{q} F_{c}^{0}\)<br/> \(K=W_{k} \tilde F_{w}^{i}\)<br/> \(V=W_{v} \tilde F_{w}^{i}\)<br/> \(COAttn(Q, K, V) = softmax(\frac{QK^{T}}{\sqrt{d}})V\)<br/> 실험 결과, \(\tilde F_{w}^{i}\)가 자기 자신(self-attention)이 아니라 \(F_{c}^{0}\)과의 relation에 집중할 때 better performance</p> </li> <li> <p>DA attention :<br/> CO attention과 비슷하지만,<br/> Query 만들 때 \(F_{c}^{0}\) 대신 \(k^{D, i}\) 사용<br/> \(\tilde F_{w}^{i}\) becomes globally adaptive to spatio-temporally-variant degradation<br/> \(k^{D, i} \in R^{H \times W \times C}\) : degradation features adjusted by conv. with \(K^{D}\) (motion-aware spatio-temporally-variant degradation kernels) 에 대해<br/> \(Q=W_{q} k^{D, i}\)<br/> DA attention은 \(Net^{D}\) 말고 \(Net^{R}\) 에서만 사용</p> </li> </ul> <p><strong>FGDF</strong></p> <ul> <li> <p>spatio-temporal Dynamic Filter :<br/> \(y(p) = \sum_{t=-N}^{N} \sum_{k=1}^{n^2} F_{c+t}^{p}(p_k) x_{c+t}(p+p_k)\)<br/> where<br/> \(c\) : center frame index<br/> \(p_k \in \{ (- \lfloor \frac{n}{2} \rfloor, - \lfloor \frac{n}{2} \rfloor), \cdots , (\lfloor \frac{n}{2} \rfloor, \lfloor \frac{n}{2} \rfloor) \}\) : sampling offset for conv. with \(n \times n\) kernel<br/> \(F \in R^{T \times H \times W \times n^{2}}\) : predicted \(n \times n\) dynamic filter<br/> \(F^p \in R^{T \times n^{2}}\) : predicted \(n \times n\) dynamic filter at position p</p> </li> <li> <p>limit :<br/> fixed position (\(p\)) and fixed surrounding neighbors (\(p_k\))<br/> \(\rightarrow\) To capture large motion, require large-sized filter</p> </li> </ul> <blockquote> <p>solution : <code class="language-plaintext highlighter-rouge">FGDF</code><br/> kernels - dynamically generated / pixel-wise (position-wise) / variable surroundings guided by optical flow<br/> \(\rightarrow\) can handle large motion with relatively small-sized filter<br/> \(y(p) = \sum_{t=-N}^{N} \sum_{k=1}^{n^2} F_{c+t}^{p}(p_k) x_{c+t}^{\ast}(p+p_k)\)<br/> where <br/> \(x_{c+t}^{\ast} = W(x_{c+t}, \boldsymbol f_{c+t})\) : <code class="language-plaintext highlighter-rouge">warped input feature</code> based on \(\boldsymbol f_{c+t}\)<br/> \(\boldsymbol f_{c+t}\) : <code class="language-plaintext highlighter-rouge">flow-mask pair</code> from frame index \(c\) to \(c+t\)</p> </blockquote> <p><strong>Overall Architecture</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/1-480.webp 480w,/assets/img/2024-05-02-FMANet/1-800.webp 800w,/assets/img/2024-05-02-FMANet/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Degradation Network \(Net^{D}\)<br/> input : blurry LR sequence \(\boldsymbol X\) and sharp HR sequence \(\boldsymbol Y\)<br/> goal : <code class="language-plaintext highlighter-rouge">predict flow and degradation kernels in sharp HR sequence</code> \(\boldsymbol Y\)</p> <ol> <li>an image flow-mask pair \(\boldsymbol f^{Y}\)</li> <li>motion-aware spatio-temporally-variant degradation kernels \(K^{D}\)<br/> \(\rightarrow\) obtain blurry LR center frame \(\boldsymbol X_{c}\) from sharp HR counterpart \(\boldsymbol Y\)</li> </ol> </blockquote> <ul> <li> <p>step 1-1. initialize<br/> RRDB : <d-cite key="rrdb">[15]</d-cite><br/> \(\boldsymbol X \rightarrow\) 3D RRDB \(\rightarrow F^{0}\)</p> </li> <li> <p>step 1-2. initialize<br/> \(F_{w}^{0} = 0\), \(\boldsymbol f = \left \lbrace f_{c \rightarrow c+t}^{j} = 0, o_{c \rightarrow c+t}^{j} = 1 \right \rbrace _{j=1:n}^{t=-N:N}\)</p> </li> <li> <p>step 2. M FRMA blocks<br/> \(F^{0}, F_{w}^{0}, \boldsymbol f^{0} \rightarrow\) \(M\) FRMA blocks \(\rightarrow F^{M}, F_{w}^{M}, \boldsymbol f^{M}\)</p> </li> <li> <p>step 3-1. <code class="language-plaintext highlighter-rouge">an</code> image flow-mask pair \(\boldsymbol f^{Y} \in R^{T \times H \times W \times (2+1) 1}\)<br/> \(\boldsymbol f^{M} \rightarrow\) Conv3d \(\rightarrow \boldsymbol f^{Y}\)</p> </li> <li> <p>step 3-2. \(\hat X_{sharp}^{D}\) only used in Temporal Anchor (TA) loss<br/> \(F^{M} \rightarrow\) Conv3d \(\rightarrow \hat X_{sharp}^{D} \in R^{T \times H \times W \times 3}\) in image domain</p> </li> <li> <p>step 3-3. motion-aware spatio-temporally-variant degradation kernels \(K^{D} \in R^{T \times H \times W \times k_{d}^{2}}\)<br/> \(K^{D}\) = softmax(Conv3d(\(r_{3 \rightarrow 4}\)(\(F_{w}^{M}\))))<br/> where<br/> \(k_{d}\) : degradation kernel size<br/> sigmoid for normalization : all kernels have <code class="language-plaintext highlighter-rouge">positive</code> values, which mimics <code class="language-plaintext highlighter-rouge">blur generation process</code></p> </li> <li> <p>step 4. FGDF downsampling to predict blurry center frame \(\hat X_{c}\)<br/> \(\hat X_{c}\) = \(W(\boldsymbol Y, s (\boldsymbol f^{Y} \uparrow _{s}))\) \(\circledast K^{D} \downarrow _{s}\)<br/> where<br/> \(\uparrow\) : \(\times s\) bilinear upsampling<br/> \(W(\boldsymbol Y, s (\boldsymbol f^{Y} \uparrow _{s}))\) : warped sharp HR sequence based on an upsampled image flow-mask pair<br/> \(\circledast K^{D} \downarrow _{s}\) : FGDF with filter \(K^{D}\) with stride \(s\)</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/17-480.webp 480w,/assets/img/2024-05-02-FMANet/17-800.webp 800w,/assets/img/2024-05-02-FMANet/17-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/17.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/1-480.webp 480w,/assets/img/2024-05-02-FMANet/1-800.webp 800w,/assets/img/2024-05-02-FMANet/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Restoration Network \(Net^{R}\)<br/> input : blurry LR sequence \(\boldsymbol X\) and \(F^{M}, \boldsymbol f^{M}, K^{D}\) from \(Net^{D}\)<br/> goal : <code class="language-plaintext highlighter-rouge">predict flow and restoration kernels in blurry LR sequence</code> \(\boldsymbol X\)</p> <ol> <li>an image flow-mask pair \(\boldsymbol f^{X}\)</li> <li>restoration kernels \(K^{R}\)<br/> \(\rightarrow\) obtain sharp HR center frame \(\hat Y_{c}\) from blurry LR counterpart \(\boldsymbol X\)</li> </ol> </blockquote> <ul> <li> <p>step 1-1. initialize \(F^{0}\)<br/> RRDB : <d-cite key="rrdb">[15]</d-cite><br/> concat(\(\boldsymbol X\), \(F^{M}\) from \(Net^{D}\)) \(\rightarrow\) 3D RRDB \(\rightarrow\) \(F^{0}\)</p> </li> <li> <p>step 1-2. initialize \(F_{w}^{0}\), \(\boldsymbol f^{0}\)<br/> \(F_{w}^{0} = 0\), \(\boldsymbol f^{0} = \boldsymbol f^{M}\) from \(Net^{D}\)</p> </li> <li> <p>step 2-1. compute \(k^{D, i} \in R^{H \times W \times C}\) for DA attention</p> </li> <li> <p>step 2-2. M FRMA blocks<br/> \(F^{0}, F_{w}^{0}, \boldsymbol f^{0}, k^{D, i} \rightarrow\) \(M\) FRMA blocks \(\rightarrow F^{M}, F_{w}^{M}, \boldsymbol f^{M}\)</p> </li> <li> <p>step 3-1. <code class="language-plaintext highlighter-rouge">an</code> image flow-mask pair \(\boldsymbol f^{X} \in R^{T \times H \times W \times (2+1) 1}\)<br/> \(\boldsymbol f^{M} \rightarrow\) Conv3d \(\rightarrow \boldsymbol f^{X}\)</p> </li> <li> <p>step 3-2. \(\hat X_{sharp}^{R}\) only used in Temporal Anchor (TA) loss<br/> \(F^{M} \rightarrow\) Conv3d \(\rightarrow \hat X_{sharp}^{R} \in R^{T \times H \times W \times 3}\) in image domain</p> </li> <li> <p>step 3-3. motion-aware spatio-temporally-variant \(\times s\) upsampling and restoration kernels \(K^{R} \in R^{T \times H \times W \times s^{2} k_{r}^{2}}\)<br/> \(K^{R}\) = Normalize(Conv3d(\(r_{3 \rightarrow 4}\)(\(F_{w}^{M}\))))<br/> where<br/> \(k_{r}\) : restoration kernel size<br/> Normalize : w.r.t all kernels at temporally co-located positions over \(X\) (\(T\) dim.에 대해 normalize)</p> </li> <li> <p>step 3-4. high-frequency detail \(\hat Y_{r}\)<br/> \(F_{w}^{M} \rightarrow\) stacked conv. and pixel shuffle \(\rightarrow \hat Y_{r}\)</p> </li> <li> <p>step 4. FGDF upsampling to predict sharp center frame \(\hat Y_{c}\)<br/> \(\hat Y_{c}\) = \(\hat Y_{r}\) + \(W(\boldsymbol X, \boldsymbol f^{X})\) \(\circledast K^{D} \uparrow _{s}\)<br/> where<br/> \(W(\boldsymbol X, \boldsymbol f^{X})\) : warped blurry LR sequence based on an image flow-mask pair<br/> \(\circledast K^{D} \uparrow _{s}\) : \(\times s\) dynamic upsampling with kernel \(K^{R}\)</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/17-480.webp 480w,/assets/img/2024-05-02-FMANet/17-800.webp 800w,/assets/img/2024-05-02-FMANet/17-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/17.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Training</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/4-480.webp 480w,/assets/img/2024-05-02-FMANet/4-800.webp 800w,/assets/img/2024-05-02-FMANet/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Stage 1. Pre-train \(Net^{D}\)</p> <ul> <li>loss 1. <code class="language-plaintext highlighter-rouge">reconstruction loss</code> for blurry LR \(X_{c}\)<br/> \(\hat X_{c}\) \(\leftrightarrow\) \(X_{c}\)</li> <li>loss 2. <code class="language-plaintext highlighter-rouge">optical flow warping loss</code> (warping from c to c+t) in \(\boldsymbol Y\)<br/> \(W(Y_{t+c}, s (\boldsymbol f_{t+c}^{Y} \uparrow _{s}))\) \(\leftrightarrow\) \(Y_{c}\)</li> <li>loss 3. <code class="language-plaintext highlighter-rouge">optical flow refining loss</code> in \(\boldsymbol Y\)<br/> \(f^{Y}\) \(\leftrightarrow\) \(f_{RAFT}^{Y}\)<br/> where<br/> \(f^{Y}\) is image optical flow (no occlusion mask) contained in \(\boldsymbol f^{Y}\)<br/> \(f_{RAFT}^{Y}\) is pseudo-GT optical flow by pre-trained RAFT model <d-cite key="Raft">[16]</d-cite></li> <li>loss 4. <code class="language-plaintext highlighter-rouge">Temporal Anchor (TA) loss</code> for sharp LR \(X_{sharp}\)<br/> <code class="language-plaintext highlighter-rouge">It anchors and sharpens each feature w.r.t corresponding frame index</code><br/> \(\hat X_{sharp}^{D}\) \(\leftrightarrow\) \(X_{sharp}\)<br/> where<br/> sharp HR sequence \(\boldsymbol Y \rightarrow\) bicubic downsampling \(\rightarrow\) GT sharp LR sequence \(X_{sharp}\)<br/> \(\rightarrow\) keep each feature temporally anchored for the corresponding frame index<br/> \(\rightarrow\) constrain the solution space to distinguish warped and unwarped features<br/> <code class="language-plaintext highlighter-rouge">???</code><br/> \(\rightarrow\) iteratively 학습하다보니 frame 0, 1, 2의 features인 \(F \in R^{T \times H \times W \times C}\) 가 점점 target frame 1의 feature인 \(F_w\) 에 가깝게 frame 0.7,, 1, 1.3 느낌으로 업데이트됨<br/> \(\rightarrow\) \(F \in R^{T \times H \times W \times C}\) 의 특성을 유지하도록 downsampled \(\boldsymbol Y\)와 비교하는 Temporal Anchor (TA) loss 추가!</li> </ul> <blockquote> <p>RAFT: Recurrent all-pairs field transforms for optical flow <d-cite key="Raft">[16]</d-cite> :<br/> 핵심 아이디어 : TBD</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/5-480.webp 480w,/assets/img/2024-05-02-FMANet/5-800.webp 800w,/assets/img/2024-05-02-FMANet/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Stage 2. Jointly train \(Net^{D}\) and \(Net^{R}\)</p> <ul> <li>loss 1. <code class="language-plaintext highlighter-rouge">restoration loss</code> for sharp HR \(Y_{c}\)<br/> \(\hat Y_{c}\) \(\leftrightarrow\) \(Y_{c}\)</li> <li>loss 2. <code class="language-plaintext highlighter-rouge">optical flow warping loss</code> (warping from c to c+t) in \(\boldsymbol X\)<br/> Stage 1.의 loss 2.와 동일한 원리</li> <li>loss 3. <code class="language-plaintext highlighter-rouge">Temporal Anchor (TA) loss</code> for sharp LR \(X_{sharp}\)<br/> Stage 1.의 loss 4.와 동일한 원리</li> <li>loss 4. \(L_{D}\)<br/> Stage 1.의 loss들<br/> <code class="language-plaintext highlighter-rouge">왜 X optical flow에 대해선 RAFT loss 안 했지??</code><br/> \(\rightarrow\) RAFT model에서 구한 optical flow는 sharp HR sequence에 대한 거라서!</li> </ul> <hr/> <h2 id="results">Results</h2> <p><strong>Settings</strong></p> <p>LR patch size : 64 \(\times\) 64<br/> the number of FRMA blocks : \(M\) = 4<br/> the number of multi-flow-mask pairs : \(n\) = 9<br/> degradation and restoration kernel size : \(k_{d}\), \(k_{r}\) = 20, 5<br/> the number of frames in sequence : \(T\) = 3 (\(N\) = 1)<br/> ratio b.w. HR and LR : \(s\) = 4<br/> multi-attention block : utilize multi-Dconv head transposed attention (MDTA) and Gated-Dconv feed-forward network (GDFN) from Restormer <d-cite key="restormer">[4]</d-cite></p> <blockquote> <p>multi-Dconv head transposed attention and Gated-Dconv feed-forward network <d-cite key="restormer">[4]</d-cite> :<br/> TBD</p> </blockquote> <p><strong>Datasets and Evaluation Metrics</strong></p> <ul> <li> <p>Datasets :<br/> REDS dataset : train and test<br/> GoPro and YouTube dataset : test (generalization)<br/> \(\rightarrow\) spatially bicubic downsampling to make LR sequence and temporally downsampling to make lower fps sequence</p> </li> <li> <p>Evaluation Metrics :<br/> PSNR and SSIM for image quality<br/> tOF for temporal consistency</p> </li> </ul> <p><strong>Comparision with SOTA</strong></p> <blockquote> <p>SOTA methods (SR) :<br/> single-image SR : SwinIR <d-cite key="swinir">[17]</d-cite> and HAT <d-cite key="hat">[18]</d-cite><br/> video SR : BasicVSR++ <d-cite key="vsr">[1]</d-cite> and FTVSR <d-cite key="ftvsr">[19]</d-cite></p> </blockquote> <blockquote> <p>SOTA methods (DB) :<br/> single-image deblurring : Restormer <d-cite key="restormer">[4]</d-cite> and FFTformer <d-cite key="fftformer">[20]</d-cite><br/> video deblurring : RVRT <d-cite key="rvrt">[6]</d-cite> and GShiftNet <d-cite key="gshiftnet">[21]</d-cite></p> </blockquote> <blockquote> <p>SOTA methods (VSRDB) :<br/> HOFFR <d-cite key="HOFFR">[7]</d-cite></p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/18-480.webp 480w,/assets/img/2024-05-02-FMANet/18-800.webp 800w,/assets/img/2024-05-02-FMANet/18-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/18.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>VSRDB methods have superior performance compared to sequential cascade of SR and DB<br/> \(\rightarrow\) SR and DB tasks are highly inter-correlated</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/7-480.webp 480w,/assets/img/2024-05-02-FMANet/7-800.webp 800w,/assets/img/2024-05-02-FMANet/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/8-480.webp 480w,/assets/img/2024-05-02-FMANet/8-800.webp 800w,/assets/img/2024-05-02-FMANet/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <p><strong>Ablation Study</strong></p> <ul> <li>FGDF<br/> FGDF is better than conventional dynamic filtering for all ranges of motion magnitudes<br/> conventional dynamic filtering is especially not good for large motion</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/13-480.webp 480w,/assets/img/2024-05-02-FMANet/13-800.webp 800w,/assets/img/2024-05-02-FMANet/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> PSNR/tOF according to the average optical flow magnitude b.w. two consecutive frames </div> <ul> <li>Design of FMA-Net <ol> <li>the number of multi-flow-mask pairs \(n\) \(\propto\) performance</li> <li>motion info. from multi-flow-mask pairs \(\boldsymbol f\) is better than motion info. from DCN (Deformable Conv.) due to self-induced sharper optical flows and occlusion masks</li> <li>RAFT loss and TA loss</li> <li>two-stage (\(Net^{D} \rightarrow\) both) training is better than end-to-end training</li> <li>multi-attention (CO + DA) is better than self-attention + SFT(spatial feature transform) <d-cite key="SFT">[22]</d-cite></li> </ol> </li> </ul> <blockquote> <p>SFT (spatial feature transform) <d-cite key="SFT">[22]</d-cite><br/> ddd</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/14-480.webp 480w,/assets/img/2024-05-02-FMANet/14-800.webp 800w,/assets/img/2024-05-02-FMANet/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="conclusion">Conclusion</h2> <p>VSRDB framework based on FGDF and FRMA</p> <ul> <li>FRMA :<br/> iteratively update features (e.g. self-induced optical flow)<br/> multi-attention (CO + DA attention)</li> <li>FGDF :<br/> predict flow-mask pair with flow-guided dynamic filters \(K^{D}\) and \(K^{R}\) that are aware of motion<br/> can handle large motion</li> <li>TA loss :<br/> temporally anchors and sharpens unwarped features</li> <li>2-stage training :<br/> because, during multi-attention of \(Net^{R}\), warped feature \(F_{w}\) is adjusted by predicted degradation \(K^{D}\) from \(Net^{D}\) in globally adaptive manner</li> </ul> <h2 id="limitation">Limitation</h2> <ul> <li>2-stage approach has longer training time than end-to-end approach</li> <li>In extreme contidions such as object rotation, it is hard to predict accurate optical flow<br/> \(\rightarrow\) learnable homography parameters or quaternion representations can be one option to handle rotational motions</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/19-480.webp 480w,/assets/img/2024-05-02-FMANet/19-800.webp 800w,/assets/img/2024-05-02-FMANet/19-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/19.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="super-resolution"/><category term="super-resolution"/><category term="deblur"/><category term="flow"/><category term="dynamic"/><category term="attention"/><summary type="html"><![CDATA[Flow-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring]]></summary></entry><entry><title type="html">NeRF</title><link href="https://semyeong-yu.github.io/blog/2024/NeRF/" rel="alternate" type="text/html" title="NeRF"/><published>2024-04-10T21:00:00+00:00</published><updated>2024-04-10T21:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/NeRF</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/NeRF/"><![CDATA[<h2 id="nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h2> <h4 id="ben-mildenhall-pratul-psrinivasan-matthew-tancik">Ben Mildenhall, Pratul P.Srinivasan, Matthew Tancik</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2003.08934">https://arxiv.org/abs/2003.08934</a><br/> project website :<br/> <a href="https://www.matthewtancik.com/nerf">https://www.matthewtancik.com/nerf</a><br/> pytorch code :<br/> <a href="https://github.com/yenchenlin/nerf-pytorch">https://github.com/yenchenlin/nerf-pytorch</a><br/> <a href="https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file">https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file</a><br/> tiny tensorflow code :<br/> <a href="https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb">https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb</a><br/> referenced blog :<br/> <a href="https://csm-kr.tistory.com/64">https://csm-kr.tistory.com/64</a><br/> <a href="https://yconquesty.github.io/blog/ml/nerf/nerf_rendering.html#the-rendering-formula">https://yconquesty.github.io/blog/ml/nerf/nerf_rendering.html#the-rendering-formula</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>여러 각도의 camera center에서 each input image pixel 방향으로 ray(r=o+td)를 쏜다.</li> <li>ray를 discrete points로 sampling한다.</li> <li>3D coordinate x와 viewing direction d를 r(x)와 r(d)로 positional encoding한다.</li> <li>r(x)를 MLP에 넣어 volume density를 얻고 여기에 r(d)까지 넣어 RGB color를 얻는다.</li> <li>coarse network와 fine network(hierarchical sampling) 각각에서 volume density와 color를 이용한 volume rendering으로 ray마다 rendering pixel color를 구한다.</li> </ol> </blockquote> <h2 id="introduction">Introduction</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/3-480.webp 480w,/assets/img/2024-04-10-NeRF/3-800.webp 800w,/assets/img/2024-04-10-NeRF/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="pipeline">Pipeline</h4> <ul> <li> <p>(a) march camera rays and generate sampling of 5D coordinates</p> </li> <li> <p>(b) represents volumetric static scene by optimizing continuous 5D function(fully-connected network)</p> </li> </ul> <ol> <li>input: single continuous <code class="language-plaintext highlighter-rouge">5D coordinate</code><br/> 3D location \(x, y, z\)<br/> 2D direction \(\theta, \phi\)</li> <li>output:<br/> <code class="language-plaintext highlighter-rouge">volume density</code> (differential opacity) (how much radiance is accumulated by a ray)<br/> <code class="language-plaintext highlighter-rouge">view-dependent RGB color</code> (emitted radiance) \(c = (r, g, b)\)</li> </ol> <ul> <li> <p>(c) synthesizes novel view by classic <code class="language-plaintext highlighter-rouge">volume rendering</code> techniques(differentiable) to accumulate(project)(composite) the color/density samples into 2D image along rays</p> </li> <li> <p>(d) loss between synthesized and GT observed images</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/2-480.webp 480w,/assets/img/2024-04-10-NeRF/2-800.webp 800w,/assets/img/2024-04-10-NeRF/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Pipeline of NeRF architecture </div> <h4 id="problem--solution">Problem &amp; Solution</h4> <p>Problem :</p> <ol> <li>not sufficiently high-resolution representation</li> <li>inefficient in the number of samples per camera ray</li> </ol> <p>Solution :</p> <ol> <li>input <code class="language-plaintext highlighter-rouge">positional encoding</code> for MLP to represent higher frequency function</li> <li><code class="language-plaintext highlighter-rouge">hierarchical sampling</code> to reduce the number of queries</li> </ol> <h4 id="contribution">Contribution</h4> <ul> <li>represent continuous scenes as 5D neural radiance fields with basic MLP to render high-resolution novel views</li> <li>differentiable volume rendering + hierarchical sampling</li> <li>positional encoding to map input 5D coordinate into higher dim. space for high-frequency scene representation</li> <li>overcome the storage costs of discretized voxel grids by encoding continuous volume into network’s parameters<br/> =&gt; require only storage costs of sampled volumetric representations</li> </ul> <h2 id="related-work">Related Work</h2> <h4 id="neural-3d-shape-representation">Neural 3D shape representation</h4> <ul> <li> <p>deep networks that map \(xyz\) coordinates to signed distance functions or occupancy fields<br/> =&gt; limit : need GT 3D geometry</p> </li> <li> <p>Niemeyer et al.<br/> =&gt; input : find directly surface intersection for each ray (can calculate exact derivatie)<br/> =&gt; output : diffuse color at each ray intersection location</p> </li> <li> <p>Sitzmann et al.<br/> =&gt; input : each 3D coordinate<br/> =&gt; output : feature vector and RGB color at each 3D coordinate<br/> =&gt; rendering by RNN that marches along each ray to decide where the surface is.</p> </li> </ul> <blockquote> <p>Limit : oversmoothed renderings, so limited to simple shapes with low geometric complexity</p> </blockquote> <h4 id="view-synthesis-and-image-based-rendering">View synthesis and image-based rendering</h4> <ul> <li> <p>Given <code class="language-plaintext highlighter-rouge">dense sampling of views</code>, novel view synthesis is possible by <code class="language-plaintext highlighter-rouge">simple light field sample interpolation</code></p> </li> <li> <p>Given <code class="language-plaintext highlighter-rouge">sparser sampling of views</code>, there are 2 ways :<br/> mesh-based representation and volumetric representation</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Mesh-based</code> representation with either diffuse(난반사) or view-dependent appearance :<br/> Directly optimize mesh representations by differentiable rasterizers or pathtracers so that we reproject and reconstruct images</p> </li> </ul> <blockquote> <p>Limit :<br/> gradient-based optimization is often difficult because of <code class="language-plaintext highlighter-rouge">local minima or discontinuities or poor loss landscape</code><br/> mesh 구조를 유지하면서 <code class="language-plaintext highlighter-rouge">gradient-based optimization하는 게 어렵</code><br/> needs a <code class="language-plaintext highlighter-rouge">template mesh</code> with fixed topology for initialization, which is unavailable in real-world</p> </blockquote> <ul> <li><code class="language-plaintext highlighter-rouge">Volumetric</code> representation :<br/> well-suited for gradient-based optimization and less distracting artifacts<br/> train : predict a sampled volumetric representation (voxel grids) from input images<br/> test : use alpha-(or learned-)compositing along rays to render novel views<br/> (alpha-compositing : 아래 volume rendering section에서 설명 예정)<br/> CNN compensates discretization artifacts from low resolution voxel grids or CNN allows voxel grids to vary on input time</li> </ul> <blockquote> <p>Limit :<br/> good results, but limited by poor time, space complexity due to discrete sampling<br/> \(\rightarrow\) discrete sampling : rendering high resol. image =&gt; finer sampling of 3D space</p> </blockquote> <blockquote> <p>Author’s solution :<br/> encode <code class="language-plaintext highlighter-rouge">continuous</code> volume into network’s parameters<br/> =&gt; higher quality rendering + require only storage cost of those <code class="language-plaintext highlighter-rouge">sampled</code> volumetric representations</p> </blockquote> <h2 id="neural-radiance-field-scene-representation">Neural Radiance Field Scene Representation</h2> <p>represent continuous scene by 5D MLP : (x, d) =&gt; (c, \(\sigma\))</p> <p>Here, there are 2 key-points!</p> <blockquote> <p>multiview consistent :<br/> c is dependent on both x and d, but \(\sigma\) is only dependent on location x<br/> 3D coordinate x =&gt; 8 fc-layers =&gt; volume-density and 256-dim. feature vector</p> </blockquote> <blockquote> <p>Lambertian reflection : diffuse(난반사) vs Specular reflection : 전반사</p> </blockquote> <blockquote> <p>non-Lambertian effects : view-dependent color change to represent specular reflection<br/> feature vector is concatenated with direction d =&gt; 1 fc-layer =&gt; view-dependent RGB color</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/6-480.webp 480w,/assets/img/2024-04-10-NeRF/6-800.webp 800w,/assets/img/2024-04-10-NeRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="volume-rendering-with-radiance-fields">Volume Rendering with Radiance Fields</h2> <h4 id="ray-from-input-image-pre-processing">Ray from input image (pre-processing)</h4> <p>We use <code class="language-plaintext highlighter-rouge">Ray</code> to synthesize <code class="language-plaintext highlighter-rouge">continuous</code>-viewpoint images from <code class="language-plaintext highlighter-rouge">discrete</code> input images</p> <blockquote> <p>\(r(t) = o + td\)<br/> o : camera’s center of projection<br/> d : viewing direction<br/> t \(\in [ t_n , t_f ]\) : distance from camera center b.w. camera’s predefined near and far planes</p> </blockquote> <blockquote> <p>How to calculate viewing direction d??</p> <ul> <li>2D pixel coordinate : \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)</li> <li>2D normalized coordinate (\(z = 1\)) by intrinsic matrix :<br/> \(\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\) = \(K^{-1}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\) = \(\begin{bmatrix} 1/f_x &amp; 0 &amp; W/2 \\ 0 &amp; 1/f_y &amp; H/2 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)<br/> Since \(y, z\) have opposite direction between the real-world coordinate and pixel coordinate, we multiply (-1)<br/> \(\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\) = \(\begin{bmatrix} 1/f_x &amp; 0 &amp; W/2 \\ 0 &amp; -1/f_y &amp; H/2 \\ 0 &amp; 0 &amp; -1 \end{bmatrix}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)<br/> Here, focal length in intrinsic matrix K is usually calculated using camear angle \(\alpha\) as \(\tan{\alpha / 2} = \frac{h/2}{f}\)</li> <li>3D coordinate by extrinsic matrix :<br/> For extrinsic matrix \([R \vert t']\),<br/> \(o = t'\)<br/> \(d = R * \begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\)<br/> Therefore, we can obtain \(r(t) = o + td\)</li> </ul> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/9-480.webp 480w,/assets/img/2024-04-10-NeRF/9-800.webp 800w,/assets/img/2024-04-10-NeRF/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="volume-rendering-from-mlp-output">Volume Rendering from MLP output</h4> <p>We use differential classical volume rendering</p> <blockquote> <p>idea : 빛이 들어올 확률이 클수록, 물체의 밀도가 높을수록 2D 이미지 픽셀에서 색상 채널의 값이 크게 나타나므로<br/> pixel’s color along a ray is equal to the integral of (transmittance) x (volume density) x (color) for all points along one ray.</p> </blockquote> <p><code class="language-plaintext highlighter-rouge">transmittance가 클수록 투명해서 no color에 가까우니까 색상 채널의 값이 작게 나타나야 하는 것 아니야?????</code></p> <blockquote> <p>Let ray \(r\) (traced through desired virtual camera) have near and far bounds \(t_n, t_f\)<br/> expected color of ray \(r\) = \(C(r) = \int_{t_n}^{t_f} T(t) \sigma (r(t)) c(r(t), d) dt\)</p> </blockquote> <ul> <li>\(T(t) = \exp(- \int_{t_n}^{t} \sigma (r(s)) ds)\) :<br/> <code class="language-plaintext highlighter-rouge">transmittance</code><br/> transmittance = 투과도 = <code class="language-plaintext highlighter-rouge">CDF</code> that a ray <code class="language-plaintext highlighter-rouge">does not hit</code> any particles from \(z_0\) to \(z\)<br/> 투과도가 클수록 투명 (no color)</li> <li>\(\sigma (r(t))\) : <code class="language-plaintext highlighter-rouge">volume density</code> along the ray (learned by MLP)<br/> volume density = <code class="language-plaintext highlighter-rouge">opacity</code> = 불투명도 = <code class="language-plaintext highlighter-rouge">extinction coefficient</code> = <code class="language-plaintext highlighter-rouge">alpha value</code> for alpha-compositing</li> <li>\(c(r(t), d)\) : object’s <code class="language-plaintext highlighter-rouge">color</code> along the ray (learned by MLP)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/10-480.webp 480w,/assets/img/2024-04-10-NeRF/10-800.webp 800w,/assets/img/2024-04-10-NeRF/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/11-480.webp 480w,/assets/img/2024-04-10-NeRF/11-800.webp 800w,/assets/img/2024-04-10-NeRF/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>volume rendering 식 유도 과정</p> </blockquote> <p>occluding objects are modeled as spherical particles with radius \(r\)<br/> There are \(A \cdot \Delta z \cdot \rho (z)\)개의 particles in the slice where \(\rho (z)\) is particle density (the number of particles per unit volume)</p> <p>Since solid particles do not overlap for \(\Delta z \rightarrow 0\),<br/> \(A \cdot \Delta z \cdot \rho (z) \cdot \pi r^2\)만큼 area is occluded<br/> 즉, cross section \(A\)에서 \(\frac{A \cdot \Delta z \cdot \rho (z) \cdot \pi r^2}{A} = \pi r^2 \cdot \rho (z) \cdot \Delta z\)의 비율만큼 occluded</p> <p>If \(\frac{A \cdot \Delta z \cdot \rho (z) \cdot \pi r^2}{A}\)만큼 rays are occluded, the light intensity decreases as<br/> \(I(z + \Delta z) = (1 - \pi r^2 \rho (z) \Delta z) \times I(z)\)</p> <p>Then the light density difference \(\Delta I = I(z + \Delta z) - I(z) = - \pi r^2 \rho (z) \Delta z \cdot I(z)\)<br/> 즉, \(dI(z) = - \pi r^2 \rho (z) I(z) dz = - \sigma (z) I(z) dz\)<br/> where <code class="language-plaintext highlighter-rouge">volume density (or opacity)</code> is \(\sigma(z) = \pi r^2 \rho (z)\)<br/> It makes sense because particle area와 particle density(particle 수)가 클수록 ray 감소량 (volume density)이 커지기 때문<br/> ODE 풀면, \(I(z) = I(z_0)\exp(- \int_{z_0}^{z} \sigma (r(s)) ds)\)</p> <p>Let’s define transmittance \(T(z) = \exp(- \int_{z_0}^{z} \sigma (r(s)) ds)\)<br/> where \(I(z) = I(z_0)T(z)\) means the <code class="language-plaintext highlighter-rouge">remainning</code> intensity after rays travel from \(z_0\) to \(z\)<br/> where <code class="language-plaintext highlighter-rouge">transmittance</code> \(T(z)\) means <code class="language-plaintext highlighter-rouge">CDF</code> that a ray <code class="language-plaintext highlighter-rouge">does not hit</code> any particles from \(z_0\) to \(z\)</p> <p>If a ray passes empty space, there is no color<br/> If a ray hits particles, there exists color (<code class="language-plaintext highlighter-rouge">radiance is emitted</code>)<br/> Let’s define \(H(z) = 1 - T(z)\), which means CDF that a ray <code class="language-plaintext highlighter-rouge">hits</code> particles from \(z_0\) to \(z\)<br/> CDF를 미분하면 PDF이므로<br/> Then PDF is \(p_{hit}(z) = \frac{dH}{dz} = - \frac{dT}{dz} = \exp(- \int_{z_0}^{z} \sigma (r(s)) ds) \sigma (z) = T(z) \sigma (z)\)</p> <p>Let a random variable \(R\) be the emitted randiance.<br/> Then PDF \(p_R(ray) = P[R = c(z)] = p_{hit}(z) = T(z) \sigma (z)\)<br/> Then the <code class="language-plaintext highlighter-rouge">color of a pixel is expected radiance for ray</code> bounded from \(t_n\) to \(t_f\)<br/> \(C(ray) = E[R] = \int_{t_n}^{t_f} R \cdot p_R dz = \int_{t_n}^{t_f} c \cdot p_{hit} dz = \int_{t_n}^{t_f} T(z) \sigma (z) c(z) dz\)</p> <p>\(t_n, t_f = 0., 1.\) for scaled-bounded and front-facing scenes after conversion to <code class="language-plaintext highlighter-rouge">NDC (normalized device coordinates)</code><br/> <a href="https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#analysis">How NDC Works?</a></p> <blockquote> <p>To apply the equation to our model by numerical quadrature,<br/> we have to sample discrete points from continuous ray</p> </blockquote> <p>Instead of deterministic quadrature(typically used for rendering voxel grids, but may limit resolution),<br/> author divides a ray \(\left[t_n, t_f\right]\) into N = 64 bins(intervals), and chooses one point \(t_i\) for each bin by uniform sampling<br/> \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N}(t_f - t_n), t_n + \frac{i}{N}(t_f - t_n)\right]\)</p> <p>Although we use discrete N samples, <code class="language-plaintext highlighter-rouge">stratified sampling(층화 표집)</code> enables MLP to be evaluated at continuous positions by optimization</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/5-480.webp 480w,/assets/img/2024-04-10-NeRF/5-800.webp 800w,/assets/img/2024-04-10-NeRF/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Discretized version for N samples by Numerical Quadrature :<br/> expected color \(\hat{C}(r) = \sum_{i=1}^{N} T_i (1 - \exp(- \sigma_{i} \delta_{i})) c_i\)</p> </blockquote> <ul> <li>\(\hat{C}(r)\) : differentiable about (\(\sigma_{i}, c_i\))</li> <li>\(\delta_{j} = t_{j+1} - t_j\) : the distance between adjacent samples</li> <li>\(T(t) = \exp(- \int_{t_n}^{t} \sigma (r(s)) ds) ~~ \rightarrow ~~ T_i = \exp(- \sum_{j=1}^{i-1} \sigma_{j} \delta_{j})\) where T_1 = 1</li> <li>\(\sigma (r(t)) dt ~~ \rightarrow ~~ \sigma_{j} \delta_{j} = 1 - \exp(-\sigma_{j} \delta_{j})\) by Taylor Series Expansion</li> <li> \[c(r(t), d) ~~ \rightarrow ~~ c_i\] </li> </ul> <p>또는</p> <p>\(p_{hit}(z_i) = \frac{dH}{dz} |_{z_i} ~~ \rightarrow ~~ H(z_{i+1}) - H(z_i) = (1 - T(z_{i+1})) - (1 - T(z_i)) = T(z_i) - T(z_{i+1}) = e^{- \sum_{j=1}^{i-1} \sigma_{j} \delta_{j}} - e^{- \sum_{j=1}^{i} \sigma_{j} \delta_{j}} = T(z_i)(1 - e^{- \sigma_{i} \delta_{i}})\)<br/> Then the <code class="language-plaintext highlighter-rouge">color of a pixel is expected radiance for ray</code> bounded from \(t_n\) to \(t_f\)<br/> \(\hat{C}(ray) = E[R] = \int_{t_n}^{t_f} R \cdot p_R dz ~~ \rightarrow ~~ \sum_{i=1}^{N} c_i \cdot p_{hit}(z_i) dz = \sum_{i=1}^{N} c_i T_i (1 - \exp(- \sigma_{i} \delta_{i}))\)</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">Final version</code> :<br/> expected color \(\hat{C}(r) = \sum_{i=1}^{N} T_i \alpha_{i} c_i\)<br/> where \(T_i = \prod_{j=1}^{i-1} (1-\alpha_{j})\) and \(\alpha_{i} = 1 - \exp(-\sigma_{i} \delta_{i})\)<br/> which reduces to traditional <code class="language-plaintext highlighter-rouge">alpha-compositing</code> problem</p> </blockquote> <p>이 때, this volume rendering 식은 <code class="language-plaintext highlighter-rouge">differentiable</code>하므로 end-to-end learning 가능!!<br/> a sequence of samples \(\boldsymbol t = {t_1, t_2, \ldots, t_N}\)에 대해<br/> \(\frac{d\hat{C}}{dc_i} |_{\boldsymbol t} = T_i \alpha_{i}\) \(\frac{d\hat{C}}{d \sigma_{i}} |_{\boldsymbol t} = c_i \times (\frac{dT_i}{d \sigma_{i}} \alpha_{i} + \frac{d \alpha_{i}}{d \sigma_{i}} T_i) = c_i \times (0 + \delta_{i}e^{-\sigma_{i}\delta_{i}} T_i) = \delta_{i} T_i c_i e^{- \sigma_{i} \delta_{i}}\)</p> <blockquote> <p>alpha-compositing :<br/> 여러 frame을 합쳐서 하나의 image로 합성하는 과정으로, 각 frame pixel마다 alpha 값(불투명도 값)(0~1)이 있어 겹치는 부분의 pixel 값을 결정</p> </blockquote> <p>By divide-and-conquer approach (tail recursion),<br/> \(c = \alpha_{1}c_{1} + (1 - \alpha_{1})(\alpha_{2}c_{2} + (1 - \alpha_{2})(\cdots)) = \alpha_{1}c_{1} + (1 - \alpha_{1})\alpha_{2}c_{2} + (1 - \alpha_{1})(1 - \alpha_{2})(\cdots) = \cdots = \sum_{i=1}^{N}(\alpha_{i}c_{i}\prod_{j=1}^{i-1}(1-\alpha_{j}))\) where \(\alpha_{0} = 0\)</p> <p>If \(\alpha_{i} = 1 - \exp(-\sigma_{i} \delta_{i})\),<br/> NeRF volume rendering 식 \(\hat{C}(r) = \sum_{i=1}^{N} T_i \alpha_{i} c_i\)과<br/> alpha-compositing 식 \(c = \sum_{i=1}^{N}(\alpha_{i}c_{i}\prod_{j=1}^{i-1}(1-\alpha_{j}))\)은<br/> SAME!!</p> <h2 id="optimizing-a-neural-radiance-field">Optimizing a Neural Radiance Field</h2> <h4 id="positional-encoding-pre-processing">Positional encoding (pre-processing)</h4> <p>kernel regression(dot product 및 더하기)을 사용하는 MLP의 특성상<br/> If we use input directly, <code class="language-plaintext highlighter-rouge">MLP is biased to learn low-frequency function</code> (oversmoothed appearance) (no detail) (spectral bias)<br/> So, low dim. input의 작은 변화에 대해 급격하게 변화하는 <code class="language-plaintext highlighter-rouge">high-frequency output은 학습 잘 못함</code></p> <p>Here, <code class="language-plaintext highlighter-rouge">fourier features</code> (sinusoidal signal은 input signal을 orthogonal space에서 표현 가능) let MLP learn high-frequency function in low-dim. domain <d-cite key="interplation">[1]</d-cite><br/> If we <code class="language-plaintext highlighter-rouge">map input into higher dim.</code> space which contains both low and high frequency info. by fourier features, MLP can fit data with <code class="language-plaintext highlighter-rouge">high-frequency variation</code><br/> Due to positional encoding, MLP can behave as <code class="language-plaintext highlighter-rouge">interpolation function</code> where \(L\) determines the bandwidth of the interpolation kernel <d-cite key="interpolation">[1]</d-cite><br/> \(r : R \rightarrow R^{2L}\) <br/> \(r(p) = (sin(2^0\pi p), cos(2^0\pi p), \cdots, sin(2^{L-1}\pi p), cos(2^{L-1}\pi p))\)<br/> \(L=10\) for \(r(x)\) where x has three coordinates<br/> \(L=4\) for \(r(d)\) where d has three components of the cartesian viewing direction unit vector</p> <p>추가로, low-dim. input 정보를 high-frequency output에 반영하기 위해서는 kernel을 거친 뒤에도 orthogonal eigenvalue들이 많이 살아있어야 하는데 stationary kernel 또는 Spherical Harmonics가 이러한 역할 수행 가능</p> <h4 id="hierarchical-volume-sampling">Hierarchical volume sampling</h4> <p>Densely evaluating N points by stratified sampling is inefficient<br/> =&gt; We don’t need much sampling at free space or occluded regions<br/> =&gt; Hierarchical sampling enables us to allocate more samples to regions we expect to contain visible content</p> <p>We simultaneously optimize 2 networks with different sampling : <code class="language-plaintext highlighter-rouge">coarse</code> and <code class="language-plaintext highlighter-rouge">fine</code></p> <blockquote> <p>coarse sampling \(N_c\)개 : 위에서 배웠던 내용<br/> author divides a ray into \(N_c\) = 64 bins(intervals), and chooses one point \(t_i\) for each bin by uniform sampling<br/> \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N_c}(t_f - t_n), t_n + \frac{i}{N_c}(t_f - t_n)\right]\)</p> </blockquote> <blockquote> <p>fine sampling \(N_f\)개 : 새로운 내용<br/> coarse sampling model’s output is a <code class="language-plaintext highlighter-rouge">weighted sum of all coarse-sampled colors</code><br/> \(\hat{C}(r) = \sum_{i=1}^{N_c} T_i \alpha_{i} c_i = \sum_{i=1}^{N_c} w_i c_i\)<br/> where we define \(w_i = T_i \alpha_{i} = T_i (1 - \exp(-\sigma_{i} \delta_{i}))\) for \(i=1,\cdots,N_c\)<br/> =&gt; Given the output of <code class="language-plaintext highlighter-rouge">coarse</code> network, we try more informed (better) sampling where samples are biased toward the <code class="language-plaintext highlighter-rouge">relevant parts of the scene volume</code><br/> =&gt; We sample \(N_f\)=128 <code class="language-plaintext highlighter-rouge">fine</code> points following a <code class="language-plaintext highlighter-rouge">piecewise-constant PDF</code> of normalized \(\frac{w_i}{\sum_{j=1}^{N_c} w_j}\)<br/> =&gt; Here, we use <code class="language-plaintext highlighter-rouge">Inverse CDF Method</code> for sampling fine points</p> </blockquote> <blockquote> <p>Inverse transform sampling = Inverse CDF Method :<br/> =&gt; PDF (probability density function) : \(f_X(x)\)<br/> =&gt; CDF (cumulative distribution function) : \(F_X(x) = P(X \leq x) = \int_{-\infty}^x f_X(x) dx\)<br/> idea : 모든 확률 분포의 CDF는 Uniform distribution을 따른다!<br/> =&gt; 따라서 CDF의 y값을 Uniform sampling하면, 그 y에 대응되는 x에 대한 (특정 PDF를 따르는) sampling을 구현할 수 있다!<br/> =&gt; 즉, CDF의 역함수를 계산할 수 있다면, 기본 난수 생성기인 Uniform sampling을 이용해서 확률 분포 X에 대한 sampling을 할 수 있다!<br/> \(F_X(x)\) ~ \(U\left[0, 1\right]\)<br/> \(X\) ~ \(F^{-1}(U\left[0, 1\right])\)</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/4-480.webp 480w,/assets/img/2024-04-10-NeRF/4-800.webp 800w,/assets/img/2024-04-10-NeRF/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We evaluate <code class="language-plaintext highlighter-rouge">coarse</code> network using \(N_c\)=64 points per ray<br/> We evaluate <code class="language-plaintext highlighter-rouge">fine</code> network using \(N_c+N_f\)=64+128=192 points per ray where we sample 128 fine points following PDF of <code class="language-plaintext highlighter-rouge">coarse</code> sampled points<br/> In result, we use a total of 64+192=256 samples per ray to compute the final rendering color \(C(r)\)</p> <h4 id="implementation-details--loss">Implementation details &amp; Loss</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/6-480.webp 480w,/assets/img/2024-04-10-NeRF/6-800.webp 800w,/assets/img/2024-04-10-NeRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>Prepare RGB images, corresponding camera poses, intrinsic parameters and scene bounds (use COLMAP structure-from-motion package to estimate these parameters)</li> <li>From H x W input image, randomly sample a batch of 4096 pixels</li> <li>calculate continuous ray from each pixel \(r(t) = o + kd\)</li> <li>coarse sampling of \(N_c\)=64 points per each ray \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N_c}(t_f - t_n), t_n + \frac{i}{N_c}(t_f - t_n)\right]\)</li> <li>positional encoding \(r(x)\) and \(r(d)\) for input</li> <li>obtain volume density \(\sigma\) by MLP with \(r(x, y, z)\) as input</li> <li>obtain color \(c\) by MLP with \(r(x, y, z)\) and \(r(\theta, \phi)\) as input</li> <li>obtain rendering color of each ray by volume rendering \(\hat{C}(r) = \sum_{i=1}^{N} T_i (1 - \exp(- \sigma_{i} \delta_{i})) c_i\) from two networks ‘coarse’ and ‘fine’</li> <li>compute loss</li> <li>Adam optimizer with learning rate from \(5 \times 10^{-4}\) to \(5 \times 10^{-5}\)</li> <li>optimization for a single scene typically takes around 100-300k iterations (1~2 days) to converge on a single NVIDIA V100 GPU</li> </ol> <p>Here, we use L2 norm for loss<br/> \(L = \sum_{r \in R} \left[{\left\|\hat{C_c}(r)-C(r)\right\|}^2+{\left\|\hat{C_f}(r)-C(r)\right\|}^2\right]\)<br/> \(C(r)\) : GT pixel RGB color<br/> \(\hat{C_c}(r)\) : rendering RGB color from coarse network : to allocate better samples in fine network<br/> \(\hat{C_f}(r)\) : rendering RGB color from fine network : our goal<br/> \(R\) : the set of all pixels(rays) across all images</p> <h2 id="results">Results</h2> <h4 id="datasets">Datasets</h4> <p>Synthetic renderings of objects</p> <ul> <li>Diffuse Synthetic 360 : 4 Lambertian objects with simple geometry</li> <li>Realistic Synthetic 360 : 8 non-Lambertian objects with complicated geometry</li> </ul> <p>Real images of complex scenes</p> <ul> <li>Real Forward-Facing : 8 scenes captured with a handheld cellphone</li> </ul> <h4 id="measurement">Measurement</h4> <ul> <li>PSNR(Peak Signal-to-Noise Ratio) \(\uparrow\) : the ratio between the maximum possible power of a signal and the power of corrupting noise \(10\log_{10}\left(\frac{(MAX)^2}{MSE}\right)\)[dB]</li> <li>SSIM(Structural Similarity Index Map) \(\uparrow\) : compare image qualities in three ways: Lumincance(\(l\)), Contrast(\(c\)), Structural(\(s\))<br/> SSIM(x, y) = \([l(x,y)]^{\alpha}[c(x,y)]^{\beta}[s(x,y)]^{\gamma}=\frac{(2\mu_{x}\mu_{y}+C_1)(2\sigma_{xy}+C_2)}{(\mu_{x}^2+\mu_{y}^2+C_1)(\sigma_{x}^2+\sigma_{y}^2+C_2)}\) where \(l(x,y)=\frac{(2\mu_{x}\mu_{y}+C_1)}{\mu_{x}^2+\mu_{y}^2+C_1}\) and \(c(x,y)=\frac{(2\sigma_{x}\sigma_{y}+C_2)}{\sigma_{x}^2+\sigma_{y}^2+C_2}\) and \(s(x,y)=\frac{\sigma_{xy}+C_3}{\sigma_{x}\sigma_{y}+C_3}\)<br/> SSIM calculator :<br/> https://darosh.github.io/image-ssim-js/test/browser_test.html</li> <li>LPIPS \(\downarrow\)</li> </ul> <h4 id="comparisons">Comparisons</h4> <ul> <li>Neural Volumes (NV) :<br/> It synthsizes novel views of objects that lie entirely within a bounded volume in front of a distinct background.<br/> It optimizes 3D conv. network to predict a discretized RGB\(\alpha\) voxel grid and a 3D warp grid.<br/> It renders novel views by marching rays through the warped voxel grid</li> <li>Scene Representation Networks (SRN) :<br/> It represents continuous scene as an opaque surface.<br/> MLP maps each 3D coordinate to a feature vector, and we optimize RNN to predict the next step size along the ray using the feature vector.<br/> The feature vector from the final step is decoded into a color for that point on the surface. Note that SRN is followup to DeepVoxels by the same authors.</li> <li>Local Light Field Fusion (LLFF) :<br/> designed for producing novel views for well-sampled forward facing scenes<br/> trained 3D conv. network directly predicts a discretized frustum-sampled RGB\(\alpha\) grid (multiplane image), and then renders novel views by alpha-compositing and blending</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/7-480.webp 480w,/assets/img/2024-04-10-NeRF/7-800.webp 800w,/assets/img/2024-04-10-NeRF/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Test for scenes from author's new synthetic dataset </div> <p>LLFF exhibits banding and ghosting artifacts<br/> SRN produces blurry and distorted renderings<br/> NV cannot capture the details<br/> NeRF captures fine details in both geometry and appearance</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/8-480.webp 480w,/assets/img/2024-04-10-NeRF/8-800.webp 800w,/assets/img/2024-04-10-NeRF/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Test for read-world scenes </div> <p>LLFF may have repeated edges because of blending between multiple renderings<br/> NeRF also correctly reconstruct partially-occluded regions<br/> SRN does not capture any high-frequency fine detail</p> <h4 id="discussion">Discussion</h4> <h4 id="ablation-studies">Ablation Studies</h4> <h2 id="conclusion">Conclusion</h2> <p>prior : MLP outputs discretized voxel representations<br/> author : MLP outputs volume density and view-dependent emitted radiance</p> <h2 id="future-work">Future Work</h2> <p>efficiency :<br/> Rather than hierarchical sampling, there is still much more progress to be made for efficient optimization and rendering of neural radiance fields</p> <p>interpretability :<br/> voxel grids or meshes admits reasoning about the expected quality, but it is unclear to analyze these issues when we encode scenes into the weights of MLP</p>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="rendering"/><category term="3d"/><summary type="html"><![CDATA[representing scenes as neural radiance fields for view synthesis]]></summary></entry></feed>