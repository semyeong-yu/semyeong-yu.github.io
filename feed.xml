<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://semyeong-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://semyeong-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-09T16:16:21+00:00</updated><id>https://semyeong-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">WandB</title><link href="https://semyeong-yu.github.io/blog/2024/WandB/" rel="alternate" type="text/html" title="WandB"/><published>2024-10-09T12:00:00+00:00</published><updated>2024-10-09T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/WandB</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/WandB/"><![CDATA[<h2 id="wandb-tutorial">WandB Tutorial</h2> <ul> <li>WandB Platform <ul> <li>Experiments : experiment tracking</li> <li>Sweeps : model optimization</li> <li>Artifacts : dataset versioning</li> <li>Tables : model evaluation</li> <li>Reports : collaborative analysis</li> </ul> </li> </ul> <h3 id="experiment">Experiment</h3> <p>wandb.init(config=, tags=, name=) # run별로 구분하기 위해 tags=[“batch=32”, f”lr={wandb.config.lr}”] 또는 name=exp1 설정 # finished run resume하고 싶으면 id=… wandb.watch(model=, criterion=, log=’all’) wandb.log(<dict>, step=epoch)</dict></p> <p>모델 저장 artifact = wandb.Artifact(‘model’, type=’model’) artifact.add_file(f”model/resnet50.pt”) wandb.log_artifact(artifact)</p> <p>이미지 저장 images=[] images.append(wandb.Image(img[0], caption=”Pred: {} Truth: {}”.format(pred[0].item(), target[0]))) wandb.log({“Image”: images}) # 100여개 정도까지가 한계</p> <p>wandb.finish()</p> <h3 id="sweep">Sweep</h3> <p>sweep.yaml program: main.py method: bayes // grid 모든 조합 탐색, random 랜덤선택, bayes 좀 더 확인 name: ddpm-sweep description: test ddpm sweep project: ddpm entity: semyu0102-viclab meric: name: val_loss //wandb.log 안에 넣은 이름 goal: minimize parameters: # param. to sweep learning_rate: min: 0.0001 max: 0.1 optimizer: values: [“adam”, “sgd”] # 탐색 list epochs: value: 5 # 탐색x parameter1: distribution: normal mu: 100 sigma: 10 early_terminate: type: hyperband # early stopping 알고리즘 min_iter: 3 command: // python 파일 실행할 때 지정할 argument</p> <p>wandb sweep sweep.yaml wandb agent <위의 output=""> --count <sweep 횟수=""></sweep></위의></p>]]></content><author><name></name></author><category term="others"/><category term="wandb"/><category term="log"/><category term="sweep"/><summary type="html"><![CDATA[WandB Tutorial]]></summary></entry><entry><title type="html">NeRF in the Wild</title><link href="https://semyeong-yu.github.io/blog/2024/NeRFW/" rel="alternate" type="text/html" title="NeRF in the Wild"/><published>2024-10-08T12:00:00+00:00</published><updated>2024-10-08T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/NeRFW</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/NeRFW/"><![CDATA[<h2 id="nerf-in-the-wild---neural-radiance-fields-for-unconstrained-photo-collections-cvpr-2021">NeRF in the Wild - Neural Radiance Fields for Unconstrained Photo Collections (CVPR 2021)</h2> <h4 id="ricardo-martin-brualla-noha-radwan-mehdi-s-m-sajjadi-jonathan-t-barron-alexey-dosovitskiy-daniel-duckworth">Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, Daniel Duckworth</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2008.02268">https://arxiv.org/abs/2008.02268</a><br/> project website :<br/> <a href="https://nerf-w.github.io/">https://nerf-w.github.io/</a><br/> code :<br/> <a href="https://github.com/kwea123/nerf_pl/tree/nerfw?tab=readme-ov-file">https://github.com/kwea123/nerf_pl/tree/nerfw?tab=readme-ov-file</a><br/> <a href="https://github.com/rover-xingyu/Ha-NeRF">https://github.com/rover-xingyu/Ha-NeRF</a><br/> youtube video :<br/> <a href="https://www.youtube.com/watch?v=mRAKVQj5LRA&amp;t=254s">https://www.youtube.com/watch?v=mRAKVQj5LRA&amp;t=254s</a><br/> reference :<br/> NeRF and 3DGS Study</p> </blockquote> <h3 id="introduction">Introduction</h3> <ul> <li>Issue : <ul> <li>Q : image 상의 동적인 물체를 어떻게 없앨 수 있을까?</li> <li>A : <code class="language-plaintext highlighter-rouge">Static Network</code>와 <code class="language-plaintext highlighter-rouge">Transient Network</code>를 분리한 뒤<br/> \(c, \sigma\) 에 대한 <code class="language-plaintext highlighter-rouge">Uncertainty</code>를 측정하자!</li> </ul> </li> <li>Contribution : <ul> <li>Latent Appearance Embedding in Static Network :<br/> 각 image의 광도 반영</li> <li>Latent Transient Embedding in Transient Network :<br/> 동적인 물체 구별</li> <li>Loss w. Uncertainty and Transient density</li> </ul> </li> <li>결과 : <ul> <li>Latent Embedding Vector 변화로 Appearance에 변화 줄 수 있음</li> <li>일시적으로 찍힌 동적인 물체를 제거할 수 있음</li> </ul> </li> </ul> <h3 id="architecture---static-network">Architecture - Static Network</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/1-480.webp 480w,/assets/img/2024-10-08-NeRFW/1-800.webp 800w,/assets/img/2024-10-08-NeRFW/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Static Network :<br/> 우리가 Novel View Synthesis 하고 싶어하는 대상을 다룸</p> </li> <li>View Direction과 함께 Appearance Embedding Vector 넣어준다는 것 말고는 기존 NeRF 구조와 same <ul> <li>\(\gamma_{x}(r(t)) \rightarrow \sigma_{i}(t)\) (3d shape)</li> <li>\(\gamma_{x}(r(t)), \gamma_{d}(d), l_{i}^{(a)} \rightarrow c_{i}(t)\) (view-dependent 3d color)</li> </ul> </li> <li>Appearance Embedding Vector : <ul> <li>image의 embedding vector (각 image의 광도 반영)</li> <li>random initialization (learnable)</li> <li>control처럼 쓰일 수 있음<br/> Embedding Vector 수정하여 Appearance(스타일)에 변화 줄 수 있음</li> <li>training dataset에 대해 \(l_{i}^{(a)}\) 를 학습하므로<br/> test할 때는 target image에 적합할 만한 Embedding Vector 골라서 사용</li> </ul> </li> </ul> <h3 id="architecture---transient-network">Architecture - Transient Network</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/2-480.webp 480w,/assets/img/2024-10-08-NeRFW/2-800.webp 800w,/assets/img/2024-10-08-NeRFW/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Transient Network : <ul> <li>우리가 Novel View Synthesis 하고 싶어하는 대상이 아닌,<br/> 동적인 물체를 다룸 (제거하기 위해)</li> <li>Bayesian learning framework를 적용하여 <code class="language-plaintext highlighter-rouge">???</code> Uncertainty를 모델링</li> </ul> </li> <li>Transient Embedding \(l_{i}^{(T)}\) 을 넣어서 동적인 물체의 transient density를 얻은 뒤 제거 가능 <ul> <li> \[\gamma_{x}(r(t)), l_{i}^{(T)} \rightarrow c_{i}^{(T)}(t), \sigma_{i}^{(T)}(t), \tilde \beta_{i}(t)\] </li> <li> \[\beta_{i}(t) = \beta_{min} + \text{log}(1+\text{exp}(\tilde \beta_{i}(t)))\] </li> </ul> </li> </ul> <h3 id="volume-rendering">Volume Rendering</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/3-480.webp 480w,/assets/img/2024-10-08-NeRFW/3-800.webp 800w,/assets/img/2024-10-08-NeRFW/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Training : (a) Static, (b) Transient 모두 사용하여 아래의 rendering 식으로 (c) Composite 만들고, 이를 (d) GT와 비교하여 학습<br/> \(\hat C_{i} (r) = \sum_{k=1}^K T_{i}(t_k)(\alpha(\sigma_{i}(t_k) \delta_{k}) c_{i}(t_k) + \alpha(\sigma_{i}^{(T)}(t_k) \delta_{k}) c_{i}^{(T)}(t_k))\)<br/> where \(T_{i}(t_k) = \text{exp}(-\sum_{k^{'}=1}^{k-1}(\sigma_{i}(t_{k^{'}}) + \delta_{i}^{(T)}(t_{k^{'}}))\delta_{k^{'}})\)</p> </li> <li> <p>Test : (a) Static만 사용</p> </li> </ul> <h3 id="optimization">Optimization</h3> <ul> <li>Coarse Model :<br/> (기존 NeRF와 유사하게) static network만 사용해서 Appearance Embedding Vector를 학습<br/> \(L = \sum_{ij} L_{c}(r_{ij})\) <ul> <li>\(L_{c}(r_{ij}) = \frac{1}{2} \| C(r_{ij}) - \hat C^{c}(r_{ij}) \|^2\)<br/> where \(\hat C^{c}(r_{ij}) = \sum_{k=1}^K T_{i}(t_k)(\alpha(\sigma_{i}(t_k) \delta_{k}) c_{i}(t_k))\)<br/> (static network만 사용)</li> </ul> </li> <li>Fine Model :<br/> static, transient network 모두 사용해서 학습<br/> \(L = \sum_{ij} L_{f}(r_{ij}) + L_{c}(r_{ij})\) <ul> <li>\(L_{c}(r_{ij}) = \frac{1}{2} \| C(r_{ij}) - \hat C^{c}(r_{ij}) \|^2\)<br/> where \(\hat C^{c}(r_{ij}) = \sum_{k=1}^K T_{i}(t_k)(\alpha(\sigma_{i}(t_k) \delta_{k}) c_{i}(t_k))\)<br/> (static network만 사용)</li> <li>\(L_{f}(r_{ij}) = \frac{\| C(r_{ij}) - \hat C^{f}(r_{ij}) \|^2}{2\beta(r)^2} + \frac{\text{log} \beta(r)^2}{2} + \frac{\lambda}{K} \sum_{k=1}^K \sigma^{(T)}(t_k)\)<br/> where \(\hat C^{f}(r_{ij}) = \sum_{k=1}^K T_{i}(t_k)(\alpha(\sigma_{i}(t_k) \delta_{k}) c_{i}(t_k) + \alpha(\sigma_{i}^{(T)}(t_k) \delta_{k}) c_{i}^{(T)}(t_k))\)<br/> (static, trasient network 모두 사용) <ul> <li>1번째 term : <code class="language-plaintext highlighter-rouge">recon. loss term</code><br/> Uncertainty \(\beta(r)\) 가 크면 recon. loss 영향력 작아짐<br/> (동적인 물체가 있어서 불확실한 부분은 loss 및 gradient 작게)</li> <li>2번째 term : <code class="language-plaintext highlighter-rouge">regularization term</code><br/> Uncertainty \(\beta(r)\) 가 너무 커지지 않도록 regularize</li> <li>3번째 term : <code class="language-plaintext highlighter-rouge">regularization term</code><br/> transient density \(\sigma^{(T)}\) 가 너무 커지지 않도록 regularize</li> </ul> </li> </ul> </li> </ul> <h3 id="results">Results</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/4-480.webp 480w,/assets/img/2024-10-08-NeRFW/4-800.webp 800w,/assets/img/2024-10-08-NeRFW/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/5-480.webp 480w,/assets/img/2024-10-08-NeRFW/5-800.webp 800w,/assets/img/2024-10-08-NeRFW/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="conclusion">Conclusion</h3> <ul> <li>동적이고 조명이 바뀌는 상황에서 촬영된 image dataset으로도 neural rendering 가능 <ul> <li>Appearance Embedding : 각 image의 광도를 반영</li> <li>Transient Embedding : target static object를 가리는 동적인 물체 구별하여 제거</li> </ul> </li> </ul> <h3 id="limitation">Limitation</h3> <ul> <li>training 개수, camera calibration error에 민감</li> </ul> <h3 id="code">Code</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/6-480.webp 480w,/assets/img/2024-10-08-NeRFW/6-800.webp 800w,/assets/img/2024-10-08-NeRFW/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/7-480.webp 480w,/assets/img/2024-10-08-NeRFW/7-800.webp 800w,/assets/img/2024-10-08-NeRFW/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="question">Question</h3> <ul> <li> <p>Q1 :<br/> \(\hat C_{i} (r) = \sum_{k=1}^K T_{i}(t_k)(\alpha(\sigma_{i}(t_k) \delta_{k}) c_{i}(t_k) + \alpha(\sigma_{i}^{(T)}(t_k) \delta_{k}) c_{i}^{(T)}(t_k))\)<br/> 위의 volume rendering 식을 보면 static network의 color, density와 transient network의 color, density가 함께 하나의 pixel color로 rendering되어 동시에 backpropagation되는데<br/> 어떻게 두 network 중에서 하필 transient network의 color, density가 동적인 물체를 구별하는 역할을 수행할 수 있느냐</p> </li> <li> <p>A1 :<br/> Coarse Model(static network 사용)과 Fine Model(static, transient network 모두 사용)을 two-stage로 분리해서 학습하기 때문에 transient network가 동적인 물체를 식별하는 역할을 잘 수행할 수 있을 것이다<br/> (NeRF and 3DGS 스터디원 피셜)</p> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="static"/><category term="transient"/><category term="uncertainty"/><summary type="html"><![CDATA[Neural Radiance Fields for Unconstrained Photo Collections (CVPR 2021)]]></summary></entry><entry><title type="html">Neuralangelo</title><link href="https://semyeong-yu.github.io/blog/2024/Neuralangelo/" rel="alternate" type="text/html" title="Neuralangelo"/><published>2024-09-30T12:00:00+00:00</published><updated>2024-09-30T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Neuralangelo</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Neuralangelo/"><![CDATA[<h2 id="neuralangelo---high-fidelity-neural-surface-reconstruction-cvpr-2023">Neuralangelo - High-Fidelity Neural Surface Reconstruction (CVPR 2023)</h2> <h4 id="zhaoshuo-li-thomas-müller-alex-evans-russell-h-taylor-mathias-unberath-ming-yu-liu-chen-hsuan-lin">Zhaoshuo Li, Thomas Müller, Alex Evans, Russell H. Taylor, Mathias Unberath, Ming-Yu Liu, Chen-Hsuan Lin</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/pdf/2306.03092">https://arxiv.org/pdf/2306.03092</a><br/> project website :<br/> <a href="https://research.nvidia.com/labs/dir/neuralangelo/">https://research.nvidia.com/labs/dir/neuralangelo/</a><br/> code :<br/> <a href="https://github.com/nvlabs/neuralangelo">https://github.com/nvlabs/neuralangelo</a><br/> reference :<br/> NeRF and 3DGS Study</p> </blockquote> <h3 id="abstract">Abstract</h3> <ul> <li> <p>multi-resolution hash grid representation<br/> with SDF-based volume rendering<br/> (3D surface recon.)</p> </li> <li> <p>no need for auxiliary data like segmentation or depth</p> </li> <li> <p>Novelty :</p> <ul> <li>numerical gradient (backpropagation locality 문제 해결)</li> <li>coarse-to-fine (점점 high resol.)</li> </ul> </li> </ul> <h3 id="related-works">Related Works</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-30-Neuralangelo/1-480.webp 480w,/assets/img/2024-09-30-Neuralangelo/1-800.webp 800w,/assets/img/2024-09-30-Neuralangelo/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-30-Neuralangelo/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Instant NGP <d-cite key="InstantNGP">[1]</d-cite> <a href="https://nvlabs.github.io/instant-ngp/">Link</a> :<br/> 모든 좌표(pixel) 각각에 대해 MLP output을 구하면 연산량이 너무 크므로<br/> 연산량 감소 및 speed-up 위해<br/> <code class="language-plaintext highlighter-rouge">Hash Grid</code>(연산량 감소)와 <code class="language-plaintext highlighter-rouge">Linear Interpolation</code>(continuous 보장)을 이용한<br/> 좌표 encoding 기법 제시 <ul> <li>STEP 1)<br/> \(d\)-dim. scene일 때<br/> input 좌표 \(x\) 가 주어졌을 때<br/> grid level별로 주위 \(2^d\)-개 좌표 선택 <ul> <li>multi-resolution (grid-level \(l\)) :<br/> \(N_l = \lfloor N_{min} \cdot b^l \rfloor\)<br/> where $$b = e^{\frac{\text{ln} N_{max} - \text{ln} N_{min}}{L-1}}<br/> MLP size가 작더라도 multi-resol. 덕분에 high approx. power 가짐</li> <li>주위 좌표 선택 :<br/> \(N_l\) 만큼 scale된 좌표 계산<br/> \(\lfloor x_l \rfloor = \lfloor x \cdot N_l \rfloor\)<br/> \(\lceil x_l \rceil = \lceil x \cdot N_l \rceil\)</li> </ul> </li> <li>STEP 2)<br/> 선택한 각 좌표에 대해 HashKey를 계산한 뒤 HashTable에서 Value 읽어옴 <ul> <li>HashKey :<br/> grid-level 마다 1개씩 HashTable이 정의되며<br/> Spatial Hash Function(2003)에 의해<br/> HashKey \(h(x) = (\text{XOR}_{i=1}^{d} x_i \pi_{i}) \text{mod} T \in [0, T-1]$ where\)d\(: dim.,\)\pi\(: dim.마다 임의로 정해둔 constant,\)T$$ : Hash Table Size</li> <li>HashValue :<br/> \(T \times F\) 의 HashTable로부터 \(F\)-dim. feature vector인 HashValue를 얻음</li> </ul> </li> <li>STEP 3)<br/> 주위 좌표까지의 거리를 기반으로<br/> HashValue들을 Linear Interpolation(weighted sum)하여<br/> grid-level 별로 1개의 feature vector로 만듬</li> <li>STEP 4)<br/> 각 grid-level 별 feature vectors와 auxiliary 값(e.g. view direction)을 concat하여<br/> 최종 feature vector 만듬</li> <li>STEP 5)<br/> shallow MLP 통과</li> <li>STEP 6)<br/> Backpropagation :<br/> MLP weight와 Hash Table의 Value(\(F\)-dim. feature vector) 업데이트</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-30-Neuralangelo/2-480.webp 480w,/assets/img/2024-09-30-Neuralangelo/2-800.webp 800w,/assets/img/2024-09-30-Neuralangelo/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-30-Neuralangelo/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="numerical-gradient">Numerical Gradient</h3> <ul> <li>Gradient : <ul> <li>analytical gradient : \(\nabla f(x_i) = \frac{\partial f(x_i)}{\partial x_i}\)</li> <li>numerical gradient : \(\text{lim}_{\epsilon_{x} \rightarrow 0} \frac{f(x_i + \epsilon_{x}) - f(x_i - \epsilon_{x})}{2\epsilon_{x}}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-30-Neuralangelo/4-480.webp 480w,/assets/img/2024-09-30-Neuralangelo/4-800.webp 800w,/assets/img/2024-09-30-Neuralangelo/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-30-Neuralangelo/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Analytical Graident </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-30-Neuralangelo/5-480.webp 480w,/assets/img/2024-09-30-Neuralangelo/5-800.webp 800w,/assets/img/2024-09-30-Neuralangelo/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-30-Neuralangelo/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Numerical Gradient </div> <ul> <li>Instant NGP <d-cite key="InstantNGP">[1]</d-cite> 에서처럼<br/> input coordinate encode하기 위해<br/> multi-resolution hash grid representation 사용 <ul> <li>문제 :<br/> SDF-based volume rendering에 multi-resolution hash grid를 직접적으로 적용하면<br/> large smooth regions의 surface에 noise 및 hole이 생김</li> <li>이유 : <ul> <li>surface recon.에서 RGB(color) 및 SDF(geometry)를 MLP output으로 얻는데<br/> surface regularization loss를 구할 때 higher-order derivatives of SDF 계산해야 함 <ul> <li>first-order derivative of SDF \(f(x_i)\) :<br/> Eikonal constraints on the surface normals 계산<br/> \(L_{eik} = \frac{1}{N} \sum_{i=1}^N (\| \nabla f(x_i) \| - 1)^2\)</li> <li>second-order derivative of SDF \(f(x_i)\) :<br/> surface curvatures 계산<br/> \(L_{curv} = \frac{1}{N} \sum_{i=1}^N | \nabla^{2} f(x_i) |\)</li> </ul> </li> <li>그리고 이러한 SDF의 higher-order derivatives 계산하기 위해<br/> <code class="language-plaintext highlighter-rouge">analytical gradient</code> \(\nabla f(x_i) = \frac{\partial f(x_i)}{\partial x_i}\) 사용</li> <li>근데, analytical gradient 사용하면<br/> <code class="language-plaintext highlighter-rouge">only backpropagate to local cell</code>의 HashValues<br/> (locality 문제 발생!)</li> <li>특히 recon.할 surface가 multiple grid cells에 걸쳐 있을 경우<br/> analytical gradient를 사용하면<br/> adjacent cells는 업데이트 안 됨</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-30-Neuralangelo/3-480.webp 480w,/assets/img/2024-09-30-Neuralangelo/3-800.webp 800w,/assets/img/2024-09-30-Neuralangelo/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-30-Neuralangelo/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig a. Analytical Gradient from local cell </div> <ul> <li>Instant NGP <d-cite key="InstantNGP">[1]</d-cite> 에서처럼<br/> input coordinate encode하기 위해<br/> multi-resolution hash grid representation 사용 <ul> <li>해결 : <ul> <li>SDF의 higher-order derivatives 계산하기 위해<br/> numerical gradient \(\text{lim}_{\epsilon \rightarrow 0} \frac{f(x_i + \epsilon) - f(x_i - \epsilon)}{2\epsilon}\) 사용</li> <li><code class="language-plaintext highlighter-rouge">forward pass</code>에서 rendering하기 위해 (<code class="language-plaintext highlighter-rouge">recon. loss</code> 구하기 위해) SDF 계산할 때는<br/> sampled point 1개만 사용</li> <li><code class="language-plaintext highlighter-rouge">regularization loss</code> 구하기 위해 SDF의 higher-order derivatives 계산할 때는<br/> adjacent cells의 SDF까지 이용하는 numerical gradient를 사용함으로써<br/> <code class="language-plaintext highlighter-rouge">backward pass</code>에서 <code class="language-plaintext highlighter-rouge">backpropagate to adjacent cells</code></li> <li>adjacent 6개의 cells \(x_i \pm \epsilon\) 각각에 대해 trilinear sampling으로 SDF 값 계산하고<br/> 그 차이를 이용해서 <code class="language-plaintext highlighter-rouge">numerical gradient</code> 계산<br/> 이는 backward pass에 이용</li> <li>local cell \(x_i\) 로만 backpropagate하는 게 아니라<br/> 주위 6개의 cells \(x_i \pm \epsilon\) 으로 backpropagate하므로<br/> <code class="language-plaintext highlighter-rouge">smoothing</code> on SDF 역할 수행</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-30-Neuralangelo/6-480.webp 480w,/assets/img/2024-09-30-Neuralangelo/6-800.webp 800w,/assets/img/2024-09-30-Neuralangelo/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-30-Neuralangelo/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig b. Numerical Gradient from adjacent cells </div> <h3 id="coarse-to-fine">Coarse-to-Fine</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-30-Neuralangelo/7-480.webp 480w,/assets/img/2024-09-30-Neuralangelo/7-800.webp 800w,/assets/img/2024-09-30-Neuralangelo/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-30-Neuralangelo/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Coarse-to-Fine :<br/> 점점 hash grid encoding resol. \(N_l\) 증가시키고<br/> 이에 맞춰서 numerical gradient의 step size \(\epsilon\) 감소시킴</li> </ul> <h3 id="code">Code</h3> <ul> <li> <p>.yaml 로부터 config Dictionary 만들기 <a href="https://github.com/NVlabs/neuralangelo/blob/main/imaginaire/config.py">Code</a></p> </li> <li> <p>.yaml에 적어놓은 module을 동적으로 읽어와서 해당 module 내 class 사용하기 <a href="https://github.com/NVlabs/neuralangelo/blob/main/imaginaire/trainers/utils/get_trainer.py">Code</a></p> </li> <li> <p>Train 껍질 <a href="https://github.com/NVlabs/neuralangelo/blob/main/train.py">Code</a></p> </li> <li> <p>Trainer <a href="https://github.com/NVlabs/neuralangelo/blob/main/projects/neuralangelo/trainer.py">Code</a> \(\rightarrow\) overriding \(\rightarrow\) <a href="https://github.com/NVlabs/neuralangelo/blob/main/imaginaire/trainers/base.py">Code</a></p> </li> <li> <p>각종 함수 계산하는 utils <a href="https://github.com/NVlabs/neuralangelo/tree/main/projects/neuralangelo/utils">Code</a></p> </li> </ul> <h3 id="question">Question</h3> <ul> <li> <p>Q1 :<br/> analytical gradient에 비해 numerical gradient가 갖는 장점을 정리해서 알려주세요</p> </li> <li>A1 : <ul> <li>numerical gradient는<br/> point 하나만 sampling해도<br/> 그 주위의 여러 samples’ feature까지 다룰 수 있음</li> <li>gradient 하나가 얼마나 넓은 범위에 영향을 미치는지에 따라 sample efficiency가 결정되고 학습의 효율성이 결정됨<br/> continuous surface 상황에서는 하나의 error에서 나오는 gradient가 여러 군데에 영향을 동시에 미치는 것이 적합함<br/> 사실 forward pass에서 많은 points를 aggregate(또는 blur)하면 analytical gradient로도 backpropagation이 여러 군데에 퍼지게 할 수 있다<br/> 하지만 그러면 forward 쪽이 blur해지면서 frequency bound가 생기고, 속도가 느려짐<br/> 따라서 forward pass 쪽은 건들지 않고 backward pass 쪽만 건드려서 (numerical gradient for regularization loss)<br/> backpropagation이 여러 군데에 퍼지게 함</li> </ul> </li> <li> <p>Q2 :<br/> analytical gradient 대신 numerical gradient 쓰기 위해 adjacent cells’ SDF까지 계산하려면 performance 상승하긴 하지만 느려지지 않나요?</p> </li> <li>A2 :<br/> Instant-NGP의 Hash Grid 방식 자체가 빨라서 ㄱㅊ<br/> 내 피셜로는 regularization loss 구할 때만 adjacent cells’ SDF 이용하므로 inference rendering speed는 그대로라서 training speed 저하 미비</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="3d"/><category term="surface"/><category term="hash"/><category term="grid"/><category term="numerical"/><category term="gradient"/><summary type="html"><![CDATA[High-Fidelity Neural Surface Reconstruction (CVPR 2023)]]></summary></entry><entry><title type="html">TensoRF</title><link href="https://semyeong-yu.github.io/blog/2024/TensoRF/" rel="alternate" type="text/html" title="TensoRF"/><published>2024-09-17T12:00:00+00:00</published><updated>2024-09-17T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/TensoRF</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/TensoRF/"><![CDATA[<h2 id="tensorf-tensorial-radiance-fields">TensoRF: Tensorial Radiance Fields</h2> <h4 id="anpei-chen-zexiang-xu-andreas-geiger-jingyi-yu-hao-su">Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, Hao Su</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2203.09517">https://arxiv.org/abs/2203.09517</a><br/> project website :<br/> <a href="https://apchenstu.github.io/TensoRF/">https://apchenstu.github.io/TensoRF/</a><br/> code :<br/> <a href="https://github.com/apchenstu/TensoRF">https://github.com/apchenstu/TensoRF</a><br/> referenced blog :<br/> <a href="https://xoft.tistory.com/42">https://xoft.tistory.com/42</a></p> </blockquote> <h2 id="abstract">Abstract</h2> <ul> <li> <p>Radiance Field (Scene)에 대해 Tensor Decomposition을 적용해보자!</p> </li> <li> <p>fast training and less computational cost</p> </li> </ul> <h2 id="tensor-decomposition">Tensor Decomposition</h2> <ul> <li> <p>외적 (outer product) :<br/> \(\begin{bmatrix} 1 &amp; 2 \\ 2 &amp; 4 \\ 4 &amp; 8 \end{bmatrix} = \begin{bmatrix} 1 \\ 2 \\ 4 \end{bmatrix} \circ \begin{bmatrix} 1 &amp; 2 \end{bmatrix}\)<br/> 위의 예시는<br/> shape (3, 2) matrix를<br/> shape (3,) vector와 shape (2,) vector의 외적으로 표현</p> </li> <li> <p>Tensor Decomposition :</p> <ul> <li>\(n\)-dim.의 data를 \(n\)개의 1D vector들의 외적으로 표현할 수 있다!<br/> 이 때, 정보 손실이 발생할 수 있으므로<br/> \(R\) 개의 rank에 대해 외적들을 더해 \(n\)-dim. data를 근사</li> <li>장점 :<br/> 고차원 data를 1D vector들로 표현할 수 있으므로<br/> speed 개선</li> <li>단점 :<br/> 수많은 1D vector들로 표현하므로<br/> GPU memory 많이 소요</li> <li>종류 :<br/> CP(CANDECOMP/PARAFAC) decomposition<br/> Tucker Decomposition<br/> Block Term Decomposition</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/1-480.webp 480w,/assets/img/2024-09-17-TensoRF/1-800.webp 800w,/assets/img/2024-09-17-TensoRF/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> reference : https://www.kolda.net/publication/TensorReview.pdf </div> <ul> <li>Tensor Decomposition w. Trilinear Interpolation :<br/> interpolation으로 1D vector A와 B의 길이를 증가시키고<br/> 그 값으로 원본 matrix 표현</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/2-480.webp 480w,/assets/img/2024-09-17-TensoRF/2-800.webp 800w,/assets/img/2024-09-17-TensoRF/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> https://xoft.tistory.com/42 </div> <h2 id="tensorf-tensor-decomposition">TensoRF Tensor Decomposition</h2> <h3 id="cpcandecompparafac-decomposition">CP(CANDECOMP/PARAFAC) Decomposition</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/3-480.webp 480w,/assets/img/2024-09-17-TensoRF/3-800.webp 800w,/assets/img/2024-09-17-TensoRF/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> https://xoft.tistory.com/42 </div> <ul> <li>shape (i, j, k)의 \(T\) 에 대해<br/> \(T = \sum_{r=1}^R v_{r}^1 \circ v_{r}^2 \circ v_{r}^3\)</li> </ul> <h3 id="vmvector-matrix-decomposition">VM(vector-matrix) Decomposition</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/4-480.webp 480w,/assets/img/2024-09-17-TensoRF/4-800.webp 800w,/assets/img/2024-09-17-TensoRF/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> https://xoft.tistory.com/42 </div> <ul> <li>shape (i, j, k)의 \(T\) 에 대해<br/> \(T = \sum_{r=1}^{R_1} v_{r}^1 \circ M_{r}^{2,3} + \sum_{r=1}^{R_2} v_{r}^2 \circ M_{r}^{1,3} + \sum_{r=1}^{R_3} v_{r}^3 \circ M_{r}^{1,2}\)</li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li>Grid-based 연구들이 training speed 높이는 데 많은 기여를 하고 있으니<br/> 다른 논문들도 한 번 읽어보자 <ul> <li>Plenoxel (CVPR 2022)</li> <li>Instant-NGP (SIGGRAPH 2022)</li> <li>DVGO (CVPR 2022)</li> </ul> </li> <li>Grid-based 연구들 <ul> <li>장점 : speed 개선</li> <li>단점 : 해상도가 증가하면 GPU memory 많이 소요<br/> 기존 연구들은 space complexity \(O(N^3)\) 인데,<br/> TensoRF는 이를 \(O(N^2)\) 으로 줄임</li> </ul> </li> </ul> <h2 id="algorithm">Algorithm</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/5-480.webp 480w,/assets/img/2024-09-17-TensoRF/5-800.webp 800w,/assets/img/2024-09-17-TensoRF/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Algorithm : <ul> <li>step 1)<br/> scene을 bounded cubic (grid)로 제한</li> <li>step 2)<br/> ray를 쏴서 sampled points를 구한 뒤<br/> 각 rank의 선과 면으로 projection하고<br/> 외적한 값들을 이용해서<br/> color와 volume density 계산</li> <li>step 2-1)<br/> volume density는 단순히 외적한 값들을 더해서 구함<br/> (VM Decomposition)</li> <li>step 2-2)<br/> color는 외적한 값들을 concat한 뒤<br/> function B와 function S에 통과시켜 얻음 <ul> <li>function B :<br/> 1개의 FC-layer<br/> appearance commonalities를 추상화는 Global Apperance Dictionary 역할</li> <li>function S :<br/> MLP 또는 SH(Spherical Harmonics) 함수</li> </ul> </li> </ul> </li> </ul> <h2 id="loss">Loss</h2> <ul> <li>sparse input images일 경우<br/> 적게 관측된 view에서는 outlier 혹은 noise가 발생할 수 있어<br/> overfitting 혹은 local minima 문제 발생<br/> \(\rightarrow\)<br/> regularization term 추가한 loss 사용<br/> e.g. TV(total variation) loss :<br/> pixel 값 간의 급격한 변화 (noise or outlier)를 억제하기 위해<br/> \(I_{i+1, j} - I_{i, j}\) 항과 \(I_{i, j+1} - I_{i, j}\) 항을 loss에 추가</li> </ul> <h2 id="coarse-to-fine">Coarse-to-Fine</h2> <ul> <li> <p>NeRF의 coarse-to-fine 기법 :<br/> \(w_i = T_i \alpha_{i}\) 의 PDF 분포에 따라<br/> 일부 구간을 더 많이 sampling</p> </li> <li>Mip-NeRF 360의 coarse-to-fine 기법 : <ul> <li>small coarse proposal-MLP는 many samples로 여러 번 evaluate하여 weight \(\hat w\) 를 구하고<br/> large fine NeRF-MLP는 less samples로 딱 한 번 evaluate하여 weight \(w\) 와 color \(c\) 를 구함</li> <li>proposal loss를 이용하여 NeRF-MLP의 지식을 proposal-MLP가 따라잡도록 함</li> </ul> </li> <li>TensoRF의 coarse-to-fine 기법 :<br/> 단순히 grid 크기를 upsampling<br/> Grid size(resolution)이 커질수록 선 또는 면이 더 촘촘해져서 3D scene의 high-freq. feature를 더 잘 잡아낼 수 있음</li> </ul> <h2 id="implementation">Implementation</h2> <ul> <li>Decomposition Rank : 총 48개 <ul> <li>RGB : 16, 4, 4</li> <li>volume density : 16, 4, 4</li> </ul> </li> <li> <p>Grid size : coarse-to-fine<br/> \(128^3\) 에서 \(300^3\) 으로 점점 증가시키면서 학습<br/> (2000, 3000, 4000, 5500, 7000 step에서 점차 증가시킴)</p> </li> <li> <p>Batch size : 4096 pixels</p> </li> <li>Adam optimizer, V100 GPU(16GB)</li> </ul> <h2 id="evaluation">Evaluation</h2> <ul> <li>기존 Grid-based 연구들과<br/> training speed는 유사하지만<br/> PSNR이 높고<br/> 모델 사이즈 및 GPU memory 사용량이 적음</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/6-480.webp 480w,/assets/img/2024-09-17-TensoRF/6-800.webp 800w,/assets/img/2024-09-17-TensoRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/7-480.webp 480w,/assets/img/2024-09-17-TensoRF/7-800.webp 800w,/assets/img/2024-09-17-TensoRF/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Ours-VM-192 : 4DGS, VM Decomposition, 192개의 rank <ul> <li>speed, PSNR : Ours-VM-192를 15000 iter.만큼만 진행했을 때 8분만에 기존 연구들보다 PSNR 높음</li> <li>memory : 기존 연구들보다 확연히 memory size 적음</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/8-480.webp 480w,/assets/img/2024-09-17-TensoRF/8-800.webp 800w,/assets/img/2024-09-17-TensoRF/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/9-480.webp 480w,/assets/img/2024-09-17-TensoRF/9-800.webp 800w,/assets/img/2024-09-17-TensoRF/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Param. 실험 : <ul> <li>Grid size가 증가할수록 성능 좋아지지만 speed 느려지고 model size 커짐</li> <li>CP Decomposition보다 VM Decomposition이 성능 더 좋음</li> <li>rank 개수가 증가할수록 성능 좋아짐</li> </ul> </li> <li>iter. : <ul> <li>iter.이 증가할수록 PSNR이 증가<br/> 5k iter.만 해도 PSNR이 30에 가까워지고, 점점 변동폭이 작아짐</li> </ul> </li> </ul> <h2 id="limitation">Limitation</h2> <ul> <li>scene을 bounded cubic 안에 제한해야<br/> projection을 통해 VM Decomposition이 가능하므로<br/> unbounded scene은 다루지 못함</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="radiance"/><category term="field"/><category term="tensor"/><category term="decomposition"/><summary type="html"><![CDATA[Tensorial Radiance Fields (ECCV 2022)]]></summary></entry><entry><title type="html">4D Gaussian Splatting</title><link href="https://semyeong-yu.github.io/blog/2024/4DGS/" rel="alternate" type="text/html" title="4D Gaussian Splatting"/><published>2024-09-14T12:00:00+00:00</published><updated>2024-09-14T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/4DGS</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/4DGS/"><![CDATA[<h2 id="4d-gaussian-splatting-for-real-time-dynamic-scene-rendering">4D Gaussian Splatting for Real-Time Dynamic Scene Rendering</h2> <h4 id="guanjun-wu-taoran-yi-jiemin-fang-lingxi-xie-xiaopeng-zhang-wei-wei-wenyu-liu-qi-tian-xinggang-wang">Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, Xinggang Wang</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2310.08528">https://arxiv.org/abs/2310.08528</a><br/> project website :<br/> <a href="https://guanjunwu.github.io/4dgs/index.html">https://guanjunwu.github.io/4dgs/index.html</a><br/> code :<br/> <a href="https://github.com/hustvl/4DGaussians">https://github.com/hustvl/4DGaussians</a><br/> referenced blog :<br/> <a href="https://xoft.tistory.com/54">https://xoft.tistory.com/54</a></p> </blockquote> <blockquote> <p>핵심 요약 :<br/> <code class="language-plaintext highlighter-rouge">3DGS를 dynamic scene에 적용하고자 할 때</code><br/> x, y, z, t를 input으로 갖는 encoder로서<br/> <code class="language-plaintext highlighter-rouge">4D scene을 2D planes로 표현하는 HexPlane 기법을 이용하겠다!</code></p> </blockquote> <h2 id="abstract">Abstract</h2> <ul> <li> <p>spatially-temporally-sparse input으로부터<br/> complex point motion을 정확하게 모델링하면서<br/> high efficiency로 real-time dynamic scene을 rendering하는 건 매우 challenging task</p> </li> <li>3DGS를 각 frame에 적용하는 게 아니라 4DGS라는 새로운 모델 제시 <ul> <li><code class="language-plaintext highlighter-rouge">오직 3DGS 한 세트</code> 필요</li> <li>4DGS framework : <ul> <li><code class="language-plaintext highlighter-rouge">Spatial-Temporal Structure Encoder</code> :<br/> HexPlane <d-cite key="neuralvoxel1">[22]</d-cite> 에서 영감을 받아<br/> decomposed neural voxel encoding algorithm을 이용해서<br/> <code class="language-plaintext highlighter-rouge">4D neural voxel을 2D voxel planes로 decompose</code>하여<br/> 2D voxel plane (param.)에 Gaussian <code class="language-plaintext highlighter-rouge">point-clouds (pts)의 spatial-temporal 정보를 encode</code></li> <li><code class="language-plaintext highlighter-rouge">Extremely Tiny Multi-head Gaussian Deformation Decoder</code> :<br/> 가벼운 MLP를 이용해서<br/> <code class="language-plaintext highlighter-rouge">Gaussian deformation을 예측</code>함</li> </ul> </li> </ul> </li> <li>4DGS :<br/> real-time (82 FPS) rendering at high (800 \(\times\) 800) resolution on RTX 3090 GPU</li> </ul> <h2 id="contribution">Contribution</h2> <ul> <li> <p>Gaussian <code class="language-plaintext highlighter-rouge">motion</code>과 <code class="language-plaintext highlighter-rouge">shape</code>-deformation을 모두 모델링할 수 있는 4DGS framework 제시<br/> w. efficient <code class="language-plaintext highlighter-rouge">Gaussian deformation field</code> network</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">multi-resolution</code> encoding<br/> (only on spatial planes)<br/> (connect nearby 3D Gaussians to build rich Gaussian features)<br/> by efficient <code class="language-plaintext highlighter-rouge">spatial-temporal structure encoder</code></p> </li> <li> <p>SOTA <code class="language-plaintext highlighter-rouge">performance</code>이면서 <code class="language-plaintext highlighter-rouge">real-time</code> rendering on <code class="language-plaintext highlighter-rouge">dynamic</code> scenes<br/> e.g. 82 FPS at resol. 800 \(\times\) 800 for synthetic dataset<br/> e.g. 30 FPS at resol. 1352 \(\times\) 1014 for real dataset</p> </li> <li> <p>4D scenes에서의 editing 및 tracking에 활용 가능</p> </li> </ul> <h2 id="related-works">Related Works</h2> <h3 id="novel-view-synthesis">Novel View Synthesis</h3> <ul> <li>static scene : <ul> <li>light fields <d-cite key="lightfield">[1]</d-cite>, mesh <d-cite key="mesh1">[2]</d-cite> <d-cite key="mesh2">[3]</d-cite> <d-cite key="mesh3">[4]</d-cite> <d-cite key="mesh4">[5]</d-cite>, voxels <d-cite key="voxel1">[6]</d-cite> <d-cite key="voxel2">[7]</d-cite> <d-cite key="voxel3">[8]</d-cite>, multi-planes <d-cite key="multiplane">[9]</d-cite> 이용한 methods</li> <li>NeRF-based methods <a href="https://semyeong-yu.github.io/blog/2024/NeRF/">NeRF</a> <a href="https://semyeong-yu.github.io/blog/2024/MipNeRF/">MipNeRF</a> <d-cite key="nerf++">[10]</d-cite></li> </ul> </li> <li>dynamic scene : <ul> <li>NeRF-based methods <d-cite key="dynerf1">[11]</d-cite> <d-cite key="dynerf2">[12]</d-cite> <d-cite key="dynerf3">[13]</d-cite></li> <li><code class="language-plaintext highlighter-rouge">explicit voxel grid</code> <d-cite key="voxeltemp1">[14]</d-cite> <d-cite key="voxeltemp2">[15]</d-cite> <d-cite key="voxeltemp3">[16]</d-cite> <d-cite key="voxeltemp4">[17]</d-cite> :<br/> temporal info. 모델링하기 위해 explicit voxel grid 사용</li> <li><code class="language-plaintext highlighter-rouge">flow-based</code> methods <d-cite key="flow1">[18]</d-cite> <d-cite key="flow2">[19]</d-cite> <d-cite key="voxeltemp3">[16]</d-cite> <d-cite key="flow3">[20]</d-cite> <d-cite key="flow4">[21]</d-cite> :<br/> nearby frames를 blending하는 warping algorithm 사용</li> <li><code class="language-plaintext highlighter-rouge">decomposed neural voxels</code> <d-cite key="neuralvoxel1">[22]</d-cite> <d-cite key="neuralvoxel2">[23]</d-cite> <d-cite key="neuralvoxel3">[24]</d-cite> <d-cite key="neuralvoxel4">[25]</d-cite> <d-cite key="neuralvoxel5">[26]</d-cite> <d-cite key="neuralvoxel6">[27]</d-cite> :<br/> 빠른 training on dynamic scenes 가능<br/> (Fig 1.의 (b))</li> <li><code class="language-plaintext highlighter-rouge">multi-view</code> setups 다루기 위한 methods <d-cite key="multi1">[28]</d-cite> <d-cite key="multi2">[29]</d-cite> <d-cite key="multi3">[30]</d-cite> <d-cite key="multi4">[31]</d-cite> <d-cite key="multi5">[32]</d-cite> <d-cite key="multi6">[33]</d-cite></li> <li>본 논문 (4DGS) :<br/> 위에서 언급된 methods는 빠른 training은 가능했지만 real-time rendering on dynamic scenes는 여전히 어려웠음<br/> \(\rightarrow\)<br/> 본 논문은 빠른 training 및 rendering pipeline 제시<br/> (Fig 1.의 (c))</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/1-480.webp 480w,/assets/img/2024-09-14-4DGS/1-800.webp 800w,/assets/img/2024-09-14-4DGS/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 1. dynamic scene rendering methods </div> <ul> <li>Fig 1. 설명 :<br/> dynamic scene을 rendering하는 여러 방법들 소개 <ul> <li>(a) : Deformation-based (Canonical Mapping) Volume Rendering<br/> point-deformation-field를 이용해서<br/> sampled points를 canonical space로 mapping<br/> (하나의 ray 위의 sampled points가 다같이 canonical space로 mapping되므로<br/> 각 point의 서로 다른 속도를 잘 모델링하지 못함)</li> <li>(b) : Time-aware Volume Rendering<br/> 각 timestamp에서의 각 point의 feature를 직접 개별적으로 계산<br/> (path는 그대로)</li> <li>(c) : 4DGS <br/> compact <code class="language-plaintext highlighter-rouge">Gaussian-deformation-field</code>를 이용해서<br/> 기존의 3D Gaussians를 특정 timestamp의 3D Gaussians로 변환<br/> ((a)와 유사하긴 하지만<br/> 각 Gaussian이 <code class="language-plaintext highlighter-rouge">ray에 의존하지 않고 서로 다른 속도로 이동</code> 가능)</li> </ul> </li> </ul> <h3 id="neural-rendering-w-point-clouds">Neural Rendering w. Point Clouds</h3> <ul> <li> <p>3D scenes를 나타내기 위해 meshes, point-clouds, voxels, hybrid ver. 등 여러 분야가 연구되어 왔는데<br/> 그 중 point-cloud representation을 volume rendering과 결합하면<br/> dynamic novel view synthesis task도 잘 수행 가능</p> </li> <li> <p>3DGS :<br/> <code class="language-plaintext highlighter-rouge">explicit</code> representation이라서,<br/> <code class="language-plaintext highlighter-rouge">differentiable</code> <code class="language-plaintext highlighter-rouge">point</code>-based splatting이라서,<br/> <code class="language-plaintext highlighter-rouge">real-time</code> renderer라서 주목받음</p> </li> <li> <p>3DGS on dynamic scenes :</p> <ul> <li>Dynamic3DGS <d-cite key="dyna3DGS">[34]</d-cite> : <ul> <li>3D Gaussian 개수를 고정하고<br/> 각 timestamp \(t_i\) 마다 각 3D Gaussian의 position, variance를 tracking</li> <li>문제점 : <ul> <li>need dense multi-view input images</li> <li>prev. frame의 모델링이 부적절하면 전체적인 성능이 떨어짐</li> <li>linear memory consumption \(O(tN)\)<br/> for \(t\)-time steps and \(N\)-3D Gaussians</li> </ul> </li> </ul> </li> <li>4DGS (본 논문) : <ul> <li>very compact network로 3D Gaussian motion을 모델링하기 때문에<br/> training 효율적이고 real-time rendering</li> <li>memory consumption \(O(N+F)\)<br/> for \(N\)-3D Gaussians, \(F\)-parameters of Gaussian-deformation-field network</li> </ul> </li> <li>4DGS (Zeyu Yang) <d-cite key="4DGS1">[35]</d-cite> : <ul> <li>marginal temporal Gaussian 분포를 기존의 3D Gaussian 분포에 추가하여<br/> 3D Gaussians를 4D로 uplift</li> <li>However, 그러면 각 3D Gaussian은 오직 their local temporal space에만 focus</li> </ul> </li> <li>Deformable-3DGS (Ziyi Yang) <d-cite key="deformable3DGS">[36]</d-cite> : <ul> <li>본 논문처럼 MLP deformation network를 도입하여 dynamic scenes의 motion을 모델링</li> <li>본 논문 (4DGS)도 이와 유사하지만 training을 효율적으로 만듦</li> </ul> </li> <li>Spacetime-GS (Zhan Li) <d-cite key="spacetimeGS">[37]</d-cite> : <ul> <li>각 3D Gaussian을 individually tracking</li> </ul> </li> </ul> </li> </ul> <h3 id="dynamic-nerf-with-deformation-fields">Dynamic NeRF with Deformation Fields</h3> <ul> <li> <p>모든 dynamic NeRF는 아래의 식을 따른다<br/> \(c, \sigma = M(x, d, t, \lambda)\)<br/> where \(c \in R^3, \sigma \in R, x \in R^3, d \in R^2, t \in R, \lambda \in R\)<br/> where \(\lambda\) is optional input (frame-dependent code to build topological and appearance changes) <d-cite key="dynerf2">[12]</d-cite> <d-cite key="wild">[38]</d-cite></p> </li> <li> <p>deformation NeRF-based methods는<br/> Fig 1. (a)에서처럼<br/> deformation network \(\phi_{t} : (x, t) \rightarrow \Delta x\) 로 world-to-canonical mapping 한 뒤<br/> RGB color와 volume density를 뽑는다<br/> \(c, \sigma = NeRF(x+\Delta x, d, \lambda)\)</p> </li> <li> <p>4DGS (본 논문)은<br/> <code class="language-plaintext highlighter-rouge">Gaussian deformation field</code> network \(F\) 이용해서<br/> time \(t\) 에서의 <code class="language-plaintext highlighter-rouge">canonical-to-world mapping</code>을 직접 계산한 뒤<br/> differential splatting(rendering) 수행</p> </li> </ul> <h2 id="method">Method</h2> <h3 id="overview-gaussian-deformation-field-network">Overview (Gaussian Deformation Field Network)</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/2-480.webp 480w,/assets/img/2024-09-14-4DGS/2-800.webp 800w,/assets/img/2024-09-14-4DGS/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 2. Pipeline of this model (Gaussian Deformation Field Network) </div> <ul> <li>Fig 2. 설명 : <ul> <li>static 3D Gaussian set을 만듦</li> <li>각 3D Gaussian의 center 좌표 \(x, y, z\) 와 timestamp \(t\) 를<br/> Gaussian Deformation Field Network의 input으로 준비</li> <li>Spatial-Temporal Structure Encoder :<br/> multi-resolution voxel planes를 query하여<br/> voxel feature를 계산<br/> (temporal 및 spatial feature를 둘 다 encode 가능)</li> <li>Tiny Multi-head Gaussian Deformation Decoder :<br/> position, rotation, scaling head에서 각각 해당 feature를 decode하여<br/> 각 3D Gaussian의 position, rotation, scaling 변화량을 얻어서<br/> timestamp \(t\) 에서의 변형된 3D Gaussians를 얻음</li> </ul> </li> </ul> <h3 id="spatial-temporal-structure-encoder">Spatial-Temporal Structure Encoder</h3> <ul> <li> <p>근처에 있는 3D Gaussians끼리는 항상 spatial 및 temporal 정보를 비슷하게 공유하고 있다.<br/> 따라서 HexPlane 기법에서는 각 Gaussian이 따로 변형되는 게 아니라,<br/> 여러 <code class="language-plaintext highlighter-rouge">adjacent 3D Gaussian</code>들이 군집처럼 연결되어 함께 변형되므로<br/> motion과 shape-deformation을 정확하게 예측할 수 있다<br/> 이로써 변형된 geometry를 더 정확히 모델링하고 avulsion(벗겨짐?)을 방지할 수 있음</p> </li> <li> <p>기존 논문 설명 (Backgrounds) :</p> <ul> <li>TensoRF : <a href="https://semyeong-yu.github.io/blog/2024/TensoRF/">Link</a></li> <li>HexPlane <d-cite key="neuralvoxel1">[22]</d-cite> :<br/> 4차원(\(XYZT\))을 모델링하기 위해<br/> 3개 타입의 rank로 decomposition (\(XY\) 평면 - \(ZT\) 평면, \(XZ\) 평면 - \(YT\) 평면, \(YZ\) 평면 - \(XT\) 평면)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/3-480.webp 480w,/assets/img/2024-09-14-4DGS/3-800.webp 800w,/assets/img/2024-09-14-4DGS/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> HexPlane Overview </div> <ul> <li>Spatial-Temporal Structure Encoder (1) : <ul> <li>vanilla 4D neural voxel은 memory를 많이 잡아먹기 때문에<br/> 4D neural voxel(\(XYZT\))을 6개의 multi-resol. planes로 decompose하는<br/> 4D K-Planes module <d-cite key="neuralvoxel2">[23]</d-cite> 사용</li> <li>3D Gaussians는 bounding plane voxels에 포함되어<br/> Gaussians의 deformation도 nearby temporal voxels에 encode될 수 있음 <code class="language-plaintext highlighter-rouge">????</code></li> <li>기존 논문들 <d-cite key="voxeltemp1">[14]</d-cite> <d-cite key="neuralvoxel1">[22]</d-cite> <d-cite key="neuralvoxel2">[23]</d-cite> <d-cite key="neuralvoxel5">[26]</d-cite> 에서 영감을 받아<br/> Spatial-Temporal Structure Encoder는<br/> multi-resolution HexPlane \(R(i, j)\) 와 tiny MLP \(\phi_{d}\) 로 구성됨</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/4-480.webp 480w,/assets/img/2024-09-14-4DGS/4-800.webp 800w,/assets/img/2024-09-14-4DGS/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Spatial-Temporal Structure Encoder (2) : <ul> <li>multi-resolution HexPlane \(R(i, j)\) :<br/> 본 논문에서는 TensoRF와 달리 Grid resol.을 점점 증가시키지 않고, 애초에 multi-resolution으로 decomposition의 rank를 구성함<br/> \(f_{h} = \cup_{l} \prod \text{interp}(R_{l}(i, j))\) <ul> <li>where<br/> \(f_h \in R^{h \ast l}\) : feature of decomposed neural voxel<br/> \(R_{l}(i, j) \in R^{h \times lN_i \times lN_j}\) : 2D voxel plane (nn.Parameter())<br/> \(h\) : hidden dim.<br/> \(\{ i, j \} \in \{ (x, y), (x, z), (y, z), (x, t), (y, t), (z, t) \}\) : 6 종류의 planes<br/> \(N\) : voxel grid의 basic resol.<br/> \(l \in \{ 1, 2 \}\) : upsampling scale (multi-resol.)<br/> \(\text{interp}\) : bilinear interpolation (plane의 grid의 네 꼭짓점으로부터 interpolation으로 voxel feature 뽑아냄)<br/> \(\prod\) : product over planes (K-Planes <d-cite key="neuralvoxel2">[23]</d-cite> 참고)<br/> \(\cup_{l}\) : multi-resol.에 대해 concat 또는 add</li> <li><a href="https://github.com/hustvl/4DGaussians/blob/master/scene/hexplane.py">Github Code</a> 에서 <ul> <li>forward()</li> <li>get_density() <ul> <li>self.grids : multi-resol. HexPlane<br/> 즉, nn.ModuleList() of init_grid_param()</li> <li>init_grid_param() : HexPlane<br/> 즉, nn.ParameterList() of nn.Parameter()<br/> where<br/> range(in_dim) = [0, 1, 2, 3] (x, y, z, t) 중에 grid_nd = 2개의 조합(plane)을 뽑아서<br/> 각 nn.Parameter()는 2D grid plane \(R_{l}(i, j)\) for \(\{ i, j \} \in \{ (x, y), (x, z), (y, z), (x, t), (y, t), (z, t) \}\)<br/> w. shape \((1, D_{out}, \text{resol.}[j], \text{resol.}[i])\)<br/> e.g. \(R_{l}(x, t)\), 즉 \(XT\) plane은 nn.Parameter()<br/> w. shape \((1, D_{out}, \text{resol.}[3], \text{resol.}[0])\)</li> </ul> </li> <li>interpolate_ms_features() : \(f_{h} = \cup_{l} \prod \text{interp}(R_{l}(i, j))\) 반환</li> <li>grid_sample_wrapper() : \(\text{interp}(R_{l}(i, j))\) 반환</li> <li>grid_sampler() : F.grid_sample() <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html">Link</a><br/> second argument(pts) 좌표에서의 값을 구하기 위해 first argument(grid \(R_{l}(i, j)\))의 값을 interpolate<br/> 그럼 이제 dynamic 3D scene을 <code class="language-plaintext highlighter-rouge">4D neural voxel</code> 대신 <code class="language-plaintext highlighter-rouge">2D voxel plane</code> \(R_{l}(i, j)\) 이라는 param.들로 표현 가능</li> </ul> </li> </ul> </li> </ul> </li> <li>Spatial-Temporal Structure Encoder (3) : <ul> <li>tiny MLP \(\phi_{d}\) :<br/> \(f_d = \phi_{d} (f_h)\)<br/> merge all the features</li> <li>공간상(e.g. \(XY\) 평면) 또는 시간상(e.g. \(XT\) 평면)으로 인접한 voxel은<br/> HexPlane \(R(i, j)\) 에서 유사한 feature를 가져서 유사한 Gaussian param. 변화량을 가지므로<br/> optimization 진행됨에 따라<br/> Gaussian의 covariance가 줄어들면서 작은 3D Gaussian들이 모여서 dense해진다 <code class="language-plaintext highlighter-rouge">?????</code></li> </ul> </li> </ul> <h3 id="extremely-tiny-multi-head-gaussian-deformation-decoder">Extremely Tiny Multi-head Gaussian Deformation Decoder</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/5-480.webp 480w,/assets/img/2024-09-14-4DGS/5-800.webp 800w,/assets/img/2024-09-14-4DGS/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Multi-head Gaussian Deformation Decoder : <ul> <li>매우 작은 multi-head decoder로 position, scaling, rotation 변화량을 얻음<br/> \(\Delta \chi = \phi_{x}(f_d)\)<br/> \(\Delta r = \phi_{r}(f_d)\)<br/> \(\Delta s = \phi_{s}(f_d)\)</li> <li>그러면 변형된 deformed 3D Gaussians 계산할 수 있음<br/> \((\chi ' , r ' , s ') = (\chi + \Delta \chi, r + \Delta r, s + \Delta s)\) 에 대해<br/> next time \(t\) 의 deformed 3D Gaussian set은<br/> \(G ' = \{ \chi ' , r ' , s ', \sigma, c \}\)</li> <li>근데 실제 implementation 할 때는 speed 증가 위해<br/> scaling(size), rotation, color, opacity는 고정하고<br/> position 변화량만 구함</li> <li><a href="https://github.com/hustvl/4DGaussians/blob/master/scene/deformation.py">Github Code</a> 에서 <ul> <li>Class deform_network()의 forward_dynamic()</li> <li>Class Deformation()의 forward_dynamic() <ul> <li>hidden : encoder(HexPlane과 MLP) 거쳐 얻은 feature</li> <li>self.pos_deform, self.scales_deform, self.rotations_deform : tiny Multi-head decoder<br/> hidden으로부터 \(\Delta \chi, \Delta r, \Delta s\) 얻음</li> <li>self.static_mlp :<br/> hidden으로부터 \(\text{mask}\) 얻음</li> <li>position :<br/> \(\chi ' = \gamma(\chi) \times \text{mask} + \Delta \chi\)</li> <li>scaling :<br/> \(s ' = \gamma(s) \times \text{mask} + \Delta s\)</li> <li>rotation :<br/> \(r ' = \gamma(r) + \Delta r\)<br/> 또는<br/> \(r ' =\) quaternion product of \(\gamma(r)\) and \(\Delta r\)</li> <li>opacity, SH 도 deform 가능하게 짜놓긴 함<br/> \(\alpha ' = \alpha \times \text{mask} + \Delta \alpha\)<br/> \(k ' = k \times \text{mask} + \Delta k\)</li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/16-480.webp 480w,/assets/img/2024-09-14-4DGS/16-800.webp 800w,/assets/img/2024-09-14-4DGS/16-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>self._deformation = deform_network(args)</p> <h3 id="optimization">Optimization</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/6-480.webp 480w,/assets/img/2024-09-14-4DGS/6-800.webp 800w,/assets/img/2024-09-14-4DGS/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p><code class="language-plaintext highlighter-rouge">warm-up</code> :<br/> 처음 3000 iter. 동안은<br/> Gaussian Deformation Field Network 없이<br/> 3DGS의 SfM points initialization 이용하여<br/> <code class="language-plaintext highlighter-rouge">static 3DGS</code> optimize 하고,<br/> 그 후에 dynamic scene에 대해 4DGS framework를 fine-tuning 형태로 학습</p> </li> <li> <p>Loss :<br/> \(L = | \hat I - I | + L_{tv}\)</p> <ul> <li>L1 recon. loss</li> <li><code class="language-plaintext highlighter-rouge">total-variational loss</code> : <ul> <li>sparse input images일 경우에 적게 관측된 view에서는 noise 및 outlier 때문에 overfitting 및 local minima 문제가 발생할 수 있으므로<br/> <code class="language-plaintext highlighter-rouge">regularization</code> term 필요</li> <li>pixel 값 간의 급격한 변화를 억제하기 위해<br/> \(I_{i+1, j} - I_{i, j}\) 항과 \(I_{i, j+1} - I_{i, j}\) 항을 loss에 추가</li> </ul> </li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <ul> <li> <p>single RTX 3090 GPU</p> </li> <li> <p>Synthetic Dataset :</p> <ul> <li>designed for monocular settings</li> <li>camera poses for each timestamp은 거의 randomly generated 수준</li> <li>scene 당 50-200 frames</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/12-480.webp 480w,/assets/img/2024-09-14-4DGS/12-800.webp 800w,/assets/img/2024-09-14-4DGS/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Real-world Dataset : <ul> <li>by HyperNeRF <d-cite key="dynerf2">[12]</d-cite> and Neu3D <d-cite key="neuralvoxel4">[25]</d-cite></li> <li>HyperNeRF dataset :<br/> one or two cameras<br/> with straightforward camera motion<br/> (1,2개의 camera를 직관적인 경로로 움직이며 촬영)</li> <li>Neu3D dataset :<br/> 15 to 20 static cameras<br/> with extended periods and complex camera motions<br/> (15-20개의 많은 정적인 camera로 오랜 시간 동안씩 촬영하며 복잡한 경로로 camera를 움직임)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/13-480.webp 480w,/assets/img/2024-09-14-4DGS/13-800.webp 800w,/assets/img/2024-09-14-4DGS/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="results">Results</h3> <ul> <li>Metrics : <ul> <li>quality :<br/> PSNR<br/> LPIPS<br/> SSIM<br/> DSSIM<br/> MS-SSIM</li> <li>speed :<br/> FPS<br/> training times</li> <li>memory :<br/> storage</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/7-480.webp 480w,/assets/img/2024-09-14-4DGS/7-800.webp 800w,/assets/img/2024-09-14-4DGS/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/8-480.webp 480w,/assets/img/2024-09-14-4DGS/8-800.webp 800w,/assets/img/2024-09-14-4DGS/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Im4D <d-cite key="multi2">[29]</d-cite> 는 본 논문과 유사하게 high-quality이지만<br/> multi-cam 방식을 쓰기 때문에 monocular scene을 모델링하기 어렵</li> </ul> <h3 id="ablation-study">Ablation Study</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/9-480.webp 480w,/assets/img/2024-09-14-4DGS/9-800.webp 800w,/assets/img/2024-09-14-4DGS/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Spatial-Temporal Structure Encoder : <ul> <li>explicit HexPlane encoder는<br/> 3DGS의 spatial 및 temporal 정보를 모두 encode 하면서<br/> purely explicit method <d-cite key="dyna3DGS">[34]</d-cite> 보다 storage 공간 아낄 수 있음</li> <li>만약에 HexPlane encoder 없이 shallow MLP encoder만 쓰면<br/> 복잡한 deformation 모델링 어렵</li> </ul> </li> <li>3D Gaussian Initialization : <ul> <li>처음에 warm-up으로 SfM points initialization 한 뒤 static 3DGS optimize 부터 해야<br/> 아래의 장점들 있음 <ul> <li>3DGS 일부가 dynamic part에 분포되도록 함</li> <li>3DGS를 미리 학습해야 deformation field가 dynamic part에 더 집중 가능</li> <li>deformation field 학습 시 numerical errors를 방지하여 훈련 과정이 더 stable</li> </ul> </li> </ul> </li> <li>Multi-head Gaussian Deformation Decoder : <ul> <li>3D Gaussian motion을 modeling함으로써 dynamic scene을 잘 표현할 수 있도록 해줌</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/15-480.webp 480w,/assets/img/2024-09-14-4DGS/15-800.webp 800w,/assets/img/2024-09-14-4DGS/15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Neural Voxel Encoder : <ul> <li>implicit MLP-based neural voxel encoder (voxel grid)가 아니라<br/> explicit Dynamic 3DGS 기법을 사용할 경우<br/> rendering quality는 떨어지지만 FPS 및 storage는 향상</li> </ul> </li> <li>Two-stage Training : <ul> <li>static 3DGS stage \(\rightarrow\) dynamic 4DGS stage (fine-tuning) 으로<br/> 분할해서 학습할 경우 성능 향상<br/> (참고로 D-NeRF, DyNeRF에서는 point-clouds가 주어지지 않아서 어려운 task를 다룸)</li> </ul> </li> <li>Image-based Loss : <ul> <li>LPIPS loss, SSIM loss 같은 image-based loss를 사용할 경우<br/> training speed도 느려지고 quality도 떨어짐</li> <li>그 이유는<br/> image-based loss로 motion 부분을 fine-tuning하는 건 어렵고 복잡</li> </ul> </li> <li>Model Capacity (MLP size) : <ul> <li>voxel plane resol. 또는 MLP 크기가 증가할수록<br/> quality 향상되지만 FPS 및 storage 악화</li> </ul> </li> <li>Fast Training : <ul> <li>7k iter. 까지만 학습해도(training 시간 짧음) 괜찮은 PSNR 달성</li> </ul> </li> </ul> <h3 id="discussion">Discussion</h3> <ul> <li>Tracking with 3D Gaussians : <ul> <li>Dynamic3DGS <d-cite key="dyna3DGS">[34]</d-cite> 와 달리<br/> 본 논문은 monocular setting에서도 low storage로 3D object tracking 가능<br/> (e.g. 10MB for 3DGS and 8MB for deformation field network)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/10-480.webp 480w,/assets/img/2024-09-14-4DGS/10-800.webp 800w,/assets/img/2024-09-14-4DGS/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Composition(Editing) with 4D Gaussians : <ul> <li>Dynamic3DGS <d-cite key="dyna3DGS">[34]</d-cite> 에서처럼<br/> 4DGS editing 가능</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/11-480.webp 480w,/assets/img/2024-09-14-4DGS/11-800.webp 800w,/assets/img/2024-09-14-4DGS/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Rendering Speed (FPS) : <ul> <li>3DGS 수와 FPS는 반비례 관계인데<br/> Gaussians 수가 30,000개 이하이면 single RTX 3090 GPU에서 90 FPS 까지 가능</li> <li>이처럼 real-time FPS를 달성하려면<br/> resolution, Gaussian 수, Gaussian deformation field network 용량, hardware constraints 등 여러 요인을 조절해야 함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/14-480.webp 480w,/assets/img/2024-09-14-4DGS/14-800.webp 800w,/assets/img/2024-09-14-4DGS/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="limitation">Limitation</h3> <ul> <li>아래의 경우엔 학습 잘 안 됨 <ul> <li><code class="language-plaintext highlighter-rouge">large motions</code>일 경우</li> <li><code class="language-plaintext highlighter-rouge">background points</code>가 없을 경우</li> <li><code class="language-plaintext highlighter-rouge">camera pose</code>가 <code class="language-plaintext highlighter-rouge">unprecise</code>(부정확)할 경우</li> </ul> </li> <li> <p>추가적인 supervision 없이<br/> <code class="language-plaintext highlighter-rouge">static</code> Gaussians와 <code class="language-plaintext highlighter-rouge">dynamic</code> Gaussians의 joint motion을 구분하는 건 아직 어려운 과제</p> </li> <li><code class="language-plaintext highlighter-rouge">urban(large)-scale</code> recon.일 경우엔<br/> 3DGS 수가 훨씬 많아서<br/> Gaussian deformation field network를 query하기에 너무 무거우므로 좀 더 compact한 algorithm이 필요</li> </ul> <h3 id="conclusion">Conclusion</h3> <ul> <li> <p>4DGS framework for <code class="language-plaintext highlighter-rouge">real-time</code> <code class="language-plaintext highlighter-rouge">dynamic</code> scene rendering</p> </li> <li>efficient deformation field network to model motions and shape-deformation <ul> <li>Spatial-temporal structure encoder :<br/> adjacent Gaussians가 비슷하게 encode되도록 spatial-temporal 정보를 encode</li> <li>Multi-head Gaussian deformation decoder :<br/> position, scaling, rotation을 각각 modeling</li> </ul> </li> <li>dynamic scenes 모델링 뿐만 아니라<br/> 4D object tracking 및 editing에도 활용 가능</li> </ul> <h2 id="code-flow">Code Flow</h2> <ul> <li>TBD</li> </ul> <h2 id="question">Question</h2> <ul> <li>Q1 : 본 논문을 한 문장으로 요약하자면,<br/> 3DGS를 dynamic scene에 적용하고자 할 때 4D 정보를 효율적으로 encode하기 위해 2D planes로 scene을 표현하는 HexPlane 기법을 이용하겠다!인데,<br/> 본 논문이 novelty가 있는지 의구심이 듭니다.</li> <li> <p>A1 : 3DGS 논문 자체가 나온 지 얼마 안 돼서<br/> 기존 논문(HexPlane) 아이디어를 3DGS에 적용하는 논문들이 아직까지는 많이 채택되는 것 같다.</p> </li> <li>Q2 : 본 포스팅에서 코드 리뷰는 encoder (HexPlane) 쪽만 진행하였는데,<br/> Multi-head Gaussian deformation decoder로 position, scaling, rotation 변화량을 구해서<br/> Deformed(변형된) 3DGS를 구하는 부분의 코드도 보고 싶습니다.</li> <li>A2 : 포스팅의 “Extremely Tiny Multi-head Gaussian Deformation Decoder” 부분에 해당 내용을 추가하였습니다.</li> </ul> <h2 id="appendix">Appendix</h2> <ul> <li>TBD</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="GS"/><category term="4d"/><category term="dynamic"/><category term="rendering"/><summary type="html"><![CDATA[4D Gaussian Splatting for Real-Time Dynamic Scene Rendering (CVPR 2024)]]></summary></entry><entry><title type="html">Lagrange Multiplier Method</title><link href="https://semyeong-yu.github.io/blog/2024/Lagrange/" rel="alternate" type="text/html" title="Lagrange Multiplier Method"/><published>2024-09-14T11:00:00+00:00</published><updated>2024-09-14T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Lagrange</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Lagrange/"><![CDATA[<p>본 포스팅 출처 : <a href="https://untitledtblog.tistory.com/96">Link</a></p> <h3 id="lagrange-multiplier-method">Lagrange Multiplier Method</h3> <ul> <li> <p>언제? :<br/> multi-variate function을 optimize할 때<br/> <code class="language-plaintext highlighter-rouge">constraint</code>가 존재할 경우<br/> 최적점의 필요조건을 찾기 위해<br/> Lagrange Multiplier Method 사용</p> </li> <li> <p>핵심 아이디어 :<br/> 주어진 function \(f\) 와 constraint \(g_{i}\) 에 대해<br/> \(f\) 와 \(g_{i}\) 의 <code class="language-plaintext highlighter-rouge">접점 (경계)</code>에 \(f\) 의 최댓(솟)값이 존재할 수도 있다!<br/> 그리고 접점에서는 \(\nabla f\) 를 \(\nabla g_{i}\) 들의 linear comb.로 표현할 수 있다!<br/> (다만, 접점은 극점이므로 반드시 최댓값 또는 최솟값이 존재하는 건 아니다)</p> </li> </ul> <h3 id="equality-constraint">Equality Constraint</h3> <ul> <li>\(g_{i}\) 가 등식일 경우 (e.g. \(g_{i} = 1 - \phi_{i}^T\phi_{i} = 0\)) :<br/> 접점에서 gradient가 평행하므로 (이에 대한 수식 증명은 참고한 포스팅 <a href="https://untitledtblog.tistory.com/96">Link</a> 에 있음)<br/> \(\nabla f = \sum_{i=1}^N \lambda_{i} \nabla g_{i}\) 로부터<br/> 아래처럼 풀면 된다<br/> (단, \(\lambda \neq 0\))<br/> (단, \(\nabla f\) 과 \(\nabla g_{i}\) 은 평행할 뿐 방향은 반대여도 됨) <ul> <li>방법 1)<br/> \(\nabla f + \sum_{i=1}^N \lambda_{i} \nabla g_{i} = 0\) 와 \(g_{i} = 0\) 을 연립하여 풀면 된다</li> <li>방법 2)<br/> Equivalently,<br/> \(L = f + \sum_{i=1}^N \lambda_{i} g_{i}\) 에 대해<br/> \(L\) 의 극소(대)점을 찾으면 된다<br/> 즉, \(f, g_{i}\) 가 \(x_{j}\) 에 대한 함수일 경우<br/> \(\frac{\partial L}{\partial x_{j}} = 0\) 과 \(\frac{\partial L}{\partial \lambda_{i}} = 0\) 을 연립하여 풀면 된다</li> </ul> </li> </ul> <h3 id="inequality-constraint">Inequality Constraint</h3> <p>등식 constraint일 때의 Lagrange Multiplier Method는 완전히 이해했는데,<br/> 부등식 constraint일 때의 Lagrange Multiplier Method는 아직 이해 못함.<br/> 추후에 고칠(이해할) 필요 있음. TBD</p> <ul> <li>부등식 constraint일 경우 <code class="language-plaintext highlighter-rouge">KKT (Karush-Kuhn-Tucker) 조건</code>을 만족해야 한다 <ul> <li>1) \(f\) 는 모든 variable (e.g. \(x, y\))에 대해 differentiable</li> <li>2) \(\lambda_{i} \nabla g_{i} = 0\)</li> <li>3) \(\lambda{i} \geq 0\)<br/> (만약 \(\lambda_{i} \lt 0\) 일 경우 \(\nabla f\) 와 \(\nabla g_{i}\) 가 평행하지만 방향이 반대라는 의미이므로 두 함수의 최적점이 서로 반대 방향에 위치하여 constraint를 만족할 수 없다)<br/> (따라서 \(\lambda_{i} \geq 0\) 이어야만 (\(\nabla f\) 방향과 \(\nabla g_{i}\) 방향이 일치해야만) \(\nabla f\) 를 \(\nabla g_{i}\) 들의 linear comb.로 표현 가능한지 아닌지를 판정할 수 있다)</li> </ul> </li> <li>\(g_{i}\) 가 부등식일 경우 (e.g. \(g_{i} = 1 - \phi_{i}^T\phi_{i} \leq 0\)) :<br/> \(\lambda_{i} \nabla g_{i} = 0\) <ul> <li>\(\nabla g_{i} = 0\) 일 경우 :<br/> constraint \(g_{i} \leq 0\) 을 항상 만족하므로<br/> \(\nabla f \geq 0\) 을 푸는 문제로 바꿔 쓸 수 있다<br/> (constraint 없이 \(f\) 만 최적화하면 됨!)</li> <li>\(\lambda_{i} = 0\) 일 경우 :<br/> \(\nabla f\) 를 \(\nabla g_{i}\) 들의 linear comb.로 표현 불가능하다는 의미이므로<br/> 비교하는 두 gradient가 평행하지 않다<br/> 따라서 gradient 방향에 따라 constraint 만족하는 지 여부가 달라지므로<br/> \(\nabla g_{i} \gt 0\) 인 경우와 \(\nabla g_{i} \lt 0\) 인 경우를 모두 따져봐서<br/> 어떤 경우(방향)가 constraint를 만족하는지 확인해야 한다</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="math"/><category term="Lagrange"/><category term="min"/><category term="max"/><category term="constraint"/><summary type="html"><![CDATA[find min(max) with constraint]]></summary></entry><entry><title type="html">EE534 Pattern Recognition</title><link href="https://semyeong-yu.github.io/blog/2024/Pattern/" rel="alternate" type="text/html" title="EE534 Pattern Recognition"/><published>2024-09-10T11:00:00+00:00</published><updated>2024-09-10T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Pattern</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Pattern/"><![CDATA[<blockquote> <p>Lecture :<br/> 24F EE534 Pattern Recognition<br/> by KAIST Munchurl Kim <a href="https://www.viclab.kaist.ac.kr/">VICLab</a></p> </blockquote> <h2 id="chapter-1-overview">Chapter 1. Overview</h2> <h3 id="discriminative-vs-generative">Discriminative vs Generative</h3> <ul> <li>Discriminative model : <ul> <li> <table> <tbody> <tr> <td>learn $$P(Y</td> <td>X)\(to maximize\)P(Y</td> <td>X)$$ directly</td> </tr> </tbody> </table> </li> <li>e.g. logistic regression, SVM, nearest neighbor, CRF, Decision Tree and Random Forest, traditional NN</li> </ul> </li> <li>Generative model : <ul> <li> <table> <tbody> <tr> <td>learn $$P(X</td> <td>Y)\(and\)P(Y)\(to maximize\)P(X, Y) = P(X</td> <td>Y)P(Y)$$</td> </tr> <tr> <td>where can learn $$P(Y</td> <td>X) \propto P(X</td> <td>Y)P(Y)$$ indirectly</td> </tr> </tbody> </table> </li> <li>e.g. Bayesian network, Autoregressive model, GAN, Diffuson model</li> </ul> </li> </ul> <h2 id="chapter-2-bayes-decision-theory">Chapter 2. Bayes Decision Theory</h2> <h3 id="bayes-decision-rule">Bayes Decision Rule</h3> <ul> <li>conditional probability density :<br/> Let \(w\) be state (class)<br/> Let \(x\) be data (continous-valued sample) <ul> <li>prior : \(P(w=w_k)\)</li> <li> <table> <tbody> <tr> <td>likelihood : PDF $$P(x</td> <td>w_k)$$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>posterior : $$P(w_k</td> <td>x) = \frac{P(x</td> <td>w_k)P(w_k)}{P(x)}$$ (Bayes Rule)</td> <td> </td> </tr> <tr> <td>where $$P(w_1</td> <td>x) + P(w_2</td> <td>x) + \cdots + P(w_N</td> <td>x) = 1$$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>evidence : $$P(x) = \sum_{k=1}^N P(x</td> <td>w_k)P(w_k) = \sum_{k=1}^N P(x, w_k)$$</td> </tr> </tbody> </table> </li> </ul> </li> <li>Bayes Decision Rule :<br/> posterior 더 큰 쪽 고름! <ul> <li>Two-class (\(w_1, w_2\)) problem :<br/> choose \(w_1\)<br/> if \(P(w_1 | x) \gt P(w_2 | x)\)<br/> if \(P(x|w_1)P(w_1) \gt P(x|w_2)P(w_2)\)<br/> if \(\frac{P(x|w_1)}{P(x|w_2)} \gt \frac{P(w_2)}{P(w_1)}\)<br/> (likehood ratio \(\gt\) threshold)</li> <li>multi-class problem :<br/> choose \(w_i\) where \(P(w_i | x)\) is the largest</li> </ul> </li> </ul> <h3 id="minimum-error">minimum error</h3> <ul> <li>minimum error :<br/> GT가 \(w_1, w_2\) 이고, Predicted가 \(R_1, R_2\) 일 때, <ul> <li>\(P(error) = \int_{-\infty}^{\infty} P(error, x)dx = \int_{-\infty}^{\infty} P(error|x)P(x)dx\)<br/> \(= \int_{R_2}P(w_1|x)P(x)dx + \int_{R_1}P(w_2|x)P(x)dx\)<br/> \(= \int_{R_2}P(x|w_1)P(w_1)dx + \int_{R_1}P(x|w_2)P(w_2)dx\)<br/> \(= \begin{cases} A+B+D &amp; \text{if} &amp; x_B \\ A+B+C+D &amp; \text{if} &amp; x^{\ast} \end{cases}\)<br/> where \(A+B+D\) is minimum error and \(C\) is reducible error<br/> (아래 그림 참고)</li> <li>\(P(correct)\)<br/> \(= \int \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_1}P(x|w_1)P(w_1)dx + \int_{R_2}P(x|w_2)P(w_2)dx\)</li> <li>\(P(error) = 1 - P(correct)\)<br/> \(= 1 - \int \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_2}P(x|w_1)P(w_1)dx + \int_{R_1}P(x|w_2)P(w_2)dx\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/1-480.webp 480w,/assets/img/2024-09-10-Pattern/1-800.webp 800w,/assets/img/2024-09-10-Pattern/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>minimum error with rejection :<br/> decision이 확실하지 않을 때는 classification 자체를 reject하는 게 적절<br/> (classification error도 줄어들고, correct classification도 줄어듬) <ul> <li>feature space \(x\) 를 rejection region \(R\) 과 acceptance region \(A\) 으로 나눠서<br/> rejection region \(R=\{ x | \text{max}_{i} P(w_i | x) \leq 1 - t\}\) 에서는 reject decision<br/> acceptance region \(A=\{ x | \text{max}_{i} P(w_i | x) \gt 1 - t\}\) 에서는 \(w_1\) or \(w_2\) 로 classification decision 수행</li> <li>\(P_c(t) = P(correct)\)<br/> \(= \int_{A} \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_1}P(x|w_1)P(w_1)dx + \int_{R_2}P(x|w_2)P(w_2)dx\)</li> <li>\(P_r(t) = P(reject)\)<br/> \(= \int_{R}P(x|w_1)P(w_1)dx + \int_{R}P(x|w_2)P(w_2)dx\)<br/> \(= \int_{R} P(x)dx\)</li> <li>\(P_e(t) = P(error)\)<br/> \(= P(error, w_1) + P(error, w_2)\)<br/> \(= 1 - P_r(t) - P_c(t)\) by 아래 식 대입<br/> where \(P(error, w_1) = \int_{-\infty}^{\infty} P(x|w_1)P(w_1)dx - P(reject, w_1) - P(correct, w_1)\)<br/> where \(P(error, w_2) = \int_{-\infty}^{\infty} P(x|w_2)P(w_2)dx - P(reject, w_2) - P(correct, w_2)\)<br/> where \(\int_{-\infty}^{\infty} P(x|w_1)P(w_1)dx + \int_{-\infty}^{\infty} P(x|w_2)P(w_2)dx = \int_{-\infty}^{\infty} P(x)dx = 1\)</li> </ul> </li> <li>Summary : <ul> <li> <table> <tbody> <tr> <td>$$P(w_i</td> <td>x)$$ : rejection/acceptance region 구하는 데 사용</td> </tr> </tbody> </table> </li> <li>\(P(x|w_i)P(w_i)\) : \(P(correct, w_i), P(reject, w_i), P(error, w_i)\) 구해서<br/> \(P_c(t), P_r(t), P_e(t)\) 구하는 데 사용</li> <li> \[P_c(t) + P_r(t) + P_e(t) = 1\] </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/2-480.webp 480w,/assets/img/2024-09-10-Pattern/2-800.webp 800w,/assets/img/2024-09-10-Pattern/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/3-480.webp 480w,/assets/img/2024-09-10-Pattern/3-800.webp 800w,/assets/img/2024-09-10-Pattern/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="bayes-decision-rule-w-bayes-risk">Bayes Decision Rule w. Bayes risk</h3> <ul> <li>Bayes risk (minimum overall risk) :<br/> \(\Omega = \{ w_1, \cdots w_c \}\) 에서 \(w_j\) 는 \(j\) -th class<br/> \(A = \{ \alpha_{1}, \cdots, \alpha_{c} \}\) 에서 \(\alpha_{i}\) 는 class \(w_i\) 라고 예측하는 action<br/> \(\lambda(\alpha_{i} | w_j) = \lambda_{ij}\) : class \(w_j\) 가 GT일 때, class \(w_i\) 로 pred. 했을 때의 loss <ul> <li>conditional risk for taking action \(\alpha_{i}\) :<br/> 특정 input \(x\) 에 대해<br/> \(R(\alpha_{i}|x) = \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j|x)\)</li> <li>overall risk for taking action \(\alpha_{i}\) :<br/> 모든 input \(x\) 에 대해 적분<br/> \(R(\alpha_{i}) = \int R(\alpha_{i}|x)P(x)dx\)<br/> \(= \int \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j|x) P(x)dx\)<br/> \(= \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j) \int P(x|w_j)dx\)<br/> \(= \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j)\)<br/> \(= \sum_{j=1}^c \lambda_{ij}P(w_j)\)<br/> where pdf(likelihood) 합 \(\int P(x|w_j)dx = 1\)</li> <li>모든 input \(x\) 에 대해 가장 loss가 최소인 class \(w_i\) 로 예측하면,<br/> minimum overall risk (= Bayes risk) 를 가짐</li> </ul> </li> <li>Bayes Decision Rule for Bayes risk : <ul> <li>Two-class (\(w_1, w_2\)) problem :<br/> choose \(w_1\)<br/> if \(R(\alpha_{1} | x) \lt R(\alpha_{2} | x)\)<br/> if \(\lambda_{11}P(w_1 | x) + \lambda_{12}P(w_2 | x) \lt \lambda_{21}P(w_1 | x) + \lambda_{22}P(w_2 | x)\)<br/> if \((\lambda_{21} - \lambda_{11})P(w_1 | x) \gt (\lambda_{12} - \lambda_{22})P(w_2 | x)\)<br/> if \(\frac{P(x | w_1)}{P(x | w_2)} \gt \frac{(\lambda_{12} - \lambda_{22})P(w_2)}{(\lambda_{21} - \lambda_{11})P(w_1)}\)<br/> if \(\frac{P(x | w_1)}{P(x | w_2)} \gt \frac{P(w_2)}{P(w_1)}\) for \(\lambda_{11}=\lambda_{22}=0\) and \(\lambda_{12}=\lambda_{21}\)<br/> (likehood ratio \(\gt\) threshold) (위의 Bayes Decision Rule에서 구한 식과 same)</li> <li> <table> <tbody> <tr> <td>loss $$\lambda(\alpha_{i}</td> <td>w_j) = \begin{cases} 0 &amp; \text{if} &amp; i=j &amp; (\text{no penalty}) \ 1 &amp; \text{if} &amp; i \neq j &amp; (\text{equal penalty}) \end{cases}$$ 일 때</td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td>conditional risk $$R(\alpha_{i}</td> <td>x) = \sum_{j=1}^c \lambda(\alpha_{i}</td> <td>w_j)P(w_j</td> <td>x) = \sum_{j=1, j \neq i}^c P(w_j</td> <td>x) = 1 - P(w_i</td> <td>x)$$ 이므로</td> </tr> <tr> <td>Bayes Decision Rule에서 conditional risk $$R(\alpha_{i}</td> <td>x)\(최소화는 posterior\)P(w_i</td> <td>x)$$ 최대화와 같음</td> <td> </td> <td> </td> <td> </td> </tr> </tbody> </table> </li> <li>multi-class problem :<br/> classifieer (discriminant function) (space-partitioning function) \(g(x)\) 에 대해<br/> choose \(w_i\) where \(g_{i}(x)\) is the largest<br/> s.t. decision boundary is \(g_{i}(x) = g_{j}(x)\) where they are the two largest discriminant functions<br/> e.g. Bayes classifier : \(g_{i}(x) = - R(\alpha_{i} | x)\) or \(g_{i}(x) = P(w_i | x)\) or \(g_{i}(x) = P(x | w_i)P(w_i)\) or \(g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i)\)</li> </ul> </li> </ul> <h3 id="discriminant-function-for-gaussian-pdf">Discriminant Function for Gaussian PDF</h3> <ul> <li> <p>\(G(\boldsymbol x) = \frac{1}{(2\pi)^{\frac{d}{2}} | \Sigma |^{\frac{1}{2}}}e^{-\frac{1}{2}(\boldsymbol x - \boldsymbol \mu)^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu)}\)<br/> where \(d \times d\) covariance \(\Sigma = E[(\boldsymbol x - \boldsymbol \mu)(\boldsymbol x - \boldsymbol \mu)^T] = E[\boldsymbol x \boldsymbol x^{T}] - \boldsymbol \mu \boldsymbol \mu^{T} = S - \boldsymbol \mu \boldsymbol \mu^{T}\)<br/> where \(S = E[\boldsymbol x \boldsymbol x^{T}]\) : standard autocorrelation matrix</p> </li> <li> <p>Discriminant function for Gaussian PDF :<br/> likelihood \(P(x | w_i)\) 를 Gaussian PDF로 둘 경우,<br/> \(g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{d}{2} \text{ln}(2\pi) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\)</p> <ul> <li>case 1) \(\Sigma_{i} = \sigma^{2} \boldsymbol I\) (모든 classes에 대해 equal covariance) (등방성(sphere)) <br/> \(g_{i}(x) = -\frac{\| \boldsymbol x - \boldsymbol \mu_{i} \|^2}{2 \sigma^{2}} + \text{ln}P(w_i)\)<br/> \(i\) 와 관련된 term만 남기면<br/> \(g_{i}(x) = \frac{1}{\sigma^{2}} \boldsymbol \mu_{i}^T \boldsymbol x - \frac{1}{2\sigma^{2}} \boldsymbol \mu_{i}^T \boldsymbol \mu_{i} + \text{ln}P(w_i)\)<br/> \(= \boldsymbol w_i^T \boldsymbol x + \boldsymbol w_{i0}\) (linear) <ul> <li>decision boundary :<br/> hyperplane \(g(x) = g_{i}(x) - g_{j}(x) = (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T(\boldsymbol x - \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) + \frac{\sigma^{2}}{(\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T} \text{ln}\frac{P(w_i)}{P(w_j)})\)<br/> \(= \boldsymbol w^T (\boldsymbol x - \boldsymbol x_0) = 0\)</li> <li>\(\boldsymbol x_0\) 를 지나고 \(\boldsymbol w = \boldsymbol \mu_{i} - \boldsymbol \mu_{j}\) 에 수직인 hyperplane</li> <li>\(\boldsymbol x_0 = \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) - \frac{\sigma^{2}}{\| \boldsymbol \mu_{i} - \boldsymbol \mu_{j} \|^2} \text{ln}\frac{P(w_i)}{P(w_j)} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})\) 이므로<br/> \(\boldsymbol x_0\) 의 위치는 \(\boldsymbol \mu_{i}\) 와 \(\boldsymbol \mu_{j}\) 의 중점에서 \(\begin{cases} \boldsymbol \mu_{j} \text{쪽으로 이동} &amp; \text{if} &amp; P(w_i) \gt P(w_j) \\ \boldsymbol \mu_{i} \text{쪽으로 이동} &amp; \text{if} &amp; P(w_i) \lt P(w_j) \end{cases}\)<br/> (\(P(w_i)\) 와 \(P(w_j)\) 중 더 작은 쪽으로 이동)<br/> (\(\sigma^{2}\) 이 (\(\| \mu_{i} - \mu_{j} \|^2\) 에 비해 비교적) 작은 경우 \(P(w_i)\) 와 \(P(w_j)\) 에 따른 \(x_0\) shift는 미약)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/4-480.webp 480w,/assets/img/2024-09-10-Pattern/4-800.webp 800w,/assets/img/2024-09-10-Pattern/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Discriminant function for Gaussian PDF :<br/> likelihood \(P(x | w_i)\) 를 Gaussian PDF로 둘 경우,<br/> \(g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{d}{2} \text{ln}(2\pi) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\) <ul> <li>case 2) \(\Sigma_{i} = \Sigma\) (symmetric) (모든 classes에 대해 equal covariance) (비등방성(hyper-ellipsoidal))<br/> \(g_{i}(x) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) + \text{ln}P(w_i)\)<br/> \(i\) 와 관련된 term만 남기면<br/> \(g_{i}(x) = \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol x - \frac{1}{2} \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol \mu_{i} + \text{ln}P(w_i)\)<br/> \(= \boldsymbol w_i^T \boldsymbol x + \boldsymbol w_{i0}\) (linear) <ul> <li>decision boundary :<br/> hyperplane \(g(x) = g_{i}(x) - g_{j}(x) = (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T \Sigma^{-1} (\boldsymbol x - \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) + \frac{1}{(\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T \Sigma^{-1}} \text{ln}\frac{P(w_i)}{P(w_j)})\)<br/> \(= \boldsymbol w^T (\boldsymbol x - \boldsymbol x_0) = 0\)</li> <li>\(\boldsymbol x_0\) 를 지나고 \(\boldsymbol w = \Sigma^{-1} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})\) 에 수직인 hyperplane</li> <li>\(\boldsymbol x_0 = \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) - \frac{\text{ln}\frac{P(w_i)}{P(w_j)}}{(\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T \Sigma^{-1} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})\) 이므로<br/> 마찬가지로 \(\boldsymbol x_0\) 의 위치는 \(\boldsymbol \mu_{i}\) 와 \(\boldsymbol \mu_{j}\) 의 중점에서 \(P(w_i)\) 와 \(P(w_j)\) 중 더 작은 쪽으로 이동</li> <li>\(\boldsymbol w = \Sigma^{-1} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})\) 는<br/> vector \(\boldsymbol \mu_{i} - \boldsymbol \mu_{j}\) 를 \(\Sigma^{-1}\) 로 회전시킨 vector를 의미</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/5-480.webp 480w,/assets/img/2024-09-10-Pattern/5-800.webp 800w,/assets/img/2024-09-10-Pattern/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Discriminant function for Gaussian PDF :<br/> likelihood \(P(x | w_i)\) 를 Gaussian PDF로 둘 경우,<br/> \(g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{d}{2} \text{ln}(2\pi) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\) <ul> <li>case 2) \(\Sigma_{i}\) is arbitrary (symmetric) (class마다 covariance 다름) (비등방성(hyper-ellipsoidal))<br/> \(g_{i}(x) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\)<br/> \(\Sigma_{i}\) 가 \(i\) 에 대한 term이므로<br/> \(g_{i}(x) = - \frac{1}{2} \boldsymbol x^T \Sigma^{-1} \boldsymbol x + \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol x - \frac{1}{2} \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol \mu_{i} - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\)<br/> \(= - \frac{1}{2} \boldsymbol x^T \Sigma^{-1} \boldsymbol x + \boldsymbol w_i^T \boldsymbol x + \boldsymbol w_{i0}\) (quadratic) 는<br/> quadratic discriminant function in \(x\) <ul> <li>decision surface :<br/> hyperquadratic (hyperplane, hypersphere, hyperellipsoidal, hyperparaboloid, hyperhyperboloid)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/6-480.webp 480w,/assets/img/2024-09-10-Pattern/6-800.webp 800w,/assets/img/2024-09-10-Pattern/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="bayes-rule-for-discrete-case">Bayes Rule for Discrete Case</h3> <ul> <li> <table> <tbody> <tr> <td>pdf 적분 $$\int p(x</td> <td>w_j) dx$$ 대신</td> <td> </td> </tr> <tr> <td>확률 합 $$lim_{\Delta x \rightarrow 0} \Sigma_{k=-\infty}^{\infty} p(x_k</td> <td>w_j) \Delta x\(\)\rightarrow\(\)\Sigma_{k=1}^m P(v_k</td> <td>w_j)$$</td> </tr> </tbody> </table> </li> <li> <p>Bayes Decision Rule은 discrete case에서도 same<br/> Bayes risk minimize 위해 conditional risk \(R(\alpha_{i} | x)\) minimize<br/> (posterior maximize와 same)</p> </li> <li>\(\boldsymbol x = [x_1, x_2, \ldots, x_d]^T\) 에서 \(x_i\) 가 0 혹은 1의 값을 갖는 Bernoulli random var.일 때 <ul> <li>class \(w_1\) 일 때 :<br/> \(x_i \sim p_i^{x_i}(1-p_i)^{1-x_i}\)<br/> \(P(\boldsymbol x | w_1) = P([x_1, x_2, \ldots, x_d]^T | w_1) = \prod_{i=1}^d P(x_i | w_1) = \prod_{i=1}^d p_i^{x_i}(1-p_i)^{1-x_i}\)</li> <li>class \(w_2\) 일 때 :<br/> \(x_i \sim q_i^{x_i}(1-q_i)^{1-x_i}\)<br/> \(P(\boldsymbol x | w_2) = P([x_1, x_2, \ldots, x_d]^T | w_2) = \prod_{i=1}^d P(x_i | w_2) = \prod_{i=1}^d q_i^{x_i}(1-q_i)^{1-x_i}\)</li> <li>likelihood ratio :<br/> \(\frac{P(\boldsymbol x | w_1)}{P(\boldsymbol x | w_2)} = \prod_{i=1}^d (\frac{p_i}{q_i})^{x_i}(\frac{1-p_i}{1-q_i})^{1-x_i}\)</li> <li>discriminant function :<br/> choose \(w_1\)<br/> if \(g(x) = \text{ln} \frac{P(\boldsymbol x | w_1)P(w_1)}{P(\boldsymbol x | w_2)P(w_2)} = \sum_{i=1}^d(x_i \text{ln}\frac{p_i}{q_i} + (1-x_i)\text{ln}\frac{1-p_i}{1-q_i}) + \text{ln}\frac{P(w_1)}{P(w_2)} = \sum_{i=1}^d w_ix_i + w_0 = \boldsymbol w^T \boldsymbol x + w_0 \gt 0\)<br/> where \(w_i = \text{ln}\frac{p_i(1-q_i)}{q_i(1-p_i)}\) and \(w_0 = \sum_{i=1}^d(\text{ln}\frac{1-p_i}{1-q_i}) + \text{ln}\frac{P(w_1)}{P(w_2)}\) <ul> <li>case 1-1) \(p_i = q_i\)<br/> \(w_i = 0\) , so \(x_i\) 는 class 결정에 영향 없음</li> <li>case 1-2) \(p_i \gt q_i\)<br/> \(w_i \gt 0\) , so \(x_i = 1\) 은 class \(w_1\) 선택에 보탬</li> <li>case 1-3) \(p_i \lt q_i\)<br/> \(w_i \lt 0\) , so \(x_i = 1\) 은 class \(w_2\) 선택에 보탬</li> <li>case 2-1) \(P(w_1)\) 값 증가 (\(\gt P(w_2)\))<br/> \(w_0\) 값이 커지므로 class \(w_1\) 선택에 보탬</li> <li>case 2-2) \(P(w_1)\) 값 감소 (\(\lt P(w_2)\))<br/> \(w_0\) 값이 작아지므로 class \(w_1\) 선택에 보탬</li> </ul> </li> </ul> </li> </ul> <h2 id="chapter-2-linear-transformation">Chapter 2. Linear Transformation</h2> <h3 id="linear-transformation">Linear Transformation</h3> <ul> <li> \[y = A^Tx\] <ul> <li>mean and variance :<br/> \(\mu_{y} = A^T \mu_{x}\)<br/> \(\Sigma_{y} = E[(y - \mu_{y})(y - \mu_{y})^T] = A^T \Sigma_{x} A\)</li> <li>Mahalanobis distance :<br/> \(d_y^2 = (y - \mu_{y})^T\Sigma_{y}^{-1}(y - \mu_{y}) = \cdots = d_x^2\)<br/> <code class="language-plaintext highlighter-rouge">linear transformation</code>을 해도 Mahalanobis distance는 <code class="language-plaintext highlighter-rouge">그대로</code>임<br/> (Euclidean distance \((x - \mu_{x})^T(x - \mu_{x})\) 는 linear transformation을 하면 variant)</li> <li>Gaussian distribution :<br/> \(x \sim N(\mu_{x}, \Sigma_{x})\) 일 때<br/> \(P(y) = (2 \pi)^{- \frac{d}{2}} | \Sigma_{y} |^{-\frac{1}{2}} \exp(-\frac{1}{2}(y - \mu_{y})^T \Sigma_{y}^{-1} (y - \mu_{y})) = (2 \pi)^{- \frac{d}{2}} | A |^{-1} | \Sigma_{x} |^{-\frac{1}{2}} \exp(-\frac{1}{2}(x - \mu_{x})^T \Sigma_{x}^{-1} (x - \mu_{x})) = \frac{1}{|A|} P(x)\)</li> </ul> </li> </ul> <h3 id="orthonormal-transformation">Orthonormal Transformation</h3> <ul> <li>\(x = \sum_{i=1}^d y_i \phi_{i}\)<br/> where \(\{ \phi_{i}, \cdots, \phi_{d} \}\) is orthonormal basis<br/> Equivalently,<br/> \(y_i = x^T \phi_{i}\)<br/> where vector \(x\) 를 i-th eigenvector \(\phi_{i}\) 에 project한 게 \(y_i\) <ul> <li>approx. \(x\) : <ul> <li>\(\{ y_{m+1}, \cdots, y_{d} \}\) 를 pre-defined constants \(\{ b_{m+1}, \cdots, b_{d} \}\) 로 대체했을 때<br/> \(\hat x(m) = \sum_{i=1}^m y_i \phi_{i} + \sum_{i=m+1}^d b_i \phi_{i}\)</li> </ul> </li> <li>optimal \(b_i\) : <ul> <li>error \(\Delta x(m) = x - \hat x(m) = \sum_{i=m+1}^d (y_i - b_i) \phi_{i}\)<br/> MSE \(\bar \epsilon^{2}(m) = E[| \Delta x(m) |^2] = E[\Delta x^T(m) \Delta x(m)] = \sum_{i=m+1}^d E[(y_i - b_i)^2]\)</li> <li>orthonormal basis \(\phi_{i}, \phi_{j}\) 에 대해<br/> \(\frac{\partial}{\partial b_i} E[(y_i - b_i)^2] = -2(E[y_i] - b_i) = 0\) 이므로<br/> MSE 최소화하는 optimal \(b_i = E[y_i]\)</li> </ul> </li> <li>optimal \(\phi_{i}\) : <ul> <li>\(x = \sum_{j=1}^d y_j \phi_{j}\) 의 양변에 \(\phi_{i}^T\) 를 곱하면<br/> \(y_i = x^T \phi_{i}\) 이고<br/> optimal \(b_i = E[y_i]\) 이므로<br/> MSE \(\bar \epsilon^{2}(m) = \sum_{i=m+1}^d E[(y_i - b_i)^2] = \sum_{i=m+1}^d E[(x^T \phi_{i} - E[x^T \phi_{i}])^T(x^T \phi_{i} - E[x^T \phi_{i}])] = \sum_{i=m+1}^d \text{Var}(\phi_{i}^{T} x) = \sum_{i=m+1}^d \phi_{i}^T \Sigma_{x} \phi_{i}\)</li> <li>orthonormality equality constraint \(\phi_{i}^T\phi_{i} = \| \phi_{i} \| = 1\) 을 만족하면서 MSE \(\bar \epsilon^{2}(m)\) 를 최소화하는 \(\phi_{i}\) 는 Lagrange multiplier Method <a href="https://semyeong-yu.github.io/blog/2024/Lagrange/">Link</a> 로 찾을 수 있다<br/> \(\rightarrow\)<br/> goal : minimize \(U(m) = \sum_{i=m+1}^d \phi_{i}^T \Sigma_{x} \phi_{i} + \sum_{i=m+1}^d \lambda_{i}(1 - \phi_{i}^T\phi_{i})\)<br/> \(\frac{\partial}{\partial x}(x^TAx) = (A + A^T)x = 2Ax\) for symmetric \(A\) 이므로<br/> \(\frac{\partial}{\partial \phi_{i}} U(m) = 2(\Sigma_{x}\phi_{i} - \lambda_{i}\phi_{i}) = 0\) 이므로<br/> MSE 최소화하는 optimal \(\phi_{i}\) 는 \(\Sigma_{x}\phi_{i} = \lambda_{i}\phi_{i}\) 을 만족하므로<br/> \(\phi_{i}\) 와 \(\lambda_{i}\) 는 covariance matrix \(\Sigma_{x}\) 의 eigenvector and eigenvalue 이다</li> </ul> </li> </ul> </li> <li>Eigenvector and Eigenvalue : <ul> <li>\(\Sigma \Phi = \Phi \Lambda\) where \(\Phi \Phi^{T} = I\)</li> <li>If \(\Sigma\) is non-singular (\(| \Sigma | \neq 0\)),<br/> all eigenvalues \(\lambda\) are non-zero</li> <li>If \(\Sigma\) is positive-definite (\(x^T \Sigma x \geq 0\) for all \(x \neq 0\)),<br/> all eigenvalues \(\lambda\) are positive</li> <li>If \(\Sigma\) is real and symmetric,<br/> all eigenvalues \(\lambda\) are real<br/> and eigenvectors(w. distinct eigenvalues) are orthogonal <ul> <li>pf)<br/> \(\Sigma \phi_{i} = \lambda_{i} \phi_{i}\) and \(\Sigma \phi_{j} = \lambda_{j} \phi_{j}\)<br/> \(\phi_{j}^T \Sigma \phi_{i} - \phi_{i}^T \Sigma \phi_{j} = \phi_{j}^T \lambda_{i} \phi_{i} - \phi_{i}^T \lambda_{j} \phi_{j}\)<br/> \(0 = (\lambda_{i} - \lambda_{j}) \phi_{j}^T \phi_{i}\) since \(\Sigma\) is symmetric<br/> \(\rightarrow \phi_{j}^T \phi_{i} = 0\) (eigenvectors are orthogonal)</li> </ul> </li> </ul> </li> <li>Orthonormal Transformation :<br/> \(y = \Phi^{T} x\)<br/> for \(\Phi = [\phi_{1}, \cdots \phi_{d}]\) and \(\Phi \Phi^{T} = I\) <ul> <li>vector \(x\) 를 i-th eigenvector \(\phi_{i}\) 에 project한 게 \(y_{i}\)<br/> 즉, vector \(x\) 를 new coordinate \(\Phi = [\phi_{1}, \cdots \phi_{d}]\) 으로 나타낸 게 vector \(y\)</li> <li>eigenvector는 principal axis를 나타내고, eigenvalue는 해당 방향으로 퍼진 정도를 나타냄</li> <li>\(y\) 의 covariance matrix인 \(\Sigma_{y}\) 는 <code class="language-plaintext highlighter-rouge">diagonal matrix</code><br/> (uncorrelated random vector \(y\)) <ul> <li>\(\Sigma_{y}\)<br/> \(= \Phi^{T} \Sigma_{x} \Phi\)<br/> \(= \Phi^{T} \Phi \Lambda\) since \(\Sigma \Phi = \Phi \Lambda\)<br/> \(= \Phi^{-1} \Phi \Lambda\) since eigenvector matrix is orthogonal matrix (\(\Phi^{T} = \Phi^{-1}\))<br/> \(= \Lambda\)</li> </ul> </li> <li>distance : <ul> <li>Mahalanobis distance는 any linear transformation에 대해 보존됨</li> <li><code class="language-plaintext highlighter-rouge">Euclidean distance</code>는 linear transformation 중 orthonormal transformation일 때만 <code class="language-plaintext highlighter-rouge">보존</code>됨<br/> \(\| y \|^2 = y^Ty = x^T \Phi \Phi^{T} x = x^T \Phi \Phi^{-1} x = x^T x = \| x \|^2\)</li> </ul> </li> </ul> </li> </ul> <h3 id="whitening-transformation">Whitening Transformation</h3> <ul> <li>Whitening Transformation :<br/> \(y = \Lambda^{-\frac{1}{2}} \Phi^{T} x = (\Phi \Lambda^{-\frac{1}{2}})^T x\)<br/> (Orthonormal Transformation을 한 뒤 추가로 \(\Lambda^{-\frac{1}{2}}\) 로 transformation) <ul> <li>\(y\) 의 covariance matrix인 \(\Sigma_{y}\) 는 <code class="language-plaintext highlighter-rouge">identity matrix</code> \(I\) <ul> <li>\(\Sigma_{y}\)<br/> \(= (\Lambda^{-\frac{1}{2}} \Phi^{T}) \Sigma_{x} (\Phi \Lambda^{-\frac{1}{2}})\)<br/> \(= \Lambda^{-\frac{1}{2}} \Lambda \Lambda^{-\frac{1}{2}}\)<br/> \(= I\)</li> </ul> </li> <li>\(\Lambda^{-\frac{1}{2}}\) 은 principal components의 scale을 \(\frac{1}{\sqrt{\lambda_{i}}}\) 배 하는 효과</li> <li>Whitening Transformation을 한 번 하고나면,<br/> 그 후에 any Orthonormal Transformation(\(y = \Phi^{T} x\) for \(\Psi \Psi^{T} = I\))을 해도<br/> covariance matrix는 항상 \(\Psi I \Psi^{T} = I\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/7-480.webp 480w,/assets/img/2024-09-10-Pattern/7-800.webp 800w,/assets/img/2024-09-10-Pattern/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="sample-separation">Sample Separation</h3> <ul> <li>Sample Separation :<br/> uncorrelated normal samples \(\sim N(0, I)\) 로부터 correlated sample \(\sim N(\mu_{x}, \Sigma_{x})\) 만들기 <ul> <li>How? :<br/> given data \(x\) 에서 \(\mu_{x}\) 를 뺀 뒤 Whitening Transformation 적용하면 \(N(0, I)\) 이므로 이 과정을 역으로 실행</li> <li>Step 1) Normal distribution으로부터 N개의 \(d\) -dim. independent vectors를 sampling<br/> \(y_1, y_2, \cdots, y_N \sim N(0, I)\)</li> <li>Step 2) Inverse-Whitening-Transformation 적용하여 Normal distribution을 x-space로 변환 \(x_k = \Phi \Lambda^{\frac{1}{2}} y_k\)<br/> for given \(\Sigma_{x}\)<br/> and its eigen-decomposition \(\Sigma_{x} \Phi = \Phi \Lambda\)</li> <li>Step 3) x-space의 samples에 \(\mu_{x}\) 더함<br/> \(x_k = \Phi \Lambda^{\frac{1}{2}} y_k + \mu_{x} \sim N(\mu_{x}, \Sigma_{x})\)<br/> for given \(\mu_{x}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/8-480.webp 480w,/assets/img/2024-09-10-Pattern/8-800.webp 800w,/assets/img/2024-09-10-Pattern/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="chapter-3-maximum-likelihood-and-bayesian-parameter-estimation">Chapter 3. Maximum-likelihood and Bayesian Parameter Estimation</h2> <ul> <li>parameter estimation : <ul> <li>Maximum Likelihood Estimation (MLE) :<br/> (true) parameters are <code class="language-plaintext highlighter-rouge">unknown</code>, but <code class="language-plaintext highlighter-rouge">fixed</code><br/> estimators are random variable</li> <li>Bayesian Estimation :<br/> parameters are <code class="language-plaintext highlighter-rouge">random variables</code> and <code class="language-plaintext highlighter-rouge">prior is known</code></li> </ul> </li> </ul> <h3 id="maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</h3> <ul> <li> <p>Assumption :<br/> training data \(D_j\) \(\sim\) likelihood \(p(D_j | w_j) = N(\mu_{j}, \Sigma_{j})\)<br/> (i.i.d random samples)</p> </li> <li>MLE : <ul> <li>likelihood :<br/> \(\hat \theta = \text{argmax}_{\theta} p(D=\{ x_1, x_2, \ldots, x_n \} | \theta) = \text{argmax}_{\theta} \prod_{k=1}^n p(x_k | \theta)\)</li> <li>log-likelihood :<br/> \(\hat \theta = \text{argmax}_{\theta} p(D=\{ x_1, x_2, \ldots, x_n \} | \theta) = \text{argmax}_{\theta} \sum_{k=1}^n \text{ln} p(x_k | \theta)\)</li> </ul> </li> <li>Gaussian likelihood : <ul> <li>unknown \(\mu\) : <ul> <li>likelihood :<br/> \(p(x_k | \mu) = (2 \pi)^{-\frac{d}{2}} | \Sigma |^{-\frac{1}{2}} \text{exp}(-\frac{1}{2}(x_k - \mu)^T \Sigma^{-1} (x_k - \mu))\)<br/> \(p(D=\{ x_1, x_2, \ldots, x_N \} | \mu) = \prod_{k=1}^N p(x_k | \mu) = (2 \pi)^{-\frac{dN}{2}} | \Sigma |^{-\frac{N}{2}} \text{exp}(-\frac{1}{2} \sum_{k=1}^N (x_k - \mu)^T \Sigma^{-1} (x_k - \mu))\)</li> <li>log-likelihood :<br/> \(\text{ln} p(D | \mu) = -\frac{dN}{2} \text{ln}(2 \pi) -\frac{N}{2} \text{ln} | \Sigma | -\frac{1}{2} \sum_{k=1}^N (x_k - \mu)^T \Sigma^{-1} (x_k - \mu)\)</li> <li>matrix derivative :<br/> \(\frac{d}{dx}(Ax) = A\)<br/> \(\frac{d}{dx}(y^TAx) = A^Ty\)<br/> \(\frac{d}{dx}(x^TAx) = (A+A^T)x\)<br/> \(\frac{d}{dA}(x^TAx) = xx^T\)<br/> \(\frac{\partial |A|}{\partial A} = (\text{adj}(A))^T = |A|(A^{-1})^T\)<br/> \(\frac{\partial \text{ln}|A|}{\partial A} = (A^{-1})^T = (A^T)^{-1}\) where \(|A| = \frac{1}{|A^{-1}|}\)</li> <li>MLE problem :<br/> \(\nabla_{\mu} \text{ln} p(D | \mu) = -\frac{1}{2} \sum_{k=1}^N ((\Sigma^{-1} + (\Sigma^{-1})^T) (x_k - \mu)) \times (-1) = (\Sigma^{-1} + (\Sigma^{-1})^T)(\sum_{k=1}^N x_k - \sum_{k=1}^N \mu) = 0\)<br/> \(\sum_{k=1}^N x_k - N \mu = 0\)<br/> \(\hat \mu_{\text{MLE}} = \frac{1}{N} \sum_{k=1}^N x_k\)</li> <li>Summary : <ul> <li>\(\hat \mu_{\text{MLE}} = \frac{1}{N} \sum_{k=1}^N x_k\)<br/> (true mean의 MLE estimator는 sample mean)</li> <li>\(E[\hat \mu_{\text{MLE}}] = \mu\)<br/> (\(\hat \mu_{\text{MLE}}\) 는 <code class="language-plaintext highlighter-rouge">unbiased</code> estimator)</li> </ul> </li> </ul> </li> <li>unknown \(\mu\) and \(\Sigma\) : <ul> <li>log-likelihood :<br/> \(\text{ln} p(D | \mu) = -\frac{dN}{2} \text{ln}(2 \pi) -\frac{N}{2} \text{ln} | \Sigma | -\frac{1}{2} \sum_{k=1}^N (x_k - \mu)^T \Sigma^{-1} (x_k - \mu) = -\frac{dN}{2} \text{ln}(2 \pi) + \frac{N}{2} \text{ln} | \Sigma^{-1} | -\frac{1}{2} \sum_{k=1}^N (x_k - \mu)^T \Sigma^{-1} (x_k - \mu)\)</li> <li>MLE problem :<br/> \(\nabla_{\Sigma^{-1}} \text{ln} p(D | \mu) = \frac{N}{2}\Sigma^{T} - \frac{1}{2} \sum_{k=1}^N (x_k - \mu)(x_k - \mu)^T = 0\)<br/> \(N \Sigma^{T} = \sum_{k=1}^N (x_k - \mu)(x_k - \mu)^T\)<br/> \(\mu = \hat \mu_{\text{MLE}}\) 대입하고, \(\Sigma\) 는 symmetric(\(\Sigma^{T} = \Sigma\))하므로<br/> \(\hat \Sigma_{\text{MLE}} = \frac{1}{N} \sum_{k=1}^N (x_k - \hat \mu_{\text{MLE}})(x_k - \hat \mu_{\text{MLE}})^T\)</li> <li>Summary : <ul> <li>\(\hat \Sigma_{\text{MLE}} = \frac{1}{N} \sum_{k=1}^N (x_k - \hat \mu_{\text{MLE}})(x_k - \hat \mu_{\text{MLE}})^T\)<br/> (\(\mu\) 먼저 estimate한 뒤 \(\Sigma\) estimate)</li> <li>\(E[\hat \Sigma_{\text{MLE}}] = \frac{1}{N} E[\sum_{k=1}^N (x_k - \hat \mu_{\text{MLE}})(x_k - \hat \mu_{\text{MLE}})^T] = \frac{N-1}{N} \Sigma \neq \Sigma\)<br/> (\(\hat \Sigma_{\text{MLE}}\) 는 <code class="language-plaintext highlighter-rouge">biased</code> estimator) <ul> <li>pf) 아래 식 이용<br/> \(E[x_i x_j^T] = \begin{cases} \Sigma + \mu \mu^{T} &amp; \text{if} &amp; i = j \\ \mu \mu^{T} &amp; \text{if} &amp; i \neq j \end{cases}\)<br/> since \(\Sigma = E[(x - \mu)(x - \mu)^T] = \cdots = E[xx^T] - \mu \mu^{T}\)<br/> since \(0 = E[(x_i - \mu)(x_j - \mu)^T] = E[x_i x_j^T] - \mu \mu^{T}\) by independence \(i \neq j\)</li> </ul> </li> <li>\(\text{lim}_{N \rightarrow \infty}E[\hat \Sigma_{\text{MLE}}] = \text{lim}_{N \rightarrow \infty} \frac{N-1}{N} \Sigma = \Sigma\)<br/> (\(\hat \Sigma_{\text{MLE}}\) 는 <code class="language-plaintext highlighter-rouge">asymptotically unbiased</code> estimator)<br/> 또는<br/> \(\hat \Sigma_{\text{MLE}} = \frac{1}{N-1} \sum_{k=1}^N (x_k - \hat \mu_{\text{MLE}})(x_k - \hat \mu_{\text{MLE}})^T\)<br/> (위처럼 설정하면 \(\hat \Sigma_{\text{MLE}}\) 는 <code class="language-plaintext highlighter-rouge">unbiased</code> estimator)</li> </ul> </li> </ul> </li> </ul> </li> <li>MLE : <ul> <li>MLE is <code class="language-plaintext highlighter-rouge">asymptotically consistent</code><br/> if \(\text{lim}_{N \rightarrow \infty} P(\| \hat \theta_{\text{MLE}} - \theta_{\text{true}} \| \leq \epsilon) = 1\) for arbitrary small \(\epsilon\)<br/> (sample 수 \(N\) 이 크면 param. estimate은 true value랑 거의 비슷)<br/> by central limit theorem and the fact that MLE is related to the sum of random var.</li> <li>MLE is <code class="language-plaintext highlighter-rouge">asymptotically efficient</code><br/> since MLE는 Cramer-Rao lower bound(any estimate이 달성할 수 있는 the lowest value of variance)</li> </ul> </li> </ul> <h3 id="bayesian-estimation">Bayesian Estimation</h3> <ul> <li>Summary : <ul> <li>MLE (maximum likelihood estimation) : <ul> <li>when \(\theta\) is unknown, but fixed</li> <li> <table> <tbody> <tr> <td>maximize likelihood $$\hat \theta_{MLE} = \text{argmax}_{\theta} p(D</td> <td>\theta)$$</td> </tr> </tbody> </table> </li> </ul> </li> <li>MAP (maximum a posterior) : <ul> <li>when \(\theta\) is random var. and prior \(p(\theta)\) is known</li> <li> <table> <tbody> <tr> <td>maximize posterior $$\hat \theta_{MAP} = \text{argmax}_{\theta} p(\theta</td> <td>D) = \text{argmax}_{\theta} \text{ln} p(D</td> <td>\theta) + \text{ln} p(\theta)$$</td> </tr> </tbody> </table> </li> </ul> </li> <li>If prior \(p(\theta)\) is constant (uniform distribution),<br/> MLE와 MAP는 same</li> </ul> </li> <li> <p>prior \(p(\theta)\) 와 posterior \(p(\theta | D)\) 가 같은 확률 분포의 형태를 가질 경우<br/> prior \(p(\theta)\) 를<br/> likehood \(p(D | \theta)\) 에 대한 <code class="language-plaintext highlighter-rouge">conjugate prior</code>라고 말한다</p> </li> <li>Gaussian case : <ul> <li>random var. \(\mu\) : <ul> <li>likelihood and conjugate prior :<br/> \(x_k \sim\) \(p(x_k | \mu) = N(\mu, \sigma^{2})\)<br/> where \(\mu \sim\) \(p(\mu) = N(\mu_{0}, \sigma_{0}^{2})\)</li> <li>posterior (수식 유도는 아래에 별도로) :<br/> \(p(\mu | D, \sigma^{2}) \propto N(\mu_{N}, \sigma_{N}^{2})\)<br/> \(= \frac{1}{\sqrt{2 \pi} \sigma_{N}} e^{-\frac{1}{2}(\frac{\mu - \mu_{N}}{\sigma_{N}})^2}\)<br/> where \(\hat \mu_{MAP} = \text{argmax}_{\mu} p(\mu | D, \sigma^{2}) = \mu_{N} = (\frac{N}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}}) \hat \mu_{MLE} + \frac{\frac{\sigma^{2}}{\sigma_{0}^{2}}}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}} \mu_{0}\)<br/> where \(\sigma_{N}^{2} = \frac{\sigma^{2}}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}}\) <ul> <li>Bayesian Learning :<br/> \(N\), 즉 sample 수가 많아질수록<br/> \(\mu_{N}\) 은 \(\hat \mu_{MLE}\) 에 가까워지고<br/> \(\sigma_{N}^{2}\), 즉 uncertainty about \(\mu_{N}\) 은 감소<br/> 따라서 \(N \rightarrow \infty\) 이면<br/> posterior \(p(\mu | D, \sigma^{2})\) 는 \(\mu_{N} = \hat \mu_{MLE}\) 에서의 Dirac delta function</li> <li>\(\hat \mu_{MAP} = \mu_{N}\) 에서<br/> \((\frac{N}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}}) \hat \mu_{MLE}\) 는 empirical data samples 부분이고<br/> \(\frac{\frac{\sigma^{2}}{\sigma_{0}^{2}}}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}} \mu_{0}\) 는 prior info. 부분</li> <li>만약 \(\sigma_{0}^{2} = 0\) 이라면<br/> prior : variance \(\sigma_{0}^{2}\) 가 매우 작으므로, certain that \(\mu = \mu_{0}\)<br/> So,<br/> posterior : \(\mu_{N} = \mu_{0}\) (data samples는 \(\mu_{N}\) 에 영향 없음)</li> <li>만약 \(\sigma^{2} \ll \sigma_{0}^{2}\) 이라면<br/> prior : variance \(\sigma_{0}^{2}\) 가 매우 크므로, so uncertain that \(\mu = \mu_{0}\)<br/> So,<br/> posterior : \(\mu_{N} = \hat \mu_{MLE}\) (data samples가 \(\mu_{N}\) 에 대부분의 영향 미침)</li> </ul> </li> <li>posterior (수식 유도) :<br/> \(p(\mu | D, \sigma^{2})\)<br/> \(\propto p(D | \mu, \sigma^{2}) p(\mu) = \frac{1}{(2 \pi \sigma^{2})^{\frac{N}{2}}(2 \pi \sigma_{0}^{2})^{\frac{1}{2}}}e^{-\frac{1}{2\sigma^{2}}\sum_{i=1}^N(x_i - \mu)^2 -\frac{1}{2\sigma_{0}^{2}}(\mu - \mu_{0})^2}\)<br/> \(\propto e^{-\frac{1}{2\sigma^{2}}\sum_{i=1}^N(x_i - \mu)^2 -\frac{1}{2\sigma_{0}^{2}}(\mu - \mu_{0})^2}\)<br/> \(\propto e^{-\frac{1}{2\sigma^{2}}(N \mu^{2} - 2 \mu \sum_{i=1}^N x_i) -\frac{1}{2\sigma_{0}^{2}}(\mu^{2} - 2 \mu \mu_{0})}\)<br/> \(= e^{-\frac{1}{2}(\mu^{2}(\frac{N}{\sigma^{2}} + \frac{1}{\sigma_{0}^{2}}) - 2 \mu (\frac{N \hat \mu_{MLE}}{\sigma^{2}} + \frac{\mu_{0}}{\sigma_{0}^{2}}))}\)<br/> \(\propto \frac{1}{\sqrt{2 \pi} \sigma_{N}} e^{-\frac{1}{2}(\frac{\mu - \mu_{N}}{\sigma_{N}})^2}\)<br/> where \(\hat \mu_{MAP} = \mu_{N} = (\frac{N\sigma_{0}^{2}}{N\sigma_{0}^{2} + \sigma^{2}}) \hat \mu_{MLE} + \frac{\sigma^{2}}{N\sigma_{0}^{2} + \sigma^{2}} \mu_{0}\)<br/> where \(\sigma_{N}^{2} = \frac{\sigma_{0}^{2} \sigma^{2}}{N\sigma_{0}^{2} + \sigma^{2}}\)</li> </ul> </li> </ul> </li> </ul> <h3 id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h3> <ul> <li>dimensionality reduction w/o losing much info.</li> </ul> <p>28p TBD</p> <h3 id="singular-value-decomposition-svd">Singular Value Decomposition (SVD)</h3> <ul> <li>쓰임 : matrix factorization <ul> <li>low-rank approximation of matrix</li> <li>pseudo-inverse of non-square matrix</li> </ul> </li> <li>Singular Value Decomposition (SVD) :<br/> \(A = U \Sigma V^T = \begin{bmatrix} u_1 &amp; u_2 &amp; \cdots &amp; u_m \end{bmatrix} \begin{bmatrix} \sigma_{1} &amp; \cdots &amp; 0 &amp; \cdots &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; \sigma_{r} &amp; \cdots &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; 0 &amp; \cdots &amp; \sigma_{m} &amp; 0 &amp; \cdots &amp; 0 \end{bmatrix} \begin{bmatrix} v_1^{T} \\ v_2^{T} \\ \vdots \\ v_n^{T} \end{bmatrix}\)<br/> where \(U\) is \(m \times m\) orthonormal matrix (\(UU^T = I\))<br/> where \(V\) is \(n \times n\) orthonormal matrix (\(VV^T = I\))<br/> where \(\Sigma\) is \(m \times n\) diagonal matrix (\(\sigma_{1} \geq \sigma_{2} \geq \cdots \geq \sigma_{m} \geq 0\)) <ul> <li>\(AA^T = U \Sigma \Sigma^{T} U^T\)<br/> \(U\) 는 \(AA^T\) 의 eigenvector matrix<br/> \(\sigma_{i} = \sqrt{\lambda_{i}}\) where \(\lambda_{i}\) 는 \(AA^T\) 의 eigenvalue</li> <li>\(A^TA = V \Sigma^{T} \Sigma V^T\)<br/> \(V\) 는 \(A^TA\) 의 eigenvector matrix<br/> \(\sigma_{i} = \sqrt{\lambda_{i}}\) where \(\lambda_{i}\) 는 \(A^TA\) 의 eigenvalue</li> </ul> </li> <li>rank and span :<br/> \(r = \text{rank}(A) = \text{rank}(\Sigma) \leq \text{min}(m, n)\) 에 대해<br/> (\(\sigma_{1} \geq \sigma_{2} \geq \cdots \geq \sigma_{r} \geq \sigma_{r+1} = \cdots = \sigma_{m} = 0\)) <ul> <li>\(\text{col}(A)\) is spanned by \(\begin{bmatrix} u_1, \cdots, u_r \end{bmatrix}\)<br/> \(AA^Tu_i = \sigma_{i}^{2} u_i\) for \(i = 1, \cdots, r\)</li> <li>\(\text{null}(A^T)\) is spanned by \(\begin{bmatrix} u_{r+1}, \cdots, u_m \end{bmatrix}\)<br/> \(AA^Tu_i = 0\) for \(i = r+1, \cdots, m\), 즉 \(A^T u_i = 0\)</li> <li>\(\text{row}(A) = \text{col}(A^T)\) is spanned by \(\begin{bmatrix} v_{1}^{T}, \cdots, v_{r}^{T} \end{bmatrix}\)<br/> \(A^TAv_i = \sigma_{i}^{2} v_i\) for \(i = 1, \cdots, r\)</li> <li>\(\text{null}(A)\) is spanned by \(\begin{bmatrix} v_{r+1}^{T}, \cdots, v_{n}^{T} \end{bmatrix}\)<br/> \(A^TAv_i = 0\) for \(i = r+1, \cdots, n\), 즉 \(A v_i = 0\)</li> <li>\(A = U \Sigma V^T\) \(\rightarrow\) \(A V = U \Sigma\)<br/> \(A v_i = \sigma_{i} u_i\) for \(i = 1, \cdots, r\)<br/> \(A v_i = 0\) for \(i = r+1, \cdots, n\)</li> </ul> </li> <li> <p>SVD (rank에 맞게 줄인 버전) :<br/> \(A = \begin{bmatrix} u_1 &amp; u_2 &amp; \cdots &amp; u_r \end{bmatrix} \begin{bmatrix} \sigma_{1} &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \sigma_{2} &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; 0 &amp; \sigma_{r} \end{bmatrix} \begin{bmatrix} v_1^{T} \\ v_2^{T} \\ \vdots \\ v_r^{T} \end{bmatrix}\)<br/> \(= \sum_{i=1}^{r} \sigma_{i} u_i v_{i}^{T}\)</p> </li> <li> <p>Eigen Decomposition :<br/> matrix \(A\) 가 square and real and symmetric일 경우<br/> \(A P = P D\)<br/> where \(P\) is orthonormal eigenvector matrix (\(P^{-1} = P^T\) and \(PP^T = I\))<br/> where \(D\) is diagonal eigenvalue matrix</p> </li> <li>SVD 계산 과정 Summary : <ul> <li>Step 1)<br/> \(A^TA\) 를 Eigen Decomposition해서,<br/> eigenvector matrix \(V\) 와 eigenvalue matrix \(\Sigma\) 를 구함<br/> 이 때, \(\Sigma\) 의 singular value는 \(A^TA\) 의 eigenvalue에 \(\sqrt{\cdot}\) 씌워서 구함</li> <li>Step 2)<br/> eigenvector matrix \(V\) 의 columns를 normalize하고<br/> \(V^TV=I\) 맞는지 확인</li> <li>Step 3)<br/> \(Av_i = \sigma_{i} u_i\), 즉 \(u_i = \sigma_{i}^{-1} A v_i\) 으로<br/> eigenvector \(u_i\) 구하고<br/> normalize해서 \(UU^T = I\) 맞는지 확인</li> <li>Step 4)<br/> \(A = U \Sigma V^T\) 맞는지 최종 확인</li> </ul> </li> <li>Relation b.w. PCA and SVD :<br/> matrix \(A\) 의 columns가 zero-mean centered일 때 <ul> <li>PCA :<br/> covariance matrix \(C = E[AA^T] = \frac{1}{N-1}AA^T\)의 eigenvectors를 구한 뒤<br/> eigenvalue 큰 순으로 잘라서 principal eigenvectors의 합으로 표현</li> <li>SVD :<br/> \(AA^T = U \Sigma^{2} U^T = V \Sigma^{2} V^T\) (\(U = V\) since \(AA^T\) is square matrix) 이므로<br/> \(AA^T\)의 eigenvector matrix \(V\) 를 구한 뒤<br/> rank \(r \leq \text{min}(m, n)\)에 맞게 줄여서 \(A = U \Sigma V^T = \sum_{i=1}^{r} \sigma_{i} u_i v_{i}^{T}\) 로 표현</li> <li>Relation :<br/> 둘은 mathematically 동일!<br/> \(C = V \frac{\Sigma^{2}}{N-1} V^T\)</li> </ul> </li> </ul> <h2 id="chapter-4-non-parametric-techniques">Chapter 4. Non-parametric Techniques</h2> <ul> <li>parametric approach :<br/> pdf가 어떤 form인지 미리 알아야 함<br/> e.g. \(N(\mu, \Sigma)\) 의 \(\mu\) 를 estimate<br/> 하지만 실제로는 pdf prior form 모를 때가 많다 <ul> <li>해결 방법 1)<br/> Non-parametric approach로 samples로부터 pdf (density \(\hat p(x) \approx p(x)\)) 를 직접 estimate<br/> e.g. Parzen window<br/> e.g. k-NN</li> <li>해결 방법 2)<br/> 아예 posterior를 직접 estimate<br/> e.g. Direct Decision Rule - Nearest-Neighbor</li> </ul> </li> </ul> <h3 id="density-estimation">Density Estimation</h3> <ul> <li>\(P(x \in R) = \int_{R} p(x)dx\) :<br/> pdf \(p(x)\) 를 안다면 위의 식으로 a sample \(x\) 가 region \(R\) 안에 속할 확률을 구할 수 있다<br/> But, pdf 모를 때는? <ul> <li>\(P \approx \frac{k}{n}\) <br/> where \(n\) 개의 samples 중 region \(R\) 안에 속하는 samples가 \(k\)개</li> <li>For very small region \(R\),<br/> \(P(x \in R) \approx \hat p(x) \cdot V\)<br/> where V is volume enclosed by \(R\)</li> <li>즉, \(\hat p(x) = \frac{k/n}{V}\) is pdf estimator of \(p(x)\) <ul> <li>case 1) fixed V (volume of region \(R\) is fixed)<br/> sample 수 많아지면 \(\text{lim}_{n \rightarrow \infty} k/n = P\) 로 수렴<br/> So, \(\hat p(x)\) is averaged ver. of \(p(x)\)</li> <li>case 2) \(V \rightarrow 0\) (volume of region \(R\) shrinks to 0)<br/> region \(R\)의 volume이 매우 작으므로 \(k \rightarrow 0\)<br/> So, \(p(x)\) 는 zero에 가깝고, \(\hat p(x)\) 는 very noisy</li> <li>case 3) 실제 상황<br/> sample 수 \(n\) is limited<br/> volume \(V\) 는 arbitrarily small일 수 없음<br/> 따라서 samples에 따라 \(\frac{k}{n}\)과 averaging by \(V\) 에 어느 정도 variance가 있음</li> </ul> </li> <li>\(\hat p(x)\) 가 \(p(x)\) 로 converge하려면 아래의 세 가지 조건 만족해야 함 <ul> <li>\(\text{lim}_{n \rightarrow \infty} V = 0\)<br/> no averaging in the limit</li> <li>If \(V\) is fixed, \(\text{lim}_{n \rightarrow \infty} k = \infty\)<br/> 그래야 \(\frac{k/n}{V}\) 가 probability \(p(x)\) 로 수렴<br/> 만약 \(\text{lim}_{n \rightarrow \infty} k = c\) 라면 \(\text{lim}_{n \rightarrow \infty} \hat p(x) = 0\)</li> <li>\(\text{lim}_{n \rightarrow \infty} \frac{k}{n} = 0\)<br/> So, \(\text{lim}_{n \rightarrow \infty} \hat p(x) = \text{lim}_{n \rightarrow \infty} \frac{k/n}{V}\) is density function</li> </ul> </li> <li>\(\text{lim}_{n \rightarrow \infty} \hat p(x) = p(x)\) 위해 <ul> <li>e.g. \(V = \frac{V_0}{\sqrt{n}}\) (Parzen window method)<br/> so that \(\text{lim}_{n \rightarrow \infty} V = 0\)</li> <li>e.g. \(k = \sqrt{n}\)<br/> so that \(\text{lim}_{n \rightarrow \infty} k = \infty\) and \(\text{lim}_{n \rightarrow \infty} \frac{k}{n} = 0\) and \(\text{lim}_{n \rightarrow \infty} V = \text{lim}_{n \rightarrow \infty} \frac{\sqrt{n}}{n \hat p(x)} = 0\)</li> </ul> </li> </ul> </li> </ul> <h3 id="density-estimation---parzen-window">Density Estimation - Parzen window</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/10-480.webp 480w,/assets/img/2024-09-10-Pattern/10-800.webp 800w,/assets/img/2024-09-10-Pattern/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/9-480.webp 480w,/assets/img/2024-09-10-Pattern/9-800.webp 800w,/assets/img/2024-09-10-Pattern/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Let’s define Parzen window as <code class="language-plaintext highlighter-rouge">unit hypercube</code><br/> \(\phi (u) = \begin{cases} 1 &amp; \text{if} &amp; | u_i | \leq \frac{1}{2} \\ 0 &amp; \text{O.W.} \end{cases}\) for \(i = 1, \ldots, d\)</p> </li> <li>\(\phi (u)\) 는 indicator function처럼 쓰여서<br/> \(x\) 를 중심으로 하고 \((h_n)^d\) 의 범위를 갖는 cube 안에 들어오는 sample 개수 \(k\) 를 세는 데 사용 <ul> <li>\(k = \sum_{i=1}^n \phi (\frac{x - x_i}{h_n})\) and \(V = (h_n)^d\)<br/> \(\rightarrow\)<br/> \(\hat p(x) = \frac{k/n}{V} = \frac{1}{n} \sum_{i=1}^n \frac{\phi (\frac{x - x_i}{h_n})}{(h_n)^d}\) :<br/> interpolated function at position \(x\) from samples \(x_i\)</li> </ul> </li> <li>Let \(\delta_{n} (x) = \frac{1}{V} \phi (\frac{x}{h_n})\)<br/> Then \(\hat p(x) = \frac{1}{n} \sum_{i=1}^n \delta_{n} (x - x_i)\) <ul> <li>\(h_n\) (\(V\)) 이 클 경우 :<br/> \(\delta_{n}(x)\) 의 variance가 커서<br/> 이를 합친 \(\hat p(x) = \frac{1}{n} \sum_{i=1}^n \delta_{n} (x - x_i)\) 는 오히려 smoothed ver. of \(p(x)\) at \(x\)<br/> (too little resolution)</li> <li>\(h_n\) (\(V\)) 이 작을 경우 :<br/> \(\delta_{n}(x)\) 의 variance가 작아서<br/> 이를 합친 \(\hat p(x) = \frac{1}{n} \sum_{i=1}^n \delta_{n} (x - x_i)\) 는 noisy estimate of \(p(x)\) (variation이 큼)<br/> (too much statistical variation)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/11-480.webp 480w,/assets/img/2024-09-10-Pattern/11-800.webp 800w,/assets/img/2024-09-10-Pattern/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>만약 \(h_n \rightarrow 0\) (\(\text{lim}_{n \rightarrow \infty} V = 0\)) 이라면 <ul> <li>\(\text{lim}_{n \rightarrow \infty} \delta_{n} (x) = \delta (x)\) (Dirac delta func.)</li> <li>\(\text{lim}_{n \rightarrow \infty} E[\hat p(x)] = \text{lim}_{n \rightarrow \infty} \frac{1}{n} \sum_{i=1}^n E_{x_i}[\frac{1}{V} \phi (\frac{x - x_i}{h_n})]\)<br/> \(= \text{lim}_{n \rightarrow \infty} \frac{1}{n} \cdot n \cdot \int \frac{1}{V} \phi (\frac{x - s}{h_n}) p(s) ds\)<br/> \(= \text{lim}_{n \rightarrow \infty} \frac{1}{V} \phi(\frac{x}{h_n}) \ast p(x)\) by definition of convolution<br/> \(= \text{lim}_{n \rightarrow \infty} \delta_{n}(x) \ast p(x)\)<br/> \(= \delta (x) \ast p(x)\)<br/> \(= p(x)\)</li> </ul> </li> </ul> <h3 id="density-estimation---knn-method">Density Estimation - kNN method</h3> <ul> <li>고정된 \(k_n\) 값에 대해<br/> \(k_n\) nearest neighbors 찾을 때까지 \(V_n\) expand<br/> \(\rightarrow\)<br/> training samples가 sparse한 곳에서 \(\hat p(x) \rightarrow 0\) 인 걸 방지</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/12-480.webp 480w,/assets/img/2024-09-10-Pattern/12-800.webp 800w,/assets/img/2024-09-10-Pattern/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>k-NN estimation :<br/> probability \(\frac{k}{n}\) 은 고정하고<br/> \(k\)-개의 sample이 들어 있는 volume \(V\) 의 크기를 통해 density estimation</li> </ul> <h3 id="classification-based-on-parzen-window-and-k-nn">Classification based on Parzen-window and k-NN</h3> <ul> <li>classification based on Parzen-window method : <ul> <li>density estimate<br/> \(\hat p(x) = \frac{1}{n} \sum_{i=1}^n (\frac{1}{V_n} \phi (\frac{x-x_i}{h_n}))\)</li> <li>classification<br/> choose \(w_1\)<br/> if \(\frac{\hat p(x | w_1)}{\hat p(x | w_2)} = \frac{\frac{1}{n_1} \sum_{i=1}^{n_1} (\frac{1}{V_{n_1}} \phi (\frac{x-x_i}{h_n}))}{\frac{1}{n_2} \sum_{i=1}^{n_2} (\frac{1}{V_{n_2}} \phi (\frac{x-x_i}{h_n}))} \gt \frac{(\lambda_{12} - \lambda_{22})P(w_2)}{(\lambda_{21} - \lambda_{11})P(w_1)}\)</li> </ul> </li> <li>classification based on k-NN method : <ul> <li>density estimate<br/> \(\hat p(x) = \frac{k_n / n}{V_n}\)</li> <li>classification<br/> choose \(w_1\)<br/> if \(\frac{\hat p(x | w_1)}{\hat p(x | w_2)} = \frac{\frac{k_1 / n_1}{V_{n_1}}}{\frac{k_2 / n_2}{V_{n_2}}} \gt \frac{(\lambda_{12} - \lambda_{22})P(w_2)}{(\lambda_{21} - \lambda_{11})P(w_1)}\)<br/> if \(\frac{V_{n_2}}{V_{n_1}} \gt \frac{n_1(\lambda_{12} - \lambda_{22})P(w_2)}{n_2(\lambda_{21} - \lambda_{11})P(w_1)}\)<br/> (\(k_1 = k_2\) is fixed for k-NN)</li> </ul> </li> </ul> <h3 id="direct-estimation-of-posteriori">Direct Estimation of Posteriori</h3> <ul> <li>NNR (Nearest Neighbor Rule) : <ul> <li>Step 1)<br/> estimate posteriori \(\hat P(w_i | x)\) directly from training set <ul> <li>classify하고 싶은 data \(x\) 를 중심으로 volume \(V\) 둠</li> <li>likelihood pdf \(\hat P(x | w_i) = \frac{k_i / n_i}{V}\)<br/> where \(n_i\) : 총 \(n\) samples 중 class \(w_i\) 에 속하는 samples 수<br/> where \(k_i\) : \(V\) 안에 속하는 \(k\) samples 중 class \(w_i\) 에 속하는 samples 수 (\(\sum_{i=1}^c k_i = k\))</li> <li>class probability \(\hat P(w) = \frac{n_i}{n}\)</li> <li> <table> <tbody> <tr> <td>joint pdf $$\hat P(x, w_i) = \hat P(x</td> <td>w_i) \hat P(w_i) = \frac{k_i / n}{V}$$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>posterior $$\hat P(w_i</td> <td>x) = \frac{\hat P(x, w_i)}{\sum_{j=1}^c \hat P(x, w_j)} = \frac{(k_i / n) / V}{\sum_{j=1}^c (k_j / n) / V} = \frac{k_i}{k}$$</td> </tr> </tbody> </table> </li> </ul> </li> <li>Step 2)<br/> classification based on estimated \(\hat P(w_i | x)\) <ul> <li> <table> <tbody> <tr> <td>choose \(w_i\) where $$i = \text{argmax}_{i} \hat P(w_i</td> <td>x) = \text{argmax}_{i} k_i$$</td> </tr> </tbody> </table> </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/13-480.webp 480w,/assets/img/2024-09-10-Pattern/13-800.webp 800w,/assets/img/2024-09-10-Pattern/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>k-NNR :<br/> volume \(V\) 를 고정하는 게 아니라 \(V\) 에 속하는 sample 수 \(k\) 를 고정 <ul> <li>1-NNR :<br/> assign test sample \(x\) to the same class as the nearest training sample \(x^{'}\)</li> <li>3-NNR :<br/> assign test sample \(x\) to the class where the nearest training samples 3개 중 2개 이상이 속한 class</li> <li>k-NNR :<br/> 무승부 방지 위해 \(k\) 는 항상 홀수로 설정</li> </ul> </li> </ul> <h3 id="asymptotic-analysis-of-nnr">Asymptotic Analysis of NNR</h3> <ul> <li>Error Bound for NNR (Nearest Negibor Rule) : <ul> <li>probability of error : <ul> <li>exact :<br/> \(P(e) = \int (1 - \sum_{i=1}^c P^2(w_i | x)) p(x)dx\)</li> <li>approximate :<br/> if all \(c\) classes have equal probability<br/> \(P(e) = 1 - \frac{1}{c}\)</li> </ul> </li> <li>error bound :<br/> \(P^{\ast} \leq P(e) \leq 2 P^{\ast}\)<br/> for Bayes rate \(P^{\ast}\)</li> </ul> </li> </ul> <p>증명해보자</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/14-480.webp 480w,/assets/img/2024-09-10-Pattern/14-800.webp 800w,/assets/img/2024-09-10-Pattern/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-10-Pattern/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>error rate :<br/> Let \(x\) be test data<br/> Let \(x^{'}\) be the nearest neighbor for 1-NNR<br/> Let \(\theta^{'}\) be the labeled class of \(x^{'}\) (pred)<br/> Let \(\theta\) be the true class (gt) <ul> <li> \[P(e | x, x^{'}) = \sum_{i} P(\theta = w_i, \theta^{'} \neq w_i | x, x^{'}) = \sum_{i} P(\theta = w_i | x) P(\theta^{'} \neq w_i | x^{'}) = \sum_{i} P(\theta = w_i | x) (1 - P(\theta^{'} = w_i | x^{'}))\] </li> <li>As \(n \rightarrow \infty\), \(x^{'} \rightarrow x\)<br/> So, \(\text{lim}_{n \rightarrow \infty} P(\theta^{'} = w_i | x^{'}) = P(\theta = w_i | x)\)</li> <li> <table> <tbody> <tr> <td>$$\text{lim}_{n \rightarrow \infty} P(e</td> <td>x, x^{‘}) = P(e</td> <td>x) = \text{lim}<em>{n \rightarrow \infty} \sum</em>{i} P(\theta = w_i</td> <td>x) (1 - P(\theta^{‘} = w_i</td> <td>x^{‘})) = \sum_{i} P(\theta = w_i</td> <td>x) (1 - P(\theta = w_i</td> <td>x)) = \sum_{i} P(\theta = w_i</td> <td>x) - \sum_{i} (P(\theta = w_i</td> <td>x))^2 = 1 - \sum_{i} (P(\theta = w_i</td> <td>x))^2</td> </tr> </tbody> </table> </li> </ul> </li> </ul> <p>4p 남음</p> <h2 id="chapter-5-tbd">Chapter 5. TBD</h2>]]></content><author><name></name></author><category term="cv-tasks"/><category term="3d"/><category term="rendering"/><summary type="html"><![CDATA[Lecture Summary (24F)]]></summary></entry><entry><title type="html">Nabla (Del) operator</title><link href="https://semyeong-yu.github.io/blog/2024/nabla/" rel="alternate" type="text/html" title="Nabla (Del) operator"/><published>2024-09-06T11:00:00+00:00</published><updated>2024-09-06T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/nabla</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/nabla/"><![CDATA[<p>본 포스팅 출처 : <a href="https://xoft.tistory.com/71">Link</a></p> <h3 id="del">Del</h3> <ul> <li>\(\nabla = \frac{\partial}{\partial x}i + \frac{\partial}{\partial y}j\)<br/> where \(\nabla\) : vector</li> <li>Del operator의 피연산자가 scalar인지 vector인지에 따라 다르게 불림</li> </ul> <h3 id="gradient">Gradient</h3> <ul> <li>\(\nabla f = \frac{\partial f}{\partial x}i + \frac{\partial f}{\partial y}j\)<br/> where \(f\) : scalar field<br/> where \(\nabla f\) : vector field</li> <li>scalar 함수 각 점에서의 방향</li> </ul> <h3 id="divergence">Divergence</h3> <ul> <li>\(\nabla f = \nabla \cdot f = (\frac{\partial}{\partial x}i + \frac{\partial}{\partial y}j) \cdot (v_x i + v_y j) = \frac{\partial v_x}{\partial x} + \frac{\partial v_y}{\partial y}\)<br/> where \(f = v_x i + v_y j\) : vector field<br/> where \(\nabla f\) : scalar field</li> <li>vector 함수 각 점에서의 발산하는 크기</li> </ul> <h3 id="curl">Curl</h3> <ul> <li>\(\nabla \times f = \frac{\partial v_x}{\partial x}i + \frac{\partial v_y}{\partial y}j\)<br/> where \(f = v_x i + v_y j\) : vector field<br/> where \(\nabla f\) : scalar field</li> <li>점의 rotation</li> </ul> <h3 id="laplacian">Laplacian</h3> <ul> <li>\(\Delta = \nabla \cdot \nabla = \text{Divergence} \cdot \text{Gradient} = \frac{\partial^{2}}{\partial x} + \frac{\partial^{2}}{\partial y}\)<br/> where \(\Delta\) : scalar (Divergence of Gradient)</li> <li>image에 Laplacian filter를 쓰면<br/> Gradient로 색상이 급격히 변하는 vector를 검출한 뒤<br/> Divergence로 vector의 발산 크기 균일 정도를 파악하여<br/> Edge를 검출할 수 있음</li> </ul>]]></content><author><name></name></author><category term="math"/><category term="nabla"/><category term="del"/><category term="scalar"/><category term="vector"/><summary type="html"><![CDATA[del, gradient, divergence, curl, laplacian]]></summary></entry><entry><title type="html">SuGaR</title><link href="https://semyeong-yu.github.io/blog/2024/SuGaR/" rel="alternate" type="text/html" title="SuGaR"/><published>2024-09-05T11:00:00+00:00</published><updated>2024-09-05T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/SuGaR</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/SuGaR/"><![CDATA[<h2 id="sugar-surface-aligned-gaussian-splatting-for-efficient-3d-mesh-reconstruction-and-high-quality-mesh-rendering">SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering</h2> <h4 id="antoine-guédon-vincent-lepetit">Antoine Guédon, Vincent Lepetit</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2311.12775">https://arxiv.org/abs/2311.12775</a><br/> project website :<br/> <a href="https://anttwo.github.io/sugar/">https://anttwo.github.io/sugar/</a><br/> code :<br/> <a href="https://github.com/Anttwo/SuGaR">https://github.com/Anttwo/SuGaR</a><br/> reference :<br/> NeRF and 3DGS Study</p> </blockquote> <h2 id="abstract">Abstract</h2> <ul> <li> <p>surface 점 sampling :<br/> surface 근처의 점 \(p\) 를<br/> <code class="language-plaintext highlighter-rouge">Gaussians의 곱 분포</code>로 sampling</p> </li> <li> <p>regularization term :<br/> 3DGS가 surface 잘 나타내도록 (well-distributed) 하기 위해<br/> <code class="language-plaintext highlighter-rouge">density</code> function 또는 <code class="language-plaintext highlighter-rouge">SDF</code>로 <code class="language-plaintext highlighter-rouge">regularization</code> loss term</p> </li> <li> <p>obtain mesh using level set points :<br/> 점 \(p\) 주위(\(3 \sigma (v)\))의 points를 sampling하고<br/> density 계산하여 oriented <code class="language-plaintext highlighter-rouge">level set points</code> 구한 뒤<br/> Poisson equation으로 <code class="language-plaintext highlighter-rouge">mesh</code> 구함</p> </li> <li> <p>mesh refinement :<br/> triangle mesh에 new Gaussians binding하여<br/> mesh optimize할 때 new Gaussians도 함께 optimize</p> </li> </ul> <h2 id="surface-aligned-3dgs">Surface-Aligned 3DGS</h2> <h3 id="regularization">Regularization</h3> <ul> <li> <p>문제 :<br/> 3DGS는 <code class="language-plaintext highlighter-rouge">unstructured</code><br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">surface</code> 나타내지 않음</p> </li> <li> <p>해결 :<br/> <code class="language-plaintext highlighter-rouge">regularization</code> loss term<br/> \(\rightarrow\) 3DGS가 well-distributed and aligned with surface (flat)</p> <ul> <li>well-distributed : <ul> <li>Gaussians끼리 <code class="language-plaintext highlighter-rouge">overlap 적음</code></li> <li>(surface에 가까운) point \(p\) 와 <code class="language-plaintext highlighter-rouge">가장 가까운 Gaussian</code> \(g^{\ast}\) 가 다른 Gaussians보다 \(p\) 의 <code class="language-plaintext highlighter-rouge">density에 훨씬 많이 기여</code><br/> \(g^{\ast} = \text{argmin}_{g}(p - \mu_{g})^T \Sigma_{g}^{-1}(p - \mu_{g})\)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/1-480.webp 480w,/assets/img/2024-09-05-SuGaR/1-800.webp 800w,/assets/img/2024-09-05-SuGaR/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>surface 근처의 점 sampling : <ul> <li>assumption :<br/> 거의 surface 위에 있다고 볼 수 있을 정도로 아주 가까운<br/> <code class="language-plaintext highlighter-rouge">surface 근처</code>의 \(p\) 를 <code class="language-plaintext highlighter-rouge">Gaussian들의 곱 분포로 sampling</code><br/> \(p \sim \prod_{g} N(\cdot; \mu_{g}, \Sigma{g})\) <ul> <li>‘3DGS가 잘 학습됐다면’ small Gaussians는 surface에 아주 가까운 점들의 확률처럼 생각할 수 있고,<br/> Gaussian이 작을수록 sampling이 중심에 집중되므로<br/> 그 small Gaussians의 곱이 나타내는 분포는 surface 근처의 좁은 영역에 집중된 분포를 나타낼 것이고,<br/> 이로부터 sampling한 점 \(p\) 는 실제 object surface에 가까울 확률이 높다는 가정</li> <li>이렇게 sampling한 points는 regularization term에 대해 high gradient를 가지는 부분임</li> </ul> </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">density function</code> : <ul> <li>\(d(p) = \sum_{g} \alpha_{g} \text{exp}(-\frac{1}{2}(p - \mu_{g})^T \Sigma_{g}^{-1}(p - \mu_{g}))\)<br/> where \(\text{exp}(-\frac{1}{2}(p - \mu_{g})^T \Sigma_{g}^{-1}(p - \mu_{g}))\) : posterior<br/> (점 \(p\) 에 더 가까운 Gaussian의 \(\alpha_{g}\) 가 \(p\) 의 density에 더 많이 기여)<br/> where \((p - \mu_{g})^T \Sigma_{g}^{-1}(p - \mu_{g})\) : <code class="language-plaintext highlighter-rouge">Mahalanobis distance</code><br/> (\(p\) 가 Gaussian distribution 평균 \(\mu_{g}\) 에서 “상대적으로” 얼마나 떨어져 있는지)<br/> (\(p\) 가 평균으로부터 같은 거리만큼 떨어져있더라도 convariance가 작은 방향에 있을수록 Mahalanobis distance가 커짐)</li> <li>approx. ideal density function \(\bar d(p) \in [0, 1]\) : <ul> <li>가정 1) well-distributed Gaussians by regularization term 이므로<br/> overlap 없다는 전제 하에 <code class="language-plaintext highlighter-rouge">하나</code>의 Gaussian \(g^{\ast}\) 가 point \(p\) 의 density 결정</li> <li>가정 2) Gaussians가 진짜 surface를 묘사하려면 <code class="language-plaintext highlighter-rouge">semi-transparent하지 않아야</code> 좋음<br/> \(\rightarrow\) \(a_{g} = 1\) for any Gaussians</li> <li>위의 가정과 아래 수식 유도(<strong>Approximation of Density function</strong>) 에 따르면<br/> \((p - \mu_{g})^T \Sigma_{g}^{-1}(p - \mu_{g}) \approx \frac{1}{s_{g}^2} \langle p-\mu_{g}, n_g \rangle^{2}\) 이고<br/> 근사해서 구한 ideal density function은<br/> \(\bar d(p) = \text{exp}(-\frac{1}{2s_{g^{\ast}}^2} \langle p-\mu_{g^{\ast}}, n_{g^{\ast}} \rangle^{2})\)<br/> where \(g^{\ast} = \text{argmin}_{g}(p - \mu_{g})^T \Sigma_{g}^{-1}(p - \mu_{g})\)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/2-480.webp 480w,/assets/img/2024-09-05-SuGaR/2-800.webp 800w,/assets/img/2024-09-05-SuGaR/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/3-480.webp 480w,/assets/img/2024-09-05-SuGaR/3-800.webp 800w,/assets/img/2024-09-05-SuGaR/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Regularization on density</code> : <ul> <li> \[R = | d(p) - \bar d(p) |\] <ul> <li>\(d\) : <code class="language-plaintext highlighter-rouge">density</code> function</li> <li>\(\bar d\) : approx. <code class="language-plaintext highlighter-rouge">ideal density</code> function<br/> where 하나의 불투명한 Gaussian이 point density 결정</li> </ul> </li> <li>근데 density function \(d\) 로 regularize하면 아래의 문제가 있다 <ul> <li>\(d\) 는 exponential term으로 이루어져 있으므로 scale이 너무 커서 optimization에 별로다</li> <li>approx. ideal density function을 구할 때 flat Gaussian으로 surface를 나타내는 게 목적이라고 가정하였는데,<br/> Gaussian이 완전히 flat 하면 \(s_{g} = 0\) 이 되어 \(\bar d(p) \rightarrow 0\) 이므로<br/> 모든 level set (표면)이 \(\mu_{g}\) 를 지나고 normal \(n_{g}\) 를 가지는 2D 상의 plane이 되어<br/> level sets 고려하는 게 무의미해진다<br/> 따라서 surface를 나타내기 위해 flat하게 Gaussian을 만드는 게 목적이지만<br/> 그렇다고 완전히 flat하면 안 됨</li> </ul> </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Regularization on SDF</code> : <ul> <li>density function 말고 <code class="language-plaintext highlighter-rouge">SDF</code> <a href="https://semyeong-yu.github.io/blog/2024/SDF/">Link</a> 로 loss 만들면 optimization 더 잘 됨<br/> (Gaussians가 surface에 더 잘 align됨)<br/> \(R = \frac{1}{| P |} \sum_{p \in P} | \hat f(p) - f(p) |\) <ul> <li>\(f(p) = \langle p-\mu_{g^{\ast}}, n_{g^{\ast}} \rangle = \pm s_{g^{\ast}} \sqrt{-2log(\bar d(p))}\) :<br/> <code class="language-plaintext highlighter-rouge">ideal distance</code> (SDF) b.w. point \(p\) and true surface<br/> (\(\bar d(p) = 1\) 이면, 즉 SDF \(f(p) = 0\) (zero level-set)이면, true surface를 나타냄)</li> <li>\(\hat f(p)\) :<br/> <code class="language-plaintext highlighter-rouge">estimated distance</code> b.w. point \(p\) and depth at projection of \(p\)<br/> (\(f(p)\) 를 직접 계산하는 건 빡세므로 training view-points에 대해 Radix Sort로 Gaussian rasterize할 때 사용한 Gaussian depth 값들을 rendering하여 depth map을 만들어서 estimated \(\hat f(p)\) 구함)</li> </ul> </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Regularization on normal</code> vector : <ul> <li>normal vector의 방향 \(n_{g}\) 을 SDF gradient 방향으로 맞춰주기 위해<br/> (normal vector 방향을 잘 잡아줘야 surface에 잘 align됨)<br/> \(R_{Norm} = \frac{1}{| P |} \sum_{p \in P} \| \frac{\nabla f(p)}{\| \nabla f(p) \|} - n_{g^{\ast}} \|^2\)</li> </ul> </li> </ul> <h3 id="approximation-of-density-function">Approximation of Density function</h3> <ul> <li> <p>density function이 실제 surface를 잘 나타낸다면<br/> \(p\) 에 가장 기여가 큰 Gaussian이 surface에 align되어 flat해야 한다<br/> 이 때, <code class="language-plaintext highlighter-rouge">flat Gaussian</code>의 경우 Mahalanobis distance의 주 요인은 <code class="language-plaintext highlighter-rouge">covariance의 가장 짧은 축</code> \(s_{g}\) 이므로<br/> 아래와 같이 approx. ideal density function 식을 유도할 수 있다</p> </li> <li> <p>\(\bar d(p) = \text{exp}(-\frac{1}{2s_{g^{\ast}}^2} \langle p-\mu_{g^{\ast}}, n_{g^{\ast}} \rangle^{2})\) 유도 TBD <code class="language-plaintext highlighter-rouge">????</code><br/> Eigendecomposition을 하면 \(\Sigma_{g} = Q \Lambda Q^T\)<br/> where \(s_g\) : convariance가 가장 작은 방향의 vector<br/> where \(n_g = \frac{s_g}{\| s_g \|}\)</p> </li> </ul> <h2 id="mesh-reconstruction">Mesh reconstruction</h2> <h3 id="obtain-mesh">Obtain Mesh</h3> <ul> <li> <p>문제 :<br/> <code class="language-plaintext highlighter-rouge">Densification</code>을 거치면 3DGS 수가 너무 <code class="language-plaintext highlighter-rouge">많아</code>지고 너무 <code class="language-plaintext highlighter-rouge">작아</code>져서<br/> texture나 detail을 나타내기 힘듦<br/> \(\rightarrow\) 거의 모든 곳에서 density function \(d = 0\) 이고,<br/> 위에서 언급했듯이 level sets 고려하는 게 의미가 없어져서<br/> Marching Cubes 기법으로 이러한 <code class="language-plaintext highlighter-rouge">sparse density function</code>의 <code class="language-plaintext highlighter-rouge">level sets</code>를 추출하기 어렵</p> </li> <li> <p>해결 :</p> <ul> <li>과정 1)<br/> Gaussians로 계산한 density function level set 상의 <code class="language-plaintext highlighter-rouge">visible</code> part에 대해 3D <code class="language-plaintext highlighter-rouge">point sampling</code><br/> \(n\) 개의 3D points \(\{ p + t_i v_i \}_{i=1}^n\) sampling<br/> where \(p\) : depth map에 따른 3D point<br/> where \(t_i \in [-3 \sigma_{g}(v), 3\sigma_{g}(v)]\) (visible part)<br/> where \(v_i\) : ray direction</li> <li>과정 2)<br/> \(d_i = d(p + t_i v_i) = \sum_{g} \alpha_{g} \text{exp}(-\frac{1}{2}((p + t_i v_i) - \mu_{g})^T \Sigma_{g}^{-1}((p + t_i v_i) - \mu_{g}))\) 로<br/> <code class="language-plaintext highlighter-rouge">density 계산</code>한 뒤 level parameter \(\lambda\) 에 대해<br/> \(d_i \lt \lambda \lt d_j\) 이면,<br/> range \([d_i, d_j]\) 안에 <code class="language-plaintext highlighter-rouge">level set point</code> 있다고 판단<br/> (아! 그 범위 안에 표면 위의 점이 있구나!)</li> <li>과정 3)<br/> 해당 level set points와 normals (oriented 3d point clouds \(\vec V\))를 이용하여<br/> <code class="language-plaintext highlighter-rouge">Poisson reconstruction</code>으로 surface <code class="language-plaintext highlighter-rouge">mesh</code> 얻음<br/> (아래에서 설명)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/4-480.webp 480w,/assets/img/2024-09-05-SuGaR/4-800.webp 800w,/assets/img/2024-09-05-SuGaR/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="mesh-by-poisson-reconstruction">Mesh by Poisson Reconstruction</h3> <ul> <li> <p>Poisson surface reconstruction :<br/> <code class="language-plaintext highlighter-rouge">3D Point Clouds</code>를 <code class="language-plaintext highlighter-rouge">3D Mesh</code>로 변환하는 고전적인 방법 (출처 : <a href="https://xoft.tistory.com/72">Link</a>)</p> </li> <li> <p>Let indicator function \(\chi_{M}(p) = \begin{cases} 1 &amp; \text{if} &amp; p \in M \\ 0 &amp; \text{if} &amp; p \notin M \end{cases}\)<br/> where \(M\) : object mesh 내부</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/5-480.webp 480w,/assets/img/2024-09-05-SuGaR/5-800.webp 800w,/assets/img/2024-09-05-SuGaR/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>주어진 <code class="language-plaintext highlighter-rouge">oriented 3d point clouds</code> \(\vec V\) 를 approx.하는 <code class="language-plaintext highlighter-rouge">indicator gradient</code> \(\nabla \chi\) 를 찾아야 한다<br/> 이를 풀기 위해 Possion Equation을 사용하자 <ul> <li><code class="language-plaintext highlighter-rouge">Possion Equation</code> :<br/> \(\nabla^{2} \phi = f\)<br/> where \(\nabla = (\frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z})\)<br/> \(\rightarrow\)<br/> \((\frac{\partial^{2}}{\partial x^2}+\frac{\partial^{2}}{\partial y^2}+\frac{\partial^{2}}{\partial z^2}) \phi (x, y, z) = f(x, y, z)\)<br/> 여기서 scalar field \(f\) 가 주어지면,<br/> scalar field \(\phi\) 를 찾을 수 있다</li> <li>\(\nabla \chi \approx \vec V\) 원하는 상황인데<br/> 양변에 divergence를 취하면<br/> \(\nabla \cdot \nabla \chi = \nabla \cdot \vec V\) 은 Poisson Equation 꼴이므로<br/> \(\nabla \cdot \vec V\) 를 알면 \(\chi\) 를 알 수 있다</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/6-480.webp 480w,/assets/img/2024-09-05-SuGaR/6-800.webp 800w,/assets/img/2024-09-05-SuGaR/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Implementation : <ul> <li>oriented 3d point clouds가 주어지면<br/> 모든 points를 포함하는 큰 육면체를 만들고<br/> 이를 <code class="language-plaintext highlighter-rouge">Octree</code> (육면체를 8등분하는 tree)를 사용하여 분할 (Fig 1.)</li> <li>input point clouds \(\vec V\) 는 주변 octree들의 합으로 설계하고,<br/> octree node의 depth는 Gaussian의 variance로 설계하여<br/> input point clouds 근처의 octreee들을 Gaussian으로 표현하면<br/> vector field \(\vec V\) (Fig 2.) 를 얻을 수 있다</li> <li>각 차원을 편미분 (Divergence)하면 scalar field \(\nabla \cdot \vec V\) (Fig 3.)를 얻을 수 있고,<br/> Poisson equation \(\nabla \cdot \nabla \chi = \nabla \cdot \vec V\) 에 의해<br/> indicator function \(\chi\) 도 알 수 있다<br/> octree의 깊이 별로 각 node의 \(\nabla \vec V\) 값 (Fig 3.)과 \(\nabla \nabla \chi\) 값 (Fig 4.)의 차이를 최소화함으로써 indicator function \(\chi\) 를 구한다</li> <li>mesh화 : input point clouds를 indicator function \(\chi\) 의 입력으로 넣어서 나온 결과값들을 평균 내고, 이 값과 같은 값을 출력하는 좌표들을 surface로 간주 (Fig 5.)하여 Marching Cube 알고리즘으로 mesh 생성<br/> (Octree Node마다 Marching Cube Polygon 생성)<br/> (여러 fine Octree Node가 하나의 coarse Octree Node를 공유할 때 생기는 문제를 해결하기 위해 fine Octree Node 면의 부분을 coarse한 면으로 projection하는 방법 사용)</li> <li>octree 깊이가 깊어질수록 시간과 memory를 많이 잡아먹긴 하지만, recon.하는 mesh 수가 더 많아서 mesh fine detail을 살릴 수 있음 (Fig 6.)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/7-480.webp 480w,/assets/img/2024-09-05-SuGaR/7-800.webp 800w,/assets/img/2024-09-05-SuGaR/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 1. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/8-480.webp 480w,/assets/img/2024-09-05-SuGaR/8-800.webp 800w,/assets/img/2024-09-05-SuGaR/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 2. vector field (oriented point clouds) </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/10-480.webp 480w,/assets/img/2024-09-05-SuGaR/10-800.webp 800w,/assets/img/2024-09-05-SuGaR/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 3. scalar field (divergence of oriented point clouds) </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/9-480.webp 480w,/assets/img/2024-09-05-SuGaR/9-800.webp 800w,/assets/img/2024-09-05-SuGaR/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 4. scalar field (laplacian of indicator function) </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/11-480.webp 480w,/assets/img/2024-09-05-SuGaR/11-800.webp 800w,/assets/img/2024-09-05-SuGaR/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 5. surface mesh </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/12-480.webp 480w,/assets/img/2024-09-05-SuGaR/12-800.webp 800w,/assets/img/2024-09-05-SuGaR/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 6. octree 깊이에 따른 mesh recon. 비교 </div> <h2 id="refine-mesh">Refine Mesh</h2> <h3 id="refine-mesh-by-gaussians">Refine Mesh by Gaussians</h3> <ul> <li> <p>문제 :<br/> Poisson reconstruction으로 구한 mesh만 사용하면 rendering quality가 좋지 않음</p> </li> <li> <p>해결 :<br/> 새로 sampling한 new Gaussians를 (triangle) mesh에 binding하고,<br/> 해당 <code class="language-plaintext highlighter-rouge">Gaussians</code>과 <code class="language-plaintext highlighter-rouge">mesh</code>를 GS rasterizer로 <code class="language-plaintext highlighter-rouge">함께 optimize</code></p> <ul> <li>과정 1)<br/> mesh surface 상에서 <code class="language-plaintext highlighter-rouge">triangle</code> 당 \(n\) 개의 <code class="language-plaintext highlighter-rouge">new thin 3D Gaussians를 sampling</code>하여<br/> Gaussians를 triangle에 bind</li> <li>과정 2)<br/> mesh vertices in barycentric coordinate (무게중심 좌표계) 이용해서<br/> 각 Gaussian의 mean을 explicitly 계산할 수 있음<br/> (barycentric coordinate : 삼각형 내부의 점을 세 꼭짓점의 가중치로 표현)</li> <li>과정 3)<br/> Gaussians를 mesh triangle에 aligned되도록 flat하게 유지하기 위해<br/> each Gaussian은 2개의 learnable scaling factor \(s_x, s_y\) 와 1개의 learnable 2D quaternion \(q=a+bi\) 을 가지고 있음<br/> (Gaussians optimize하여 mesh optimze될 때 new thin Gaussians도 함께 optimize)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/13-480.webp 480w,/assets/img/2024-09-05-SuGaR/13-800.webp 800w,/assets/img/2024-09-05-SuGaR/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="code-flow">Code Flow</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-05-SuGaR/14-480.webp 480w,/assets/img/2024-09-05-SuGaR/14-800.webp 800w,/assets/img/2024-09-05-SuGaR/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-05-SuGaR/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 출처 : NeRF and 3DGS Study </div> <h2 id="question">Question</h2> <ul> <li>Q1 : well-distributed 가정을 따르는 approx. ideal density function을 직접 구해서 이를 density function과 비교하는데, GT 역할을 하는 approx. ideal density function이, 변하는 learnable Gaussian으로 구한 것이어도 학습이 안정적임?</li> <li>A1 : TBD</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="gaussian"/><category term="splatting"/><category term="rendering"/><category term="surface"/><summary type="html"><![CDATA[Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering (CVPR 2024)]]></summary></entry><entry><title type="html">CLIP</title><link href="https://semyeong-yu.github.io/blog/2024/CLIP/" rel="alternate" type="text/html" title="CLIP"/><published>2024-09-03T11:00:00+00:00</published><updated>2024-09-03T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/CLIP</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/CLIP/"><![CDATA[<h2 id="clip-contrastive-language-image-pre-training">CLIP: Contrastive Language-Image Pre-training</h2> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2103.00020">https://arxiv.org/abs/2103.00020</a><br/> code :<br/> <a href="https://github.com/openai/CLIP">https://github.com/openai/CLIP</a><br/> referenced blog :<br/> <a href="https://xoft.tistory.com/67">https://xoft.tistory.com/67</a></p> </blockquote> <h3 id="intro">Intro</h3> <ul> <li>CLIP : text와 image 간의 관계를 사용하는 다양한 task에 적용 가능</li> </ul> <h3 id="contrastive-pre-training">Contrastive Pre-training</h3> <ul> <li><code class="language-plaintext highlighter-rouge">contrastive learning</code> :<br/> labeling 없는 self-supervised learning 기법 중 하나로,<br/> 같은 class라면 embedding distance를 최소화하고<br/> 다른 class라면 embedding distance를 최대화한다 <ul> <li>contrastive loss<br/> \(L_{cont}^m(x_i, x_j, f) = 1 \{ y_i = y_j \} \| f(x_i) - f(x_j) \|^2 + 1 \{ y_i \neq y_j \} \text{max}(0, m - \| f(x_i) - f(x_j) \|^2)\)</li> <li>triplet loss<br/> \(L_{trip}^m(x, x^{+}, x^{-}, f) = max(0, \| f(x) - f(x^{+}) \|^2 - \| f(x) - f(x^{-}) \|^2 + m)\)</li> <li>\(N+1\) - Tuplet loss<br/> \(L_{tupl}(x, x^{+}, \{ x_{i}^{-} \}_{i=1}^{N-1}, f) = log(1 + \Sigma_{i=1}^{N-1}\text{exp}(f(x)^T f(x_{i}^{-}) - f(x)^T f(x^{+}))) = - log(\frac{\text{exp}(f(x)^T f(x^{+}))}{\text{exp}(f(x)^T f(x^{+})) + \Sigma_{i=1}^{N-1} \text{exp}(f(x)^T f(x_{i}^{-}))})\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-03-CLIP/1-480.webp 480w,/assets/img/2024-09-03-CLIP/1-800.webp 800w,/assets/img/2024-09-03-CLIP/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-03-CLIP/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> cosine similarity matrix가 identity matrix (I) 에 가깝도록 학습 </div> <ul> <li>image-text pair로 구성된 dataset에 대해<br/> image, text를 각각 encoder로 embedding한 뒤<br/> 같은 pair는 거리 최소화하고<br/> 다른 pair는 거리 최대화하도록<br/> constrative learning으로 두 encoder를 학습</li> </ul> <h3 id="application">Application</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-03-CLIP/2-480.webp 480w,/assets/img/2024-09-03-CLIP/2-800.webp 800w,/assets/img/2024-09-03-CLIP/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-03-CLIP/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>dataset classifier 만들기 또는 zero-shot prediction 등에<br/> pre-trained CLIP model 사용 가능</li> </ul>]]></content><author><name></name></author><category term="cv-tasks"/><category term="contrastive"/><category term="image"/><category term="text"/><summary type="html"><![CDATA[Contrastive Language-Image Pre-training]]></summary></entry></feed>