<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://semyeong-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://semyeong-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-15T01:34:25+00:00</updated><id>https://semyeong-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">MASt3R</title><link href="https://semyeong-yu.github.io/blog/2024/MASt3R/" rel="alternate" type="text/html" title="MASt3R"/><published>2024-11-21T12:00:00+00:00</published><updated>2024-11-21T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/MASt3R</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/MASt3R/"><![CDATA[<h2 id="grounding-image-matching-in-3d-with-mast3r">Grounding Image Matching in 3D with MASt3R</h2> <h4 id="vincent-leroy-yohann-cabon-jérôme-revaud">Vincent Leroy, Yohann Cabon, Jérôme Revaud</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2406.09756">https://arxiv.org/abs/2406.09756</a><br/> project website :<br/> <a href="https://europe.naverlabs.com/blog/mast3r-matching-and-stereo-3d-reconstruction/">https://europe.naverlabs.com/blog/mast3r-matching-and-stereo-3d-reconstruction/</a><br/> code :<br/> <a href="https://github.com/naver/mast3r">https://github.com/naver/mast3r</a><br/> reference :<br/> <a href="https://xoft.tistory.com/100">https://xoft.tistory.com/100</a></p> </blockquote> <h3 id="contribution">Contribution</h3> <ul> <li>DUSt3R : <ul> <li>많은 연산을 필요로 하는 <code class="language-plaintext highlighter-rouge">SfM 생략</code> (pose-free)</li> <li>transformer 기반으로<br/> <code class="language-plaintext highlighter-rouge">2D(img pixel)-to-3D(point map)</code> mapping 예측하여<br/> <code class="language-plaintext highlighter-rouge">regression-based</code> 3D recon. 수행</li> <li>predicted pointmap 기반으로<br/> intrinsic/extrinsic camera param. 추정 가능</li> </ul> </li> <li>MASt3R : <ul> <li>DUSt3R 후속 논문으로,<br/> DUSt3R을 활용하여 <code class="language-plaintext highlighter-rouge">Image Matching에 특화</code>시킴 <ul> <li><code class="language-plaintext highlighter-rouge">Image Matching</code> 문제를 <code class="language-plaintext highlighter-rouge">3D 상에서</code> 풂</li> <li>quality 향상 및 속도 개선 및 많은 images 수 커버 가능</li> </ul> </li> </ul> </li> </ul> <h3 id="image-matching">Image Matching</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/1-480.webp 480w,/assets/img/2024-11-21-MASt3R/1-800.webp 800w,/assets/img/2024-11-21-MASt3R/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Image Matching 문제를 3D 상에서 풀기 때문에<br/> 2개 image의 공통 영역이 매우 적더라도 Image Matching 잘 수행 가능!</p> </li> <li> <p>Image Matching 기법의 문제 및 해결 :</p> <ul> <li>문제 1)<br/> 전통적인 Image Matching은 keypoint을 추출하여 local-invariant descriptor로 변형한 뒤<br/> feature space에서 거리를 비교하여 Matching을 수행했음<br/> 조명 변화와 시점 변화에도 정확했고 적은 keypoint 수로도 [ms] 단위로 Matching 가능했음<br/> 하지만 geometry context는 고려하지 못했고<br/> 반복 패턴이나 low-texture 영역에서는 잘 수행하지 못했음</li> <li>해결 1)<br/> local descriptor 대신 global descriptor를 이용하는 SuperGlue(2020) 기법</li> <li>문제 2)<br/> SuperGlue(2020)의 경우 keypoint descriptor가 충분히 encode되지 않으면 matching 도중에 global text를 활용할 수 없었음</li> <li>해결 2)<br/> keypoint 대신 image 전체를 한 번에 matching하는 dense holistic matching 기법<br/> thanks to global attention<br/> e.g. LoFTR(2021) : 반복 패턴 및 low-texture 영역에 robust하고 dense correspondence 만들 수 있음</li> <li>문제 3)<br/> LoFTR(2021)은 Map-free localization benchmark의 VCRE 평가에서 낮은 성능<br/> 현실적으로 Image Matching task는 같은 3D point에 대응되는 pixel을 찾는 문제인데 지금까지 전통적인 matching 기법들은 전부 2D 상에서 이루어졌기 때문</li> <li>해결 3)<br/> 2D pixel - 3D point correspondence 다루는 DUSt3R 활용</li> <li>문제 4)<br/> DUSt3R은 3D recon.을 목적으로 만들어졌기 때문에<br/> 시점 변화에는 강인하지만 Image Matching에서는 비교적 부정확</li> <li>해결 4)<br/> MASt3R(본 논문)에서는 DUSt3R을 Image Matching에 특화하는 방법에 대해 다룸!</li> </ul> </li> </ul> <h3 id="dust3r-framework">DUSt3R Framework</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/2-480.webp 480w,/assets/img/2024-11-21-MASt3R/2-800.webp 800w,/assets/img/2024-11-21-MASt3R/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>DUSt3R과 달라진 점 : <ul> <li>DUSt3R에서는 3D recon.이 목적이었기 때문에<br/> view-point가 달라지더라도 같은 크기의 물체를 recon.해야 함<br/> 즉, scale-invariant하도록 만들기 위해<br/> 각 view-point에서 averaged depth 값으로 나누어 normalize해주었음<br/> \(\rightarrow\)<br/> MASt3R에서는 서로 다른 scale의 images인 상태에서 image matching task를 수행해야 하므로<br/> (scale을 고려해야 하므로)<br/> regression loss에서 <code class="language-plaintext highlighter-rouge">scale(depth) normalization 파트를 없앰</code></li> </ul> </li> <li>Loss : <ul> <li><code class="language-plaintext highlighter-rouge">regression loss</code> :<br/> \(L_{regr} (v, i) = \| \frac{1}{z} X_{i}^{v, 1} - \frac{1}{\bar z} \bar X_{i}^{v, 1} \|\) <ul> <li>\(i\) : each point, \(v\) : each view</li> <li>\(z = \bar z\) : averaged depth of GT point</li> </ul> </li> <li>confidence loss :<br/> \(L_{conf} = \sum_{v \in \{ 1, 2 \}} \sum_{i \in D^{v}} C_{i}^{v, 1} L_{regr}(v, i) - \alpha \text{log} C_{i}^{v, 1}\) <ul> <li>\(C_{i}^{v, 1}\) : confidence score<br/> 물체인 부분에서는 3D point를 비교적 정확히 예측할 수 있으므로 confidence가 높고,<br/> 하늘 또는 반투명인 부분에서는 3D point를 정확하게 예측할 수 없으므로 confidence가 낮게 나옴</li> <li>\(C_{i}^{v, 1} L_{regr}(v, i)\) :<br/> confidence가 큰 <code class="language-plaintext highlighter-rouge">(확실한) point</code>에서는 GT와의 <code class="language-plaintext highlighter-rouge">regression loss</code> \(L_{regr}\) 가 더 <code class="language-plaintext highlighter-rouge">작도록</code></li> <li>\(- \alpha \text{log} C_{i}^{v, 1}\) : regularization term<br/> <code class="language-plaintext highlighter-rouge">confidence</code> \(C_{i}^{v, 1}\) 값이 <code class="language-plaintext highlighter-rouge">너무 작아지지 않도록</code></li> </ul> </li> <li>matching loss (<code class="language-plaintext highlighter-rouge">cross-entropy classification loss</code>) :<br/> \(L_{match} = - \sum_{(i, j) \in \hat M} (\text{log} \frac{s_{\tau} (i, j)}{\sum_{k \in P^{1}} s_{\tau} (k, j)} + \text{log} \frac{s_{\tau} (i, j)}{\sum_{k \in P^{2}} s_{\tau} (i, k)})\)<br/> 다음 section에서 언급할 예정</li> <li>total loss :<br/> \(L_{tot} = L_{conf} + \beta L_{match}\)</li> </ul> </li> </ul> <h3 id="matching-prediction-head">Matching Prediction Head</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/2-480.webp 480w,/assets/img/2024-11-21-MASt3R/2-800.webp 800w,/assets/img/2024-11-21-MASt3R/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>기존 DUSt3R의 Head output :<br/> extreme한 view-point 변화에도 robust <ul> <li>per-pixel <code class="language-plaintext highlighter-rouge">Pointmap</code> \(X_{i}^{v, 1} \in R^{H \times W \times 3}\)</li> <li>per-pixel <code class="language-plaintext highlighter-rouge">Confidence</code> score \(C_{i}^{v, 1} \in R^{H \times W}\)</li> </ul> </li> <li>MASt3R의 new Head output : <ul> <li>per-pixel <code class="language-plaintext highlighter-rouge">Local Feature</code> \(D_{i}^{v} \in R^{H \times W \times d}\) (\(d = 24\))</li> </ul> </li> <li>Fast NN :<br/> Fast Reciprocal Matching by Nearest Neighborhood<br/> (다음 section에서 설명 예정) <ul> <li>predicted <code class="language-plaintext highlighter-rouge">Pointmap</code> 이용하여 <code class="language-plaintext highlighter-rouge">Geometrical matching</code> 수행 <ul> <li>2개의 image에 대한 3D pointmap을 겹쳤을 때 align되도록<br/> <code class="language-plaintext highlighter-rouge">3D 공간 상에서 pixel correspondence</code>를 찾음</li> <li>\(X^{2, k}\) 중에 3D point \(X_{i}^{1, k}\) 와 가장 가까운 3D point가 \(X_{j}^{2, k}\) 이고,<br/> 동시에 \(X^{1, k}\) 중에 3D point \(X_{j}^{2, k}\) 와 가장 가까운 3D point가 \(X_{i}^{1, k}\) 일 때<br/> 두 pixel \(i, j\) 사이에 correspondence 있다고 함</li> </ul> </li> <li>predicted <code class="language-plaintext highlighter-rouge">Local Feature</code> 이용하여 <code class="language-plaintext highlighter-rouge">Feature-based matching</code> 수행</li> <li>얘네들 어떻게 합치는지 <code class="language-plaintext highlighter-rouge">???</code></li> </ul> </li> <li>Loss : <ul> <li>matching loss :<br/> constrastive learning에 사용되는 infoNCE Loss를 변형하여<br/> \(L_{match} = - \sum_{(i, j) \in \hat M} (\text{log} \frac{s_{\tau} (i, j)}{\sum_{k \in P^{1}} s_{\tau} (k, j)} + \text{log} \frac{s_{\tau} (i, j)}{\sum_{k \in P^{2}} s_{\tau} (i, k)})\)<br/> where \(s_{\tau} (i, j) = \text{exp}(- \tau D_{i}^{1 T} D_{j}^{2})\)<br/> 근데 \(s_{\tau} (i, j)\) 는 similarity score이니까 \(\text{exp}\) 안에 - 가 없어야 할 것 같음! 오타인가? 아니면 내가 잘못 생각하고 있나?<br/> <code class="language-plaintext highlighter-rouge">?????</code> <ul> <li>cross-entropy classification loss 꼴<br/> (regression loss 꼴 아님)<br/> \(\rightarrow\)<br/> 정확히 correct pixel pair (<code class="language-plaintext highlighter-rouge">positive sample</code>) \((i, j)\) 에 대해서는 \(s_{\tau} (i, j)\) 이 높아지고<br/> nearby pixel (<code class="language-plaintext highlighter-rouge">negative sample</code>) \((i+1, j)\) 에 대해서는 \(s_{\tau} (i+1, j)\) 이 낮아지도록 설계하여<br/> <code class="language-plaintext highlighter-rouge">nearby pixel로 regression하는 게 아니라</code> <code class="language-plaintext highlighter-rouge">정확한 correct pixel pair를 분류</code>하도록 하므로<br/> high-precision image matching 가능</li> <li>positive sample :<br/> pixel correspondence가 있는 pixel pair (2개 image에서 모두 나타나고 3D point가 일치하는 pixel pair)<br/> 1번 image의 \(i\)-th pixel이 2번 image의 \(j\)-th pixel과 correspondence 있다면<br/> \((i, j) \in \hat M = \{ (i, j) | \hat X_{i}^{1, 1} = \hat X_{j}^{2, 1} \}\)<br/> where \(X^{v, 1}\) : 1번 view-point를 중심좌표계로 두고 \(v\) 번 view에서 보이는 3D point 좌표</li> <li>negative sample :<br/> positive sample들을 모은 뒤<br/> \(\hat M\) 에서 대응되지 않는 pixel pair</li> <li>\(P^{1} = \{ i | (i, j) \in \hat M \}\) and \(P^{2} = \{ j | (i, j) \in \hat M \}\)<br/> 따라서 log 안의 분자는 positive sample의 score에 해당하고<br/> log 안의 분모는 negative sample의 score에 해당</li> </ul> </li> </ul> </li> </ul> <h3 id="fast-reciprocal-matching">Fast Reciprocal Matching</h3> <p>그렇다면 위에서 positive sample \(M\) 은 어떻게 찾을까?</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/4-480.webp 480w,/assets/img/2024-11-21-MASt3R/4-800.webp 800w,/assets/img/2024-11-21-MASt3R/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Fast Reciprocal Matching :<br/> feature space \(D\) 에서 reciprocal matching 수행 <ul> <li>기존 matching :<br/> Nearest Neighbor 기법 사용하여<br/> \(D^{1}\) 중에 \(D_{j}^{2}\) 와 가장 유사한 pixel이 \(D_{i}^{1}\) 이고,<br/> 동시에 \(D^{2}\) 중에 \(D_{i}^{1}\) 와 가장 유사한 pixel이 \(D_{j}^{2}\) 일 때<br/> 해당 \((i, j)\) pair에는 pixel correspondence가 있다고 함 <ul> <li>complexity \(O(W^{2} H^{2})\)</li> </ul> </li> <li>Fast Reciprocal matching :<br/> <code class="language-plaintext highlighter-rouge">연산 줄이기 위해 image의 부분 pixel들로 matching</code> 진행 <ul> <li>Step 1)<br/> image 1 에서 \(k\) 개의 pixel을 uniform sampling하여 \(U^{0}\) 로 표기</li> <li>Step 2)<br/> 기존 matching 방법대로 Nearest Neighbor 기법 사용하여<br/> mapping from \(U^{0}\) to \(V^{1}\) 진행<br/> (\(V^{t+1}\) : image 2에서 \(U^{t}\) 와 가장 유사한 pixel들의 집합)</li> <li>Step 3)<br/> 기존 matching 방법대로 Nearest Neighbor 기법 사용하여<br/> 다시 mapping from \(V^{1}\) to \(U^{1}\) 진행<br/> (\(U^{t+1}\) : image 1에서 \(V^{t+1}\) 와 가장 유사한 pixel들의 집합)</li> <li>Step 4)<br/> 만약 \(U^{t}\) 와 \(U^{t+1}\) 이 같다면 reciprocal pair로 저장<br/> \(M_{k}^{t} = \{ (i, j) | i \in U^{t+1}, j \in V^{t+1} \}\)</li> <li>Step 5)<br/> 또 다른 \(k\) 개의 pixel을 uniform sampling하여 Step 1) ~ Step 4)를 반복<br/> 이 때, 이전 loop에서 matching된 \(U^{t}\) 는 빼서 계산하므로 (\(U^{t+1} = U^{t+1} \setminus U^{t}\))<br/> 점점 un-converged point \(| U^{t} |\) 가 줄어들어 수렴</li> <li>Step 6)<br/> 최종적으로 reciprocal pair 집합 (positive sample) 만들 수 있음<br/> \(M_{k} = \cup_{t} M_{k}^{t}\)</li> <li>complexity \(O(kWH)\)<br/> (모든 pixel 조합을 비교하지 않음)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/5-480.webp 480w,/assets/img/2024-11-21-MASt3R/5-800.webp 800w,/assets/img/2024-11-21-MASt3R/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/6-480.webp 480w,/assets/img/2024-11-21-MASt3R/6-800.webp 800w,/assets/img/2024-11-21-MASt3R/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="coarse-to-fine-matching">Coarse-to-Fine Matching</h3> <ul> <li>Coarse-to-Fine Matching :<br/> <code class="language-plaintext highlighter-rouge">연산 줄이기 위해</code><br/> <code class="language-plaintext highlighter-rouge">저해상도 image pair에서 Fast Reciprocal Matching 수행하여 집중해야 할 영역(window pair)을 찾고,</code><br/> <code class="language-plaintext highlighter-rouge">고해상도 window pair에서 Fine Matching 수행하여 fine pixel correpondence 얻음</code><br/> low-resolution algorithm으로 high-resolution images를 match하기 위한 기법 <ul> <li>Step 1)<br/> \(k\) 배 subsampling하여 two downscaled images에서 Fast Reciprocal Matching 수행<br/> coarse pixel pair 집합을 \(M_{k}^{0}\) 으로 표기</li> <li>Step 2)<br/> 원본 고해상도 image 위에 grid of overlapping window \(\in R^{w \times 4}\) 를 만든 뒤<br/> (each window crop measures 512 pixels in its largest dimension <code class="language-plaintext highlighter-rouge">?????</code>)<br/> (인접한 windows overlap by 50%)<br/> coarse pixel pair \(M_{k}^{0}\) 를 가장 많이 포함하는 window pair \((w_{1}, w_{2}) \in W^{1} \times W^{2}\) 찾음</li> <li>Step 3)<br/> coarse pixel pair \(M_{k}^{0}\) 의 90%가 커버될 때까지<br/> greedy fashion으로 window pair를 추가</li> <li>Step 4)<br/> 최종적으로 each window pair를 두 이미지로 보고,<br/> each window pair에 대해 각각 matching 수행하여 fine pixel pair 집합 구함<br/> Then they are finally mapped back to the original image coordinates and concatenated,<br/> thus providing dense full-resolution matching</li> </ul> </li> </ul> <h3 id="experiments">Experiments</h3> <ul> <li>Map-free Localization</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/7-480.webp 480w,/assets/img/2024-11-21-MASt3R/7-800.webp 800w,/assets/img/2024-11-21-MASt3R/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/9-480.webp 480w,/assets/img/2024-11-21-MASt3R/9-800.webp 800w,/assets/img/2024-11-21-MASt3R/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/8-480.webp 480w,/assets/img/2024-11-21-MASt3R/8-800.webp 800w,/assets/img/2024-11-21-MASt3R/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Multi-view Relative Pose Estimation</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/10-480.webp 480w,/assets/img/2024-11-21-MASt3R/10-800.webp 800w,/assets/img/2024-11-21-MASt3R/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Visual Localization</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/11-480.webp 480w,/assets/img/2024-11-21-MASt3R/11-800.webp 800w,/assets/img/2024-11-21-MASt3R/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Multi-view 3D Reconstruction</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/12-480.webp 480w,/assets/img/2024-11-21-MASt3R/12-800.webp 800w,/assets/img/2024-11-21-MASt3R/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/13-480.webp 480w,/assets/img/2024-11-21-MASt3R/13-800.webp 800w,/assets/img/2024-11-21-MASt3R/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="future-work">Future Work</h3> <ul> <li>pose 없이 2D-to-3D 수행하는 DUSt3R, MASt3R에<br/> 3DGS를 적용한 NoPoSplat <a href="https://noposplat.github.io/">Link</a></li> </ul> <h3 id="question">Question</h3> <ul> <li> <p>Q1 :<br/> DUSt3R와 달리 MASt3R에서는 averaged depth 값으로 나누지 않는 이유를 다시 설명해주실 수 있을까요?</p> </li> <li>A1 : <ul> <li>DUSt3R에서는 3D recon.이 목적이었기 때문에<br/> view-point가 달라지더라도 같은 크기의 물체를 recon.해야 함<br/> 즉, scale-invariant하도록 만들기 위해<br/> 각 view-point에서 averaged depth 값으로 나누어 normalize해주었음</li> <li>MASt3R에서는 서로 다른 scale의 images인 상태 그 자체에서 image matching task를 수행해야 하므로<br/> (scale을 고려해야 하므로)<br/> regression loss에서 scale(depth) normalization 파트를 없앰</li> </ul> </li> <li> <p>Q2 :<br/> 3D 상에서 matching을 수행하므로 image 1에서 잘 보이는 부분이 image 2에서 잘 보이지 않더라도 잘 matching된다고 하셨는데,<br/> 왜 3D 상에서 matching을 수행한다고 하는지 이해가 되지 않습니다.</p> </li> <li>A2 :<br/> Fast Reciprocal Matching 기법으로 두 가지 matching을 수행하는데, <ul> <li>predicted 3D pointmap을 이용한 geometrical matching :<br/> 2개의 image에 대한 3D pointmap을 겹쳤을 때 align되도록<br/> 3D 공간 상에서 pixel correspondence를 찾음 <ul> <li>\(X^{2, k}\) 중에 3D point \(X_{i}^{1, k}\) 와 가장 가까운 3D point가 \(X_{j}^{2, k}\) 이고,<br/> 동시에 \(X^{1, k}\) 중에 3D point \(X_{j}^{2, k}\) 와 가장 가까운 3D point가 \(X_{i}^{1, k}\) 일 때<br/> 두 pixel \(i, j\) 사이에 correspondence 있다고 함</li> </ul> </li> <li>predicted local feature를 이용한 feature-based matching</li> </ul> </li> <li> <p>Q3 :<br/> geometrical matching 결과랑 feature-based matching 결과를 어떻게 합치나요?</p> </li> <li> <p>A3 :<br/> 그 부분은 아직 살펴보지 못해서 코드를 한 번 봐야 알 수 있을 것 같습니다. 알아본 뒤 블로그 포스팅에 업데이트해놓도록 하겠습니다.<br/> TBD <code class="language-plaintext highlighter-rouge">?????</code></p> </li> <li> <p>Q4 :<br/> \(U^{t} = U^{t+1}\) 이면 reciprocal pair로 저장하는데<br/> 만약 실패한 pixels가 많으면 결국 complexity가 \(O(kWH)\) 로 낮아지지 않을 것 같은데<br/> complexity \(O(kWH)\) 가 어떻게 달성되나요?</p> </li> <li> <p>A4 :<br/> TBD <code class="language-plaintext highlighter-rouge">?????</code></p> </li> <li> <p>Q5 :<br/> Fast Reciprocal Matching에서 \(U^{t} \rightarrow V^{t+1}\) 와 \(V^{t+1} \rightarrow U^{t+1}\) 을 어떻게 정의하나요?<br/> \(U^{t} \rightarrow V^{t+1}\) 는 Nearest Neighbor mapping이고 \(V^{t+1} \rightarrow U^{t+1}\) 는 transformer로 학습된 mapping 으로 진행하는 식인가요?</p> </li> <li>A5 :<br/> 아니요, \(U^{t} \rightarrow V^{t+1}\) 와 \(V^{t+1} \rightarrow U^{t+1}\) 둘 다 Nearest Neighbor mapping입니다.<br/> \(D^{1}\) 중에 \(D_{j}^{2}\) 와 가장 유사한 pixel이 \(D_{i}^{1}\) 이고,<br/> 동시에 \(D^{2}\) 중에 \(D_{i}^{1}\) 와 가장 유사한 pixel이 \(D_{j}^{2}\) 일 때<br/> 해당 \((i, j)\) pair에는 pixel correspondence가 있다고 합니다.<br/> 즉, \(i\) 랑 가장 가까운 게 \(j\) 이더라도, \(j\) 랑 가장 가까운 게 \(i\) 가 아닐 수도 있다는 의미입니다.<br/> 따라서 \(i \in U^{t}\) 와 가장 가까운 게 \(j \in V^{t+1}\) 이고, \(j \in V^{t+1}\) 와 가장 가까운 게 \(i \in U^{t+1}\) 일 때 reciprocal pair로 저장(correspondence 존재)합니다.</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="point"/><category term="regression"/><category term="pose"/><category term="free"/><summary type="html"><![CDATA[Grounding Image Matching in 3D with MASt3R]]></summary></entry><entry><title type="html">DUSt3R</title><link href="https://semyeong-yu.github.io/blog/2024/DUSt3R/" rel="alternate" type="text/html" title="DUSt3R"/><published>2024-11-19T12:00:00+00:00</published><updated>2024-11-19T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/DUSt3R</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/DUSt3R/"><![CDATA[<h2 id="dust3r---geometric-3d-vision-made-easy-cvpr-2024">DUSt3R - Geometric 3D Vision Made Easy (CVPR 2024)</h2> <h4 id="shuzhe-wang-vincent-leroy-yohann-cabon-boris-chidlovskii-jerome-revaud">Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, Jerome Revaud</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2312.14132">https://arxiv.org/abs/2312.14132</a><br/> project website :<br/> <a href="https://europe.naverlabs.com/research/publications/dust3r-geometric-3d-vision-made-easy/">https://europe.naverlabs.com/research/publications/dust3r-geometric-3d-vision-made-easy/</a><br/> code :<br/> <a href="https://github.com/naver/dust3r">https://github.com/naver/dust3r</a><br/> reference :<br/> <a href="https://xoft.tistory.com/83">https://xoft.tistory.com/83</a></p> </blockquote> <h3 id="contribution">Contribution</h3> <ul> <li> <p>MVS(Multi-View Stereo) 분야에서는 일반적으로 camera param.를 알아야 해서<br/> SfM(Structure from Motion)을 사용해서 camera param. estimaton을 하지만<br/> 이는 많은 연산 필요</p> </li> <li> <p>DUSt3R :</p> <ul> <li>많은 연산을 필요로 하는 <code class="language-plaintext highlighter-rouge">SfM 생략</code> (pose-free)</li> <li>transformer 기반으로<br/> <code class="language-plaintext highlighter-rouge">2D(img pixel)-to-3D(point map)</code> mapping 예측하여<br/> <code class="language-plaintext highlighter-rouge">regression-based</code> 3D recon. 수행</li> <li>1번 view를 기준으로 2번 view의 3D points를 <code class="language-plaintext highlighter-rouge">상대적으로 align</code>하므로<br/> (3D point의 절대적인 위치를 추정하는 게 아니므로)<br/> intrinsic/extrinsic <code class="language-plaintext highlighter-rouge">camera param. 몰라도</code> ok</li> <li>predicted pointmap 기반으로<br/> intrinsic/extrinsic camera param. 추정 가능</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/1-480.webp 480w,/assets/img/2024-11-19-DUSt3R/1-800.webp 800w,/assets/img/2024-11-19-DUSt3R/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="algorithm">Algorithm</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/2-480.webp 480w,/assets/img/2024-11-19-DUSt3R/2-800.webp 800w,/assets/img/2024-11-19-DUSt3R/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Algorithm : <ul> <li>Step 1) input<br/> image 2장</li> <li>Step 2) ViT encoder<br/> 두 images의 feature 비교하기 위해<br/> <code class="language-plaintext highlighter-rouge">Siamese</code> (shared weight) 구조 사용</li> <li>Step 3) Transformer decoder<br/> 두 features의 관계를 학습하여<br/> aligned pointmap 만들기 위해<br/> <code class="language-plaintext highlighter-rouge">self-attention and cross-attention</code> 수행</li> <li>Step 4) Head output<br/> per-pixel <code class="language-plaintext highlighter-rouge">Pointmap</code> \(X_{i}^{v, 1} \in R^{W \times H \times 3}\)<br/> and<br/> per-pixel <code class="language-plaintext highlighter-rouge">Confidence</code> score \(C_{i}^{v, 1} \in R^{W \times H}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/3-480.webp 480w,/assets/img/2024-11-19-DUSt3R/3-800.webp 800w,/assets/img/2024-11-19-DUSt3R/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>1번 camera : base view, 2번 camera : reference view<br/> \(G_{i}^{1}\) : 1번 view feature의 Transformer Decoder에서 \(i\)-th Block<br/> \(G_{i}^{2}\) : 2번 view feature의 Transformer Decoder에서 \(i\)-th Block</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/4-480.webp 480w,/assets/img/2024-11-19-DUSt3R/4-800.webp 800w,/assets/img/2024-11-19-DUSt3R/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Pointmap :<br/> \(X^{1, 1}\) : 1번 view-point를 중심좌표계로 두고 1번 view에서 보이는 3D point 좌표<br/> \(X^{2, 1}\) : 1번 view-point를 중심좌표계로 두고 2번 view에서 보이는 3D point 좌표</p> </li> <li>Confidence score :<br/> \(C_{i}^{v, 1}\) : 1번 view 시점을 기준으로 \(v\) 번 view에서 보이는 \(i\)-th 3D point의 confidence score <ul> <li>물체인 부분에서는 3D point를 비교적 정확히 예측할 수 있으므로 confidence가 높고,<br/> 하늘 또는 반투명인 부분에서는 3D point를 정확하게 예측할 수 없으므로 confidence가 낮게 나옴</li> <li>\(C_{i}^{v, 1} = 1 + \text{exp}(\tilde C_{i}^{v, 1}) \gt 1\) 로 설정하여<br/> 하나의 view에만 존재해서 추정하기 어려운 3D point의 경우에는 extrapolate할 수 있도록 <code class="language-plaintext highlighter-rouge">???</code></li> </ul> </li> <li>1번 view를 기준으로 2번 view의 3D points를 <code class="language-plaintext highlighter-rouge">상대적으로 align</code>하므로<br/> 3D point의 <code class="language-plaintext highlighter-rouge">절대적인 위치를 추정하는 게 아니므로</code><br/> intrinsic/extrinsic <code class="language-plaintext highlighter-rouge">camera param. 몰라도</code> ok</li> </ul> <h3 id="loss">Loss</h3> <ul> <li>regression loss :<br/> \(L_{regr} (v, i) = \| \frac{1}{z} X_{i}^{v, 1} - \frac{1}{\bar z} \bar X_{i}^{v, 1} \|\) <ul> <li>\(i\) : each point, \(v\) : each view</li> <li>\(z = \text{norm}(X^{1, 1}, X^{2, 1})\) : averaged depth of prediction point</li> <li>\(\bar z = \text{norm}(\bar X^{1, 1}, \bar X^{2, 1})\) : averaged depth of GT point</li> <li> <table> <tbody> <tr> <td>$$\text{norm}(X^{1, 1}, X^{2, 1}) = \frac{1}{</td> <td>D^{1}</td> <td>+</td> <td>D^{2}</td> <td>} \sum_{v \in { 1, 2 }} \sum_{i \in D^{v}} | X_{i}^{v, 1} |$$ : 모든 depth 값에 대한 평균</td> </tr> </tbody> </table> </li> </ul> </li> <li>final loss :<br/> \(L_{conf} = \sum_{v \in \{ 1, 2 \}} \sum_{i \in D^{v}} C_{i}^{v, 1} L_{regr}(v, i) - \alpha \text{log} C_{i}^{v, 1}\) <ul> <li>\(C_{i}^{v, 1} L_{regr}(v, i)\) :<br/> confidence가 큰 <code class="language-plaintext highlighter-rouge">(확실한) point</code>에서는 GT와의 <code class="language-plaintext highlighter-rouge">regression loss</code> \(L_{regr}\) 가 더 <code class="language-plaintext highlighter-rouge">작도록</code></li> <li>\(- \alpha \text{log} C_{i}^{v, 1}\) : regularization term<br/> <code class="language-plaintext highlighter-rouge">confidence</code> \(C_{i}^{v, 1}\) 값이 <code class="language-plaintext highlighter-rouge">너무 작아지지 않도록</code></li> </ul> </li> </ul> <h3 id="experiments">Experiments</h3> <ul> <li>Model<br/> CroCo pre-trained model 사용하여 weight initialization <ul> <li>encoder : ViT-Large</li> <li>decoder : ViT-Base</li> <li>head : DPT (ViT를 Depth Estimation에 적용한 연구)</li> </ul> </li> </ul> <h3 id="downstream---stereo-pixel-matching">Downstream - stereo pixel matching</h3> <ul> <li>2개의 image에 대한 3D pointmap을 겹쳤을 때 align되도록<br/> <code class="language-plaintext highlighter-rouge">3D 공간 상에서 pixel correspondence</code>를 찾음 <ul> <li>\(X^{2, k}\) 중에 3D point \(X_{i}^{1, k}\) 와 가장 가까운 3D point가 \(X_{j}^{2, k}\) 이고,<br/> 동시에 \(X^{1, k}\) 중에 3D point \(X_{j}^{2, k}\) 와 가장 가까운 3D point가 \(X_{i}^{1, k}\) 일 때<br/> 두 pixel \(i, j\) 사이에 correspondence 있다고 함</li> <li>모든 pixel에 대해 correspondence가 생기지는 않음</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/5-480.webp 480w,/assets/img/2024-11-19-DUSt3R/5-800.webp 800w,/assets/img/2024-11-19-DUSt3R/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="downstream---camera-intrinsic-estimation">Downstream - camera intrinsic estimation</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/6-480.webp 480w,/assets/img/2024-11-19-DUSt3R/6-800.webp 800w,/assets/img/2024-11-19-DUSt3R/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>camera intrinsic :<br/> camera intrinsic을 추정한다는 것은<br/> 보통 sclaing matrix, 즉 focal length를 추정한다는 얘기임 <ul> <li>2D translation : principal point의 위치<br/> (보통 이미지의 정가운데)</li> <li>2D shear : 카메라가 기울어진 정도<br/> (보통 카메라는 기울어져 있지 않으므로 shear matrix는 고려 X)</li> <li>2D scaling : focal length</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/7-480.webp 480w,/assets/img/2024-11-19-DUSt3R/7-800.webp 800w,/assets/img/2024-11-19-DUSt3R/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>위의 수식 설명 : <ul> <li>focal length는 3D point를 2D image plane으로 projection시킬 때 사용됨<br/> Weiszfeld algorithm을 이용해서 2D 상에서 위의 반복 최적화 문제를 풀면<br/> 해당 optimal <code class="language-plaintext highlighter-rouge">focal length</code>를 가질 때 <code class="language-plaintext highlighter-rouge">2D image와 3D point가 align</code>됨</li> <li>camera-coordinate에서 최적화 수행<br/> where pixel-coordinate : 2D \((i, j) \in ([0, W], [0, H])\) (좌상단이 원점)<br/> where camera-coordinate : 2D $$i^{‘}, j^{‘} \in ([-\frac{W}{2}, \frac{W}{2}], [-\frac{H}{2}, \frac{H}{2}]) (정중앙이 원점)<br/> where world-coordinate : 3D</li> </ul> </li> </ul> <h3 id="downstream---camera-extrinsic-estimation">Downstream - camera extrinsic estimation</h3> <ul> <li>Relative Pose Estimation : <ul> <li>방법 1)<br/> 위에서 언급한 2D pixel matching과 intrinsic estimation을 수행한 뒤<br/> Eight-Point algorithm 등 이용해서<br/> epipolar(essential) matrix와 relative pose를 추정</li> <li>방법 2)<br/> 서로 다른 view 시점에서 보이는 3D pointmap이 동일해지도록<br/> SVD-based procrustes alignment algorithm 이용해 3D 상에서 반복 최적화 문제를 풀어서<br/> optimal rotation matrix \(R\), translation vector \(t\), scale factor \(\sigma\) 추정 <ul> <li>procrustes alignment algorithm은 noise 및 outlier에 민감하므로<br/> 주어진 3D point와 corresponding 2D point를 바탕으로 camera pose를 추정하는 PnP algorithm과<br/> random sampling 방식의 RANSAC (Random Sample Consensus) algorithm 이용해서 위 수식의 해를 찾음<br/> <code class="language-plaintext highlighter-rouge">?????</code></li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/8-480.webp 480w,/assets/img/2024-11-19-DUSt3R/8-800.webp 800w,/assets/img/2024-11-19-DUSt3R/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Absolute Pose Estimation (visual localization) : <ul> <li>방법 1)<br/> 위에서 언급한 2D pixel matching과 instrinsic estimation을 수행한 뒤<br/> PnP RANSAC algorithm 이용해서 optimal rotation matrix 및 translation vector 추정</li> <li>방법 2)<br/> GT pointmap을 이용<br/> 즉, 위에서 언급한 Relative Pose Estimation을 수행할 때<br/> 해당 GT로 scale을 맞춰서 진행</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/9-480.webp 480w,/assets/img/2024-11-19-DUSt3R/9-800.webp 800w,/assets/img/2024-11-19-DUSt3R/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Experiment on Absolute Pose Estimation : <ul> <li>test dataset :<br/> 7Scenes, Cambridge Landmark<br/> (training에 사용되지 않은 dataset)</li> <li>각 값은 translation error (cm) / rotation error (degree)</li> <li>방식 :<br/> query image가 주어지면<br/> 가장 관련 있는 image를 test dataset에서 찾아<br/> 2개 image 간의 pixel을 matching하여 Absolute Camera Pose 계산<br/> (근데 query image와 test dataset image 간의 GT camera pose가 존재하나 <code class="language-plaintext highlighter-rouge">???</code>)</li> <li>비교 :<br/> FM(feature matching 기법), E2E(end-to-end learning 기법)과 비교했을 때<br/> SOTA 성능은 아니지만<br/> DUSt3R이 visual localization을 목적으로 학습되지 않았는데도 오차가 작다는 것을 확인할 수 있음</li> </ul> </li> </ul> <h3 id="downstream---global-alignment">Downstream - Global Alignment</h3> <ul> <li>Global Alignment :<br/> 3장 이상의 images로부터 예측한 Pointmap을 3D space에서 align하는 방법 <ul> <li>여러 장의 images를 다루기 위해<br/> <code class="language-plaintext highlighter-rouge">Graph</code> 만듦 (각 image가 vertex이고, 같은 visual contents를 공유하고 있으면 edge)</li> <li>DUSt3R 이용해서<br/> 모든 edge pair에 대해 Pointmap \(X_{i}^{v, 1} \in R^{W \times H \times 3}\) 과 Confidence score \(C_{i}^{v, 1} \in R^{W \times H}\) 계산</li> <li>여러 장의 images로 3D 상에서 반복 최적화 문제 풀어서<br/> 여러 장의 images로부터 얻은 Pointmap들이 3D 상에서 align되도록 함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/10-480.webp 480w,/assets/img/2024-11-19-DUSt3R/10-800.webp 800w,/assets/img/2024-11-19-DUSt3R/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>위의 수식 설명 :<br/> 3D 상에서 위의 반복 최적화 문제를 풀어서<br/> optimal \(\xi_{i}^{v}, \sigma_{e}, P_{e}\) 얻으면<br/> \(N\) 개의 images로 얻은 \(N\) 개의 <code class="language-plaintext highlighter-rouge">3D Pointmap을 align</code>하여<br/> Global pointmap을 얻을 수 있음<br/> 코드 : <a href="https://github.com/naver/dust3r/blob/01b2f1d1e6c6c386f95a007406defb5b8a5d2042/dust3r/cloud_opt/optimizer.py">Code</a> <ul> <li>\(C_{i}^{v, e}\) : confidence score from DUSt3R prediction<br/> (image \(e\) 의 view 시점을 기준으로, image \(v\) view에서 보이는 \(i\)-th pixel에 대응되는 값)</li> <li>\(X_{i}^{v, e}\) : pointmap from DUSt3R prediction</li> <li>\(\xi_{i}^{v}\) : global pointmap in world-coordinate</li> <li>\(\sigma_{e}\) : edge로 연결되어 있는 2개 images 간의 scale factor<br/> (0이 되는 것을 방지하기 위해 \(\prod_{e} \sigma_{e} = 1\) 로 설계)</li> <li>\(P_{e}\) : edge로 연결되어 있는 2개 images 간의 relative pose</li> </ul> </li> <li>위의 방법은<br/> <code class="language-plaintext highlighter-rouge">전통적인 SfM bundle adjustment 방법과 달리</code><br/> <code class="language-plaintext highlighter-rouge">빠르고 단순하게 regression(gradient descent)-based</code>로 반복 최적화 문제 풂 <ul> <li>bundle adjustment :<br/> 2D reprojection error 최소화</li> <li>본 논문 :<br/> 2D reprojection 뿐만 아니라 3D projection error을 같이 최소화</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/13-480.webp 480w,/assets/img/2024-11-19-DUSt3R/13-800.webp 800w,/assets/img/2024-11-19-DUSt3R/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="downstream---depth-estimation">Downstream - Depth Estimation</h3> <ul> <li>Monocular Depth Estimation</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/11-480.webp 480w,/assets/img/2024-11-19-DUSt3R/11-800.webp 800w,/assets/img/2024-11-19-DUSt3R/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Multi-view Depth Estimation</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/12-480.webp 480w,/assets/img/2024-11-19-DUSt3R/12-800.webp 800w,/assets/img/2024-11-19-DUSt3R/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="downstream---dense-3d-reconstruction">Downstream - Dense 3D reconstruction</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/14-480.webp 480w,/assets/img/2024-11-19-DUSt3R/14-800.webp 800w,/assets/img/2024-11-19-DUSt3R/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="limitation">Limitation</h3> <ul> <li>각 downstream task에서 SOTA 급은 아니지만<br/> 그래도 pose 없이 높은 성능 이뤘다는 데에 의미가 있음</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="point"/><category term="regression"/><category term="pose"/><category term="free"/><summary type="html"><![CDATA[Geometric 3D Vision Made Easy (CVPR 2024)]]></summary></entry><entry><title type="html">Deblurring 3D Gaussian Splatting</title><link href="https://semyeong-yu.github.io/blog/2024/Deblurring3DGS/" rel="alternate" type="text/html" title="Deblurring 3D Gaussian Splatting"/><published>2024-10-30T12:00:00+00:00</published><updated>2024-10-30T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Deblurring3DGS</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Deblurring3DGS/"><![CDATA[<h2 id="deblurring-3d-gaussian-splatting-eccv-2024">Deblurring 3D Gaussian Splatting (ECCV 2024)</h2> <h4 id="byeonghyeon-lee-howoong-lee-xiangyu-sun-usman-ali-eunbyung-park">Byeonghyeon Lee, Howoong Lee, Xiangyu Sun, Usman Ali, Eunbyung Park</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2401.00834">https://arxiv.org/abs/2401.00834</a><br/> project website :<br/> <a href="https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/">https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/</a><br/> code :<br/> <a href="https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting">https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting</a></p> </blockquote> <blockquote> <p>핵심 :</p> <ol> <li>defocus blur 구현 :<br/> MLP로 covariance(rotation, scaling)의 변화량을 모델링해서<br/> covariance를 키워서<br/> defocus-blurred image 얻음</li> <li>camera motion blur 구현 :<br/> MLP로 position 및 covariance의 변화량을 모델링해서<br/> M개의 3DGS sets를 만든 뒤<br/> 이걸로 만든 M개의 sharp imgs를 average해서<br/> camera-motion-blurred image 얻음</li> <li>위의 MLP를 training에서만 사용하므로<br/> still real-time rendering at inference</li> <li>sparse point clouds 보상하기 위해 points 추가<br/> 또한 먼 거리에 있는 3DGS는 덜 prune out</li> </ol> </blockquote> <h3 id="introduction">Introduction</h3> <ul> <li>3DGS : <ul> <li>novel-view로 inference할 때<br/> NeRF는 새로운 각도를 MLP에 넣어야만 color, opacity 얻을 수 있지만<br/> 3DGS는 spherical harmonics, explicit 기법이라 새로운 각도에 대해서도 바로 color, opacity 얻을 수 있어서<br/> volume rendering이 빠름</li> <li>differentiable splatting-based rasterization with parallelism</li> </ul> </li> <li>본 논문 : <ul> <li>핵심 : <ul> <li>각 3DGS의 <code class="language-plaintext highlighter-rouge">covariance</code>를 수정하여 <code class="language-plaintext highlighter-rouge">blur(adjacent pixels의 혼합)를 모델링하는 작은 MLP</code> 사용</li> <li>training 시에는 MLP output 곱해서 blurry image를 생성하고<br/> inference 시에는 MLP 사용하지 않아서 real-time으로 sharp image 생성</li> </ul> </li> <li>문제 : <ul> <li>3DGS는 initial point cloud에 많이 의존하는데<br/> given images가 <code class="language-plaintext highlighter-rouge">blurry</code>하면 SfM은 유효한 feature를 식별하지 못해서 <code class="language-plaintext highlighter-rouge">매우 적은 수의 point</code> cloud를 추출함</li> <li>심지어 depth가 크면 SfM은 맨 끝에 있는 점을 거의 추출하지 않음</li> </ul> </li> <li>해결 : <ul> <li>sparse point cloud를 방지하고자<br/> <code class="language-plaintext highlighter-rouge">N-nearest-neighbor interpolation으로 points 추가</code></li> <li>먼 거리의 평면에 많은 Gaussian을 유지하기 위해<br/> <code class="language-plaintext highlighter-rouge">위치에 따라 Gaussian pruning</code></li> </ul> </li> <li>contribution :<br/> SOTA qualtiy인데 훨씬 빠른 rendering speed (\(\gt 200\) FPS)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/1-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/1-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Overall Architecture </div> <h3 id="related-works">Related Works</h3> <ul> <li>Image Deblurring : <ul> <li>\(g(x) = \sum_{s \in S_{h}} h(x, s) f(x) + n(x)\)<br/> where \(g(x)\) : blurry image and \(f(x)\) : latent sharp image<br/> where \(h(x, s)\) : blur kernel or PSF (Point Spread Function)<br/> where \(n(x)\) : additive white Gaussian noise (occurs in nature images)</li> <li>지금까지 2D image deblurring은 많이 연구되어 왔는데<br/> 3D scene deblurring은 3D view consistency 부족 때문에 연구하기 어려웠음</li> </ul> </li> <li>Fast NeRF : <ul> <li>방법 1)<br/> use additional data-structure to reduce the size and number of MLP layers<br/> but, fail to reach real-time view synthesis <ul> <li>grid-based :<br/> Hexplane, TensoRF, K-planes, Mip-grid, Masked wavelet representation, Direct voxel grid optimization, F2-nerf</li> <li>hash-based :<br/> InstantNGP, Zip-nerf</li> </ul> </li> <li>방법 2)<br/> trained param.을 faster representation으로 bake해서 real-time rendering <ul> <li>Baking neural radiance fields, Merf, Bakedsdf</li> </ul> </li> </ul> </li> <li>Deblurring NeRF :<br/> 자세한 건 <a href="https://semyeong-yu.github.io/blog/2024/DeblurNeRF/">Link</a> 참조 <ul> <li>DoF-NeRF <d-cite key="DofNeRF">[1]</d-cite> : <ul> <li>단점 :<br/> train하기 위해 all-in-focus image와 blurry image 모두 필요<br/> (all-in-focus image : 화면 전체가 초점이 맞춰져 있는 image)</li> </ul> </li> <li>Deblur-NeRF <d-cite key="DeblurNeRF">[2]</d-cite> : <ul> <li>장점 :<br/> train할 때 all-in-focus image 필요 없음</li> <li>핵심 :<br/> additional small MLP 사용해서<br/> per-pixel blur kernel 예측</li> </ul> </li> <li>DP-NeRF <d-cite key="DpNeRF">[3]</d-cite> and PDRF <d-cite key="PDRF">[4]</d-cite> : <ul> <li>Deblur-NeRF 발전시킴</li> </ul> </li> <li>Hybrid <d-cite key="Hybrid">[5]</d-cite> and Sharp-NeRF <d-cite key="SharpNeRF">[6]</d-cite> and BAD-NeRF <d-cite key="BADNeRF">[7]</d-cite> : <ul> <li>camera motion blur와 defocus blur 중 하나만 다룸</li> </ul> </li> </ul> </li> <li>Deblurring NeRF 요약 : <ul> <li>deblur task 잘 수행하지만<br/> NeRF 자체가 rendering time이 오래 걸림<br/> \(\rightarrow\)<br/> real-time differentiable rasterizer 이용하는<br/> 3DGS로 deblur task 수행하자!</li> </ul> </li> </ul> <h3 id="background">Background</h3> <ul> <li> <p>3DGS <a href="https://semyeong-yu.github.io/blog/2024/GS/">Link</a> 참고</p> </li> <li> <p>Blur :</p> <ul> <li>Defocus Blur :<br/> 렌즈의 <code class="language-plaintext highlighter-rouge">초점이 맞지 않아서</code> 흐려진 경우<br/> e.g. 인물 사진에서 인물만 초점이 맞고 배경은 흐릿한 경우</li> <li>Camera Motion Blur :<br/> 셔터가 열려 있는 동안 카메라가 움직이거나 피사체가 <code class="language-plaintext highlighter-rouge">움직여서</code> 흐려진 경우<br/> e.g. 달리는 자동차를 촬영한 경우</li> </ul> </li> </ul> <h3 id="defocus-blur">Defocus Blur</h3> <ul> <li>Motivation : <ul> <li>Defocus Blur는 일반적으로<br/> 실제 image와 PSF(point spread func.)(2D Gaussian function) 간의<br/> convolution으로 모델링<br/> 즉, a pixel이 주위 pixels에 영향을 미칠 경우 blur</li> <li>여기서 영감을 받아<br/> <code class="language-plaintext highlighter-rouge">covariance(크기)가 큰 3DGS는 Blur</code>를 유발하고<br/> <code class="language-plaintext highlighter-rouge">covariance(크기)가 작은 3DGS는 Sharp</code> image에 기여한다고 가정<br/> (covariance(dispersion)가 클수록 Gaussian이 더 많은 pixels에 걸쳐 있으니까<br/> 더 많은 이웃한 pixels 간의 interference 표현 가능)</li> <li>그렇다면 covariance \(\Sigma = R S S^{T} R^{T}\) 를 변경하여 Blur를 모델링해야겠다!</li> </ul> </li> <li>Defocus Blur를 모델링하는 MLP :<br/> \((\delta r_{j}, \delta s_{j}) = F_{\theta}(\gamma(x_{j}), r_{j}, s_{j}, \gamma(v))\)<br/> where input : \(j\)-th Gaussian’s position, rotation, scale, view-direction<br/> where output : \(j\)-th Gaussian’s rotation change, scale change<br/> (\(\gamma\) : positional encoding) <ul> <li>transformed 3DGS : <ul> <li>rotation quaternion : \(\hat r_{j} = r_{j} \cdot \text{min}(1.0, \lambda_{s} \delta r_{j} + (1 - \lambda_{s}))\)</li> <li>scaling : \(\hat s_{j} = s_{j} \cdot \text{min}(1.0, \lambda_{s} \delta s_{j} + (1 - \lambda_{s}))\) <ul> <li>\(\cdot\) : element-wise multiplication</li> <li>\(\lambda_{s}\) 로 scale하고 \((1 - \lambda_{s})\) 로 shift : for optimization stability <code class="language-plaintext highlighter-rouge">???</code></li> <li>rotation 및 scaling 변화량의 <code class="language-plaintext highlighter-rouge">최솟값을 1로 clip</code> :<br/> \(\hat s_{j} \geq s_{j}\) 이므로 transformed 3DGS는 <code class="language-plaintext highlighter-rouge">더 큰 covariance</code>를 가져서<br/> <code class="language-plaintext highlighter-rouge">Defocus Blur</code>의 근본 원인인 주변 정보의 interference을 모델링할 수 있게 됨</li> </ul> </li> </ul> </li> <li>inference :<br/> scaling factor로 covariance 변화시키는 게 blur kernel의 역할을 하므로<br/> <code class="language-plaintext highlighter-rouge">training</code> 시에는 <code class="language-plaintext highlighter-rouge">transformed 3DGS</code>가 <code class="language-plaintext highlighter-rouge">blurry</code> image를 생성하지만<br/> <code class="language-plaintext highlighter-rouge">inference</code> 시에는 MLP를 사용하지 않은 <code class="language-plaintext highlighter-rouge">기존 3DGS</code>가 <code class="language-plaintext highlighter-rouge">sharp</code> image를 생성<br/> \(\rightarrow\)<br/> training할 때는 MLP forwarding과 간단한 element-wise multiplication만 추가 비용이고,<br/> inference할 때는 MLP를 사용하지 않아 Vanilla-3DGS와 모든 단계가 동일하므로<br/> <code class="language-plaintext highlighter-rouge">추가 비용 없이 real-time rendering</code> 가능</li> </ul> </li> </ul> <h3 id="selective-blurring">Selective Blurring</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/2-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/2-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>초점에 의한 Defocus Blur는 <code class="language-plaintext highlighter-rouge">영역마다 흐린 수준이 다름</code><br/> 본 논문에서는 <code class="language-plaintext highlighter-rouge">각 3DGS마다</code> 다르게 \(\delta_{r}, \delta_{s}\) 를 추정하므로<br/> Gaussian의 covariance를 선택적으로 확대시킬 수 있어서<br/> 영역에 따라 다르게 blurring 할 수 있으므로<br/> <code class="language-plaintext highlighter-rouge">pixel 단위의 blurring</code>을 보다 유연하게 모델링 가능 <ul> <li>defocus blur가 심한 영역에 있는 3DGS는 \(\delta_{s}\) 가 더 크도록</li> <li>당연히 단일 유형의 Gaussian Blur kernel을 써서 평균 blurring을 모델링하는 것보다<br/> 본 논문에서처럼 3DGS마다 다른 Blur kernel을 적용하여 pixel 단위 blurring을 모델링하는 게 더 좋음!</li> </ul> </li> </ul> <h3 id="camera-motion-blur">Camera motion Blur</h3> <ul> <li> <p>셔터가 열려 있는 exposure time 동안<br/> camera movement가 있으면<br/> light intensities from multipe sources가 inter-mixed되어<br/> Camera motion Blur 발생</p> </li> <li> <p>Camera motion Blur를 모델링하는 MLP :<br/> \({(\delta x_{j}^{(i)}, \delta r_{j}^{(i)}, \delta s_{j}^{(i)})}_{i=1}^{M} = F_{\theta}(\gamma(x_{j}), r_{j}, s_{j}, \gamma(v))\)</p> <ul> <li>transformed 3DGS : <ul> <li>3D position : \(\hat x_{j}^{(i)} = x_{j} + \lambda_{p} \delta x_{j}^{(i)}\) (shift)</li> <li>rotation quaternion : \(\hat r_{j}^{(i)} = r_{j} \cdot \delta r_{j}^{(i)}\) (element-wise multiplication)</li> <li>scaling : \(\hat s_{j}^{(i)} = s_{j} \cdot \delta s_{j}^{(i)}\) (element-wise multiplication) <ul> <li>Camera motion Blur의 경우<br/> Defocus Blur와 달리 covariance를 무조건 키워야 되는 게 아니므로<br/> min-clip by 1.0 없음</li> </ul> </li> </ul> </li> <li>Camera motion Blur :<br/> \(I_{b} = \frac{1}{M} \sum_{i=1}^{M} I_{i}\) <ul> <li>셔터가 열려 있는 동안 카메라가 움직이는 각 discrete moment는<br/> 각 3DGS set에 대응됨</li> <li>\(j\)-th Gaussian 의 <code class="language-plaintext highlighter-rouge">camera movement</code>를 나타내기 위해<br/> <code class="language-plaintext highlighter-rouge">M개의 auxiliary 3DGS sets</code> 만들어서<br/> <code class="language-plaintext highlighter-rouge">M개의 clean images</code> rendering해서<br/> <code class="language-plaintext highlighter-rouge">average</code>해서 camera-motion-blurred image 얻음</li> </ul> </li> <li>inference :<br/> 마찬가지로 <code class="language-plaintext highlighter-rouge">inference</code> 시에는 MLP를 사용하지 않은 <code class="language-plaintext highlighter-rouge">기존 3DGS</code>가 clean image를 생성<br/> \(\rightarrow\)<br/> inference할 때는 MLP로 \(M\)-개의 3DGS sets 만들지 않고<br/> Vanilla-3DGS와 모든 단계가 동일하므로<br/> <code class="language-plaintext highlighter-rouge">추가 비용 없이 real-time rendering</code> 가능</li> </ul> </li> </ul> <h3 id="compensation-for-sparse-point-cloud">Compensation for Sparse Point Cloud</h3> <ul> <li> <p>문제 1)<br/> 3DGS는 initial point cloud에 많이 의존하는데<br/> given input multi-view images가 <code class="language-plaintext highlighter-rouge">blurry</code>하면<br/> SfM은 유효한 feature를 식별하지 못해서<br/> 매우 적은 수의 <code class="language-plaintext highlighter-rouge">sparse point cloud</code>를 추출함</p> </li> <li> <p>해결 :</p> <ul> <li>sparse point cloud를 방지하고자<br/> \(N_{st}\) iter. 후에 \(N_{p}\)-개의 points를 uniform \(U(\alpha, \beta)\) 에서 sampling하여 추가<br/> where \(\alpha\) : 기존 point cloud 위치의 최솟값<br/> where \(\beta\) : 기존 point cloud 위치의 최댓값</li> <li>새로운 point의 <code class="language-plaintext highlighter-rouge">색상은 KNN(K-Nearest-Neighbor) interpolation</code>으로 할당</li> <li>새로운 points를 uniform 분포에서 sampling해서 <code class="language-plaintext highlighter-rouge">빈 공간에 불필요한 points</code>가 생길 수 있으므로<br/> nearest neighbor까지의 거리가 threshold \(t_{d}\) 를 초과하는 points는 <code class="language-plaintext highlighter-rouge">폐기</code></li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/3-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/3-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/12-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/12-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 가운데는 without adding points, 오른쪽은 with adding extra points </div> <ul> <li> <p>문제 2)<br/> 심지어 depth of field가 크면<br/> SfM은 맨 끝에 있는 점을 거의 추출하지 않음</p> </li> <li> <p>해결 :<br/> Deblur-NeRF dataset은 forward-facing scene으로만 구성되어 있으므로<br/> dataset에 기록된 <code class="language-plaintext highlighter-rouge">z-axis 값</code>은 <code class="language-plaintext highlighter-rouge">relative depth</code> from any viewpoint라고 볼 수 있음</p> <ul> <li>방법 1) 먼 거리에 있는 3DGS 수 늘리기<br/> 먼 거리의 평면에 있는 3DGS에 대해 denisfy<br/> \(\rightarrow\)<br/> 과도한 densification은 Blur 모델링을 방해하고 추가 계산 비용 필요</li> <li>방법 2) <code class="language-plaintext highlighter-rouge">먼 거리에 있는 3DGS는 덜 prune out</code><br/> pruning threshold를 깊이에 따라 다르게 scaling<br/> as \(t_{p}, 0.9 t_{p}, \cdots , \frac{1}{w_{p}} t_{p}\)<br/> (먼 거리의 3DGS일수록 낮은 threshold) <br/> \(\rightarrow\)<br/> real-time rendering을 고려했을 때<br/> 유연한 pruning으로도 먼 거리의 3DGS sparsity를 보상하기에 충분하다는 걸 경험적으로 발견</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/4-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/4-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="experiment">Experiment</h3> <ul> <li>Setting : <ul> <li>dataset : Deblur-NeRF dataset <ul> <li>have both synthetic and real images</li> <li>has camera motion blur or defocus blur</li> </ul> </li> <li>GPU : NVIDIA RTX 4090 GPU (24GB)</li> <li>optimzier : Adam</li> <li>iter. : \(20,000\)</li> <li>Blur를 모델링하는 small MLP : <ul> <li>lr : \(1e^{-3}\)</li> <li>hidden layer : 4 <ul> <li>3 layers : shared</li> <li>1 layer : head for each \(\delta\)</li> </ul> </li> <li>hidden unit : 64</li> <li>activation : ReLU</li> <li>initialization : Xavier</li> <li>scaling factor for \(\delta\) : \(\lambda_{s}, \lambda_{p} = 1 e^{-2}\)</li> </ul> </li> <li>sparse point cloud를 보상하기 위해 <ul> <li>\(N_{st} = 2,500\) iter. 후에 \(N_{p}\) 개의 point 추가<br/> \(N_{p}\) 는 기존 point cloud 규모에 비례하며 최대 200,000개</li> <li>색상은 \(K = 4\) 의 KNN interpolation으로 할당</li> <li>nearest neighbor까지의 거리가 \(t_{d} = 2\) 을 초과하는 point는 폐기</li> </ul> </li> <li>먼 거리에 있는 3DGS는 덜 pruning하기 위해<br/> pruning threshold를 깊이에 따라 다르게 scaling <ul> <li>pruning threshold \(t_{p} = 5 e^{-3}\) and densification threshold \(2 e^{-4}\)<br/> for real defocus blur dataset</li> <li>pruning threshold \(t_{p} = 1 e^{-2}\) and densification threshold \(5 e^{-4}\)<br/> for real camera motion blur dataset</li> <li>pruning threshold multiplier \(w_{p} = 3\)</li> </ul> </li> <li>camera motion blur를 구현하기 위해<br/> \(M = 5\) 개의 3DGS sets 만들어서<br/> \(M = 5\) 개의 clean images를 average</li> </ul> </li> </ul> <h3 id="results">Results</h3> <ul> <li>Results : <ul> <li><code class="language-plaintext highlighter-rouge">SOTA deblurring NeRF</code>만큼 <code class="language-plaintext highlighter-rouge">PSNR</code> 높음</li> <li><code class="language-plaintext highlighter-rouge">3DGS</code>만큼 <code class="language-plaintext highlighter-rouge">FPS</code> 높음</li> <li>비교 대상으로 쓰인 논문들 : <ul> <li>Deblur-NeRF, Sharp-NeRF, DP-NeRF, PDRF</li> <li>original 3DGS</li> <li>Restormer로 input training images 먼저 deblur한 뒤 original 3DGS</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/5-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/5-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/6-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/6-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> real-world Defocus Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/7-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/7-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> real-world Defocus Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/8-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/8-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> synthesized Defocus Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/9-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/9-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> synthesized Defocus Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/13-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/13-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> real-world Camera motion Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/14-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/14-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> real-world Camera motion Blur Dataset </div> <h3 id="ablation-study">Ablation Study</h3> <ul> <li>Ablation Study : <ul> <li>Extra points allocation</li> <li>Depth-based pruning</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/10-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/10-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Extra points allocation </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/11-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/11-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Depth-based pruning </div> <h3 id="limitation-and-future-work">Limitation and Future Work</h3> <ul> <li>Limitation : <ul> <li>volumetric rendering 기반의 NeRF-based deblurring 기법들을<br/> rasterization 기반의 3DGS에 적용하기 어렵<br/> \(\rightarrow\)<br/> MLP로 <code class="language-plaintext highlighter-rouge">world-space</code>에서의 rays 또는 kernels를 변형하는 대신<br/> MLP로 <code class="language-plaintext highlighter-rouge">rasterized image space</code>에서의 kernels를 변형하면<br/> Deblurring 3DGS 구현 가능<br/> \(\rightarrow\)<br/> 하지만 kernel interpolation 방향으로 가면<br/> pixel interpolation은 추가 비용이 발생하며<br/> 3DGS의 geometry를 implicitly 변형하는 것일 뿐이므로<br/> 해당 방법은 3DGS로 blur를 모델링하는 최적의 방법이 아닐 것이다<br/> 이를 개선하기 위한 future works 필요</li> </ul> </li> </ul> <h3 id="code-review">Code Review</h3> <ul> <li>blur kernel 함수 :<br/> Defocus Blur 및 Camera motion Blur <ul> <li>정의 : <a href="https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/scene/blur_kernel.py#L74">https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/scene/blur_kernel.py#L74</a></li> <li>호출 : <a href="https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/gaussian_renderer/__init__.py#L101">https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/gaussian_renderer/<strong>init</strong>.py#L101</a></li> </ul> </li> <li>sparse point cloud 보상하기 위해 add points : <ul> <li><a href="https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/scene/gaussian_model.py#L444">https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/scene/gaussian_model.py#L444</a></li> </ul> </li> </ul> <h3 id="question">Question</h3> <ul> <li> <p>Q1 :<br/> small MLP는 어떤 architecture로 구성되어 있나요?<br/> Dust3R 기반의 논문들을 보면<br/> transformer 등 pre-trained complex model 가져와서 쓰는데<br/> feed-forward 방식으로 학습하므로<br/> 빠르면서도 성능이 좋습니다. 이를 적용할 수 있지 않을까요?</p> </li> <li> <p>A1 :<br/> 일단 본 논문에서는 fast training 유지하기 위해 shallow MLP가 simply fc layers로 구성되어 있습니다<br/> 말씀해주신대로 simple shallow MLP 대신 더 좋은 network 쓰면 성능이 더 좋아질 것 같다고 생각하는데,<br/> deblurring task를 다룬 본 논문 이후의 논문들을 아직 읽어보지 않아서<br/> 혹시 읽어보고 좋은 아이디어 있다면 공유하도록 하겠습니다.</p> </li> <li> <p>Q2 :<br/> 본 논문이 deblurring task를 위해 pre-trained 3DGS를 가져와서 fine-tuning하는 것인가요?</p> </li> <li> <p>A2 :<br/> 아닙니다. 기존 3DGS에 blur를 모델링하는 MLP만 추가해서 scratch부터 training하고,<br/> 이로써 input image가 더러워도(blurry하더라도) clean image를 rendering할 수 있게 됩니다.<br/> 그리고 기존 3DGS와 같이 per-scene 방식으로 학습하는 것으로 알고 있습니다.</p> </li> <li> <p>Q3 :<br/> blurry input image를 dataset에서 미리 빼버리면 deblurring을 해야 하는 상황이 없어지잖아요<br/> 이처럼 제가 생각하기에는 굳이 deblurring을 해야 하나 라는 생각이 듭니다.</p> </li> <li> <p>A3 :<br/> 일단 deblurring이라는 게 super-resolution처럼 하나의 task로 생각할 수 있습니다<br/> input image가 blurry 할 수 있는데 말씀해주신대로 이를 dataset에서 미리 뺀다는 것 자체가 manual effort를 필요로 합니다 (이를 model이 대신 해준다면 좋겠죠)<br/> 그리고 만약 주어진 모델이 deblurring을 수행할 수 있다면 다른 모델의 앞단에 쓰여서 blur를 제거하는 pre-processing 용도로도 쓰일 수 있습니다.<br/> 이로써 input images가 현실에서 있을 법한 더러운(blurry) 이미지더라도 상관 없이 input으로 쓸 수 있습니다.<br/> 2D image 또는 video를 deblurring하는 논문들은 이미 많이 있는데<br/> 3D scene deblurring의 경우에는 3D view consistency 때문에 어려움이 있었습니다.<br/> 그러다가 3DGS 등장 이후로 처음 3DGS deblurring을 시도한 논문이 본 논문이라고 보시면 될 것 같습니다.</p> </li> <li> <p>Q4 :<br/> 그렇다면 deblurring task라는 게 uncertainty를 해결하는 것이라고 볼 수 있을까요? 아니면 이것과는 별개의 task로 봐야 할까요?</p> </li> <li> <p>A4 :<br/> (3D recon. 및 novel view synthesis에서 uncertainty라는 용어가 자주 등장하는데, 관련 논문들을 아직 많이 읽어보지 않아서 확실하게 답변드리지 못하겠습니다.)</p> </li> <li> <p>Q5 :<br/> dataset에 있는 image들이 blurry하지 않고 clean(sharp) 하더라도<br/> camera explore 하면서 novel view에 대해 rendering을 하다보면 rendered image에 blur가 생길 수 있을 것 같은데<br/> deblurring이라는 게 이러한 blur도 제거해주나요?</p> </li> <li> <p>A5 :<br/> 일단 본 논문에서 deblur를 하는 원리는 covariance를 조정하는 MLP로 blur를 모델링하여<br/> 해당 MLP(blur 담당)를 사용하지 않는 inference에서는 deblurred image가 rendering되는 것입니다<br/> 하지만 input이 blurry해서 생긴 blur가 아니라 rendering하다보니 생기는 artifacts로서의 blur의 경우라면<br/> 해당 MLP가 artifacts로서의 blur도 잘 모델링해줄지는 모르겠습니다. 더 찾아봐야 할 것 같습니다.</p> </li> <li> <p>Q6 :<br/> 혹시 본 논문을 읽으면서 생각해보셨던 limitation이 있을까요? 논문에 적혀있는 것 말고 개인적인 생각이 있으신지 궁금합니다.<br/> 저는 뭔가 본 논문의 알고리즘이 artificial하다는 생각이 들었습니다.</p> </li> <li> <p>A6 :<br/> (개인적으로 생각해본 limitation 답변 못 드림 ㅠㅠ 앞으로는 논문 읽을 때 novelty 말고도 limitation이 무엇일지 생각하는 습관 길러보자!)<br/> 기존 deblur nerf에서는 deblur kernel을 이용해서 여러 ray를 쏴서 2D 상에서 pixel들을 interpolate해서 blur를 모델링하는데<br/> deblurring 3DGS에서는 3D 상에서 Gaussian covariance를 키우는 방식으로 interpolate를 비슷하게 구현했다는 논리(가정)이고<br/> 결과적으로 성능이 좋게 나왔으니 본인들 주장(가정)이 맞았다 인 것 같아서 말씀해주신대로 artificial한 느낌이 들긴 하네요</p> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="3DGS"/><category term="deblur"/><summary type="html"><![CDATA[ECCV 2024]]></summary></entry><entry><title type="html">EE534 Pattern Recognition Final</title><link href="https://semyeong-yu.github.io/blog/2024/Pattern2/" rel="alternate" type="text/html" title="EE534 Pattern Recognition Final"/><published>2024-10-28T11:00:00+00:00</published><updated>2024-10-28T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Pattern2</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Pattern2/"><![CDATA[<blockquote> <p>Lecture :<br/> 24F EE534 Pattern Recognition<br/> by KAIST Munchurl Kim <a href="https://www.viclab.kaist.ac.kr/">VICLab</a></p> </blockquote> <h2 id="chapter-6-linear-discriminant-functions">Chapter 6. Linear Discriminant Functions</h2> <h3 id="linearly-non-separable-svm">Linearly Non-Separable SVM</h3> <ul> <li>new constraint :<br/> \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}\)<br/> \(\xi_{i}\) 를 도입하여 이제는 inside margin or misclassified 도 가능하지만 대신 \(C \sum_{i=1}^{N} \xi_{i}\) 를 loss에 넣어서 큰 \(\xi_{i}\) 값을 penalize <ul> <li>\(\xi = 0\) : outside margin or support vector</li> <li>\(0 \lt \xi \leq 1\) : inside margin (correctly classified, but margin violation)</li> <li>\(\xi \gt 1\) : misclassified</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/2.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/2.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>방법 1) 1-norm-soft-margin <ul> <li>constrained primal form :<br/> minimize \(J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \xi_{i}\)<br/> subject to \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}\) and \(\xi_{i} \geq 0\) <ul> <li>unconstrained primal form :<br/> 이 때 위의 두 가지 constraints는 \(\xi_{i} = \text{max}(0, 1 - y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}))\) 로 하나로 합칠 수 있음<br/> 따라서<br/> minimize \(J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \text{max}(0, 1 - y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}))\)</li> <li>regularization param. \(C\) : <ul> <li>small \(C\) : 큰 \(\xi_{i}\) 값도 허용하므로 margin 커짐</li> <li>large \(C\) : 큰 \(\xi_{i}\) 값은 허용 안 하므로 margin 작아짐</li> <li>\(C = \infty\) : non-zero \(\xi_{i}\) 값 허용 안 하므로 hard margin (no sample inside margin)<br/> (Linearly Separable SVM 에 해당함)</li> </ul> </li> </ul> </li> <li>Lagrangian :<br/> minimize \(L(\boldsymbol w, w_{0}, \xi, \boldsymbol \lambda, \boldsymbol \mu) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \xi_{i} - \sum_{i}^{N} \mu_{i} \xi_{i} - \sum_{i}^{N} \lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (1 - \xi_{i}))\)<br/> subject to \(\xi_{i}, \mu_{i}, \lambda_{i} \geq 0\) <ul> <li> \[\nabla_{\boldsymbol w} L = 0 \rightarrow \boldsymbol w = \sum_{i=1}^{N} \lambda_{i} y_{i} \boldsymbol x_{i}\] </li> <li> \[\nabla_{w_{0}} L = 0 \rightarrow \sum_{i=1}^{N} \lambda_{i} y_{i} = 0\] </li> <li> \[\nabla_{\xi_{i}} L = 0 \rightarrow C - \mu_{i} - \lambda_{i} = 0\] </li> </ul> </li> <li>KKT condition 중 slackness condition : <ul> <li> \[\lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (1 - \xi_{i})) = 0\] </li> <li> \[\mu_{i} \xi_{i} = 0\] </li> </ul> </li> <li>dual form :<br/> 위의 세 가지 식을 대입하여 \(\boldsymbol w, w_{0}, \xi_{i}, \mu_{i}\) 를 소거하면<br/> maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}\)<br/> subject to \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\) and \(0 \leq \lambda_{i} \leq C\)</li> <li>Summary : <ul> <li>Step 1) optimal \(\lambda_{i}^{\ast}\) 구하기<br/> \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\) and \(0 \leq \lambda_{i} \leq C\) 이용해서<br/> \(\nabla_{\lambda_{i}} L = 0\) 으로 아래의 dual form 풀어서<br/> (maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}\))<br/> optimal \(\lambda_{i}\) 얻음</li> <li>Step 2) optimal \(\boldsymbol w^{\ast}, w_{0}^{\ast}\) 구하기 <ul> <li>\(\boldsymbol w^{\ast} = \sum_{i=1}^{N_{s}} \lambda_{i}^{\ast} y_{i} x_{i}\)<br/> (\(N_{s}\) : support vector 개수)<br/> (hyperplane 결정할 때는 \(\lambda_{i} \gt 0\) 중에 \(\xi = 0\) 인 support vectors만 고려!!)</li> <li>\(w_{0}^{\ast} = \frac{1}{y_{j}} - \sum_{i=1}^{N_{s}} \lambda_{i}^{\ast} y_{i} x_{i}^{T} x_{j} = \frac{1}{y_{j}} - \boldsymbol w^{\ast T} x_{j}\)<br/> (support vector \(x_{j}\) 1개 사용)<br/> 또는<br/> \(w_{0}^{\ast} = \frac{1}{N_{s}} \sum_{j=1}^{N_{s}} (\frac{1}{y_{j}} - \sum_{i=1}^{N_{s}} \lambda_{i}^{\ast} y_{i} x_{i}^{T} x_{j}) = \frac{1}{N_{s}} \sum_{j=1}^{N_{s}} (\frac{1}{y_{j}} - \boldsymbol w^{\ast T} x_{j})\)<br/> (support vector \(x_{j}\) \(N_{s}\)-개 모두 사용하여 average value)</li> </ul> </li> <li>Tip : hard margin (no sample inside margin) 의 경우<br/> 육안으로 어떤 sample이 support vector일지 판단 가능하다면<br/> complementary slackness condition (\(\lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (1 - \xi_{i})) = 0\)) 에서<br/> support vector만 \(\lambda_{i} \gt 0\) 이므로<br/> 연립해서 \(\boldsymbol w^{\ast}, w_{0}^{\ast}\) 바로 구할 수 있음</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/1.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/1.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/1.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/1.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>방법 2) 2-norm-soft-margin <ul> <li>차이점 1) primal form<br/> minimize \(J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \xi_{i}\)<br/> subject to \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}\) and \(\xi_{i} \geq 0\)<br/> 대신<br/> minimize \(J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + \frac{1}{2} C \sum_{i=1}^{N} \xi_{i}^{2}\)<br/> subject to \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}\) and \(\xi_{i} \geq 0\)</li> <li>차이점 2) Lagrangian<br/> \(\nabla_{\xi_{i}} L(\boldsymbol w, w_{0}, \boldsymbol \xi, \boldsymbol \lambda, \boldsymbol \mu) = 0\) 했을 때<br/> \(C - \mu_{i} - \lambda_{i} = 0\)<br/> 대신<br/> \(C \xi_{i} - \mu_{i} - \lambda_{i} = 0\)</li> <li>차이점 3) dual form<br/> maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}\)<br/> subject to \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\) and \(0 \leq \lambda_{i} \leq C\)<br/> 대신<br/> maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j} - \frac{1}{2C} \sum_{i=1}^{N} (\lambda_{i} + \mu_{i})^{2}\)<br/> subject to \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\) and \(0 \leq \lambda_{i} \leq C\)</li> </ul> </li> <li>Remark : <ul> <li>Linearly Non-Separable SVM에서<br/> \(C \rightarrow \infty\) 하면 Linearly Separable SVM<br/> e.g. non-linear에서는 \(0 \leq \lambda_{i} \leq C\) 인데, linear에서는 \(0 \leq \lambda_{i} \lt \infty\)</li> <li>SVM의 한계 :<br/> high computational complexity<br/> (SVM training은 주로 batch mode로 진행되어 memory를 많이 필요로 하므로<br/> training dataset을 subset으로 나눠서 training 진행)</li> <li>지금까지는 SVM for two-category만 살펴봤는데,<br/> M-class 의 경우 M개의 discriminant function \(g_{i}(x)\) 를 design하여<br/> assign \(x\) to class \(w_{i}\) if \(i = \text{argmax}_{k} g_{k}(x)\)</li> </ul> </li> </ul> <h3 id="v-svm">v-SVM</h3> <ul> <li>v-SVM : <ul> <li>hyperplane<br/> \(\boldsymbol w^{T} \boldsymbol x_{i} + w_{0} = \pm 1\)<br/> 대신<br/> \(\boldsymbol w^{T} \boldsymbol x_{i} + w_{0} = \pm \rho\)<br/> where \(\rho \geq 0\) : var. to be optimized</li> <li>margin<br/> margin은 \(\frac{2 \rho}{\| w \|}\) 이므로<br/> margin을 maximize하려면<br/> \(\| w \|\) minimize 뿐만 아니라 \(\rho\) maximize하면 되므로<br/> 둘 다 primal form loss term에 넣음</li> <li>primal form<br/> minimize \(J(\boldsymbol w, \xi, \rho) = \frac{1}{2} \| \boldsymbol w \|^{2} - v \rho + \frac{1}{N} \sum_{i=1}^{N} \xi_{i}\)<br/> subject to \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq \rho - \xi_{i}\) and \(\xi_{i} \geq 0\) and \(\rho \geq 0\)</li> <li>Lagrangian<br/> \(L(\boldsymbol w, w_{0}, \boldsymbol \xi, \rho, \boldsymbol \lambda, \boldsymbol \mu, \delta) = \frac{1}{2} \| \boldsymbol w \|^{2} - v \rho + \frac{1}{N} \sum_{i=1}^{N} \xi_{i} - \sum_{i}^{N} \mu_{i} \xi_{i} - \sum_{i}^{N} \lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (\rho - \xi_{i})) - \delta \rho\) <ul> <li>\(\nabla_{\boldsymbol w} L = 0\) 했을 때<br/> \(\boldsymbol w = \sum_{i=1}^{N} \lambda_{i} y_{i} \boldsymbol x_{i}\)</li> <li>\(\nabla_{w_{0}} L = 0\) 했을 때<br/> \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\)</li> <li>\(\nabla_{\xi_{i}} L = 0\) 했을 때<br/> \(\mu_{i} + \lambda_{i} = \frac{1}{N}\)</li> <li>\(\nabla_{\rho} L = 0\) 했을 때<br/> \(\sum_{i=1}^{N} \lambda_{i} - \delta = v\)</li> </ul> </li> <li>KKT condition 중 complementary slackness<br/> For \(\lambda_{i} \geq 0\) and \(\mu_{i} \geq 0\) and \(\delta \geq 0\), <ul> <li> \[\lambda_{i} (y_{i}(\boldsymbol w^{T} \boldsymbol x + w_{0}) - (\rho - \xi_{i})) = 0\] </li> <li> \[\mu_{i} \xi_{i} = 0\] </li> <li> \[\delta \rho = 0\] </li> </ul> </li> <li>dual form<br/> maximize \(L(\lambda) = - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}\)<br/> subject to \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\) and \(0 \leq \lambda_{i} \leq \frac{1}{N}\) and \(\sum_{i=1}^{N} \lambda_{i} = \delta + v \geq v\) <ul> <li>\(\lambda\) 만 explicitly 남아 있고,<br/> margin var. \(\rho\) 와 slack var. \(\xi_{i}\) 는 constraint의 bounds에 implicitly 들어 있음</li> <li>v-SVM에서는 \(\sum_{i=1}^{N} \lambda_{i}\) term이 없으므로<br/> optimal \(\lambda_{i}\) 는 quadratically homogeneous solution</li> <li>새로운 constraint \(\sum_{i=1}^{N} \lambda_{i} \geq v\) 있음</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/3.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/3.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Remark <ul> <li>v-SVM의 경우 \(0 \leq v \leq 1\) 이어야 optimizable</li> <li>C-SVM에 비해 v-SVM은<br/> error rate와 support vector 수 bound 측면에서 장점 <code class="language-plaintext highlighter-rouge">???</code></li> <li>\(\rho \gt 0\) 일 때 \(\delta = 0\) 이므로<br/> \(\sum_{i=1}^{N} \lambda_{i} = v\)</li> <li>loss (error)에 기여하는 애들은<br/> \(\xi_{i} \gt 0\), 즉 \(\mu_{i} = 0\), 즉 \(\lambda_{i} = \frac{1}{N}\) 이다<br/> 따라서 error rate = \(\sum_{i=1}^{N_{error}} \lambda_{i} = \frac{N_{error}}{N} \leq \sum_{i=1}^{N} \lambda_{i} = v\)<br/> 즉, error rate \(\frac{N_{error}}{N} \leq v\) 이고<br/> total number errors \(N_{error} \leq N v\)</li> <li>Since \(0 \lt \lambda_{i} \lt 1\) for support vector \(i\),<br/> \(v = \sum_{i=1}^{N} \lambda_{i} = \sum_{i=1}^{N_{s}} \lambda_{i} \leq \sum_{i=1}^{N_{s}} \frac{1}{N} = \frac{N_{s}}{N}\)<br/> 즉, \(vN \leq N_{s}\)<br/> (\(\lambda_{i} \gt 0\) 중에 \(\xi = 0\) 인 support vectors만 고려하면 \(\sum_{i=1}^{N} \lambda_{i} = \sum_{i=1}^{N_{s}}\) !!)</li> <li>\(\frac{N_{error}}{N} \leq v \leq \frac{N_{s}}{N}\) 이므로<br/> \(v\) optimize하면 error rate와 support vector 개수도 bound 알 수 있음</li> <li>support vector 수 \(N_{s}\) 는 classifier performance에 있어서 매우 중요<br/> (\(N_{s}\) 가 클수록 inner product 많이 계산해야 돼서 computational cost 높아짐)<br/> (\(N_{s}\) 가 크면 training set 이외의 data에 대한 performance가 제한되어 poor generalization)</li> </ul> </li> </ul> <h3 id="kernel-method-for-svm">Kernel Method for SVM</h3> <ul> <li> <p>discriminant function :<br/> \(x\) 의 inner product 꼴<br/> \(g(\boldsymbol x) = \boldsymbol w^{T} \boldsymbol x + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \boldsymbol x_{i}^{T} \boldsymbol x + w_{0}\)</p> </li> <li> <p>Cover’s theorem :<br/> non-linearly separable D-dim. space는<br/> linearly separable space of high enough dim. 으로 transform 될 수 있다<br/> (separating hyperplane의 optimality는 관심사 아님)</p> </li> <li> <p>Kernel Method for SVM :<br/> discriminant function \(g(\boldsymbol x) = \boldsymbol w^{T} \boldsymbol x + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \boldsymbol x_{i}^{T} \boldsymbol x + w_{0}\) 에서<br/> kernel \(K(\boldsymbol x_{i}, \boldsymbol x) = \boldsymbol x_{i}^{T} \boldsymbol x\)<br/> (inner product b.w. support vector and input vector)<br/> 대신<br/> 다른 kernel \(K(\boldsymbol x_{i}, \boldsymbol x) = \Phi(\boldsymbol x_{i})^{T} \Phi(\boldsymbol x)\) 을 써서<br/> non-linearly separable samples도 분류해보자!</p> <ul> <li>Step 1)<br/> input vector \(\boldsymbol x\) 와 training samples \(\boldsymbol x_{i}\) 를 <code class="language-plaintext highlighter-rouge">high-dim.으로 project</code> by function \(\Phi(\cdot)\)</li> <li>Step 2)<br/> transformed vector \(\Phi (\boldsymbol x)\) 와 \(\Phi (\boldsymbol x_{i})\) 에 대해 linear SVM 적용<br/> \(g(\boldsymbol x) = \boldsymbol w^{T} \Phi (\boldsymbol x) + w_{0}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/4.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/4.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Kernel Trick</code> :<br/> \(\boldsymbol x_{i}^{T} \boldsymbol x_{j}\) 대신 \(K(\boldsymbol x_{i}, \boldsymbol x_{j}) = \Phi(\boldsymbol x_{i})^{T} \Phi(\boldsymbol x_{j})\) 쓰면 됨!! <ul> <li>optimization of dual form :<br/> maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}\) <br/> 대신<br/> maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} K(\boldsymbol x_{i}, \boldsymbol x_{j})\)<br/> where \($K(\boldsymbol x_{i}, \boldsymbol x_{j}) = \Phi(\boldsymbol x_{i})^{T} \Phi(\boldsymbol x_{j})\)</li> <li>hyperplane :<br/> \(g(\boldsymbol x) = \boldsymbol w^{T} \boldsymbol x + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \boldsymbol x_{i}^{T} \boldsymbol x + w_{0} = 0\)<br/> 대신<br/> \(g(\boldsymbol x) = \boldsymbol w^{T} \Phi(\boldsymbol x) + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \Phi(\boldsymbol x_{i})^{T} \Phi(\boldsymbol x) + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} K(\boldsymbol x_{i}, \boldsymbol x) + w_{0} = 0\)<br/> where \(\boldsymbol w = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \Phi(\boldsymbol x_{i})\)</li> </ul> </li> <li>Remark : <ul> <li>polynomial learning machine, radial-basis function network, two-layer perceptron(single hidden layer) 와 같은<br/> kernel-based learning machine을 만들 때<br/> support vector learning algorithm을 사용 <ul> <li>polynomial :<br/> \(K(x, z) = (x^{T} z + 1)^{q}\) for \(q \gt 0\)</li> <li>radial-basis function :<br/> \(K(x, z) = \text{exp}(-\frac{\| x - z \|^{2}}{\sigma^{2}})\)</li> <li>hyperbolic tangent :<br/> \(K(x, z) = \text{tanh}(\beta x^{T} z + \gamma)\) where typical value is \(\beta = 2\) and \(\gamma = 1\)</li> </ul> </li> </ul> </li> <li>문제 풀이 예시 :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/6-480.webp 480w,/assets/img/2024-10-28-Pattern2/6-800.webp 800w,/assets/img/2024-10-28-Pattern2/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/5-480.webp 480w,/assets/img/2024-10-28-Pattern2/5-800.webp 800w,/assets/img/2024-10-28-Pattern2/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/7-480.webp 480w,/assets/img/2024-10-28-Pattern2/7-800.webp 800w,/assets/img/2024-10-28-Pattern2/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/8-480.webp 480w,/assets/img/2024-10-28-Pattern2/8-800.webp 800w,/assets/img/2024-10-28-Pattern2/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/9-480.webp 480w,/assets/img/2024-10-28-Pattern2/9-800.webp 800w,/assets/img/2024-10-28-Pattern2/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="chapter-6-multilayer-neural-networks">Chapter 6. Multilayer Neural Networks</h2> <ul> <li>activation function : <ul> <li>unipolar sigmoid :<br/> \(\phi (x) = \frac{1}{1 + exp(-x)}\)<br/> \(\phi^{'} (x) = \phi (x) (1 - \phi (x))\)</li> <li>bipolar sigmoid (tanh) :<br/> \(\phi (x) = \text{tanh} (x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\)<br/> \(\phi^{'} (x) = 1 - \text{tanh}^{2} (x) = 1 - \phi^{2} (x)\) <ul> <li>tanh가 sigmoid보다 gradient 더 커서 같은 \(\eta\) 일 때 학습 더 많이 함</li> </ul> </li> <li>ReLU</li> </ul> </li> <li>weight initialization : <ul> <li>zero-mean uniform distribution \(U(0, \sigma^{2})\)<br/> where \(\sigma^{2}\) is chosen so that std of induced local fields of neurons lie in the linear transition interval of sigmoid activation function</li> </ul> </li> <li>weight update :<br/> \(w_{ji}(n+1) = w_{ji}(n) + \eta \delta_{j} (n) y_{i} (n) + \alpha (w_{ji} (n) - w_{ji} (n-1))\)<br/> where \(\eta\) : learning rate<br/> where \(\alpha\) : momentum constant<br/> (momentum in back-prop has stabilizing effect when gradient has oscillate in sign)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/11.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/11.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/11.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/11.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="back-propagation-algorithm">Back-propagation Algorithm</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/10.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/10.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/10.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/10.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="issues-on-neural-networks">Issues on Neural Networks</h3> <ul> <li>Stopping criteria : <ul> <li>Euclidean norm of gradient reaches sufficiently small threshold</li> <li>absolute rate of change in average squared error per epoch is sufficiently small</li> <li>generalization performance (tested after each iter.) has peaked</li> </ul> </li> <li>Weight Update : <ul> <li>sample-by-sample mode :<br/> weights are updated after presenting each training sample<br/> \(w_{ji}(n+1) = w_{ji}(n) + \eta \delta_{j} (n) y_{i} (n) + \alpha (w_{ji} (n) - w_{ji} (n-1))\) <ul> <li>very sensitive to each sample so that the weight update term is very noisy</li> </ul> </li> <li>batch mode :<br/> weights are updated after presenting entire set of training samples<br/> \(w_{ji}(t+1) = w_{ji}(t) + \eta \frac{1}{N} \sum_{n=1}^{N} \delta_{j} (n) y_{i} (n) + \alpha (w_{ji} (t) - w_{ji} (t-1))\)</li> </ul> </li> <li>k-fold cross validation :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/12.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/12.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/12.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/12.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/13.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/13.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/13.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/13.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Normalization : Whitening <ul> <li>mean removal</li> <li>de-correlation</li> <li>scaling for equal covariance<br/> (then input var. in training set becomes uncorrelated)<br/> (then gradient descent converges faster)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/14.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/14.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/14.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/14.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Gradient Vanish 해결 방법 : <ul> <li>방법 1) Batch Normalization<br/> Batch Normalization을 해서 input이 \(0\) 주위의 가파른 부분에 머무르도록 함<br/> 근데 sigmoid의 경우 gradient가 [0, 0.25] 이고, tanh의 경우 gradient가 [0, 1] 이므로<br/> 여전히 gradient vanishing 문제 발생</li> <li>방법 2) ReLU activation<br/> ReLU의 gradient는 0 또는 1이므로<br/> gradient value 1의 경우 gradient vanishing 문제 없음</li> <li>방법 3) Residual Network<br/> skip connection 사용하여<br/> non-linear activation 통과하지 않고 바로 gradient가 흘러들어갈 수 있도록 함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/15.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/15.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/15.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/15.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/16.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/16.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/16.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/16.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Bias-Variance dilemma : <ul> <li>prediction error의 종류로는 bias, variance, irreducible error가 있음<br/> MSE \(E_{x, y, D}[\epsilon^{2} (x)] = E_{x, y, D}[(h_{D}(x) - y)^{2}]\)<br/> \(= E_{x, D}[(h_{D}(x) - \bar h(x))^{2}] + E_{x}[(\bar h(x) - \bar y(x))^{2}] + E_{x, y}[(\bar y(x) - y)^{2}]\) <ul> <li>variance of model : \(E_{x, D}[(h_{D}(x) - \bar h(x))^{2}]\)<br/> where \(h_{D}(x)\) : model output<br/> where \(\bar h(x)\) : model mean <ul> <li>different test dataset으로 테스트했을 때 변하는 정도</li> <li>particular training dataset에 overfitting된 정도</li> </ul> </li> <li>bias of model : \(\bar h(x) - \bar y(x)\)<br/> where \(\bar y(x)\) : label mean <ul> <li>data를 아무리 많이 학습시켜도 model 특성 때문에 발생하는 inherent error<br/> e.g. linear classifier is biased to a particular kind of solution</li> </ul> </li> <li>data noise : \(E_{x, y}[(\bar y(x) - y)^{2}]\) <ul> <li>ambiguity due to data distribution and feature representation</li> </ul> </li> </ul> </li> <li>trade-off : <ul> <li>too simple model : high bias<br/> \(\rightarrow\) 해결법 : pick more complex model</li> <li>too complex model : high variance<br/> \(\rightarrow\) naive 해결법 : use more data, but it requires high cost</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/17.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/17.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/17.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/17.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>SSE (Sum of Squared Errors) :<br/> 위의 유도 결과에서<br/> 두 번째 term은 model weight와 무관하므로<br/> back-propagation은 첫 번째 term만 minimize<br/> 즉, \(y_{k}(x; W) \approx P(w_{k} | x)\)<br/> 이 때, 잘 approx.하려면 model이 충분한 layers, neurons를 가지고 있어야 함 <ul> <li>\(y_{k}(x; W)\) : model output</li> <li> <table> <tbody> <tr> <td>$$P(w_{k}</td> <td>x)$$ : true posteriori probability (Bayes linear discriminant function)</td> </tr> </tbody> </table> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/18.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/18.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/18.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/18.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="chapter-7-stochastic-methods-for-pattern-classification">Chapter 7. Stochastic Methods for Pattern Classification</h2> <ul> <li>Learning : <ul> <li>deterministic learning :<br/> 무조건 loss 작아지는 방향으로 이동<br/> e.g. gradient descent</li> <li>stochastic learning :<br/> 작은 확률이겠지만 energy 커지는 방향으로 이동하는 것도 허용<br/> \(\rightarrow\) local minima에 빠지는 문제 해결하여 global minimum에 도달 가능 e.g. Simulated Annealing, Boltzmann Machine</li> </ul> </li> <li>Boltzmann Machine : <ul> <li>neuron : <ul> <li>visible neuron : \(\alpha = \{ \alpha^{i}, \alpha^{o} \}\)</li> <li>hidden neuron : \(\beta\)</li> </ul> </li> <li>probability \(P(\alpha) = \sum_{\beta} P(\alpha \beta) = \sum_{\beta} \frac{e^{- E_{\alpha \beta} / T}}{Z} = \sum_{\beta} \frac{e^{- E_{\alpha \beta} / T}}{\sum_{\alpha \beta} e^{- E_{\alpha \beta} / T}}\)<br/> where energy \(E_{\gamma} = - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} w_{ij} s_{i} s_{j}\)<br/> where \(w_{ij} = w_{ji}\) is symmetric with \(w_{ii} = 0\)<br/> 즉, energy \(E_{\gamma}\) 가 낮을수록 configuration \(\alpha\) 일 확률 \(P(\alpha)\) 가 높음</li> <li>goal :<br/> \(P(\alpha)\) 가 \(Q(\alpha)\) 와 최대한 가까워지도록<br/> weight \(w_{ij}\) 를 학습 <ul> <li>marginal (estimated) distribution of generated samples \(P(\alpha)\) :<br/> network가 free-running일 때 (all neurons가 자유롭게 업데이트될 수 있을 때)<br/> visible neuron이 \(\alpha\) 일 확률</li> <li>observed (desired) distribution of training samples \(Q(\alpha)\) :<br/> network의 visible neuron이 \(\alpha\) 로 clamped되었을 때<br/> visible neuron이 \(\alpha\) 일 확률</li> <li>KL-divergence \(D_{KL} (Q(\alpha), P(\alpha)) = \sum_{\alpha} Q(\alpha) \text{log} \frac{Q(\alpha)}{P(\alpha)}\) 를 minimize하면<br/> \(\Delta w_{ij} = - \eta \frac{\partial D_{KL}}{\partial w_{ij}} = \eta \sum_{\alpha} \frac{Q(\alpha)}{P(\alpha)} \frac{\partial P(\alpha)}{\partial w_{ij}} = \frac{\eta}{2T}(\sum_{\alpha \beta} Q(\alpha) P(\beta | \alpha) s_{i} s_{j} - \sum_{\alpha^{'} \beta^{'}} P(\alpha^{'} \beta^{'}) s_{i} s_{j}) = \frac{\eta}{2T}(E_{Q}[s_{i} s_{j}]_{\text{clamped by} \alpha} - E[s_{i} s_{j}]_{\text{free}})\)<br/> (pf는 아래에)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/19.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/19.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/19.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/19.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Boltzmann Machine with I-O Association : <ul> <li>neuron : <ul> <li>visible neuron : <ul> <li>input neuron : \(\alpha\)</li> <li>output neuron : \(\gamma\)</li> </ul> </li> <li>hidden neuron : \(\beta\)</li> </ul> </li> <li>goal :<br/> \(P(\gamma | \alpha)\) 가 \(Q(\gamma | \alpha)\) 와 최대한 가까워지도록<br/> weight \(w_{ij}\) 를 학습 <ul> <li>KL-divergence \(D_{KL} (Q(\gamma | \alpha), P(\gamma | \alpha)) = D_{KL} (Q(\alpha \gamma), P(\alpha \gamma)) - D_{KL} (Q(\alpha), P(\alpha))\) 를 minimize하면<br/> \(\Delta w_{ij} = \frac{\eta}{2T}(E_{Q}[s_{i} s_{j}]_{\text{clamped by} \alpha \gamma} - E[s_{i} s_{j}]_{\text{clamped by} \alpha})\)<br/> (pf는 아래에)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/20.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/20.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/20.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/20.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/21.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/21.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/21.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/21.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/22.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/22.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/22.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/22.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Restricted Boltzmann Machine (RBM) :<br/> Boltzmann Machine with bi-partite graph of visible and hidden units <ul> <li>probability \(P(v) = \sum_{h} P(v, h) = \sum_{h} \frac{e^{- E(v, h)}}{Z}\)<br/> where (intractable) partition function \(Z = \sum_{v, h} e^{- E(v, h)}\)<br/> where energy \(E(v, h) = - h^{T} W v - b^{T} v - c^{T} h\) for \(T=1\)</li> <li>goal :<br/> training dataset의 distribution을 학습!! <ul> <li>훈련 끝나고나면 해당 distribution을 따르는 new sample을 generate할 수 있음!! image inpainting에도 적용 가능</li> <li>label과의 joint distribution을 학습하여 classification 수행 가능!! feed-forward layer를 initialize할 때 RBM weight 사용 가능</li> <li>feature extractor 역할 수행 가능!!</li> </ul> </li> </ul> </li> <li>Training RBM 수식 유도 : <ul> <li>weight \(\theta = [W, b, c]\) 에 대해<br/> \(\hat \theta = \text{argmax}_{\theta} \text{ln} P(v | \theta) = \text{argmax}_{\theta} \text{ln} \sum_{h} \frac{e^{- E(v, h | \theta)}}{Z} = \text{argmax}_{\theta} \text{ln} \sum_{h} e^{- E(v, h | \theta)} - \text{ln} Z\)<br/> \(\rightarrow\)<br/> \(\frac{\partial - \text{ln} P(v | \theta)}{\partial \theta} = \cdots = \sum_{h} P(h | v, \theta) \frac{\partial E(v, h | \theta)}{\partial \theta} - \sum_{v, h} P(v, h | \theta) \frac{\partial E(v, h | \theta)}{\partial \theta} = E_{h \sim P(h | v, \theta)}[\frac{\partial E(v, h | \theta)}{\partial \theta}] - E_{(v, h) \sim P(v, h | \theta)}[\frac{\partial E(v, h | \theta)}{\partial \theta}]\) <ul> <li>오른쪽 expectation 식 :<br/> model distribution \(P(v, h | \theta)\) 의 모든 경우의 수에 대해 expectation \(E[\cdot]\) 계산하려면 \(2^{m+n}\) 로 costly하므로<br/> MCMC (Markov chain Monte Carlo) 기법으로 해당 distribution에서 <code class="language-plaintext highlighter-rouge">Gibbs sampling</code> 수행하여<br/> 오른쪽 expectation 식을 sample mean으로 approx.</li> </ul> </li> <li>RBM의 경우 visible units끼리, hidden units끼리는 connection 없으므로<br/> given visible units에 대해 hidden units끼리는 <code class="language-plaintext highlighter-rouge">conditionally independent</code>하므로<br/> 같은 layer에 있는 units는 쉽게 jointly(동시에) Gibbs sampling 가능<br/> \(\rightarrow\)<br/> <code class="language-plaintext highlighter-rouge">Gibbs sampling</code>은 두 단계로 요약 가능 (block Gibbs sampling) <ul> <li>Step 1)<br/> sample \(h\) based on \(P(h | v)\)</li> <li>Step 2)<br/> sample \(v\) based on \(P(v | h)\)</li> </ul> </li> <li>Conditional Distribution :<br/> proof는 아래 아래 사진에 <ul> <li> \[P(h_{i} = 1 | v) = \sigma (\sum_{j=1}^{m} w_{ij} v_{j} + c_{i}) = \sigma (\boldsymbol w_{i} \cdot \boldsymbol v + c_{i})\] </li> <li> \[P(v_{j} = 1 | h) = \sigma (\sum_{i=1}^{n} w_{ij} h_{i} + b_{j}) = \sigma (\boldsymbol w_{j} \cdot \boldsymbol h + b_{j})\] </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/27.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/27.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/27.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/27.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/23.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/23.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/23.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/23.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Training RBM 수식 유도 (continued) : <ul> <li>goal :<br/> \(P(v | \theta)\) 가 \(Q(v)\) 와 최대한 가까워지도록<br/> weight \(W, b, c\) 를 학습 <ul> <li></li> <li>Gradient of Log-Likelihood :<br/> (proof는 아래 사진에)<br/> Let’s define<br/> \(\boldsymbol h (\boldsymbol v_{0}) = P(\boldsymbol h = \boldsymbol 1 | \boldsymbol v_{0}, \theta) = \sigma(\boldsymbol W \boldsymbol v_{0} + \boldsymbol b)\) <br/> \(\boldsymbol v_{0} \in S\) : clamped training sample where \(S\) is training dataset<br/> \(\boldsymbol v_{k}\) : generated sample by RBM<br/> KL-divergence \(D_{KL} (Q(v), P(v | \theta)) = \sum_{v} Q(v) \text{log} \frac{Q(v)}{P(v | \theta)}\) 를 minimize하려면 <ul> <li> \[\Delta W \propto \frac{1}{N} \sum_{\boldsymbol v_{0} \in S} \boldsymbol h (\boldsymbol v_{0}) \boldsymbol v_{0}^{T} - \boldsymbol h (\boldsymbol v_{k}) \boldsymbol v_{k}^{T}\] </li> <li> \[\Delta b \propto \frac{1}{N} \sum_{\boldsymbol v_{0} \in S} \boldsymbol v_{0}^{T} - \boldsymbol v_{k}^{T}\] </li> <li> \[\Delta c \propto \frac{1}{N} \sum_{\boldsymbol v_{0} \in S} \boldsymbol h (\boldsymbol v_{0}) - \boldsymbol h (\boldsymbol v_{k})\] </li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/24.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/24.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/24.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/24.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>k-step Contrastive Divergence :<br/> model expectation은 exponential cost 가지므로<br/> sampling from RBM distribution<br/> 대신<br/> k-step sampling from Gibbs chain initialized with training data<br/> Then, \(\frac{\partial - \text{ln} P(v | \theta)}{\partial \theta} \approx \sum_{h} P(h | v_{0}) \frac{\partial E(v_{0}, h)}{\partial \theta} - \sum_{h} P(h | v_{k}) \frac{\partial E(v_{k}, h)}{\partial \theta}\)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/26.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/26.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/26.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/26.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Training RBM Summary : <ul> <li>Step 1) <code class="language-plaintext highlighter-rouge">Positive Phase</code><br/> training sample \(\boldsymbol v_{0}\) 에서 시작하여<br/> sample \(h_{i}\) from \(Q(h_{i} = 1 | v_{j}) = \sigma (\boldsymbol w_{i} \cdot \boldsymbol v + c_{i})\)<br/> \(\rightarrow\)<br/> \(h_{i} = \begin{cases} 1 &amp; \text{if} &amp; \text{rand}(0, 1) \lt Q(h_{i}=1 | v_{j}) \\ 0 &amp; \text{O.W.} &amp; \text{} \end{cases}\)</li> <li>Step 2) <code class="language-plaintext highlighter-rouge">Negative Phase</code> (Recon. Phase)<br/> sample \(v_{j}\) from \(P(v_{j} = 1 | h_{i}) = \sigma (\boldsymbol w_{j} \cdot \boldsymbol h + b_{j})\)<br/> \(\rightarrow\)<br/> \(v_{j} = \begin{cases} 1 &amp; \text{if} &amp; \text{rand}(0, 1) \lt P(v_{j}=1 | h_{i}) \\ 0 &amp; \text{O.W.} &amp; \text{} \end{cases}\)</li> <li>Step 3) 위의 과정을 k-step 반복<br/> \(\boldsymbol v_{0}\) 으로부터 \(\boldsymbol h(\boldsymbol v_{0})\) 얻고<br/> \(\cdots\) (Gibbs sampling k-step 반복) \(\cdots\)<br/> \(\boldsymbol v_{k}\) 으로부터 \(\boldsymbol h(\boldsymbol v_{k})\) 얻음</li> <li>Step 4) <code class="language-plaintext highlighter-rouge">Update Weights</code> <ul> <li> \[\Delta W \propto \frac{1}{N} \sum_{\boldsymbol v_{0} \in S} \boldsymbol h (\boldsymbol v_{0}) \boldsymbol v_{0}^{T} - \boldsymbol h (\boldsymbol v_{k}) \boldsymbol v_{k}^{T}\] </li> <li> \[\Delta b \propto \frac{1}{N} \sum_{\boldsymbol v_{0} \in S} \boldsymbol v_{0}^{T} - \boldsymbol v_{k}^{T}\] </li> <li> \[\Delta c \propto \frac{1}{N} \sum_{\boldsymbol v_{0} \in S} \boldsymbol h (\boldsymbol v_{0}) - \boldsymbol h (\boldsymbol v_{k})\] </li> </ul> </li> </ul> </li> <li>RBM Code :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/25.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/25.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/25.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/25.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Example :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/28.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/28.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/28.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/28.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/29.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/29.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/29.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/29.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="chapter-8-non-metric-methods-for-pattern-classification">Chapter 8. Non-metric Methods for Pattern Classification</h2> <p>training data 전체에 부합하는 parametric boundary 찾는 대신<br/> feature space를 여러 tree level에서 sub-regions로 나눠서 classify independently</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/30.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/30.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/30.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/30.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Decision Tree :<br/> not unique for partition <ul> <li>probability that \(\boldsymbol x \in u(t)\) has class \(w_{j}\) :<br/> \(N(t=5) = 7\), \(N_{1} (t=5) = 2\), \(N_{2} (t=5) = 5\) 에 대해<br/> \(P(w_{1} | t=5) = \frac{2}{7}\), \(P(w_{2} | t=5) = \frac{5}{7}\)</li> <li>node impurity and splitting :<br/> \(I(t) = \phi (P(w_{1} | t), \cdots, P(w_{c} | t))\) where \(\sum_{i=1}^{c} P(w_{i} | t) = 1\) <ul> <li>maximum, minimum : <ul> <li> <table> <tbody> <tr> <td>$$P(w_{1}</td> <td>t) = \cdots = P(w_{c}</td> <td>t) = \frac{1}{c}$$ 일 때 maximum 값 (가장 impure)</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$$P(w_{j}</td> <td>t) = 1\(and\)P(w_{i}</td> <td>t) = 0\(for all\)i \neq j$$ 일 때 minimum 값 (가장 pure)</td> </tr> </tbody> </table> </li> </ul> </li> <li>종류 : <ul> <li> <table> <tbody> <tr> <td>Gini criterion : $$I(t) = 1 - \sum_{i} P(w_{i}</td> <td>t)^{2}$$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>Entropy : $$I(t) = - \sum_{i} P(w_{i}</td> <td>t) \text{log}<em>{2} P(w</em>{i}</td> <td>t)$$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>Classification error : $$I(t) = 1 - \text{max}<em>{i} P(w</em>{i}</td> <td>t) = \frac{N_{minor}(t)}{N(t)}$$</td> </tr> </tbody> </table> </li> </ul> </li> <li>parent node에서 child node로 옮겼을 때 <code class="language-plaintext highlighter-rouge">impurity가 많이 감소할수록 잘 split</code>한 것임<br/> 즉, \(\Delta I(t) = I(t) - (I(t_{L}) \frac{N(t_{L})}{N(t)} + I(t_{R}) \frac{N(t_{R})}{N(t)})\) 가 클수록 better split<br/> e.g. \(I(t) = 1 - \text{max}_{i} P(w_{i} | t)\) 를 사용할 경우 위의 그림에서 \(\Delta I(t=3) = \frac{3}{13} - (\frac{1}{4} \frac{4}{13} + \frac{0}{9} \frac{9}{13}) = \frac{2}{13}\)</li> <li>terminating splitting : <ul> <li>stopping rule :<br/> stop splitting the node if \(\Delta I(t) \lt \tau\)</li> <li>pruning :<br/> 일단 (nearly) pure class 될 때까지 tree 늘린 뒤<br/> subtree를 terminal node로 대체</li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/31.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/31.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/31.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/31.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/32.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/32.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/32.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/32.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Pruning Decision Tree : <ul> <li>Let’s define <ul> <li>\(l(t), r(t)\) : node \(t\) 의 left, right child (\(l(t) = r(t) = 0\) for terminal node \(t\))</li> <li>\(\hat T\) : set of terminal(leaf) nodes of tree \(T\)</li> <li> \[P(t) = \frac{N(t)}{N(t=1)}\] </li> <li> <table> <tbody> <tr> <td>$$P(w_{j}</td> <td>t) = \frac{N_{j}(t)}{N(t)}$$</td> </tr> <tr> <td>where $$\sum_{j=1}^{c} P(w_{j}</td> <td>t) = 1$$</td> </tr> </tbody> </table> </li> <li>\(P_{L}(t) = \frac{P(l(t))}{P(t)} = \frac{N(l(t))}{N(t)}\)<br/> \(P_{R}(t) = \frac{P(r(t))}{P(t)} = \frac{N(r(t))}{N(t)}\)<br/> where \(P_{L}(t) + P_{R}(t) = 1\)</li> </ul> </li> <li>subtree and pruned subtree : <ul> <li><code class="language-plaintext highlighter-rouge">subtree</code> : 하나의 node에서 출발해서 its child node들을 가져오는데 양쪽 다 가져와야 함 (위 그림 참고)</li> <li><code class="language-plaintext highlighter-rouge">pruned subtree</code> \(T_{1} \leq T\) : tree \(T\) 와 root node가 같아야 함 (위 그림 참고)</li> </ul> </li> <li> <table> <tbody> <tr> <td>tree \(T\) has class labels $${ w_{j}(t)</td> <td>t \in \hat T }\(and disjoint partition\){ u(t)</td> <td>t \in \hat T }$</td> </tr> </tbody> </table> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/33.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/33.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/33.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/33.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Pruning Algorithm : <ul> <li>Let’s define <ul> <li>misclassification rate at <code class="language-plaintext highlighter-rouge">node</code> \(t\) :<br/> \(R(t) = \frac{M(t)}{N(t=1)} = \frac{M(t)}{N(t)} \frac{N(t)}{N(t=1)} = r(t) p(t)\)<br/> (node \(t\) 가 <code class="language-plaintext highlighter-rouge">terminal node라고 생각</code>)<br/> where \(r(t) = \frac{M(t)}{N(t)}\) : classification error at node \(t\)<br/> where \(p(t) = \frac{N(t)}{N(t=1)}\) : frequency at node \(t\)</li> <li>total misclassification rate for <code class="language-plaintext highlighter-rouge">tree</code> \(T\) :<br/> \(R(T) = \sum_{t \in \hat T} R(t) = \frac{1}{N(t=1)} \sum_{t \in \hat T} M(t)\)<br/> (for <code class="language-plaintext highlighter-rouge">all terminal nodes</code> of tree \(T\))</li> <li>cost-complexity at node \(t\) :<br/> \(R_{\alpha}(t) = R(t) + \alpha\)</li> <li>cost-complexity for tree \(T\) :<br/> \(R_{\alpha}(T) = \sum_{t \in \hat T} R_{\alpha} (t) = R(T) + \alpha | \hat T |\)</li> <li>cost-complexity for subtree \(T_{t}\) :<br/> \(R_{\alpha}(T_{t}) = \sum_{t \in \hat T_{t}} R_{\alpha} (t) = R(T_{t}) + \alpha | \hat T_{t} |\)<br/> where \(T_{t}\) : node \(t\) 를 <code class="language-plaintext highlighter-rouge">root node</code>로 하는 subtree</li> <li>\(\alpha = g(t)\) s.t. \(R_{\alpha}(t) = R_{\alpha}(T_{t})\) :<br/> \(R_{\alpha}(t) = R_{\alpha}(T_{t})\) 이도록 하는 \(\alpha\)<br/> 즉, \(R(t) + \alpha = R(T_{t}) + \alpha | \hat T_{t} |\) 이도록 하는 \(\alpha\) 즉, \(\alpha = g(t) = \frac{R(t) - R(T_{t})}{| \hat T_{t} | - 1}\)</li> </ul> </li> <li>Pruning : <ul> <li>\(g(t)\) 가 <code class="language-plaintext highlighter-rouge">최소</code>인 node \(t\) 를 prune!!<br/> \(\alpha = g(t) = \frac{R(t) - R(T_{t})}{| \hat T_{t} | - 1}\) 가 최소라는 말은, <ul> <li>case 1)<br/> node \(t\) 가 terminal node이든 (\(R(t)\)), node \(t\) 아래로 children이 있든 (\(R(T_{t})\)) 차이가 크지 않아서 prune해도 ok<br/> 즉, \(R(t) - R(T_{t})\) 가 작다</li> <li>case 2)<br/> node \(t\) 아래로 children이 너무 많아서 (large \(| \hat T_{t} |\)) prune하는 게 나음</li> </ul> </li> <li>prune할수록 \(| \hat T_{t} |\) 가 작아지니까<br/> tree는 작아지고<br/> cost \(\alpha = g(t)\) 는 커짐</li> <li>test set 또는 cross-validation 이용해서<br/> training set에 너무 overfitting된 nodes를 pruning함으로써<br/> better generalization</li> </ul> </li> </ul> </li> <li>Pruning Example :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/34.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/34.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/34.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/34.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/35.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/35.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/35.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/35.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="chapter-9-algorithm-independent-machine-learning">Chapter 9. Algorithm-Independent Machine Learning</h2> <ul> <li>Resampling : <ul> <li>Comparison : <ul> <li>Sampling :<br/> select groups within population</li> <li>Resampling :<br/> accuracy 높이고 uncertainty of estimate quantify하기 위해<br/> reselect new samples, that can provide more info. about population, based on one observed sample<br/> and estimate population(distribution) param. multiple times from data</li> </ul> </li> <li>종류 :<br/> resampling 기법 중 유명한 건 <ul> <li>jackknife</li> <li>bootstrap</li> </ul> </li> </ul> </li> </ul> <p>How to estimate bias and variance of estimator(statistic)?</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/36.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/36.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/36.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/36.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Jackknife : <ul> <li>bias 줄이기 위해<br/> remove samples from available dataset<br/> and recalculate the estimator</li> <li>may fail in estimating non-smooth statistic<br/> (smooth statistic : small change in data causes small change in statistic)</li> <li>\(i\)-th jackknife sample :<br/> \(i\)-th observation 제거<br/> \(x_{i} = (x_{1}, \cdots, x_{i-1}, x_{i+1}, \cdots, x_{n})\)<br/> \(\rightarrow\)<br/> estimate \(\hat \theta_{(i)}\) based on \(x_{i}\)</li> <li>\(i\)-th pseudo-sample :<br/> \(\tilde \theta_{(i)} = n \hat \theta - (n-1) \hat \theta_{(i)}\)</li> <li>jackknife estimate of bias (\(\hat b_{jack}(\hat \theta)\)) :<br/> \(\hat b_{jack}(\hat \theta) = (n-1)(\hat \theta_{(\cdot)} - \hat \theta)\)<br/> where \(\hat \theta_{(\cdot)} = \frac{1}{n} \sum_{i=1}^{n} \hat \theta_{(i)}\)</li> <li>bias-corrected jackknife estimate :<br/> \(\hat \theta_{jack} = \hat \theta - \hat b_{jack}(\hat \theta) = n \hat \theta - (n-1) \hat \theta_{(\cdot)} = \frac{1}{n} \sum_{i=1}^{n} \tilde \theta_{i}\)<br/> Then bias-corrected \(\hat \theta_{jack}\) 은 \(\hat \theta\) 에 비해 bias 작음</li> <li>jackknife estimate of variance (\(\hat v_{jack}(\hat \theta)\)) :<br/> \(\hat v_{jack} (\hat \theta) = \hat v (\hat \theta_{jack}) = \frac{s_{jack}^{2}}{n} = \frac{1}{n} \frac{1}{n-1} \sum_{i=1}^{n} (\tilde \theta_{(i)} - \frac{1}{n} \sum_{i=1}^{n} \tilde \theta_{(i)})^{2} = \frac{n-1}{n} \sum_{i=1}^{n} (\hat \theta_{(i)} - \hat \theta_{(\cdot)})^{2} = \frac{(n-1)^{2}}{n} s^{2}\)<br/> where \(s_{jack}^{2}\) is sample variance of \(n\) pseudo-samples<br/> where \(s\) is sample variance of \(n\) estimates</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/37.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/37.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/37.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/37.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="cv-tasks"/><category term="cv"/><summary type="html"><![CDATA[Lecture Summary (24F)]]></summary></entry><entry><title type="html">pixelSplat</title><link href="https://semyeong-yu.github.io/blog/2024/pixelSplat/" rel="alternate" type="text/html" title="pixelSplat"/><published>2024-10-25T12:00:00+00:00</published><updated>2024-10-25T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/pixelSplat</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/pixelSplat/"><![CDATA[<h2 id="pixelsplat---3d-gaussian-splats-from-image-pairs-for-scalable-generalizable-3d-reconstruction-cvpr-2024">pixelSplat - 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction (CVPR 2024)</h2> <h4 id="david-charatan-sizhe-li-andrea-tagliasacchi-vincent-sitzmann">David Charatan, Sizhe Li, Andrea Tagliasacchi, Vincent Sitzmann</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2312.12337">https://arxiv.org/abs/2312.12337</a><br/> project website :<br/> <a href="https://davidcharatan.com/pixelsplat/">https://davidcharatan.com/pixelsplat/</a><br/> code :<br/> <a href="https://github.com/dcharatan/pixelsplat">https://github.com/dcharatan/pixelsplat</a><br/> reference :<br/> NeRF and 3DGS Study</p> </blockquote> <h3 id="abstract">Abstract</h3> <ul> <li>pixelSplat :<br/> reconstruct a 3DGS primitive-based parameterization of 3D radiance field from only two images</li> </ul> <h3 id="introduction">Introduction</h3> <ul> <li>Problem : <ul> <li><code class="language-plaintext highlighter-rouge">scale ambiguity</code> :<br/> camera pose has arbitrary scale factor</li> <li><code class="language-plaintext highlighter-rouge">local minima</code> :<br/> primitive param.을 random initialization으로부터 직접 optimize하면 local minima 문제 발생</li> </ul> </li> <li>Contribution : <ul> <li>two-view image encoder :<br/> <code class="language-plaintext highlighter-rouge">two-view Epipolar Sampling, Epipolar Attention</code> 덕분에<br/> scale ambiguity 문제 극복</li> <li>pixel-aligned Gaussian param. prediction module :<br/> depth를 <code class="language-plaintext highlighter-rouge">sampling</code>하기 때문에<br/> local minima 문제 극복</li> </ul> </li> <li>Solution : <ul> <li>feed-forward model 이, a pair of images로부터,<br/> 3DGS primitives로 parameterized되는 3D radiance field recon.을 학습</li> </ul> </li> <li>model : <ul> <li><code class="language-plaintext highlighter-rouge">per-scene model</code> :<br/> <code class="language-plaintext highlighter-rouge">각각의 scene</code>을 학습하기 위해 <code class="language-plaintext highlighter-rouge">정해진 하나의 points set</code>을 <code class="language-plaintext highlighter-rouge">iteratively</code> update<br/> \(\rightarrow\)<br/> local minima 등 문제 있어서<br/> 3DGS에서는 non-differentiable Adaptive Density Control 기법으로 해결하려 하지만<br/> 이는 일반화 불가능</li> <li><code class="language-plaintext highlighter-rouge">feed-forward model</code> :<br/> <code class="language-plaintext highlighter-rouge">scene마다 얻은 points set</code>을 <code class="language-plaintext highlighter-rouge">한 번에 feed-forward</code>로 넣어서 학습<br/> differentiable하게 일반화 가능 <ul> <li>attention</li> <li>MASt3R(-SfM), Spann3R, Splatt3R, DUSt3R (잘 모름. 더 서치해봐야 함.)</li> </ul> </li> </ul> </li> </ul> <h3 id="background">Background</h3> <ul> <li>local minima : <ul> <li>언제 발생? :<br/> random location에 initialize된 Gaussian primitives가<br/> 최종 목적지까지 a few std보다 더 <code class="language-plaintext highlighter-rouge">많이 움직</code>여야 될 때<br/> gradient가 vanish 되어버려서<br/> 또는<br/> 최종 목적지까지 loss가 monotonically decrease하지 않을 때<br/> local minima 발생</li> <li>해결법? :<br/> 3DGS에서는<br/> non-differentiable pruning and splitting 기법인<br/> Adaptive Density Control을 사용하지만<br/> 본 논문에서는<br/> <code class="language-plaintext highlighter-rouge">differentiable</code> parameterization of Gaussian primitives 소개</li> </ul> </li> </ul> <h3 id="scale-ambiguity">Scale Ambiguity</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/4-480.webp 480w,/assets/img/2024-10-25-pixelSplat/4-800.webp 800w,/assets/img/2024-10-25-pixelSplat/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Scale Ambiguity 문제 : <ul> <li><code class="language-plaintext highlighter-rouge">SfM 단계</code>에서 camera pose를 계산할 때<br/> real-world-scale pose \(T_{j}^{m}\) 을<br/> metric pose \(s_{i} T_{j}^{m}\) 으로 scale하여 사용 <ul> <li>\(s_{i}\) :<br/> arbitrary scale factor</li> <li>metric pose \(s_{i} T_{j}^{m}\) :<br/> real-world-scale pose의 translation component를 \(s_{i}\) 만큼 scale</li> </ul> </li> <li>single image의 camera pose \(s_{i} T_{j}^{m}\) 로부터<br/> arbitrary scale factor \(s_{i}\) 를 복원하는 건 불가능</li> </ul> </li> <li>Scale Ambiguity 해결 : <ul> <li>two-view encoder에서 <code class="language-plaintext highlighter-rouge">a pair of images</code> 로부터<br/> arbitrary scale factor \(s_{i}\) 복원</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/1-480.webp 480w,/assets/img/2024-10-25-pixelSplat/1-800.webp 800w,/assets/img/2024-10-25-pixelSplat/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Two-view encoder Overview : <ul> <li>Step 1) Per-Image Encoder</li> <li>Step 2) Epipolar Sampling</li> <li>Step 3) Epipolar Attention</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/2-480.webp 480w,/assets/img/2024-10-25-pixelSplat/2-800.webp 800w,/assets/img/2024-10-25-pixelSplat/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Step 1) Per-Image Encoder </div> <ul> <li>Step 1) <code class="language-plaintext highlighter-rouge">Per-Image Encoder</code> :<br/> each view (two images)를 각각 feature \(F\), \(\tilde F\) 로 encode</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/3-480.webp 480w,/assets/img/2024-10-25-pixelSplat/3-800.webp 800w,/assets/img/2024-10-25-pixelSplat/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Step 2) Epipolar Sampling </div> <ul> <li>Step 2) <code class="language-plaintext highlighter-rouge">Epipolar Sampling</code> :<br/> Features 1 from Image 1의 <code class="language-plaintext highlighter-rouge">ray</code>로 <code class="language-plaintext highlighter-rouge">query</code> 만들고,<br/> Features 2 from Image 2의 <code class="language-plaintext highlighter-rouge">epipolar samples</code> 및 <code class="language-plaintext highlighter-rouge">depth</code> 로 <code class="language-plaintext highlighter-rouge">key, value</code> 만들어서,<br/> attention으로 depth scale을 잘 학습하는 게 목적<br/> (attention : depth 정보와 함께, Image 1의 ray가 Image 2의 epipolar line 위 어떤 sample에 더 많이 attention하는지)<br/> (epipolar line은 학습하는 게 아니라 수학 식으로 계산) <ul> <li>Query :<br/> \(q = Q \cdot F [u]\)<br/> where \(F\) : Features 1 from Image 1 <br/> where \(F [u]\) : ray feature at each pixel (in pixel coordinate)</li> <li>Key, Value :<br/> \(s = \tilde F [\tilde u_{l}] \oplus \gamma (\tilde d_{\tilde u_{l}})\)<br/> where \(\tilde F\) : Features 2 from Image 2<br/> where \(\tilde F [\tilde u_{l}]\) : samples on epipolar line<br/> where \(\tilde d_{\tilde u_{l}}\) : Image 2의 camera 원점까지의 거리 <ul> <li> \[k_{l} = K \cdot s\] </li> <li> \[v_{l} = V \cdot s\] </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/5-480.webp 480w,/assets/img/2024-10-25-pixelSplat/5-800.webp 800w,/assets/img/2024-10-25-pixelSplat/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Step 3) Epipolar Attention 중 Epipolar Cross-Attention </div> <ul> <li>Step 3) Epipolar Attention : <ul> <li><code class="language-plaintext highlighter-rouge">Epipolar Cross-Attention</code> :<br/> 앞서 만든 \(q, k_{l}, v_{l}\) 로 <code class="language-plaintext highlighter-rouge">cross-attention 수행</code>하여<br/> per-pixel <code class="language-plaintext highlighter-rouge">correpondence b.w. ray and epipolar sample</code> 찾음으로써<br/> per-pixel feature \(F [u]\) 가 이제<br/> arbitrary scale factor \(s\) 에 consistent한<br/> <code class="language-plaintext highlighter-rouge">scaled depth를 encode</code>하도록 update <ul> <li>\(F [u] += Att(q, k_{l}, v_{l})\)<br/> where \(+=\) : skip-connection<br/> where \(Att\) : softmax attention</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Per-Image Self-Attention</code> :<br/> Cross-Attention 끝난 뒤 마지막에 Per-Image Self-Attention 수행하여<br/> propagate scaled depth estimates<br/> to parts of the image feature maps<br/> that may not have any epipolar correspondences <ul> <li> \[F += SelfAttention(F)\] </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/9-480.webp 480w,/assets/img/2024-10-25-pixelSplat/9-800.webp 800w,/assets/img/2024-10-25-pixelSplat/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="gaussian-parameter-prediction">Gaussian Parameter Prediction</h3> <ul> <li>앞선 과정들 덕분에<br/> scale-aware feature map \(F, \tilde F\) 를 이용하여<br/> Gaussian param. \(g_{k} = (\mu_{k}, \Sigma_{k}, \alpha_{k}, S_{k})\) 를 예측 <ul> <li>2D image 상의 <code class="language-plaintext highlighter-rouge">모든 각 pixel은 3D 상의 point에 대응</code>되어<br/> 최종적인 Gaussian primitives set은<br/> just union of each image</li> </ul> </li> <li>3D position Prediction : <ul> <li>방법 1) Baseline :<br/> predict point estimate of 3D position \(\mu\) <ul> <li>\(\boldsymbol \mu = \boldsymbol o + d_{u} \boldsymbol d\)<br/> where \(u\) : 2D pixel coordiante<br/> where \(\boldsymbol d = R K^{-1} [u, 1]^{T}\) : ray direction<br/> where \(d_{u} = g_{\theta}(F [u])\) : depth obtained by neural network</li> <li>문제 :<br/> depth 자체를 neural network로 추정하는 건 local minima 문제 발생하기 쉬움</li> </ul> </li> <li>방법 2) 본 논문 방식 :<br/> predict <code class="language-plaintext highlighter-rouge">probability density</code> of 3D position \(\mu\) <ul> <li>핵심 :<br/> neural network로<br/> depth 자체를 예측하는 게 아니라<br/> differentiable probability distribution of likelihood of depth along ray를 예측</li> <li>Step 1)<br/> depth를 \(Z\)-bins로 discretize<br/> \(b_{z} = ((1 - \frac{z}{Z})(\frac{1}{d_{near}} - \frac{1}{d_{far}}) + \frac{1}{d_{far}})^{-1} \in [d_{near}, d_{far}]\)<br/> for \(z \in [0, Z]\) : depth index</li> <li>Step 2)<br/> discrete probability \(\phi\) 로부터 index \(z\) 를 sampling<br/> \(z \sim p_{\phi}(z)\)</li> <li>Step 3)<br/> ray를 쏴서(unproject) Gaussian mean \(\mu\) 계산<br/> \(\boldsymbol \mu = \boldsymbol o + (b_{z} + \delta_{z}) \boldsymbol d\)<br/> where \(\phi\) : depth(\(z\)) probability obtained by neural network<br/> where \(\delta_{z}\) : depth offset obtained by neural network</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/6-480.webp 480w,/assets/img/2024-10-25-pixelSplat/6-800.webp 800w,/assets/img/2024-10-25-pixelSplat/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Gaussian Parameter Prediction : <ul> <li>scale-aware feature map \(F, \tilde F\) 과 neural network \(f\) 를 이용하여<br/> \(\phi, \delta, \Sigma, S = f(F [u])\)<br/> where \(\phi, \delta, \Sigma, S\) : depth probability, depth offset, covariance, spherical harmonics coeff. <ul> <li><code class="language-plaintext highlighter-rouge">3D position</code>(mean) :<br/> \(\phi, \delta\) 이용해서<br/> \(\boldsymbol \mu = \boldsymbol o + (b_{z} + \delta_{z}) \boldsymbol d\)</li> <li><code class="language-plaintext highlighter-rouge">Covariance</code> :<br/> \(\Sigma\)</li> <li><code class="language-plaintext highlighter-rouge">Spherical Harmonics Coeff.</code> :<br/> \(S\)</li> <li><code class="language-plaintext highlighter-rouge">Opacity</code> :<br/> \(\phi\) 이용해서<br/> \(\alpha = \phi_{z}\)<br/> \(=\) probability of sampled depth \(z\)<br/> (so that we make sampling differentiable)</li> </ul> </li> <li>각 pixel마다 3DGS point에 대응되므로<br/> pixel-aligned Gaussians라고 부름</li> </ul> </li> </ul> <h3 id="experiments">Experiments</h3> <ul> <li>Setup : <ul> <li>Dataset :<br/> camera pose is computed by SfM <ul> <li>RealEstate 10k</li> <li>ACID</li> </ul> </li> <li>Baseline : <ul> <li>pixelNeRF</li> <li>GPNR</li> <li>Method of Du et al.</li> </ul> </li> <li>Metric : <ul> <li>PSNR</li> <li>SSIM</li> <li>LPIPS</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/7-480.webp 480w,/assets/img/2024-10-25-pixelSplat/7-800.webp 800w,/assets/img/2024-10-25-pixelSplat/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Result : <ul> <li>performance much better</li> <li>inference time faster</li> <li>less memory per ray</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/8-480.webp 480w,/assets/img/2024-10-25-pixelSplat/8-800.webp 800w,/assets/img/2024-10-25-pixelSplat/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/10-480.webp 480w,/assets/img/2024-10-25-pixelSplat/10-800.webp 800w,/assets/img/2024-10-25-pixelSplat/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Ablation Study </div> <ul> <li>Ablation Study : <ul> <li>Epipolar Encoder : Epipolar Sampling and Epipolar Attention</li> <li>Depth Encoding : freq.-based positional encoding \(\gamma(\tilde d_{\tilde u_{l}})\)</li> <li>Probabilistic Sampling : depth index \(z \sim p_{\phi}(z)\)</li> <li>Depth Regularization : <code class="language-plaintext highlighter-rouge">???</code></li> </ul> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="3DGS"/><category term="image"/><category term="pair"/><category term="scalable"/><summary type="html"><![CDATA[3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction (CVPR 2024)]]></summary></entry><entry><title type="html">3DGS Code Review</title><link href="https://semyeong-yu.github.io/blog/2024/3DGScode/" rel="alternate" type="text/html" title="3DGS Code Review"/><published>2024-10-11T12:00:00+00:00</published><updated>2024-10-11T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/3DGScode</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/3DGScode/"><![CDATA[<h2 id="3dgs-code-review">3DGS Code Review</h2> <blockquote> <p>code :<br/> <a href="https://github.com/graphdeco-inria/gaussian-splatting">https://github.com/graphdeco-inria/gaussian-splatting</a><br/> reference :<br/> https://charlieppark.kr<br/> NeRF and 3DGS Study</p> </blockquote> <h3 id="trainpy">train.py</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-11-3DGScode/1-480.webp 480w,/assets/img/2024-10-11-3DGScode/1-800.webp 800w,/assets/img/2024-10-11-3DGScode/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-11-3DGScode/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 1. train.py Algorithm </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gaussians</span> <span class="o">=</span> <span class="nc">GaussianModel</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">sh_degree</span><span class="p">)</span>
<span class="n">scene</span> <span class="o">=</span> <span class="nc">Scene</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">gaussians</span><span class="p">)</span>
</code></pre></div></div> <h3 id="gaussian-initialize">Gaussian Initialize</h3> <ul> <li>Fig 1.의 빨간 박스 : <ul> <li>pcd로부터 gaussians를 initialize</li> </ul> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Scene</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(...):</span>
    <span class="bp">...</span>
    <span class="n">self</span><span class="p">.</span><span class="n">gaussians</span><span class="p">.</span><span class="nf">create_from_pcd</span><span class="p">(</span><span class="n">scene_info</span><span class="p">.</span><span class="n">point_cloud</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">cameras_extent</span><span class="p">)</span>  
</code></pre></div></div> <ul> <li>\(\text{class Scene.__init__()}\) : <ul> <li>scene_info :<br/> Colmap 또는 Blender의 pcd, camera info.를 받아옴 <ul> <li>pcd : scene_info.point_cloud</li> <li>camera : scene_info.train_cameras, scene_info.test_cameras</li> </ul> </li> <li>self.gaussians.create_from_pcd() :<br/> pcd로부터 gaussians를 initialize</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-11-3DGScode/9-480.webp 480w,/assets/img/2024-10-11-3DGScode/9-800.webp 800w,/assets/img/2024-10-11-3DGScode/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-11-3DGScode/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> class Scene.__init__() </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-11-3DGScode/10-480.webp 480w,/assets/img/2024-10-11-3DGScode/10-800.webp 800w,/assets/img/2024-10-11-3DGScode/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-11-3DGScode/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> sceneLoadTypeCallbacks &gt; readColmapSceneInfo </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-11-3DGScode/2-480.webp 480w,/assets/img/2024-10-11-3DGScode/2-800.webp 800w,/assets/img/2024-10-11-3DGScode/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-11-3DGScode/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> GaussianModel.create_from_pcd() </div> <h3 id="densification">Densification</h3> <ul> <li>Fig 1.의 초록 박스 : <ul> <li>densification (clone and split)</li> <li>class GaussianModel densify_and_prune()</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-11-3DGScode/3-480.webp 480w,/assets/img/2024-10-11-3DGScode/3-800.webp 800w,/assets/img/2024-10-11-3DGScode/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-11-3DGScode/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> train.py Densification </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-11-3DGScode/4-480.webp 480w,/assets/img/2024-10-11-3DGScode/4-800.webp 800w,/assets/img/2024-10-11-3DGScode/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-11-3DGScode/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> GaussianModel.add_densification_stats() </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-11-3DGScode/5-480.webp 480w,/assets/img/2024-10-11-3DGScode/5-800.webp 800w,/assets/img/2024-10-11-3DGScode/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-11-3DGScode/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> GaussianModel.densify_and_prune() </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-11-3DGScode/6-480.webp 480w,/assets/img/2024-10-11-3DGScode/6-800.webp 800w,/assets/img/2024-10-11-3DGScode/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-11-3DGScode/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> GaussianModel.densify_and_clone(), GaussianModel.densify_and_split() </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-11-3DGScode/7-480.webp 480w,/assets/img/2024-10-11-3DGScode/7-800.webp 800w,/assets/img/2024-10-11-3DGScode/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-11-3DGScode/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> GaussianModel.densification_postfix() </div> <h3 id="gs-rasterize">GS Rasterize</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-11-3DGScode/8-480.webp 480w,/assets/img/2024-10-11-3DGScode/8-800.webp 800w,/assets/img/2024-10-11-3DGScode/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-11-3DGScode/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 2. GS Rasterize Algorithm </div> <ul> <li>Fig 2.의 노란 박스 : <ul> <li>cuda로 구현</li> <li></li> </ul> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="3DGS"/><category term="code"/><summary type="html"><![CDATA[3D Gaussian Splatting for Real-Time Radiance Field Rendering (SIGGRAPH 2023)]]></summary></entry><entry><title type="html">WandB</title><link href="https://semyeong-yu.github.io/blog/2024/WandB/" rel="alternate" type="text/html" title="WandB"/><published>2024-10-09T12:00:00+00:00</published><updated>2024-10-09T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/WandB</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/WandB/"><![CDATA[<h2 id="wandb-tutorial">WandB Tutorial</h2> <ul> <li>WandB Platform <ul> <li>Experiments : experiment tracking</li> <li>Sweeps : model optimization</li> <li>Artifacts : dataset versioning</li> <li>Tables : model evaluation</li> <li>Reports : collaborative analysis</li> </ul> </li> </ul> <h3 id="experiment">Experiment</h3> <ul> <li>wandb.init() : <ul> <li>run별로 구분하기 위해<br/> name 또는 tags 설정</li> <li>이미 끝난 run을 resume하고 싶으면<br/> 해당 run의 id 설정</li> </ul> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">wandb</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">project</span><span class="o">=</span><span class="sh">"</span><span class="s">ddpm</span><span class="sh">"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">exp1</span><span class="sh">"</span><span class="p">,</span> <span class="n">entity</span><span class="o">=</span><span class="sh">"</span><span class="s">semyu0102-viclab</span><span class="sh">"</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">__dict__</span><span class="p">,</span> <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">batch=32</span><span class="sh">"</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="s">lr=</span><span class="si">{</span><span class="n">wandb</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">lr</span><span class="si">}</span><span class="sh">"</span><span class="p">],</span> <span class="nb">id</span><span class="o">=</span><span class="p">...)</span>
<span class="c1"># wandb.run.name = wandb.run.id
# wandb.run.name = "exp1"
# wandb.run.save()
# wandb.config.update({"epochs":4, "batch_size":32})
</span></code></pre></div></div> <ul> <li>wandb.watch() :<br/> hook into model’s grad. or param. <ul> <li>log : ‘gradients’, ‘parameters’, ‘all’, ‘None’</li> <li>log_freq=N : N batch마다 gradients 또는 parameters 기록</li> </ul> </li> </ul> <pre><code class="language-Python">wandb.watch(model, criterion=criterion, log='all', log_freq=100)
</code></pre> <ul> <li>wandb.log() :<br/> step을 가로축으로 하여 <strong>dict</strong> 기록</li> </ul> <pre><code class="language-Python">wandb.log(__dict__, step=epoch)
</code></pre> <ul> <li>wandb.finish() :<br/> finish the run</li> </ul> <pre><code class="language-Python">wandb.finish()
</code></pre> <ul> <li>모델 저장 :</li> </ul> <pre><code class="language-Python">artifact = wandb.Artifact('model', type='model')
artifact.add_file(f"model/resnet50.pt")
wandb.log_artifact(artifact)
</code></pre> <ul> <li>이미지 저장 :</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">images</span><span class="o">=</span><span class="p">[]</span>
<span class="n">images</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">wandb</span><span class="p">.</span><span class="nc">Image</span><span class="p">(</span><span class="n">img</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">caption</span><span class="o">=</span><span class="sh">"</span><span class="s">Pred: {} Truth: {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">pred</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">item</span><span class="p">(),</span> <span class="n">target</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span>
<span class="c1"># img[0] : np array or PIL or matplotlib.figure.Figure ...
</span><span class="n">wandb</span><span class="p">.</span><span class="nf">log</span><span class="p">({</span><span class="sh">"</span><span class="s">Image</span><span class="sh">"</span><span class="p">:</span> <span class="n">images</span><span class="p">})</span> <span class="c1"># 100여개 정도까지가 한계
</span></code></pre></div></div> <ul> <li>히스토그램 저장 :</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">wandb</span><span class="p">.</span><span class="nf">log</span><span class="p">({</span><span class="sh">"</span><span class="s">gradients</span><span class="sh">"</span><span class="p">:</span> <span class="n">wandb</span><span class="p">.</span><span class="nc">Histogram</span><span class="p">(</span><span class="n">sequence</span><span class="p">)})</span> 
<span class="c1"># sequence : np array ...
</span></code></pre></div></div> <h3 id="sweep">Sweep</h3> <ul> <li>Sweep :<br/> hyperparam. tuning에 쓰임</li> </ul> <pre><code class="language-Python">wandb sweep sweep.yaml # sweep_id 출력
wandb agent &lt;sweep_id&gt; --count &lt;sweep 횟수&gt;
</code></pre> <ul> <li>sweep.yaml : <ul> <li>program :<br/> sweep 실행할 code file</li> <li>method : <ul> <li>bayes : bayesian 최적화를 통해 이전 실험 결과를 기반으로 효율적 탐색</li> <li>grid : 모든 조합 탐색</li> <li>random : random 선택</li> </ul> </li> <li>name, description</li> <li>project :<br/> sweep할 wandb project명</li> <li>entity :<br/> wandb 계정 유저명</li> <li>metric :<br/> hyperparam. tuning을 통해 이뤄야 할 목표 <ul> <li>name :<br/> wandb.log() 안에 넣은 이름으로 설정해야 함</li> </ul> </li> <li>parameters :<br/> tuning할 hyperparam. <ul> <li>min, max : 해당 범위 내에서 튜닝</li> <li>values : 해당 list 선택지 중에서 튜닝</li> <li>value : single 값으로, 튜닝 안 함</li> <li>distribution: normal, mu, sigma : 해당 \(N(\text{mu}, \text{sigma})\) 분포로 sampling한 값 중에 튜닝</li> </ul> </li> <li>early_terminate :<br/> 학습이 비효율적으로 진행될 경우 early stopping으로 자원 절약 <ul> <li>type :<br/> early stopping 알고리즘 선택 <ul> <li>hyperband</li> <li>median</li> </ul> </li> <li>min_iter, max_iter : 최소, 최대 반복 횟수 범위 안에서 early stopping</li> </ul> </li> <li>command :<br/> code file 실행할 때의 commands <ul> <li>${…} : parameters에서 정의한 hyperparam. 값을 동적으로 대입</li> </ul> </li> </ul> </li> </ul> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">program</span><span class="pi">:</span> <span class="s">main.py</span>
<span class="na">method</span><span class="pi">:</span> <span class="s">bayes</span>
<span class="na">name</span><span class="pi">:</span> <span class="s">ddpm-sweep</span>
<span class="na">description</span><span class="pi">:</span> <span class="s">test ddpm sweep</span>
<span class="na">project</span><span class="pi">:</span> <span class="s">ddpm</span>
<span class="na">entity</span><span class="pi">:</span> <span class="s">semyu0102-viclab</span>
<span class="na">meric</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">val_loss</span>
  <span class="na">goal</span><span class="pi">:</span> <span class="s">minimize</span>
<span class="na">parameters</span><span class="pi">:</span>
  <span class="na">learning_rate</span><span class="pi">:</span> 
    <span class="na">min</span><span class="pi">:</span> <span class="m">0.0001</span>
    <span class="na">max</span><span class="pi">:</span> <span class="m">0.1</span>
  <span class="na">optimizer</span><span class="pi">:</span>
    <span class="na">values</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">adam"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">sgd"</span><span class="pi">]</span>
  <span class="na">epochs</span><span class="pi">:</span>
    <span class="na">value</span><span class="pi">:</span> <span class="m">5</span>
  <span class="na">parameter1</span><span class="pi">:</span>
    <span class="na">distribution</span><span class="pi">:</span> <span class="s">normal</span>
    <span class="na">mu</span><span class="pi">:</span> <span class="m">100</span>
    <span class="na">sigma</span><span class="pi">:</span> <span class="m">10</span>
<span class="na">early_terminate</span><span class="pi">:</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">hyperband</span>
  <span class="na">min_iter</span><span class="pi">:</span> <span class="m">3</span>
<span class="na">command</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">python</span>
  <span class="pi">-</span> <span class="s">train.py</span>
  <span class="pi">-</span> <span class="s">--learning_rate=${learning_rate}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="others"/><category term="wandb"/><category term="log"/><category term="sweep"/><summary type="html"><![CDATA[WandB Tutorial]]></summary></entry><entry><title type="html">NeRF in the Wild</title><link href="https://semyeong-yu.github.io/blog/2024/NeRFW/" rel="alternate" type="text/html" title="NeRF in the Wild"/><published>2024-10-08T12:00:00+00:00</published><updated>2024-10-08T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/NeRFW</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/NeRFW/"><![CDATA[<h2 id="nerf-in-the-wild---neural-radiance-fields-for-unconstrained-photo-collections-cvpr-2021">NeRF in the Wild - Neural Radiance Fields for Unconstrained Photo Collections (CVPR 2021)</h2> <h4 id="ricardo-martin-brualla-noha-radwan-mehdi-s-m-sajjadi-jonathan-t-barron-alexey-dosovitskiy-daniel-duckworth">Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, Daniel Duckworth</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2008.02268">https://arxiv.org/abs/2008.02268</a><br/> project website :<br/> <a href="https://nerf-w.github.io/">https://nerf-w.github.io/</a><br/> code :<br/> <a href="https://github.com/kwea123/nerf_pl/tree/nerfw?tab=readme-ov-file">https://github.com/kwea123/nerf_pl/tree/nerfw?tab=readme-ov-file</a><br/> <a href="https://github.com/rover-xingyu/Ha-NeRF">https://github.com/rover-xingyu/Ha-NeRF</a><br/> youtube video :<br/> <a href="https://www.youtube.com/watch?v=mRAKVQj5LRA&amp;t=254s">https://www.youtube.com/watch?v=mRAKVQj5LRA&amp;t=254s</a><br/> reference :<br/> NeRF and 3DGS Study</p> </blockquote> <h3 id="introduction">Introduction</h3> <ul> <li>Issue : <ul> <li>Q : image 상의 동적인 물체를 어떻게 없앨 수 있을까?</li> <li>A : <code class="language-plaintext highlighter-rouge">Static Network</code>와 <code class="language-plaintext highlighter-rouge">Transient Network</code>를 분리한 뒤<br/> \(c, \sigma\) 에 대한 <code class="language-plaintext highlighter-rouge">Uncertainty</code>를 측정하자!</li> </ul> </li> <li>Contribution : <ul> <li>Latent Appearance Embedding in Static Network :<br/> 각 image의 광도 반영</li> <li>Latent Transient Embedding in Transient Network :<br/> 동적인 물체 구별</li> <li>Loss w. Uncertainty and Transient density</li> </ul> </li> <li>결과 : <ul> <li>Latent Embedding Vector 변화로 Appearance에 변화 줄 수 있음</li> <li>일시적으로 찍힌 동적인 물체를 제거할 수 있음</li> </ul> </li> </ul> <h3 id="architecture">Architecture</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/8-480.webp 480w,/assets/img/2024-10-08-NeRFW/8-800.webp 800w,/assets/img/2024-10-08-NeRFW/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="architecture---static-network">Architecture - Static Network</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/1-480.webp 480w,/assets/img/2024-10-08-NeRFW/1-800.webp 800w,/assets/img/2024-10-08-NeRFW/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Static Network :<br/> 우리가 Novel View Synthesis 하고 싶어하는 대상을 다룸</p> </li> <li>View Direction과 함께 Appearance Embedding Vector 넣어준다는 것 말고는 기존 NeRF 구조와 same <ul> <li>\(\gamma_{x}(r(t)) \rightarrow \sigma_{i}(t)\) (3d shape)</li> <li>\(\gamma_{x}(r(t)), \gamma_{d}(d), l_{i}^{(a)} \rightarrow c_{i}(t)\) (view-dependent 3d color)</li> </ul> </li> <li>Appearance Embedding Vector : <ul> <li>image의 embedding vector (각 image의 광도 반영)</li> <li>random initialization (learnable)</li> <li>control처럼 쓰일 수 있음<br/> Embedding Vector 수정하여 Appearance(스타일)에 변화 줄 수 있음</li> <li>training dataset에 대해 \(l_{i}^{(a)}\) 를 학습하므로<br/> test할 때는 target image에 적합할 만한 Embedding Vector 골라서 사용</li> </ul> </li> </ul> <h3 id="architecture---transient-network">Architecture - Transient Network</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/2-480.webp 480w,/assets/img/2024-10-08-NeRFW/2-800.webp 800w,/assets/img/2024-10-08-NeRFW/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Transient Network : <ul> <li>우리가 Novel View Synthesis 하고 싶어하는 대상이 아닌,<br/> 동적인 물체를 다룸 (제거하기 위해)</li> <li>Bayesian learning framework를 적용하여 <code class="language-plaintext highlighter-rouge">???</code> Uncertainty를 모델링</li> </ul> </li> <li>Transient Embedding \(l_{i}^{(T)}\) 을 넣어서 동적인 물체의 transient density를 얻은 뒤 제거 가능 <ul> <li> \[\gamma_{x}(r(t)), l_{i}^{(T)} \rightarrow c_{i}^{(T)}(t), \sigma_{i}^{(T)}(t), \tilde \beta_{i}(t)\] </li> <li> \[\beta_{i}(t) = \beta_{min} + \text{log}(1+\text{exp}(\tilde \beta_{i}(t)))\] </li> </ul> </li> </ul> <h3 id="volume-rendering">Volume Rendering</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/3-480.webp 480w,/assets/img/2024-10-08-NeRFW/3-800.webp 800w,/assets/img/2024-10-08-NeRFW/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Training : (a) Static, (b) Transient 모두 사용하여 아래의 rendering 식으로 (c) Composite 만들고, 이를 (d) GT와 비교하여 학습<br/> \(\hat C_{i} (r) = \sum_{k=1}^K T_{i}(t_k)(\alpha(\sigma_{i}(t_k) \delta_{k}) c_{i}(t_k) + \alpha(\sigma_{i}^{(T)}(t_k) \delta_{k}) c_{i}^{(T)}(t_k))\)<br/> where \(T_{i}(t_k) = \text{exp}(-\sum_{k^{'}=1}^{k-1}(\sigma_{i}(t_{k^{'}}) + \delta_{i}^{(T)}(t_{k^{'}}))\delta_{k^{'}})\)</p> </li> <li> <p>Test : (a) Static만 사용</p> </li> </ul> <h3 id="optimization">Optimization</h3> <ul> <li>Coarse Model :<br/> (기존 NeRF와 유사하게) static network만 사용해서 Appearance Embedding Vector를 학습<br/> \(L = \sum_{ij} L_{c}(r_{ij})\) <ul> <li>\(L_{c}(r_{ij}) = \frac{1}{2} \| C(r_{ij}) - \hat C^{c}(r_{ij}) \|^2\)<br/> where \(\hat C^{c}(r_{ij}) = \sum_{k=1}^K T_{i}(t_k)(\alpha(\sigma_{i}(t_k) \delta_{k}) c_{i}(t_k))\)<br/> (static network만 사용)</li> </ul> </li> <li>Fine Model :<br/> Coarse Model의 weight를 바탕으로 fine-sampling<br/> static, transient network 모두 사용해서 학습<br/> \(L = \sum_{ij} L_{f}(r_{ij}) + L_{c}(r_{ij})\) <ul> <li>\(L_{c}(r_{ij}) = \frac{1}{2} \| C(r_{ij}) - \hat C^{c}(r_{ij}) \|^2\)<br/> where \(\hat C^{c}(r_{ij}) = \sum_{k=1}^K T_{i}(t_k)(\alpha(\sigma_{i}(t_k) \delta_{k}) c_{i}(t_k))\)<br/> (static network만 사용)</li> <li>\(L_{f}(r_{ij}) = \frac{\| C(r_{ij}) - \hat C^{f}(r_{ij}) \|^2}{2\beta(r)^2} + \frac{\text{log} \beta(r)^2}{2} + \frac{\lambda}{K} \sum_{k=1}^K \sigma^{(T)}(t_k)\)<br/> where \(\hat C^{f}(r_{ij}) = \sum_{k=1}^K T_{i}(t_k)(\alpha(\sigma_{i}(t_k) \delta_{k}) c_{i}(t_k) + \alpha(\sigma_{i}^{(T)}(t_k) \delta_{k}) c_{i}^{(T)}(t_k))\)<br/> (static, trasient network 모두 사용) <ul> <li>1번째 term : <code class="language-plaintext highlighter-rouge">recon. loss term</code><br/> Uncertainty \(\beta(r)\) 가 크면 recon. loss 영향력 작아짐<br/> (동적인 물체가 있어서 불확실한 부분은 loss 및 gradient 작게)</li> <li>2번째 term : <code class="language-plaintext highlighter-rouge">regularization term</code><br/> Uncertainty \(\beta(r)\) 가 너무 커지지 않도록 regularize</li> <li>3번째 term : <code class="language-plaintext highlighter-rouge">regularization term</code><br/> transient density \(\sigma^{(T)}\) 가 너무 커지지 않도록 regularize</li> </ul> </li> </ul> </li> </ul> <h3 id="results">Results</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/4-480.webp 480w,/assets/img/2024-10-08-NeRFW/4-800.webp 800w,/assets/img/2024-10-08-NeRFW/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/5-480.webp 480w,/assets/img/2024-10-08-NeRFW/5-800.webp 800w,/assets/img/2024-10-08-NeRFW/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="conclusion">Conclusion</h3> <ul> <li>동적이고 조명이 바뀌는 상황에서 촬영된 image dataset으로도 neural rendering 가능 <ul> <li>Appearance Embedding : 각 image의 광도를 반영</li> <li>Transient Embedding : target static object를 가리는 동적인 물체 구별하여 제거</li> </ul> </li> </ul> <h3 id="limitation">Limitation</h3> <ul> <li>training 개수, camera calibration error에 민감</li> </ul> <h3 id="code">Code</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/6-480.webp 480w,/assets/img/2024-10-08-NeRFW/6-800.webp 800w,/assets/img/2024-10-08-NeRFW/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/7-480.webp 480w,/assets/img/2024-10-08-NeRFW/7-800.webp 800w,/assets/img/2024-10-08-NeRFW/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="question">Question</h3> <ul> <li> <p>Q1 :<br/> \(\hat C_{i} (r) = \sum_{k=1}^K T_{i}(t_k)(\alpha(\sigma_{i}(t_k) \delta_{k}) c_{i}(t_k) + \alpha(\sigma_{i}^{(T)}(t_k) \delta_{k}) c_{i}^{(T)}(t_k))\)<br/> 위의 volume rendering 식을 보면 static network의 color, density와 transient network의 color, density가 함께 하나의 pixel color로 rendering되어 동시에 backpropagation되는데<br/> 어떻게 두 network 중에서 하필 transient network의 color, density가 동적인 물체를 구별하는 역할을 수행할 수 있느냐</p> </li> <li> <p>A1 :<br/> Coarse Model(static network 사용)과 Fine Model(static, transient network 모두 사용)을 two-stage로 분리해서 학습하여<br/> transient embedding을 넣은 transient network가 동적인 물체를 식별하는 역할을 잘 수행할 수 있을 것이다</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/8-480.webp 480w,/assets/img/2024-10-08-NeRFW/8-800.webp 800w,/assets/img/2024-10-08-NeRFW/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="static"/><category term="transient"/><category term="uncertainty"/><summary type="html"><![CDATA[Neural Radiance Fields for Unconstrained Photo Collections (CVPR 2021)]]></summary></entry><entry><title type="html">Neuralangelo (and InstantNGP)</title><link href="https://semyeong-yu.github.io/blog/2024/Neuralangelo/" rel="alternate" type="text/html" title="Neuralangelo (and InstantNGP)"/><published>2024-09-30T12:00:00+00:00</published><updated>2024-09-30T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Neuralangelo</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Neuralangelo/"><![CDATA[<h2 id="neuralangelo---high-fidelity-neural-surface-reconstruction-cvpr-2023">Neuralangelo - High-Fidelity Neural Surface Reconstruction (CVPR 2023)</h2> <h4 id="zhaoshuo-li-thomas-müller-alex-evans-russell-h-taylor-mathias-unberath-ming-yu-liu-chen-hsuan-lin">Zhaoshuo Li, Thomas Müller, Alex Evans, Russell H. Taylor, Mathias Unberath, Ming-Yu Liu, Chen-Hsuan Lin</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/pdf/2306.03092">https://arxiv.org/pdf/2306.03092</a><br/> project website :<br/> <a href="https://research.nvidia.com/labs/dir/neuralangelo/">https://research.nvidia.com/labs/dir/neuralangelo/</a><br/> code :<br/> <a href="https://github.com/nvlabs/neuralangelo">https://github.com/nvlabs/neuralangelo</a><br/> reference :<br/> NeRF and 3DGS Study</p> </blockquote> <h3 id="abstract">Abstract</h3> <ul> <li> <p>multi-resolution hash grid representation<br/> with SDF-based volume rendering<br/> (3D surface recon.)</p> </li> <li> <p>no need for auxiliary data like segmentation or depth</p> </li> <li> <p>Novelty :</p> <ul> <li>numerical gradient (backpropagation locality 문제 해결)</li> <li>coarse-to-fine (점점 high resol.)</li> </ul> </li> </ul> <h3 id="related-works">Related Works</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-30-Neuralangelo/1-480.webp 480w,/assets/img/2024-09-30-Neuralangelo/1-800.webp 800w,/assets/img/2024-09-30-Neuralangelo/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-30-Neuralangelo/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Instant NGP <d-cite key="InstantNGP">[1]</d-cite> <a href="https://nvlabs.github.io/instant-ngp/">Link</a> :<br/> 모든 좌표(pixel) 각각에 대해 deep MLP output을 구하면 연산량이 너무 크므로<br/> 연산량 감소 및 speed-up 위해<br/> <code class="language-plaintext highlighter-rouge">Hash Grid</code>(연산량 감소)와 <code class="language-plaintext highlighter-rouge">Linear Interpolation</code>(continuous 보장)을 이용한<br/> 좌표 encoding 기법 제시 <ul> <li>STEP 1)<br/> \(d\)-dim. scene일 때<br/> input 좌표 \(x\) 가 주어졌을 때<br/> grid level별로 주위 \(2^d\)-개 좌표 선택 <ul> <li>multi-resolution (grid-level \(l\)) :<br/> \(N_l = \lfloor N_{min} \cdot b^l \rfloor\)<br/> where \(b = e^{\frac{\text{ln} N_{max} - \text{ln} N_{min}}{L-1}}\)<br/> MLP size가 작더라도 multi-resol. 덕분에 high approx. power 가짐</li> <li>주위 좌표 선택 :<br/> \(N_l\) 만큼 scale된 좌표 계산<br/> \(\lfloor x_l \rfloor = \lfloor x \cdot N_l \rfloor\)<br/> \(\lceil x_l \rceil = \lceil x \cdot N_l \rceil\)</li> </ul> </li> <li>STEP 2)<br/> 선택한 각 좌표에 대해 HashKey를 계산한 뒤 HashTable에서 Value 읽어옴 <ul> <li>HashKey :<br/> grid-level 마다 1개씩 HashTable이 정의되며<br/> Spatial Hash Function(2003)에 의해<br/> HashKey \(h(x) = (\text{XOR}_{i=1}^{d} x_i \pi_{i})\ \text{mod}\ T \in [0, T-1]\)<br/> where \(d\) : dim., \(\pi\) : dim.마다 임의로 정해둔 constant, \(T\) : Hash Table Size</li> <li>HashValue :<br/> \(T \times F\) 의 HashTable로부터 \(F\)-dim. feature vector인 HashValue를 얻음</li> </ul> </li> <li>STEP 3)<br/> 주위 좌표까지의 거리를 기반으로<br/> HashValue들을 Linear Interpolation(weighted sum)하여<br/> grid-level 별로 1개의 feature vector로 만듬</li> <li>STEP 4)<br/> 각 grid-level 별 feature vectors와 auxiliary 값(e.g. view direction)을 concat하여<br/> 최종 feature vector 만듬</li> <li>STEP 5)<br/> shallow MLP 통과</li> <li>STEP 6)<br/> Backpropagation :<br/> MLP weight와 Hash Table의 Value(\(F\)-dim. feature vector) 업데이트</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-30-Neuralangelo/2-480.webp 480w,/assets/img/2024-09-30-Neuralangelo/2-800.webp 800w,/assets/img/2024-09-30-Neuralangelo/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-30-Neuralangelo/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="numerical-gradient">Numerical Gradient</h3> <ul> <li>Gradient : <ul> <li>analytical gradient : \(\nabla f(x_i) = \frac{\partial f(x_i)}{\partial x_i}\)</li> <li>numerical gradient : \(\text{lim}_{\epsilon_{x} \rightarrow 0} \frac{f(x_i + \epsilon_{x}) - f(x_i - \epsilon_{x})}{2\epsilon_{x}}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-30-Neuralangelo/4-480.webp 480w,/assets/img/2024-09-30-Neuralangelo/4-800.webp 800w,/assets/img/2024-09-30-Neuralangelo/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-30-Neuralangelo/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Analytical Graident </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-30-Neuralangelo/5-480.webp 480w,/assets/img/2024-09-30-Neuralangelo/5-800.webp 800w,/assets/img/2024-09-30-Neuralangelo/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-30-Neuralangelo/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Numerical Gradient </div> <ul> <li>Instant NGP <d-cite key="InstantNGP">[1]</d-cite> 에서처럼<br/> input coordinate encode하기 위해<br/> multi-resolution hash grid representation 사용 <ul> <li>문제 :<br/> SDF-based volume rendering에 multi-resolution hash grid를 직접적으로 적용하면<br/> large smooth regions의 surface에 noise 및 hole이 생김</li> <li>이유 : <ul> <li>surface recon.에서 RGB(color) 및 SDF(geometry)를 MLP output으로 얻는데<br/> surface regularization loss를 구할 때 higher-order derivatives of SDF 계산해야 함 <ul> <li>first-order derivative of SDF \(f(x_i)\) :<br/> Eikonal constraints on the surface normals 계산<br/> \(L_{eik} = \frac{1}{N} \sum_{i=1}^N (\| \nabla f(x_i) \| - 1)^2\)</li> <li>second-order derivative of SDF \(f(x_i)\) :<br/> surface curvatures 계산<br/> \(L_{curv} = \frac{1}{N} \sum_{i=1}^N | \nabla^{2} f(x_i) |\)</li> </ul> </li> <li>그리고 이러한 SDF의 higher-order derivatives 계산하기 위해<br/> <code class="language-plaintext highlighter-rouge">analytical gradient</code> \(\nabla f(x_i) = \frac{\partial f(x_i)}{\partial x_i}\) 사용</li> <li>근데, analytical gradient 사용하면<br/> <code class="language-plaintext highlighter-rouge">only backpropagate to local cell</code>의 HashValues<br/> (locality 문제 발생!)</li> <li>특히 recon.할 surface가 multiple grid cells에 걸쳐 있을 경우<br/> analytical gradient를 사용하면<br/> adjacent cells는 업데이트 안 됨</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-30-Neuralangelo/3-480.webp 480w,/assets/img/2024-09-30-Neuralangelo/3-800.webp 800w,/assets/img/2024-09-30-Neuralangelo/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-30-Neuralangelo/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig a. Analytical Gradient from local cell </div> <ul> <li>Instant NGP <d-cite key="InstantNGP">[1]</d-cite> 에서처럼<br/> input coordinate encode하기 위해<br/> multi-resolution hash grid representation 사용 <ul> <li>해결 : <ul> <li>SDF의 higher-order derivatives 계산하기 위해<br/> numerical gradient \(\text{lim}_{\epsilon \rightarrow 0} \frac{f(x_i + \epsilon) - f(x_i - \epsilon)}{2\epsilon}\) 사용</li> <li><code class="language-plaintext highlighter-rouge">forward pass</code>에서 rendering하기 위해 (<code class="language-plaintext highlighter-rouge">recon. loss</code> 구하기 위해) SDF 계산할 때는<br/> sampled point 1개만 사용</li> <li><code class="language-plaintext highlighter-rouge">regularization loss</code> 구하기 위해 SDF의 higher-order derivatives 계산할 때는<br/> adjacent cells의 SDF까지 이용하는 numerical gradient를 사용함으로써<br/> <code class="language-plaintext highlighter-rouge">backward pass</code>에서 <code class="language-plaintext highlighter-rouge">backpropagate to adjacent cells</code></li> <li>adjacent 6개의 cells \(x_i \pm \epsilon\) 각각에 대해 trilinear sampling으로 SDF 값 계산하고<br/> 그 차이를 이용해서 <code class="language-plaintext highlighter-rouge">numerical gradient</code> 계산<br/> 이는 backward pass에 이용</li> <li>local cell \(x_i\) 로만 backpropagate하는 게 아니라<br/> 주위 6개의 cells \(x_i \pm \epsilon\) 으로 backpropagate하므로<br/> <code class="language-plaintext highlighter-rouge">smoothing</code> on SDF 역할 수행</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-30-Neuralangelo/6-480.webp 480w,/assets/img/2024-09-30-Neuralangelo/6-800.webp 800w,/assets/img/2024-09-30-Neuralangelo/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-30-Neuralangelo/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig b. Numerical Gradient from adjacent cells </div> <h3 id="coarse-to-fine">Coarse-to-Fine</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-30-Neuralangelo/7-480.webp 480w,/assets/img/2024-09-30-Neuralangelo/7-800.webp 800w,/assets/img/2024-09-30-Neuralangelo/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-30-Neuralangelo/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Coarse-to-Fine :<br/> 점점 hash grid encoding resol. \(N_l\) 증가시키고<br/> 이에 맞춰서 numerical gradient의 step size \(\epsilon\) 감소시킴</li> </ul> <h3 id="code">Code</h3> <ul> <li> <p>.yaml 로부터 config Dictionary 만들기 <a href="https://github.com/NVlabs/neuralangelo/blob/main/imaginaire/config.py">Code</a></p> </li> <li> <p>.yaml에 적어놓은 module을 동적으로 읽어와서 해당 module 내 class 사용하기 <a href="https://github.com/NVlabs/neuralangelo/blob/main/imaginaire/trainers/utils/get_trainer.py">Code</a></p> </li> <li> <p>Train 껍질 <a href="https://github.com/NVlabs/neuralangelo/blob/main/train.py">Code</a></p> </li> <li> <p>Trainer <a href="https://github.com/NVlabs/neuralangelo/blob/main/projects/neuralangelo/trainer.py">Code</a> \(\rightarrow\) overriding \(\rightarrow\) <a href="https://github.com/NVlabs/neuralangelo/blob/main/imaginaire/trainers/base.py">Code</a></p> </li> <li> <p>각종 함수 계산하는 utils <a href="https://github.com/NVlabs/neuralangelo/tree/main/projects/neuralangelo/utils">Code</a></p> </li> </ul> <h3 id="question">Question</h3> <ul> <li> <p>Q1 :<br/> analytical gradient에 비해 numerical gradient가 갖는 장점을 정리해서 알려주세요</p> </li> <li>A1 : <ul> <li>numerical gradient는<br/> point 하나만 sampling해도<br/> 그 주위의 여러 samples’ feature까지 다룰 수 있음</li> <li>gradient 하나가 얼마나 넓은 범위에 영향을 미치는지에 따라 sample efficiency가 결정되고 학습의 효율성이 결정됨<br/> continuous surface 상황에서는 하나의 error에서 나오는 gradient가 여러 군데에 영향을 동시에 미치는 것이 적합함<br/> 사실 forward pass에서 많은 points를 aggregate(또는 blur)하면 analytical gradient로도 backpropagation이 여러 군데에 퍼지게 할 수 있다<br/> 하지만 그러면 forward 쪽이 blur해지면서 frequency bound가 생기고, 속도가 느려짐<br/> 따라서 forward pass 쪽은 건들지 않고 backward pass 쪽만 건드려서 (numerical gradient for regularization loss)<br/> backpropagation이 여러 군데에 퍼지게 함</li> </ul> </li> <li> <p>Q2 :<br/> analytical gradient 대신 numerical gradient 쓰기 위해 adjacent cells’ SDF까지 계산하려면 performance 상승하긴 하지만 느려지지 않나요?</p> </li> <li>A2 :<br/> Instant-NGP의 Hash Grid 방식 자체가 빨라서 ㄱㅊ<br/> 내 피셜로는 regularization loss 구할 때만 adjacent cells’ SDF 이용하므로 inference rendering speed는 그대로라서 training speed 저하 미비</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="3d"/><category term="surface"/><category term="hash"/><category term="grid"/><category term="numerical"/><category term="gradient"/><summary type="html"><![CDATA[High-Fidelity Neural Surface Reconstruction (CVPR 2023)]]></summary></entry><entry><title type="html">TensoRF</title><link href="https://semyeong-yu.github.io/blog/2024/TensoRF/" rel="alternate" type="text/html" title="TensoRF"/><published>2024-09-17T12:00:00+00:00</published><updated>2024-09-17T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/TensoRF</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/TensoRF/"><![CDATA[<h2 id="tensorf-tensorial-radiance-fields">TensoRF: Tensorial Radiance Fields</h2> <h4 id="anpei-chen-zexiang-xu-andreas-geiger-jingyi-yu-hao-su">Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, Hao Su</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2203.09517">https://arxiv.org/abs/2203.09517</a><br/> project website :<br/> <a href="https://apchenstu.github.io/TensoRF/">https://apchenstu.github.io/TensoRF/</a><br/> code :<br/> <a href="https://github.com/apchenstu/TensoRF">https://github.com/apchenstu/TensoRF</a><br/> referenced blog :<br/> <a href="https://xoft.tistory.com/42">https://xoft.tistory.com/42</a></p> </blockquote> <h2 id="abstract">Abstract</h2> <ul> <li> <p>Radiance Field (Scene)에 대해 Tensor Decomposition을 적용해보자!</p> </li> <li> <p>fast training and less computational cost</p> </li> </ul> <h2 id="tensor-decomposition">Tensor Decomposition</h2> <ul> <li> <p>외적 (outer product) :<br/> \(\begin{bmatrix} 1 &amp; 2 \\ 2 &amp; 4 \\ 4 &amp; 8 \end{bmatrix} = \begin{bmatrix} 1 \\ 2 \\ 4 \end{bmatrix} \circ \begin{bmatrix} 1 &amp; 2 \end{bmatrix}\)<br/> 위의 예시는<br/> shape (3, 2) matrix를<br/> shape (3,) vector와 shape (2,) vector의 외적으로 표현</p> </li> <li> <p>Tensor Decomposition :</p> <ul> <li>\(n\)-dim.의 data를 \(n\)개의 1D vector들의 외적으로 표현할 수 있다!<br/> 이 때, 정보 손실이 발생할 수 있으므로<br/> \(R\) 개의 rank에 대해 외적들을 더해 \(n\)-dim. data를 근사</li> <li>장점 :<br/> 고차원 data를 1D vector들로 표현할 수 있으므로<br/> speed 개선</li> <li>단점 :<br/> 수많은 1D vector들로 표현하므로<br/> GPU memory 많이 소요</li> <li>종류 :<br/> CP(CANDECOMP/PARAFAC) decomposition<br/> Tucker Decomposition<br/> Block Term Decomposition</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/1-480.webp 480w,/assets/img/2024-09-17-TensoRF/1-800.webp 800w,/assets/img/2024-09-17-TensoRF/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> reference : https://www.kolda.net/publication/TensorReview.pdf </div> <ul> <li>Tensor Decomposition w. Trilinear Interpolation :<br/> interpolation으로 1D vector A와 B의 길이를 증가시키고<br/> 그 값으로 원본 matrix 표현</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/2-480.webp 480w,/assets/img/2024-09-17-TensoRF/2-800.webp 800w,/assets/img/2024-09-17-TensoRF/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> https://xoft.tistory.com/42 </div> <h2 id="tensorf-tensor-decomposition">TensoRF Tensor Decomposition</h2> <h3 id="cpcandecompparafac-decomposition">CP(CANDECOMP/PARAFAC) Decomposition</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/3-480.webp 480w,/assets/img/2024-09-17-TensoRF/3-800.webp 800w,/assets/img/2024-09-17-TensoRF/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> https://xoft.tistory.com/42 </div> <ul> <li>shape (i, j, k)의 \(T\) 에 대해<br/> \(T = \sum_{r=1}^R v_{r}^1 \circ v_{r}^2 \circ v_{r}^3\)</li> </ul> <h3 id="vmvector-matrix-decomposition">VM(vector-matrix) Decomposition</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/4-480.webp 480w,/assets/img/2024-09-17-TensoRF/4-800.webp 800w,/assets/img/2024-09-17-TensoRF/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> https://xoft.tistory.com/42 </div> <ul> <li>shape (i, j, k)의 \(T\) 에 대해<br/> \(T = \sum_{r=1}^{R_1} v_{r}^1 \circ M_{r}^{2,3} + \sum_{r=1}^{R_2} v_{r}^2 \circ M_{r}^{1,3} + \sum_{r=1}^{R_3} v_{r}^3 \circ M_{r}^{1,2}\)</li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li>Grid-based 연구들이 training speed 높이는 데 많은 기여를 하고 있으니<br/> 다른 논문들도 한 번 읽어보자 <ul> <li>Plenoxel (CVPR 2022)</li> <li>Instant-NGP (SIGGRAPH 2022)</li> <li>DVGO (CVPR 2022)</li> </ul> </li> <li>Grid-based 연구들 <ul> <li>장점 : speed 개선</li> <li>단점 : 해상도가 증가하면 GPU memory 많이 소요<br/> 기존 연구들은 space complexity \(O(N^3)\) 인데,<br/> TensoRF는 이를 \(O(N^2)\) 으로 줄임</li> </ul> </li> </ul> <h2 id="algorithm">Algorithm</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/5-480.webp 480w,/assets/img/2024-09-17-TensoRF/5-800.webp 800w,/assets/img/2024-09-17-TensoRF/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Algorithm : <ul> <li>step 1)<br/> scene을 bounded cubic (grid)로 제한</li> <li>step 2)<br/> ray를 쏴서 sampled points를 구한 뒤<br/> 각 rank의 선과 면으로 projection하고<br/> 외적한 값들을 이용해서<br/> color와 volume density 계산</li> <li>step 2-1)<br/> volume density는 단순히 외적한 값들을 더해서 구함<br/> (VM Decomposition)</li> <li>step 2-2)<br/> color는 외적한 값들을 concat한 뒤<br/> function B와 function S에 통과시켜 얻음 <ul> <li>function B :<br/> 1개의 FC-layer<br/> appearance commonalities를 추상화는 Global Apperance Dictionary 역할</li> <li>function S :<br/> MLP 또는 SH(Spherical Harmonics) 함수</li> </ul> </li> </ul> </li> </ul> <h2 id="loss">Loss</h2> <ul> <li>sparse input images일 경우<br/> 적게 관측된 view에서는 outlier 혹은 noise가 발생할 수 있어<br/> overfitting 혹은 local minima 문제 발생<br/> \(\rightarrow\)<br/> regularization term 추가한 loss 사용<br/> e.g. TV(total variation) loss :<br/> pixel 값 간의 급격한 변화 (noise or outlier)를 억제하기 위해<br/> \(I_{i+1, j} - I_{i, j}\) 항과 \(I_{i, j+1} - I_{i, j}\) 항을 loss에 추가</li> </ul> <h2 id="coarse-to-fine">Coarse-to-Fine</h2> <ul> <li> <p>NeRF의 coarse-to-fine 기법 :<br/> \(w_i = T_i \alpha_{i}\) 의 PDF 분포에 따라<br/> 일부 구간을 더 많이 sampling</p> </li> <li>Mip-NeRF 360의 coarse-to-fine 기법 : <ul> <li>small coarse proposal-MLP는 many samples로 여러 번 evaluate하여 weight \(\hat w\) 를 구하고<br/> large fine NeRF-MLP는 less samples로 딱 한 번 evaluate하여 weight \(w\) 와 color \(c\) 를 구함</li> <li>proposal loss를 이용하여 NeRF-MLP의 지식을 proposal-MLP가 따라잡도록 함</li> </ul> </li> <li>TensoRF의 coarse-to-fine 기법 :<br/> 단순히 grid 크기를 upsampling<br/> Grid size(resolution)이 커질수록 선 또는 면이 더 촘촘해져서 3D scene의 high-freq. feature를 더 잘 잡아낼 수 있음</li> </ul> <h2 id="implementation">Implementation</h2> <ul> <li>Decomposition Rank : 총 48개 <ul> <li>RGB : 16, 4, 4</li> <li>volume density : 16, 4, 4</li> </ul> </li> <li> <p>Grid size : coarse-to-fine<br/> \(128^3\) 에서 \(300^3\) 으로 점점 증가시키면서 학습<br/> (2000, 3000, 4000, 5500, 7000 step에서 점차 증가시킴)</p> </li> <li> <p>Batch size : 4096 pixels</p> </li> <li>Adam optimizer, V100 GPU(16GB)</li> </ul> <h2 id="evaluation">Evaluation</h2> <ul> <li>기존 Grid-based 연구들과<br/> training speed는 유사하지만<br/> PSNR이 높고<br/> 모델 사이즈 및 GPU memory 사용량이 적음</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/6-480.webp 480w,/assets/img/2024-09-17-TensoRF/6-800.webp 800w,/assets/img/2024-09-17-TensoRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/7-480.webp 480w,/assets/img/2024-09-17-TensoRF/7-800.webp 800w,/assets/img/2024-09-17-TensoRF/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Ours-VM-192 : 4DGS, VM Decomposition, 192개의 rank <ul> <li>speed, PSNR : Ours-VM-192를 15000 iter.만큼만 진행했을 때 8분만에 기존 연구들보다 PSNR 높음</li> <li>memory : 기존 연구들보다 확연히 memory size 적음</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/8-480.webp 480w,/assets/img/2024-09-17-TensoRF/8-800.webp 800w,/assets/img/2024-09-17-TensoRF/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/9-480.webp 480w,/assets/img/2024-09-17-TensoRF/9-800.webp 800w,/assets/img/2024-09-17-TensoRF/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Param. 실험 : <ul> <li>Grid size가 증가할수록 성능 좋아지지만 speed 느려지고 model size 커짐</li> <li>CP Decomposition보다 VM Decomposition이 성능 더 좋음</li> <li>rank 개수가 증가할수록 성능 좋아짐</li> </ul> </li> <li>iter. : <ul> <li>iter.이 증가할수록 PSNR이 증가<br/> 5k iter.만 해도 PSNR이 30에 가까워지고, 점점 변동폭이 작아짐</li> </ul> </li> </ul> <h2 id="limitation">Limitation</h2> <ul> <li>scene을 bounded cubic 안에 제한해야<br/> projection을 통해 VM Decomposition이 가능하므로<br/> unbounded scene은 다루지 못함</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="radiance"/><category term="field"/><category term="tensor"/><category term="decomposition"/><summary type="html"><![CDATA[Tensorial Radiance Fields (ECCV 2022)]]></summary></entry></feed>