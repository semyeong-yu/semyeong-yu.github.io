<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://semyeong-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://semyeong-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-23T12:09:01+00:00</updated><id>https://semyeong-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">CUDA Programming</title><link href="https://semyeong-yu.github.io/blog/2024/CUDA/" rel="alternate" type="text/html" title="CUDA Programming"/><published>2024-11-23T12:00:00+00:00</published><updated>2024-11-23T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/CUDA</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/CUDA/"><![CDATA[<h2 id="cuda-programming">CUDA Programming</h2> <blockquote> <p>reference :<br/> <a href="https://www.youtube.com/watch?v=n6M8R8-PlnE&amp;t=557s">How CUDA Programming Works</a><br/> <a href="https://www.youtube.com/watch?v=ot1wyQCutSA&amp;list=PLTgRMOcmRb3O5Xc8PJckYdbyCr5HPGx4e">Learning CUDA 10 Programming</a><br/> NeRF and 3DGS Study</p> </blockquote> <h3 id="introduction-to-cuda">Introduction to CUDA</h3> <ul> <li>API : <ul> <li>Drive API :<br/> low-level API (not conveninent)<br/> shipped along with display driver</li> <li>Runtime API :<br/> high-level API (with nvcc)<br/> included with the CUDA toolkit</li> <li>standard CUDA API functions :<br/> \(\#\) include \(&lt;\) cuda_runtime_api.h \(&gt;\)</li> <li>Driver API ver.이 Runtime API ver.보다 최신꺼여야 함</li> </ul> </li> <li>Execution Model : <ul> <li>Kernel :<br/> executed by N CUDA threads in parallel<br/> threads \(\rightarrow\) blocks \(\rightarrow\) grid</li> <li>Hardware Architecture :<br/> 스펙은 CUDA Capability version number로 확인 가능 <ul> <li>Streaming multiprocessors (SMs) :<br/> 각 GPU 당 여러 SMs (global memory 공유)<br/> 각 SM 당 여러 CUDA cores (cache, registers 공유)<br/> 예측 (branch prediction 또는 speculative execution) 없음</li> <li>SIMT architecture (Single-Instruction-Multiple-Thread) :</li> <li>warp :<br/> 32 threads를 그룹지어서 scheduling한 걸 하나의 unit으로 run<br/> warp에 필요한 execution context는 lifetime 내내 SM에 있기 때문에 switching warps 위한 overhead 없어서 좋음</li> <li>running a Kernel :<br/> available SM에 blocks 할당<br/> \(\rightarrow\) block을 warps로 split<br/> \(\rightarrow\) SM 당 multiple warps on block 실행<br/> \(\rightarrow\) block 실행이 끝나면 SM을 free시키고 new block을 scheduling until entire grid is done</li> </ul> </li> </ul> </li> </ul> <h3 id="performance-optimization">Performance Optimization</h3> <ul> <li>NVIDIA Visual Profiler in CUDA toolkit :<br/> Hardware performance counters on GPU (admin user <a href="https://developer.nvidia.com/nvidia-development-tools-solutions-err_nvgpuctrperm-permission-issue-performance-counters">Link</a> 자격 있어야 접근 가능) 이용해서<br/> device code와 CUDA API calls를 추적</li> </ul> <h3 id="parallel-algorithm">Parallel Algorithm</h3> <h3 id="gpu-accelerated-libraries">GPU Accelerated Libraries</h3> <h3 id="advanced-cuda-topics">Advanced CUDA Topics</h3> <h3 id="summary">Summary</h3>]]></content><author><name></name></author><category term="others"/><category term="radiance"/><category term="field"/><category term="tensor"/><category term="decomposition"/><summary type="html"><![CDATA[.cu coding]]></summary></entry><entry><title type="html">Structure-from-Motion Revisited (COLMAP)</title><link href="https://semyeong-yu.github.io/blog/2024/Colmap/" rel="alternate" type="text/html" title="Structure-from-Motion Revisited (COLMAP)"/><published>2024-11-23T11:00:00+00:00</published><updated>2024-11-23T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Colmap</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Colmap/"><![CDATA[<h2 id="structure-from-motion-revisited">Structure-from-Motion Revisited</h2> <h4 id="johannes-l-schonberger-jan-michael-frahm">Johannes L. Schonberger, Jan-Michael Frahm</h4> <blockquote> <p>paper :<br/> <a href="chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://openaccess.thecvf.com/content_cvpr_2016/papers/Schonberger_Structure-From-Motion_Revisited_CVPR_2016_paper.pdf">chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://openaccess.thecvf.com/content_cvpr_2016/papers/Schonberger_Structure-From-Motion_Revisited_CVPR_2016_paper.pdf</a><br/> referenced blog :<br/> <a href="https://xoft.tistory.com/88">https://xoft.tistory.com/88</a></p> </blockquote> <h3 id="colmap">COLMAP</h3> <ul> <li>COLMAP :<br/> SfM (Structure from Motion)과 MVS (Multi-View Stereo)를 수행하는 library</li> <li><code class="language-plaintext highlighter-rouge">SfM</code> : <ul> <li>input : images</li> <li>output : camera parameter(intrinsic, extrinsic), 3D point cloud</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">MVS</code> : <ul> <li>input : SfM의 output</li> <li>output : 3D model reconstruction</li> </ul> </li> </ul> <h3 id="sfm-history">SfM History</h3> <ul> <li> <p>SfM 시초 :<br/> “3d model acquisition from extended image sequences”, 1996.<br/> “Structure from motion without correspondence”, CVPR, 2000.<br/> “Automatic camera recovery for closed or open image sequences”, ECCV, 1998.<br/> “Relative 3d reconstruction using multiple uncalibrated images”, IJR, 1995.<br/> “Visual modeling with a hand-held camera”, IJCV, 2004</p> </li> <li> <p>internet images로 3D reconstruction 수행 :<br/> “Multi-view matching for unordered image sets, or How do I organize my holiday snaps?”, ECCV, 2002.<br/> “Photo tourism: exploring photo collections in 3d”, ACM TOG, 2006.<br/> “Detailed real-time urban 3d reconstruction from video”, IJCV, 2008.</p> </li> <li>input images 수 늘리는 연구 : <ul> <li>수천장 처리 : “Building rome in a day”, ICCV, 2009.</li> <li>수백만장 처리 : “Building Rome on a Cloudless Day”, ECCV, 2010.<br/> “Towards linear-time incremental structure from motion”, 3DV, 2013.<br/> “From Single Image Query to Detailed 3D Reconstruction”, CVPR, 2015.<br/> “From Dusk Till Dawn: Modeling in the Dark”, CVPR, 2016.</li> <li>수억장 처리 : “Reconstructing the World* in Six Days *(As Captured by the Yahoo 100 Million Image Dataset)”, CVPR, 2015.</li> </ul> </li> <li>SfM 연구 발전 방향 :<br/> incremental, hierarchical, global 으로 총 3가지 발전 방향이 있었고,<br/> 그 중 images를 sequentially 처리하는 incremental SfM이 가장 인기 있었지만<br/> robustness, accuracy, completeness, scalability 관점에서 general SfM을 만들기 어려웠음<br/> \(\rightarrow\) 근데 이를 본 논문 (COLMAP)에서 해결!!</li> </ul> <h3 id="incremental-sfm">Incremental SfM</h3> <ul> <li> <p>images</p> </li> <li>correspondence search <ul> <li><code class="language-plaintext highlighter-rouge">feature extraction</code> :<br/> image마다 geometric-radiometric-invariant feature 추출<br/> e.g. SIFT <a href="https://velog.io/@everyman123/SIFT-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0">link</a> (scale-rotation-invariant feature extraction)<br/> e.g. SURF, HOG, BRIEF, ORB 등 <a href="https://ggommappooh.tistory.com/entry/%EC%BB%B4%ED%93%A8%ED%84%B0-%EB%B9%84%EC%A0%84-Feature-Descriptors">link</a></li> <li><code class="language-plaintext highlighter-rouge">matching</code> :<br/> extracted feature를 바탕으로 2 images에서 같은 scene part 찾기<br/> image마다 feature를 비교하므로 time complexity \(O(N_I^2 N_F^2)\)<br/> e.g. “Building Rome in a day”, ICCV, 2009.<br/> e.g. “Building Rome on a Cloudless Day”, ECCV, 2010.<br/> e.g. “Vocmatch: Efficient multiview correspondence for structure from motion”, ECCV, 2014.<br/> e.g. “Reconstructing the World in Six Days (As Captured by the Yahoo 100 Million Image Dataset)”, CVPR, 2015.<br/> e.g. “MatchMiner: Efficient Spanning Structure Mining in Large Image Collections”, ECCV, 2012.<br/> e.g. “PAIGE: PAirwise Image Geometry Encoding for Improved Efficiency in Structure-from-Motion”, CVPR, 2015.<br/> e.g. “Towards linear-time incremental structure from motion”, 3DV, 2013.<br/> e..g CNN-based SuperGlue(2019)<br/> e.g. ViT-based LoFTR(2021)</li> <li><code class="language-plaintext highlighter-rouge">geometric verification</code> : <ul> <li>matching 결과를 보장하기 위한 검증 과정 (걸러냄)</li> <li>GRIC과 같은 기법을 통해 어떤 geometry model로 검증할 지 결정<br/> geometry model 예시 : Fundametal Matrix, Trifocal Tensor, Projective Matrix, Calibration Matrix, Rigid Transformation, Affine Transformation</li> <li>Epipolar Geometry로 검증하는 예시 : Fundamental Matrix로 relative camera pose를 추정한 뒤 camera1의 feature points를 camera2로 projection 했을 때 matching points로 잘 mapping 되는지 검증<br/> Fundamental Matrix 계산하기 위해 eight-point, five-point, RANSAC, LMedS, QDEGSAC 등의 기법 사용 가능</li> <li>output : scene graph (verified image pair 및 correpondence map)</li> </ul> </li> </ul> </li> <li>incremental reconstruction <ul> <li><code class="language-plaintext highlighter-rouge">initialization</code> :<br/> 최초로 등록한 2개의 images 선택 (매우 중요한 단계임)<br/> 여러 camera로부터 많이 overlap되는 scene을 가진 images로 선택하면, Bundle Adjustment 단계에서 overlap part가 반복적으로 최적화되면서 reconstruction 성능이 높아지지만 연산 시간도 늘어남</li> <li><code class="language-plaintext highlighter-rouge">image registration</code> : <ul> <li>camera pose를 world-coordinate에 등록한다</li> <li>initial 2 images의 경우 : fundamental matrix를 알고 있으므로 intrinsic param.와 extrinsic param. (camera pose)을 추정할 수 있다</li> <li>이후 images의 경우 : 이미 등록된 image들과의 feature correspondence와 PnP 알고리즘을 사용하여 intrinsic param.와 extrinsic param.를 추정할 수 있다</li> <li>PnP 알고리즘 : 3D points 위치와 projected 2D points 위치를 기반으로 camera pose를 추정</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">triangulation</code> :<br/> ddd</li> <li><code class="language-plaintext highlighter-rouge">bundle adjustment</code> :<br/> ddd</li> <li><code class="language-plaintext highlighter-rouge">outlier filtering</code> :<br/> RANSAC 및 minimal pose solver 등의 기법을 사용하여 outlier를 걸러냄<br/> RANSAC : dataset의 randomly selected points에 대해 Fundamental Matrix 등 geometry model을 추정한 뒤 해당 model이 dataset에 얼마나 잘 부합하는지를 반복적으로 검증</li> </ul> </li> <li>3D reconstruction</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="SfM"/><category term="pose"/><category term="3d"/><summary type="html"><![CDATA[SfM library]]></summary></entry><entry><title type="html">Plenoxels</title><link href="https://semyeong-yu.github.io/blog/2024/Plenoxels/" rel="alternate" type="text/html" title="Plenoxels"/><published>2024-11-23T11:00:00+00:00</published><updated>2024-11-23T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Plenoxels</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Plenoxels/"><![CDATA[<h2 id="plenoxels-radiance-fields-without-neural-networks">Plenoxels: Radiance Fields without Neural Networks</h2> <h4 id="alex-yu-sara-fridovich-keil-matthew-tancik-qinhong-chen-benjamin-recht-angjoo-kanazawa">Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, Angjoo Kanazawa</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2112.05131">https://arxiv.org/abs/2112.05131</a><br/> project website :<br/> <a href="https://alexyu.net/plenoxels/">https://alexyu.net/plenoxels/</a><br/> code :<br/> <a href="https://github.com/sxyu/svox2">https://github.com/sxyu/svox2</a></p> </blockquote> <h2 id="tbd">TBD</h2> <h3 id="tbd-1">TBD</h3>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="3d"/><category term="rendering"/><summary type="html"><![CDATA[Radiance Fields without Neural Networks (CVPR 2022 oral)]]></summary></entry><entry><title type="html">Surface Reconstruction</title><link href="https://semyeong-yu.github.io/blog/2024/SDF/" rel="alternate" type="text/html" title="Surface Reconstruction"/><published>2024-11-23T11:00:00+00:00</published><updated>2024-11-23T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/SDF</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/SDF/"><![CDATA[<h2 id="brief-review-on-surface-reconstruction-method">Brief Review on Surface Reconstruction Method</h2> <blockquote> <p>referece : NeRF and 3DGS Study</p> </blockquote> <h2 id="nerf-based-3d-surface-reconstruction">NeRF-based 3D Surface Reconstruction</h2> <h2 id="dvr">DVR</h2> <h3 id="tbd">TBD</h3> <ul> <li>MLP와 함께 사용할 수 있도록 Differentiable Volumetric Rendering 방법을 제시</li> </ul> <h2 id="idr">IDR</h2> <ul> <li>직접 말고 implicitly</li> <li>BRDF : 빛이 물체에 튕겨서 우리 눈에 들어올 때 얼만큼 정보를 넘기는지인데, 이를 포함한 rendering 식을 MLP M으로 implicitly 예측</li> <li>f : geometry를 설계, M : appearance를 설계, 둘을 잇기 위해 가운데 layer</li> <li>DVR과 IDR은 background가 noisy하면 surface 잘 못 찾아서 배경 없애는 mask 필요</li> <li>ray가 물체와 처음 만나는 점만 이용하는데 이를 UNISURF에서 해결</li> </ul> <h2 id="unisurf">UNISURF</h2> <ul> <li>DVR과 IDR을 합친 느낌</li> <li>optimization할수록 delta_k 가 점점 줄어듬</li> <li>IDR 식 그대로 사용</li> </ul> <h2 id="neus">NeuS</h2> <ul> <li>occupancy network 대신 SDF로 surface를 나타내자!<br/> SDF가 0이 되는 지점이 surface <ul> <li>IDR, NeuS, …가 SDF 사용</li> <li>eikonal loss 사용 가능해서 surface 잘 나타낼 수 있음</li> <li>PDF 정의 가능 <ul> <li>surface 근처에서 PDF 값이 크다</li> </ul> </li> <li>unbiased : 정확히 surface에서 weight가 locally maximal value를 가져야 한다</li> <li>occlusion-aware : multiple surface인 상황에서 view-point과 더 가까운 surface가 color에 더 많은 기여를 해야 한다</li> </ul> </li> </ul> <h2 id="volsdf">VolSDF</h2> <ul> <li>logistic density distribution 대신 Laplace 로 SDF 만듬</li> <li>opacity O를 CDF라 생각하면 이를 미분한 tau(t)는 PDF이고 이를 rendering에 사용</li> </ul> <h2 id="bakedsdf">BakedSDF</h2> <ul> <li>VolSDF + MipNeRF360</li> <li>VolSDF에서처럼 surface 찾기 -&gt; marching cube로 mesh extraction -&gt; spherical Gaussians로 appearance 표현</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="3d"/><category term="rendering"/><category term="surface"/><summary type="html"><![CDATA[Summary]]></summary></entry><entry><title type="html">MASt3R</title><link href="https://semyeong-yu.github.io/blog/2024/MASt3R/" rel="alternate" type="text/html" title="MASt3R"/><published>2024-11-21T12:00:00+00:00</published><updated>2024-11-21T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/MASt3R</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/MASt3R/"><![CDATA[<h2 id="grounding-image-matching-in-3d-with-mast3r">Grounding Image Matching in 3D with MASt3R</h2> <h4 id="vincent-leroy-yohann-cabon-jérôme-revaud">Vincent Leroy, Yohann Cabon, Jérôme Revaud</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2406.09756">https://arxiv.org/abs/2406.09756</a><br/> project website :<br/> <a href="https://europe.naverlabs.com/blog/mast3r-matching-and-stereo-3d-reconstruction/">https://europe.naverlabs.com/blog/mast3r-matching-and-stereo-3d-reconstruction/</a><br/> code :<br/> <a href="https://github.com/naver/mast3r">https://github.com/naver/mast3r</a><br/> reference :<br/> <a href="https://xoft.tistory.com/100">https://xoft.tistory.com/100</a></p> </blockquote> <h3 id="contribution">Contribution</h3> <ul> <li> <p>MVS(Multi-View Stereo) 분야에서는 일반적으로 camera param.를 알아야 해서<br/> SfM(Structure from Motion)을 사용해서 camera param. estimaton을 하지만<br/> 이는 많은 연산 필요</p> </li> <li> <p>DUSt3R :<br/> <code class="language-plaintext highlighter-rouge">SfM 생략</code>하고<br/> <code class="language-plaintext highlighter-rouge">regression-based</code><br/> <code class="language-plaintext highlighter-rouge">2D(img)-to-3D(point map) mapping network</code> 이용해서 3D recon. 수행</p> </li> </ul> <h3 id="image-matching">Image Matching</h3> <h3 id="dust3r-framework">DUSt3R Framework</h3> <h3 id="matching-prediction-head">Matching Prediction Head</h3> <h3 id="fast-reciprocal-matching">Fast Reciprocal Matching</h3> <h3 id="coarse-to-fine-matching">Coarse-to-Fine Matching</h3> <h3 id="experiments">Experiments</h3>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="point"/><category term="regression"/><category term="pose"/><category term="free"/><summary type="html"><![CDATA[Grounding Image Matching in 3D with MASt3R]]></summary></entry><entry><title type="html">DUSt3R</title><link href="https://semyeong-yu.github.io/blog/2024/DUSt3R/" rel="alternate" type="text/html" title="DUSt3R"/><published>2024-11-19T12:00:00+00:00</published><updated>2024-11-19T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/DUSt3R</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/DUSt3R/"><![CDATA[<h2 id="dust3r---geometric-3d-vision-made-easy-cvpr-2024">DUSt3R - Geometric 3D Vision Made Easy (CVPR 2024)</h2> <h4 id="shuzhe-wang-vincent-leroy-yohann-cabon-boris-chidlovskii-jerome-revaud">Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, Jerome Revaud</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2312.14132">https://arxiv.org/abs/2312.14132</a><br/> project website :<br/> <a href="https://europe.naverlabs.com/research/publications/dust3r-geometric-3d-vision-made-easy/">https://europe.naverlabs.com/research/publications/dust3r-geometric-3d-vision-made-easy/</a><br/> code :<br/> <a href="https://github.com/naver/dust3r">https://github.com/naver/dust3r</a><br/> reference :<br/> <a href="https://xoft.tistory.com/83">https://xoft.tistory.com/83</a></p> </blockquote> <h3 id="contribution">Contribution</h3> <ul> <li> <p>MVS(Multi-View Stereo) 분야에서는 일반적으로 camera param.를 알아야 해서<br/> SfM(Structure from Motion)을 사용해서 camera param. estimaton을 하지만<br/> 이는 많은 연산 필요</p> </li> <li> <p>DUSt3R :</p> <ul> <li><code class="language-plaintext highlighter-rouge">SfM 생략</code>하고<br/> <code class="language-plaintext highlighter-rouge">regression-based</code><br/> <code class="language-plaintext highlighter-rouge">2D(img)-to-3D(point map) mapping network</code> 이용해서 3D recon. 수행</li> <li>1번 view를 기준으로 2번 view의 3D points를 <code class="language-plaintext highlighter-rouge">상대적으로 align</code>하므로<br/> (3D point의 <code class="language-plaintext highlighter-rouge">절대적인 위치를 추정하는 게 아니므로</code>)<br/> intrinsic/extrinsic <code class="language-plaintext highlighter-rouge">camera param. 몰라도</code> ok</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/1-480.webp 480w,/assets/img/2024-11-19-DUSt3R/1-800.webp 800w,/assets/img/2024-11-19-DUSt3R/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="algorithm">Algorithm</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/2-480.webp 480w,/assets/img/2024-11-19-DUSt3R/2-800.webp 800w,/assets/img/2024-11-19-DUSt3R/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Algorithm : <ul> <li>Step 1) input<br/> image 2장</li> <li>Step 2) ViT encoder<br/> 두 images의 feature 비교하기 위해<br/> <code class="language-plaintext highlighter-rouge">Siamese</code> (shared weight) 구조 사용</li> <li>Step 3) Transformer decoder<br/> 두 features의 관계를 학습하여<br/> aligned pointmap 만들기 위해<br/> <code class="language-plaintext highlighter-rouge">self-attention and cross-attention</code> 수행</li> <li>Step 4) Head output<br/> per-pixel <code class="language-plaintext highlighter-rouge">Pointmap</code> \(X_{i}^{v, 1} \in R^{W \times H \times 3}\)<br/> and<br/> per-pixel <code class="language-plaintext highlighter-rouge">Confidence</code> score \(C_{i}^{v, 1} \in R^{W \times H}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/3-480.webp 480w,/assets/img/2024-11-19-DUSt3R/3-800.webp 800w,/assets/img/2024-11-19-DUSt3R/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>1번 camera : base view, 2번 camera : reference view<br/> \(G_{i}^{1}\) : 1번 view feature의 Transformer Decoder에서 \(i\)-th Block<br/> \(G_{i}^{2}\) : 2번 view feature의 Transformer Decoder에서 \(i\)-th Block</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/4-480.webp 480w,/assets/img/2024-11-19-DUSt3R/4-800.webp 800w,/assets/img/2024-11-19-DUSt3R/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Pointmap :<br/> \(X^{1, 1}\) : 1번 view 시점을 기준으로 1번 view에서 보이는 3D point 좌표<br/> \(X^{2, 1}\) : 1번 view 시점을 기준으로 2번 view에서 보이는 3D point 좌표</p> </li> <li>Confidence score :<br/> \(C_{i}^{v, 1}\) : 1번 view 시점을 기준으로 \(v\) 번 view에서 보이는 \(i\)-th 3D point의 confidence score <ul> <li>물체인 부분에서는 3D point를 비교적 정확히 예측할 수 있으므로 confidence가 높고,<br/> 하늘 또는 반투명인 부분에서는 3D point를 정확하게 예측할 수 없으므로 confidence가 낮게 나옴</li> <li>\(C_{i}^{v, 1} = 1 + \text{exp}(\tilde C_{i}^{v, 1}) \gt 1\) 로 설정하여<br/> 하나의 view에만 존재해서 추정하기 어려운 3D point의 경우에는 extrapolate할 수 있도록 <code class="language-plaintext highlighter-rouge">???</code></li> </ul> </li> <li>1번 view를 기준으로 2번 view의 3D points를 <code class="language-plaintext highlighter-rouge">상대적으로 align</code>하므로<br/> 3D point의 <code class="language-plaintext highlighter-rouge">절대적인 위치를 추정하는 게 아니므로</code><br/> intrinsic/extrinsic <code class="language-plaintext highlighter-rouge">camera param. 몰라도</code> ok</li> </ul> <h3 id="loss">Loss</h3> <ul> <li>regression loss :<br/> \(L_{regr} (v, i) = \| \frac{1}{z} X_{i}^{v, 1} - \frac{1}{\bar z} \bar X_{i}^{v, 1} \|\) <ul> <li>\(i\) : each point, \(v\) : each view</li> <li>\(z = \text{norm}(X^{1, 1}, X^{2, 1})\) : averaged depth of prediction point</li> <li>\(\bar z = \text{norm}(\bar X^{1, 1}, \bar X^{2, 1})\) : averaged depth of GT point</li> <li> <table> <tbody> <tr> <td>$$\text{norm}(X^{1, 1}, X^{2, 1}) = \frac{1}{</td> <td>D^{1}</td> <td>+</td> <td>D^{2}</td> <td>} \sum_{v \in { 1, 2 }} \sum_{i \in D^{v}} | X_{i}^{v, 1} |$$ : 모든 depth 값에 대한 평균</td> </tr> </tbody> </table> </li> </ul> </li> <li>final loss :<br/> \(L_{conf} = \sum_{v \in \{ 1, 2 \}} \sum_{i \in D^{v}} C_{i}^{v, 1} L_{regr}(v, i) - \alpha \text{log} C_{i}^{v, 1}\) <ul> <li>\(C_{i}^{v, 1} L_{regr}(v, i)\) :<br/> confidence가 큰 <code class="language-plaintext highlighter-rouge">(확실한) point</code>에서는 GT와의 <code class="language-plaintext highlighter-rouge">regression loss</code> \(L_{regr}\) 가 더 <code class="language-plaintext highlighter-rouge">작도록</code></li> <li>\(- \alpha \text{log} C_{i}^{v, 1}\) : regularization term<br/> <code class="language-plaintext highlighter-rouge">confidence</code> \(C_{i}^{v, 1}\) 값이 <code class="language-plaintext highlighter-rouge">너무 작아지지 않도록</code></li> </ul> </li> </ul> <h3 id="experiments">Experiments</h3> <ul> <li>Model<br/> CroCo pre-trained model 사용하여 weight initialization <ul> <li>encoder : ViT-Large</li> <li>decoder : ViT-Base</li> <li>head : DPT (ViT를 Depth Estimation에 적용한 연구)</li> </ul> </li> </ul> <h3 id="downstream---stereo-pixel-matching">Downstream - stereo pixel matching</h3> <ul> <li>2개의 image에 대한 pointmap을 겹쳤을 때 align되도록<br/> <code class="language-plaintext highlighter-rouge">pixel correspondence</code>를 찾음 <ul> <li>\(X^{2, k}\) 중에 3D point \(X_{i}^{1, k}\) 와 가장 가까운 3D point가 \(X_{j}^{2, k}\) 이고,<br/> \(X^{1, k}\) 중에 3D point \(X_{j}^{2, k}\) 와 가장 가까운 3D point가 \(X_{i}^{1, k}\) 일 때<br/> 두 pixel \(i, j\) 사이에 correspondence 있다고 함</li> <li>모든 pixel에 대해 correspondence가 생기지는 않음</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/5-480.webp 480w,/assets/img/2024-11-19-DUSt3R/5-800.webp 800w,/assets/img/2024-11-19-DUSt3R/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="downstream---camera-intrinsic-estimation">Downstream - camera intrinsic estimation</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/6-480.webp 480w,/assets/img/2024-11-19-DUSt3R/6-800.webp 800w,/assets/img/2024-11-19-DUSt3R/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>camera intrinsic :<br/> camera intrinsic을 추정한다는 것은<br/> 보통 sclaing matrix, 즉 focal length를 추정한다는 얘기임 <ul> <li>2D translation : principal point의 위치<br/> (보통 이미지의 정가운데)</li> <li>2D shear : 카메라가 기울어진 정도<br/> (보통 카메라는 기울어져 있지 않으므로 shear matrix는 고려 X)</li> <li>2D scaling : focal length</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/7-480.webp 480w,/assets/img/2024-11-19-DUSt3R/7-800.webp 800w,/assets/img/2024-11-19-DUSt3R/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>위의 수식 설명 : <ul> <li>focal length는 3D point를 2D image plane으로 projection시킬 때 사용됨<br/> Weiszfeld algorithm을 이용해서 2D 상에서 위의 반복 최적화 문제를 풀면<br/> 해당 optimal <code class="language-plaintext highlighter-rouge">focal length</code>를 가질 때 <code class="language-plaintext highlighter-rouge">2D image와 3D point가 align</code>됨</li> <li>camera-coordinate에서 최적화 수행<br/> where pixel-coordinate : 2D \((i, j) \in ([0, W], [0, H])\) (좌상단이 원점)<br/> where camera-coordinate : 2D $$i^{‘}, j^{‘} \in ([-\frac{W}{2}, \frac{W}{2}], [-\frac{H}{2}, \frac{H}{2}]) (정중앙이 원점)<br/> where world-coordinate : 3D</li> </ul> </li> </ul> <h3 id="downstream---camera-extrinsic-estimation">Downstream - camera extrinsic estimation</h3> <ul> <li>Relative Pose Estimation : <ul> <li>방법 1)<br/> 위에서 언급한 2D pixel matching과 intrinsic estimation을 수행한 뒤<br/> Eight-Point algorithm 등 이용해서<br/> epipolar(essential) matrix와 relative pose를 추정</li> <li>방법 2)<br/> 서로 다른 view 시점에서 보이는 3D pointmap이 동일해지도록<br/> SVD-based procrustes alignment algorithm 이용해 3D 상에서 반복 최적화 문제를 풀어서<br/> optimal rotation matrix \(R\), translation vector \(t\), scale factor \(\sigma\) 추정 <ul> <li>procrustes alignment algorithm은 noise 및 outlier에 민감하므로<br/> 주어진 3D point와 corresponding 2D point를 바탕으로 camera pose를 추정하는 PnP algorithm과<br/> random sampling 방식의 RANSAC (Random Sample Consensus) algorithm 이용해서 위 수식의 해를 찾음<br/> <code class="language-plaintext highlighter-rouge">?????</code></li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/8-480.webp 480w,/assets/img/2024-11-19-DUSt3R/8-800.webp 800w,/assets/img/2024-11-19-DUSt3R/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Absolute Pose Estimation (visual localization) : <ul> <li>방법 1)<br/> 위에서 언급한 2D pixel matching과 instrinsic estimation을 수행한 뒤<br/> PnP RANSAC algorithm 이용해서 optimal rotation matrix 및 translation vector 추정</li> <li>방법 2)<br/> GT pointmap을 이용<br/> 즉, 위에서 언급한 Relative Pose Estimation을 수행할 때<br/> 해당 GT로 scale을 맞춰서 진행</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/9-480.webp 480w,/assets/img/2024-11-19-DUSt3R/9-800.webp 800w,/assets/img/2024-11-19-DUSt3R/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Experiment on Absolute Pose Estimation : <ul> <li>test dataset :<br/> 7Scenes, Cambridge Landmark<br/> (training에 사용되지 않은 dataset)</li> <li>각 값은 translation error (cm) / rotation error (degree)</li> <li>방식 :<br/> query image가 주어지면<br/> 가장 관련 있는 image를 test dataset에서 찾아<br/> 2개 image 간의 pixel을 matching하여 Absolute Camera Pose 계산<br/> (근데 query image와 test dataset image 간의 GT camera pose가 존재하나 <code class="language-plaintext highlighter-rouge">???</code>)</li> <li>비교 :<br/> FM(feature matching 기법), E2E(end-to-end learning 기법)과 비교했을 때<br/> SOTA 성능은 아니지만<br/> DUSt3R이 visual localization을 목적으로 학습되지 않았는데도 오차가 작다는 것을 확인할 수 있음</li> </ul> </li> </ul> <h3 id="downstream---global-alignment">Downstream - Global Alignment</h3> <ul> <li>Global Alignment :<br/> 3장 이상의 images로부터 예측한 Pointmap을 3D space에서 align하는 방법 <ul> <li>여러 장의 images를 다루기 위해<br/> <code class="language-plaintext highlighter-rouge">Graph</code> 만듦 (각 image가 vertex이고, 같은 visual contents를 공유하고 있으면 edge)</li> <li>DUSt3R 이용해서<br/> 모든 edge pair에 대해 Pointmap \(X_{i}^{v, 1} \in R^{W \times H \times 3}\) 과 Confidence score \(C_{i}^{v, 1} \in R^{W \times H}\) 계산</li> <li>여러 장의 images로 3D 상에서 반복 최적화 문제 풀어서<br/> 여러 장의 images로부터 얻은 Pointmap들이 3D 상에서 align되도록 함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/10-480.webp 480w,/assets/img/2024-11-19-DUSt3R/10-800.webp 800w,/assets/img/2024-11-19-DUSt3R/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>위의 수식 설명 :<br/> 3D 상에서 위의 반복 최적화 문제를 풀어서<br/> optimal \(\xi_{i}^{v}, \sigma_{e}, P_{e}\) 얻으면<br/> \(N\) 개의 images로 얻은 \(N\) 개의 <code class="language-plaintext highlighter-rouge">3D Pointmap을 align</code>하여<br/> Global pointmap을 얻을 수 있음<br/> 코드 : <a href="https://github.com/naver/dust3r/blob/01b2f1d1e6c6c386f95a007406defb5b8a5d2042/dust3r/cloud_opt/optimizer.py">Code</a> <ul> <li>\(C_{i}^{v, e}\) : confidence score from DUSt3R prediction<br/> (image \(e\) 의 view 시점을 기준으로, image \(v\) view에서 보이는 \(i\)-th pixel에 대응되는 값)</li> <li>\(X_{i}^{v, e}\) : pointmap from DUSt3R prediction</li> <li>\(\xi_{i}^{v}\) : global pointmap in world-coordinate</li> <li>\(\sigma_{e}\) : edge로 연결되어 있는 2개 images 간의 scale factor<br/> (0이 되는 것을 방지하기 위해 \(\prod_{e} \sigma_{e} = 1\) 로 설계)</li> <li>\(P_{e}\) : edge로 연결되어 있는 2개 images 간의 relative pose</li> </ul> </li> <li>위의 방법은<br/> <code class="language-plaintext highlighter-rouge">전통적인 SfM bundle adjustment 방법과 달리</code><br/> <code class="language-plaintext highlighter-rouge">빠르고 단순하게 regression(gradient descent)-based</code>로 반복 최적화 문제 풂 <ul> <li>bundle adjustment :<br/> 2D reprojection error 최소화</li> <li>본 논문 :<br/> 2D reprojection 뿐만 아니라 3D projection error을 같이 최소화</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/13-480.webp 480w,/assets/img/2024-11-19-DUSt3R/13-800.webp 800w,/assets/img/2024-11-19-DUSt3R/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="downstream---depth-estimation">Downstream - Depth Estimation</h3> <ul> <li>Monocular Depth Estimation</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/11-480.webp 480w,/assets/img/2024-11-19-DUSt3R/11-800.webp 800w,/assets/img/2024-11-19-DUSt3R/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Multi-view Depth Estimation</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/12-480.webp 480w,/assets/img/2024-11-19-DUSt3R/12-800.webp 800w,/assets/img/2024-11-19-DUSt3R/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="downstream---dense-3d-reconstruction">Downstream - Dense 3D reconstruction</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/14-480.webp 480w,/assets/img/2024-11-19-DUSt3R/14-800.webp 800w,/assets/img/2024-11-19-DUSt3R/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="point"/><category term="regression"/><category term="pose"/><category term="free"/><summary type="html"><![CDATA[Geometric 3D Vision Made Easy (CVPR 2024)]]></summary></entry><entry><title type="html">Deblurring NeRF Summary</title><link href="https://semyeong-yu.github.io/blog/2024/DeblurNeRF/" rel="alternate" type="text/html" title="Deblurring NeRF Summary"/><published>2024-11-05T12:00:00+00:00</published><updated>2024-11-05T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/DeblurNeRF</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/DeblurNeRF/"><![CDATA[<h2 id="deblurring-nerf-summary">Deblurring NeRF Summary</h2> <h3 id="dof-nerf">DoF-NeRF</h3> <blockquote> <p>reference :<br/> <a href="https://jseobyun.tistory.com/301">DoF-NeRF</a></p> </blockquote> <d-cite key="DofNeRF">[1]</d-cite> <p>단점 :<br/> train하기 위해 all-in-focus image와 blurry image 모두 필요<br/> (all-in-focus image : 화면 전체가 초점이 맞춰져 있는 image)</p> <h3 id="deblur-nerf">Deblur-NeRF</h3> <d-cite key="DeblurNeRF">[2]</d-cite> <p>장점 :<br/> train할 때 all-in-focus image 필요 없음<br/> 핵심 :<br/> additional small MLP 사용해서<br/> per-pixel blur kernel 예측</p> <h3 id="dp-nerf">DP-NeRF</h3> <d-cite key="DpNeRF">[3]</d-cite> <h3 id="pdrf">PDRF</h3> <d-cite key="PDRF">[4]</d-cite> <h3 id="hybrid">Hybrid</h3> <p>Hybrid <d-cite key="Hybrid">[5]</d-cite></p> <p>camera motion blur와 defocus blur 중 하나만 다룸</p> <h3 id="sharp-nerf">Sharp-NeRF</h3> <d-cite key="SharpNeRF">[6]</d-cite> <p>camera motion blur와 defocus blur 중 하나만 다룸</p> <h3 id="bad-nerf">BAD-NeRF</h3> <d-cite key="BADNeRF">[7]</d-cite> <p>camera motion blur와 defocus blur 중 하나만 다룸</p> <h3 id="future-work">Future Work</h3> <ul> <li>NeRF (implicit, stochastic method) 자체가 rendering이 오래 걸려서<br/> 3DGS Deblurring도 많이 연구되고 있음!</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="3DGS"/><category term="deblur"/><summary type="html"><![CDATA[brief summary]]></summary></entry><entry><title type="html">Deblurring 3D Gaussian Splatting</title><link href="https://semyeong-yu.github.io/blog/2024/Deblurring3DGS/" rel="alternate" type="text/html" title="Deblurring 3D Gaussian Splatting"/><published>2024-10-30T12:00:00+00:00</published><updated>2024-10-30T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Deblurring3DGS</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Deblurring3DGS/"><![CDATA[<h2 id="deblurring-3d-gaussian-splatting-eccv-2024">Deblurring 3D Gaussian Splatting (ECCV 2024)</h2> <h4 id="byeonghyeon-lee-howoong-lee-xiangyu-sun-usman-ali-eunbyung-park">Byeonghyeon Lee, Howoong Lee, Xiangyu Sun, Usman Ali, Eunbyung Park</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2401.00834">https://arxiv.org/abs/2401.00834</a><br/> project website :<br/> <a href="https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/">https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/</a><br/> code :<br/> <a href="https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting">https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting</a></p> </blockquote> <blockquote> <p>핵심 :</p> <ol> <li>defocus blur 구현 :<br/> MLP로 covariance(rotation, scaling)의 변화량을 모델링해서<br/> covariance를 키워서<br/> defocus-blurred image 얻음</li> <li>camera motion blur 구현 :<br/> MLP로 position 및 covariance의 변화량을 모델링해서<br/> M개의 3DGS sets를 만든 뒤<br/> 이걸로 만든 M개의 sharp imgs를 average해서<br/> camera-motion-blurred image 얻음</li> <li>위의 MLP를 training에서만 사용하므로<br/> still real-time rendering at inference</li> <li>sparse point clouds 보상하기 위해 points 추가<br/> 또한 먼 거리에 있는 3DGS는 덜 prune out</li> </ol> </blockquote> <h3 id="introduction">Introduction</h3> <ul> <li>3DGS : <ul> <li>novel-view로 inference할 때<br/> NeRF는 새로운 각도를 MLP에 넣어야만 color, opacity 얻을 수 있지만<br/> 3DGS는 spherical harmonics, explicit 기법이라 새로운 각도에 대해서도 바로 color, opacity 얻을 수 있어서<br/> volume rendering이 빠름</li> <li>differentiable splatting-based rasterization with parallelism</li> </ul> </li> <li>본 논문 : <ul> <li>핵심 : <ul> <li>각 3DGS의 <code class="language-plaintext highlighter-rouge">covariance</code>를 수정하여 <code class="language-plaintext highlighter-rouge">blur(adjacent pixels의 혼합)를 모델링하는 작은 MLP</code> 사용</li> <li>training 시에는 MLP output 곱해서 blurry image를 생성하고<br/> inference 시에는 MLP 사용하지 않아서 real-time으로 sharp image 생성</li> </ul> </li> <li>문제 : <ul> <li>3DGS는 initial point cloud에 많이 의존하는데<br/> given images가 <code class="language-plaintext highlighter-rouge">blurry</code>하면 SfM은 유효한 feature를 식별하지 못해서 <code class="language-plaintext highlighter-rouge">매우 적은 수의 point</code> cloud를 추출함</li> <li>심지어 depth가 크면 SfM은 맨 끝에 있는 점을 거의 추출하지 않음</li> </ul> </li> <li>해결 : <ul> <li>sparse point cloud를 방지하고자<br/> <code class="language-plaintext highlighter-rouge">N-nearest-neighbor interpolation으로 points 추가</code></li> <li>먼 거리의 평면에 많은 Gaussian을 유지하기 위해<br/> <code class="language-plaintext highlighter-rouge">위치에 따라 Gaussian pruning</code></li> </ul> </li> <li>contribution :<br/> SOTA qualtiy인데 훨씬 빠른 rendering speed (\(\gt 200\) FPS)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/1-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/1-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Overall Architecture </div> <h3 id="related-works">Related Works</h3> <ul> <li>Image Deblurring : <ul> <li>\(g(x) = \sum_{s \in S_{h}} h(x, s) f(x) + n(x)\)<br/> where \(g(x)\) : blurry image and \(f(x)\) : latent sharp image<br/> where \(h(x, s)\) : blur kernel or PSF (Point Spread Function)<br/> where \(n(x)\) : additive white Gaussian noise (occurs in nature images)</li> <li>지금까지 2D image deblurring은 많이 연구되어 왔는데<br/> 3D scene deblurring은 3D view consistency 부족 때문에 연구하기 어려웠음</li> </ul> </li> <li>Fast NeRF : <ul> <li>방법 1)<br/> use additional data-structure to reduce the size and number of MLP layers<br/> but, fail to reach real-time view synthesis <ul> <li>grid-based :<br/> Hexplane, TensoRF, K-planes, Mip-grid, Masked wavelet representation, Direct voxel grid optimization, F2-nerf</li> <li>hash-based :<br/> InstantNGP, Zip-nerf</li> </ul> </li> <li>방법 2)<br/> trained param.을 faster representation으로 bake해서 real-time rendering <ul> <li>Baking neural radiance fields, Merf, Bakedsdf</li> </ul> </li> </ul> </li> <li>Deblurring NeRF :<br/> 자세한 건 <a href="https://semyeong-yu.github.io/blog/2024/DeblurNeRF/">Link</a> 참조 <ul> <li>DoF-NeRF <d-cite key="DofNeRF">[1]</d-cite> : <ul> <li>단점 :<br/> train하기 위해 all-in-focus image와 blurry image 모두 필요<br/> (all-in-focus image : 화면 전체가 초점이 맞춰져 있는 image)</li> </ul> </li> <li>Deblur-NeRF <d-cite key="DeblurNeRF">[2]</d-cite> : <ul> <li>장점 :<br/> train할 때 all-in-focus image 필요 없음</li> <li>핵심 :<br/> additional small MLP 사용해서<br/> per-pixel blur kernel 예측</li> </ul> </li> <li>DP-NeRF <d-cite key="DpNeRF">[3]</d-cite> and PDRF <d-cite key="PDRF">[4]</d-cite> : <ul> <li>Deblur-NeRF 발전시킴</li> </ul> </li> <li>Hybrid <d-cite key="Hybrid">[5]</d-cite> and Sharp-NeRF <d-cite key="SharpNeRF">[6]</d-cite> and BAD-NeRF <d-cite key="BADNeRF">[7]</d-cite> : <ul> <li>camera motion blur와 defocus blur 중 하나만 다룸</li> </ul> </li> </ul> </li> <li>Deblurring NeRF 요약 : <ul> <li>deblur task 잘 수행하지만<br/> NeRF 자체가 rendering time이 오래 걸림<br/> \(\rightarrow\)<br/> real-time differentiable rasterizer 이용하는<br/> 3DGS로 deblur task 수행하자!</li> </ul> </li> </ul> <h3 id="background">Background</h3> <ul> <li> <p>3DGS <a href="https://semyeong-yu.github.io/blog/2024/GS/">Link</a> 참고</p> </li> <li> <p>Blur :</p> <ul> <li>Defocus Blur :<br/> 렌즈의 <code class="language-plaintext highlighter-rouge">초점이 맞지 않아서</code> 흐려진 경우<br/> e.g. 인물 사진에서 인물만 초점이 맞고 배경은 흐릿한 경우</li> <li>Camera Motion Blur :<br/> 셔터가 열려 있는 동안 카메라가 움직이거나 피사체가 <code class="language-plaintext highlighter-rouge">움직여서</code> 흐려진 경우<br/> e.g. 달리는 자동차를 촬영한 경우</li> </ul> </li> </ul> <h3 id="defocus-blur">Defocus Blur</h3> <ul> <li>Motivation : <ul> <li>Defocus Blur는 일반적으로<br/> 실제 image와 PSF(point spread func.)(2D Gaussian function) 간의<br/> convolution으로 모델링<br/> 즉, a pixel이 주위 pixels에 영향을 미칠 경우 blur</li> <li>여기서 영감을 받아<br/> <code class="language-plaintext highlighter-rouge">covariance(크기)가 큰 3DGS는 Blur</code>를 유발하고<br/> <code class="language-plaintext highlighter-rouge">covariance(크기)가 작은 3DGS는 Sharp</code> image에 기여한다고 가정<br/> (covariance(dispersion)가 클수록 Gaussian이 더 많은 pixels에 걸쳐 있으니까<br/> 더 많은 이웃한 pixels 간의 interference 표현 가능)</li> <li>그렇다면 covariance \(\Sigma = R S S^{T} R^{T}\) 를 변경하여 Blur를 모델링해야겠다!</li> </ul> </li> <li>Defocus Blur를 모델링하는 MLP :<br/> \((\delta r_{j}, \delta s_{j}) = F_{\theta}(\gamma(x_{j}), r_{j}, s_{j}, \gamma(v))\)<br/> where input : \(j\)-th Gaussian’s position, rotation, scale, view-direction<br/> where output : \(j\)-th Gaussian’s rotation change, scale change<br/> (\(\gamma\) : positional encoding) <ul> <li>transformed 3DGS : <ul> <li>rotation quaternion : \(\hat r_{j} = r_{j} \cdot \text{min}(1.0, \lambda_{s} \delta r_{j} + (1 - \lambda_{s}))\)</li> <li>scaling : \(\hat s_{j} = s_{j} \cdot \text{min}(1.0, \lambda_{s} \delta s_{j} + (1 - \lambda_{s}))\) <ul> <li>\(\cdot\) : element-wise multiplication</li> <li>\(\lambda_{s}\) 로 scale하고 \((1 - \lambda_{s})\) 로 shift : for optimization stability <code class="language-plaintext highlighter-rouge">???</code></li> <li>rotation 및 scaling 변화량의 <code class="language-plaintext highlighter-rouge">최솟값을 1로 clip</code> :<br/> \(\hat s_{j} \geq s_{j}\) 이므로 transformed 3DGS는 <code class="language-plaintext highlighter-rouge">더 큰 covariance</code>를 가져서<br/> <code class="language-plaintext highlighter-rouge">Defocus Blur</code>의 근본 원인인 주변 정보의 interference을 모델링할 수 있게 됨</li> </ul> </li> </ul> </li> <li>inference :<br/> scaling factor로 covariance 변화시키는 게 blur kernel의 역할을 하므로<br/> <code class="language-plaintext highlighter-rouge">training</code> 시에는 <code class="language-plaintext highlighter-rouge">transformed 3DGS</code>가 <code class="language-plaintext highlighter-rouge">blurry</code> image를 생성하지만<br/> <code class="language-plaintext highlighter-rouge">inference</code> 시에는 MLP를 사용하지 않은 <code class="language-plaintext highlighter-rouge">기존 3DGS</code>가 <code class="language-plaintext highlighter-rouge">sharp</code> image를 생성<br/> \(\rightarrow\)<br/> training할 때는 MLP forwarding과 간단한 element-wise multiplication만 추가 비용이고,<br/> inference할 때는 MLP를 사용하지 않아 Vanilla-3DGS와 모든 단계가 동일하므로<br/> <code class="language-plaintext highlighter-rouge">추가 비용 없이 real-time rendering</code> 가능</li> </ul> </li> </ul> <h3 id="selective-blurring">Selective Blurring</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/2-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/2-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>초점에 의한 Defocus Blur는 <code class="language-plaintext highlighter-rouge">영역마다 흐린 수준이 다름</code><br/> 본 논문에서는 <code class="language-plaintext highlighter-rouge">각 3DGS마다</code> 다르게 \(\delta_{r}, \delta_{s}\) 를 추정하므로<br/> Gaussian의 covariance를 선택적으로 확대시킬 수 있어서<br/> 영역에 따라 다르게 blurring 할 수 있으므로<br/> <code class="language-plaintext highlighter-rouge">pixel 단위의 blurring</code>을 보다 유연하게 모델링 가능 <ul> <li>defocus blur가 심한 영역에 있는 3DGS는 \(\delta_{s}\) 가 더 크도록</li> <li>당연히 단일 유형의 Gaussian Blur kernel을 써서 평균 blurring을 모델링하는 것보다<br/> 본 논문에서처럼 3DGS마다 다른 Blur kernel을 적용하여 pixel 단위 blurring을 모델링하는 게 더 좋음!</li> </ul> </li> </ul> <h3 id="camera-motion-blur">Camera motion Blur</h3> <ul> <li> <p>셔터가 열려 있는 exposure time 동안<br/> camera movement가 있으면<br/> light intensities from multipe sources가 inter-mixed되어<br/> Camera motion Blur 발생</p> </li> <li> <p>Camera motion Blur를 모델링하는 MLP :<br/> \({(\delta x_{j}^{(i)}, \delta r_{j}^{(i)}, \delta s_{j}^{(i)})}_{i=1}^{M} = F_{\theta}(\gamma(x_{j}), r_{j}, s_{j}, \gamma(v))\)</p> <ul> <li>transformed 3DGS : <ul> <li>3D position : \(\hat x_{j}^{(i)} = x_{j} + \lambda_{p} \delta x_{j}^{(i)}\) (shift)</li> <li>rotation quaternion : \(\hat r_{j}^{(i)} = r_{j} \cdot \delta r_{j}^{(i)}\) (element-wise multiplication)</li> <li>scaling : \(\hat s_{j}^{(i)} = s_{j} \cdot \delta s_{j}^{(i)}\) (element-wise multiplication) <ul> <li>Camera motion Blur의 경우<br/> Defocus Blur와 달리 covariance를 무조건 키워야 되는 게 아니므로<br/> min-clip by 1.0 없음</li> </ul> </li> </ul> </li> <li>Camera motion Blur :<br/> \(I_{b} = \frac{1}{M} \sum_{i=1}^{M} I_{i}\) <ul> <li>셔터가 열려 있는 동안 카메라가 움직이는 각 discrete moment는<br/> 각 3DGS set에 대응됨</li> <li>\(j\)-th Gaussian 의 <code class="language-plaintext highlighter-rouge">camera movement</code>를 나타내기 위해<br/> <code class="language-plaintext highlighter-rouge">M개의 auxiliary 3DGS sets</code> 만들어서<br/> <code class="language-plaintext highlighter-rouge">M개의 clean images</code> rendering해서<br/> <code class="language-plaintext highlighter-rouge">average</code>해서 camera-motion-blurred image 얻음</li> </ul> </li> <li>inference :<br/> 마찬가지로 <code class="language-plaintext highlighter-rouge">inference</code> 시에는 MLP를 사용하지 않은 <code class="language-plaintext highlighter-rouge">기존 3DGS</code>가 clean image를 생성<br/> \(\rightarrow\)<br/> inference할 때는 MLP로 \(M\)-개의 3DGS sets 만들지 않고<br/> Vanilla-3DGS와 모든 단계가 동일하므로<br/> <code class="language-plaintext highlighter-rouge">추가 비용 없이 real-time rendering</code> 가능</li> </ul> </li> </ul> <h3 id="compensation-for-sparse-point-cloud">Compensation for Sparse Point Cloud</h3> <ul> <li> <p>문제 1)<br/> 3DGS는 initial point cloud에 많이 의존하는데<br/> given input multi-view images가 <code class="language-plaintext highlighter-rouge">blurry</code>하면<br/> SfM은 유효한 feature를 식별하지 못해서<br/> 매우 적은 수의 <code class="language-plaintext highlighter-rouge">sparse point cloud</code>를 추출함</p> </li> <li> <p>해결 :</p> <ul> <li>sparse point cloud를 방지하고자<br/> \(N_{st}\) iter. 후에 \(N_{p}\)-개의 points를 uniform \(U(\alpha, \beta)\) 에서 sampling하여 추가<br/> where \(\alpha\) : 기존 point cloud 위치의 최솟값<br/> where \(\beta\) : 기존 point cloud 위치의 최댓값</li> <li>새로운 point의 <code class="language-plaintext highlighter-rouge">색상은 KNN(K-Nearest-Neighbor) interpolation</code>으로 할당</li> <li>새로운 points를 uniform 분포에서 sampling해서 <code class="language-plaintext highlighter-rouge">빈 공간에 불필요한 points</code>가 생길 수 있으므로<br/> nearest neighbor까지의 거리가 threshold \(t_{d}\) 를 초과하는 points는 <code class="language-plaintext highlighter-rouge">폐기</code></li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/3-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/3-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/12-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/12-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 가운데는 without adding points, 오른쪽은 with adding extra points </div> <ul> <li> <p>문제 2)<br/> 심지어 depth of field가 크면<br/> SfM은 맨 끝에 있는 점을 거의 추출하지 않음</p> </li> <li> <p>해결 :<br/> Deblur-NeRF dataset은 forward-facing scene으로만 구성되어 있으므로<br/> dataset에 기록된 <code class="language-plaintext highlighter-rouge">z-axis 값</code>은 <code class="language-plaintext highlighter-rouge">relative depth</code> from any viewpoint라고 볼 수 있음</p> <ul> <li>방법 1) 먼 거리에 있는 3DGS 수 늘리기<br/> 먼 거리의 평면에 있는 3DGS에 대해 denisfy<br/> \(\rightarrow\)<br/> 과도한 densification은 Blur 모델링을 방해하고 추가 계산 비용 필요</li> <li>방법 2) <code class="language-plaintext highlighter-rouge">먼 거리에 있는 3DGS는 덜 prune out</code><br/> pruning threshold를 깊이에 따라 다르게 scaling<br/> as \(t_{p}, 0.9 t_{p}, \cdots , \frac{1}{w_{p}} t_{p}\)<br/> (먼 거리의 3DGS일수록 낮은 threshold) <br/> \(\rightarrow\)<br/> real-time rendering을 고려했을 때<br/> 유연한 pruning으로도 먼 거리의 3DGS sparsity를 보상하기에 충분하다는 걸 경험적으로 발견</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/4-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/4-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="experiment">Experiment</h3> <ul> <li>Setting : <ul> <li>dataset : Deblur-NeRF dataset <ul> <li>have both synthetic and real images</li> <li>has camera motion blur or defocus blur</li> </ul> </li> <li>GPU : NVIDIA RTX 4090 GPU (24GB)</li> <li>optimzier : Adam</li> <li>iter. : \(20,000\)</li> <li>Blur를 모델링하는 small MLP : <ul> <li>lr : \(1e^{-3}\)</li> <li>hidden layer : 4 <ul> <li>3 layers : shared</li> <li>1 layer : head for each \(\delta\)</li> </ul> </li> <li>hidden unit : 64</li> <li>activation : ReLU</li> <li>initialization : Xavier</li> <li>scaling factor for \(\delta\) : \(\lambda_{s}, \lambda_{p} = 1 e^{-2}\)</li> </ul> </li> <li>sparse point cloud를 보상하기 위해 <ul> <li>\(N_{st} = 2,500\) iter. 후에 \(N_{p}\) 개의 point 추가<br/> \(N_{p}\) 는 기존 point cloud 규모에 비례하며 최대 200,000개</li> <li>색상은 \(K = 4\) 의 KNN interpolation으로 할당</li> <li>nearest neighbor까지의 거리가 \(t_{d} = 2\) 을 초과하는 point는 폐기</li> </ul> </li> <li>먼 거리에 있는 3DGS는 덜 pruning하기 위해<br/> pruning threshold를 깊이에 따라 다르게 scaling <ul> <li>pruning threshold \(t_{p} = 5 e^{-3}\) and densification threshold \(2 e^{-4}\)<br/> for real defocus blur dataset</li> <li>pruning threshold \(t_{p} = 1 e^{-2}\) and densification threshold \(5 e^{-4}\)<br/> for real camera motion blur dataset</li> <li>pruning threshold multiplier \(w_{p} = 3\)</li> </ul> </li> <li>camera motion blur를 구현하기 위해<br/> \(M = 5\) 개의 3DGS sets 만들어서<br/> \(M = 5\) 개의 clean images를 average</li> </ul> </li> </ul> <h3 id="results">Results</h3> <ul> <li>Results : <ul> <li><code class="language-plaintext highlighter-rouge">SOTA deblurring NeRF</code>만큼 <code class="language-plaintext highlighter-rouge">PSNR</code> 높음</li> <li><code class="language-plaintext highlighter-rouge">3DGS</code>만큼 <code class="language-plaintext highlighter-rouge">FPS</code> 높음</li> <li>비교 대상으로 쓰인 논문들 : <ul> <li>Deblur-NeRF, Sharp-NeRF, DP-NeRF, PDRF</li> <li>original 3DGS</li> <li>Restormer로 input training images 먼저 deblur한 뒤 original 3DGS</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/5-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/5-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/6-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/6-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> real-world Defocus Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/7-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/7-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> real-world Defocus Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/8-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/8-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> synthesized Defocus Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/9-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/9-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> synthesized Defocus Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/13-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/13-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> real-world Camera motion Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/14-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/14-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> real-world Camera motion Blur Dataset </div> <h3 id="ablation-study">Ablation Study</h3> <ul> <li>Ablation Study : <ul> <li>Extra points allocation</li> <li>Depth-based pruning</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/10-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/10-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Extra points allocation </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/11-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/11-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Depth-based pruning </div> <h3 id="limitation-and-future-work">Limitation and Future Work</h3> <ul> <li>Limitation : <ul> <li>volumetric rendering 기반의 NeRF-based deblurring 기법들을<br/> rasterization 기반의 3DGS에 적용하기 어렵<br/> \(\rightarrow\)<br/> MLP로 <code class="language-plaintext highlighter-rouge">world-space</code>에서의 rays 또는 kernels를 변형하는 대신<br/> MLP로 <code class="language-plaintext highlighter-rouge">rasterized image space</code>에서의 kernels를 변형하면<br/> Deblurring 3DGS 구현 가능<br/> \(\rightarrow\)<br/> 하지만 kernel interpolation 방향으로 가면<br/> pixel interpolation은 추가 비용이 발생하며<br/> 3DGS의 geometry를 implicitly 변형하는 것일 뿐이므로<br/> 해당 방법은 3DGS로 blur를 모델링하는 최적의 방법이 아닐 것이다<br/> 이를 개선하기 위한 future works 필요</li> </ul> </li> </ul> <h3 id="code-review">Code Review</h3> <ul> <li>blur kernel 함수 :<br/> Defocus Blur 및 Camera motion Blur <ul> <li>정의 : <a href="https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/scene/blur_kernel.py#L74">https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/scene/blur_kernel.py#L74</a></li> <li>호출 : <a href="https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/gaussian_renderer/__init__.py#L101">https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/gaussian_renderer/<strong>init</strong>.py#L101</a></li> </ul> </li> <li>sparse point cloud 보상하기 위해 add points : <ul> <li><a href="https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/scene/gaussian_model.py#L444">https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/scene/gaussian_model.py#L444</a></li> </ul> </li> </ul> <h3 id="question">Question</h3> <ul> <li> <p>Q1 :<br/> small MLP는 어떤 architecture로 구성되어 있나요?<br/> Dust3R 기반의 논문들을 보면<br/> transformer 등 pre-trained complex model 가져와서 쓰는데<br/> feed-forward 방식으로 학습하므로<br/> 빠르면서도 성능이 좋습니다. 이를 적용할 수 있지 않을까요?</p> </li> <li> <p>A1 :<br/> 일단 본 논문에서는 fast training 유지하기 위해 shallow MLP가 simply fc layers로 구성되어 있습니다<br/> 말씀해주신대로 simple shallow MLP 대신 더 좋은 network 쓰면 성능이 더 좋아질 것 같다고 생각하는데,<br/> deblurring task를 다룬 본 논문 이후의 논문들을 아직 읽어보지 않아서<br/> 혹시 읽어보고 좋은 아이디어 있다면 공유하도록 하겠습니다.</p> </li> <li> <p>Q2 :<br/> 본 논문이 deblurring task를 위해 pre-trained 3DGS를 가져와서 fine-tuning하는 것인가요?</p> </li> <li> <p>A2 :<br/> 아닙니다. 기존 3DGS에 blur를 모델링하는 MLP만 추가해서 scratch부터 training하고,<br/> 이로써 input image가 더러워도(blurry하더라도) clean image를 rendering할 수 있게 됩니다.<br/> 그리고 기존 3DGS와 같이 per-scene 방식으로 학습하는 것으로 알고 있습니다.</p> </li> <li> <p>Q3 :<br/> blurry input image를 dataset에서 미리 빼버리면 deblurring을 해야 하는 상황이 없어지잖아요<br/> 이처럼 제가 생각하기에는 굳이 deblurring을 해야 하나 라는 생각이 듭니다.</p> </li> <li> <p>A3 :<br/> 일단 deblurring이라는 게 super-resolution처럼 하나의 task로 생각할 수 있습니다<br/> input image가 blurry 할 수 있는데 말씀해주신대로 이를 dataset에서 미리 뺀다는 것 자체가 manual effort를 필요로 합니다 (이를 model이 대신 해준다면 좋겠죠)<br/> 그리고 만약 주어진 모델이 deblurring을 수행할 수 있다면 다른 모델의 앞단에 쓰여서 blur를 제거하는 pre-processing 용도로도 쓰일 수 있습니다.<br/> 이로써 input images가 현실에서 있을 법한 더러운(blurry) 이미지더라도 상관 없이 input으로 쓸 수 있습니다.<br/> 2D image 또는 video를 deblurring하는 논문들은 이미 많이 있는데<br/> 3D scene deblurring의 경우에는 3D view consistency 때문에 어려움이 있었습니다.<br/> 그러다가 3DGS 등장 이후로 처음 3DGS deblurring을 시도한 논문이 본 논문이라고 보시면 될 것 같습니다.</p> </li> <li> <p>Q4 :<br/> 그렇다면 deblurring task라는 게 uncertainty를 해결하는 것이라고 볼 수 있을까요? 아니면 이것과는 별개의 task로 봐야 할까요?</p> </li> <li> <p>A4 :<br/> (3D recon. 및 novel view synthesis에서 uncertainty라는 용어가 자주 등장하는데, 관련 논문들을 아직 많이 읽어보지 않아서 확실하게 답변드리지 못하겠습니다.)</p> </li> <li> <p>Q5 :<br/> dataset에 있는 image들이 blurry하지 않고 clean(sharp) 하더라도<br/> camera explore 하면서 novel view에 대해 rendering을 하다보면 rendered image에 blur가 생길 수 있을 것 같은데<br/> deblurring이라는 게 이러한 blur도 제거해주나요?</p> </li> <li> <p>A5 :<br/> 일단 본 논문에서 deblur를 하는 원리는 covariance를 조정하는 MLP로 blur를 모델링하여<br/> 해당 MLP(blur 담당)를 사용하지 않는 inference에서는 deblurred image가 rendering되는 것입니다<br/> 하지만 input이 blurry해서 생긴 blur가 아니라 rendering하다보니 생기는 artifacts로서의 blur의 경우라면<br/> 해당 MLP가 artifacts로서의 blur도 잘 모델링해줄지는 모르겠습니다. 더 찾아봐야 할 것 같습니다.</p> </li> <li> <p>Q6 :<br/> 혹시 본 논문을 읽으면서 생각해보셨던 limitation이 있을까요? 논문에 적혀있는 것 말고 개인적인 생각이 있으신지 궁금합니다.<br/> 저는 뭔가 본 논문의 알고리즘이 artificial하다는 생각이 들었습니다.</p> </li> <li> <p>A6 :<br/> (개인적으로 생각해본 limitation 답변 못 드림 ㅠㅠ 앞으로는 논문 읽을 때 novelty 말고도 limitation이 무엇일지 생각하는 습관 길러보자!)<br/> 기존 deblur nerf에서는 deblur kernel을 이용해서 여러 ray를 쏴서 2D 상에서 pixel들을 interpolate해서 blur를 모델링하는데<br/> deblurring 3DGS에서는 3D 상에서 Gaussian covariance를 키우는 방식으로 interpolate를 비슷하게 구현했다는 논리(가정)이고<br/> 결과적으로 성능이 좋게 나왔으니 본인들 주장(가정)이 맞았다 인 것 같아서 말씀해주신대로 artificial한 느낌이 들긴 하네요</p> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="3DGS"/><category term="deblur"/><summary type="html"><![CDATA[ECCV 2024]]></summary></entry><entry><title type="html">EE534 Pattern Recognition Final</title><link href="https://semyeong-yu.github.io/blog/2024/Pattern2/" rel="alternate" type="text/html" title="EE534 Pattern Recognition Final"/><published>2024-10-28T11:00:00+00:00</published><updated>2024-10-28T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Pattern2</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Pattern2/"><![CDATA[<blockquote> <p>Lecture :<br/> 24F EE534 Pattern Recognition<br/> by KAIST Munchurl Kim <a href="https://www.viclab.kaist.ac.kr/">VICLab</a></p> </blockquote> <h2 id="chapter-6-linear-discriminant-functions">Chapter 6. Linear Discriminant Functions</h2> <h3 id="linearly-non-separable-svm">Linearly Non-Separable SVM</h3> <ul> <li>new constraint :<br/> \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}\)<br/> \(\xi_{i}\) 를 도입하여 이제는 inside margin or misclassified 도 가능하지만 대신 \(C \sum_{i=1}^{N} \xi_{i}\) 를 loss에 넣어서 큰 \(\xi_{i}\) 값을 penalize <ul> <li>\(\xi = 0\) : outside margin or support vector</li> <li>\(0 \lt \xi \leq 1\) : inside margin (correctly classified, but margin violation)</li> <li>\(\xi \gt 1\) : misclassified</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/2-480.webp 480w,/assets/img/2024-10-28-Pattern2/2-800.webp 800w,/assets/img/2024-10-28-Pattern2/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>방법 1) 1-norm-soft-margin <ul> <li>constrained primal form :<br/> minimize \(J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \xi_{i}\)<br/> subject to \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}\) and \(\xi_{i} \geq 0\) <ul> <li>unconstrained primal form :<br/> 이 때 위의 두 가지 constraints는 \(\xi_{i} = \text{max}(0, 1 - y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}))\) 로 하나로 합칠 수 있음<br/> 따라서<br/> minimize \(J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \text{max}(0, 1 - y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}))\)</li> <li>regularization param. \(C\) : <ul> <li>small \(C\) : 큰 \(\xi_{i}\) 값도 허용하므로 margin 커짐</li> <li>large \(C\) : 큰 \(\xi_{i}\) 값은 허용 안 하므로 margin 작아짐</li> <li>\(C = \infty\) : non-zero \(\xi_{i}\) 값 허용 안 하므로 hard margin (no sample inside margin)<br/> (Linearly Separable SVM 에 해당함)</li> </ul> </li> </ul> </li> <li>Lagrangian :<br/> minimize \(L(\boldsymbol w, w_{0}, \xi, \boldsymbol \lambda, \boldsymbol \mu) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \xi_{i} - \sum_{i}^{N} \mu_{i} \xi_{i} - \sum_{i}^{N} \lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (1 - \xi_{i}))\)<br/> subject to \(\xi_{i}, \mu_{i}, \lambda_{i} \geq 0\) <ul> <li> \[\nabla_{\boldsymbol w} L = 0 \rightarrow \boldsymbol w = \sum_{i=1}^{N} \lambda_{i} y_{i} \boldsymbol x_{i}\] </li> <li> \[\nabla_{w_{0}} L = 0 \rightarrow \sum_{i=1}^{N} \lambda_{i} y_{i} = 0\] </li> <li> \[\nabla_{\xi_{i}} L = 0 \rightarrow C - \mu_{i} - \lambda_{i} = 0\] </li> </ul> </li> <li>KKT condition 중 slackness condition : <ul> <li> \[\lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (1 - \xi_{i})) = 0\] </li> <li> \[\mu_{i} \xi_{i} = 0\] </li> </ul> </li> <li>dual form :<br/> 위의 세 가지 식을 대입하여 \(\boldsymbol w, w_{0}, \xi_{i}, \mu_{i}\) 를 소거하면<br/> maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}\)<br/> subject to \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\) and \(0 \leq \lambda_{i} \leq C\)</li> <li>Summary : <ul> <li>Step 1) optimal \(\lambda_{i}^{\ast}\) 구하기<br/> \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\) and \(0 \leq \lambda_{i} \leq C\) 이용해서<br/> \(\nabla_{\lambda_{i}} L = 0\) 으로 아래의 dual form 풀어서<br/> (maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}\))<br/> optimal \(\lambda_{i}\) 얻음</li> <li>Step 2) optimal \(\boldsymbol w^{\ast}, w_{0}^{\ast}\) 구하기 <ul> <li>\(\boldsymbol w^{\ast} = \sum_{i=1}^{N_{s}} \lambda_{i}^{\ast} y_{i} x_{i}\)<br/> (\(N_{s}\) : support vector 개수)<br/> (hyperplane 결정할 때는 \(\lambda_{i} \gt 0\) 중에 \(\xi = 0\) 인 support vectors만 고려!!)</li> <li>\(w_{0}^{\ast} = \frac{1}{y_{j}} - \sum_{i=1}^{N_{s}} \lambda_{i}^{\ast} y_{i} x_{i}^{T} x_{j} = \frac{1}{y_{j}} - \boldsymbol w^{\ast T} x_{j}\)<br/> (support vector \(x_{j}\) 1개 사용)<br/> 또는<br/> \(w_{0}^{\ast} = \frac{1}{N_{s}} \sum_{j=1}^{N_{s}} (\frac{1}{y_{j}} - \sum_{i=1}^{N_{s}} \lambda_{i}^{\ast} y_{i} x_{i}^{T} x_{j}) = \frac{1}{N_{s}} \sum_{j=1}^{N_{s}} (\frac{1}{y_{j}} - \boldsymbol w^{\ast T} x_{j})\)<br/> (support vector \(x_{j}\) \(N_{s}\)-개 모두 사용하여 average value)</li> </ul> </li> <li>Tip : hard margin (no sample inside margin) 의 경우<br/> 육안으로 어떤 sample이 support vector일지 판단 가능하다면<br/> complementary slackness condition (\(\lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (1 - \xi_{i})) = 0\)) 에서<br/> support vector만 \(\lambda_{i} \gt 0\) 이므로<br/> 연립해서 \(\boldsymbol w^{\ast}, w_{0}^{\ast}\) 바로 구할 수 있음</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/1-480.webp 480w,/assets/img/2024-10-28-Pattern2/1-800.webp 800w,/assets/img/2024-10-28-Pattern2/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>방법 2) 2-norm-soft-margin <ul> <li>차이점 1) primal form<br/> minimize \(J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \xi_{i}\)<br/> subject to \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}\) and \(\xi_{i} \geq 0\)<br/> 대신<br/> minimize \(J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + \frac{1}{2} C \sum_{i=1}^{N} \xi_{i}^{2}\)<br/> subject to \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}\) and \(\xi_{i} \geq 0\)</li> <li>차이점 2) Lagrangian<br/> \(\nabla_{\xi_{i}} L(\boldsymbol w, w_{0}, \boldsymbol \xi, \boldsymbol \lambda, \boldsymbol \mu) = 0\) 했을 때<br/> \(C - \mu_{i} - \lambda_{i} = 0\)<br/> 대신<br/> \(C \xi_{i} - \mu_{i} - \lambda_{i} = 0\)</li> <li>차이점 3) dual form<br/> maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}\)<br/> subject to \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\) and \(0 \leq \lambda_{i} \leq C\)<br/> 대신<br/> maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j} - \frac{1}{2C} \sum_{i=1}^{N} (\lambda_{i} + \mu_{i})^{2}\)<br/> subject to \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\) and \(0 \leq \lambda_{i} \leq C\)</li> </ul> </li> <li>Remark : <ul> <li>Linearly Non-Separable SVM에서<br/> \(C \rightarrow \infty\) 하면 Linearly Separable SVM<br/> e.g. non-linear에서는 \(0 \leq \lambda_{i} \leq C\) 인데, linear에서는 \(0 \leq \lambda_{i} \lt \infty\)</li> <li>SVM의 한계 :<br/> high computational complexity<br/> (SVM training은 주로 batch mode로 진행되어 memory를 많이 필요로 하므로<br/> training dataset을 subset으로 나눠서 training 진행)</li> <li>지금까지는 SVM for two-category만 살펴봤는데,<br/> M-class 의 경우 M개의 discriminant function \(g_{i}(x)\) 를 design하여<br/> assign \(x\) to class \(w_{i}\) if \(i = \text{argmax}_{k} g_{k}(x)\)</li> </ul> </li> </ul> <h3 id="v-svm">v-SVM</h3> <ul> <li>v-SVM : <ul> <li>hyperplane<br/> \(\boldsymbol w^{T} \boldsymbol x_{i} + w_{0} = \pm 1\)<br/> 대신<br/> \(\boldsymbol w^{T} \boldsymbol x_{i} + w_{0} = \pm \rho\)<br/> where \(\rho \geq 0\) : var. to be optimized</li> <li>margin<br/> margin은 \(\frac{2 \rho}{\| w \|}\) 이므로<br/> margin을 maximize하려면<br/> \(\| w \|\) minimize 뿐만 아니라 \(\rho\) maximize하면 되므로<br/> 둘 다 primal form loss term에 넣음</li> <li>primal form<br/> minimize \(J(\boldsymbol w, \xi, \rho) = \frac{1}{2} \| \boldsymbol w \|^{2} - v \rho + \frac{1}{N} \sum_{i=1}^{N} \xi_{i}\)<br/> subject to \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq \rho - \xi_{i}\) and \(\xi_{i} \geq 0\) and \(\rho \geq 0\)</li> <li>Lagrangian<br/> \(L(\boldsymbol w, w_{0}, \boldsymbol \xi, \rho, \boldsymbol \lambda, \boldsymbol \mu, \delta) = \frac{1}{2} \| \boldsymbol w \|^{2} - v \rho + \frac{1}{N} \sum_{i=1}^{N} \xi_{i} - \sum_{i}^{N} \mu_{i} \xi_{i} - \sum_{i}^{N} \lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (\rho - \xi_{i})) - \delta \rho\) <ul> <li>\(\nabla_{\boldsymbol w} L = 0\) 했을 때<br/> \(\boldsymbol w = \sum_{i=1}^{N} \lambda_{i} y_{i} \boldsymbol x_{i}\)</li> <li>\(\nabla_{w_{0}} L = 0\) 했을 때<br/> \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\)</li> <li>\(\nabla_{\xi_{i}} L = 0\) 했을 때<br/> \(\mu_{i} + \lambda_{i} = \frac{1}{N}\)</li> <li>\(\nabla_{\rho} L = 0\) 했을 때<br/> \(\sum_{i=1}^{N} \lambda_{i} - \delta = v\)</li> </ul> </li> <li>KKT condition 중 complementary slackness<br/> For \(\lambda_{i} \geq 0\) and \(\mu_{i} \geq 0\) and \(\delta \geq 0\), <ul> <li> \[\lambda_{i} (y_{i}(\boldsymbol w^{T} \boldsymbol x + w_{0}) - (\rho - \xi_{i})) = 0\] </li> <li> \[\mu_{i} \xi_{i} = 0\] </li> <li> \[\delta \rho = 0\] </li> </ul> </li> <li>dual form<br/> maximize \(L(\lambda) = - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}\)<br/> subject to \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\) and \(0 \leq \lambda_{i} \leq \frac{1}{N}\) and \(\sum_{i=1}^{N} \lambda_{i} = \delta + v \geq v\) <ul> <li>\(\lambda\) 만 explicitly 남아 있고,<br/> margin var. \(\rho\) 와 slack var. \(\xi_{i}\) 는 constraint의 bounds에 implicitly 들어 있음</li> <li>v-SVM에서는 \(\sum_{i=1}^{N} \lambda_{i}\) term이 없으므로<br/> optimal \(\lambda_{i}\) 는 quadratically homogeneous solution</li> <li>새로운 constraint \(\sum_{i=1}^{N} \lambda_{i} \geq v\) 있음</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/3-480.webp 480w,/assets/img/2024-10-28-Pattern2/3-800.webp 800w,/assets/img/2024-10-28-Pattern2/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Remark <ul> <li>v-SVM의 경우 \(0 \leq v \leq 1\) 이어야 optimizable</li> <li>C-SVM에 비해 v-SVM은<br/> error rate와 support vector 수 bound 측면에서 장점 <code class="language-plaintext highlighter-rouge">???</code></li> <li>\(\rho \gt 0\) 일 때 \(\delta = 0\) 이므로<br/> \(\sum_{i=1}^{N} \lambda_{i} = v\)</li> <li>loss (error)에 기여하는 애들은<br/> \(\xi_{i} \gt 0\), 즉 \(\mu_{i} = 0\), 즉 \(\lambda_{i} = \frac{1}{N}\) 이다<br/> 따라서 error rate = \(\sum_{i=1}^{N_{error}} \lambda_{i} = \frac{N_{error}}{N} \leq \sum_{i=1}^{N} \lambda_{i} = v\)<br/> 즉, error rate \(\frac{N_{error}}{N} \leq v\) 이고<br/> total number errors \(N_{error} \leq N v\)</li> <li>Since \(0 \lt \lambda_{i} \lt 1\) for support vector \(i\),<br/> \(v = \sum_{i=1}^{N} \lambda_{i} = \sum_{i=1}^{N_{s}} \lambda_{i} \leq \sum_{i=1}^{N_{s}} \frac{1}{N} = \frac{N_{s}}{N}\)<br/> 즉, \(vN \leq N_{s}\)<br/> (\(\lambda_{i} \gt 0\) 중에 \(\xi = 0\) 인 support vectors만 고려하면 \(\sum_{i=1}^{N} \lambda_{i} = \sum_{i=1}^{N_{s}}\) !!)</li> <li>\(\frac{N_{error}}{N} \leq v \leq \frac{N_{s}}{N}\) 이므로<br/> \(v\) optimize하면 error rate와 support vector 개수도 bound 알 수 있음</li> <li>support vector 수 \(N_{s}\) 는 classifier performance에 있어서 매우 중요<br/> (\(N_{s}\) 가 클수록 inner product 많이 계산해야 돼서 computational cost 높아짐)<br/> (\(N_{s}\) 가 크면 training set 이외의 data에 대한 performance가 제한되어 poor generalization)</li> </ul> </li> </ul> <h3 id="kernel-method-for-svm">Kernel Method for SVM</h3> <ul> <li> <p>discriminant function :<br/> \(x\) 의 inner product 꼴<br/> \(g(\boldsymbol x) = \boldsymbol w^{T} \boldsymbol x + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \boldsymbol x_{i}^{T} \boldsymbol x + w_{0}\)</p> </li> <li> <p>Cover’s theorem :<br/> non-linearly separable D-dim. space는<br/> linearly separable space of high enough dim. 으로 transform 될 수 있다<br/> (separating hyperplane의 optimality는 관심사 아님)</p> </li> <li> <p>Kernel Method for SVM :<br/> discriminant function \(g(\boldsymbol x) = \boldsymbol w^{T} \boldsymbol x + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \boldsymbol x_{i}^{T} \boldsymbol x + w_{0}\) 에서<br/> kernel \(K(\boldsymbol x_{i}, \boldsymbol x) = \boldsymbol x_{i}^{T} \boldsymbol x\)<br/> (inner product b.w. support vector and input vector)<br/> 대신<br/> 다른 kernel \(K(\boldsymbol x_{i}, \boldsymbol x) = \Phi(\boldsymbol x_{i})^{T} \Phi(\boldsymbol x)\) 을 써서<br/> non-linearly separable samples도 분류해보자!</p> <ul> <li>Step 1)<br/> input vector \(\boldsymbol x\) 와 training samples \(\boldsymbol x_{i}\) 를 <code class="language-plaintext highlighter-rouge">high-dim.으로 project</code> by function \(\Phi(\cdot)\)</li> <li>Step 2)<br/> transformed vector \(\Phi (\boldsymbol x)\) 와 \(\Phi (\boldsymbol x_{i})\) 에 대해 linear SVM 적용<br/> \(g(\boldsymbol x) = \boldsymbol w^{T} \Phi (\boldsymbol x) + w_{0}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/4-480.webp 480w,/assets/img/2024-10-28-Pattern2/4-800.webp 800w,/assets/img/2024-10-28-Pattern2/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Kernel Trick</code> :<br/> \(\boldsymbol x_{i}^{T} \boldsymbol x_{j}\) 대신 \(K(\boldsymbol x_{i}, \boldsymbol x_{j}) = \Phi(\boldsymbol x_{i})^{T} \Phi(\boldsymbol x_{j})\) 쓰면 됨!! <ul> <li>optimization of dual form :<br/> maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}\) <br/> 대신<br/> maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} K(\boldsymbol x_{i}, \boldsymbol x_{j})\)<br/> where \($K(\boldsymbol x_{i}, \boldsymbol x_{j}) = \Phi(\boldsymbol x_{i})^{T} \Phi(\boldsymbol x_{j})\)</li> <li>hyperplane :<br/> \(g(\boldsymbol x) = \boldsymbol w^{T} \boldsymbol x + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \boldsymbol x_{i}^{T} \boldsymbol x + w_{0} = 0\)<br/> 대신<br/> \(g(\boldsymbol x) = \boldsymbol w^{T} \Phi(\boldsymbol x) + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \Phi(\boldsymbol x_{i})^{T} \Phi(\boldsymbol x) + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} K(\boldsymbol x_{i}, \boldsymbol x) + w_{0} = 0\)<br/> where \(\boldsymbol w = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \Phi(\boldsymbol x_{i})\)</li> </ul> </li> <li>Remark : <ul> <li>polynomial learning machine, radial-basis function network, two-layer perceptron(single hidden layer) 와 같은<br/> kernel-based learning machine을 만들 때<br/> support vector learning algorithm을 사용 <ul> <li>polynomial :<br/> \(K(x, z) = (x^{T} z + 1)^{q}\) for \(q \gt 0\)</li> <li>radial-basis function :<br/> \(K(x, z) = \text{exp}(-\frac{\| x - z \|^{2}}{\sigma^{2}})\)</li> <li>hyperbolic tangent :<br/> \(K(x, z) = \text{tanh}(\beta x^{T} z + \gamma)\) where typical value is \(\beta = 2\) and \(\gamma = 1\)</li> </ul> </li> </ul> </li> <li>문제 풀이 예시 :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/6-480.webp 480w,/assets/img/2024-10-28-Pattern2/6-800.webp 800w,/assets/img/2024-10-28-Pattern2/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/5-480.webp 480w,/assets/img/2024-10-28-Pattern2/5-800.webp 800w,/assets/img/2024-10-28-Pattern2/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/7-480.webp 480w,/assets/img/2024-10-28-Pattern2/7-800.webp 800w,/assets/img/2024-10-28-Pattern2/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/8-480.webp 480w,/assets/img/2024-10-28-Pattern2/8-800.webp 800w,/assets/img/2024-10-28-Pattern2/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/9-480.webp 480w,/assets/img/2024-10-28-Pattern2/9-800.webp 800w,/assets/img/2024-10-28-Pattern2/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="chapter-6-multilayer-neural-networks">Chapter 6. Multilayer Neural Networks</h2> <ul> <li>activation function : <ul> <li>unipolar sigmoid :<br/> \(\phi (x) = \frac{1}{1 + exp(-x)}\)<br/> \(\phi^{'} (x) = \phi (x) (1 - \phi (x))\)</li> <li>bipolar sigmoid (tanh) :<br/> \(\phi (x) = \text{tanh} (x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\)<br/> \(\phi^{'} (x) = 1 - \text{tanh}^{2} (x) = 1 - \phi^{2} (x)\) <ul> <li>tanh가 sigmoid보다 gradient 더 큼</li> </ul> </li> <li>ReLU</li> </ul> </li> <li>weight initialization : <ul> <li>zero-mean uniform distribution \(U(0, \sigma^{2})\)<br/> where \(\sigma^{2}\) is chosen so that std of induced local fields of neurons lie in the linear transition interval of sigmoid activation function</li> </ul> </li> <li>weight update :<br/> \(w_{ji}(n+1) = w_{ji}(n) + \eta \delta_{j} (n) y_{i} (n) + \alpha (w_{ji} (n) - w_{ji} (n-1))\)<br/> where \(\eta\) : learning rate<br/> where \(\alpha\) : momentum constant<br/> (momentum in back-prop has stabilizing effect when gradient has oscillate in sign)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/11-480.webp 480w,/assets/img/2024-10-28-Pattern2/11-800.webp 800w,/assets/img/2024-10-28-Pattern2/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="back-propagation-algorithm">Back-propagation Algorithm</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/10-480.webp 480w,/assets/img/2024-10-28-Pattern2/10-800.webp 800w,/assets/img/2024-10-28-Pattern2/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="issues-on-neural-networks">Issues on Neural Networks</h3> <ul> <li>Stopping criteria : <ul> <li>Euclidean norm of gradient reaches sufficiently small threshold</li> <li>absolute rate of change in average squared error per epoch is sufficiently small</li> <li>generalization performance (tested after each iter.) has peaked</li> </ul> </li> <li>Weight Update : <ul> <li>sample-by-sample mode :<br/> weights are updated after presenting each training sample<br/> \(w_{ji}(n+1) = w_{ji}(n) + \eta \delta_{j} (n) y_{i} (n) + \alpha (w_{ji} (n) - w_{ji} (n-1))\) <ul> <li>very sensitive to each sample so that the weight update term is very noisy</li> </ul> </li> <li>batch mode :<br/> weights are updated after presenting entire set of training samples<br/> \(w_{ji}(t+1) = w_{ji}(t) + \eta \frac{1}{N} \sum_{n=1}^{N} \delta_{j} (n) y_{i} (n) + \alpha (w_{ji} (t) - w_{ji} (t-1))\)</li> </ul> </li> <li>k-fold cross validation : <ul> <li>If validation error increases, training stops</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/12-480.webp 480w,/assets/img/2024-10-28-Pattern2/12-800.webp 800w,/assets/img/2024-10-28-Pattern2/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/13-480.webp 480w,/assets/img/2024-10-28-Pattern2/13-800.webp 800w,/assets/img/2024-10-28-Pattern2/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Normalization : Whitening <ul> <li>mean removal</li> <li>de-correlation</li> <li>scaling for equal covariance<br/> (then input var. in training set becomes uncorrelated)<br/> (then gradient descent converges faster)</li> </ul> </li> <li>Gradient Vanish :<br/> 48p TBD</li> </ul>]]></content><author><name></name></author><category term="cv-tasks"/><category term="cv"/><summary type="html"><![CDATA[Lecture Summary (24F)]]></summary></entry><entry><title type="html">pixelSplat</title><link href="https://semyeong-yu.github.io/blog/2024/pixelSplat/" rel="alternate" type="text/html" title="pixelSplat"/><published>2024-10-25T12:00:00+00:00</published><updated>2024-10-25T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/pixelSplat</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/pixelSplat/"><![CDATA[<h2 id="pixelsplat---3d-gaussian-splats-from-image-pairs-for-scalable-generalizable-3d-reconstruction-cvpr-2024">pixelSplat - 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction (CVPR 2024)</h2> <h4 id="david-charatan-sizhe-li-andrea-tagliasacchi-vincent-sitzmann">David Charatan, Sizhe Li, Andrea Tagliasacchi, Vincent Sitzmann</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2312.12337">https://arxiv.org/abs/2312.12337</a><br/> project website :<br/> <a href="https://davidcharatan.com/pixelsplat/">https://davidcharatan.com/pixelsplat/</a><br/> code :<br/> <a href="https://github.com/dcharatan/pixelsplat">https://github.com/dcharatan/pixelsplat</a><br/> reference :<br/> NeRF and 3DGS Study</p> </blockquote> <h3 id="abstract">Abstract</h3> <ul> <li>pixelSplat :<br/> reconstruct a 3DGS primitive-based parameterization of 3D radiance field from only two images</li> </ul> <h3 id="introduction">Introduction</h3> <ul> <li>Problem : <ul> <li><code class="language-plaintext highlighter-rouge">scale ambiguity</code> :<br/> camera pose has arbitrary scale factor</li> <li><code class="language-plaintext highlighter-rouge">local minima</code> :<br/> primitive param.을 random initialization으로부터 직접 optimize하면 local minima 문제 발생</li> </ul> </li> <li>Contribution : <ul> <li>two-view image encoder :<br/> <code class="language-plaintext highlighter-rouge">two-view Epipolar Sampling, Epipolar Attention</code> 덕분에<br/> scale ambiguity 문제 극복</li> <li>pixel-aligned Gaussian param. prediction module :<br/> depth를 <code class="language-plaintext highlighter-rouge">sampling</code>하기 때문에<br/> local minima 문제 극복</li> </ul> </li> <li>Solution : <ul> <li>feed-forward model 이, a pair of images로부터,<br/> 3DGS primitives로 parameterized되는 3D radiance field recon.을 학습</li> </ul> </li> <li>model : <ul> <li><code class="language-plaintext highlighter-rouge">per-scene model</code> :<br/> <code class="language-plaintext highlighter-rouge">각각의 scene</code>을 학습하기 위해 <code class="language-plaintext highlighter-rouge">정해진 하나의 points set</code>을 <code class="language-plaintext highlighter-rouge">iteratively</code> update<br/> \(\rightarrow\)<br/> local minima 등 문제 있어서<br/> 3DGS에서는 non-differentiable Adaptive Density Control 기법으로 해결하려 하지만<br/> 이는 일반화 불가능</li> <li><code class="language-plaintext highlighter-rouge">feed-forward model</code> :<br/> <code class="language-plaintext highlighter-rouge">scene마다 얻은 points set</code>을 <code class="language-plaintext highlighter-rouge">한 번에 feed-forward</code>로 넣어서 학습<br/> differentiable하게 일반화 가능 <ul> <li>attention</li> <li>MASt3R(-SfM), Spann3R, Splatt3R, DUSt3R (잘 모름. 더 서치해봐야 함.)</li> </ul> </li> </ul> </li> </ul> <h3 id="background">Background</h3> <ul> <li>local minima : <ul> <li>언제 발생? :<br/> random location에 initialize된 Gaussian primitives가<br/> 최종 목적지까지 a few std보다 더 <code class="language-plaintext highlighter-rouge">많이 움직</code>여야 될 때<br/> gradient가 vanish 되어버려서<br/> 또는<br/> 최종 목적지까지 loss가 monotonically decrease하지 않을 때<br/> local minima 발생</li> <li>해결법? :<br/> 3DGS에서는<br/> non-differentiable pruning and splitting 기법인<br/> Adaptive Density Control을 사용하지만<br/> 본 논문에서는<br/> <code class="language-plaintext highlighter-rouge">differentiable</code> parameterization of Gaussian primitives 소개</li> </ul> </li> </ul> <h3 id="scale-ambiguity">Scale Ambiguity</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/4-480.webp 480w,/assets/img/2024-10-25-pixelSplat/4-800.webp 800w,/assets/img/2024-10-25-pixelSplat/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Scale Ambiguity 문제 : <ul> <li><code class="language-plaintext highlighter-rouge">SfM 단계</code>에서 camera pose를 계산할 때<br/> real-world-scale pose \(T_{j}^{m}\) 을<br/> metric pose \(s_{i} T_{j}^{m}\) 으로 scale하여 사용 <ul> <li>\(s_{i}\) :<br/> arbitrary scale factor</li> <li>metric pose \(s_{i} T_{j}^{m}\) :<br/> real-world-scale pose의 translation component를 \(s_{i}\) 만큼 scale</li> </ul> </li> <li>single image의 camera pose \(s_{i} T_{j}^{m}\) 로부터<br/> arbitrary scale factor \(s_{i}\) 를 복원하는 건 불가능</li> </ul> </li> <li>Scale Ambiguity 해결 : <ul> <li>two-view encoder에서 <code class="language-plaintext highlighter-rouge">a pair of images</code> 로부터<br/> arbitrary scale factor \(s_{i}\) 복원</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/1-480.webp 480w,/assets/img/2024-10-25-pixelSplat/1-800.webp 800w,/assets/img/2024-10-25-pixelSplat/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Two-view encoder Overview : <ul> <li>Step 1) Per-Image Encoder</li> <li>Step 2) Epipolar Sampling</li> <li>Step 3) Epipolar Attention</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/2-480.webp 480w,/assets/img/2024-10-25-pixelSplat/2-800.webp 800w,/assets/img/2024-10-25-pixelSplat/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Step 1) Per-Image Encoder </div> <ul> <li>Step 1) <code class="language-plaintext highlighter-rouge">Per-Image Encoder</code> :<br/> each view (two images)를 각각 feature \(F\), \(\tilde F\) 로 encode</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/3-480.webp 480w,/assets/img/2024-10-25-pixelSplat/3-800.webp 800w,/assets/img/2024-10-25-pixelSplat/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Step 2) Epipolar Sampling </div> <ul> <li>Step 2) <code class="language-plaintext highlighter-rouge">Epipolar Sampling</code> :<br/> Features 1 from Image 1의 <code class="language-plaintext highlighter-rouge">ray</code>로 <code class="language-plaintext highlighter-rouge">query</code> 만들고,<br/> Features 2 from Image 2의 <code class="language-plaintext highlighter-rouge">epipolar samples</code> 및 <code class="language-plaintext highlighter-rouge">depth</code> 로 <code class="language-plaintext highlighter-rouge">key, value</code> 만들어서,<br/> attention으로 depth scale을 잘 학습하는 게 목적<br/> (attention : depth 정보와 함께, Image 1의 ray가 Image 2의 epipolar line 위 어떤 sample에 더 많이 attention하는지)<br/> (epipolar line은 학습하는 게 아니라 수학 식으로 계산) <ul> <li>Query :<br/> \(q = Q \cdot F [u]\)<br/> where \(F\) : Features 1 from Image 1 <br/> where \(F [u]\) : ray feature at each pixel (in pixel coordinate)</li> <li>Key, Value :<br/> \(s = \tilde F [\tilde u_{l}] \oplus \gamma (\tilde d_{\tilde u_{l}})\)<br/> where \(\tilde F\) : Features 2 from Image 2<br/> where \(\tilde F [\tilde u_{l}]\) : samples on epipolar line<br/> where \(\tilde d_{\tilde u_{l}}\) : Image 2의 camera 원점까지의 거리 <ul> <li> \[k_{l} = K \cdot s\] </li> <li> \[v_{l} = V \cdot s\] </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/5-480.webp 480w,/assets/img/2024-10-25-pixelSplat/5-800.webp 800w,/assets/img/2024-10-25-pixelSplat/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Step 3) Epipolar Attention 중 Epipolar Cross-Attention </div> <ul> <li>Step 3) Epipolar Attention : <ul> <li><code class="language-plaintext highlighter-rouge">Epipolar Cross-Attention</code> :<br/> 앞서 만든 \(q, k_{l}, v_{l}\) 로 <code class="language-plaintext highlighter-rouge">cross-attention 수행</code>하여<br/> per-pixel <code class="language-plaintext highlighter-rouge">correpondence b.w. ray and epipolar sample</code> 찾음으로써<br/> per-pixel feature \(F [u]\) 가 이제<br/> arbitrary scale factor \(s\) 에 consistent한<br/> <code class="language-plaintext highlighter-rouge">scaled depth를 encode</code>하도록 update <ul> <li>\(F [u] += Att(q, k_{l}, v_{l})\)<br/> where \(+=\) : skip-connection<br/> where \(Att\) : softmax attention</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Per-Image Self-Attention</code> :<br/> Cross-Attention 끝난 뒤 마지막에 Per-Image Self-Attention 수행하여<br/> propagate scaled depth estimates<br/> to parts of the image feature maps<br/> that may not have any epipolar correspondences <ul> <li> \[F += SelfAttention(F)\] </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/9-480.webp 480w,/assets/img/2024-10-25-pixelSplat/9-800.webp 800w,/assets/img/2024-10-25-pixelSplat/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="gaussian-parameter-prediction">Gaussian Parameter Prediction</h3> <ul> <li>앞선 과정들 덕분에<br/> scale-aware feature map \(F, \tilde F\) 를 이용하여<br/> Gaussian param. \(g_{k} = (\mu_{k}, \Sigma_{k}, \alpha_{k}, S_{k})\) 를 예측 <ul> <li>2D image 상의 <code class="language-plaintext highlighter-rouge">모든 각 pixel은 3D 상의 point에 대응</code>되어<br/> 최종적인 Gaussian primitives set은<br/> just union of each image</li> </ul> </li> <li>3D position Prediction : <ul> <li>방법 1) Baseline :<br/> predict point estimate of 3D position \(\mu\) <ul> <li>\(\boldsymbol \mu = \boldsymbol o + d_{u} \boldsymbol d\)<br/> where \(u\) : 2D pixel coordiante<br/> where \(\boldsymbol d = R K^{-1} [u, 1]^{T}\) : ray direction<br/> where \(d_{u} = g_{\theta}(F [u])\) : depth obtained by neural network</li> <li>문제 :<br/> depth 자체를 neural network로 추정하는 건 local minima 문제 발생하기 쉬움</li> </ul> </li> <li>방법 2) 본 논문 방식 :<br/> predict <code class="language-plaintext highlighter-rouge">probability density</code> of 3D position \(\mu\) <ul> <li>핵심 :<br/> neural network로<br/> depth 자체를 예측하는 게 아니라<br/> differentiable probability distribution of likelihood of depth along ray를 예측</li> <li>Step 1)<br/> depth를 \(Z\)-bins로 discretize<br/> \(b_{z} = ((1 - \frac{z}{Z})(\frac{1}{d_{near}} - \frac{1}{d_{far}}) + \frac{1}{d_{far}})^{-1} \in [d_{near}, d_{far}]\)<br/> for \(z \in [0, Z]\) : depth index</li> <li>Step 2)<br/> discrete probability \(\phi\) 로부터 index \(z\) 를 sampling<br/> \(z \sim p_{\phi}(z)\)</li> <li>Step 3)<br/> ray를 쏴서(unproject) Gaussian mean \(\mu\) 계산<br/> \(\boldsymbol \mu = \boldsymbol o + (b_{z} + \delta_{z}) \boldsymbol d\)<br/> where \(\phi\) : depth(\(z\)) probability obtained by neural network<br/> where \(\delta_{z}\) : depth offset obtained by neural network</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/6-480.webp 480w,/assets/img/2024-10-25-pixelSplat/6-800.webp 800w,/assets/img/2024-10-25-pixelSplat/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Gaussian Parameter Prediction : <ul> <li>scale-aware feature map \(F, \tilde F\) 과 neural network \(f\) 를 이용하여<br/> \(\phi, \delta, \Sigma, S = f(F [u])\)<br/> where \(\phi, \delta, \Sigma, S\) : depth probability, depth offset, covariance, spherical harmonics coeff. <ul> <li><code class="language-plaintext highlighter-rouge">3D position</code>(mean) :<br/> \(\phi, \delta\) 이용해서<br/> \(\boldsymbol \mu = \boldsymbol o + (b_{z} + \delta_{z}) \boldsymbol d\)</li> <li><code class="language-plaintext highlighter-rouge">Covariance</code> :<br/> \(\Sigma\)</li> <li><code class="language-plaintext highlighter-rouge">Spherical Harmonics Coeff.</code> :<br/> \(S\)</li> <li><code class="language-plaintext highlighter-rouge">Opacity</code> :<br/> \(\phi\) 이용해서<br/> \(\alpha = \phi_{z}\)<br/> \(=\) probability of sampled depth \(z\)<br/> (so that we make sampling differentiable)</li> </ul> </li> <li>각 pixel마다 3DGS point에 대응되므로<br/> pixel-aligned Gaussians라고 부름</li> </ul> </li> </ul> <h3 id="experiments">Experiments</h3> <ul> <li>Setup : <ul> <li>Dataset :<br/> camera pose is computed by SfM <ul> <li>RealEstate 10k</li> <li>ACID</li> </ul> </li> <li>Baseline : <ul> <li>pixelNeRF</li> <li>GPNR</li> <li>Method of Du et al.</li> </ul> </li> <li>Metric : <ul> <li>PSNR</li> <li>SSIM</li> <li>LPIPS</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/7-480.webp 480w,/assets/img/2024-10-25-pixelSplat/7-800.webp 800w,/assets/img/2024-10-25-pixelSplat/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Result : <ul> <li>performance much better</li> <li>inference time faster</li> <li>less memory per ray</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/8-480.webp 480w,/assets/img/2024-10-25-pixelSplat/8-800.webp 800w,/assets/img/2024-10-25-pixelSplat/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/10-480.webp 480w,/assets/img/2024-10-25-pixelSplat/10-800.webp 800w,/assets/img/2024-10-25-pixelSplat/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Ablation Study </div> <ul> <li>Ablation Study : <ul> <li>Epipolar Encoder : Epipolar Sampling and Epipolar Attention</li> <li>Depth Encoding : freq.-based positional encoding \(\gamma(\tilde d_{\tilde u_{l}})\)</li> <li>Probabilistic Sampling : depth index \(z \sim p_{\phi}(z)\)</li> <li>Depth Regularization : <code class="language-plaintext highlighter-rouge">???</code></li> </ul> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="3DGS"/><category term="image"/><category term="pair"/><category term="scalable"/><summary type="html"><![CDATA[3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction (CVPR 2024)]]></summary></entry></feed>