<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://semyeong-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://semyeong-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-07-02T07:53:22+00:00</updated><id>https://semyeong-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">3D Rotation-Quaternion</title><link href="https://semyeong-yu.github.io/blog/2024/Quaternion/" rel="alternate" type="text/html" title="3D Rotation-Quaternion"/><published>2024-07-01T14:00:00+00:00</published><updated>2024-07-01T14:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Quaternion</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Quaternion/"><![CDATA[<h2 id="lecture-06-3d-rotations-and-complex-representations-cmu-15-462662">Lecture 06: 3D Rotations and Complex Representations (CMU 15-462/662)</h2> <blockquote> <p>referenced video :<br/> <a href="https://www.youtube.com/watch?v=YF5ZUlKxSgE&amp;list=PL9_jI1bdZmz2emSh0UQ5iOdT2xRHFHL7E&amp;index=7">3D Rotations and Quaternion</a><br/> referenced blog :<br/> <a href="https://blog.naver.com/hblee4119/223188806834">Quaternion</a></p> </blockquote> <h2 id="3d-rotation">3D Rotation</h2> <ul> <li> <p>Degrees of Freedom = 3</p> </li> <li> <p>2D rotation에서는 order of rotations 노상관, but<br/> 3D rotation에서는 <code class="language-plaintext highlighter-rouge">order of rotations 중요</code></p> </li> <li></li> </ul> <h2 id="quaternion">Quaternion</h2> <ul> <li>4 \(\times\) 1 <code class="language-plaintext highlighter-rouge">quaternion</code> \(q\) 으로 3 \(\times\) 3 <code class="language-plaintext highlighter-rouge">rotation matrix</code> 만드는 방법 : <a href="https://github.com/graphdeco-inria/gaussian-splatting/blob/b2ada78a779ba0455dfdc2b718bdf1726b05a1b6/utils/general_utils.py#L78">build_rotation(r)</a> <pre><code class="language-Python">def build_rotation(r):
  norm = torch.sqrt(r[:,0]*r[:,0] + r[:,1]*r[:,1] + r[:,2]*r[:,2] + r[:,3]*r[:,3])
  q = r / norm[:, None] # use normalized quaternion

  R = torch.zeros((q.size(0), 3, 3), device='cuda')

  r = q[:, 0]
  x = q[:, 1]
  y = q[:, 2]
  z = q[:, 3]

  R[:, 0, 0] = 1 - 2 * (y*y + z*z)
  R[:, 0, 1] = 2 * (x*y - r*z)
  R[:, 0, 2] = 2 * (x*z + r*y)
  R[:, 1, 0] = 2 * (x*y + r*z)
  R[:, 1, 1] = 1 - 2 * (x*x + z*z)
  R[:, 1, 2] = 2 * (y*z - r*x)
  R[:, 2, 0] = 2 * (x*z - r*y)
  R[:, 2, 1] = 2 * (y*z + r*x)
  R[:, 2, 2] = 1 - 2 * (x*x + y*y)
  return R
</code></pre> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="quaternion"/><category term="rotation"/><summary type="html"><![CDATA[Quaternion for Rotation Matrix]]></summary></entry><entry><title type="html">Gaussian Splatting</title><link href="https://semyeong-yu.github.io/blog/2024/GS/" rel="alternate" type="text/html" title="Gaussian Splatting"/><published>2024-07-01T10:00:00+00:00</published><updated>2024-07-01T10:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/GS</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/GS/"><![CDATA[<h2 id="3d-gaussian-splatting-for-real-time-radiance-field-rendering">3D Gaussian Splatting for Real-Time Radiance Field Rendering</h2> <h4 id="bernhard-kerbl-georgios-kopanas-thomas-leimkühler-george-drettakis">Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, George Drettakis</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2308.04079">https://arxiv.org/abs/2308.04079</a><br/> project website :<br/> <a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/</a><br/> code :<br/> <a href="https://github.com/graphdeco-inria/gaussian-splatting">https://github.com/graphdeco-inria/gaussian-splatting</a><br/> referenced blog :<br/> <a href="https://xoft.tistory.com/51">https://xoft.tistory.com/51</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>DD</li> </ol> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-GS/2-480.webp 480w,/assets/img/2024-07-01-GS/2-800.webp 800w,/assets/img/2024-07-01-GS/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-GS/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="abstract">Abstract</h2> <ul> <li>novel 3D Gaussian scene representation with real-time differentiable renderer<br/> <code class="language-plaintext highlighter-rouge">수많은 3D Gaussian이 모여 scene을 구성</code>하고 있다!</li> <li>Very Fast rendering (\(\geq\) 100 FPS)</li> <li>Higher Quality than SOTA Mip-NeRF360(2022)</li> <li>Faster Training than SOTA InstantNGP(2022)</li> </ul> <h2 id="introduction">Introduction</h2> <h3 id="why-3d-gaussian">Why 3D Gaussian?</h3> <p>3D scene representation 방법</p> <ol> <li><code class="language-plaintext highlighter-rouge">Mesh or Point</code> <ul> <li>explicit</li> <li>good for fast GPU/CUDA-based rasterization(3D \(\rightarrow\) 2D)</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">NeRF</code> method <ul> <li>implicit (MLP)</li> <li>ray marching</li> <li>continuous coordinate-based representation</li> <li>interpolate values stored in voxels, hash grids, or points</li> <li>But,,, <code class="language-plaintext highlighter-rouge">stochastic sampling</code> for rendering 때문에 <code class="language-plaintext highlighter-rouge">연산량이 많고 noise</code> 생김</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">3D Gaussian</code> method <ul> <li>explicit</li> <li>differentiable volumetric representation</li> <li>efficient rasterization(projection and \(\alpha\)-blending)</li> </ul> </li> </ol> <h3 id="rendering-nerf-vs-3dgs">Rendering (NeRF vs 3DGS)</h3> <ul> <li>NeRF : <ul> <li>ray per pixel 쏴서 coarse(stratified) and fine(PDF) sampling하고,</li> <li>MLP로 sampled points의 color 및 volume density를 구하고,</li> <li>이 값들을 volume rendering 식으로 summation</li> </ul> </li> <li>3DGS : <ul> <li>image를 tile(14 \(\times\) 14 pixel)들로 나누고,</li> <li>tile마다 Gaussian을 Depth에 따라 정렬한 뒤</li> <li>앞에서부터 뒤로 \(\alpha\)-blending</li> </ul> </li> </ul> <h2 id="related-work">Related Work</h2> <p>생략 (추후에 다시 볼 수도)</p> <h2 id="overview">Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-GS/1-480.webp 480w,/assets/img/2024-07-01-GS/1-800.webp 800w,/assets/img/2024-07-01-GS/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-GS/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For unbounded and complete scenes,<br/> For 1080p high resolution and real-time(\(\geq\) 30 fps) rendering,</p> <ol> <li> <p><code class="language-plaintext highlighter-rouge">input</code> :<br/> Most point-based methods require <code class="language-plaintext highlighter-rouge">MVS</code>(Multi-View Stereo) data,<br/> but 3DGS only needs <code class="language-plaintext highlighter-rouge">SfM points</code> for initialization<br/> COLMAP 등 SfM(Structure-from-Motion) camera calibration으로 얻은 <code class="language-plaintext highlighter-rouge">sparse point cloud</code>에서 시작해서 scene을 3D Gaussians로 나타냄으로써 <code class="language-plaintext highlighter-rouge">empty space에서의 불필요한 계산을 하지 않도록</code> continuous volumetric radiance fields 정보를 저장<br/> NeRF-synthetic dataset의 경우 3DGS 는 random initialization으로도 좋은 퀄리티 달성</p> </li> <li><code class="language-plaintext highlighter-rouge">optimization</code> interleaved with <code class="language-plaintext highlighter-rouge">adaptive density control</code> : <ul> <li>optimize 4 parameters : 3D position(mean), anisotropic covariance, opacity, and spherical harmonic coeff.(color)<br/> <code class="language-plaintext highlighter-rouge">highly anisotropic volumetric splats</code>는 <code class="language-plaintext highlighter-rouge">fine structures</code>를 compact하게 나타낼 수 있음!!<br/> <code class="language-plaintext highlighter-rouge">spherical harmonics</code>를 통해 <code class="language-plaintext highlighter-rouge">directional appearance(color)</code>를 잘 나타낼 수 있음!!<d-cite key="Plenoxels">[1]</d-cite><d-cite key="InstantNGP">[2]</d-cite></li> <li>adaptive density control : gradient 기반으로 Gaussian 형태를 변화시키기 위해, add and occasionally remove 3D Gaussians during optimization</li> </ul> </li> <li>differentiable visibility-aware <code class="language-plaintext highlighter-rouge">real-time rendering</code> :<br/> perform \(\alpha\)-blending of <code class="language-plaintext highlighter-rouge">anisotropic splats</code> respecting visibility order<br/> by fast <code class="language-plaintext highlighter-rouge">GPU sorting</code> algorithm and <code class="language-plaintext highlighter-rouge">tile-based rasterization</code>(projection and \(\alpha\)-blending)<br/> 한편, accumulated \(\alpha\) values를 tracking함으로써 <code class="language-plaintext highlighter-rouge">Gaussians 수에 제약 없이</code> 빠른 backward pass도 가능</li> </ol> <hr/> <h3 id="pseudo-code">Pseudo-Code</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-GS/3-480.webp 480w,/assets/img/2024-07-01-GS/3-800.webp 800w,/assets/img/2024-07-01-GS/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-GS/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>빨간 박스 : initialization<br/> 파란 박스 : optimization<br/> 초록 박스 : 특정 iter.마다 Gaussian을 clone, split, remove</p> <h2 id="differentiable-3d-gaussian-splatting">Differentiable 3D Gaussian Splatting</h2> <h3 id="3d-gaussian">3D Gaussian</h3> <ul> <li> <p><code class="language-plaintext highlighter-rouge">differentiable</code> volumetric representation의 특성을 가지고 있으면서도 빠른 rendering을 위해 <code class="language-plaintext highlighter-rouge">unstructured and explicit</code>한 게 무엇이 있을까?<br/> \(\rightarrow\) 3D Gaussian !!</p> </li> <li> <p>a point를 a small planar circle with a normal이라고 가정하는 이전 Point-based rendering 논문들 <d-cite key="Point1">[3]</d-cite> <d-cite key="Point2">[4]</d-cite> 과 달리<br/> <code class="language-plaintext highlighter-rouge">SfM points는 sparse해서 normals(법선)를 estimate하기 어려울</code> 뿐만 아니라, estimate 한다 해도 very noisy normals를 optimize하는 것은 매우 어렵<br/> \(\rightarrow\) normals 필요 없는 3D Gaussians !!<br/> \(G(x) = e^{-\frac{1}{2}(x)^T\Sigma^{-1}(x)}\)</p> </li> </ul> <h2 id="parameters-to-train">Parameters to train</h2> <ol> <li><code class="language-plaintext highlighter-rouge">scale vector</code> \(s\) and <code class="language-plaintext highlighter-rouge">quaternion</code> \(q\) for <code class="language-plaintext highlighter-rouge">covariance matrix</code></li> <li><code class="language-plaintext highlighter-rouge">spherical harmonics</code>(SH) coeff. for <code class="language-plaintext highlighter-rouge">color</code></li> <li><code class="language-plaintext highlighter-rouge">opacity</code> \(\alpha\)</li> <li><code class="language-plaintext highlighter-rouge">3D position</code> for <code class="language-plaintext highlighter-rouge">mean</code></li> </ol> <h3 id="covariance-matrix">Covariance matrix</h3> <blockquote> <p>covariance matrix and scale vector(scale) and quaternion(rotation)</p> </blockquote> <ul> <li> <p>covariance matrix는 positive semi-definite \(x^T M x \geq 0\) for all \(x \in R^n\)이어야만 physical meaning을 가지는데,<br/> \(\Sigma\) 를 직접 바로 optimize하면 invalid covariance matrix가 될 수 있음<br/> 그렇다면!!<br/> \(\Sigma\) 가 <code class="language-plaintext highlighter-rouge">positive semi-definite</code>이도록 \(\Sigma = R S S^T R^T\) 로 정의해서<br/> \(\Sigma\) 대신 <code class="language-plaintext highlighter-rouge">x,y,z-axis scale</code>을 나타내는 <code class="language-plaintext highlighter-rouge">3D vector</code> \(s\) 와 <code class="language-plaintext highlighter-rouge">rotation</code>을 나타내는 4 \(\times\) 1 <code class="language-plaintext highlighter-rouge">quaternion</code> \(q\) 를 optimize 하자!!<br/> quaternion에 대한 설명은 <a href="https://semyeong-yu.github.io/blog/2024/Quaternion">Quaternion</a> 링크 참고!!</p> </li> <li><code class="language-plaintext highlighter-rouge">scale</code> matrix \(S\) <code class="language-plaintext highlighter-rouge">초기값</code> : <a href="https://github.com/graphdeco-inria/gaussian-splatting/blob/b2ada78a779ba0455dfdc2b718bdf1726b05a1b6/scene/gaussian_model.py#L134C1-L134C1">GaussianModel().create_from_pcd()</a><br/> SfM sparse point cloud의 각 점에 대해 가장 가까운 점 3개까지의 거리의 평균을 각 axis별로 구한 것을 3 \(\times\) 1 \(d\)라 할 때<br/> 3 \(\times\) 1 \(log(\sqrt{d})\) 의 값을 복사하여 3 \(\times\) 3 matrix \(S\)를 초기화 <pre><code class="language-Python">dist2 = torch.clamp_min(distCUDA2(torch.from_numpy(np.asarray(pcd.points)).float().cuda()), 0.0000001)
scales = torch.log(torch.sqrt(dist2))[...,None].repeat(1, 3)
</code></pre> </li> <li><code class="language-plaintext highlighter-rouge">rotation</code> matrix \(R\) <code class="language-plaintext highlighter-rouge">초기값</code> :<br/> 각 점에 대해 \(\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}\) 으로 초기화 <pre><code class="language-Python">rots = torch.zeros((fused_point_cloud.shape[0], 4), device="cuda")
rots[:, 0] = 1
</code></pre> </li> <li><code class="language-plaintext highlighter-rouge">anisotropic covariance</code>는 다양한 모양의 geometry를 나타내기 위해 optimize하기에 적합!</li> </ul> <blockquote> <p>param. gradient 직접 유도</p> </blockquote> <p>training할 때 automatic differentiation으로 인한 overhead를 방지하기 위해 param. gradient를 직접 유도함!</p> <p>Appendix A. <code class="language-plaintext highlighter-rouge">?????</code></p> <blockquote> <p>Project 3D Gaussians to 2D</p> </blockquote> <ul> <li> <p><code class="language-plaintext highlighter-rouge">world coordinate</code> :<br/> \(\Sigma\) : 3 \(\times\) 3 covariance matrix of 3D Gaussian</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">image coordiante</code> :<br/> \(\Sigma^{\ast} = J W \Sigma W^T J^T\) : covariance matrix of 2D splat<br/> where<br/> \(W\) : <code class="language-plaintext highlighter-rouge">viewing transformation</code> matrix from world coordinate to camera coordinate<br/> \(J\) : <code class="language-plaintext highlighter-rouge">Jacobian</code> matrix of the affine <code class="language-plaintext highlighter-rouge">approximation</code> of the <code class="language-plaintext highlighter-rouge">projective transformation</code> from camera coordinate to image coordinate</p> </li> </ul> <p><code class="language-plaintext highlighter-rouge">?????</code></p> <h3 id="spherical-harmonicssh-coeff">Spherical Harmonics(SH) coeff.</h3> <h3 id="opacity">opacity</h3> <h3 id="3d-positionmean">3D position(mean)</h3> <h2 id="optimization-with-adaptive-density-control-of-3d-gaussians">Optimization with Adaptive Density Control of 3D Gaussians</h2> <h3 id="optimization">Optimization</h3> <h3 id="adaptive-control-of-gaussians">Adaptive Control of Gaussians</h3> <h2 id="fast-differentiable-rasterizer-for-gaussians">Fast Differentiable Rasterizer for Gaussians</h2> <h2 id="results">Results</h2> <h2 id="discussion">Discussion</h2>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="gaussian"/><category term="splatting"/><category term="rendering"/><category term="3d"/><category term="view"/><category term="synthesis"/><summary type="html"><![CDATA[3D GS for Real-Time Radiance Field Rendering]]></summary></entry><entry><title type="html">Diffusion-DDPM</title><link href="https://semyeong-yu.github.io/blog/2024/Diffusion/" rel="alternate" type="text/html" title="Diffusion-DDPM"/><published>2024-06-25T15:00:00+00:00</published><updated>2024-06-25T15:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Diffusion</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Diffusion/"><![CDATA[<h2 id="diffusion">Diffusion</h2> <h3 id="diffusion-model">Diffusion Model</h3> <ul> <li>forward process : <code class="language-plaintext highlighter-rouge">fixed</code> Gaussian noise 더함</li> <li>reverse process : <code class="language-plaintext highlighter-rouge">learned</code> Gaussian noise 뺌 (mean, std를 학습)</li> </ul> <h3 id="likelihood">Likelihood</h3> <p>아래 둘 다 관측값 \(x\)가 나올 확률인데,</p> <ul> <li><code class="language-plaintext highlighter-rouge">probability</code> \(P(x | \theta)\) : <code class="language-plaintext highlighter-rouge">확률 분포가 고정</code>된 상태에서, <code class="language-plaintext highlighter-rouge">관측되는 사건이 변화</code>할 때의 확률<br/> 예: 선택 가능한 정수를 1~5로 제한(확률 분포 고정)했을 때, 관측 목표값이 1~5 중 한 개의 숫자(관측 사건 변화)일 경우</li> <li><code class="language-plaintext highlighter-rouge">likelihood</code> \(L(\theta | x)\) : <code class="language-plaintext highlighter-rouge">관측된 사건이 고정</code>된 상태에서, <code class="language-plaintext highlighter-rouge">확률 분포 몰라서 가정</code>할 때의 확률<br/> 예: 선택 가능한 정수를 1~5가 아니라 1~10 또는 4~50으로 바꾸면서(확률 분포 모름), 2가 관측될 확률을 계산(관측 사건 고정)할 경우<br/> 예: 어떤(모르는) 확률 분포를 따르는 task를 n회 반복 수행하여 관측했을 때 random var. 종류를 가정할 수도 있고 특정 random var.의 parameter를 가정할 수도 있다</li> </ul> <h3 id="markov-process">Markov process</h3> <ul> <li><code class="language-plaintext highlighter-rouge">Markov</code> process (= Markov chain = <code class="language-plaintext highlighter-rouge">memoryless</code> process) : Markov property를 가지는 discrete stochastic process<br/> \(P[s_{t+1}|s_t] = P[s_{t+1}|s_1, \ldots, s_t]\)</li> </ul> <h3 id="kl-divergence">KL-divergence</h3> <ul> <li>\(H(p, q) = - \sum p_i log q_i\) : 두 확률분포 p, q의 cross entropy<br/> (보통 \(p\)는 GT, \(q\)는 predicted)</li> <li>\(H(p) = - \sum p_i log p_i\) : p’s entropy (상수값)</li> <li>\(KL(p \| q) = H(p, q) - H(p) = \sum p_i log \frac{p_i}{q_i}\) : 두 확률분포 p, q의 차이<br/> \(H(p)\)는 상수값이므로 <code class="language-plaintext highlighter-rouge">KL-divergence minimize = cross entropy minimize</code></li> <li>모르는 분포 \(p(x)\)를 N개 sampling하여 trained \(q(x | \theta)\)로 근사하고자 할 때,<br/> \(KL(p \| q) \simeq \frac{1}{N} \sum_{n=1}^{N} {-log q(x_n | \theta) + log p(x_n)}\) :<br/> \(log p(x_n)\)은 \(\theta\)에 독립이므로 <code class="language-plaintext highlighter-rouge">KL-divergence minimize = negative log likelihood minimize = MLE</code></li> </ul> <p>KL-diverence 특성 :</p> <ol> <li>\(KL(p \| q) \geq 0\) : 확률분포 p = q일 때 최소</li> <li>\(KL(p \| q) \neq KL(q \| p)\) (asymmetric) : 거리 개념이 아님<br/> 거리 개념으로 쓰는 방법 : 2가지 KL-divergence를 평균내는 방식의 \(JSD(p \| q)\)</li> </ol> <h3 id="diffusion-algorithm">Diffusion Algorithm</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-06-25-Diffusion/1-480.webp 480w,/assets/img/2024-06-25-Diffusion/1-800.webp 800w,/assets/img/2024-06-25-Diffusion/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-06-25-Diffusion/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>forward process (\(q\)) :<br/> \(q(X_t | X_{t-1}) = N(X_t ; \mu_{X_{t-1}}, \Sigma_{X_{t-1}}) = N(X_t ; \sqrt{1-\beta_t} \cdot X_{t-1}, \beta_t \cdot I)\)<br/> where \(\beta_t\) : noise 주입 정도 (상수값)<br/> t가 증가할수록 \(\beta_t\)가 증가하여 다른 pixel(\(I\))을 선택하므로 noise가 강해진다</p> </li> <li> <p>backward process (\(p_{\theta}\)) :<br/> image prior \(q(X_t)\)를 모르기 때문에 \(q(X_{t-1} | X_t)\)를 계산할 수 없으므로<br/> 목표 : \(q(X_{t-1} | X_t)\)를 근사하는 \(p_{\theta}(X_{t-1} | X_t)\) 학습<br/> 즉, 확률분포 \(q\)에서 관측한 값 \(x\)로 \(p_{\theta | x}\)의 likelihood를 구했을 때 그 값이 최대가 되도록 하는 <code class="language-plaintext highlighter-rouge">MLE Problem</code><br/> 즉, minimize \(E_q [- log p_{\theta} (x_0)]\)</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Diffusion Model</code> Naive Loss 수식 :<br/> 확률분포 \(q\)로 sampling했을 때,<br/> \(E_{x_T \sim q(x_T|x_0)}[- log p_{\theta}(x_0)] \leq\)<br/> \(E_q [D_{KL}(q(x_T | x_0) \| p_{\theta} (x_T)) + \sum_{t \gt 1} D_{KL}(q(x_{t-1} | x_t, x_0) \| p_{\theta} (x_{t-1} | x_t)) - log p_{\theta} (x_0 | x_1)]\)</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">DDPM</code>(Denoising Diffusion Probabilistic Model)(2020) Loss 수식 :<br/> \(E_{t, x_0, \epsilon} [\| \epsilon - \epsilon_{\theta}(\sqrt{\bar \alpha_{t}}x_0 + \sqrt{1-\bar \alpha_{t}} \epsilon, t) \|^{2}]\)<br/> where \(\epsilon \sim N(0, I)\)<br/> 즉, \(\epsilon_{\theta}\)가 Standard Gaussian 분포 \(\epsilon\)를 따르도록!<br/> 이 때, \(\epsilon_{\theta}\)의 input은 \(q(x_t | x_0)\)와 \(t\) !</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Distribution Summary</code> :</p> </li> </ul> <p>Let \(\alpha_t = 1 - \beta_t\) and \(\bar \alpha_t = \prod_{s=1}^t \alpha_s\) and \(\epsilon \sim N(0, I)\)</p> <ol> <li> \[x_t \sim q(x_t | x_{t-1}, x_0) = q(x_t | x_{t-1}) = N(x_t ; \sqrt{\alpha_{t}} \cdot x_{t-1}, \beta_{t} \cdot \boldsymbol I)\] </li> <li> \[x_t \sim q(x_t | x_0) = N(x_t; \sqrt{\bar \alpha_{t}} x_{0}, (1-\bar \alpha_{t}) \boldsymbol I) = \sqrt{\bar \alpha_{t}}x_0 + \sqrt{1-\bar \alpha_{t}} \epsilon\] </li> <li>\(x_{t-1} \sim q(x_{t-1} | x_t, x_0) = N(x_{t-1}; \tilde \mu_{t}(x_t), \tilde \beta_{t})\)<br/> where \(\tilde \mu_{t}(x_t) = \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{t})\)<br/> and \(\tilde \beta_{t} = \frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_{t}} \beta_{t}\)</li> <li>\(x_{t-1} \sim p_{\theta}(x_{t-1} | x_t) = N(x_{t-1}; \mu_{\theta}(x_t, t), \tilde \beta_{t})\)<br/> where \(\mu_{\theta}(x_t, t) = \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{\theta}(x_t, t))\)<br/> and \(\tilde \beta_{t} = \frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_{t}} \beta_{t}\)<br/> (training param.로 학습하는 부분은 \(\epsilon_{\theta}(x_t, t)\) 뿐!!)</li> </ol> <p>(위의 수식 유도과정은 지금부터 아래에서 다룰 예정)</p> <h3 id="diffusion-model-및-ddpm-loss-수식-유도">Diffusion Model 및 DDPM Loss 수식 유도</h3> <blockquote> <p>Step 1. ELBO (<code class="language-plaintext highlighter-rouge">Evidence Lower Bound</code>) 꼴로 변환</p> </blockquote> <p>\(log p_{\theta}(x_0)\)<br/> \(= E_{x_T \sim q(x_T|x_0)}[log p_{\theta}(x_0)]\)<br/> \(= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[log \frac{p_{\theta}(x_{0:T})}{p_{\theta}(x_{1:T}|x_0)}]\)<br/> \(= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[log \frac{p_{\theta}(x_{0:T})}{q(x_{1:T}|x_0)}] + E_{x_{1:T} \sim q(x_{1:T}|x_0)}[log \frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{1:T}|x_0)}] \cdots (\ast)\)<br/> (\(p_{\theta}(x_{1:T}|x_0)\)은 <code class="language-plaintext highlighter-rouge">intractable</code>하므로 KL divergence 항에 넣어서 제거!)</p> <p>마지막 식의 오른쪽 항은 아래와 같이 <code class="language-plaintext highlighter-rouge">KL divergence</code> 꼴이다.<br/> \(E_{x_{1:T} \sim q(x_{1:T}|x_0)}[log \frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{1:T}|x_0)}] = \sum q(x_{1:T}|x_0) log \frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{1:T}|x_0)} = D_{KL}(q(x_{1:T}|x_0) \| p_{\theta}(x_{1:T}|x_0))\)<br/> \(q(x_{1:T}|x_0)\)는 계산할 수 있지만 \(p_{\theta}(x_{1:T}|x_0)\)는 계산할 수 없으므로 KL divergence의 특성 \(KL(p \| q) \geq 0\)을 이용하면<br/> \((\ast)\) 으로부터<br/> \(log p_{\theta}(x_0) \geq E_{x_{1:T} \sim q(x_{1:T}|x_0)}[log \frac{p_{\theta}(x_{0:T})}{q(x_{1:T}|x_0)}]\)<br/> 즉, \(E_{x_T \sim q(x_T|x_0)}[- log p_{\theta}(x_0)] \leq E_{x_{1:T} \sim q(x_{1:T}|x_0)}[- log \frac{p_{\theta}(x_{0:T})}{q(x_{1:T}|x_0)}]\)</p> <blockquote> <p>Step 2. <code class="language-plaintext highlighter-rouge">Markov property</code> 이용하여 <code class="language-plaintext highlighter-rouge">Diffusion Model Naive Loss</code> 유도</p> </blockquote> \[E_{x_{1:T} \sim q(x_{1:T}|x_0)}[- log \frac{p_{\theta}(x_{0:T})}{q(x_{1:T}|x_0)}]\] \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[log \frac{q(x_{1:T}|x_0)}{p_{\theta}(x_{0:T})}]\] \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[log \frac{\prod_{t=1}^{T} q(x_t|x_{t-1})}{p_{\theta}(x_T) \prod_{t=1}^T p_{\theta}(x_{t-1}|x_t)}]\] <p>by memoryless <code class="language-plaintext highlighter-rouge">Markov chain property</code></p> \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[- log p_{\theta}(x_T) + \sum_{t=1}^{T} log \frac{q(x_t|x_{t-1})}{p_{\theta}(x_{t-1}|x_t)}]\] \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[- log p_{\theta}(x_T) + \sum_{t=2}^{T} log \frac{q(x_t|x_{t-1})}{p_{\theta}(x_{t-1}|x_t)} + log \frac{q(x_1|x_0)}{p_{\theta}(x_0|x_1)}]\] \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[- log p_{\theta}(x_T) + \sum_{t=2}^{T} log \frac{q(x_t|x_{t-1}, x_0)}{p_{\theta}(x_{t-1}|x_t)} + log \frac{q(x_1|x_0)}{p_{\theta}(x_0|x_1)}]\] <p>by memoryless <code class="language-plaintext highlighter-rouge">Markov property</code><br/> <code class="language-plaintext highlighter-rouge">tractable</code>하도록 만들기 위해 \(q(x_t|x_{t-1})\) 의 조건부에 \(x_0\) 추가</p> \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[- log p_{\theta}(x_T) + \sum_{t=2}^{T} log (\frac{q(x_{t-1}|x_t, x_0)}{p_{\theta}(x_{t-1}|x_t)} \cdot \frac{q(x_t|x_0)}{q(x_{t-1}|x_0)}) + log \frac{q(x_1|x_0)}{p_{\theta}(x_0|x_1)}]\] <p>by <code class="language-plaintext highlighter-rouge">Bayes</code> 정리<br/> \(P(A|B \bigcap C) = \frac{P(B|A \bigcap C) \cdot P(A|C)}{P(B|C)}\)</p> <p>\(= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[- log p_{\theta}(x_T) + \sum_{t=2}^{T} log \frac{q(x_{t-1}|x_t, x_0)}{p_{\theta}(x_{t-1}|x_t)} + log \frac{q(x_T|x_0)}{q(x_1|x_0)} + log \frac{q(x_1|x_0)}{p_{\theta}(x_0|x_1)}]\)<br/> by log 곱셈으로 소거</p> \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[log \frac{q(x_T|x_0)}{p_{\theta}(x_T)} + \sum_{t=2}^{T} log \frac{q(x_{t-1}|x_t, x_0)}{p_{\theta}(x_{t-1}|x_t)} - log p_{\theta}(x_0|x_1)]\] \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[D_{KL}(q(x_T|x_0) \| p_{\theta}(x_T)) + \sum_{t=2}^{T} D_{KL}(q(x_{t-1}|x_t, x_0) \| p_{\theta}(x_{t-1}|x_t)) - log p_{\theta}(x_0|x_1)]\] <p>by <code class="language-plaintext highlighter-rouge">KL divergence</code> 식 \(D_{KL}(P \| Q) = \sum P(x) log (\frac{P(x)}{Q(x)})\)</p> <blockquote> <p>Step 3. <code class="language-plaintext highlighter-rouge">DDPM Loss</code> 유도</p> </blockquote> <ol> <li> <p>\(L_T = D_{KL}(q(x_T | x_0) \| p(x_T))\) : <code class="language-plaintext highlighter-rouge">regularization</code> loss<br/> <code class="language-plaintext highlighter-rouge">마지막 상태</code> \(x_T\)에서 확률분포 q, p의 차이를 최소화<br/> noise 주입 정도인 \(\beta_t\)는 미리 정해둔 schedule에 따른 상수값(fixed)이므로<br/> \(q(x_T | x_0)\)는 training과 관계없이 \(x_T\)가 항상 Gaussian 분포를 따르도록 한다.<br/> \(x_T\)가 <code class="language-plaintext highlighter-rouge">Gaussian 분포</code>를 따르므로 \(q(x_T | x_0)\)와 \(p(x_T)\)는 거의 유사하고,<br/> 결과적으로 둘의 KL divergence인 \(L_T\)는 항상 0에 가까운 값을 가지므로 training에서 \(L_T\) term은 <code class="language-plaintext highlighter-rouge">제외</code></p> </li> <li> <p>\(L_{t-1} = D_{KL}(q(x_{t-1} | x_t, x_0) \| p(x_{t-1} | x_t))\) : <code class="language-plaintext highlighter-rouge">denoising process</code> loss<br/> <code class="language-plaintext highlighter-rouge">현재 상태</code> \(x_t\)가 주어질 때 <code class="language-plaintext highlighter-rouge">이전 상태</code> \(x_{t-1}\)가 나올 확률 분포 q, p의 차이를 최소화</p> </li> <li> <p>\(L_0 = - log p_{\theta} (x_0 | x_1)\) : <code class="language-plaintext highlighter-rouge">reconstruction</code> loss<br/> q를 sampling했을 때 \(p_{\theta} (x_0 | x_1)\)를 최대화하여 (MLE) 확률분포 q, p의 차이를 최소화<br/> 전체적으로 봤을 때 \(L_0\)는 무수히 많은 time step \(T \sim 1000\) 중 단일 시점에서의 log likelihood 값이므로<br/> <code class="language-plaintext highlighter-rouge">값이 너무 작아서</code> training에서 \(L_0\) term은 <code class="language-plaintext highlighter-rouge">제외</code></p> </li> </ol> <p>Let’s only minimize the second term<br/> \(E_{x_{1:T} \sim q(x_{1:T}|x_0)}[\sum_{t=2}^{T} L_{t-1}] = E_{x_{1:T} \sim q(x_{1:T}|x_0)}[\sum_{t=2}^{T} D_{KL}(q(x_{t-1} | x_t, x_0) \| p(x_{t-1} | x_t))]\)</p> <blockquote> <p>Step 4. Gaussian param. \(\mu, \sigma\)로 KL-divergence 나타내기</p> </blockquote> <ul> <li> <p><code class="language-plaintext highlighter-rouge">Gaussian Integral</code> :<br/> \(\int_{-\infty}^{\infty} e^{-x^2}dx = \sqrt{\pi}\) and \(\int_{-\infty}^{\infty} x^2 e^{-ax^2}dx = \frac{1}{2}\sqrt{\pi}a^{-\frac{3}{2}}\)</p> </li> <li> <p>Integral of \(p(x)logp(x)\) for Gaussian \(p(x)\) :<br/> For \(p(x) = \frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} e^{-\frac{(x-\mu_{1})^2}{2 \sigma_{1}^2}} \sim N(\mu_{1}, \sigma_{1})\),<br/> \(\int p(x) log p(x) dx\)<br/> \(= \int \frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} e^{-\frac{(x-\mu_{1})^2}{2 \sigma_{1}^2}} log (\frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} e^{-\frac{(x-\mu_{1})^2}{2 \sigma_{1}^2}}) dx\)<br/> \(= \int \frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} e^{-t^2} (log \frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} - t^2) \sqrt{2} \sigma_{1} dt\)<br/> by 치환 \(t = \frac{x-\mu_{1}}{\sqrt{2}\sigma_{1}}\)<br/> \(= \frac{1}{\sqrt{\pi}} \int e^{-t^2} (log \frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} - t^2) dt\)<br/> \(= - \frac{log(2\pi \sigma_{1}^{2})}{2 \sqrt{\pi}} \int e^{-t^2} dt - \frac{1}{\sqrt{\pi}} \int t^2 e^{-t^2} dt\)<br/> \(= - \frac{log(2\pi \sigma_{1}^{2})}{2 \sqrt{\pi}} \cdot \sqrt{\pi} - \frac{1}{\sqrt{\pi}} \cdot \frac{\sqrt{\pi}}{2}\) by <code class="language-plaintext highlighter-rouge">Gaussian integral</code><br/> \(= - \frac{log(2\pi \sigma_{1}^{2})}{2} - \frac{1}{2}\)<br/> \(= - \frac{1}{2} (1 + log(2\pi \sigma_{1}^{2}))\)</p> </li> <li> <p>Integral of \(p(x)logq(x)\) for Gaussian \(p(x)\) and \(q(x)\) :<br/> For \(p(x) = \frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} e^{-\frac{(x-\mu_{1})^2}{2 \sigma_{1}^2}} \sim N(\mu_{1}, \sigma_{1})\)<br/> and \(q(x) = \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} e^{-\frac{(x-\mu_{2})^2}{2 \sigma_{2}^2}} \sim N(\mu_{2}, \sigma_{2})\),<br/> \(\int p(x) log q(x) dx\)<br/> \(= \int p(x) log \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} e^{-\frac{(x-\mu_{2})^2}{2 \sigma_{2}^2}} dx\)<br/> \(= \int p(x) log \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} dx - \int p(x) \frac{(x-\mu_{2})^2}{2 \sigma_{2}^2} dx\)<br/> \(= log \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} - \int p(x) \frac{(x-\mu_{2})^2}{2 \sigma_{2}^2} dx\)<br/> \(= log \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} - \frac{\int p(x) x^2 dx - \int 2 \mu_{2} x p(x) dx + \mu_{2}^2 \int p(x) dx}{2 \sigma_{2}^2}\)<br/> \(= log \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} - \frac{E_{1}(x^2) - 2 \mu_{2} E_{1}(x) + \mu_{2}^2}{2 \sigma_{2}^2}\)<br/> \(= log \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} - \frac{\sigma_{1}^2 + \mu_{1}^2 - 2 \mu_{2} \mu_{1} + \mu_{2}^2}{2 \sigma_{2}^2}\)<br/> by \(Var(X) = E[X^2] - (E[X])^2\)<br/> \(= - \frac{1}{2} log (2 \pi \sigma_{2}^{2}) - \frac{\sigma_{1}^2 + (\mu_{1} - \mu_{2})^2}{2 \sigma_{2}^2}\)</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">KL divergence</code> for Gaussian \(p(x)\) and \(q(x)\) :<br/> For \(p(x) = \frac{1}{\sqrt{2 \pi \sigma_{1}^{2}}} e^{-\frac{(x-\mu_{1})^2}{2 \sigma_{1}^2}} \sim N(\mu_{1}, \sigma_{1})\)<br/> and \(q(x) = \frac{1}{\sqrt{2 \pi \sigma_{2}^{2}}} e^{-\frac{(x-\mu_{2})^2}{2 \sigma_{2}^2}} \sim N(\mu_{2}, \sigma_{2})\),<br/> \(D_{KL}(p \| q)\)<br/> \(= \int p(x) log \frac{p(x)}{q(x)} dx\)<br/> \(= \int p(x) logp(x) dx - \int p(x) log q(x)dx\)<br/> \(= - \frac{1}{2} (1 + log(2\pi \sigma_{1}^{2})) - (- \frac{1}{2} log (2 \pi \sigma_{2}^{2}) - \frac{\sigma_{1}^2 + (\mu_{1} - \mu_{2})^2}{2 \sigma_{2}^2})\)<br/> \(= - \frac{1}{2} + \frac{1}{2} log (\frac{2 \pi \sigma_{2}^2}{2 \pi \sigma_{1}^2}) + \frac{\sigma_{1}^2 + (\mu_{1} - \mu_{2})^2}{2 \sigma_{2}^2}\)<br/> \(= - \frac{1}{2} + log (\frac{\sigma_{2}}{\sigma_{1}}) + \frac{\sigma_{1}^2 + (\mu_{1} - \mu_{2})^2}{2 \sigma_{2}^2}\)</p> </li> </ul> <blockquote> <p>Step 5. Only Minimize the second term in Diffusion Loss</p> </blockquote> \[E_{x_{1:T} \sim q(x_{1:T}|x_0)}[\sum_{t=2}^{T} L_{t-1}]\] \[= E_{x_{1:T} \sim q(x_{1:T}|x_0)}[\sum_{t=2}^{T} D_{KL}(q(x_{t-1} | x_t, x_0) \| p(x_{t-1} | x_t))]\] <p>Let \(\sigma\) <code class="language-plaintext highlighter-rouge">std have no learning param. (상수값)</code><br/> Let \(q(x_{t-1} | x_t, x_0)\) have Gaussian mean \(\tilde \mu_{t}\)<br/> Let \(p_{\theta}(x_{t-1} | x_t)\) have Gaussian mean \(\mu_{\theta}\)</p> <p>By Step 4., since \(\sigma\) is fixed, we have to minimize</p> \[E_{x_{1:T} \sim q(x_{1:T}|x_0)}[\frac{1}{2 \sigma_{t}^2} \| \tilde \mu_{t} (x_t, x_0) - \mu_{\theta} (x_t, t) \|^2] + C\] <p><code class="language-plaintext highlighter-rouge">Now we have to know</code> \(\tilde \mu_{t}\) and \(\mu_{\theta}\)</p> <blockquote> <p>Step 6. Obtain \(q(x_t \| x_0)\) from \(q(x_t \| x_{t-1})\)</p> </blockquote> <ol> <li> <p>Let’s define \(q(x_t | x_{t-1}) = N(x_t ; \sqrt{1-\beta_{t}} \cdot x_{t-1}, \beta_{t} \cdot \boldsymbol I)\)<br/> where noise 주입 비율인 \(\beta_{t}\)는 t에 따라 증가하는 상수값이고,<br/> noise 주입 비율이 커질수록 분산이 커지는 건 reasonable</p> </li> <li> <p>Let’s define \(\alpha_{t} = 1 - \beta_{t}\) and \(\bar \alpha_{t} = \prod_{s=1}^t \alpha_{s}\)<br/> where \(\bar \alpha_{t}\)는 \(s=1\)부터 \(s=t\)까지 \(\alpha_{s} = 1 - \beta_{s}\)의 누적곱</p> </li> </ol> <p>When \(\epsilon_{t-1}, \epsilon_{t-2}, \cdots, \epsilon_0 \sim N(0, I)\),<br/> \(x_t = \mu + \sigma \cdot \epsilon = \sqrt{\alpha_{t}} x_{t-1} + \sqrt{1-\alpha_{t}} \cdot \epsilon_{t-1}\)<br/> \(\cdots\)<br/> \(x_t = \sqrt{\bar \alpha_{t}} x_{0} + \sqrt{1-\bar \alpha_{t}} \cdot \epsilon\)<br/> where \(\epsilon \sim N(0, I)\)<br/> by merging two Gaussians \(N(0, \sigma_{1}^2 I), N(0, \sigma_{2}^2 I) \rightarrow N(0, (\sigma_{1}^2 + \sigma_{2}^2) I)\)</p> <p>Therefore,<br/> \(q(x_t | x_0) = N(x_t; \sqrt{\bar \alpha_{t}} x_{0}, (1-\bar \alpha_{t}) \boldsymbol I)\)</p> <p>즉, 우리가 정의한 Gaussian \(q(x_t \| x_{t-1})\) 으로부터 Gaussian \(q(x_t\|x_0)\) 를 얻어냈다!</p> <blockquote> <p>Step 7. Obtain \(q(x_{t-1} \| x_t, x_0)\)</p> </blockquote> <p><code class="language-plaintext highlighter-rouge">Remind that</code></p> <ol> <li> \[q(x_t | x_{t-1}, x_0) = q(x_t | x_{t-1}) = N(x_t ; \sqrt{\alpha_{t}} \cdot x_{t-1}, \beta_{t} \cdot \boldsymbol I)\] </li> <li> \[q(x_t | x_0) = N(x_t; \sqrt{\bar \alpha_{t}} x_{0}, (1-\bar \alpha_{t}) \boldsymbol I)\] </li> </ol> <p>\(q(x_{t-1} | x_t, x_0)\)<br/> \(= q(x_t | x_{t-1}, x_0) \frac{q(x_{t-1} | x_0)}{q(x_t | x_0)}\)<br/> \(\propto \exp (- \frac{(x_t - \sqrt{\alpha_{t}} x_{t-1})^2}{2 \beta_{t}} - \frac{(x_{t-1} - \sqrt{\bar \alpha_{t-1}} x_{0})^2}{2 (1-\bar \alpha_{t-1})} + \frac{(x_{t} - \sqrt{\bar \alpha_{t}} x_{0})^2}{2 (1-\bar \alpha_{t})})\)<br/> \(= \exp (- \frac{1}{2} ((\frac{\alpha_{t}}{\beta_{t}} + \frac{1}{1 - \bar \alpha_{t-1}})x_{t-1}^2 - (\frac{2 \sqrt{\alpha_t}}{\beta_{t}} x_t + \frac{2\sqrt{\bar \alpha_{t-1}}}{1 - \bar \alpha_{t-1}} x_0) x_{t-1} + C(x_t, x_0)))\)</p> <p>\(q(x_{t-1} | x_t, x_0)\) 또한 Gaussian이라서<br/> \(\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x-\mu)^2}{2 \sigma^2}}\) 꼴이므로<br/> \(q(x_{t-1} | x_t, x_0)\)의 지수부분을 \(x_{t-1}\)에 대한 이차식 꼴로 정리하면<br/> 계수 비교를 통해 \(q(x_{t-1} | x_t, x_0)\)의 mean, variance를 알 수 있음!</p> <ul> <li>\(q(x_{t-1} | x_t, x_0)\) 의 variance :<br/> \(\frac{1}{\sigma^{2}}\)<br/> \(= \frac{\alpha_{t}}{\beta_{t}} + \frac{1}{1 - \bar \alpha_{t-1}}\)<br/> \(= \frac{\alpha_{t} - \alpha_{t} \bar \alpha_{t-1} + \beta_{t}}{\beta_{t}(1 - \bar \alpha_{t-1})}\)<br/> \(= \frac{\alpha_{t} - \bar \alpha_{t} + 1 - \alpha_{t}}{\beta_{t}(1 - \bar \alpha_{t-1})}\)<br/> \(= \frac{1 - \bar \alpha_{t}}{\beta_{t}(1 - \bar \alpha_{t-1})}\)</li> </ul> <p>따라서 \(\sigma^{2} = \frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_{t}} \beta_{t}\)</p> <ul> <li>\(q(x_{t-1} | x_t, x_0)\) 의 mean :<br/> \(- \frac{2 \mu}{\sigma^{2}}\)<br/> \(= - (\frac{2 \sqrt{\alpha_t}}{\beta_{t}} x_t + \frac{2\sqrt{\bar \alpha_{t-1}}}{1 - \bar \alpha_{t-1}} x_0)\)<br/> \(\rightarrow \mu = (\frac{\sqrt{\alpha_t}}{\beta_{t}} x_t + \frac{\sqrt{\bar \alpha_{t-1}}}{1 - \bar \alpha_{t-1}} x_0) \cdot \sigma^{2}\)<br/> \(= (\frac{\sqrt{\alpha_t}}{\beta_{t}} x_t + \frac{\sqrt{\bar \alpha_{t-1}}}{1 - \bar \alpha_{t-1}} x_0) \cdot (\frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_{t}} \beta_{t})\)<br/> \(= \frac{\sqrt{\alpha_t} x_t (1 - \bar \alpha_{t-1}) + \beta_{t} x_0 \sqrt{\bar \alpha_{t-1}}}{\beta_{t}(1 - \bar \alpha_{t-1})} \cdot (\frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_{t}} \beta_{t})\)<br/> \(= \frac{\sqrt{\alpha_t} x_t (1 - \bar \alpha_{t-1}) + \beta_{t} x_0 \sqrt{\bar \alpha_{t-1}}}{1 - \bar \alpha_{t}}\)</li> </ul> <p>따라서<br/> \(\mu = \frac{\sqrt{\bar \alpha_{t-1}} \beta_{t}}{1 - \bar \alpha_{t}} x_0 + \frac{\sqrt{\alpha_t} (1 - \bar \alpha_{t-1})}{1 - \bar \alpha_{t}} x_t\)</p> <p>\(q(x_t | x_0) = N(x_t; \sqrt{\bar \alpha_{t}} x_{0}, (1-\bar \alpha_{t}) \boldsymbol I)\)이므로<br/> \(x_0\) <code class="language-plaintext highlighter-rouge">소거</code>하기 위해<br/> \(x_t = \sqrt{\bar \alpha_{t}} x_{0} + \sqrt{1-\bar \alpha_{t}} \epsilon\) 대입하면</p> <p>\(\mu_{t} = \frac{\sqrt{\bar \alpha_{t-1}} \beta_{t}}{1 - \bar \alpha_{t}} x_0 + \frac{\sqrt{\alpha_t} (1 - \bar \alpha_{t-1})}{1 - \bar \alpha_{t}} x_t\)<br/> \(= \frac{\sqrt{\bar \alpha_{t-1}} \beta_{t}}{1 - \bar \alpha_{t}} (\frac{1}{\sqrt{\bar \alpha_{t}}}(x_t - \sqrt{1 - \bar \alpha_{t}} \epsilon_{t})) + \frac{\sqrt{\alpha_t} (1 - \bar \alpha_{t-1})}{1 - \bar \alpha_{t}} x_t\)<br/> \(= \frac{\sqrt{\bar \alpha_{t-1}} (1 - \alpha_{t})}{1 - \bar \alpha_{t}} (\frac{1}{\sqrt{\bar \alpha_{t}}}(x_t - \sqrt{1 - \bar \alpha_{t}} \epsilon_{t})) + \frac{\sqrt{\alpha_t} (1 - \bar \alpha_{t-1})}{1 - \bar \alpha_{t}} x_t\)<br/> \(= \frac{\sqrt{k} (1 - \alpha_{t})}{1 - \alpha_{t} k} (\frac{1}{\sqrt{\alpha_{t} k}}(x_t - \sqrt{1 - \alpha_{t} k} \epsilon_{t})) + \frac{\sqrt{\alpha_t} (1 - k)}{1 - \alpha_{t} k} x_t\)<br/> by \(k = \bar \alpha_{t-1}\) 및 \(\alpha_{t}k = \bar \alpha_{t}\)로 치환<br/> \(= \frac{1}{\sqrt{\alpha_{t}}} x_t - \frac{1 - \alpha_{t}}{\sqrt{\alpha_{t}}\sqrt{1 - \alpha_{t}k}} \epsilon_{t}\)<br/> \(= \frac{1}{\sqrt{\alpha_{t}}} x_t - \frac{1 - \alpha_{t}}{\sqrt{\alpha_{t}}\sqrt{1 - \bar \alpha_{t}}} \epsilon_{t}\)<br/> \(= \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{t})\)</p> <ul> <li>\(q(x_{t-1} | x_t, x_0)\) <code class="language-plaintext highlighter-rouge">결과</code> :<br/> \(q(x_{t-1} | x_t, x_0) = N(x_{t-1}; \tilde \mu_{t}(x_t), \tilde \beta_{t})\)<br/> where \(\tilde \mu_{t}(x_t) = \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{t})\)<br/> and \(\tilde \beta_{t} = \frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_{t}} \beta_{t}\)</li> </ul> <blockquote> <p>Step 8. Obtain \(p_{\theta}(x_{t-1} \| x_t)\)</p> </blockquote> <p>우리의 목적은<br/> \(D_{KL}(q(x_{t-1} | x_t, x_0) \| p_{\theta}(x_{t-1} | x_t))\) 최소화<br/> 즉, \(p\)의 분포를 \(q\)의 분포에 approx.하는 것이다</p> <p>\(q(x_{t-1} | x_t, x_0)\) 의 mean, variance 인<br/> \(\tilde \mu_{t}(x_t) = \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{t})\)<br/> \(\tilde \beta_{t} = \frac{1 - \bar \alpha_{t-1}}{1 - \bar \alpha_{t}} \beta_{t}\) 에서<br/> \(x_t, \alpha_{t}, \beta_{t}\)는 입력값 및 미리 정해놓는 상수값이라서<br/> deep learning network인 \(\epsilon_{\theta}\) 가 시간 t에 따라 \(\epsilon_{t} \sim N(0, I)\) 을 학습하도록 하기 위해서<br/> <code class="language-plaintext highlighter-rouge">training param.로 학습할 수 있는 부분</code>은 \(\epsilon_{t} \sim N(0, I)\) 뿐이다<br/> 즉, <code class="language-plaintext highlighter-rouge">p와 q의 분포에서 차이가 날 수 있는 부분은 epsilon 뿐!</code></p> <p>따라서 \(p_{\theta}(x_{t-1} | x_t)\)의 평균인 \(\mu_{\theta}(x_t, t)\) 는<br/> \(\tilde \mu_{t}(x_t) = \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{t})\) 에서<br/> \(\epsilon_{t}\) 만 \(\epsilon_{\theta}(x_t, t)\) 로 바꾼 값이다<br/> \(\mu_{\theta}(x_t, t) = \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{\theta}(x_t, t))\)</p> <blockquote> <p>Step 9. Final <code class="language-plaintext highlighter-rouge">DDPM Loss</code></p> </blockquote> <p>Step 5.에 따르면 we have to minimize<br/> \(E_{x_{1:T} \sim q(x_{1:T}|x_0)}[\frac{1}{2 \sigma_{t}^2} \| \tilde \mu_{t} (x_t, x_0) - \mu_{\theta} (x_t, t) \|^2] + C\)</p> <p>\(E_{x_0, \epsilon}[\frac{1}{2 \|\Sigma(x_t, t) \|^2} \| \tilde \mu_{t} (x_t, x_0) - \mu_{\theta} (x_t, t) \|^2]\)<br/> \(= E_{x_0, \epsilon}[\frac{1}{2 \|\Sigma \|^2} \| \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{t}) - \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{\theta}(x_t, t)) \|^2]\)<br/> \(= E_{x_0, \epsilon}[\frac{(1 - \alpha_{t})^2}{2 \|\Sigma \|^2 \alpha_{t} (1 - \bar \alpha_{t})} \| \epsilon_{t} - \epsilon_{\theta}(x_t, t) \|^2]\)<br/> \(= E_{x_0, \epsilon}[\frac{(1 - \alpha_{t})^2}{2 \|\Sigma \|^2 \alpha_{t} (1 - \bar \alpha_{t})} \| \epsilon_{t} - \epsilon_{\theta}(\sqrt{\bar \alpha_{t}} x_{0} + \sqrt{1-\bar \alpha_{t}} \epsilon, t) \|^2]\)<br/> since \(x_t = \sqrt{\bar \alpha_{t}} x_{0} + \sqrt{1-\bar \alpha_{t}} \epsilon\)</p> <p>앞의 weight term을 제거하면<br/> <code class="language-plaintext highlighter-rouge">최종 Loss 값</code>은 드디어!!!<br/> \(= E_{x_0, \epsilon}[\| \epsilon_{t} - \epsilon_{\theta}(\sqrt{\bar \alpha_{t}} x_{0} + \sqrt{1-\bar \alpha_{t}} \epsilon, t) \|^2]\)<br/> where \(x_t = q(x_t | x_0) = \sqrt{\bar \alpha_{t}} x_{0} + \sqrt{1-\bar \alpha_{t}} \epsilon\) and \(\epsilon \sim N(0, I)\)</p> <h3 id="ddpm-pseudo-code">DDPM Pseudo-Code</h3> <ul> <li><code class="language-plaintext highlighter-rouge">forward</code> process : <code class="language-plaintext highlighter-rouge">Training</code> \(\epsilon_{\theta}\) for given input image \(x_0\)</li> </ul> <pre><code class="language-Python">while (converge){
  x_0 ~ q(x_0) # input image
  t ~ Uniform({1, ..., T}) # time step (integer)
  epsilon ~ N(0, I) # Gaussian target epsilon

  # gradient descent by DDPM loss
}
</code></pre> <p>DDPM loss :<br/> \(E_{x_0, \epsilon}[\| \epsilon_{t} - \epsilon_{\theta}(\sqrt{\bar \alpha_{t}} x_{0} + \sqrt{1-\bar \alpha_{t}} \epsilon, t) \|^2]\)</p> <ul> <li><code class="language-plaintext highlighter-rouge">backward</code> process : <code class="language-plaintext highlighter-rouge">Sampling</code> from Gaussian noise img to new img by trained \(\epsilon_{\theta}\)</li> </ul> <pre><code class="language-Python">x_T ~ N(0, I) # start with Gaussian noise image

for (t = T, ..., 1){
  z ~ N(0, I) if t &gt; 1 else z = 0  
  # sampling x_{t-1} from x_t by p_{theta}(x_{t-1} | x_t)
}
</code></pre> <p>Sampling :<br/> \(x_{t-1} = \frac{1}{\sqrt{\alpha_{t}}} (x_t - \frac{1 - \alpha_{t}}{\sqrt{1 - \bar \alpha_{t}}} \epsilon_{\theta}(x_t, t)) + \sigma_{t} z\)</p> <blockquote> <p>출처 블로그 :<br/> <a href="https://xoft.tistory.com/32">Diffusion Model</a><br/> <a href="https://xoft.tistory.com/33?category=1156151">DDPM 수식 유도</a><br/> <a href="https://woongchan789.tistory.com/12">DDPM 수식 유도</a></p> </blockquote>]]></content><author><name></name></author><category term="generative"/><category term="diffusion"/><category term="generative"/><summary type="html"><![CDATA[Diffusion Study]]></summary></entry><entry><title type="html">NeRF-Code</title><link href="https://semyeong-yu.github.io/blog/2024/NeRFcode/" rel="alternate" type="text/html" title="NeRF-Code"/><published>2024-06-23T15:00:00+00:00</published><updated>2024-06-23T15:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/NeRFcode</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/NeRFcode/"><![CDATA[<h2 id="nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h2> <h4 id="ben-mildenhall-pratul-psrinivasan-matthew-tancik">Ben Mildenhall, Pratul P.Srinivasan, Matthew Tancik</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2003.08934">https://arxiv.org/abs/2003.08934</a><br/> project website :<br/> <a href="https://www.matthewtancik.com/nerf">https://www.matthewtancik.com/nerf</a><br/> pytorch code :<br/> <a href="https://github.com/yenchenlin/nerf-pytorch">https://github.com/yenchenlin/nerf-pytorch</a><br/> <a href="https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file">https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file</a><br/> tiny tensorflow code :<br/> <a href="https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb">https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb</a></p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_rays_np</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">c2w</span><span class="p">):</span>
    <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">W</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">W</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">H</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">H</span><span class="p">))</span> <span class="c1"># pytorch's meshgrid has indexing='ij', so both i and j have shape (W, H)
</span>    <span class="n">i</span> <span class="o">=</span> <span class="n">i</span><span class="p">.</span><span class="nf">t</span><span class="p">()</span> <span class="c1"># width grid : shape (H, W)
</span>    <span class="n">j</span> <span class="o">=</span> <span class="n">j</span><span class="p">.</span><span class="nf">t</span><span class="p">()</span> <span class="c1"># height grid : shape (H, W)
</span>
    <span class="c1"># Apply intrinsic matrix
</span>    <span class="n">dirs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([(</span><span class="n">i</span><span class="o">-</span><span class="n">K</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span><span class="o">/</span><span class="n">K</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="p">(</span><span class="n">j</span><span class="o">-</span><span class="n">K</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span> <span class="o">/</span> <span class="n">K</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">i</span><span class="p">)],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># dirs : shape (H, W, 3) : H*W개의 3D rays
</span>    
    <span class="c1"># Apply extrinsic matrix
</span>    <span class="c1"># Rotate ray directions from camera frame to the world frame by applying dot product
</span>    <span class="n">rays_d</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dirs</span><span class="p">[...,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">c2w</span><span class="p">[:</span><span class="mi">3</span><span class="p">,</span> <span class="p">:</span><span class="mi">3</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># same with "rays_d = [c2w.dot(dir) for dir in dirs]"
</span>    <span class="c1"># dirs[..., np.newaxis, :] : shape (H, W, 1, 3) -&gt; (H, W, 3, 3) by broadcasting 
</span>    <span class="c1"># c2w[:3, :3] : shape (3, 3) -&gt; (H, W, 3, 3) by broadcasting
</span>    <span class="c1"># rays_d : shape (H, W, 3)
</span>    
    <span class="c1"># Translate camera frame's origin to the world frame. It is the origin of all rays.
</span>    <span class="n">rays_o</span> <span class="o">=</span> <span class="n">c2w</span><span class="p">[:</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">expand</span><span class="p">(</span><span class="n">rays_d</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="c1"># rays_o : shape (3, H*W) -&gt; (H, W, 3)
</span>    <span class="k">return</span> <span class="n">rays_o</span><span class="p">,</span> <span class="n">rays_d</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="rendering"/><category term="3d"/><summary type="html"><![CDATA[NeRF Code Review]]></summary></entry><entry><title type="html">MipNeRF</title><link href="https://semyeong-yu.github.io/blog/2024/MipNeRF/" rel="alternate" type="text/html" title="MipNeRF"/><published>2024-06-17T21:00:00+00:00</published><updated>2024-06-17T21:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/MipNeRF</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/MipNeRF/"><![CDATA[<h2 id="mip-nerf-a-multiscale-representation-for-anti-aliasing-neural-radiance-fields">Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</h2> <h4 id="jonathan-t-barron-ben-mildenhall-matthew-tancik-peter-hedman-ricardo-martin-brualla-pratul-p-srinivasan">Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2103.13415">https://arxiv.org/abs/2103.13415</a><br/> project website :<br/> <a href="https://jonbarron.info/mipnerf/">https://jonbarron.info/mipnerf/</a><br/> pytorch code :<br/> <a href="https://github.com/bebeal/mipnerf-pytorch">https://github.com/bebeal/mipnerf-pytorch</a><br/> <a href="https://github.com/google/mipnerf">https://github.com/google/mipnerf</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>Mip-NeRF는 training하는 동안, <code class="language-plaintext highlighter-rouge">임의의 scale</code>에 대해 query 받을 수 있는, scene의 <code class="language-plaintext highlighter-rouge">anti-aliased</code> <code class="language-plaintext highlighter-rouge">pre-filtered representation</code>을 학습한다.</li> <li>camera center로부터 각 pixel로 3D conical frustum을 쏜 다음, the <code class="language-plaintext highlighter-rouge">frustum을 multi-variate Gaussian으로 근사</code>한 뒤, Gaussian 내 좌표를 positional encoding한 것에 대해 <code class="language-plaintext highlighter-rouge">integral</code> \(E \left[ \gamma (x) \right]\) 계산</li> </ol> </blockquote> <h2 id="introduction">Introduction</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-06-17-MipNeRF/2-480.webp 480w,/assets/img/2024-06-17-MipNeRF/2-800.webp 800w,/assets/img/2024-06-17-MipNeRF/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-06-17-MipNeRF/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>기존 NeRF의 문제점 :<br/> rendering 위해 sampling할 때 <code class="language-plaintext highlighter-rouge">single ray</code> per pixel 쏴서 composite 하므로<br/> dataset images에 있는 물체의 크기가 일정하지 않을 때 (multiple resolutions) multi-scales images에 대해 학습하더라도<br/> high-resolution은 <code class="language-plaintext highlighter-rouge">blurry</code> rendering<br/> low-resolution은 <code class="language-plaintext highlighter-rouge">aliased</code> rendering<br/> 그렇다고 multiple rays per pixel through its footprint로 brute-force supersampling하는 것은 정확하긴 하겠지만 too costly 비현실적</p> </li> <li> <p>Minmapping Approach :<br/> classic 컴퓨터 그래픽스 분야에서 rendering할 때 aliasing을 없애기 위한 <code class="language-plaintext highlighter-rouge">pre-filtering</code> 기법<br/> 본 논문인 Mip-NeRF가 여기서 영감을 얻음<br/> signal(e.g. image)을 diff. downsampling scales로 나타낸 뒤 pixel footprint를 근거로 ray에 사용하기 위한 <code class="language-plaintext highlighter-rouge">적절한 scale을 고른다</code><br/> render time에 할 복잡할 일을 precomputation phase에 미리 하는 것일 뿐이긴 하지만, 주어진 texture마다 한 번만 minmap을 만들면 된다는 장점이 있다</p> </li> <li> <p>Mip-NeRF :</p> <ul> <li>represent pre-filtered scene at <code class="language-plaintext highlighter-rouge">continuous space of scales</code></li> <li>ray 대신 <code class="language-plaintext highlighter-rouge">conical frustum</code> 사용해서 <code class="language-plaintext highlighter-rouge">anti-aliased</code> rendering with fine details</li> <li>multiscale variant of dataset에 대해 평균 error rate 60% 감소</li> <li>NeRF가 hierarchical sampling을 위해 coarse and fine MLP를 분리했다면, Mip-NeRF는 <code class="language-plaintext highlighter-rouge">scale-aware</code>하므로 <code class="language-plaintext highlighter-rouge">single MLP만으로 충분</code><br/> 따라서 NeRF보다 7% 빠르고, param. 수는 절반</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-06-17-MipNeRF/1-480.webp 480w,/assets/img/2024-06-17-MipNeRF/1-800.webp 800w,/assets/img/2024-06-17-MipNeRF/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-06-17-MipNeRF/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>차이 : 기존 NeRF는 <code class="language-plaintext highlighter-rouge">a single point</code>를 encode하고, Mip-NeRF는 <code class="language-plaintext highlighter-rouge">a region of space</code>를 encode</p> </li> <li> <p>기존 NeRF :<br/> camera center로부터 각 pixel로 ray를 하나 쏜 다음 point sampling한 뒤 positional encoding</p> </li> <li> <p>Mip-NeRF :<br/> camera center로부터 각 pixel로 3D conical frustum을 쏜 다음, 3D point 및 그 주위의 Gaussian region을 encode하기 위해 <code class="language-plaintext highlighter-rouge">IPE(integrated positional encoding)</code><br/> IPE : the <code class="language-plaintext highlighter-rouge">frustum을 multi-variate Gaussian으로 근사</code>한 뒤, Gaussian 내 좌표를 positional encoding한 것에 대해 <code class="language-plaintext highlighter-rouge">integral</code> \(E \left[ \gamma (x) \right]\) 계산</p> </li> </ul> <h2 id="related-work">Related Work</h2> <h4 id="anti-aliasing-in-rendering">Anti-aliasing in Rendering</h4> <blockquote> <p><code class="language-plaintext highlighter-rouge">anti-aliasing</code>을 위한 고전적인 방법으로는 두 가지가 있다.</p> </blockquote> <ol> <li><code class="language-plaintext highlighter-rouge">supersampling</code> : <ul> <li>rendering할 때 <code class="language-plaintext highlighter-rouge">multiple rays per pixel</code>을 쏴서 Nyquist frequency에 가깝게 supersampling</li> <li><code class="language-plaintext highlighter-rouge">expensive</code> as runtime increases linearly with the supersampling rate, so used only in <code class="language-plaintext highlighter-rouge">offline</code> rendering</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">pre-filtering</code> : <ul> <li>target sampling rate에 맞춰서 Nyquist frequency를 줄이기 위해 scene에 <code class="language-plaintext highlighter-rouge">lowpass-filter</code>를 씌운 버전 사용</li> <li>scene을 미리 downsampling <code class="language-plaintext highlighter-rouge">multi-scales</code> (sparse voxel octree 또는 minmap)로 나타낸 뒤, <code class="language-plaintext highlighter-rouge">ray 대신 cone</code>을 추적하여 cone과 scene이 만나는 곳의 cone’s footprint에 대응되는 <code class="language-plaintext highlighter-rouge">적절한 scale</code>을 골라서 사용 (target sampling rate에 맞는 적절한 scale)</li> <li>scene에 filter 씌운 버전을 한 번만 미리 계산하면 되므로, better for <code class="language-plaintext highlighter-rouge">real-time</code> rendering</li> </ul> </li> </ol> <blockquote> <p>그런데 아래의 이유로 고전적인 multi-scale representation은 적용 불가능<br/> input scene의 geometry를 미리 알 수 없음<br/> input scene의 <code class="language-plaintext highlighter-rouge">scale이 continuous</code>하므로 a fixed number of scales (discrete)과 상황이 다름</p> </blockquote> <p>\(\rightarrow\) 결론 : 따라서 Mip-NeRF는 training하는 동안, <code class="language-plaintext highlighter-rouge">임의의 scale</code>에 대해 query 받을 수 있는, scene의 <code class="language-plaintext highlighter-rouge">pre-filtered representation</code>을 학습한다.</p> <h4 id="scene-representations-for-view-synthesis">Scene Representations for View Synthesis</h4> <ul> <li> <p>만약 images of scene are captured <code class="language-plaintext highlighter-rouge">densely</code> 라면, novel view synthesis 위해, intermediate representation of scene을 reconstruct하지 않고 <code class="language-plaintext highlighter-rouge">light field interpolation</code> 기법 사용</p> </li> <li>만약 images of scene are captured <code class="language-plaintext highlighter-rouge">sparsely</code> 라면, novel view synthesis 위해, <code class="language-plaintext highlighter-rouge">explicit representation</code> of the scene’s 3D geometry and appearance를 reconstruct <ul> <li><code class="language-plaintext highlighter-rouge">polygon-mesh-based</code> representation (<code class="language-plaintext highlighter-rouge">discrete, classic</code>) :<br/> with either diffuse or view-dependent textures<br/> can be stored efficiently, but mesh geometry optimization에 gradient descent를 이용하는 것은 불연속점 및 극소점 때문에 어렵</li> <li><code class="language-plaintext highlighter-rouge">volumetric</code> representation : <ul> <li><code class="language-plaintext highlighter-rouge">voxel-grid-based</code> representation (<code class="language-plaintext highlighter-rouge">discrete, classic</code>) : deep learning으로 학습 가능, but 고해상도의 scene에는 부적합</li> <li><code class="language-plaintext highlighter-rouge">coordinate-based</code> neural representation (<code class="language-plaintext highlighter-rouge">continuous, recent</code>) : 3D 좌표를 그 위치에서의 scene의 특징(e.g. volume density, radiance)으로 mapping하는 continuous function을 MLP로 예측 <ul> <li>implicit surface representation</li> <li><code class="language-plaintext highlighter-rouge">implicit volumetric NeRF representation</code></li> </ul> </li> </ul> </li> </ul> </li> <li>문제점 :<br/> 그동안 view synthesis를 할 때 sampling 및 aliasing에는 덜 주목했다<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">classic discrete</code> representation의 경우 앞에서 소개한 minmap, octree와 같은 고전적인 multi-scale pre-filtering 기법을 쓰면 aliasing 없이 rendering 가능하다<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">coordinate-based</code> representation의 경우 scale이 continuous하므로 고전적인 anti-aliasing 기법은 사용할 수 없다<br/> \(\rightarrow\) sparse voxel octree 기반의 multi-scale representation으로 implicit surface의 continuous neural representation을 구현한 논문 <d-cite key="continuouspriori">[1]</d-cite> 있긴 하지만, scene geometry의 priori를 알아야 한다는 제약이 있다<br/> \(\rightarrow\) Mip-NeRF는 training하는 동안, <code class="language-plaintext highlighter-rouge">임의의 scale</code>에 대해 query 받을 수 있는, scene의 anti-aliased <code class="language-plaintext highlighter-rouge">pre-filtered representation</code>을 학습한다.</li> </ul> <h2 id="method">Method</h2> <h4 id="cone-tracing-and-positional-encoding">Cone Tracing and Positional Encoding</h4> <p>dd</p> <h4 id="architecture">Architecture</h4> <p>dd</p> <h2 id="results">Results</h2> <p>dd</p> <h2 id="conclusion">Conclusion</h2> <p>dd</p>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="rendering"/><category term="3d"/><category term="multiscale"/><category term="antialiasing"/><summary type="html"><![CDATA[A Multiscale Representation for Anti-Aliasing Neural Radiance Fields]]></summary></entry><entry><title type="html">SegmentAnything</title><link href="https://semyeong-yu.github.io/blog/2024/SegmentAnything/" rel="alternate" type="text/html" title="SegmentAnything"/><published>2024-05-29T14:00:00+00:00</published><updated>2024-05-29T14:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/SegmentAnything</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/SegmentAnything/"><![CDATA[<h3 id="segmentanything">SegmentAnything</h3> <h4 id="alexander-kirillov-eric-mintun-nikhila-ravi-hanzi-mao-chloe-rolland-laura-gustafson-tete-xiao-spencer-whitehead-alexander-c-berg-wan-yen-lo-piotr-dollár-ross-girshick">Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2304.02643">https://arxiv.org/abs/2304.02643</a><br/> 출처 : Vision study mkd님</p> </blockquote> <hr/> <h2 id="abstract">Abstract</h2> <ul> <li> <p>Task<br/> Promptable Image Segmentation</p> </li> <li> <p>Model Architecture<br/> image encoder + prompt encoder + mask decoder</p> </li> <li> <p>Generate Data (Data Engine)<br/> assisted-manual stage \(\rightarrow\) semi-automatic stage \(\rightarrow\) fully-automatic stage<br/> data ‘SA-1B’ : 1B masks with 11M images</p> </li> <li> <p>Enable Zero-Shot Generalization<br/> Zero-Shot transfer to various tasks</p> </li> <li> <p>Code Review</p> </li> </ul> <h2 id="overview">Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/2-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/2-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>prompt : mask를 생성할 대상을 지정<br/> point, BB, mask(rough area), text(preliminary) 중 하나</p> </li> <li> <p>valid masks : segmented mask를 하나가 아닌 3개 (whole, part, sub-part) 생성<br/> ambiguous prompt에 대응하기 위해, zero shot을 위해<br/> 3개의 masks 중 GT와 가장 유사한(confidence score가 가장 높은) mask의 loss만 사용</p> </li> </ul> <h2 id="model">Model</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/3-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/3-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Image Encoder : MAE (Masked AutoEncoder) 방식의 ViT<br/> MAE 요약 : 이미지를 grid로 나누고 patches 중 일부를 가린 뒤 원본을 복원하도록 학습하고, 학습이 끝난 후에는 encoder embedding만 사용<br/> ViT-H/16 : 14 \(\times\) 14 windowed attention and 4 global attention blocks</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/4-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/4-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Prompt Encoder :<br/> Mask (dense prompt) : conv. 거친 후 image embedding에 pixel-wise sum (mask가 없는 pixel의 경우 ‘no mask’ prompt 사용)<br/> Point (sparse prompt) : positional encoding + learned embedding(fg or bg)<br/> BB (sparse prompt) : positional encoding + learned embedding(top-left or bottom-right)<br/> text (sparse prompt) : by CLIP text encoder</p> </li> <li> <p>Loss :</p> <ol> <li>Mask loss : related to mask prediction<br/> 1-1. focal loss : \(L(p_{t}) = - (1-p_{t})^{r}log(p_{t})\) where \((1-p_{t})^{r}\) gives more weight to few hard examples (\(p_{t} \sim 0\))<br/> 1-2. dice loss : 1 - dice score where dice score = \(\frac{2 \times Area(A \cap B)}{Area(A) + Area(B)}\)</li> </ol> </li> </ul> <ol> <li>IoU loss : related to confidence score<br/> MSE loss</li> </ol> <h2 id="data--develop-data-engine-by-curriculum-learning">Data : Develop Data Engine by Curriculum Learning</h2> <ul> <li> <p>Assisted-manual stage :<br/> public segmentation dataset \(\rightarrow\) SAM \(\rightarrow\) pixel-wise manual augmentation \(\rightarrow\) re-train<br/> After re-training, the number of masks per image increased from 20 to 44 in average<br/> Collect 4.3M masks from 0.12M images</p> </li> <li> <p>Semi-automatic stage :<br/> dataset from previous stage (4.3M masks) \(\rightarrow\) SAM \(\rightarrow\) mask predict 실패한(제외된) object를 annotate \(\rightarrow\) re-train<br/> After re-training, the number of masks per image increased from 44 to 72 in average<br/> Collect 5.9M masks from 0.18M images (totally 4.3M + 5.9M = 10.2M masks)</p> </li> <li> <p>Fully-automatic stage :<br/> dataset from previous stage (10.2M masks) : image에 32 \(\times\) 32 grid points 찍음 \(\rightarrow\) SAM<br/> ambiguity-aware training (whole, part, sub-part 구분 가능)<br/> After filtering masks with high confidence score,<br/> Collect SA-1B dataset : 1.1B masks from 11M images (various HR masks)<br/> 99.1% is fully-automatically generated<br/> follow RAI (Responsible AI) : no bias and blur human faces</p> </li> </ul> <h2 id="task">Task</h2> <p>generalizable (zero-shot transfer to various tasks)</p> <ul> <li>Zero-Shot Transfer Tasks : <ol> <li>Zero-Shot Single Point Valid Mask Evaluation</li> <li>Zero-Shot Edge Detection</li> <li>Zero-Shot Object Proposals</li> <li>Zero-Shot Instance Segmentation</li> <li>Zero-Shot Text-to-Mask (CLIP)</li> </ol> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/4-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/4-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Zero-Shot Single Point Valid Mask Evaluation :<br/> point 찍었을 때 그에 해당하는 mask를 얼마나 잘 생성하는가<br/> use one most-confident mask<br/> compare with RITM model on 23 datasets</p> </li> <li> <p>Zero-Shot Edge Detection :</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/6-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/6-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>About filter : 블로그 맨 아랫 부분에 설명해놓음</p> <ul> <li> <p>Zero-Shot Object Proposals :<br/> mask 예측 후 object의 identity(class)를 얼마나 잘 맞추는가</p> </li> <li> <p>Zero-Shot Instance Segmentation :</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/7-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/7-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Zero-Shot Text-to-Mask :<br/> image \(\rightarrow\) CLIP \(\rightarrow\) image embedding as input<br/> text \(\rightarrow\) CLIP \(\rightarrow\) text embedding as SAM prompt</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/8-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/8-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>SAM’s latent space에서 similar mask embedding vectors within threshold를 추출한 결과 실제로도 semantically similar</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/9-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/9-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A query is indicated by magenta box : top row shows matches at a low threshold and bottom row shows matches at a high threshold </div> <h2 id="code-review">Code Review</h2> <p>다음에 해야지… 라고 미뤄둠..ㅎㅎ</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/10-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/10-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/11-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/11-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/11.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/12-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/12-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/13-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/13-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/13.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/14-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/14-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/14.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/15-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/15-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/15.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/16-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/16-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/16-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/16.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/17-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/17-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/17-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/17.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/18-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/18-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/18-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/18.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/19-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/19-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/19-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/19.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/20-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/20-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/20-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/20.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/21-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/21-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/21-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/21.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/22-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/22-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/22-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/22.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/23-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/23-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/23-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/23.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/24-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/24-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/24-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/24.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/25-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/25-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/25-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/25.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/26-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/26-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/26-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/26.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/27-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/27-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/27-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/27.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/28-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/28-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/28-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/28.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/29-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/29-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/29-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/29.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/30-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/30-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/30-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/30.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/31-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/31-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/31-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/31.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/32-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/32-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/32-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/32.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/33-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/33-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/33-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/33.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/34-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/34-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/34-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/34.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/35-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/35-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/35-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/35.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/36-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/36-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/36-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/36.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/37-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/37-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/37-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/37.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/38-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/38-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/38-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/38.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/39-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/39-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/39-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/39.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/40-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/40-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/40-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/40.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/41-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/41-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/41-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/41.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container>]]></content><author><name></name></author><category term="cv-tasks"/><category term="image"/><category term="segmentation"/><summary type="html"><![CDATA[Promptable Image Segmentation]]></summary></entry><entry><title type="html">FMANet</title><link href="https://semyeong-yu.github.io/blog/2024/FMANet/" rel="alternate" type="text/html" title="FMANet"/><published>2024-05-02T14:00:00+00:00</published><updated>2024-05-02T14:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/FMANet</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/FMANet/"><![CDATA[<h3 id="fma-net--flow-guided-dynamic-filtering-and-iterative-feature-refinement-with-multi-attention-for-joint-video-super-resolution-and-deblurring">FMA-Net : Flow-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring</h3> <h4 id="geunhyuk-youk-jihyong-oh-munchurl-kim">Geunhyuk Youk, Jihyong Oh, Munchurl Kim</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2401.03707">https://arxiv.org/abs/2401.03707</a><br/> project website :<br/> <a href="https://kaist-viclab.github.io/fmanet-site/">https://kaist-viclab.github.io/fmanet-site/</a><br/> pytorch code :<br/> <a href="https://github.com/KAIST-VICLab/FMA-Net">https://github.com/KAIST-VICLab/FMA-Net</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> </blockquote> <hr/> <img-comparison-slider> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/9-480.webp 480w,/assets/img/2024-05-02-FMANet/9-800.webp 800w,/assets/img/2024-05-02-FMANet/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/10-480.webp 480w,/assets/img/2024-05-02-FMANet/10-800.webp 800w,/assets/img/2024-05-02-FMANet/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> <img-comparison-slider> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/11-480.webp 480w,/assets/img/2024-05-02-FMANet/11-800.webp 800w,/assets/img/2024-05-02-FMANet/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/12-480.webp 480w,/assets/img/2024-05-02-FMANet/12-800.webp 800w,/assets/img/2024-05-02-FMANet/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> <h2 id="abstract">Abstract</h2> <p><strong>Task : Joint learning of VSRDB (<code class="language-plaintext highlighter-rouge">video super-resolution and deblurring</code>)</strong></p> <ul> <li>restore HR video from blurry LR video<br/> challenging because should handle two types of degradation (SR and deblurring) simultaneously</li> <li>super-resolution : LR vs HR</li> <li>deblurring : blurry vs sharp</li> </ul> <p><strong>FGDF (<code class="language-plaintext highlighter-rouge">flow-guided dynamic filtering</code>)</strong></p> <ul> <li>precise estimation of both <code class="language-plaintext highlighter-rouge">spatio-temporally-variant</code> <code class="language-plaintext highlighter-rouge">degradation</code> and <code class="language-plaintext highlighter-rouge">restoration</code> kernels that are aware of motion trajectories (not stick to fixed positions)</li> <li>effectively <code class="language-plaintext highlighter-rouge">handle large motions with small-sized kernels</code> (naive dynamic filtering의 한계 극복)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/2-480.webp 480w,/assets/img/2024-05-02-FMANet/2-800.webp 800w,/assets/img/2024-05-02-FMANet/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>DCN (Deformable Conv.) : learn position-invariant \(n \times n\) filter coeff.<br/> vs<br/> DF (Dynamic filtering) : learn position-wise \(n \times n\) dynamic filter coeff.</p> <p>DF (Dynamic Filtering) : fixed surroundings<br/> vs<br/> FGDF (Flow Guided DF) : variable surroundings by learned optical flow</p> <p><strong>FRMA (<code class="language-plaintext highlighter-rouge">iterative feature refinement with multi-attention</code>)</strong></p> <p>refine features by iterative updates<br/> loss : TA (temporal anchor)<br/> multi-attention :</p> <ul> <li><code class="language-plaintext highlighter-rouge">center-oriented</code> attention (focus on target frame)</li> <li><code class="language-plaintext highlighter-rouge">degradation-aware</code> attention (use degradation kernels in globally adaptive manner)</li> </ul> <hr/> <h2 id="related-work">Related Work</h2> <p><strong>VSR (Video Super-Resolution)</strong></p> <p>Based on the number of input frames,</p> <ol> <li><code class="language-plaintext highlighter-rouge">sliding window</code>-based method : recover HR frames by using neighboring frames within a sliding window<br/> use CNN, optical flow estimation, deformable conv., or transformer focusing on temporal alignment<br/> vs</li> <li><code class="language-plaintext highlighter-rouge">recurrent</code>-based method : sequentially propagate the latent features of one frame to the next frame<br/> Chan et al. <d-cite key="vsr">[1]</d-cite> BasicVSR++ : combine bidirectional propagation of past and future frames into current frame features<br/> limit : gradient vanishing</li> </ol> <p><strong>DB (Video Deblurring)</strong></p> <p>Zhang et al. <d-cite key="adversarial">[2]</d-cite> 3D CNN<br/> Li et al. <d-cite key="groupshift">[3]</d-cite> grouped spatial-temporal shifts<br/> transformer-based : Restormer <d-cite key="restormer">[4]</d-cite>, Stripformer <d-cite key="stripformer">[5]</d-cite>, RVRT <d-cite key="rvrt">[6]</d-cite></p> <p><strong>Joint learning of VSRDB (not sequential cascade of VSR and DB)</strong></p> <p>Previous works are mostly designed for ISRDB</p> <p>Fang et al. <d-cite key="HOFFR">[7]</d-cite> HOFFR : the first deep-learning-based VSRDB<br/> limit : struggle to deblur spatially-variant motion blur because 2D CNN has spatially-equivariant and input-independent filters</p> <p><strong>Dynamic Filter Network</strong></p> <p>predict spatially-variant degradation or restoration kernels</p> <p>Zhou et al. <d-cite key="adaptivefilter">[8]</d-cite> :<br/> spatially adaptive deblurring filter for recurrent video deblurring<br/> Kim et al. <d-cite key="koalanet">[9]</d-cite> KOALAnet :<br/> blind SR predicts spatially-variant degradation and upsampling filters</p> <ul> <li>limit : apply dynamic filtering only to the reference frame (target position and its fixed surrounding neighbors), so cannot accurately exploit spatio-temporally-variant-motion info. from adjacent frames</li> <li>limit : if apply dynamic filtering to adjacent frames \(\rightarrow\) large-sized filters are required to capture large motions \(\rightarrow\) high computational complexity</li> <li>limit : <d-cite key="separableconv">[10]</d-cite> suggested two separable large 1D kernels to approximate a large 2D kernel \(\rightarrow\) does not capture fine detail, so inappropriate for video</li> </ul> <hr/> <h2 id="method">Method</h2> <p><strong>Overview</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/1-480.webp 480w,/assets/img/2024-05-02-FMANet/1-800.webp 800w,/assets/img/2024-05-02-FMANet/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>FMA-Net : VSRDB framework based on FGDF and FRMA<br/> allow for small-to-large motion representation learning</p> <ul> <li>input : <code class="language-plaintext highlighter-rouge">blurry LR sequence</code> \(X = \left\lbrace X_{c-N}:X_{c+N} \right\rbrace \in R^{T \times H \times W \times 3}\) where \(T=2N+1\) and \(c\) is a center frame index</li> <li>goal : predict <code class="language-plaintext highlighter-rouge">sharp HR center frame</code> \(\hat Y_{c} \in R^{sH \times sW \times 3}\) where \(s\) is SR scale factor</li> </ul> <ol> <li><code class="language-plaintext highlighter-rouge">degradation</code> learning network \(Net^{D}\) : learn <code class="language-plaintext highlighter-rouge">motion-aware</code> <code class="language-plaintext highlighter-rouge">spatio-temporally-variant</code> degradation kernels</li> <li><code class="language-plaintext highlighter-rouge">restoration</code> network \(Net^{R}\) : utilize these degradation kernels in a globally adaptive manner to restore center frame \(X_c\)</li> <li>\(Net^{D}\) and \(Net^{R}\) consist of FRMA blocks and FGDF module</li> </ol> <p><strong>FRMA block</strong></p> <p>pre-trained optical flow network : unstable for blurry frames and computationally expensive</p> <p>vs</p> <blockquote> <p>FRMA block :<br/> learn <code class="language-plaintext highlighter-rouge">self-induced</code> optical flow in a residual learning manner<br/> learn <code class="language-plaintext highlighter-rouge">multiple</code> optical flows with corresponding occlusion masks<br/> \(\rightarrow\) flow diversity enables to learn one-to-many relations b.w. pixels in a target frame and its neighbor frames<br/> \(\rightarrow\) beneficial since <code class="language-plaintext highlighter-rouge">blurry frame's pixel info. is spread due to light accumulation</code></p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/3-480.webp 480w,/assets/img/2024-05-02-FMANet/3-800.webp 800w,/assets/img/2024-05-02-FMANet/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Three features</p> <ol> <li>\(F \in R^{T \times H \times W \times C}\) :<br/> <code class="language-plaintext highlighter-rouge">temporally-anchored (unwarped)</code> feature at each frame index \(0 \sim T-1\)</li> <li>\(F_w \in R^{H \times W \times C}\) :<br/> <code class="language-plaintext highlighter-rouge">warped</code> feature</li> <li>\(\boldsymbol f = \left \lbrace f_{c \rightarrow c+t}^{j}, o_{c \rightarrow c+t}^{j} \right \rbrace _{j=1:n}^{t=-N:N} \in R^{T \times H \times W \times (2+1)n}\) :<br/> multi-<code class="language-plaintext highlighter-rouge">flow-mask</code> pairs<br/> \(f_{c \rightarrow c+t}^{j}\) : learnable optical flow<br/> \(o_{c \rightarrow c+t}^{j}\) : learnable occlusion mask (sigmoid for stability)<br/> \(n\) is the number of multi-flow-mask pairs from the center frame index \(c\) to each frame index<br/> <code class="language-plaintext highlighter-rouge">왜 dim. (2+1)???</code></li> </ol> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/16-480.webp 480w,/assets/img/2024-05-02-FMANet/16-800.webp 800w,/assets/img/2024-05-02-FMANet/16-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>(i+1)-th Feature Refinement : 위첨자로 표기<br/> <code class="language-plaintext highlighter-rouge">feature refine 식 기원??</code></p> <ol> <li>\(F^{i+1}\)=RDB(\(F^{i}\)) :<br/> RDB <d-cite key="RDB">[11]</d-cite></li> <li>\(\boldsymbol f^{i+1}\) = \(\boldsymbol f^{i}\) + Conv3d(concat(\(\boldsymbol f^{i}\), \(W\)(\(F^{i+1}\), \(\boldsymbol f^{i}\)), \(F_{c}^{0}\)))<br/> \(W\)(\(F^{i+1}\), \(\boldsymbol f^{i}\)) : warp \(F^{i+1}\) to center frame index \(c\) based on \(f^{i}\)<br/> \(W\) : occlusion-aware backward warping<br/> concat : along channel dim.<br/> \(F_{c}^{0} \in R^{H \times W \times C}\) : feature map at center frame index \(c\) of the initial feature \(F^{0} \in R^{T \times H \times W \times C}\)</li> <li>\(\tilde F_{w}^{i}\) = Conv2d(concat(\(F_{w}^{i}\), \(r_{4 \rightarrow 3}\)(\(W\)(\(F^{i+1}\), \(\boldsymbol f^{i+1}\)))))<br/> \(r_{4 \rightarrow 3}\) : reshape from \(R^{T \times H \times W \times C}\) to \(R^{H \times W \times TC}\) for feature aggregation</li> <li>\(F_w^{i+1}\) = Multi-Attn(\(\tilde F_{w}^{i}\), \(F_{c}^{0}\)(, \(k^{D, i}\)))</li> </ol> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/15-480.webp 480w,/assets/img/2024-05-02-FMANet/15-800.webp 800w,/assets/img/2024-05-02-FMANet/15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>RDB Network <d-cite key="RDB">[11]</d-cite> :<br/> TBD</p> </blockquote> <blockquote> <p>RRDB Network <d-cite key="rrdb">[15]</d-cite> :<br/> TBD</p> </blockquote> <blockquote> <p>Occlusion-Aware Backward Warping <d-cite key="warp">[12]</d-cite> <d-cite key="warpp">[13]</d-cite> <d-cite key="warppp">[14]</d-cite> :<br/> TBD</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/3-480.webp 480w,/assets/img/2024-05-02-FMANet/3-800.webp 800w,/assets/img/2024-05-02-FMANet/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Multi-Attention :</p> <ul> <li><code class="language-plaintext highlighter-rouge">CO(center-oriented)</code> attention :<br/> better align \(\tilde F_{w}^{i}\) to \(F_{c}^{0}\) (center feature map of initial temporally-anchored feature)</li> <li><code class="language-plaintext highlighter-rouge">DA(degradation-aware)</code> attention :<br/> \(\tilde F_{w}^{i}\) becomes globally adaptive to spatio-temporally variant degradation by using degradation kernels \(K^{D}\)</li> </ul> </blockquote> <ul> <li> <p>CO attention :<br/> \(Q=W_{q} F_{c}^{0}\)<br/> \(K=W_{k} \tilde F_{w}^{i}\)<br/> \(V=W_{v} \tilde F_{w}^{i}\)<br/> \(COAttn(Q, K, V) = softmax(\frac{QK^{T}}{\sqrt{d}})V\)<br/> 실험 결과, \(\tilde F_{w}^{i}\)가 자기 자신(self-attention)이 아니라 \(F_{c}^{0}\)과의 relation에 집중할 때 better performance</p> </li> <li> <p>DA attention :<br/> CO attention과 비슷하지만,<br/> Query 만들 때 \(F_{c}^{0}\) 대신 \(k^{D, i}\) 사용<br/> \(\tilde F_{w}^{i}\) becomes globally adaptive to spatio-temporally-variant degradation<br/> \(k^{D, i} \in R^{H \times W \times C}\) : degradation features adjusted by conv. with \(K^{D}\) (motion-aware spatio-temporally-variant degradation kernels) 에 대해<br/> \(Q=W_{q} k^{D, i}\)<br/> DA attention은 \(Net^{D}\) 말고 \(Net^{R}\) 에서만 사용</p> </li> </ul> <p><strong>FGDF</strong></p> <ul> <li> <p>spatio-temporal Dynamic Filter :<br/> \(y(p) = \sum_{t=-N}^{N} \sum_{k=1}^{n^2} F_{c+t}^{p}(p_k) x_{c+t}(p+p_k)\)<br/> where<br/> \(c\) : center frame index<br/> \(p_k \in \{ (- \lfloor \frac{n}{2} \rfloor, - \lfloor \frac{n}{2} \rfloor), \cdots , (\lfloor \frac{n}{2} \rfloor, \lfloor \frac{n}{2} \rfloor) \}\) : sampling offset for conv. with \(n \times n\) kernel<br/> \(F \in R^{T \times H \times W \times n^{2}}\) : predicted \(n \times n\) dynamic filter<br/> \(F^p \in R^{T \times n^{2}}\) : predicted \(n \times n\) dynamic filter at position p</p> </li> <li> <p>limit :<br/> fixed position (\(p\)) and fixed surrounding neighbors (\(p_k\))<br/> \(\rightarrow\) To capture large motion, require large-sized filter</p> </li> </ul> <blockquote> <p>solution : <code class="language-plaintext highlighter-rouge">FGDF</code><br/> kernels - dynamically generated / pixel-wise (position-wise) / variable surroundings guided by optical flow<br/> \(\rightarrow\) can handle large motion with relatively small-sized filter<br/> \(y(p) = \sum_{t=-N}^{N} \sum_{k=1}^{n^2} F_{c+t}^{p}(p_k) x_{c+t}^{\ast}(p+p_k)\)<br/> where <br/> \(x_{c+t}^{\ast} = W(x_{c+t}, \boldsymbol f_{c+t})\) : <code class="language-plaintext highlighter-rouge">warped input feature</code> based on \(\boldsymbol f_{c+t}\)<br/> \(\boldsymbol f_{c+t}\) : <code class="language-plaintext highlighter-rouge">flow-mask pair</code> from frame index \(c\) to \(c+t\)</p> </blockquote> <p><strong>Overall Architecture</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/1-480.webp 480w,/assets/img/2024-05-02-FMANet/1-800.webp 800w,/assets/img/2024-05-02-FMANet/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Degradation Network \(Net^{D}\)<br/> input : blurry LR sequence \(\boldsymbol X\) and sharp HR sequence \(\boldsymbol Y\)<br/> goal : <code class="language-plaintext highlighter-rouge">predict flow and degradation kernels in sharp HR sequence</code> \(\boldsymbol Y\)</p> <ol> <li>an image flow-mask pair \(\boldsymbol f^{Y}\)</li> <li>motion-aware spatio-temporally-variant degradation kernels \(K^{D}\)<br/> \(\rightarrow\) obtain blurry LR center frame \(\boldsymbol X_{c}\) from sharp HR counterpart \(\boldsymbol Y\)</li> </ol> </blockquote> <ul> <li> <p>step 1-1. initialize<br/> RRDB : <d-cite key="rrdb">[15]</d-cite><br/> \(\boldsymbol X \rightarrow\) 3D RRDB \(\rightarrow F^{0}\)</p> </li> <li> <p>step 1-2. initialize<br/> \(F_{w}^{0} = 0\), \(\boldsymbol f = \left \lbrace f_{c \rightarrow c+t}^{j} = 0, o_{c \rightarrow c+t}^{j} = 1 \right \rbrace _{j=1:n}^{t=-N:N}\)</p> </li> <li> <p>step 2. M FRMA blocks<br/> \(F^{0}, F_{w}^{0}, \boldsymbol f^{0} \rightarrow\) \(M\) FRMA blocks \(\rightarrow F^{M}, F_{w}^{M}, \boldsymbol f^{M}\)</p> </li> <li> <p>step 3-1. <code class="language-plaintext highlighter-rouge">an</code> image flow-mask pair \(\boldsymbol f^{Y} \in R^{T \times H \times W \times (2+1) 1}\)<br/> \(\boldsymbol f^{M} \rightarrow\) Conv3d \(\rightarrow \boldsymbol f^{Y}\)</p> </li> <li> <p>step 3-2. \(\hat X_{sharp}^{D}\) only used in Temporal Anchor (TA) loss<br/> \(F^{M} \rightarrow\) Conv3d \(\rightarrow \hat X_{sharp}^{D} \in R^{T \times H \times W \times 3}\) in image domain</p> </li> <li> <p>step 3-3. motion-aware spatio-temporally-variant degradation kernels \(K^{D} \in R^{T \times H \times W \times k_{d}^{2}}\)<br/> \(K^{D}\) = softmax(Conv3d(\(r_{3 \rightarrow 4}\)(\(F_{w}^{M}\))))<br/> where<br/> \(k_{d}\) : degradation kernel size<br/> sigmoid for normalization : all kernels have <code class="language-plaintext highlighter-rouge">positive</code> values, which mimics <code class="language-plaintext highlighter-rouge">blur generation process</code></p> </li> <li> <p>step 4. FGDF downsampling to predict blurry center frame \(\hat X_{c}\)<br/> \(\hat X_{c}\) = \(W(\boldsymbol Y, s (\boldsymbol f^{Y} \uparrow _{s}))\) \(\circledast K^{D} \downarrow _{s}\)<br/> where<br/> \(\uparrow\) : \(\times s\) bilinear upsampling<br/> \(W(\boldsymbol Y, s (\boldsymbol f^{Y} \uparrow _{s}))\) : warped sharp HR sequence based on an upsampled image flow-mask pair<br/> \(\circledast K^{D} \downarrow _{s}\) : FGDF with filter \(K^{D}\) with stride \(s\)</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/17-480.webp 480w,/assets/img/2024-05-02-FMANet/17-800.webp 800w,/assets/img/2024-05-02-FMANet/17-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/17.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/1-480.webp 480w,/assets/img/2024-05-02-FMANet/1-800.webp 800w,/assets/img/2024-05-02-FMANet/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Restoration Network \(Net^{R}\)<br/> input : blurry LR sequence \(\boldsymbol X\) and \(F^{M}, \boldsymbol f^{M}, K^{D}\) from \(Net^{D}\)<br/> goal : <code class="language-plaintext highlighter-rouge">predict flow and restoration kernels in blurry LR sequence</code> \(\boldsymbol X\)</p> <ol> <li>an image flow-mask pair \(\boldsymbol f^{X}\)</li> <li>restoration kernels \(K^{R}\)<br/> \(\rightarrow\) obtain sharp HR center frame \(\hat Y_{c}\) from blurry LR counterpart \(\boldsymbol X\)</li> </ol> </blockquote> <ul> <li> <p>step 1-1. initialize \(F^{0}\)<br/> RRDB : <d-cite key="rrdb">[15]</d-cite><br/> concat(\(\boldsymbol X\), \(F^{M}\) from \(Net^{D}\)) \(\rightarrow\) 3D RRDB \(\rightarrow\) \(F^{0}\)</p> </li> <li> <p>step 1-2. initialize \(F_{w}^{0}\), \(\boldsymbol f^{0}\)<br/> \(F_{w}^{0} = 0\), \(\boldsymbol f^{0} = \boldsymbol f^{M}\) from \(Net^{D}\)</p> </li> <li> <p>step 2-1. compute \(k^{D, i} \in R^{H \times W \times C}\) for DA attention</p> </li> <li> <p>step 2-2. M FRMA blocks<br/> \(F^{0}, F_{w}^{0}, \boldsymbol f^{0}, k^{D, i} \rightarrow\) \(M\) FRMA blocks \(\rightarrow F^{M}, F_{w}^{M}, \boldsymbol f^{M}\)</p> </li> <li> <p>step 3-1. <code class="language-plaintext highlighter-rouge">an</code> image flow-mask pair \(\boldsymbol f^{X} \in R^{T \times H \times W \times (2+1) 1}\)<br/> \(\boldsymbol f^{M} \rightarrow\) Conv3d \(\rightarrow \boldsymbol f^{X}\)</p> </li> <li> <p>step 3-2. \(\hat X_{sharp}^{R}\) only used in Temporal Anchor (TA) loss<br/> \(F^{M} \rightarrow\) Conv3d \(\rightarrow \hat X_{sharp}^{R} \in R^{T \times H \times W \times 3}\) in image domain</p> </li> <li> <p>step 3-3. motion-aware spatio-temporally-variant \(\times s\) upsampling and restoration kernels \(K^{R} \in R^{T \times H \times W \times s^{2} k_{r}^{2}}\)<br/> \(K^{R}\) = Normalize(Conv3d(\(r_{3 \rightarrow 4}\)(\(F_{w}^{M}\))))<br/> where<br/> \(k_{r}\) : restoration kernel size<br/> Normalize : w.r.t all kernels at temporally co-located positions over \(X\) (\(T\) dim.에 대해 normalize)</p> </li> <li> <p>step 3-4. high-frequency detail \(\hat Y_{r}\)<br/> \(F_{w}^{M} \rightarrow\) stacked conv. and pixel shuffle \(\rightarrow \hat Y_{r}\)</p> </li> <li> <p>step 4. FGDF upsampling to predict sharp center frame \(\hat Y_{c}\)<br/> \(\hat Y_{c}\) = \(\hat Y_{r}\) + \(W(\boldsymbol X, \boldsymbol f^{X})\) \(\circledast K^{D} \uparrow _{s}\)<br/> where<br/> \(W(\boldsymbol X, \boldsymbol f^{X})\) : warped blurry LR sequence based on an image flow-mask pair<br/> \(\circledast K^{D} \uparrow _{s}\) : \(\times s\) dynamic upsampling with kernel \(K^{R}\)</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/17-480.webp 480w,/assets/img/2024-05-02-FMANet/17-800.webp 800w,/assets/img/2024-05-02-FMANet/17-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/17.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Training</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/4-480.webp 480w,/assets/img/2024-05-02-FMANet/4-800.webp 800w,/assets/img/2024-05-02-FMANet/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Stage 1. Pre-train \(Net^{D}\)</p> <ul> <li>loss 1. <code class="language-plaintext highlighter-rouge">reconstruction loss</code> for blurry LR \(X_{c}\)<br/> \(\hat X_{c}\) \(\leftrightarrow\) \(X_{c}\)</li> <li>loss 2. <code class="language-plaintext highlighter-rouge">optical flow warping loss</code> (warping from c to c+t) in \(\boldsymbol Y\)<br/> \(W(Y_{t+c}, s (\boldsymbol f_{t+c}^{Y} \uparrow _{s}))\) \(\leftrightarrow\) \(Y_{c}\)</li> <li>loss 3. <code class="language-plaintext highlighter-rouge">optical flow refining loss</code> in \(\boldsymbol Y\)<br/> \(f^{Y}\) \(\leftrightarrow\) \(f_{RAFT}^{Y}\)<br/> where<br/> \(f^{Y}\) is image optical flow (no occlusion mask) contained in \(\boldsymbol f^{Y}\)<br/> \(f_{RAFT}^{Y}\) is pseudo-GT optical flow by pre-trained RAFT model <d-cite key="Raft">[16]</d-cite></li> <li>loss 4. <code class="language-plaintext highlighter-rouge">Temporal Anchor (TA) loss</code> for sharp LR \(X_{sharp}\)<br/> <code class="language-plaintext highlighter-rouge">It anchors and sharpens each feature w.r.t corresponding frame index</code><br/> \(\hat X_{sharp}^{D}\) \(\leftrightarrow\) \(X_{sharp}\)<br/> where<br/> sharp HR sequence \(\boldsymbol Y \rightarrow\) bicubic downsampling \(\rightarrow\) GT sharp LR sequence \(X_{sharp}\)<br/> \(\rightarrow\) keep each feature temporally anchored for the corresponding frame index<br/> \(\rightarrow\) constrain the solution space to distinguish warped and unwarped features<br/> <code class="language-plaintext highlighter-rouge">???</code></li> </ul> <blockquote> <p>RAFT: Recurrent all-pairs field transforms for optical flow <d-cite key="Raft">[16]</d-cite> :<br/> 핵심 아이디어 : TBD</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/5-480.webp 480w,/assets/img/2024-05-02-FMANet/5-800.webp 800w,/assets/img/2024-05-02-FMANet/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Stage 2. Jointly train \(Net^{D}\) and \(Net^{R}\)</p> <ul> <li>loss 1. <code class="language-plaintext highlighter-rouge">restoration loss</code> for sharp HR \(Y_{c}\)<br/> \(\hat Y_{c}\) \(\leftrightarrow\) \(Y_{c}\)</li> <li>loss 2. <code class="language-plaintext highlighter-rouge">optical flow warping loss</code> (warping from c to c+t) in \(\boldsymbol X\)<br/> Stage 1.의 loss 2.와 동일한 원리</li> <li>loss 3. <code class="language-plaintext highlighter-rouge">Temporal Anchor (TA) loss</code> for sharp LR \(X_{sharp}\)<br/> Stage 1.의 loss 4.와 동일한 원리</li> <li>loss 4. \(L_{D}\)<br/> Stage 1.의 loss들<br/> <code class="language-plaintext highlighter-rouge">왜 X optical flow에 대해선 RAFT loss 안 했지??</code></li> </ul> <hr/> <h2 id="results">Results</h2> <p><strong>Settings</strong></p> <p>LR patch size : 64 \(\times\) 64<br/> the number of FRMA blocks : \(M\) = 4<br/> the number of multi-flow-mask pairs : \(n\) = 9<br/> degradation and restoration kernel size : \(k_{d}\), \(k_{r}\) = 20, 5<br/> the number of frames in sequence : \(T\) = 3 (\(N\) = 1)<br/> ratio b.w. HR and LR : \(s\) = 4<br/> multi-attention block : utilize multi-Dconv head transposed attention (MDTA) and Gated-Dconv feed-forward network (GDFN) from Restormer <d-cite key="restormer">[4]</d-cite></p> <blockquote> <p>multi-Dconv head transposed attention and Gated-Dconv feed-forward network <d-cite key="restormer">[4]</d-cite> :<br/> TBD</p> </blockquote> <p><strong>Datasets and Evaluation Metrics</strong></p> <ul> <li> <p>Datasets :<br/> REDS dataset : train and test<br/> GoPro and YouTube dataset : test (generalization)<br/> \(\rightarrow\) spatially bicubic downsampling to make LR sequence and temporally downsampling to make lower fps sequence</p> </li> <li> <p>Evaluation Metrics :<br/> PSNR and SSIM for image quality<br/> tOF for temporal consistency</p> </li> </ul> <p><strong>Comparision with SOTA</strong></p> <blockquote> <p>SOTA methods (SR) :<br/> single-image SR : SwinIR <d-cite key="swinir">[17]</d-cite> and HAT <d-cite key="hat">[18]</d-cite><br/> video SR : BasicVSR++ <d-cite key="vsr">[1]</d-cite> and FTVSR <d-cite key="ftvsr">[19]</d-cite></p> </blockquote> <blockquote> <p>SOTA methods (DB) :<br/> single-image deblurring : Restormer <d-cite key="restormer">[4]</d-cite> and FFTformer <d-cite key="fftformer">[20]</d-cite><br/> video deblurring : RVRT <d-cite key="rvrt">[6]</d-cite> and GShiftNet <d-cite key="gshiftnet">[21]</d-cite></p> </blockquote> <blockquote> <p>SOTA methods (VSRDB) :<br/> HOFFR <d-cite key="HOFFR">[7]</d-cite></p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/18-480.webp 480w,/assets/img/2024-05-02-FMANet/18-800.webp 800w,/assets/img/2024-05-02-FMANet/18-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/18.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>VSRDB methods have superior performance compared to sequential cascade of SR and DB<br/> \(\rightarrow\) SR and DB tasks are highly inter-correlated</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/7-480.webp 480w,/assets/img/2024-05-02-FMANet/7-800.webp 800w,/assets/img/2024-05-02-FMANet/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/8-480.webp 480w,/assets/img/2024-05-02-FMANet/8-800.webp 800w,/assets/img/2024-05-02-FMANet/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <p><strong>Ablation Study</strong></p> <ul> <li>FGDF<br/> FGDF is better than conventional dynamic filtering for all ranges of motion magnitudes<br/> conventional dynamic filtering is especially not good for large motion</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/13-480.webp 480w,/assets/img/2024-05-02-FMANet/13-800.webp 800w,/assets/img/2024-05-02-FMANet/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> PSNR/tOF according to the average optical flow magnitude b.w. two consecutive frames </div> <ul> <li>Design of FMA-Net <ol> <li>the number of multi-flow-mask pairs \(n\) \(\propto\) performance</li> <li>motion info. from multi-flow-mask pairs \(\boldsymbol f\) is better than motion info. from DCN (Deformable Conv.) due to self-induced sharper optical flows and occlusion masks</li> <li>RAFT loss and TA loss</li> <li>two-stage (\(Net^{D} \rightarrow\) both) training is better than end-to-end training</li> <li>multi-attention (CO + DA) is better than self-attention + SFT(spatial feature transform) <d-cite key="SFT">[22]</d-cite></li> </ol> </li> </ul> <blockquote> <p>SFT (spatial feature transform) <d-cite key="SFT">[22]</d-cite><br/> ddd</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/14-480.webp 480w,/assets/img/2024-05-02-FMANet/14-800.webp 800w,/assets/img/2024-05-02-FMANet/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="conclusion">Conclusion</h2> <p>VSRDB framework based on FGDF and FRMA</p> <ul> <li>FRMA :<br/> iteratively update features (e.g. self-induced optical flow)<br/> multi-attention (CO + DA attention)</li> <li>FGDF :<br/> predict flow-mask pair with flow-guided dynamic filters \(K^{D}\) and \(K^{R}\) that are aware of motion<br/> can handle large motion</li> <li>TA loss :<br/> temporally anchors and sharpens unwarped features</li> <li>2-stage training :<br/> because, during multi-attention of \(Net^{R}\), warped feature \(F_{w}\) is adjusted by predicted degradation \(K^{D}\) from \(Net^{D}\) in globally adaptive manner</li> </ul> <h2 id="limitation">Limitation</h2> <ul> <li>2-stage approach has longer training time than end-to-end approach</li> <li>In extreme contidions such as object rotation, it is hard to predict accurate optical flow<br/> \(\rightarrow\) learnable homography parameters or quaternion representations can be one option to handle rotational motions</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/19-480.webp 480w,/assets/img/2024-05-02-FMANet/19-800.webp 800w,/assets/img/2024-05-02-FMANet/19-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/19.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="super-resolution"/><category term="super-resolution"/><category term="deblur"/><category term="flow"/><category term="dynamic"/><category term="attention"/><summary type="html"><![CDATA[Flow-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring]]></summary></entry><entry><title type="html">NeRF</title><link href="https://semyeong-yu.github.io/blog/2024/NeRF/" rel="alternate" type="text/html" title="NeRF"/><published>2024-04-10T21:00:00+00:00</published><updated>2024-04-10T21:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/NeRF</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/NeRF/"><![CDATA[<h2 id="nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h2> <h4 id="ben-mildenhall-pratul-psrinivasan-matthew-tancik">Ben Mildenhall, Pratul P.Srinivasan, Matthew Tancik</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2003.08934">https://arxiv.org/abs/2003.08934</a><br/> project website :<br/> <a href="https://www.matthewtancik.com/nerf">https://www.matthewtancik.com/nerf</a><br/> pytorch code :<br/> <a href="https://github.com/yenchenlin/nerf-pytorch">https://github.com/yenchenlin/nerf-pytorch</a><br/> <a href="https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file">https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file</a><br/> tiny tensorflow code :<br/> <a href="https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb">https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb</a><br/> referenced blog :<br/> <a href="https://csm-kr.tistory.com/64">https://csm-kr.tistory.com/64</a><br/> <a href="https://yconquesty.github.io/blog/ml/nerf/nerf_rendering.html#the-rendering-formula">https://yconquesty.github.io/blog/ml/nerf/nerf_rendering.html#the-rendering-formula</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>여러 각도의 camera center에서 each input image pixel 방향으로 ray(r=o+td)를 쏜다.</li> <li>ray를 discrete points로 sampling한다.</li> <li>3D coordinate x와 viewing direction d를 r(x)와 r(d)로 positional encoding한다.</li> <li>r(x)를 MLP에 넣어 volume density를 얻고 여기에 r(d)까지 넣어 RGB color를 얻는다.</li> <li>coarse network와 fine network(hierarchical sampling) 각각에서 volume density와 color를 이용한 volume rendering으로 ray마다 rendering pixel color를 구한다.</li> </ol> </blockquote> <h2 id="introduction">Introduction</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/3-480.webp 480w,/assets/img/2024-04-10-NeRF/3-800.webp 800w,/assets/img/2024-04-10-NeRF/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="pipeline">Pipeline</h4> <ul> <li> <p>(a) march camera rays and generate sampling of 5D coordinates</p> </li> <li> <p>(b) represents volumetric static scene by optimizing continuous 5D function(fully-connected network)</p> </li> </ul> <ol> <li>input: single continuous <code class="language-plaintext highlighter-rouge">5D coordinate</code><br/> 3D location \(x, y, z\)<br/> 2D direction \(\theta, \phi\)</li> <li>output:<br/> <code class="language-plaintext highlighter-rouge">volume density</code> (differential opacity) (how much radiance is accumulated by a ray)<br/> <code class="language-plaintext highlighter-rouge">view-dependent RGB color</code> (emitted radiance) \(c = (r, g, b)\)</li> </ol> <ul> <li> <p>(c) synthesizes novel view by classic <code class="language-plaintext highlighter-rouge">volume rendering</code> techniques(differentiable) to accumulate(project)(composite) the color/density samples into 2D image along rays</p> </li> <li> <p>(d) loss between synthesized and GT observed images</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/2-480.webp 480w,/assets/img/2024-04-10-NeRF/2-800.webp 800w,/assets/img/2024-04-10-NeRF/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Pipeline of NeRF architecture </div> <h4 id="problem--solution">Problem &amp; Solution</h4> <p>Problem :</p> <ol> <li>not sufficiently high-resolution representation</li> <li>inefficient in the number of samples per camera ray</li> </ol> <p>Solution :</p> <ol> <li>input <code class="language-plaintext highlighter-rouge">positional encoding</code> for MLP to represent higher frequency function</li> <li><code class="language-plaintext highlighter-rouge">hierarchical sampling</code> to reduce the number of queries</li> </ol> <h4 id="contribution">Contribution</h4> <ul> <li>represent continuous scenes as 5D neural radiance fields with basic MLP to render high-resolution novel views</li> <li>differentiable volume rendering + hierarchical sampling</li> <li>positional encoding to map input 5D coordinate into higher dim. space for high-frequency scene representation</li> <li>overcome the storage costs of discretized voxel grids by encoding continuous volume into network’s parameters<br/> =&gt; require only storage costs of sampled volumetric representations</li> </ul> <h2 id="related-work">Related Work</h2> <h4 id="neural-3d-shape-representation">Neural 3D shape representation</h4> <ul> <li> <p>deep networks that map \(xyz\) coordinates to signed distance functions or occupancy fields<br/> =&gt; limit : need GT 3D geometry</p> </li> <li> <p>Niemeyer et al.<br/> =&gt; input : find directly surface intersection for each ray (can calculate exact derivatie)<br/> =&gt; output : diffuse color at each ray intersection location</p> </li> <li> <p>Sitzmann et al.<br/> =&gt; input : each 3D coordinate<br/> =&gt; output : feature vector and RGB color at each 3D coordinate<br/> =&gt; rendering by RNN that marches along each ray to decide where the surface is.</p> </li> </ul> <blockquote> <p>Limit : oversmoothed renderings, so limited to simple shapes with low geometric complexity</p> </blockquote> <h4 id="view-synthesis-and-image-based-rendering">View synthesis and image-based rendering</h4> <ul> <li> <p>Given <code class="language-plaintext highlighter-rouge">dense sampling of views</code>, novel view synthesis is possible by <code class="language-plaintext highlighter-rouge">simple light field sample interpolation</code></p> </li> <li> <p>Given <code class="language-plaintext highlighter-rouge">sparser sampling of views</code>, there are 2 ways :<br/> mesh-based representation and volumetric representation</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Mesh-based</code> representation with either diffuse(난반사) or view-dependent appearance :<br/> Directly optimize mesh representations by differentiable rasterizers or pathtracers so that we reproject and reconstruct images</p> </li> </ul> <blockquote> <p>Limit :<br/> gradient-based optimization is often difficult because of <code class="language-plaintext highlighter-rouge">local minima or discontinuities or poor loss landscape</code><br/> mesh 구조를 유지하면서 <code class="language-plaintext highlighter-rouge">gradient-based optimization하는 게 어렵</code><br/> needs a <code class="language-plaintext highlighter-rouge">template mesh</code> with fixed topology for initialization, which is unavailable in real-world</p> </blockquote> <ul> <li><code class="language-plaintext highlighter-rouge">Volumetric</code> representation :<br/> well-suited for gradient-based optimization and less distracting artifacts<br/> train : predict a sampled volumetric representation (voxel grids) from input images<br/> test : use alpha-(or learned-)compositing along rays to render novel views<br/> (alpha-compositing : 아래 volume rendering section에서 설명 예정)<br/> CNN compensates discretization artifacts from low resolution voxel grids or CNN allows voxel grids to vary on input time</li> </ul> <blockquote> <p>Limit :<br/> good results, but limited by poor time, space complexity due to discrete sampling<br/> \(\rightarrow\) discrete sampling : rendering high resol. image =&gt; finer sampling of 3D space</p> </blockquote> <blockquote> <p>Author’s solution :<br/> encode <code class="language-plaintext highlighter-rouge">continuous</code> volume into network’s parameters<br/> =&gt; higher quality rendering + require only storage cost of those <code class="language-plaintext highlighter-rouge">sampled</code> volumetric representations</p> </blockquote> <h2 id="neural-radiance-field-scene-representation">Neural Radiance Field Scene Representation</h2> <p>represent continuous scene by 5D MLP : (x, d) =&gt; (c, \(\sigma\))</p> <p>Here, there are 2 key-points!</p> <blockquote> <p>multiview consistent :<br/> c is dependent on both x and d, but \(\sigma\) is only dependent on location x<br/> 3D coordinate x =&gt; 8 fc-layers =&gt; volume-density and 256-dim. feature vector</p> </blockquote> <blockquote> <p>Lambertian reflection : diffuse(난반사) vs Specular reflection : 전반사</p> </blockquote> <blockquote> <p>non-Lambertian effects : view-dependent color change to represent specular reflection<br/> feature vector is concatenated with direction d =&gt; 1 fc-layer =&gt; view-dependent RGB color</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/6-480.webp 480w,/assets/img/2024-04-10-NeRF/6-800.webp 800w,/assets/img/2024-04-10-NeRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="volume-rendering-with-radiance-fields">Volume Rendering with Radiance Fields</h2> <h4 id="ray-from-input-image-pre-processing">Ray from input image (pre-processing)</h4> <p>We use <code class="language-plaintext highlighter-rouge">Ray</code> to synthesize <code class="language-plaintext highlighter-rouge">continuous</code>-viewpoint images from <code class="language-plaintext highlighter-rouge">discrete</code> input images</p> <blockquote> <p>\(r(t) = o + td\)<br/> o : camera’s center of projection<br/> d : viewing direction<br/> t \(\in [ t_n , t_f ]\) : distance from camera center b.w. camera’s predefined near and far planes</p> </blockquote> <blockquote> <p>How to calculate viewing direction d??</p> <ul> <li>2D pixel coordinate : \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)</li> <li>2D normalized coordinate (\(z = 1\)) by intrinsic matrix :<br/> \(\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\) = \(K^{-1}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\) = \(\begin{bmatrix} 1/f_x &amp; 0 &amp; W/2 \\ 0 &amp; 1/f_y &amp; H/2 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)<br/> Since \(y, z\) have opposite direction between the real-world coordinate and pixel coordinate, we multiply (-1)<br/> \(\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\) = \(\begin{bmatrix} 1/f_x &amp; 0 &amp; W/2 \\ 0 &amp; -1/f_y &amp; H/2 \\ 0 &amp; 0 &amp; -1 \end{bmatrix}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)<br/> Here, focal length in intrinsic matrix K is usually calculated using camear angle \(\alpha\) as \(\tan{\alpha / 2} = \frac{h/2}{f}\)</li> <li>3D coordinate by extrinsic matrix :<br/> For extrinsic matrix \([R \vert t']\),<br/> \(o = t'\)<br/> \(d = R * \begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\)<br/> Therefore, we can obtain \(r(t) = o + td\)</li> </ul> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/9-480.webp 480w,/assets/img/2024-04-10-NeRF/9-800.webp 800w,/assets/img/2024-04-10-NeRF/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="volume-rendering-from-mlp-output">Volume Rendering from MLP output</h4> <p>We use differential classical volume rendering</p> <blockquote> <p>idea : 빛이 들어올 확률이 클수록, 물체의 밀도가 높을수록 2D 이미지 픽셀에서 색상 채널의 값이 크게 나타나므로<br/> pixel’s color along a ray is equal to the integral of (transmittance) x (volume density) x (color) for all points along one ray.</p> </blockquote> <p><code class="language-plaintext highlighter-rouge">transmittance가 클수록 투명해서 no color에 가까우니까 색상 채널의 값이 작게 나타나야 하는 것 아니야?????</code></p> <blockquote> <p>Let ray \(r\) (traced through desired virtual camera) have near and far bounds \(t_n, t_f\)<br/> expected color of ray \(r\) = \(C(r) = \int_{t_n}^{t_f} T(t) \sigma (r(t)) c(r(t), d) dt\)</p> </blockquote> <ul> <li>\(T(t) = \exp(- \int_{t_n}^{t} \sigma (r(s)) ds)\) :<br/> <code class="language-plaintext highlighter-rouge">transmittance</code><br/> transmittance = 투과도 = <code class="language-plaintext highlighter-rouge">CDF</code> that a ray <code class="language-plaintext highlighter-rouge">does not hit</code> any particles from \(z_0\) to \(z\)<br/> 투과도가 클수록 투명 (no color)</li> <li>\(\sigma (r(t))\) : <code class="language-plaintext highlighter-rouge">volume density</code> along the ray (learned by MLP)<br/> volume density = <code class="language-plaintext highlighter-rouge">opacity</code> = 불투명도 = <code class="language-plaintext highlighter-rouge">extinction coefficient</code> = <code class="language-plaintext highlighter-rouge">alpha value</code> for alpha-compositing</li> <li>\(c(r(t), d)\) : object’s <code class="language-plaintext highlighter-rouge">color</code> along the ray (learned by MLP)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/10-480.webp 480w,/assets/img/2024-04-10-NeRF/10-800.webp 800w,/assets/img/2024-04-10-NeRF/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/11-480.webp 480w,/assets/img/2024-04-10-NeRF/11-800.webp 800w,/assets/img/2024-04-10-NeRF/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>volume rendering 식 유도 과정</p> </blockquote> <p>occluding objects are modeled as spherical particles with radius \(r\)<br/> There are \(A \cdot \Delta z \cdot \rho (z)\)개의 particles in the slice where \(\rho (z)\) is particle density (the number of particles per unit volume)</p> <p>Since solid particles do not overlap for \(\Delta z \rightarrow 0\),<br/> \(A \cdot \Delta z \cdot \rho (z) \cdot \pi r^2\)만큼 area is occluded<br/> 즉, cross section \(A\)에서 \(\frac{A \cdot \Delta z \cdot \rho (z) \cdot \pi r^2}{A} = \pi r^2 \cdot \rho (z) \cdot \Delta z\)의 비율만큼 occluded</p> <p>If \(\frac{A \cdot \Delta z \cdot \rho (z) \cdot \pi r^2}{A}\)만큼 rays are occluded, the light intensity decreases as<br/> \(I(z + \Delta z) = (1 - \pi r^2 \rho (z) \Delta z) \times I(z)\)</p> <p>Then the light density difference \(\Delta I = I(z + \Delta z) - I(z) = - \pi r^2 \rho (z) \Delta z \cdot I(z)\)<br/> 즉, \(dI(z) = - \pi r^2 \rho (z) I(z) dz = - \sigma (z) I(z) dz\)<br/> where <code class="language-plaintext highlighter-rouge">volume density (or opacity)</code> is \(\sigma(z) = \pi r^2 \rho (z)\)<br/> It makes sense because particle area와 particle density(particle 수)가 클수록 ray 감소량 (volume density)이 커지기 때문<br/> ODE 풀면, \(I(z) = I(z_0)\exp(- \int_{z_0}^{z} \sigma (r(s)) ds)\)</p> <p>Let’s define transmittance \(T(z) = \exp(- \int_{z_0}^{z} \sigma (r(s)) ds)\)<br/> where \(I(z) = I(z_0)T(z)\) means the <code class="language-plaintext highlighter-rouge">remainning</code> intensity after rays travel from \(z_0\) to \(z\)<br/> where <code class="language-plaintext highlighter-rouge">transmittance</code> \(T(z)\) means <code class="language-plaintext highlighter-rouge">CDF</code> that a ray <code class="language-plaintext highlighter-rouge">does not hit</code> any particles from \(z_0\) to \(z\)</p> <p>If a ray passes empty space, there is no color<br/> If a ray hits particles, there exists color (<code class="language-plaintext highlighter-rouge">radiance is emitted</code>)<br/> Let’s define \(H(z) = 1 - T(z)\), which means CDF that a ray <code class="language-plaintext highlighter-rouge">hits</code> particles from \(z_0\) to \(z\)<br/> CDF를 미분하면 PDF이므로<br/> Then PDF is \(p_{hit}(z) = \frac{dH}{dz} = - \frac{dT}{dz} = \exp(- \int_{z_0}^{z} \sigma (r(s)) ds) \sigma (z) = T(z) \sigma (z)\)</p> <p>Let a random variable \(R\) be the emitted randiance.<br/> Then PDF \(p_R(ray) = P[R = c(z)] = p_{hit}(z) = T(z) \sigma (z)\)<br/> Then the <code class="language-plaintext highlighter-rouge">color of a pixel is expected radiance for ray</code> bounded from \(t_n\) to \(t_f\)<br/> \(C(ray) = E[R] = \int_{t_n}^{t_f} R \cdot p_R dz = \int_{t_n}^{t_f} c \cdot p_{hit} dz = \int_{t_n}^{t_f} T(z) \sigma (z) c(z) dz\)</p> <p>\(t_n, t_f = 0., 1.\) for scaled-bounded and front-facing scenes after conversion to <code class="language-plaintext highlighter-rouge">NDC (normalized device coordinates)</code><br/> <a href="https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#analysis">How NDC Works?</a></p> <blockquote> <p>To apply the equation to our model by numerical quadrature,<br/> we have to sample discrete points from continuous ray</p> </blockquote> <p>Instead of deterministic quadrature(typically used for rendering voxel grids, but may limit resolution),<br/> author divides a ray \(\left[t_n, t_f\right]\) into N = 64 bins(intervals), and chooses one point \(t_i\) for each bin by uniform sampling<br/> \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N}(t_f - t_n), t_n + \frac{i}{N}(t_f - t_n)\right]\)</p> <p>Although we use discrete N samples, <code class="language-plaintext highlighter-rouge">stratified sampling(층화 표집)</code> enables MLP to be evaluated at continuous positions by optimization</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/5-480.webp 480w,/assets/img/2024-04-10-NeRF/5-800.webp 800w,/assets/img/2024-04-10-NeRF/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Discretized version for N samples by Numerical Quadrature :<br/> expected color \(\hat{C}(r) = \sum_{i=1}^{N} T_i (1 - \exp(- \sigma_{i} \delta_{i})) c_i\)</p> </blockquote> <ul> <li>\(\hat{C}(r)\) : differentiable about (\(\sigma_{i}, c_i\))</li> <li>\(\delta_{j} = t_{j+1} - t_j\) : the distance between adjacent samples</li> <li>\(T(t) = \exp(- \int_{t_n}^{t} \sigma (r(s)) ds) ~~ \rightarrow ~~ T_i = \exp(- \sum_{j=1}^{i-1} \sigma_{j} \delta_{j})\) where T_1 = 1</li> <li>\(\sigma (r(t)) dt ~~ \rightarrow ~~ \sigma_{j} \delta_{j} = 1 - \exp(-\sigma_{j} \delta_{j})\) by Taylor Series Expansion</li> <li> \[c(r(t), d) ~~ \rightarrow ~~ c_i\] </li> </ul> <p>또는</p> <p>\(p_{hit}(z_i) = \frac{dH}{dz} |_{z_i} ~~ \rightarrow ~~ H(z_{i+1}) - H(z_i) = (1 - T(z_{i+1})) - (1 - T(z_i)) = T(z_i) - T(z_{i+1}) = e^{- \sum_{j=1}^{i-1} \sigma_{j} \delta_{j}} - e^{- \sum_{j=1}^{i} \sigma_{j} \delta_{j}} = T(z_i)(1 - e^{- \sigma_{i} \delta_{i}})\)<br/> Then the <code class="language-plaintext highlighter-rouge">color of a pixel is expected radiance for ray</code> bounded from \(t_n\) to \(t_f\)<br/> \(\hat{C}(ray) = E[R] = \int_{t_n}^{t_f} R \cdot p_R dz ~~ \rightarrow ~~ \sum_{i=1}^{N} c_i \cdot p_{hit}(z_i) dz = \sum_{i=1}^{N} c_i T_i (1 - \exp(- \sigma_{i} \delta_{i}))\)</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">Final version</code> :<br/> expected color \(\hat{C}(r) = \sum_{i=1}^{N} T_i \alpha_{i} c_i\)<br/> where \(T_i = \prod_{j=1}^{i-1} (1-\alpha_{j})\) and \(\alpha_{i} = 1 - \exp(-\sigma_{i} \delta_{i})\)<br/> which reduces to traditional <code class="language-plaintext highlighter-rouge">alpha-compositing</code> problem</p> </blockquote> <p>이 때, this volume rendering 식은 <code class="language-plaintext highlighter-rouge">differentiable</code>하므로 end-to-end learning 가능!!<br/> a sequence of samples \(\boldsymbol t = {t_1, t_2, \ldots, t_N}\)에 대해<br/> \(\frac{d\hat{C}}{dc_i} |_{\boldsymbol t} = T_i \alpha_{i}\) \(\frac{d\hat{C}}{d \sigma_{i}} |_{\boldsymbol t} = c_i \times (\frac{dT_i}{d \sigma_{i}} \alpha_{i} + \frac{d \alpha_{i}}{d \sigma_{i}} T_i) = c_i \times (0 + \delta_{i}e^{-\sigma_{i}\delta_{i}} T_i) = \delta_{i} T_i c_i e^{- \sigma_{i} \delta_{i}}\)</p> <blockquote> <p>alpha-compositing :<br/> 여러 frame을 합쳐서 하나의 image로 합성하는 과정으로, 각 frame pixel마다 alpha 값(불투명도 값)(0~1)이 있어 겹치는 부분의 pixel 값을 결정</p> </blockquote> <p>By divide-and-conquer approach (tail recursion),<br/> \(c = \alpha_{1}c_{1} + (1 - \alpha_{1})(\alpha_{2}c_{2} + (1 - \alpha_{2})(\cdots)) = \alpha_{1}c_{1} + (1 - \alpha_{1})\alpha_{2}c_{2} + (1 - \alpha_{1})(1 - \alpha_{2})(\cdots) = \cdots = \sum_{i=1}^{N}(\alpha_{i}c_{i}\prod_{j=1}^{i-1}(1-\alpha_{j}))\) where \(\alpha_{0} = 0\)</p> <p>If \(\alpha_{i} = 1 - \exp(-\sigma_{i} \delta_{i})\),<br/> NeRF volume rendering 식 \(\hat{C}(r) = \sum_{i=1}^{N} T_i \alpha_{i} c_i\)과<br/> alpha-compositing 식 \(c = \sum_{i=1}^{N}(\alpha_{i}c_{i}\prod_{j=1}^{i-1}(1-\alpha_{j}))\)은<br/> SAME!!</p> <h2 id="optimizing-a-neural-radiance-field">Optimizing a Neural Radiance Field</h2> <h4 id="positional-encoding-pre-processing">Positional encoding (pre-processing)</h4> <p>If we use input directly, MLP is biased to learn low-frequency function (oversmoothed appearance) (no detail)<br/> If we map input into <code class="language-plaintext highlighter-rouge">higher dim.</code> space which contains info. from low frequency to high frequency, MLP can fit data with <code class="language-plaintext highlighter-rouge">high-frequency variation</code><br/> Due to positional encoding, MLP can behave as <code class="language-plaintext highlighter-rouge">interpolation function</code> where \(L\) determines the bandwidth of the interpolation kernel <d-cite key="interpolation">[1]</d-cite><br/> \(r : R \rightarrow R^{2L}\) <br/> \(r(p) = (sin(2^0\pi p), cos(2^0\pi p), \cdots, sin(2^{L-1}\pi p), cos(2^{L-1}\pi p))\)<br/> \(L=10\) for \(r(x)\) where x has three coordinates<br/> \(L=4\) for \(r(d)\) where d has three components of the cartesian viewing direction unit vector</p> <h4 id="hierarchical-volume-sampling">Hierarchical volume sampling</h4> <p>Densely evaluating N points by stratified sampling is inefficient<br/> =&gt; We don’t need much sampling at free space or occluded regions<br/> =&gt; Hierarchical sampling enables us to allocate more samples to regions we expect to contain visible content</p> <p>We simultaneously optimize 2 networks with different sampling : <code class="language-plaintext highlighter-rouge">coarse</code> and <code class="language-plaintext highlighter-rouge">fine</code></p> <blockquote> <p>coarse sampling \(N_c\)개 : 위에서 배웠던 내용<br/> author divides a ray into \(N_c\) = 64 bins(intervals), and chooses one point \(t_i\) for each bin by uniform sampling<br/> \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N_c}(t_f - t_n), t_n + \frac{i}{N_c}(t_f - t_n)\right]\)</p> </blockquote> <blockquote> <p>fine sampling \(N_f\)개 : 새로운 내용<br/> coarse sampling model’s output is a <code class="language-plaintext highlighter-rouge">weighted sum of all coarse-sampled colors</code><br/> \(\hat{C}(r) = \sum_{i=1}^{N_c} T_i \alpha_{i} c_i = \sum_{i=1}^{N_c} w_i c_i\)<br/> where we define \(w_i = T_i \alpha_{i} = T_i (1 - \exp(-\sigma_{i} \delta_{i}))\) for \(i=1,\cdots,N_c\)<br/> =&gt; Given the output of <code class="language-plaintext highlighter-rouge">coarse</code> network, we try more informed (better) sampling where samples are biased toward the <code class="language-plaintext highlighter-rouge">relevant parts of the scene volume</code><br/> =&gt; We sample \(N_f\)=128 <code class="language-plaintext highlighter-rouge">fine</code> points following a <code class="language-plaintext highlighter-rouge">piecewise-constant PDF</code> of normalized \(\frac{w_i}{\sum_{j=1}^{N_c} w_j}\)<br/> =&gt; Here, we use <code class="language-plaintext highlighter-rouge">Inverse CDF Method</code> for sampling fine points</p> </blockquote> <blockquote> <p>Inverse transform sampling = Inverse CDF Method :<br/> =&gt; PDF (probability density function) : \(f_X(x)\)<br/> =&gt; CDF (cumulative distribution function) : \(F_X(x) = P(X \leq x) = \int_{-\infty}^x f_X(x) dx\)<br/> idea : 모든 확률 분포의 CDF는 Uniform distribution을 따른다!<br/> =&gt; 따라서 CDF의 y값을 Uniform sampling하면, 그 y에 대응되는 x에 대한 (특정 PDF를 따르는) sampling을 구현할 수 있다!<br/> =&gt; 즉, CDF의 역함수를 계산할 수 있다면, 기본 난수 생성기인 Uniform sampling을 이용해서 확률 분포 X에 대한 sampling을 할 수 있다!<br/> \(F_X(x)\) ~ \(U\left[0, 1\right]\)<br/> \(X\) ~ \(F^{-1}(U\left[0, 1\right])\)</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/4-480.webp 480w,/assets/img/2024-04-10-NeRF/4-800.webp 800w,/assets/img/2024-04-10-NeRF/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We evaluate <code class="language-plaintext highlighter-rouge">coarse</code> network using \(N_c\)=64 points per ray<br/> We evaluate <code class="language-plaintext highlighter-rouge">fine</code> network using \(N_c+N_f\)=64+128=192 points per ray where we sample 128 fine points following PDF of <code class="language-plaintext highlighter-rouge">coarse</code> sampled points<br/> In result, we use a total of 64+192=256 samples per ray to compute the final rendering color \(C(r)\)</p> <h4 id="implementation-details--loss">Implementation details &amp; Loss</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/6-480.webp 480w,/assets/img/2024-04-10-NeRF/6-800.webp 800w,/assets/img/2024-04-10-NeRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>Prepare RGB images, corresponding camera poses, intrinsic parameters and scene bounds (use COLMAP structure-from-motion package to estimate these parameters)</li> <li>From H x W input image, randomly sample a batch of 4096 pixels</li> <li>calculate continuous ray from each pixel \(r(t) = o + kd\)</li> <li>coarse sampling of \(N_c\)=64 points per each ray \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N_c}(t_f - t_n), t_n + \frac{i}{N_c}(t_f - t_n)\right]\)</li> <li>positional encoding \(r(x)\) and \(r(d)\) for input</li> <li>obtain volume density \(\sigma\) by MLP with \(r(x, y, z)\) as input</li> <li>obtain color \(c\) by MLP with \(r(x, y, z)\) and \(r(\theta, \phi)\) as input</li> <li>obtain rendering color of each ray by volume rendering \(\hat{C}(r) = \sum_{i=1}^{N} T_i (1 - \exp(- \sigma_{i} \delta_{i})) c_i\) from two networks ‘coarse’ and ‘fine’</li> <li>compute loss</li> <li>Adam optimizer with learning rate from \(5 \times 10^{-4}\) to \(5 \times 10^{-5}\)</li> <li>optimization for a single scene typically takes around 100-300k iterations (1~2 days) to converge on a single NVIDIA V100 GPU</li> </ol> <p>Here, we use L2 norm for loss<br/> \(L = \sum_{r \in R} \left[{\left\|\hat{C_c}(r)-C(r)\right\|}^2+{\left\|\hat{C_f}(r)-C(r)\right\|}^2\right]\)<br/> \(C(r)\) : GT pixel RGB color<br/> \(\hat{C_c}(r)\) : rendering RGB color from coarse network : to allocate better samples in fine network<br/> \(\hat{C_f}(r)\) : rendering RGB color from fine network : our goal<br/> \(R\) : the set of all pixels(rays) across all images</p> <h2 id="results">Results</h2> <h4 id="datasets">Datasets</h4> <p>Synthetic renderings of objects</p> <ul> <li>Diffuse Synthetic 360 : 4 Lambertian objects with simple geometry</li> <li>Realistic Synthetic 360 : 8 non-Lambertian objects with complicated geometry</li> </ul> <p>Real images of complex scenes</p> <ul> <li>Real Forward-Facing : 8 scenes captured with a handheld cellphone</li> </ul> <h4 id="measurement">Measurement</h4> <ul> <li>PSNR(Peak Signal-to-Noise Ratio) \(\uparrow\) : the ratio between the maximum possible power of a signal and the power of corrupting noise \(10\log_{10}\left(\frac{(MAX)^2}{MSE}\right)\)[dB]</li> <li>SSIM(Structural Similarity Index Map) \(\uparrow\) : compare image qualities in three ways: Lumincance(\(l\)), Contrast(\(c\)), Structural(\(s\))<br/> SSIM(x, y) = \([l(x,y)]^{\alpha}[c(x,y)]^{\beta}[s(x,y)]^{\gamma}=\frac{(2\mu_{x}\mu_{y}+C_1)(2\sigma_{xy}+C_2)}{(\mu_{x}^2+\mu_{y}^2+C_1)(\sigma_{x}^2+\sigma_{y}^2+C_2)}\) where \(l(x,y)=\frac{(2\mu_{x}\mu_{y}+C_1)}{\mu_{x}^2+\mu_{y}^2+C_1}\) and \(c(x,y)=\frac{(2\sigma_{x}\sigma_{y}+C_2)}{\sigma_{x}^2+\sigma_{y}^2+C_2}\) and \(s(x,y)=\frac{\sigma_{xy}+C_3}{\sigma_{x}\sigma_{y}+C_3}\)<br/> SSIM calculator :<br/> https://darosh.github.io/image-ssim-js/test/browser_test.html</li> <li>LPIPS \(\downarrow\)</li> </ul> <h4 id="comparisons">Comparisons</h4> <ul> <li>Neural Volumes (NV) :<br/> It synthsizes novel views of objects that lie entirely within a bounded volume in front of a distinct background.<br/> It optimizes 3D conv. network to predict a discretized RGB\(\alpha\) voxel grid and a 3D warp grid.<br/> It renders novel views by marching rays through the warped voxel grid</li> <li>Scene Representation Networks (SRN) :<br/> It represents continuous scene as an opaque surface.<br/> MLP maps each 3D coordinate to a feature vector, and we optimize RNN to predict the next step size along the ray using the feature vector.<br/> The feature vector from the final step is decoded into a color for that point on the surface. Note that SRN is followup to DeepVoxels by the same authors.</li> <li>Local Light Field Fusion (LLFF) :<br/> designed for producing novel views for well-sampled forward facing scenes<br/> trained 3D conv. network directly predicts a discretized frustum-sampled RGB\(\alpha\) grid (multiplane image), and then renders novel views by alpha-compositing and blending</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/7-480.webp 480w,/assets/img/2024-04-10-NeRF/7-800.webp 800w,/assets/img/2024-04-10-NeRF/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Test for scenes from author's new synthetic dataset </div> <p>LLFF exhibits banding and ghosting artifacts<br/> SRN produces blurry and distorted renderings<br/> NV cannot capture the details<br/> NeRF captures fine details in both geometry and appearance</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/8-480.webp 480w,/assets/img/2024-04-10-NeRF/8-800.webp 800w,/assets/img/2024-04-10-NeRF/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Test for read-world scenes </div> <p>LLFF may have repeated edges because of blending between multiple renderings<br/> NeRF also correctly reconstruct partially-occluded regions<br/> SRN does not capture any high-frequency fine detail</p> <h4 id="discussion">Discussion</h4> <h4 id="ablation-studies">Ablation Studies</h4> <h2 id="conclusion">Conclusion</h2> <p>prior : MLP outputs discretized voxel representations<br/> author : MLP outputs volume density and view-dependent emitted radiance</p> <h2 id="future-work">Future Work</h2> <p>efficiency :<br/> Rather than hierarchical sampling, there is still much more progress to be made for efficient optimization and rendering of neural radiance fields</p> <p>interpretability :<br/> voxel grids or meshes admits reasoning about the expected quality, but it is unclear to analyze these issues when we encode scenes into the weights of MLP</p>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="rendering"/><category term="3d"/><summary type="html"><![CDATA[representing scenes as neural radiance fields for view synthesis]]></summary></entry><entry><title type="html">SfMLearner</title><link href="https://semyeong-yu.github.io/blog/2024/SfMLearner/" rel="alternate" type="text/html" title="SfMLearner"/><published>2024-04-06T17:00:00+00:00</published><updated>2024-04-06T17:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/SfMLearner</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/SfMLearner/"><![CDATA[<h1 id="unsupervised-learning-of-depth-and-ego-motion-from-video">Unsupervised Learning of Depth and Ego-Motion from Video</h1> <h4 id="tinghui-zhou-matthew-brown-noah-snavely-david-g-lowe">Tinghui Zhou, Matthew Brown, Noah Snavely, David G. Lowe</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/1704.07813">https://arxiv.org/abs/1704.07813</a><br/> code :<br/> <a href="https://github.com/tinghuiz/SfMLearner">https://github.com/tinghuiz/SfMLearner</a></p> </blockquote> <h2 id="introduction">Introduction</h2> <ul> <li>SfM : Structure from Motion<br/> end-to-end unsupervised learning from monocular video (only one camera lens)</li> <li><code class="language-plaintext highlighter-rouge">single-view</code> depth estimation by per-pixel depth map</li> <li><code class="language-plaintext highlighter-rouge">multi-view</code> camera motion (= <code class="language-plaintext highlighter-rouge">ego-motion</code> = <code class="language-plaintext highlighter-rouge">pose</code>) by <code class="language-plaintext highlighter-rouge">6-DoF transformation matrices</code></li> <li><code class="language-plaintext highlighter-rouge">unsupervised</code> learning : 직접적인 GT data가 아니라 <code class="language-plaintext highlighter-rouge">view synthesis (reconstruction term)를 supervision</code>으로 씀</li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li>simultaneous estimation of structure and motion through deep learning</li> <li>end-to-end learning of transformation matrix without learning geometry explicitly</li> <li>learning of 3D single-view from registered 2D views</li> <li>unsupervised/self-supervised learning from video</li> </ul> <h2 id="method">Method</h2> <h4 id="approach">Approach</h4> <p>Assumption :<br/> Scenes, which we are interested in, are mostly rigid, so changes across different frames are dominated by camera motion</p> <h4 id="view-synthesis-as-supervision">View Synthesis as supervision</h4> <ul> <li>View Synthesis : as supervision of depth and pose (추후 설명 예정)</li> <li>loss function (reconstruction term) :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/1-480.webp 480w,/assets/img/2024-04-06-SfMLearner/1-800.webp 800w,/assets/img/2024-04-06-SfMLearner/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>\(p\) : index of target view’s pixel coordinates<br/> \(s\) : index of source views<br/> \(I_{t}(p)\) : target view<br/> \(\hat I_{s}(p)\) : source view warped to target coordinate frame (= reconstructed target view) using predicted depth \(\hat D_{t}\) and \(4 \times 4\) camera transformation matrix \(\hat T_{t \rightarrow s}\) and source view \(I_{s}\)</p> <ul> <li>pipeline for depth and pose estimation :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/2-480.webp 480w,/assets/img/2024-04-06-SfMLearner/2-800.webp 800w,/assets/img/2024-04-06-SfMLearner/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="differentiable-depth-image-based-rendering">Differentiable depth image-based rendering</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/3-480.webp 480w,/assets/img/2024-04-06-SfMLearner/3-800.webp 800w,/assets/img/2024-04-06-SfMLearner/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li> <p><code class="language-plaintext highlighter-rouge">Depth CNN</code>을 통해 <code class="language-plaintext highlighter-rouge">target view</code> (single view)로부터 <code class="language-plaintext highlighter-rouge">depth prediction</code> \(\hat D_{t}\) 얻기</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Pose CNN</code>을 통해 <code class="language-plaintext highlighter-rouge">target &amp; source view</code> (multi-view)로부터 \(4 \times 4\) <code class="language-plaintext highlighter-rouge">camera transformation matrix</code> \(\hat T_{t \rightarrow s}\) 얻기</p> </li> <li>target view의 pixels를 source view coordinate으로 <code class="language-plaintext highlighter-rouge">project</code>하기<br/> 값이 아니라 <code class="language-plaintext highlighter-rouge">대응되는 위치</code>를 구하기 위해<br/> projection할 때 depth와 pose 이용 <ul> <li>monocular camera이므로 두 카메라 사이의 상대적인 위치를 설명하는 \([R \vert t]\)는 고려 안함</li> <li>\(K^{-1}p_{t}\) : target view coordinate에서 2D 좌표 \(\rightarrow\) 3D 좌표</li> <li>\(\hat D_{t}(p_{t})K^{-1}p_{t}\) : target view의 3D depth map (= 2D depth \(\times\) 3D 좌표)<br/> full 3D volumetric은 아니고, surface만 나타내는 3D target</li> <li>\(\hat T_{t \rightarrow s} \hat D_{t}(p_{t})K^{-1}p_{t}\) : 3D depth map projected from target view to source view</li> <li>\(K \hat T_{t \rightarrow s} \hat D_{t}(p_{t})K^{-1}p_{t}\) : source view coordinate에서 3D 좌표 \(\rightarrow\) 2D 좌표<br/> <code class="language-plaintext highlighter-rouge">target view의 pixel 좌푯값을 source view의 좌푯값으로 project하는 데 중간에 depth map이 왜 필요한 거지???</code></li> </ul> </li> <li>source view coordinate에서 differentiable bilinear <code class="language-plaintext highlighter-rouge">interpolation</code>으로 value 얻은 뒤 <code class="language-plaintext highlighter-rouge">warp to target coordinate</code> (= <code class="language-plaintext highlighter-rouge">reconstructed target view</code>)<br/> source view의 pixel 값들을 이용해서 reconstruct target view</li> </ol> <h4 id="modeling-the-model-limitation">Modeling the model limitation</h4> <p>Assumption :</p> <ol> <li> <p>objects are static except camera (changes are dominated by camera motion)<br/> 물체들이 움직이지 않아야 Depth CNN과 Pose CNN이 같은 coordinate에 대해 project할 수 있다.</p> </li> <li> <p>there is no occlusion/disocclusion between target view and source view<br/> target view와 source views 중 하나라도 물체가 가려져서 안보인다면 projection 정보가 없어 학습에 문제가 된다.</p> </li> <li> <p>surface is Lambertain so that photo-consistency error is meaningful<br/> 어떤 방향에서 보든 표면이 isotropic 똑같은 밝기로 보인다고 가정 \(\rightarrow\) photo-consistency에 차이가 있을 경우 이는 다른 surface를 의미함</p> </li> </ol> <h4 id="overcoming-the-gradient-locality-at-loss-term">Overcoming the gradient locality at loss term</h4> <ol> <li> <p>To improve robustness, train additional network which predicts <code class="language-plaintext highlighter-rouge">explainability soft mask</code> \(\hat E_{s}\) (= <code class="language-plaintext highlighter-rouge">per-pixel weight</code>), and add it to reconstruction loss term.<br/> deep-learning model은 black-box이므로 explainablity는 중요한 요소</p> </li> <li> <p>trivial sol. \(\hat E_{s} = 0\)을 방지하기 위해, add <code class="language-plaintext highlighter-rouge">regularization</code> term that encourages nonzero prediction of \(\hat E_{s}\)</p> </li> <li> <p>직접 pixel intensity difference로 reconstruction loss를 얻으므로, GT depth &amp; pose로 project하여 얻은 \(p_{s}\) 가 low-texture region or far region에 있을 경우 training 방해 (common issue in motion estimation)<br/> \(\rightarrow\) 해결 1. use conv. encoder-decoder with small bottleneck<br/> \(\rightarrow\) 해결 2. add <code class="language-plaintext highlighter-rouge">multi-scale</code> and <code class="language-plaintext highlighter-rouge">smoothness loss</code> term<br/> (less sensitive to architecture choice, so 이 논문은 해결 2. 적용)</p> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/4-480.webp 480w,/assets/img/2024-04-06-SfMLearner/4-800.webp 800w,/assets/img/2024-04-06-SfMLearner/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> s : source view image index / p : target view pixel index </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/5-480.webp 480w,/assets/img/2024-04-06-SfMLearner/5-800.webp 800w,/assets/img/2024-04-06-SfMLearner/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> l : multi-scale / s : source view image index </div> <h4 id="network-architecture">Network Architecture</h4> <ul> <li>Network 1. <code class="language-plaintext highlighter-rouge">Single-view Depth CNN</code><br/> input : target view<br/> output : per-pixel depth map<br/> DispNet encoder-decoder architecture</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/6-480.webp 480w,/assets/img/2024-04-06-SfMLearner/6-800.webp 800w,/assets/img/2024-04-06-SfMLearner/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Network 2. <code class="language-plaintext highlighter-rouge">Multi-view Pose CNN</code> (아래 figure의 파란 부분)<br/> input : target view concatenated with all source views<br/> output : 6-DoF relative poses between target view and each source view<br/> (Pose CNN estimates <code class="language-plaintext highlighter-rouge">6 channels (3 Euler angles + 3D translation vector)</code> for each source view, and then it is converted to \(4 \times 4\) <code class="language-plaintext highlighter-rouge">transformation matrix</code>)</li> </ul> <p><code class="language-plaintext highlighter-rouge">어떻게 transformation matrix로 변환???</code></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/7-480.webp 480w,/assets/img/2024-04-06-SfMLearner/7-800.webp 800w,/assets/img/2024-04-06-SfMLearner/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/7-480.webp 480w,/assets/img/2024-04-06-SfMLearner/7-800.webp 800w,/assets/img/2024-04-06-SfMLearner/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Network 3. <code class="language-plaintext highlighter-rouge">Explainablity soft mask</code> (= <code class="language-plaintext highlighter-rouge">reconstruction weight per pixel</code>) (위의 figure의 빨간 부분)<br/> output : multi-scale explainability masks<br/> (it estimates <code class="language-plaintext highlighter-rouge">2 channels</code> for each source view at each prediction layer)</li> </ul> <p><code class="language-plaintext highlighter-rouge">weight per pixel인데 왜 2 channels are needed for explainability mask???</code></p> <h2 id="experiments">Experiments</h2> <p>Train : BN, Adam optimizer, monocular camera (one camera lens), resize input image<br/> Test : arbitrary input image size</p> <h4 id="single-view-depth-estimation">Single-view depth estimation</h4> <ul> <li>train model on the split (exclude frames from test sequences and exclude static scene’s pixels with mean optical flow magnitude &lt; 1)</li> <li>pre-trained on Cityscapes dataset / fine-tuned on KITTI dataset / test on Make3D dataset</li> <li>may improve if we also use left-right cycle consistency loss</li> <li>ablation study 결과, explainablity mask를 추가하고 fine-tuning하는 게 더 좋은 성능 도출</li> </ul> <h4 id="multi-view-pose-estimation">Multi-view pose estimation</h4> <ul> <li>trained on KITTI odometry(change in position over time by motion sensor) dataset</li> <li>measurement :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/8-480.webp 480w,/assets/img/2024-04-06-SfMLearner/8-800.webp 800w,/assets/img/2024-04-06-SfMLearner/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>ATE : Absolute Trajectory Error<br/> left/right turning magnitude : coordinate diff. in the side-direction between start and ending frame at test<br/> Mean Odom. : mean of car motion for 5-frame snippets from GT odometry dataset<br/> ORB-SLAM(full) : recover odometry using all frames for loop closure and re-localization<br/> ORB-SLAM(short) : Ours에서처럼, use 5-frame snippets as input<br/> \(\rightarrow\) 특히 small left/right turning magnitude (car is mostly driving forward) 상황에서 Ours가 ORB-SLAM(short)보다 성능 더 좋으므로 monocular SLAM system의 local estimation module을 Ours가 대체할 수 있을 것이라 예상​<br/> (<code class="language-plaintext highlighter-rouge">SLAM 논문 아직 안 읽어봄. 읽어보자.</code>)</p> <h4 id="visualizing-explainability-prediction">Visualizing Explainability Prediction</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/9-480.webp 480w,/assets/img/2024-04-06-SfMLearner/9-800.webp 800w,/assets/img/2024-04-06-SfMLearner/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> highlighted pixels at explainability mask : predicted to be unexplainable </div> <p>explainability = per-pixel weight (confidence 느낌) for reconstruction</p> <p>row 1 ~ 3 : due to motion (dynamic objects are unexplainable)<br/> row 4 ~ 5 : due to occlusion/visibility (disappeared objects are unexplainable)<br/> row 6 ~ 7 : due to other factors (e.g. depth CNN has low confidence on thin structures)</p> <h2 id="discussion">Discussion</h2> <h4 id="contribution">Contribution</h4> <ul> <li>end-to-end <code class="language-plaintext highlighter-rouge">unsupervised</code> learning from <code class="language-plaintext highlighter-rouge">monocular</code> sequences<br/> (기존에는 gt depth로 depth supervision 또는 calibrated stereo images로 pose supervision이었지만, 본 논문은 <code class="language-plaintext highlighter-rouge">view synthesis (reconstruction)을 supervision으로</code> 써서 unsupervised learning으로도 comparable performance 달성)</li> <li>depth CNN recognizes common structural features of objects, and pose CNN uses image correspondence with estimating camera motion</li> </ul> <h4 id="limitation">Limitation</h4> <ol> <li> <p><code class="language-plaintext highlighter-rouge">dynamic objects (X) / occlusion (X) / must be Lambertain surface / vast open scenes (X) / when objects are close to the front of camera (X) / thin structure (X)</code><br/> \(\rightarrow\) 위의 한계들을 개선하고자 explainablity mask (= per-pixel reconstruction confidence 느낌) 도입했지만, it is implicit consideration</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">assume that camera intrinsic K is given</code>, so not generalized to the random videos with unknown camera types</p> </li> <li> <p>predict simplified 3D depth map of <code class="language-plaintext highlighter-rouge">surface</code> (<code class="language-plaintext highlighter-rouge">not full 3D volumetric representation</code>)</p> </li> </ol> <p>중간중간에 있는 질문들은 아직 이해하지 못해서 남겨놓은 코멘트입니다.<br/> 추후에 다시 읽어보고 이해했다면 업데이트할 예정입니다.<br/> 혹시 알고 계신 분이 있으면 댓글로 남겨주시면 감사하겠습니다!</p>]]></content><author><name></name></author><category term="depth-estimation"/><category term="unsupervised"/><category term="depth"/><category term="ego"/><category term="motion"/><category term="video"/><summary type="html"><![CDATA[Unsupervised Learning of Depth and Ego-Motion from Video]]></summary></entry><entry><title type="html">Monocular Depth Estimation with Left Right Consistency</title><link href="https://semyeong-yu.github.io/blog/2024/Monocular_Depth_Estimation_LR_Consistency/" rel="alternate" type="text/html" title="Monocular Depth Estimation with Left Right Consistency"/><published>2024-04-05T17:00:00+00:00</published><updated>2024-04-05T17:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Monocular_Depth_Estimation_LR_Consistency</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Monocular_Depth_Estimation_LR_Consistency/"><![CDATA[<h1 id="unsupervised-monocular-depth-estimation-with-left-right-consistency">Unsupervised Monocular Depth Estimation with Left-Right Consistency</h1> <h4 id="clement-godard-oisin-mac-aodha-gabriel-j-brostow">Clement Godard, Oisin Mac Aodha, Gabriel J. Brostow</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/1609.03677">https://arxiv.org/abs/1609.03677</a><br/> referenced blog :<br/> <a href="https://blog.naver.com/dncks1107/223104039030">https://blog.naver.com/dncks1107/223104039030</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ul> <li>unsupervised mono (single image as input) depth estimation</li> <li>need for binocular stereo image pairs at training</li> <li>novel loss function : left-right consistency b.w. disparity maps</li> </ul> </blockquote> <h2 id="backgrounds">Backgrounds</h2> <h4 id="stereo-depth-estimation">Stereo Depth Estimation</h4> <p>인간은 물체를 두 개의 눈을 통해 바라보고 그 차이를 이용하여 대상까지의 거리를 예측한다. AI는 이러한 인간의 시각 시스템을 모방하여 stereo depth estimation을 통해 대상까지의 깊이를 추정할 수 있다.<br/> Stereo image란 카메라 두 대를 사용하여 찍은 두 이미지를 의미하고, disparity는 한 쌍의 stereo image 간의 pixel difference를 의미한다.<br/> <code class="language-plaintext highlighter-rouge">stereo depth estimation은 stereo image 한 쌍 (left image, right image)을 network의 input</code>으로 넣어 이미지 간의 disparity를 통해 depth를 추정하는 것이다.<br/> <code class="language-plaintext highlighter-rouge">stereo depth estimation의 경우, epipolar geometry라는 수학적 원리에 의해 depth를 계산하기 때문에 비교적 정확하지만 카메라와 물체 사이의 거리가 멀어질수록 불리해진다.</code></p> <h4 id="monocular-depth-estimation">Monocular Depth Estimation</h4> <p><code class="language-plaintext highlighter-rouge">monocular depth estimation은 위와 달리 하나의 image만을 network의 input</code>으로 넣어 depth를 추정하는 방법이다. 물론 test-phase에서 하나의 image를 input으로 넣겠다는 의미이고, 이 논문의 경우 training loss를 구할 때는 stereo image 쌍을 모두 이용하였다.<br/> <code class="language-plaintext highlighter-rouge">mono depth estimation의 경우, 믿을 만한 근거(epipolar geometry와 같은 수학적 원리)가 없기 때문에 정확도가 떨어지지만 하나의 image만 input으로 넣기 때문에 전처리 과정이 간단하고 메모리도 덜 필요로 하여, 어느 정도의 정확도만 확보된다면 실생활에서 적용 가능 범위가 더 넓다.</code></p> <h4 id="monocular-and-stereo-camera">Monocular and Stereo Camera</h4> <p><code class="language-plaintext highlighter-rouge">monocular camera</code> : 특정 시간 t에 한 개의 camera 렌즈를 사용<br/> <code class="language-plaintext highlighter-rouge">stereo camera</code> : 특정 시간 t에 6~7cm 떨어진 두 개의 camera 렌즈를 사용</p> <h2 id="abstract">Abstract</h2> <p>기존의 supervised depth estimation 방식은 성능은 좋지만, 구하기 어려운 pixel-wise ground-truth depth data를 대량으로 필요로 한다는 단점이 있었다. 그래서 본 논문의 저자는 ground-truth depth 정보가 없는 stereo image 쌍으로부터 pixel-level depth map을 합성하도록 훈련하는 unsupervised depth estimation 방식을 제안한다. 효과적인 표기를 위해 아래의 notation을 사용하자.<br/> \(I^{l}\) : left image<br/> \(I^{r}\) : right image<br/> \(d^{r}\) : disparity map from left to right<br/> \(d^{l}\) : disparity map from right to left</p> <p>그렇다면 stereo image \(I^{l}, I^{r}\)로부터 어떻게 depth를 추정할까? <code class="language-plaintext highlighter-rouge">image rectfiication</code>을 거친 뒤, depth를 직접 예측하는 게 아니라 우선 두 개의 <code class="language-plaintext highlighter-rouge">disparity map (dense correspondence field)</code> \(d^{r}, d^{l}\) 을 생성한다. 여기서 disparity map이란, image의 한 pixel이 다른 image의 어느 pixel에 대응하는지에 대한 정보를 의미한다. 이후 \(I^{l}\)과 \(d^{r}\)을 이용하여 \(I^{r \ast} = I^{l}(d^{r})\)을 reconstruct하고, \(I^{r}\)과 \(d^{l}\)을 이용하여 \(I^{l \ast} = I^{r}(d^{l})\)을 reconstruct 한 뒤, \(I^{r \ast}\)과 \(I^{r}\) 간의 reconstruction loss와 \(I^{l \ast}\)과 \(I^{l}\) 간의 <code class="language-plaintext highlighter-rouge">reconstruction loss</code>를 이용하여 모델을 학습시킨다. 그런데 reconstruction loss만 사용한다면 depth image의 quality가 저하된다고 한다. 따라서 본 논문의 저자는 ​\(d^{r}\)과 (projected \(d^{l}\)) = \(d^{l}(d^{r})\) 간의 차이도 고려하는 <code class="language-plaintext highlighter-rouge">left-right disparity consistency loss</code>라는 논문의 핵심 아이디어를 제안하였다.</p> <h2 id="contribution">Contribution</h2> <ul> <li>end-to-end unsupervised monocular depth estimation</li> <li>new training loss that enforces left-right disparity consistency</li> </ul> <h2 id="related-work">Related Work</h2> <h4 id="supervised-stereo-depth-estimation">supervised stereo depth estimation</h4> <p>DispNet (Mayer et al.) :<br/> directly predict the disparity for each pixel by regression loss<br/> 단점 : need lots of ground-truth disparity data and stereo image pairs, which are hard to obtain in real-world</p> <h4 id="unsupervised-depth-estimation">unsupervised depth estimation</h4> <p>Deep Stereo (Flynne et al.) :<br/> select pixels from nearby images and generate new views by using the relative pose of multiple cameras<br/> 단점 : At test phase, need several nearby posed images, which is not mono depth estimation</p> <p>Deep3D (Xie et al.) :<br/> make a distribution over all the possible disparities for each pixel and generate right view from an input left image by using image reconstruction loss<br/> 단점 : need much memory if there are lots of possible disparities. So, it is not scalable to bigger output resolutions</p> <p>Garg et al. :<br/> 본 논문과 유사하게 unsupervised mono depth estimation with image reconstruction loss<br/> 단점 : not fully differentiable (이를 보완하고자 Taylor approximation을 수행하긴 했지만 이는 more challenging to optimize)</p> <h2 id="method">Method</h2> <h4 id="depth-estimation-as-image-reconstruction">Depth Estimation as Image Reconstruction</h4> <p>핵심 아이디어 : calibrated binocular(stereo) camera로 같은 시간에 찍은 한 쌍의 stereo image가 주어졌을 때, <code class="language-plaintext highlighter-rouge">하나의 image로부터 다른 image를 reconstruct 할 수 있다면 그 장면의 3D 구조를 알 수 있다!</code></p> <p>우선 a stereo image pair에 대해 image rectification을 거친 뒤 만약 <code class="language-plaintext highlighter-rouge">disparity map을 얻었다면 아래의 도식에 의해 depth map으로 변환</code>할 수 있다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/1.JPG-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/1.JPG-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/1.JPG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/1.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>b : baseline distance between two camera centers (상수)<br/> f : camera focal length (상수)<br/> 가로로 뻗은 직선 : rectified image plane<br/> d : predicted disparity<br/> d^ : depth<br/> b : d^ = b - d : d^ - f 이므로 d^ (b - d) = b (d^ - f) 이고, 이를 정리하면 d^d = bf, 즉 <code class="language-plaintext highlighter-rouge">d^ = bf / d</code> 이다.<br/> 만약 disparity \(d = x_{r} - x_{l}\) 과 depth Z = d^를 얻었다면, 아래의 도식으로 X, Y 값도 얻을 수 있어서 3D point 좌표를 알 수 있다.<br/> <code class="language-plaintext highlighter-rouge">(아래의 도식은 뭘 말하는거지?)</code><br/> \(x = \frac{f \cdot X}{Z} + p_x\)</p> <h4 id="depth-estimation-network">Depth Estimation Network</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/2.JPG-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/2.JPG-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/2.JPG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/2.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>위의 figure에서 볼 수 있듯이 Naive 버전은 input left image와 align할 output reconstructed left image가 없다. 한편, No LR 버전은 align할 output reconstructed left image는 존재하지만, <code class="language-plaintext highlighter-rouge">left-right consistency가 보장되지 않기 때문에 'texture-copy' artifacts와 depth discontinuities(boundaries)에서의 errors가 생기는 문제가 있다.</code> 본 논문의 model은 disparity \(d^{r}, d^{l}\) 을 동시에 추론함으로써 이러한 문제들을 모두 해결하였다.<br/> 위의 figure에서 볼 수 있듯이 mono depth estimation이므로 CNN의 <code class="language-plaintext highlighter-rouge">input으로 left image만을 넣어서 disparity dr, dl 을 동시에 추론</code>하였다. 이를 통해 두 disparity 간의 consistency를 어느 정도 강제할 수 있고 결과적으로 더 정확한 depth estimation이 가능해진다. 참고로 <code class="language-plaintext highlighter-rouge">right image는 image reconstruction과 training loss를 구할 때만 사용</code>된다.<br/> disparity를 구한 뒤에는 <code class="language-plaintext highlighter-rouge">bilinear sampler와 backward mapping을 통해 image reconstruction</code>을 수행한다. 이 때, <code class="language-plaintext highlighter-rouge">STN(spatial transformer network)의 bilinear sampler를 이용하기 때문에 위의 일련의 과정은 fully convolutional and fully differentiable</code>하다.</p> <p>backward mapping :<br/> 결과 영상으로 mapping 되는 원본 영상에서의 좌표를 계산하여 해당 밝기값을 가져온다. 이 때, 원본 영상에서의 좌표는 실숫값이므로 bilinear interpolation (output pixel = the weighted sum of four input pixel)을 사용한다. 결과 영상의 각 pixel에 대해 값을 가져오므로 forward mapping에서의 hole 발생은 일어나지 않는다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/3-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/3-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/4-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/4-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>본 논문의 model은 크게 두 부분으로 나뉜다. : encoder (conv1~conv7b) and decoder (upconv7~)<br/> 본 논문의 model은 output으로서 disparity \(d^{r}, d^{l}\)을 동시에 추론하는데, 이를 <code class="language-plaintext highlighter-rouge">four different output scales</code>에 대해 반복한다.</p> <h4 id="train-loss">Train Loss</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/5-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/5-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/6-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/6-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>\(C_s\) : loss at output scale s</p> <blockquote> <p>\(C_{ap}^{l}\) :<br/> appearance matching loss for left image (<code class="language-plaintext highlighter-rouge">image reconstruction term</code>)<br/> How much \(I^{r}(d^{l})\) appears similar to \(I^{l}\)</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/7-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/7-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>이 때, <code class="language-plaintext highlighter-rouge">SSIM (Structural Similarity Index Measure)</code>는 두 images 간의 차이가 작을수록 1 에 가까운 값을 가지며, 정확한 정의는 아래를 참고하자.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/8-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/8-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>\(C_{ds}^{l}\) :<br/> disparity smoothness loss (<code class="language-plaintext highlighter-rouge">smoothness term</code>)<br/> How much \(d^{l}\) is smooth</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/9-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/9-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>real-world에서 depth가 급격하게 변하는 경우 image boundary 혹은 texture change가 있는 부분이므로 image plane에서도 해당 부분의 image gradient가 크게 나타난다. 따라서 image gradient가 큰 부분에서는 disparity (depth) 변화를 허용하지만, image gradient가 작은 부분에서는 disparity (depth)가 부드럽게 변하도록 하는 것이 disparity smoothness loss의 역할이다.</p> <blockquote> <p>\(C_{lr}^{l}\) :<br/> left-right consistency (<code class="language-plaintext highlighter-rouge">left-right disparity consistency term</code>)<br/> How much \(d^{l}\) and \(d^{r}(d^{l})\) are consistent</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/10-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/10-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>image reconstruction 뿐만 아니라 left-right disparity consistency까지 고려함으로써 <code class="language-plaintext highlighter-rouge">depth estimation의 accuracy</code>를 향상시킬 수 있다.</p> <h2 id="results--limitations">Results &amp; Limitations</h2> <h4 id="results">Results</h4> <p>Train : on Cityscapes and KITTI 2015 dataset using two different test splits<br/> Test : on other datasets like Make3D and CamVid</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/11-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/11-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/12-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/12-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><code class="language-plaintext highlighter-rouge">Post-processing</code> :<br/> original left image로부터 구한 disparity map을 \(d^{l}\) 라 하고,<br/> flipped left image로부터 구한 disparity map을 \(d^{l \ast}\) 라 하고,<br/> 이를 다시 flip한 걸 \(d^{l \ast \ast}\) 라 할 때,<br/> \(d^{l}\)의 경우 stereo disocclusions which create disparity ramps(경사) on both the left side of the image and the left of occluders 가 있을 수 있는데,<br/> \(d^{l \ast \ast}\)의 경우 disparity ramps are located on the right side of the image and the right of occluders 이므로<br/> <code class="language-plaintext highlighter-rouge">We combine both disparity maps to form the final disparity map</code> by assigning the first 5% on the left of the image using \(d^{l \ast \ast}\) and the last 5% on the right to the disparities from \(d^{l}\). The central part of the final disparity map is the average of \(d^{l \ast \ast}\) and \(d^{l}\).<br/> 이러한 post-processing을 통해 can reduce the effect of stereo disocclusions, and lead to better accuracy and less visual artifacts,<br/> but double the amount of test time<br/> (<code class="language-plaintext highlighter-rouge">stereo disocclusions의 영향을 줄이기 위한 post-processing 과정 아직 완벽하게 이해하지는 못했음</code>)</p> <h4 id="limitations">Limitations</h4> <ol> <li> <p>left-right consistency와 post-processing으로 quality 향상을 이룬 건 맞지만, <code class="language-plaintext highlighter-rouge">두 images에서 모두 안 보이는 occlusion region에서의 pixels 때문에 occlusion boundaries에서는 여전히 artifacts가 존재</code>한다. training phase에서 occclusion에 대해 explicitly reasoning하는 것으로 이 문제를 개선할 수는 있지만, supervised methods 또한 모든 pixels에 대해 항상 valid depth를 가지는 것은 아님에 주목할 필요가 있다.</p> </li> <li> <p>training phase에서 <code class="language-plaintext highlighter-rouge">rectified and temporally aligned (image rectification을 거치고 동시에 찍은) stereo image pairs가 필요</code>하다. 이 말은 즉슨, single-view dataset은 training에 쓸 수 없다. (fine-tune하는 것만 가능하다.)</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">image reconstruction term에 의존</code>한다. 이 말은 즉슨, <code class="language-plaintext highlighter-rouge">specular and transparent (거울 같이 반사하는 and 투명한) surfaces에서는 inconsistent depth</code>가 생긴다. 이는 더 정교한 similarity measures를 사용함으로써 개선될 수 있다.</p> </li> </ol> <h2 id="conclusion">Conclusion</h2> <ul> <li>unsupervised mono (single image as input) depth estimation \(\rightarrow\) no need for expensive GT depth</li> <li>need for binocular stereo image pairs at training</li> <li>novel loss function : left-right consistency b.w. disparity maps \(\rightarrow\) improve quality of depth map</li> <li>can generalize to unseen datasets</li> </ul> <h2 id="future-work">Future Work</h2> <ul> <li>extend to videos (can add temporal consistency)</li> <li>investigate sparse input as an alternative training signal<br/> (<code class="language-plaintext highlighter-rouge">?? 이해 못함</code>)</li> <li>our model estimates per-pixel depth, but it would be also interesting to predict the full occupancy of the scene<br/> (<code class="language-plaintext highlighter-rouge">?? 이해 못함</code>)</li> </ul> <p>중간중간에 있는 질문들은 아직 이해하지 못해서 남겨놓은 코멘트입니다.<br/> 추후에 다시 읽어보고 이해했다면 업데이트할 예정입니다.<br/> 혹시 알고 계신 분이 있으면 댓글로 남겨주시면 감사하겠습니다!</p>]]></content><author><name></name></author><category term="depth-estimation"/><category term="unsupervised"/><category term="monocular"/><category term="depth"/><category term="consistency"/><summary type="html"><![CDATA[Unsupervised Monocular Depth Estimation with Left-Right Consistency]]></summary></entry></feed>