<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://semyeong-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://semyeong-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-31T07:52:59+00:00</updated><id>https://semyeong-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Deblurring 3D Gaussian Splatting</title><link href="https://semyeong-yu.github.io/blog/2024/DeblurGS/" rel="alternate" type="text/html" title="Deblurring 3D Gaussian Splatting"/><published>2024-10-30T12:00:00+00:00</published><updated>2024-10-30T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/DeblurGS</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/DeblurGS/"><![CDATA[<h2 id="deblurring-3d-gaussian-splatting-eccv-2024">Deblurring 3D Gaussian Splatting (ECCV 2024)</h2> <h4 id="byeonghyeon-lee-howoong-lee-xiangyu-sun-usman-ali-eunbyung-park">Byeonghyeon Lee, Howoong Lee, Xiangyu Sun, Usman Ali, Eunbyung Park</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2401.00834">https://arxiv.org/abs/2401.00834</a><br/> project website :<br/> <a href="https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/">https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/</a><br/> code :<br/> <a href="https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting">https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting</a></p> </blockquote> <h3 id="introduction">Introduction</h3> <ul> <li>3DGS : <ul> <li>novel-view로 inference할 때<br/> NeRF는 새로운 각도를 MLP에 넣어야만 color, opacity 얻을 수 있지만<br/> 3DGS는 spherical harmonics, explicit 기법이라 새로운 각도에 대해서도 바로 color, opacity 얻을 수 있어서<br/> volume rendering이 빠름</li> <li>differentiable splatting-based rasterization with parallelism</li> </ul> </li> <li>본 논문 : <ul> <li>핵심 : <ul> <li>각 3DGS의 <code class="language-plaintext highlighter-rouge">covariance</code>를 수정하여 <code class="language-plaintext highlighter-rouge">blur(adjacent pixels의 혼합)를 모델링하는 작은 MLP</code> 사용</li> <li>training 시에는 MLP output 곱해서 blurry image를 생성하고<br/> inference 시에는 MLP 사용하지 않아서 real-time으로 sharp image 생성</li> </ul> </li> <li>문제 : <ul> <li>3DGS는 initial point cloud에 많이 의존하는데<br/> given images가 <code class="language-plaintext highlighter-rouge">blurry</code>하면 SfM은 유효한 feature를 식별하지 못해서 <code class="language-plaintext highlighter-rouge">매우 적은 수의 point</code> cloud를 추출함</li> <li>심지어 depth가 크면 SfM은 맨 끝에 있는 점을 거의 추출하지 않음</li> </ul> </li> <li>해결 : <ul> <li>sparse point cloud를 방지하고자<br/> <code class="language-plaintext highlighter-rouge">N-nearest-neighbor interpolation으로 points 추가</code></li> <li>먼 거리의 평면에 많은 Gaussian을 유지하기 위해<br/> <code class="language-plaintext highlighter-rouge">위치에 따라 Gaussian pruning</code></li> </ul> </li> <li>contribution :<br/> SOTA qualtiy인데 훨씬 빠른 rendering speed (\(\gt 200\) FPS)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-DeblurGS/1-480.webp 480w,/assets/img/2024-10-30-DeblurGS/1-800.webp 800w,/assets/img/2024-10-30-DeblurGS/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-DeblurGS/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Overall Architecture </div> <h3 id="related-works">Related Works</h3> <p>TBD <code class="language-plaintext highlighter-rouge">???</code></p> <p>Related Works : Deblur-NeRF: Neural Radiance Fields from Blurry Images (CVPR 2022) DP-NeRF: Deblurred Neural Radiance Field with Physical Scene Priors (CVPR 2023)</p> <h3 id="background">Background</h3> <ul> <li> <p>3DGS <a href="https://semyeong-yu.github.io/blog/2024/GS/">Link</a> 참고</p> </li> <li> <p>Blur :</p> <ul> <li>Defocus Blur :<br/> 렌즈의 <code class="language-plaintext highlighter-rouge">초점이 맞지 않아서</code> 흐려진 경우<br/> e.g. 인물 사진에서 인물만 초점이 맞고 배경은 흐릿한 경우</li> <li>Camera Motion Blur :<br/> 셔터가 열려 있는 동안 카메라가 움직이거나 피사체가 <code class="language-plaintext highlighter-rouge">움직여서</code> 흐려진 경우<br/> e.g. 달리는 자동차를 촬영한 경우</li> </ul> </li> </ul> <h3 id="defocus-blur">Defocus Blur</h3> <ul> <li>Motivation : <ul> <li>Defocus Blur는 일반적으로<br/> PSF(point spread func.)로 알려진 2D Gaussian function과 실제 image의<br/> convolution으로 모델링<br/> 즉, a pixel이 주위 pixels에 영향을 미칠 경우 blur</li> <li>여기서 영감을 받아<br/> covariance(크기)가 큰 3DGS는 Blur를 유발하고<br/> covariance(크기)가 작은 3DGS는 선명한 detail에 기여한다고 가정</li> <li>그렇다면 covariance \(\Sigma = R S S^{T} R^{T}\) 를 변경하여 Blur를 모델링해야겠다!</li> </ul> </li> <li>Defocus Blur를 모델링하는 MLP :<br/> \((\delta_{r}, \delta_{s}) = F_{\theta}(\gamma(x), r, s, \gamma(v))\)<br/> where input : position, rotation, scale, view-direction<br/> where output : rotation change, scale change<br/> (\(\gamma\) : positional encoding) <ul> <li>transformed 3DGS :<br/> \(r^{'} = r \cdot \delta_{r}\) and \(s^{'} = s \cdot \delta_{s}\) (element-wise multiplication)</li> <li>Defocus Blur :<br/> MLP output \(\delta_{r}, \delta_{s}\) 의 <code class="language-plaintext highlighter-rouge">최솟값을 1로 clip</code><br/> \(\rightarrow\)<br/> 그럼 \(s^{'}\) 이 \(s\) 보다 크거나 같으므로 transformed 3DGS는 더 <code class="language-plaintext highlighter-rouge">큰 covariance</code>를 가져서<br/> <code class="language-plaintext highlighter-rouge">Defocus Blur</code>의 근본 원인인 주변 정보의 간섭을 모델링할 수 있게 됨</li> <li>inference :<br/> <code class="language-plaintext highlighter-rouge">training</code> 시에는 <code class="language-plaintext highlighter-rouge">transformed 3DGS</code>가 <code class="language-plaintext highlighter-rouge">blurry</code> image를 생성하지만<br/> <code class="language-plaintext highlighter-rouge">inference</code> 시에는 MLP를 사용하지 않은 <code class="language-plaintext highlighter-rouge">기존 3DGS</code>가 <code class="language-plaintext highlighter-rouge">sharp</code> image를 생성<br/> \(\rightarrow\)<br/> training할 때는 MLP forwarding과 간단한 element-wise multiplication만 추가 비용이고,<br/> inference할 때는 MLP를 사용하지 않아 Vanilla-3DGS와 모든 단계가 동일하므로 <code class="language-plaintext highlighter-rouge">추가 비용 없이 real-time rendering</code> 가능</li> </ul> </li> </ul> <h3 id="selective-blurring">Selective Blurring</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-DeblurGS/2-480.webp 480w,/assets/img/2024-10-30-DeblurGS/2-800.webp 800w,/assets/img/2024-10-30-DeblurGS/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-DeblurGS/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>초점에 따른 Defocus Blur는 <code class="language-plaintext highlighter-rouge">영역마다 흐린 수준이 다름</code><br/> 본 논문에서는 <code class="language-plaintext highlighter-rouge">각 3DGS마다</code> 다르게 \(\delta_{r}, \delta_{s}\) 를 추정하므로<br/> Gaussian의 covariance를 선택적으로 확대시킬 수 있어서<br/> 영역에 따라 다르게 blurring 할 수 있으므로<br/> <code class="language-plaintext highlighter-rouge">pixel 단위의 blurring</code>을 보다 유연하게 모델링 가능 <ul> <li>defocus blur가 심한 영역에 있는 3DGS는 \(\delta_{s}\) 가 더 크도록</li> <li>당연히 단일 유형의 Gaussian Blur kernel을 써서 평균 blurring을 모델링하는 것보다<br/> 본 논문에서처럼 3DGS마다 다른 Blur kernel을 적용하여 pixel 단위 blurring을 모델링하는 게 더 좋음!</li> </ul> </li> </ul> <h3 id="camera-motion-blur">Camera motion Blur</h3> <p>TBD <code class="language-plaintext highlighter-rouge">???</code></p> <h3 id="compensation-for-sparse-point-cloud">Compensation for Sparse Point Cloud</h3> <ul> <li> <p>문제 1)<br/> 3DGS는 initial point cloud에 많이 의존하는데<br/> given images가 <code class="language-plaintext highlighter-rouge">blurry</code>하면 SfM은 유효한 feature를 식별하지 못해서 <code class="language-plaintext highlighter-rouge">매우 적은 수의 point</code> cloud를 추출함</p> </li> <li> <p>해결 :</p> <ul> <li>sparse point cloud를 방지하고자<br/> \(N_{st}\) iter. 후에 \(N_{p}\)-개의 points를 uniform \(U(\alpha, \beta)\) 에서 sampling하여 추가<br/> where \(\alpha\) : 기존 point cloud 위치의 최솟값<br/> where \(\beta\) : 기존 point cloud 위치의 최댓값</li> <li>새로운 point의 <code class="language-plaintext highlighter-rouge">색상은 KNN(K-Nearest-Neighbor) interpolation</code>으로 할당</li> <li>새로운 points를 uniform 분포에서 sampling해서 <code class="language-plaintext highlighter-rouge">빈 공간에 불필요한 points</code>가 생길 수 있으므로<br/> nearest neighbor까지의 거리가 threshold \(t_{d}\) 를 초과하는 points는 <code class="language-plaintext highlighter-rouge">폐기</code></li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-DeblurGS/3-480.webp 480w,/assets/img/2024-10-30-DeblurGS/3-800.webp 800w,/assets/img/2024-10-30-DeblurGS/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-DeblurGS/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>문제 2)<br/> 심지어 depth가 크면 SfM은 맨 끝에 있는 점을 거의 추출하지 않음</p> </li> <li> <p>해결 :</p> <ul> <li>방법 1) 먼 거리에 있는 3DGS 수 늘리기<br/> 먼 거리의 평면에 있는 3DGS에 대해 denisfy<br/> \(\rightarrow\)<br/> 과도한 densification은 Blur 모델링을 방해하고 추가 계산 비용 필요</li> <li>방법 2) <code class="language-plaintext highlighter-rouge">먼 거리에 있는 3DGS는 덜 pruning</code><br/> pruning threshold를 깊이에 따라 다르게 scaling<br/> as \(t_{p}, 0.9 t_{p}, \cdots , w_{p} t_{p}\)<br/> (먼 거리의 3DGS일수록 낮은 threshold) <br/> \(\rightarrow\)<br/> real-time rendering을 고려했을 때<br/> 유연한 pruning으로도 먼 거리의 3DGS sparsity를 보상하기에 충분하다는 걸 경험적으로 발견</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-DeblurGS/4-480.webp 480w,/assets/img/2024-10-30-DeblurGS/4-800.webp 800w,/assets/img/2024-10-30-DeblurGS/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-DeblurGS/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="experiment">Experiment</h3> <ul> <li>Setting : <ul> <li>GPU : NVIDIA RTX 4090 GPU (24GB)</li> <li>optimzier : Adam</li> <li>iter. : \(30,000\)</li> <li>Blur를 모델링하는 small MLP : <ul> <li>lr : \(10^{-3}\)</li> <li>hidden layer : 3</li> <li>hidden unit : 64</li> <li>activation : ReLU</li> <li>initialization : Xavier</li> </ul> </li> <li>sparse point cloud를 보상하기 위해 <ul> <li>\(N_{st} = 2,500\) iter. 후에 \(N_{p} = 100,000\) 개의 point 추가</li> <li>색상은 \(K = 4\) 의 KNN interpolation으로 할당</li> <li>nearest neighbor까지의 거리가 \(t_{d} = 10\) 을 초과하는 point는 폐기</li> </ul> </li> <li>먼 거리에 있는 3DGS는 덜 pruning하기 위해<br/> pruning threshold를 깊이에 따라 다르게 scaling<br/> as \(t_{p}, 0.9 t_{p}, \cdots , w_{p} t_{p}\)<br/> where \(t_{p} = 5 \times 10^{-3}\) and \(w_{p} = 0.3\)</li> </ul> </li> <li>Results :<br/> SOTA deblurring NeRF만큼 PSNR이 높은데<br/> 3DGS만큼 FPS도 높음</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-DeblurGS/5-480.webp 480w,/assets/img/2024-10-30-DeblurGS/5-800.webp 800w,/assets/img/2024-10-30-DeblurGS/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-DeblurGS/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-DeblurGS/6-480.webp 480w,/assets/img/2024-10-30-DeblurGS/6-800.webp 800w,/assets/img/2024-10-30-DeblurGS/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-DeblurGS/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-DeblurGS/7-480.webp 480w,/assets/img/2024-10-30-DeblurGS/7-800.webp 800w,/assets/img/2024-10-30-DeblurGS/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-DeblurGS/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> real-world Defocus Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-DeblurGS/8-480.webp 480w,/assets/img/2024-10-30-DeblurGS/8-800.webp 800w,/assets/img/2024-10-30-DeblurGS/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-DeblurGS/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-DeblurGS/9-480.webp 480w,/assets/img/2024-10-30-DeblurGS/9-800.webp 800w,/assets/img/2024-10-30-DeblurGS/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-DeblurGS/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> synthesized Defocus Blur Dataset </div> <ul> <li>Ablation Study : <ul> <li>Extra points allocation</li> <li>Depth-based pruning</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-DeblurGS/10-480.webp 480w,/assets/img/2024-10-30-DeblurGS/10-800.webp 800w,/assets/img/2024-10-30-DeblurGS/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-DeblurGS/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Extra points allocation </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-DeblurGS/11-480.webp 480w,/assets/img/2024-10-30-DeblurGS/11-800.webp 800w,/assets/img/2024-10-30-DeblurGS/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-DeblurGS/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Depth-based pruning </div> <h3 id="limitation-and-future-work">Limitation and Future Work</h3> <p>TBD <code class="language-plaintext highlighter-rouge">???</code></p> <h3 id="code-review">Code Review</h3> <p>TBD <code class="language-plaintext highlighter-rouge">???</code></p>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="3DGS"/><category term="deblur"/><summary type="html"><![CDATA[ECCV 2024]]></summary></entry><entry><title type="html">EE534 Pattern Recognition Final</title><link href="https://semyeong-yu.github.io/blog/2024/Pattern2/" rel="alternate" type="text/html" title="EE534 Pattern Recognition Final"/><published>2024-10-28T11:00:00+00:00</published><updated>2024-10-28T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Pattern2</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Pattern2/"><![CDATA[<blockquote> <p>Lecture :<br/> 24F EE534 Pattern Recognition<br/> by KAIST Munchurl Kim <a href="https://www.viclab.kaist.ac.kr/">VICLab</a></p> </blockquote> <h2 id="chapter-6-linear-discriminant-functions">Chapter 6. Linear Discriminant Functions</h2> <h3 id="linearly-non-separable-svm">Linearly Non-Separable SVM</h3> <ul> <li>new constraint :<br/> \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}\)<br/> \(\xi_{i}\) 를 도입하여 이제는 inside margin or misclassified 도 가능하지만 대신 \(C \sum_{i=1}^{N} \xi_{i}\) 를 loss에 넣어서 큰 \(\xi_{i}\) 값을 penalize <ul> <li>\(\xi = 0\) : outside margin or support vector</li> <li>\(0 \lt \xi \leq 1\) : inside margin (correctly classified, but margin violation)</li> <li>\(\xi \gt 1\) : misclassified</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/2-480.webp 480w,/assets/img/2024-10-28-Pattern2/2-800.webp 800w,/assets/img/2024-10-28-Pattern2/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>방법 1) 1-norm-soft-margin <ul> <li>constrained primal form :<br/> minimize \(J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \xi_{i}\)<br/> subject to \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}\) and \(\xi_{i} \geq 0\) <ul> <li>unconstrained primal form :<br/> 이 때 위의 두 가지 constraints는 \(\xi_{i} = \text{max}(0, 1 - y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}))\) 로 하나로 합칠 수 있음<br/> 따라서<br/> minimize \(J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \text{max}(0, 1 - y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}))\)</li> <li>regularization param. \(C\) : <ul> <li>small \(C\) : 큰 \(\xi_{i}\) 값도 허용하므로 margin 커짐</li> <li>large \(C\) : 큰 \(\xi_{i}\) 값은 허용 안 하므로 margin 작아짐</li> <li>\(C = \infty\) : non-zero \(\xi_{i}\) 값 허용 안 하므로 hard margin (no sample inside margin)<br/> (Linearly Separable SVM 에 해당함)</li> </ul> </li> </ul> </li> <li>Lagrangian :<br/> minimize \(L(\boldsymbol w, w_{0}, \xi, \boldsymbol \lambda, \boldsymbol \mu) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \xi_{i} - \sum_{i}^{N} \mu_{i} \xi_{i} - \sum_{i}^{N} \lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (1 - \xi_{i}))\)<br/> subject to \(\xi_{i}, \mu_{i}, \lambda_{i} \geq 0\) <ul> <li> \[\nabla_{\boldsymbol w} L = 0 \rightarrow \boldsymbol w = \sum_{i=1}^{N} \lambda_{i} y_{i} \boldsymbol x_{i}\] </li> <li> \[\nabla_{w_{0}} L = 0 \rightarrow \sum_{i=1}^{N} \lambda_{i} y_{i} = 0\] </li> <li> \[\nabla_{\xi_{i}} L = 0 \rightarrow C - \mu_{i} - \lambda_{i} = 0\] </li> </ul> </li> <li>KKT condition 중 slackness condition : <ul> <li> \[\lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (1 - \xi_{i})) = 0\] </li> <li> \[\mu_{i} \xi_{i} = 0\] </li> </ul> </li> <li>dual form :<br/> 위의 세 가지 식을 대입하여 \(\boldsymbol w, w_{0}, \xi_{i}, \mu_{i}\) 를 소거하면<br/> maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}\)<br/> subject to \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\) and \(0 \leq \lambda_{i} \leq C\)</li> <li>Summary : <ul> <li>Step 1) optimal \(\lambda_{i}^{\ast}\) 구하기<br/> \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\) and \(0 \leq \lambda_{i} \leq C\) 이용해서<br/> \(\nabla_{\lambda_{i}} L = 0\) 으로 아래의 dual form 풀어서<br/> (maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}\))<br/> optimal \(\lambda_{i}\) 얻음</li> <li>Step 2) optimal \(\boldsymbol w^{\ast}, w_{0}^{\ast}\) 구하기 <ul> <li>\(\boldsymbol w^{\ast} = \sum_{i=1}^{N_{s}} \lambda_{i}^{\ast} y_{i} x_{i}\)<br/> (\(N_{s}\) : support vector 개수)</li> <li>\(w_{0}^{\ast} = \frac{1}{y_{j}} - \sum_{i=1}^{N_{s}} \lambda_{i}^{\ast} y_{i} x_{i}^{T} x_{j} = \frac{1}{y_{j}} - \boldsymbol w^{\ast T} x_{j}\)<br/> (support vector \(x_{j}\) 1개 사용)<br/> 또는<br/> \(w_{0}^{\ast} = \frac{1}{N_{s}} \sum_{j=1}^{N_{s}} (\frac{1}{y_{j}} - \sum_{i=1}^{N_{s}} \lambda_{i}^{\ast} y_{i} x_{i}^{T} x_{j}) = \frac{1}{N_{s}} \sum_{j=1}^{N_{s}} (\frac{1}{y_{j}} - \boldsymbol w^{\ast T} x_{j})\)<br/> (support vector \(x_{j}\) \(N_{s}\)-개 모두 사용하여 average value)</li> </ul> </li> <li>Tip : hard margin (no sample inside margin) 의 경우<br/> 육안으로 어떤 sample이 support vector일지 판단 가능하다면<br/> complementary slackness condition (\(\lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (1 - \xi_{i})) = 0\)) 에서<br/> support vector만 \(\lambda_{i} \gt 0\) 이므로<br/> 연립해서 \(\boldsymbol w^{\ast}, w_{0}^{\ast}\) 바로 구할 수 있음</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/1-480.webp 480w,/assets/img/2024-10-28-Pattern2/1-800.webp 800w,/assets/img/2024-10-28-Pattern2/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>방법 2) 2-norm-soft-margin <ul> <li>차이점 1) primal form<br/> minimize \(J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \xi_{i}\)<br/> subject to \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}\) and \(\xi_{i} \geq 0\)<br/> 대신<br/> minimize \(J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + \frac{1}{2} C \sum_{i=1}^{N} \xi_{i}^{2}\)<br/> subject to \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}\) and \(\xi_{i} \geq 0\)</li> <li>차이점 2) Lagrangian<br/> \(\nabla_{\xi_{i}} L(\boldsymbol w, w_{0}, \xi, \boldsymbol \lambda, \boldsymbol \mu) = 0\) 했을 때<br/> \(C - \mu_{i} - \lambda_{i} = 0\)<br/> 대신<br/> \(C \xi_{i} - \mu_{i} - \lambda_{i} = 0\)</li> <li>차이점 3) dual form<br/> maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}\)<br/> subject to \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\) and \(0 \leq \lambda_{i} \leq C\)<br/> 대신<br/> maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j} - \frac{1}{2C} \sum_{i=1}^{N} (\lambda_{i} + \mu_{i})^{2}\)<br/> subject to \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\) and \(0 \leq \lambda_{i} \leq C\)</li> </ul> </li> <li>Remark : <ul> <li>Linearly Non-Separable SVM에서<br/> \(C \rightarrow \infty\) 하면 Linearly Separable SVM<br/> e.g. non-linear에서는 \(0 \leq \lambda_{i} \leq C\) 인데, linear에서는 \(0 \leq \lambda_{i} \lt \infty\)</li> <li>SVM의 한계 :<br/> high computational complexity<br/> (SVM training은 주로 batch mode로 진행되어 memory를 많이 필요로 하므로<br/> training dataset을 subset으로 나눠서 training 진행)</li> <li>지금까지는 SVM for two-category만 살펴봤는데,<br/> M-class 의 경우 M개의 discriminant function \(g_{i}(x)\) 를 design하여<br/> assign \(x\) to class \(w_{i}\) if \(i = \text{argmax}_{k} g_{k}(x)\)</li> </ul> </li> </ul> <h3 id="v-svm">v-SVM</h3> <ul> <li>v-SVM : <ul> <li>차이점 1) hyperplane<br/> \(\boldsymbol w^{T} \boldsymbol x_{i} + w_{0} = \pm 1\)<br/> 대신<br/> \(\boldsymbol w^{T} \boldsymbol x_{i} + w_{0} = \pm \rho\)<br/> where \(\rho \geq 0\) : var. to be optimized</li> <li>차이점 2) margin<br/> margin은 \(\frac{2 \rho}{\| w \|}\) 이므로<br/> margin을 maximize하려면<br/> \(\| w \|\) minimize 뿐만 아니라 \(\rho\) maximize하면 되므로<br/> 이를 primal form loss term에 넣음</li> <li>차이점 3) primal form<br/> minimize \(J(\boldsymbol w, \xi, \rho) = \frac{1}{2} \| \boldsymbol w \|^{2} - v \rho + C \sum_{i=1}^{N} \xi_{i}\)<br/> subject to \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq \rho - \xi_{i}\) and \(\xi_{i} \geq 0\) and \(\rho \geq 0\)</li> <li>차이점 4) Lagrangian<br/> TBD 59p</li> </ul> </li> </ul> <h2 id="chapter-6-multilayer-neural-networks">Chapter 6. Multilayer Neural Networks</h2> <h3 id="introduction">Introduction</h3> <h3 id="back-propagation-algorithm">Back-propagation Algorithm</h3> <h3 id="issues-on-neural-networks">Issues on Neural Networks</h3>]]></content><author><name></name></author><category term="cv-tasks"/><category term="cv"/><summary type="html"><![CDATA[Lecture Summary (24F)]]></summary></entry><entry><title type="html">pixelSplat</title><link href="https://semyeong-yu.github.io/blog/2024/pixelSplat/" rel="alternate" type="text/html" title="pixelSplat"/><published>2024-10-25T12:00:00+00:00</published><updated>2024-10-25T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/pixelSplat</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/pixelSplat/"><![CDATA[<h2 id="pixelsplat---3d-gaussian-splats-from-image-pairs-for-scalable-generalizable-3d-reconstruction-cvpr-2024">pixelSplat - 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction (CVPR 2024)</h2> <h4 id="david-charatan-sizhe-li-andrea-tagliasacchi-vincent-sitzmann">David Charatan, Sizhe Li, Andrea Tagliasacchi, Vincent Sitzmann</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2312.12337">https://arxiv.org/abs/2312.12337</a><br/> project website :<br/> <a href="https://davidcharatan.com/pixelsplat/">https://davidcharatan.com/pixelsplat/</a><br/> code :<br/> <a href="https://github.com/dcharatan/pixelsplat">https://github.com/dcharatan/pixelsplat</a><br/> reference :<br/> NeRF and 3DGS Study</p> </blockquote> <h3 id="abstract">Abstract</h3> <ul> <li>pixelSplat :<br/> reconstruct a 3DGS primitive-based parameterization of 3D radiance field from only two images</li> </ul> <h3 id="introduction">Introduction</h3> <ul> <li>Problem : <ul> <li><code class="language-plaintext highlighter-rouge">scale ambiguity</code> :<br/> camera pose has arbitrary scale factor</li> <li><code class="language-plaintext highlighter-rouge">local minima</code> :<br/> primitive param.을 random initialization으로부터 직접 optimize하면 local minima 문제 발생</li> </ul> </li> <li>Contribution : <ul> <li>two-view image encoder :<br/> <code class="language-plaintext highlighter-rouge">two-view Epipolar Sampling, Epipolar Attention</code> 덕분에<br/> scale ambiguity 문제 극복</li> <li>pixel-aligned Gaussian param. prediction module :<br/> depth를 <code class="language-plaintext highlighter-rouge">sampling</code>하기 때문에<br/> local minima 문제 극복</li> </ul> </li> <li>Solution : <ul> <li>feed-forward model 이, a pair of images로부터,<br/> 3DGS primitives로 parameterized되는 3D radiance field recon.을 학습</li> </ul> </li> <li>model : <ul> <li><code class="language-plaintext highlighter-rouge">per-scene model</code> :<br/> <code class="language-plaintext highlighter-rouge">각각의 scene</code>을 학습하기 위해 <code class="language-plaintext highlighter-rouge">정해진 하나의 points set</code>을 <code class="language-plaintext highlighter-rouge">iteratively</code> update<br/> \(\rightarrow\)<br/> local minima 등 문제 있어서<br/> 3DGS에서는 non-differentiable Adaptive Density Control 기법으로 해결하려 하지만<br/> 이는 일반화 불가능</li> <li><code class="language-plaintext highlighter-rouge">feed-forward model</code> :<br/> <code class="language-plaintext highlighter-rouge">scene마다 얻은 points set</code>을 <code class="language-plaintext highlighter-rouge">한 번에 feed-forward</code>로 넣어서 학습<br/> differentiable하게 일반화 가능 <ul> <li>attention</li> <li>MASt3R(-SfM), Spann3R, Splatt3R, DUSt3R (잘 모름. 더 서치해봐야 함.)</li> </ul> </li> </ul> </li> </ul> <h3 id="background">Background</h3> <ul> <li>local minima : <ul> <li>언제 발생? :<br/> random location에 initialize된 Gaussian primitives가<br/> 최종 목적지까지 a few std보다 더 <code class="language-plaintext highlighter-rouge">많이 움직</code>여야 될 때<br/> gradient가 vanish 되어버려서<br/> 또는<br/> 최종 목적지까지 loss가 monotonically decrease하지 않을 때<br/> local minima 발생</li> <li>해결법? :<br/> 3DGS에서는<br/> non-differentiable pruning and splitting 기법인<br/> Adaptive Density Control을 사용하지만<br/> 본 논문에서는<br/> <code class="language-plaintext highlighter-rouge">differentiable</code> parameterization of Gaussian primitives 소개</li> </ul> </li> </ul> <h3 id="scale-ambiguity">Scale Ambiguity</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/4-480.webp 480w,/assets/img/2024-10-25-pixelSplat/4-800.webp 800w,/assets/img/2024-10-25-pixelSplat/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Scale Ambiguity 문제 : <ul> <li><code class="language-plaintext highlighter-rouge">SfM 단계</code>에서 camera pose를 계산할 때<br/> real-world-scale pose \(T_{j}^{m}\) 을<br/> metric pose \(s_{i} T_{j}^{m}\) 으로 scale하여 사용 <ul> <li>\(s_{i}\) :<br/> arbitrary scale factor</li> <li>metric pose \(s_{i} T_{j}^{m}\) :<br/> real-world-scale pose의 translation component를 \(s_{i}\) 만큼 scale</li> </ul> </li> <li>single image의 camera pose \(s_{i} T_{j}^{m}\) 로부터<br/> arbitrary scale factor \(s_{i}\) 를 복원하는 건 불가능</li> </ul> </li> <li>Scale Ambiguity 해결 : <ul> <li>two-view encoder에서 <code class="language-plaintext highlighter-rouge">a pair of images</code> 로부터<br/> arbitrary scale factor \(s_{i}\) 복원</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/1-480.webp 480w,/assets/img/2024-10-25-pixelSplat/1-800.webp 800w,/assets/img/2024-10-25-pixelSplat/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Two-view encoder Overview : <ul> <li>Step 1) Per-Image Encoder</li> <li>Step 2) Epipolar Sampling</li> <li>Step 3) Epipolar Attention</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/2-480.webp 480w,/assets/img/2024-10-25-pixelSplat/2-800.webp 800w,/assets/img/2024-10-25-pixelSplat/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Step 1) Per-Image Encoder </div> <ul> <li>Step 1) <code class="language-plaintext highlighter-rouge">Per-Image Encoder</code> :<br/> each view (two images)를 각각 feature \(F\), \(\tilde F\) 로 encode</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/3-480.webp 480w,/assets/img/2024-10-25-pixelSplat/3-800.webp 800w,/assets/img/2024-10-25-pixelSplat/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Step 2) Epipolar Sampling </div> <ul> <li>Step 2) <code class="language-plaintext highlighter-rouge">Epipolar Sampling</code> :<br/> Features 1 from Image 1의 <code class="language-plaintext highlighter-rouge">ray</code>로 <code class="language-plaintext highlighter-rouge">query</code> 만들고,<br/> Features 2 from Image 2의 <code class="language-plaintext highlighter-rouge">epipolar samples</code> 및 <code class="language-plaintext highlighter-rouge">depth</code> 로 <code class="language-plaintext highlighter-rouge">key, value</code> 만들어서,<br/> attention으로 depth scale을 잘 학습하는 게 목적<br/> (attention : depth 정보와 함께, Image 1의 ray가 Image 2의 epipolar line 위 어떤 sample에 더 많이 attention하는지)<br/> (epipolar line은 학습하는 게 아니라 수학 식으로 계산) <ul> <li>Query :<br/> \(q = Q \cdot F [u]\)<br/> where \(F\) : Features 1 from Image 1 <br/> where \(F [u]\) : ray feature at each pixel (in pixel coordinate)</li> <li>Key, Value :<br/> \(s = \tilde F [\tilde u_{l}] \oplus \gamma (\tilde d_{\tilde u_{l}})\)<br/> where \(\tilde F\) : Features 2 from Image 2<br/> where \(\tilde F [\tilde u_{l}]\) : samples on epipolar line<br/> where \(\tilde d_{\tilde u_{l}}\) : Image 2의 camera 원점까지의 거리 <ul> <li> \[k_{l} = K \cdot s\] </li> <li> \[v_{l} = V \cdot s\] </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/5-480.webp 480w,/assets/img/2024-10-25-pixelSplat/5-800.webp 800w,/assets/img/2024-10-25-pixelSplat/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Step 3) Epipolar Attention 중 Epipolar Cross-Attention </div> <ul> <li>Step 3) Epipolar Attention : <ul> <li><code class="language-plaintext highlighter-rouge">Epipolar Cross-Attention</code> :<br/> 앞서 만든 \(q, k_{l}, v_{l}\) 로 <code class="language-plaintext highlighter-rouge">cross-attention 수행</code>하여<br/> per-pixel <code class="language-plaintext highlighter-rouge">correpondence b.w. ray and epipolar sample</code> 찾음으로써<br/> per-pixel feature \(F [u]\) 가 이제<br/> arbitrary scale factor \(s\) 에 consistent한<br/> <code class="language-plaintext highlighter-rouge">scaled depth를 encode</code>하도록 update <ul> <li>\(F [u] += Att(q, k_{l}, v_{l})\)<br/> where \(+=\) : skip-connection<br/> where \(Att\) : softmax attention</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Per-Image Self-Attention</code> :<br/> Cross-Attention 끝난 뒤 마지막에 Per-Image Self-Attention 수행하여<br/> propagate scaled depth estimates<br/> to parts of the image feature maps<br/> that may not have any epipolar correspondences <ul> <li> \[F += SelfAttention(F)\] </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/9-480.webp 480w,/assets/img/2024-10-25-pixelSplat/9-800.webp 800w,/assets/img/2024-10-25-pixelSplat/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="gaussian-parameter-prediction">Gaussian Parameter Prediction</h3> <ul> <li>앞선 과정들 덕분에<br/> scale-aware feature map \(F, \tilde F\) 를 이용하여<br/> Gaussian param. \(g_{k} = (\mu_{k}, \Sigma_{k}, \alpha_{k}, S_{k})\) 를 예측 <ul> <li>2D image 상의 <code class="language-plaintext highlighter-rouge">모든 각 pixel은 3D 상의 point에 대응</code>되어<br/> 최종적인 Gaussian primitives set은<br/> just union of each image</li> </ul> </li> <li>3D position Prediction : <ul> <li>방법 1) Baseline :<br/> predict point estimate of 3D position \(\mu\) <ul> <li>\(\boldsymbol \mu = \boldsymbol o + d_{u} \boldsymbol d\)<br/> where \(u\) : 2D pixel coordiante<br/> where \(\boldsymbol d = R K^{-1} [u, 1]^{T}\) : ray direction<br/> where \(d_{u} = g_{\theta}(F [u])\) : depth obtained by neural network</li> <li>문제 :<br/> depth 자체를 neural network로 추정하는 건 local minima 문제 발생하기 쉬움</li> </ul> </li> <li>방법 2) 본 논문 방식 :<br/> predict <code class="language-plaintext highlighter-rouge">probability density</code> of 3D position \(\mu\) <ul> <li>핵심 :<br/> neural network로<br/> depth 자체를 예측하는 게 아니라<br/> differentiable probability distribution of likelihood of depth along ray를 예측</li> <li>Step 1)<br/> depth를 \(Z\)-bins로 discretize<br/> \(b_{z} = ((1 - \frac{z}{Z})(\frac{1}{d_{near}} - \frac{1}{d_{far}}) + \frac{1}{d_{far}})^{-1} \in [d_{near}, d_{far}]\)<br/> for \(z \in [0, Z]\) : depth index</li> <li>Step 2)<br/> discrete probability \(\phi\) 로부터 index \(z\) 를 sampling<br/> \(z \sim p_{\phi}(z)\)</li> <li>Step 3)<br/> ray를 쏴서(unproject) Gaussian mean \(\mu\) 계산<br/> \(\boldsymbol \mu = \boldsymbol o + (b_{z} + \delta_{z}) \boldsymbol d\)<br/> where \(\phi\) : depth(\(z\)) probability obtained by neural network<br/> where \(\delta_{z}\) : depth offset obtained by neural network</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/6-480.webp 480w,/assets/img/2024-10-25-pixelSplat/6-800.webp 800w,/assets/img/2024-10-25-pixelSplat/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Gaussian Parameter Prediction : <ul> <li>scale-aware feature map \(F, \tilde F\) 과 neural network \(f\) 를 이용하여<br/> \(\phi, \delta, \Sigma, S = f(F [u])\)<br/> where \(\phi, \delta, \Sigma, S\) : depth probability, depth offset, covariance, spherical harmonics coeff. <ul> <li><code class="language-plaintext highlighter-rouge">3D position</code>(mean) :<br/> \(\phi, \delta\) 이용해서<br/> \(\boldsymbol \mu = \boldsymbol o + (b_{z} + \delta_{z}) \boldsymbol d\)</li> <li><code class="language-plaintext highlighter-rouge">Covariance</code> :<br/> \(\Sigma\)</li> <li><code class="language-plaintext highlighter-rouge">Spherical Harmonics Coeff.</code> :<br/> \(S\)</li> <li><code class="language-plaintext highlighter-rouge">Opacity</code> :<br/> \(\phi\) 이용해서<br/> \(\alpha = \phi_{z}\)<br/> \(=\) probability of sampled depth \(z\)<br/> (so that we make sampling differentiable)</li> </ul> </li> <li>각 pixel마다 3DGS point에 대응되므로<br/> pixel-aligned Gaussians라고 부름</li> </ul> </li> </ul> <h3 id="experiments">Experiments</h3> <ul> <li>Setup : <ul> <li>Dataset :<br/> camera pose is computed by SfM <ul> <li>RealEstate 10k</li> <li>ACID</li> </ul> </li> <li>Baseline : <ul> <li>pixelNeRF</li> <li>GPNR</li> <li>Method of Du et al.</li> </ul> </li> <li>Metric : <ul> <li>PSNR</li> <li>SSIM</li> <li>LPIPS</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/7-480.webp 480w,/assets/img/2024-10-25-pixelSplat/7-800.webp 800w,/assets/img/2024-10-25-pixelSplat/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Result : <ul> <li>performance much better</li> <li>inference time faster</li> <li>less memory per ray</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/8-480.webp 480w,/assets/img/2024-10-25-pixelSplat/8-800.webp 800w,/assets/img/2024-10-25-pixelSplat/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/10-480.webp 480w,/assets/img/2024-10-25-pixelSplat/10-800.webp 800w,/assets/img/2024-10-25-pixelSplat/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-25-pixelSplat/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Ablation Study </div> <ul> <li>Ablation Study : <ul> <li>Epipolar Encoder : Epipolar Sampling and Epipolar Attention</li> <li>Depth Encoding : freq.-based positional encoding \(\gamma(\tilde d_{\tilde u_{l}})\)</li> <li>Probabilistic Sampling : depth index \(z \sim p_{\phi}(z)\)</li> <li>Depth Regularization : <code class="language-plaintext highlighter-rouge">???</code></li> </ul> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="3DGS"/><category term="image"/><category term="pair"/><category term="scalable"/><summary type="html"><![CDATA[3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction (CVPR 2024)]]></summary></entry><entry><title type="html">3DGS Code Review</title><link href="https://semyeong-yu.github.io/blog/2024/3DGScode/" rel="alternate" type="text/html" title="3DGS Code Review"/><published>2024-10-11T12:00:00+00:00</published><updated>2024-10-11T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/3DGScode</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/3DGScode/"><![CDATA[<h2 id="3dgs-code-review">3DGS Code Review</h2> <blockquote> <p>code :<br/> <a href="https://github.com/graphdeco-inria/gaussian-splatting">https://github.com/graphdeco-inria/gaussian-splatting</a><br/> reference :<br/> https://charlieppark.kr<br/> NeRF and 3DGS Study</p> </blockquote> <h3 id="trainpy">train.py</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-11-3DGScode/1-480.webp 480w,/assets/img/2024-10-11-3DGScode/1-800.webp 800w,/assets/img/2024-10-11-3DGScode/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-11-3DGScode/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 1. train.py Algorithm </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gaussians</span> <span class="o">=</span> <span class="nc">GaussianModel</span><span class="p">(</span><span class="n">dataset</span><span class="p">.</span><span class="n">sh_degree</span><span class="p">)</span>
<span class="n">scene</span> <span class="o">=</span> <span class="nc">Scene</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">gaussians</span><span class="p">)</span>
</code></pre></div></div> <h3 id="gaussian-initialize">Gaussian Initialize</h3> <ul> <li>Fig 1.의 빨간 박스 : <ul> <li>pcd로부터 gaussians를 initialize</li> </ul> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Scene</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(...):</span>
    <span class="bp">...</span>
    <span class="n">self</span><span class="p">.</span><span class="n">gaussians</span><span class="p">.</span><span class="nf">create_from_pcd</span><span class="p">(</span><span class="n">scene_info</span><span class="p">.</span><span class="n">point_cloud</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">cameras_extent</span><span class="p">)</span>  
</code></pre></div></div> <ul> <li>\(\text{class Scene.__init__()}\) : <ul> <li>scene_info :<br/> Colmap 또는 Blender의 pcd, camera info.를 받아옴 <ul> <li>pcd : scene_info.point_cloud</li> <li>camera : scene_info.train_cameras, scene_info.test_cameras</li> </ul> </li> <li>self.gaussians.create_from_pcd() :<br/> pcd로부터 gaussians를 initialize</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-11-3DGScode/9-480.webp 480w,/assets/img/2024-10-11-3DGScode/9-800.webp 800w,/assets/img/2024-10-11-3DGScode/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-11-3DGScode/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> class Scene.__init__() </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-11-3DGScode/10-480.webp 480w,/assets/img/2024-10-11-3DGScode/10-800.webp 800w,/assets/img/2024-10-11-3DGScode/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-11-3DGScode/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> sceneLoadTypeCallbacks &gt; readColmapSceneInfo </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-11-3DGScode/2-480.webp 480w,/assets/img/2024-10-11-3DGScode/2-800.webp 800w,/assets/img/2024-10-11-3DGScode/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-11-3DGScode/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> GaussianModel.create_from_pcd() </div> <h3 id="densification">Densification</h3> <ul> <li>Fig 1.의 초록 박스 : <ul> <li>densification (clone and split)</li> <li>class GaussianModel densify_and_prune()</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-11-3DGScode/3-480.webp 480w,/assets/img/2024-10-11-3DGScode/3-800.webp 800w,/assets/img/2024-10-11-3DGScode/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-11-3DGScode/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> train.py Densification </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-11-3DGScode/4-480.webp 480w,/assets/img/2024-10-11-3DGScode/4-800.webp 800w,/assets/img/2024-10-11-3DGScode/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-11-3DGScode/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> GaussianModel.add_densification_stats() </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-11-3DGScode/5-480.webp 480w,/assets/img/2024-10-11-3DGScode/5-800.webp 800w,/assets/img/2024-10-11-3DGScode/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-11-3DGScode/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> GaussianModel.densify_and_prune() </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-11-3DGScode/6-480.webp 480w,/assets/img/2024-10-11-3DGScode/6-800.webp 800w,/assets/img/2024-10-11-3DGScode/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-11-3DGScode/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> GaussianModel.densify_and_clone(), GaussianModel.densify_and_split() </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-11-3DGScode/7-480.webp 480w,/assets/img/2024-10-11-3DGScode/7-800.webp 800w,/assets/img/2024-10-11-3DGScode/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-11-3DGScode/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> GaussianModel.densification_postfix() </div> <h3 id="gs-rasterize">GS Rasterize</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-11-3DGScode/8-480.webp 480w,/assets/img/2024-10-11-3DGScode/8-800.webp 800w,/assets/img/2024-10-11-3DGScode/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-11-3DGScode/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 2. GS Rasterize Algorithm </div> <ul> <li>Fig 2.의 노란 박스 : <ul> <li>cuda로 구현</li> <li></li> </ul> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="3DGS"/><category term="code"/><summary type="html"><![CDATA[3D Gaussian Splatting for Real-Time Radiance Field Rendering (SIGGRAPH 2023)]]></summary></entry><entry><title type="html">WandB</title><link href="https://semyeong-yu.github.io/blog/2024/WandB/" rel="alternate" type="text/html" title="WandB"/><published>2024-10-09T12:00:00+00:00</published><updated>2024-10-09T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/WandB</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/WandB/"><![CDATA[<h2 id="wandb-tutorial">WandB Tutorial</h2> <ul> <li>WandB Platform <ul> <li>Experiments : experiment tracking</li> <li>Sweeps : model optimization</li> <li>Artifacts : dataset versioning</li> <li>Tables : model evaluation</li> <li>Reports : collaborative analysis</li> </ul> </li> </ul> <h3 id="experiment">Experiment</h3> <ul> <li>wandb.init() : <ul> <li>run별로 구분하기 위해<br/> name 또는 tags 설정</li> <li>이미 끝난 run을 resume하고 싶으면<br/> 해당 run의 id 설정</li> </ul> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">wandb</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">project</span><span class="o">=</span><span class="sh">"</span><span class="s">ddpm</span><span class="sh">"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">exp1</span><span class="sh">"</span><span class="p">,</span> <span class="n">entity</span><span class="o">=</span><span class="sh">"</span><span class="s">semyu0102-viclab</span><span class="sh">"</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">__dict__</span><span class="p">,</span> <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">batch=32</span><span class="sh">"</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="s">lr=</span><span class="si">{</span><span class="n">wandb</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">lr</span><span class="si">}</span><span class="sh">"</span><span class="p">],</span> <span class="nb">id</span><span class="o">=</span><span class="p">...)</span>
<span class="c1"># wandb.run.name = wandb.run.id
# wandb.run.name = "exp1"
# wandb.run.save()
# wandb.config.update({"epochs":4, "batch_size":32})
</span></code></pre></div></div> <ul> <li>wandb.watch() :<br/> hook into model’s grad. or param. <ul> <li>log : ‘gradients’, ‘parameters’, ‘all’, ‘None’</li> <li>log_freq=N : N batch마다 gradients 또는 parameters 기록</li> </ul> </li> </ul> <pre><code class="language-Python">wandb.watch(model, criterion=criterion, log='all', log_freq=100)
</code></pre> <ul> <li>wandb.log() :<br/> step을 가로축으로 하여 <strong>dict</strong> 기록</li> </ul> <pre><code class="language-Python">wandb.log(__dict__, step=epoch)
</code></pre> <ul> <li>wandb.finish() :<br/> finish the run</li> </ul> <pre><code class="language-Python">wandb.finish()
</code></pre> <ul> <li>모델 저장 :</li> </ul> <pre><code class="language-Python">artifact = wandb.Artifact('model', type='model')
artifact.add_file(f"model/resnet50.pt")
wandb.log_artifact(artifact)
</code></pre> <ul> <li>이미지 저장 :</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">images</span><span class="o">=</span><span class="p">[]</span>
<span class="n">images</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">wandb</span><span class="p">.</span><span class="nc">Image</span><span class="p">(</span><span class="n">img</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">caption</span><span class="o">=</span><span class="sh">"</span><span class="s">Pred: {} Truth: {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">pred</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">item</span><span class="p">(),</span> <span class="n">target</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span>
<span class="c1"># img[0] : np array or PIL or matplotlib.figure.Figure ...
</span><span class="n">wandb</span><span class="p">.</span><span class="nf">log</span><span class="p">({</span><span class="sh">"</span><span class="s">Image</span><span class="sh">"</span><span class="p">:</span> <span class="n">images</span><span class="p">})</span> <span class="c1"># 100여개 정도까지가 한계
</span></code></pre></div></div> <ul> <li>히스토그램 저장 :</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">wandb</span><span class="p">.</span><span class="nf">log</span><span class="p">({</span><span class="sh">"</span><span class="s">gradients</span><span class="sh">"</span><span class="p">:</span> <span class="n">wandb</span><span class="p">.</span><span class="nc">Histogram</span><span class="p">(</span><span class="n">sequence</span><span class="p">)})</span> 
<span class="c1"># sequence : np array ...
</span></code></pre></div></div> <h3 id="sweep">Sweep</h3> <ul> <li>Sweep :<br/> hyperparam. tuning에 쓰임</li> </ul> <pre><code class="language-Python">wandb sweep sweep.yaml # sweep_id 출력
wandb agent &lt;sweep_id&gt; --count &lt;sweep 횟수&gt;
</code></pre> <ul> <li>sweep.yaml : <ul> <li>program :<br/> sweep 실행할 code file</li> <li>method : <ul> <li>bayes : bayesian 최적화를 통해 이전 실험 결과를 기반으로 효율적 탐색</li> <li>grid : 모든 조합 탐색</li> <li>random : random 선택</li> </ul> </li> <li>name, description</li> <li>project :<br/> sweep할 wandb project명</li> <li>entity :<br/> wandb 계정 유저명</li> <li>metric :<br/> hyperparam. tuning을 통해 이뤄야 할 목표 <ul> <li>name :<br/> wandb.log() 안에 넣은 이름으로 설정해야 함</li> </ul> </li> <li>parameters :<br/> tuning할 hyperparam. <ul> <li>min, max : 해당 범위 내에서 튜닝</li> <li>values : 해당 list 선택지 중에서 튜닝</li> <li>value : single 값으로, 튜닝 안 함</li> <li>distribution: normal, mu, sigma : 해당 \(N(\text{mu}, \text{sigma})\) 분포로 sampling한 값 중에 튜닝</li> </ul> </li> <li>early_terminate :<br/> 학습이 비효율적으로 진행될 경우 early stopping으로 자원 절약 <ul> <li>type :<br/> early stopping 알고리즘 선택 <ul> <li>hyperband</li> <li>median</li> </ul> </li> <li>min_iter, max_iter : 최소, 최대 반복 횟수 범위 안에서 early stopping</li> </ul> </li> <li>command :<br/> code file 실행할 때의 commands <ul> <li>${…} : parameters에서 정의한 hyperparam. 값을 동적으로 대입</li> </ul> </li> </ul> </li> </ul> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">program</span><span class="pi">:</span> <span class="s">main.py</span>
<span class="na">method</span><span class="pi">:</span> <span class="s">bayes</span>
<span class="na">name</span><span class="pi">:</span> <span class="s">ddpm-sweep</span>
<span class="na">description</span><span class="pi">:</span> <span class="s">test ddpm sweep</span>
<span class="na">project</span><span class="pi">:</span> <span class="s">ddpm</span>
<span class="na">entity</span><span class="pi">:</span> <span class="s">semyu0102-viclab</span>
<span class="na">meric</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">val_loss</span>
  <span class="na">goal</span><span class="pi">:</span> <span class="s">minimize</span>
<span class="na">parameters</span><span class="pi">:</span>
  <span class="na">learning_rate</span><span class="pi">:</span> 
    <span class="na">min</span><span class="pi">:</span> <span class="m">0.0001</span>
    <span class="na">max</span><span class="pi">:</span> <span class="m">0.1</span>
  <span class="na">optimizer</span><span class="pi">:</span>
    <span class="na">values</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">adam"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">sgd"</span><span class="pi">]</span>
  <span class="na">epochs</span><span class="pi">:</span>
    <span class="na">value</span><span class="pi">:</span> <span class="m">5</span>
  <span class="na">parameter1</span><span class="pi">:</span>
    <span class="na">distribution</span><span class="pi">:</span> <span class="s">normal</span>
    <span class="na">mu</span><span class="pi">:</span> <span class="m">100</span>
    <span class="na">sigma</span><span class="pi">:</span> <span class="m">10</span>
<span class="na">early_terminate</span><span class="pi">:</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">hyperband</span>
  <span class="na">min_iter</span><span class="pi">:</span> <span class="m">3</span>
<span class="na">command</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">python</span>
  <span class="pi">-</span> <span class="s">train.py</span>
  <span class="pi">-</span> <span class="s">--learning_rate=${learning_rate}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="others"/><category term="wandb"/><category term="log"/><category term="sweep"/><summary type="html"><![CDATA[WandB Tutorial]]></summary></entry><entry><title type="html">NeRF in the Wild</title><link href="https://semyeong-yu.github.io/blog/2024/NeRFW/" rel="alternate" type="text/html" title="NeRF in the Wild"/><published>2024-10-08T12:00:00+00:00</published><updated>2024-10-08T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/NeRFW</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/NeRFW/"><![CDATA[<h2 id="nerf-in-the-wild---neural-radiance-fields-for-unconstrained-photo-collections-cvpr-2021">NeRF in the Wild - Neural Radiance Fields for Unconstrained Photo Collections (CVPR 2021)</h2> <h4 id="ricardo-martin-brualla-noha-radwan-mehdi-s-m-sajjadi-jonathan-t-barron-alexey-dosovitskiy-daniel-duckworth">Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, Daniel Duckworth</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2008.02268">https://arxiv.org/abs/2008.02268</a><br/> project website :<br/> <a href="https://nerf-w.github.io/">https://nerf-w.github.io/</a><br/> code :<br/> <a href="https://github.com/kwea123/nerf_pl/tree/nerfw?tab=readme-ov-file">https://github.com/kwea123/nerf_pl/tree/nerfw?tab=readme-ov-file</a><br/> <a href="https://github.com/rover-xingyu/Ha-NeRF">https://github.com/rover-xingyu/Ha-NeRF</a><br/> youtube video :<br/> <a href="https://www.youtube.com/watch?v=mRAKVQj5LRA&amp;t=254s">https://www.youtube.com/watch?v=mRAKVQj5LRA&amp;t=254s</a><br/> reference :<br/> NeRF and 3DGS Study</p> </blockquote> <h3 id="introduction">Introduction</h3> <ul> <li>Issue : <ul> <li>Q : image 상의 동적인 물체를 어떻게 없앨 수 있을까?</li> <li>A : <code class="language-plaintext highlighter-rouge">Static Network</code>와 <code class="language-plaintext highlighter-rouge">Transient Network</code>를 분리한 뒤<br/> \(c, \sigma\) 에 대한 <code class="language-plaintext highlighter-rouge">Uncertainty</code>를 측정하자!</li> </ul> </li> <li>Contribution : <ul> <li>Latent Appearance Embedding in Static Network :<br/> 각 image의 광도 반영</li> <li>Latent Transient Embedding in Transient Network :<br/> 동적인 물체 구별</li> <li>Loss w. Uncertainty and Transient density</li> </ul> </li> <li>결과 : <ul> <li>Latent Embedding Vector 변화로 Appearance에 변화 줄 수 있음</li> <li>일시적으로 찍힌 동적인 물체를 제거할 수 있음</li> </ul> </li> </ul> <h3 id="architecture">Architecture</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/8-480.webp 480w,/assets/img/2024-10-08-NeRFW/8-800.webp 800w,/assets/img/2024-10-08-NeRFW/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="architecture---static-network">Architecture - Static Network</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/1-480.webp 480w,/assets/img/2024-10-08-NeRFW/1-800.webp 800w,/assets/img/2024-10-08-NeRFW/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Static Network :<br/> 우리가 Novel View Synthesis 하고 싶어하는 대상을 다룸</p> </li> <li>View Direction과 함께 Appearance Embedding Vector 넣어준다는 것 말고는 기존 NeRF 구조와 same <ul> <li>\(\gamma_{x}(r(t)) \rightarrow \sigma_{i}(t)\) (3d shape)</li> <li>\(\gamma_{x}(r(t)), \gamma_{d}(d), l_{i}^{(a)} \rightarrow c_{i}(t)\) (view-dependent 3d color)</li> </ul> </li> <li>Appearance Embedding Vector : <ul> <li>image의 embedding vector (각 image의 광도 반영)</li> <li>random initialization (learnable)</li> <li>control처럼 쓰일 수 있음<br/> Embedding Vector 수정하여 Appearance(스타일)에 변화 줄 수 있음</li> <li>training dataset에 대해 \(l_{i}^{(a)}\) 를 학습하므로<br/> test할 때는 target image에 적합할 만한 Embedding Vector 골라서 사용</li> </ul> </li> </ul> <h3 id="architecture---transient-network">Architecture - Transient Network</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/2-480.webp 480w,/assets/img/2024-10-08-NeRFW/2-800.webp 800w,/assets/img/2024-10-08-NeRFW/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Transient Network : <ul> <li>우리가 Novel View Synthesis 하고 싶어하는 대상이 아닌,<br/> 동적인 물체를 다룸 (제거하기 위해)</li> <li>Bayesian learning framework를 적용하여 <code class="language-plaintext highlighter-rouge">???</code> Uncertainty를 모델링</li> </ul> </li> <li>Transient Embedding \(l_{i}^{(T)}\) 을 넣어서 동적인 물체의 transient density를 얻은 뒤 제거 가능 <ul> <li> \[\gamma_{x}(r(t)), l_{i}^{(T)} \rightarrow c_{i}^{(T)}(t), \sigma_{i}^{(T)}(t), \tilde \beta_{i}(t)\] </li> <li> \[\beta_{i}(t) = \beta_{min} + \text{log}(1+\text{exp}(\tilde \beta_{i}(t)))\] </li> </ul> </li> </ul> <h3 id="volume-rendering">Volume Rendering</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/3-480.webp 480w,/assets/img/2024-10-08-NeRFW/3-800.webp 800w,/assets/img/2024-10-08-NeRFW/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Training : (a) Static, (b) Transient 모두 사용하여 아래의 rendering 식으로 (c) Composite 만들고, 이를 (d) GT와 비교하여 학습<br/> \(\hat C_{i} (r) = \sum_{k=1}^K T_{i}(t_k)(\alpha(\sigma_{i}(t_k) \delta_{k}) c_{i}(t_k) + \alpha(\sigma_{i}^{(T)}(t_k) \delta_{k}) c_{i}^{(T)}(t_k))\)<br/> where \(T_{i}(t_k) = \text{exp}(-\sum_{k^{'}=1}^{k-1}(\sigma_{i}(t_{k^{'}}) + \delta_{i}^{(T)}(t_{k^{'}}))\delta_{k^{'}})\)</p> </li> <li> <p>Test : (a) Static만 사용</p> </li> </ul> <h3 id="optimization">Optimization</h3> <ul> <li>Coarse Model :<br/> (기존 NeRF와 유사하게) static network만 사용해서 Appearance Embedding Vector를 학습<br/> \(L = \sum_{ij} L_{c}(r_{ij})\) <ul> <li>\(L_{c}(r_{ij}) = \frac{1}{2} \| C(r_{ij}) - \hat C^{c}(r_{ij}) \|^2\)<br/> where \(\hat C^{c}(r_{ij}) = \sum_{k=1}^K T_{i}(t_k)(\alpha(\sigma_{i}(t_k) \delta_{k}) c_{i}(t_k))\)<br/> (static network만 사용)</li> </ul> </li> <li>Fine Model :<br/> Coarse Model의 weight를 바탕으로 fine-sampling<br/> static, transient network 모두 사용해서 학습<br/> \(L = \sum_{ij} L_{f}(r_{ij}) + L_{c}(r_{ij})\) <ul> <li>\(L_{c}(r_{ij}) = \frac{1}{2} \| C(r_{ij}) - \hat C^{c}(r_{ij}) \|^2\)<br/> where \(\hat C^{c}(r_{ij}) = \sum_{k=1}^K T_{i}(t_k)(\alpha(\sigma_{i}(t_k) \delta_{k}) c_{i}(t_k))\)<br/> (static network만 사용)</li> <li>\(L_{f}(r_{ij}) = \frac{\| C(r_{ij}) - \hat C^{f}(r_{ij}) \|^2}{2\beta(r)^2} + \frac{\text{log} \beta(r)^2}{2} + \frac{\lambda}{K} \sum_{k=1}^K \sigma^{(T)}(t_k)\)<br/> where \(\hat C^{f}(r_{ij}) = \sum_{k=1}^K T_{i}(t_k)(\alpha(\sigma_{i}(t_k) \delta_{k}) c_{i}(t_k) + \alpha(\sigma_{i}^{(T)}(t_k) \delta_{k}) c_{i}^{(T)}(t_k))\)<br/> (static, trasient network 모두 사용) <ul> <li>1번째 term : <code class="language-plaintext highlighter-rouge">recon. loss term</code><br/> Uncertainty \(\beta(r)\) 가 크면 recon. loss 영향력 작아짐<br/> (동적인 물체가 있어서 불확실한 부분은 loss 및 gradient 작게)</li> <li>2번째 term : <code class="language-plaintext highlighter-rouge">regularization term</code><br/> Uncertainty \(\beta(r)\) 가 너무 커지지 않도록 regularize</li> <li>3번째 term : <code class="language-plaintext highlighter-rouge">regularization term</code><br/> transient density \(\sigma^{(T)}\) 가 너무 커지지 않도록 regularize</li> </ul> </li> </ul> </li> </ul> <h3 id="results">Results</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/4-480.webp 480w,/assets/img/2024-10-08-NeRFW/4-800.webp 800w,/assets/img/2024-10-08-NeRFW/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/5-480.webp 480w,/assets/img/2024-10-08-NeRFW/5-800.webp 800w,/assets/img/2024-10-08-NeRFW/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="conclusion">Conclusion</h3> <ul> <li>동적이고 조명이 바뀌는 상황에서 촬영된 image dataset으로도 neural rendering 가능 <ul> <li>Appearance Embedding : 각 image의 광도를 반영</li> <li>Transient Embedding : target static object를 가리는 동적인 물체 구별하여 제거</li> </ul> </li> </ul> <h3 id="limitation">Limitation</h3> <ul> <li>training 개수, camera calibration error에 민감</li> </ul> <h3 id="code">Code</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/6-480.webp 480w,/assets/img/2024-10-08-NeRFW/6-800.webp 800w,/assets/img/2024-10-08-NeRFW/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/7-480.webp 480w,/assets/img/2024-10-08-NeRFW/7-800.webp 800w,/assets/img/2024-10-08-NeRFW/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="question">Question</h3> <ul> <li> <p>Q1 :<br/> \(\hat C_{i} (r) = \sum_{k=1}^K T_{i}(t_k)(\alpha(\sigma_{i}(t_k) \delta_{k}) c_{i}(t_k) + \alpha(\sigma_{i}^{(T)}(t_k) \delta_{k}) c_{i}^{(T)}(t_k))\)<br/> 위의 volume rendering 식을 보면 static network의 color, density와 transient network의 color, density가 함께 하나의 pixel color로 rendering되어 동시에 backpropagation되는데<br/> 어떻게 두 network 중에서 하필 transient network의 color, density가 동적인 물체를 구별하는 역할을 수행할 수 있느냐</p> </li> <li> <p>A1 :<br/> Coarse Model(static network 사용)과 Fine Model(static, transient network 모두 사용)을 two-stage로 분리해서 학습하여<br/> transient embedding을 넣은 transient network가 동적인 물체를 식별하는 역할을 잘 수행할 수 있을 것이다</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-08-NeRFW/8-480.webp 480w,/assets/img/2024-10-08-NeRFW/8-800.webp 800w,/assets/img/2024-10-08-NeRFW/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-08-NeRFW/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="static"/><category term="transient"/><category term="uncertainty"/><summary type="html"><![CDATA[Neural Radiance Fields for Unconstrained Photo Collections (CVPR 2021)]]></summary></entry><entry><title type="html">Neuralangelo</title><link href="https://semyeong-yu.github.io/blog/2024/Neuralangelo/" rel="alternate" type="text/html" title="Neuralangelo"/><published>2024-09-30T12:00:00+00:00</published><updated>2024-09-30T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Neuralangelo</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Neuralangelo/"><![CDATA[<h2 id="neuralangelo---high-fidelity-neural-surface-reconstruction-cvpr-2023">Neuralangelo - High-Fidelity Neural Surface Reconstruction (CVPR 2023)</h2> <h4 id="zhaoshuo-li-thomas-müller-alex-evans-russell-h-taylor-mathias-unberath-ming-yu-liu-chen-hsuan-lin">Zhaoshuo Li, Thomas Müller, Alex Evans, Russell H. Taylor, Mathias Unberath, Ming-Yu Liu, Chen-Hsuan Lin</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/pdf/2306.03092">https://arxiv.org/pdf/2306.03092</a><br/> project website :<br/> <a href="https://research.nvidia.com/labs/dir/neuralangelo/">https://research.nvidia.com/labs/dir/neuralangelo/</a><br/> code :<br/> <a href="https://github.com/nvlabs/neuralangelo">https://github.com/nvlabs/neuralangelo</a><br/> reference :<br/> NeRF and 3DGS Study</p> </blockquote> <h3 id="abstract">Abstract</h3> <ul> <li> <p>multi-resolution hash grid representation<br/> with SDF-based volume rendering<br/> (3D surface recon.)</p> </li> <li> <p>no need for auxiliary data like segmentation or depth</p> </li> <li> <p>Novelty :</p> <ul> <li>numerical gradient (backpropagation locality 문제 해결)</li> <li>coarse-to-fine (점점 high resol.)</li> </ul> </li> </ul> <h3 id="related-works">Related Works</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-30-Neuralangelo/1-480.webp 480w,/assets/img/2024-09-30-Neuralangelo/1-800.webp 800w,/assets/img/2024-09-30-Neuralangelo/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-30-Neuralangelo/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Instant NGP <d-cite key="InstantNGP">[1]</d-cite> <a href="https://nvlabs.github.io/instant-ngp/">Link</a> :<br/> 모든 좌표(pixel) 각각에 대해 MLP output을 구하면 연산량이 너무 크므로<br/> 연산량 감소 및 speed-up 위해<br/> <code class="language-plaintext highlighter-rouge">Hash Grid</code>(연산량 감소)와 <code class="language-plaintext highlighter-rouge">Linear Interpolation</code>(continuous 보장)을 이용한<br/> 좌표 encoding 기법 제시 <ul> <li>STEP 1)<br/> \(d\)-dim. scene일 때<br/> input 좌표 \(x\) 가 주어졌을 때<br/> grid level별로 주위 \(2^d\)-개 좌표 선택 <ul> <li>multi-resolution (grid-level \(l\)) :<br/> \(N_l = \lfloor N_{min} \cdot b^l \rfloor\)<br/> where \(b = e^{\frac{\text{ln} N_{max} - \text{ln} N_{min}}{L-1}}\)<br/> MLP size가 작더라도 multi-resol. 덕분에 high approx. power 가짐</li> <li>주위 좌표 선택 :<br/> \(N_l\) 만큼 scale된 좌표 계산<br/> \(\lfloor x_l \rfloor = \lfloor x \cdot N_l \rfloor\)<br/> \(\lceil x_l \rceil = \lceil x \cdot N_l \rceil\)</li> </ul> </li> <li>STEP 2)<br/> 선택한 각 좌표에 대해 HashKey를 계산한 뒤 HashTable에서 Value 읽어옴 <ul> <li>HashKey :<br/> grid-level 마다 1개씩 HashTable이 정의되며<br/> Spatial Hash Function(2003)에 의해<br/> HashKey \(h(x) = (\text{XOR}_{i=1}^{d} x_i \pi_{i}) \text{mod} T \in [0, T-1]\)<br/> where \(d\) : dim., \(\pi\) : dim.마다 임의로 정해둔 constant, \(T\) : Hash Table Size</li> <li>HashValue :<br/> \(T \times F\) 의 HashTable로부터 \(F\)-dim. feature vector인 HashValue를 얻음</li> </ul> </li> <li>STEP 3)<br/> 주위 좌표까지의 거리를 기반으로<br/> HashValue들을 Linear Interpolation(weighted sum)하여<br/> grid-level 별로 1개의 feature vector로 만듬</li> <li>STEP 4)<br/> 각 grid-level 별 feature vectors와 auxiliary 값(e.g. view direction)을 concat하여<br/> 최종 feature vector 만듬</li> <li>STEP 5)<br/> shallow MLP 통과</li> <li>STEP 6)<br/> Backpropagation :<br/> MLP weight와 Hash Table의 Value(\(F\)-dim. feature vector) 업데이트</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-30-Neuralangelo/2-480.webp 480w,/assets/img/2024-09-30-Neuralangelo/2-800.webp 800w,/assets/img/2024-09-30-Neuralangelo/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-30-Neuralangelo/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="numerical-gradient">Numerical Gradient</h3> <ul> <li>Gradient : <ul> <li>analytical gradient : \(\nabla f(x_i) = \frac{\partial f(x_i)}{\partial x_i}\)</li> <li>numerical gradient : \(\text{lim}_{\epsilon_{x} \rightarrow 0} \frac{f(x_i + \epsilon_{x}) - f(x_i - \epsilon_{x})}{2\epsilon_{x}}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-30-Neuralangelo/4-480.webp 480w,/assets/img/2024-09-30-Neuralangelo/4-800.webp 800w,/assets/img/2024-09-30-Neuralangelo/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-30-Neuralangelo/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Analytical Graident </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-30-Neuralangelo/5-480.webp 480w,/assets/img/2024-09-30-Neuralangelo/5-800.webp 800w,/assets/img/2024-09-30-Neuralangelo/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-30-Neuralangelo/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Numerical Gradient </div> <ul> <li>Instant NGP <d-cite key="InstantNGP">[1]</d-cite> 에서처럼<br/> input coordinate encode하기 위해<br/> multi-resolution hash grid representation 사용 <ul> <li>문제 :<br/> SDF-based volume rendering에 multi-resolution hash grid를 직접적으로 적용하면<br/> large smooth regions의 surface에 noise 및 hole이 생김</li> <li>이유 : <ul> <li>surface recon.에서 RGB(color) 및 SDF(geometry)를 MLP output으로 얻는데<br/> surface regularization loss를 구할 때 higher-order derivatives of SDF 계산해야 함 <ul> <li>first-order derivative of SDF \(f(x_i)\) :<br/> Eikonal constraints on the surface normals 계산<br/> \(L_{eik} = \frac{1}{N} \sum_{i=1}^N (\| \nabla f(x_i) \| - 1)^2\)</li> <li>second-order derivative of SDF \(f(x_i)\) :<br/> surface curvatures 계산<br/> \(L_{curv} = \frac{1}{N} \sum_{i=1}^N | \nabla^{2} f(x_i) |\)</li> </ul> </li> <li>그리고 이러한 SDF의 higher-order derivatives 계산하기 위해<br/> <code class="language-plaintext highlighter-rouge">analytical gradient</code> \(\nabla f(x_i) = \frac{\partial f(x_i)}{\partial x_i}\) 사용</li> <li>근데, analytical gradient 사용하면<br/> <code class="language-plaintext highlighter-rouge">only backpropagate to local cell</code>의 HashValues<br/> (locality 문제 발생!)</li> <li>특히 recon.할 surface가 multiple grid cells에 걸쳐 있을 경우<br/> analytical gradient를 사용하면<br/> adjacent cells는 업데이트 안 됨</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-30-Neuralangelo/3-480.webp 480w,/assets/img/2024-09-30-Neuralangelo/3-800.webp 800w,/assets/img/2024-09-30-Neuralangelo/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-30-Neuralangelo/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig a. Analytical Gradient from local cell </div> <ul> <li>Instant NGP <d-cite key="InstantNGP">[1]</d-cite> 에서처럼<br/> input coordinate encode하기 위해<br/> multi-resolution hash grid representation 사용 <ul> <li>해결 : <ul> <li>SDF의 higher-order derivatives 계산하기 위해<br/> numerical gradient \(\text{lim}_{\epsilon \rightarrow 0} \frac{f(x_i + \epsilon) - f(x_i - \epsilon)}{2\epsilon}\) 사용</li> <li><code class="language-plaintext highlighter-rouge">forward pass</code>에서 rendering하기 위해 (<code class="language-plaintext highlighter-rouge">recon. loss</code> 구하기 위해) SDF 계산할 때는<br/> sampled point 1개만 사용</li> <li><code class="language-plaintext highlighter-rouge">regularization loss</code> 구하기 위해 SDF의 higher-order derivatives 계산할 때는<br/> adjacent cells의 SDF까지 이용하는 numerical gradient를 사용함으로써<br/> <code class="language-plaintext highlighter-rouge">backward pass</code>에서 <code class="language-plaintext highlighter-rouge">backpropagate to adjacent cells</code></li> <li>adjacent 6개의 cells \(x_i \pm \epsilon\) 각각에 대해 trilinear sampling으로 SDF 값 계산하고<br/> 그 차이를 이용해서 <code class="language-plaintext highlighter-rouge">numerical gradient</code> 계산<br/> 이는 backward pass에 이용</li> <li>local cell \(x_i\) 로만 backpropagate하는 게 아니라<br/> 주위 6개의 cells \(x_i \pm \epsilon\) 으로 backpropagate하므로<br/> <code class="language-plaintext highlighter-rouge">smoothing</code> on SDF 역할 수행</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-30-Neuralangelo/6-480.webp 480w,/assets/img/2024-09-30-Neuralangelo/6-800.webp 800w,/assets/img/2024-09-30-Neuralangelo/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-30-Neuralangelo/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig b. Numerical Gradient from adjacent cells </div> <h3 id="coarse-to-fine">Coarse-to-Fine</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-30-Neuralangelo/7-480.webp 480w,/assets/img/2024-09-30-Neuralangelo/7-800.webp 800w,/assets/img/2024-09-30-Neuralangelo/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-30-Neuralangelo/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Coarse-to-Fine :<br/> 점점 hash grid encoding resol. \(N_l\) 증가시키고<br/> 이에 맞춰서 numerical gradient의 step size \(\epsilon\) 감소시킴</li> </ul> <h3 id="code">Code</h3> <ul> <li> <p>.yaml 로부터 config Dictionary 만들기 <a href="https://github.com/NVlabs/neuralangelo/blob/main/imaginaire/config.py">Code</a></p> </li> <li> <p>.yaml에 적어놓은 module을 동적으로 읽어와서 해당 module 내 class 사용하기 <a href="https://github.com/NVlabs/neuralangelo/blob/main/imaginaire/trainers/utils/get_trainer.py">Code</a></p> </li> <li> <p>Train 껍질 <a href="https://github.com/NVlabs/neuralangelo/blob/main/train.py">Code</a></p> </li> <li> <p>Trainer <a href="https://github.com/NVlabs/neuralangelo/blob/main/projects/neuralangelo/trainer.py">Code</a> \(\rightarrow\) overriding \(\rightarrow\) <a href="https://github.com/NVlabs/neuralangelo/blob/main/imaginaire/trainers/base.py">Code</a></p> </li> <li> <p>각종 함수 계산하는 utils <a href="https://github.com/NVlabs/neuralangelo/tree/main/projects/neuralangelo/utils">Code</a></p> </li> </ul> <h3 id="question">Question</h3> <ul> <li> <p>Q1 :<br/> analytical gradient에 비해 numerical gradient가 갖는 장점을 정리해서 알려주세요</p> </li> <li>A1 : <ul> <li>numerical gradient는<br/> point 하나만 sampling해도<br/> 그 주위의 여러 samples’ feature까지 다룰 수 있음</li> <li>gradient 하나가 얼마나 넓은 범위에 영향을 미치는지에 따라 sample efficiency가 결정되고 학습의 효율성이 결정됨<br/> continuous surface 상황에서는 하나의 error에서 나오는 gradient가 여러 군데에 영향을 동시에 미치는 것이 적합함<br/> 사실 forward pass에서 많은 points를 aggregate(또는 blur)하면 analytical gradient로도 backpropagation이 여러 군데에 퍼지게 할 수 있다<br/> 하지만 그러면 forward 쪽이 blur해지면서 frequency bound가 생기고, 속도가 느려짐<br/> 따라서 forward pass 쪽은 건들지 않고 backward pass 쪽만 건드려서 (numerical gradient for regularization loss)<br/> backpropagation이 여러 군데에 퍼지게 함</li> </ul> </li> <li> <p>Q2 :<br/> analytical gradient 대신 numerical gradient 쓰기 위해 adjacent cells’ SDF까지 계산하려면 performance 상승하긴 하지만 느려지지 않나요?</p> </li> <li>A2 :<br/> Instant-NGP의 Hash Grid 방식 자체가 빨라서 ㄱㅊ<br/> 내 피셜로는 regularization loss 구할 때만 adjacent cells’ SDF 이용하므로 inference rendering speed는 그대로라서 training speed 저하 미비</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="3d"/><category term="surface"/><category term="hash"/><category term="grid"/><category term="numerical"/><category term="gradient"/><summary type="html"><![CDATA[High-Fidelity Neural Surface Reconstruction (CVPR 2023)]]></summary></entry><entry><title type="html">TensoRF</title><link href="https://semyeong-yu.github.io/blog/2024/TensoRF/" rel="alternate" type="text/html" title="TensoRF"/><published>2024-09-17T12:00:00+00:00</published><updated>2024-09-17T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/TensoRF</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/TensoRF/"><![CDATA[<h2 id="tensorf-tensorial-radiance-fields">TensoRF: Tensorial Radiance Fields</h2> <h4 id="anpei-chen-zexiang-xu-andreas-geiger-jingyi-yu-hao-su">Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, Hao Su</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2203.09517">https://arxiv.org/abs/2203.09517</a><br/> project website :<br/> <a href="https://apchenstu.github.io/TensoRF/">https://apchenstu.github.io/TensoRF/</a><br/> code :<br/> <a href="https://github.com/apchenstu/TensoRF">https://github.com/apchenstu/TensoRF</a><br/> referenced blog :<br/> <a href="https://xoft.tistory.com/42">https://xoft.tistory.com/42</a></p> </blockquote> <h2 id="abstract">Abstract</h2> <ul> <li> <p>Radiance Field (Scene)에 대해 Tensor Decomposition을 적용해보자!</p> </li> <li> <p>fast training and less computational cost</p> </li> </ul> <h2 id="tensor-decomposition">Tensor Decomposition</h2> <ul> <li> <p>외적 (outer product) :<br/> \(\begin{bmatrix} 1 &amp; 2 \\ 2 &amp; 4 \\ 4 &amp; 8 \end{bmatrix} = \begin{bmatrix} 1 \\ 2 \\ 4 \end{bmatrix} \circ \begin{bmatrix} 1 &amp; 2 \end{bmatrix}\)<br/> 위의 예시는<br/> shape (3, 2) matrix를<br/> shape (3,) vector와 shape (2,) vector의 외적으로 표현</p> </li> <li> <p>Tensor Decomposition :</p> <ul> <li>\(n\)-dim.의 data를 \(n\)개의 1D vector들의 외적으로 표현할 수 있다!<br/> 이 때, 정보 손실이 발생할 수 있으므로<br/> \(R\) 개의 rank에 대해 외적들을 더해 \(n\)-dim. data를 근사</li> <li>장점 :<br/> 고차원 data를 1D vector들로 표현할 수 있으므로<br/> speed 개선</li> <li>단점 :<br/> 수많은 1D vector들로 표현하므로<br/> GPU memory 많이 소요</li> <li>종류 :<br/> CP(CANDECOMP/PARAFAC) decomposition<br/> Tucker Decomposition<br/> Block Term Decomposition</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/1-480.webp 480w,/assets/img/2024-09-17-TensoRF/1-800.webp 800w,/assets/img/2024-09-17-TensoRF/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> reference : https://www.kolda.net/publication/TensorReview.pdf </div> <ul> <li>Tensor Decomposition w. Trilinear Interpolation :<br/> interpolation으로 1D vector A와 B의 길이를 증가시키고<br/> 그 값으로 원본 matrix 표현</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/2-480.webp 480w,/assets/img/2024-09-17-TensoRF/2-800.webp 800w,/assets/img/2024-09-17-TensoRF/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> https://xoft.tistory.com/42 </div> <h2 id="tensorf-tensor-decomposition">TensoRF Tensor Decomposition</h2> <h3 id="cpcandecompparafac-decomposition">CP(CANDECOMP/PARAFAC) Decomposition</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/3-480.webp 480w,/assets/img/2024-09-17-TensoRF/3-800.webp 800w,/assets/img/2024-09-17-TensoRF/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> https://xoft.tistory.com/42 </div> <ul> <li>shape (i, j, k)의 \(T\) 에 대해<br/> \(T = \sum_{r=1}^R v_{r}^1 \circ v_{r}^2 \circ v_{r}^3\)</li> </ul> <h3 id="vmvector-matrix-decomposition">VM(vector-matrix) Decomposition</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/4-480.webp 480w,/assets/img/2024-09-17-TensoRF/4-800.webp 800w,/assets/img/2024-09-17-TensoRF/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> https://xoft.tistory.com/42 </div> <ul> <li>shape (i, j, k)의 \(T\) 에 대해<br/> \(T = \sum_{r=1}^{R_1} v_{r}^1 \circ M_{r}^{2,3} + \sum_{r=1}^{R_2} v_{r}^2 \circ M_{r}^{1,3} + \sum_{r=1}^{R_3} v_{r}^3 \circ M_{r}^{1,2}\)</li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li>Grid-based 연구들이 training speed 높이는 데 많은 기여를 하고 있으니<br/> 다른 논문들도 한 번 읽어보자 <ul> <li>Plenoxel (CVPR 2022)</li> <li>Instant-NGP (SIGGRAPH 2022)</li> <li>DVGO (CVPR 2022)</li> </ul> </li> <li>Grid-based 연구들 <ul> <li>장점 : speed 개선</li> <li>단점 : 해상도가 증가하면 GPU memory 많이 소요<br/> 기존 연구들은 space complexity \(O(N^3)\) 인데,<br/> TensoRF는 이를 \(O(N^2)\) 으로 줄임</li> </ul> </li> </ul> <h2 id="algorithm">Algorithm</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/5-480.webp 480w,/assets/img/2024-09-17-TensoRF/5-800.webp 800w,/assets/img/2024-09-17-TensoRF/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Algorithm : <ul> <li>step 1)<br/> scene을 bounded cubic (grid)로 제한</li> <li>step 2)<br/> ray를 쏴서 sampled points를 구한 뒤<br/> 각 rank의 선과 면으로 projection하고<br/> 외적한 값들을 이용해서<br/> color와 volume density 계산</li> <li>step 2-1)<br/> volume density는 단순히 외적한 값들을 더해서 구함<br/> (VM Decomposition)</li> <li>step 2-2)<br/> color는 외적한 값들을 concat한 뒤<br/> function B와 function S에 통과시켜 얻음 <ul> <li>function B :<br/> 1개의 FC-layer<br/> appearance commonalities를 추상화는 Global Apperance Dictionary 역할</li> <li>function S :<br/> MLP 또는 SH(Spherical Harmonics) 함수</li> </ul> </li> </ul> </li> </ul> <h2 id="loss">Loss</h2> <ul> <li>sparse input images일 경우<br/> 적게 관측된 view에서는 outlier 혹은 noise가 발생할 수 있어<br/> overfitting 혹은 local minima 문제 발생<br/> \(\rightarrow\)<br/> regularization term 추가한 loss 사용<br/> e.g. TV(total variation) loss :<br/> pixel 값 간의 급격한 변화 (noise or outlier)를 억제하기 위해<br/> \(I_{i+1, j} - I_{i, j}\) 항과 \(I_{i, j+1} - I_{i, j}\) 항을 loss에 추가</li> </ul> <h2 id="coarse-to-fine">Coarse-to-Fine</h2> <ul> <li> <p>NeRF의 coarse-to-fine 기법 :<br/> \(w_i = T_i \alpha_{i}\) 의 PDF 분포에 따라<br/> 일부 구간을 더 많이 sampling</p> </li> <li>Mip-NeRF 360의 coarse-to-fine 기법 : <ul> <li>small coarse proposal-MLP는 many samples로 여러 번 evaluate하여 weight \(\hat w\) 를 구하고<br/> large fine NeRF-MLP는 less samples로 딱 한 번 evaluate하여 weight \(w\) 와 color \(c\) 를 구함</li> <li>proposal loss를 이용하여 NeRF-MLP의 지식을 proposal-MLP가 따라잡도록 함</li> </ul> </li> <li>TensoRF의 coarse-to-fine 기법 :<br/> 단순히 grid 크기를 upsampling<br/> Grid size(resolution)이 커질수록 선 또는 면이 더 촘촘해져서 3D scene의 high-freq. feature를 더 잘 잡아낼 수 있음</li> </ul> <h2 id="implementation">Implementation</h2> <ul> <li>Decomposition Rank : 총 48개 <ul> <li>RGB : 16, 4, 4</li> <li>volume density : 16, 4, 4</li> </ul> </li> <li> <p>Grid size : coarse-to-fine<br/> \(128^3\) 에서 \(300^3\) 으로 점점 증가시키면서 학습<br/> (2000, 3000, 4000, 5500, 7000 step에서 점차 증가시킴)</p> </li> <li> <p>Batch size : 4096 pixels</p> </li> <li>Adam optimizer, V100 GPU(16GB)</li> </ul> <h2 id="evaluation">Evaluation</h2> <ul> <li>기존 Grid-based 연구들과<br/> training speed는 유사하지만<br/> PSNR이 높고<br/> 모델 사이즈 및 GPU memory 사용량이 적음</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/6-480.webp 480w,/assets/img/2024-09-17-TensoRF/6-800.webp 800w,/assets/img/2024-09-17-TensoRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/7-480.webp 480w,/assets/img/2024-09-17-TensoRF/7-800.webp 800w,/assets/img/2024-09-17-TensoRF/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Ours-VM-192 : 4DGS, VM Decomposition, 192개의 rank <ul> <li>speed, PSNR : Ours-VM-192를 15000 iter.만큼만 진행했을 때 8분만에 기존 연구들보다 PSNR 높음</li> <li>memory : 기존 연구들보다 확연히 memory size 적음</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/8-480.webp 480w,/assets/img/2024-09-17-TensoRF/8-800.webp 800w,/assets/img/2024-09-17-TensoRF/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-17-TensoRF/9-480.webp 480w,/assets/img/2024-09-17-TensoRF/9-800.webp 800w,/assets/img/2024-09-17-TensoRF/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-17-TensoRF/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Param. 실험 : <ul> <li>Grid size가 증가할수록 성능 좋아지지만 speed 느려지고 model size 커짐</li> <li>CP Decomposition보다 VM Decomposition이 성능 더 좋음</li> <li>rank 개수가 증가할수록 성능 좋아짐</li> </ul> </li> <li>iter. : <ul> <li>iter.이 증가할수록 PSNR이 증가<br/> 5k iter.만 해도 PSNR이 30에 가까워지고, 점점 변동폭이 작아짐</li> </ul> </li> </ul> <h2 id="limitation">Limitation</h2> <ul> <li>scene을 bounded cubic 안에 제한해야<br/> projection을 통해 VM Decomposition이 가능하므로<br/> unbounded scene은 다루지 못함</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="radiance"/><category term="field"/><category term="tensor"/><category term="decomposition"/><summary type="html"><![CDATA[Tensorial Radiance Fields (ECCV 2022)]]></summary></entry><entry><title type="html">4D Gaussian Splatting</title><link href="https://semyeong-yu.github.io/blog/2024/4DGS/" rel="alternate" type="text/html" title="4D Gaussian Splatting"/><published>2024-09-14T12:00:00+00:00</published><updated>2024-09-14T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/4DGS</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/4DGS/"><![CDATA[<h2 id="4d-gaussian-splatting-for-real-time-dynamic-scene-rendering">4D Gaussian Splatting for Real-Time Dynamic Scene Rendering</h2> <h4 id="guanjun-wu-taoran-yi-jiemin-fang-lingxi-xie-xiaopeng-zhang-wei-wei-wenyu-liu-qi-tian-xinggang-wang">Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, Xinggang Wang</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2310.08528">https://arxiv.org/abs/2310.08528</a><br/> project website :<br/> <a href="https://guanjunwu.github.io/4dgs/index.html">https://guanjunwu.github.io/4dgs/index.html</a><br/> code :<br/> <a href="https://github.com/hustvl/4DGaussians">https://github.com/hustvl/4DGaussians</a><br/> referenced blog :<br/> <a href="https://xoft.tistory.com/54">https://xoft.tistory.com/54</a></p> </blockquote> <blockquote> <p>핵심 요약 :<br/> <code class="language-plaintext highlighter-rouge">3DGS를 dynamic scene에 적용하고자 할 때</code><br/> x, y, z, t를 input으로 갖는 encoder로서<br/> <code class="language-plaintext highlighter-rouge">4D scene을 2D planes로 표현하는 HexPlane 기법을 이용하겠다!</code></p> </blockquote> <h2 id="abstract">Abstract</h2> <ul> <li> <p>spatially-temporally-sparse input으로부터<br/> complex point motion을 정확하게 모델링하면서<br/> high efficiency로 real-time dynamic scene을 rendering하는 건 매우 challenging task</p> </li> <li>3DGS를 각 frame에 적용하는 게 아니라 4DGS라는 새로운 모델 제시 <ul> <li><code class="language-plaintext highlighter-rouge">오직 3DGS 한 세트</code> 필요</li> <li>4DGS framework : <ul> <li><code class="language-plaintext highlighter-rouge">Spatial-Temporal Structure Encoder</code> :<br/> HexPlane <d-cite key="neuralvoxel1">[22]</d-cite> 에서 영감을 받아<br/> decomposed neural voxel encoding algorithm을 이용해서<br/> <code class="language-plaintext highlighter-rouge">4D neural voxel을 2D voxel planes로 decompose</code>하여<br/> 2D voxel plane (param.)에 Gaussian <code class="language-plaintext highlighter-rouge">point-clouds (pts)의 spatial-temporal 정보를 encode</code></li> <li><code class="language-plaintext highlighter-rouge">Extremely Tiny Multi-head Gaussian Deformation Decoder</code> :<br/> 가벼운 MLP를 이용해서<br/> <code class="language-plaintext highlighter-rouge">Gaussian deformation을 예측</code>함</li> </ul> </li> </ul> </li> <li>4DGS :<br/> real-time (82 FPS) rendering at high (800 \(\times\) 800) resolution on RTX 3090 GPU</li> </ul> <h2 id="contribution">Contribution</h2> <ul> <li> <p>Gaussian <code class="language-plaintext highlighter-rouge">motion</code>과 <code class="language-plaintext highlighter-rouge">shape</code>-deformation을 모두 모델링할 수 있는 4DGS framework 제시<br/> w. efficient <code class="language-plaintext highlighter-rouge">Gaussian deformation field</code> network</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">multi-resolution</code> encoding<br/> (only on spatial planes)<br/> (connect nearby 3D Gaussians to build rich Gaussian features)<br/> by efficient <code class="language-plaintext highlighter-rouge">spatial-temporal structure encoder</code></p> </li> <li> <p>SOTA <code class="language-plaintext highlighter-rouge">performance</code>이면서 <code class="language-plaintext highlighter-rouge">real-time</code> rendering on <code class="language-plaintext highlighter-rouge">dynamic</code> scenes<br/> e.g. 82 FPS at resol. 800 \(\times\) 800 for synthetic dataset<br/> e.g. 30 FPS at resol. 1352 \(\times\) 1014 for real dataset</p> </li> <li> <p>4D scenes에서의 editing 및 tracking에 활용 가능</p> </li> </ul> <h2 id="related-works">Related Works</h2> <h3 id="novel-view-synthesis">Novel View Synthesis</h3> <ul> <li>static scene : <ul> <li>light fields <d-cite key="lightfield">[1]</d-cite>, mesh <d-cite key="mesh1">[2]</d-cite> <d-cite key="mesh2">[3]</d-cite> <d-cite key="mesh3">[4]</d-cite> <d-cite key="mesh4">[5]</d-cite>, voxels <d-cite key="voxel1">[6]</d-cite> <d-cite key="voxel2">[7]</d-cite> <d-cite key="voxel3">[8]</d-cite>, multi-planes <d-cite key="multiplane">[9]</d-cite> 이용한 methods</li> <li>NeRF-based methods <a href="https://semyeong-yu.github.io/blog/2024/NeRF/">NeRF</a> <a href="https://semyeong-yu.github.io/blog/2024/MipNeRF/">MipNeRF</a> <d-cite key="nerf++">[10]</d-cite></li> </ul> </li> <li>dynamic scene : <ul> <li>NeRF-based methods <d-cite key="dynerf1">[11]</d-cite> <d-cite key="dynerf2">[12]</d-cite> <d-cite key="dynerf3">[13]</d-cite></li> <li><code class="language-plaintext highlighter-rouge">explicit voxel grid</code> <d-cite key="voxeltemp1">[14]</d-cite> <d-cite key="voxeltemp2">[15]</d-cite> <d-cite key="voxeltemp3">[16]</d-cite> <d-cite key="voxeltemp4">[17]</d-cite> :<br/> temporal info. 모델링하기 위해 explicit voxel grid 사용</li> <li><code class="language-plaintext highlighter-rouge">flow-based</code> methods <d-cite key="flow1">[18]</d-cite> <d-cite key="flow2">[19]</d-cite> <d-cite key="voxeltemp3">[16]</d-cite> <d-cite key="flow3">[20]</d-cite> <d-cite key="flow4">[21]</d-cite> :<br/> nearby frames를 blending하는 warping algorithm 사용</li> <li><code class="language-plaintext highlighter-rouge">decomposed neural voxels</code> <d-cite key="neuralvoxel1">[22]</d-cite> <d-cite key="neuralvoxel2">[23]</d-cite> <d-cite key="neuralvoxel3">[24]</d-cite> <d-cite key="neuralvoxel4">[25]</d-cite> <d-cite key="neuralvoxel5">[26]</d-cite> <d-cite key="neuralvoxel6">[27]</d-cite> :<br/> 빠른 training on dynamic scenes 가능<br/> (Fig 1.의 (b))</li> <li><code class="language-plaintext highlighter-rouge">multi-view</code> setups 다루기 위한 methods <d-cite key="multi1">[28]</d-cite> <d-cite key="multi2">[29]</d-cite> <d-cite key="multi3">[30]</d-cite> <d-cite key="multi4">[31]</d-cite> <d-cite key="multi5">[32]</d-cite> <d-cite key="multi6">[33]</d-cite></li> <li>본 논문 (4DGS) :<br/> 위에서 언급된 methods는 빠른 training은 가능했지만 real-time rendering on dynamic scenes는 여전히 어려웠음<br/> \(\rightarrow\)<br/> 본 논문은 빠른 training 및 rendering pipeline 제시<br/> (Fig 1.의 (c))</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/1-480.webp 480w,/assets/img/2024-09-14-4DGS/1-800.webp 800w,/assets/img/2024-09-14-4DGS/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 1. dynamic scene rendering methods </div> <ul> <li>Fig 1. 설명 :<br/> dynamic scene을 rendering하는 여러 방법들 소개 <ul> <li>(a) : Deformation-based (Canonical Mapping) Volume Rendering<br/> point-deformation-field를 이용해서<br/> sampled points를 canonical space로 mapping<br/> (하나의 ray 위의 sampled points가 다같이 canonical space로 mapping되므로<br/> 각 point의 서로 다른 속도를 잘 모델링하지 못함)</li> <li>(b) : Time-aware Volume Rendering<br/> 각 timestamp에서의 각 point의 feature를 직접 개별적으로 계산<br/> (path는 그대로)</li> <li>(c) : 4DGS <br/> compact <code class="language-plaintext highlighter-rouge">Gaussian-deformation-field</code>를 이용해서<br/> 기존의 3D Gaussians를 특정 timestamp의 3D Gaussians로 변환<br/> ((a)와 유사하긴 하지만<br/> 각 Gaussian이 <code class="language-plaintext highlighter-rouge">ray에 의존하지 않고 서로 다른 속도로 이동</code> 가능)</li> </ul> </li> </ul> <h3 id="neural-rendering-w-point-clouds">Neural Rendering w. Point Clouds</h3> <ul> <li> <p>3D scenes를 나타내기 위해 meshes, point-clouds, voxels, hybrid ver. 등 여러 분야가 연구되어 왔는데<br/> 그 중 point-cloud representation을 volume rendering과 결합하면<br/> dynamic novel view synthesis task도 잘 수행 가능</p> </li> <li> <p>3DGS :<br/> <code class="language-plaintext highlighter-rouge">explicit</code> representation이라서,<br/> <code class="language-plaintext highlighter-rouge">differentiable</code> <code class="language-plaintext highlighter-rouge">point</code>-based splatting이라서,<br/> <code class="language-plaintext highlighter-rouge">real-time</code> renderer라서 주목받음</p> </li> <li> <p>3DGS on dynamic scenes :</p> <ul> <li>Dynamic3DGS <d-cite key="dyna3DGS">[34]</d-cite> : <ul> <li>3D Gaussian 개수를 고정하고<br/> 각 timestamp \(t_i\) 마다 각 3D Gaussian의 position, variance를 tracking</li> <li>문제점 : <ul> <li>need dense multi-view input images</li> <li>prev. frame의 모델링이 부적절하면 전체적인 성능이 떨어짐</li> <li>linear memory consumption \(O(tN)\)<br/> for \(t\)-time steps and \(N\)-3D Gaussians</li> </ul> </li> </ul> </li> <li>4DGS (본 논문) : <ul> <li>very compact network로 3D Gaussian motion을 모델링하기 때문에<br/> training 효율적이고 real-time rendering</li> <li>memory consumption \(O(N+F)\)<br/> for \(N\)-3D Gaussians, \(F\)-parameters of Gaussian-deformation-field network</li> </ul> </li> <li>4DGS (Zeyu Yang) <d-cite key="4DGS1">[35]</d-cite> : <ul> <li>marginal temporal Gaussian 분포를 기존의 3D Gaussian 분포에 추가하여<br/> 3D Gaussians를 4D로 uplift</li> <li>However, 그러면 각 3D Gaussian은 오직 their local temporal space에만 focus</li> </ul> </li> <li>Deformable-3DGS (Ziyi Yang) <d-cite key="deformable3DGS">[36]</d-cite> : <ul> <li>본 논문처럼 MLP deformation network를 도입하여 dynamic scenes의 motion을 모델링</li> <li>본 논문 (4DGS)도 이와 유사하지만 training을 효율적으로 만듦</li> </ul> </li> <li>Spacetime-GS (Zhan Li) <d-cite key="spacetimeGS">[37]</d-cite> : <ul> <li>각 3D Gaussian을 individually tracking</li> </ul> </li> </ul> </li> </ul> <h3 id="dynamic-nerf-with-deformation-fields">Dynamic NeRF with Deformation Fields</h3> <ul> <li> <p>모든 dynamic NeRF는 아래의 식을 따른다<br/> \(c, \sigma = M(x, d, t, \lambda)\)<br/> where \(c \in R^3, \sigma \in R, x \in R^3, d \in R^2, t \in R, \lambda \in R\)<br/> where \(\lambda\) is optional input (frame-dependent code to build topological and appearance changes) <d-cite key="dynerf2">[12]</d-cite> <d-cite key="wild">[38]</d-cite></p> </li> <li> <p>deformation NeRF-based methods는<br/> Fig 1. (a)에서처럼<br/> deformation network \(\phi_{t} : (x, t) \rightarrow \Delta x\) 로 world-to-canonical mapping 한 뒤<br/> RGB color와 volume density를 뽑는다<br/> \(c, \sigma = NeRF(x+\Delta x, d, \lambda)\)</p> </li> <li> <p>4DGS (본 논문)은<br/> <code class="language-plaintext highlighter-rouge">Gaussian deformation field</code> network \(F\) 이용해서<br/> time \(t\) 에서의 <code class="language-plaintext highlighter-rouge">canonical-to-world mapping</code>을 직접 계산한 뒤<br/> differential splatting(rendering) 수행</p> </li> </ul> <h2 id="method">Method</h2> <h3 id="overview-gaussian-deformation-field-network">Overview (Gaussian Deformation Field Network)</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/2-480.webp 480w,/assets/img/2024-09-14-4DGS/2-800.webp 800w,/assets/img/2024-09-14-4DGS/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 2. Pipeline of this model (Gaussian Deformation Field Network) </div> <ul> <li>Fig 2. 설명 : <ul> <li>static 3D Gaussian set을 만듦</li> <li>각 3D Gaussian의 center 좌표 \(x, y, z\) 와 timestamp \(t\) 를<br/> Gaussian Deformation Field Network의 input으로 준비</li> <li>Spatial-Temporal Structure Encoder :<br/> multi-resolution voxel planes를 query하여<br/> voxel feature를 계산<br/> (temporal 및 spatial feature를 둘 다 encode 가능)</li> <li>Tiny Multi-head Gaussian Deformation Decoder :<br/> position, rotation, scaling head에서 각각 해당 feature를 decode하여<br/> 각 3D Gaussian의 position, rotation, scaling 변화량을 얻어서<br/> timestamp \(t\) 에서의 변형된 3D Gaussians를 얻음</li> </ul> </li> </ul> <h3 id="spatial-temporal-structure-encoder">Spatial-Temporal Structure Encoder</h3> <ul> <li> <p>근처에 있는 3D Gaussians끼리는 항상 spatial 및 temporal 정보를 비슷하게 공유하고 있다.<br/> 따라서 HexPlane 기법에서는 각 Gaussian이 따로 변형되는 게 아니라,<br/> 여러 <code class="language-plaintext highlighter-rouge">adjacent 3D Gaussian</code>들이 군집처럼 연결되어 함께 변형되므로<br/> motion과 shape-deformation을 정확하게 예측할 수 있다<br/> 이로써 변형된 geometry를 더 정확히 모델링하고 avulsion(벗겨짐?)을 방지할 수 있음</p> </li> <li> <p>기존 논문 설명 (Backgrounds) :</p> <ul> <li>TensoRF : <a href="https://semyeong-yu.github.io/blog/2024/TensoRF/">Link</a></li> <li>HexPlane <d-cite key="neuralvoxel1">[22]</d-cite> :<br/> 4차원(\(XYZT\))을 모델링하기 위해<br/> 3개 타입의 rank로 decomposition (\(XY\) 평면 - \(ZT\) 평면, \(XZ\) 평면 - \(YT\) 평면, \(YZ\) 평면 - \(XT\) 평면)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/3-480.webp 480w,/assets/img/2024-09-14-4DGS/3-800.webp 800w,/assets/img/2024-09-14-4DGS/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> HexPlane Overview </div> <ul> <li>Spatial-Temporal Structure Encoder (1) : <ul> <li>vanilla 4D neural voxel은 memory를 많이 잡아먹기 때문에<br/> 4D neural voxel(\(XYZT\))을 6개의 multi-resol. planes로 decompose하는<br/> 4D K-Planes module <d-cite key="neuralvoxel2">[23]</d-cite> 사용</li> <li>3D Gaussians는 bounding plane voxels에 포함되어<br/> Gaussians의 deformation도 nearby temporal voxels에 encode될 수 있음 <code class="language-plaintext highlighter-rouge">????</code></li> <li>기존 논문들 <d-cite key="voxeltemp1">[14]</d-cite> <d-cite key="neuralvoxel1">[22]</d-cite> <d-cite key="neuralvoxel2">[23]</d-cite> <d-cite key="neuralvoxel5">[26]</d-cite> 에서 영감을 받아<br/> Spatial-Temporal Structure Encoder는<br/> multi-resolution HexPlane \(R(i, j)\) 와 tiny MLP \(\phi_{d}\) 로 구성됨</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/4-480.webp 480w,/assets/img/2024-09-14-4DGS/4-800.webp 800w,/assets/img/2024-09-14-4DGS/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Spatial-Temporal Structure Encoder (2) : <ul> <li>multi-resolution HexPlane \(R(i, j)\) :<br/> 본 논문에서는 TensoRF와 달리 Grid resol.을 점점 증가시키지 않고, 애초에 multi-resolution으로 decomposition의 rank를 구성함<br/> \(f_{h} = \cup_{l} \prod \text{interp}(R_{l}(i, j))\) <ul> <li>where<br/> \(f_h \in R^{h \ast l}\) : feature of decomposed neural voxel<br/> \(R_{l}(i, j) \in R^{h \times lN_i \times lN_j}\) : 2D voxel plane (nn.Parameter())<br/> \(h\) : hidden dim.<br/> \(\{ i, j \} \in \{ (x, y), (x, z), (y, z), (x, t), (y, t), (z, t) \}\) : 6 종류의 planes<br/> \(N\) : voxel grid의 basic resol.<br/> \(l \in \{ 1, 2 \}\) : upsampling scale (multi-resol.)<br/> \(\text{interp}\) : bilinear interpolation (plane의 grid의 네 꼭짓점으로부터 interpolation으로 voxel feature 뽑아냄)<br/> \(\prod\) : product over planes (K-Planes <d-cite key="neuralvoxel2">[23]</d-cite> 참고)<br/> \(\cup_{l}\) : multi-resol.에 대해 concat 또는 add</li> <li><a href="https://github.com/hustvl/4DGaussians/blob/master/scene/hexplane.py">Github Code</a> 에서 <ul> <li>forward()</li> <li>get_density() <ul> <li>self.grids : multi-resol. HexPlane<br/> 즉, nn.ModuleList() of init_grid_param()</li> <li>init_grid_param() : HexPlane<br/> 즉, nn.ParameterList() of nn.Parameter()<br/> where<br/> range(in_dim) = [0, 1, 2, 3] (x, y, z, t) 중에 grid_nd = 2개의 조합(plane)을 뽑아서<br/> 각 nn.Parameter()는 2D grid plane \(R_{l}(i, j)\) for \(\{ i, j \} \in \{ (x, y), (x, z), (y, z), (x, t), (y, t), (z, t) \}\)<br/> w. shape \((1, D_{out}, \text{resol.}[j], \text{resol.}[i])\)<br/> e.g. \(R_{l}(x, t)\), 즉 \(XT\) plane은 nn.Parameter()<br/> w. shape \((1, D_{out}, \text{resol.}[3], \text{resol.}[0])\)</li> </ul> </li> <li>interpolate_ms_features() : \(f_{h} = \cup_{l} \prod \text{interp}(R_{l}(i, j))\) 반환</li> <li>grid_sample_wrapper() : \(\text{interp}(R_{l}(i, j))\) 반환</li> <li>grid_sampler() : F.grid_sample() <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html">Link</a><br/> second argument(pts) 좌표에서의 값을 구하기 위해 first argument(grid \(R_{l}(i, j)\))의 값을 interpolate<br/> 그럼 이제 dynamic 3D scene을 <code class="language-plaintext highlighter-rouge">4D neural voxel</code> 대신 <code class="language-plaintext highlighter-rouge">2D voxel plane</code> \(R_{l}(i, j)\) 이라는 param.들로 표현 가능</li> </ul> </li> </ul> </li> </ul> </li> <li>Spatial-Temporal Structure Encoder (3) : <ul> <li>tiny MLP \(\phi_{d}\) :<br/> \(f_d = \phi_{d} (f_h)\)<br/> merge all the features</li> <li>공간상(e.g. \(XY\) 평면) 또는 시간상(e.g. \(XT\) 평면)으로 인접한 voxel은<br/> HexPlane \(R(i, j)\) 에서 유사한 feature를 가져서 유사한 Gaussian param. 변화량을 가지므로<br/> optimization 진행됨에 따라<br/> Gaussian의 covariance가 줄어들면서 작은 3D Gaussian들이 모여서 dense해진다 <code class="language-plaintext highlighter-rouge">?????</code></li> </ul> </li> </ul> <h3 id="extremely-tiny-multi-head-gaussian-deformation-decoder">Extremely Tiny Multi-head Gaussian Deformation Decoder</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/5-480.webp 480w,/assets/img/2024-09-14-4DGS/5-800.webp 800w,/assets/img/2024-09-14-4DGS/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Multi-head Gaussian Deformation Decoder : <ul> <li>매우 작은 multi-head decoder로 position, scaling, rotation 변화량을 얻음<br/> \(\Delta \chi = \phi_{x}(f_d)\)<br/> \(\Delta r = \phi_{r}(f_d)\)<br/> \(\Delta s = \phi_{s}(f_d)\)</li> <li>그러면 변형된 deformed 3D Gaussians 계산할 수 있음<br/> \((\chi ' , r ' , s ') = (\chi + \Delta \chi, r + \Delta r, s + \Delta s)\) 에 대해<br/> next time \(t\) 의 deformed 3D Gaussian set은<br/> \(G ' = \{ \chi ' , r ' , s ', \sigma, c \}\)</li> <li>근데 실제 implementation 할 때는 speed 증가 위해<br/> scaling(size), rotation, color, opacity는 고정하고<br/> position 변화량만 구함</li> <li><a href="https://github.com/hustvl/4DGaussians/blob/master/scene/deformation.py">Github Code</a> 에서 <ul> <li>Class deform_network()의 forward_dynamic()</li> <li>Class Deformation()의 forward_dynamic() <ul> <li>hidden : encoder(HexPlane과 MLP) 거쳐 얻은 feature</li> <li>self.pos_deform, self.scales_deform, self.rotations_deform : tiny Multi-head decoder<br/> hidden으로부터 \(\Delta \chi, \Delta r, \Delta s\) 얻음</li> <li>self.static_mlp :<br/> hidden으로부터 \(\text{mask}\) 얻음</li> <li>position :<br/> \(\chi ' = \gamma(\chi) \times \text{mask} + \Delta \chi\)</li> <li>scaling :<br/> \(s ' = \gamma(s) \times \text{mask} + \Delta s\)</li> <li>rotation :<br/> \(r ' = \gamma(r) + \Delta r\)<br/> 또는<br/> \(r ' =\) quaternion product of \(\gamma(r)\) and \(\Delta r\)</li> <li>opacity, SH 도 deform 가능하게 짜놓긴 함<br/> \(\alpha ' = \alpha \times \text{mask} + \Delta \alpha\)<br/> \(k ' = k \times \text{mask} + \Delta k\)</li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/16-480.webp 480w,/assets/img/2024-09-14-4DGS/16-800.webp 800w,/assets/img/2024-09-14-4DGS/16-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>self._deformation = deform_network(args)</p> <h3 id="optimization">Optimization</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/6-480.webp 480w,/assets/img/2024-09-14-4DGS/6-800.webp 800w,/assets/img/2024-09-14-4DGS/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p><code class="language-plaintext highlighter-rouge">warm-up</code> :<br/> 처음 3000 iter. 동안은<br/> Gaussian Deformation Field Network 없이<br/> 3DGS의 SfM points initialization 이용하여<br/> <code class="language-plaintext highlighter-rouge">static 3DGS</code> optimize 하고,<br/> 그 후에 dynamic scene에 대해 4DGS framework를 fine-tuning 형태로 학습</p> </li> <li> <p>Loss :<br/> \(L = | \hat I - I | + L_{tv}\)</p> <ul> <li>L1 recon. loss</li> <li><code class="language-plaintext highlighter-rouge">total-variational loss</code> : <ul> <li>sparse input images일 경우에 적게 관측된 view에서는 noise 및 outlier 때문에 overfitting 및 local minima 문제가 발생할 수 있으므로<br/> <code class="language-plaintext highlighter-rouge">regularization</code> term 필요</li> <li>pixel 값 간의 급격한 변화를 억제하기 위해<br/> \(I_{i+1, j} - I_{i, j}\) 항과 \(I_{i, j+1} - I_{i, j}\) 항을 loss에 추가</li> </ul> </li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <ul> <li> <p>single RTX 3090 GPU</p> </li> <li> <p>Synthetic Dataset :</p> <ul> <li>designed for monocular settings</li> <li>camera poses for each timestamp은 거의 randomly generated 수준</li> <li>scene 당 50-200 frames</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/12-480.webp 480w,/assets/img/2024-09-14-4DGS/12-800.webp 800w,/assets/img/2024-09-14-4DGS/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Real-world Dataset : <ul> <li>by HyperNeRF <d-cite key="dynerf2">[12]</d-cite> and Neu3D <d-cite key="neuralvoxel4">[25]</d-cite></li> <li>HyperNeRF dataset :<br/> one or two cameras<br/> with straightforward camera motion<br/> (1,2개의 camera를 직관적인 경로로 움직이며 촬영)</li> <li>Neu3D dataset :<br/> 15 to 20 static cameras<br/> with extended periods and complex camera motions<br/> (15-20개의 많은 정적인 camera로 오랜 시간 동안씩 촬영하며 복잡한 경로로 camera를 움직임)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/13-480.webp 480w,/assets/img/2024-09-14-4DGS/13-800.webp 800w,/assets/img/2024-09-14-4DGS/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="results">Results</h3> <ul> <li>Metrics : <ul> <li>quality :<br/> PSNR<br/> LPIPS<br/> SSIM<br/> DSSIM<br/> MS-SSIM</li> <li>speed :<br/> FPS<br/> training times</li> <li>memory :<br/> storage</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/7-480.webp 480w,/assets/img/2024-09-14-4DGS/7-800.webp 800w,/assets/img/2024-09-14-4DGS/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/8-480.webp 480w,/assets/img/2024-09-14-4DGS/8-800.webp 800w,/assets/img/2024-09-14-4DGS/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Im4D <d-cite key="multi2">[29]</d-cite> 는 본 논문과 유사하게 high-quality이지만<br/> multi-cam 방식을 쓰기 때문에 monocular scene을 모델링하기 어렵</li> </ul> <h3 id="ablation-study">Ablation Study</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/9-480.webp 480w,/assets/img/2024-09-14-4DGS/9-800.webp 800w,/assets/img/2024-09-14-4DGS/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Spatial-Temporal Structure Encoder : <ul> <li>explicit HexPlane encoder는<br/> 3DGS의 spatial 및 temporal 정보를 모두 encode 하면서<br/> purely explicit method <d-cite key="dyna3DGS">[34]</d-cite> 보다 storage 공간 아낄 수 있음</li> <li>만약에 HexPlane encoder 없이 shallow MLP encoder만 쓰면<br/> 복잡한 deformation 모델링 어렵</li> </ul> </li> <li>3D Gaussian Initialization : <ul> <li>처음에 warm-up으로 SfM points initialization 한 뒤 static 3DGS optimize 부터 해야<br/> 아래의 장점들 있음 <ul> <li>3DGS 일부가 dynamic part에 분포되도록 함</li> <li>3DGS를 미리 학습해야 deformation field가 dynamic part에 더 집중 가능</li> <li>deformation field 학습 시 numerical errors를 방지하여 훈련 과정이 더 stable</li> </ul> </li> </ul> </li> <li>Multi-head Gaussian Deformation Decoder : <ul> <li>3D Gaussian motion을 modeling함으로써 dynamic scene을 잘 표현할 수 있도록 해줌</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/15-480.webp 480w,/assets/img/2024-09-14-4DGS/15-800.webp 800w,/assets/img/2024-09-14-4DGS/15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Neural Voxel Encoder : <ul> <li>implicit MLP-based neural voxel encoder (voxel grid)가 아니라<br/> explicit Dynamic 3DGS 기법을 사용할 경우<br/> rendering quality는 떨어지지만 FPS 및 storage는 향상</li> </ul> </li> <li>Two-stage Training : <ul> <li>static 3DGS stage \(\rightarrow\) dynamic 4DGS stage (fine-tuning) 으로<br/> 분할해서 학습할 경우 성능 향상<br/> (참고로 D-NeRF, DyNeRF에서는 point-clouds가 주어지지 않아서 어려운 task를 다룸)</li> </ul> </li> <li>Image-based Loss : <ul> <li>LPIPS loss, SSIM loss 같은 image-based loss를 사용할 경우<br/> training speed도 느려지고 quality도 떨어짐</li> <li>그 이유는<br/> image-based loss로 motion 부분을 fine-tuning하는 건 어렵고 복잡</li> </ul> </li> <li>Model Capacity (MLP size) : <ul> <li>voxel plane resol. 또는 MLP 크기가 증가할수록<br/> quality 향상되지만 FPS 및 storage 악화</li> </ul> </li> <li>Fast Training : <ul> <li>7k iter. 까지만 학습해도(training 시간 짧음) 괜찮은 PSNR 달성</li> </ul> </li> </ul> <h3 id="discussion">Discussion</h3> <ul> <li>Tracking with 3D Gaussians : <ul> <li>Dynamic3DGS <d-cite key="dyna3DGS">[34]</d-cite> 와 달리<br/> 본 논문은 monocular setting에서도 low storage로 3D object tracking 가능<br/> (e.g. 10MB for 3DGS and 8MB for deformation field network)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/10-480.webp 480w,/assets/img/2024-09-14-4DGS/10-800.webp 800w,/assets/img/2024-09-14-4DGS/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Composition(Editing) with 4D Gaussians : <ul> <li>Dynamic3DGS <d-cite key="dyna3DGS">[34]</d-cite> 에서처럼<br/> 4DGS editing 가능</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/11-480.webp 480w,/assets/img/2024-09-14-4DGS/11-800.webp 800w,/assets/img/2024-09-14-4DGS/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Rendering Speed (FPS) : <ul> <li>3DGS 수와 FPS는 반비례 관계인데<br/> Gaussians 수가 30,000개 이하이면 single RTX 3090 GPU에서 90 FPS 까지 가능</li> <li>이처럼 real-time FPS를 달성하려면<br/> resolution, Gaussian 수, Gaussian deformation field network 용량, hardware constraints 등 여러 요인을 조절해야 함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/14-480.webp 480w,/assets/img/2024-09-14-4DGS/14-800.webp 800w,/assets/img/2024-09-14-4DGS/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-09-14-4DGS/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="limitation">Limitation</h3> <ul> <li>아래의 경우엔 학습 잘 안 됨 <ul> <li><code class="language-plaintext highlighter-rouge">large motions</code>일 경우</li> <li><code class="language-plaintext highlighter-rouge">background points</code>가 없을 경우</li> <li><code class="language-plaintext highlighter-rouge">camera pose</code>가 <code class="language-plaintext highlighter-rouge">unprecise</code>(부정확)할 경우</li> </ul> </li> <li> <p>추가적인 supervision 없이<br/> <code class="language-plaintext highlighter-rouge">static</code> Gaussians와 <code class="language-plaintext highlighter-rouge">dynamic</code> Gaussians의 joint motion을 구분하는 건 아직 어려운 과제</p> </li> <li><code class="language-plaintext highlighter-rouge">urban(large)-scale</code> recon.일 경우엔<br/> 3DGS 수가 훨씬 많아서<br/> Gaussian deformation field network를 query하기에 너무 무거우므로 좀 더 compact한 algorithm이 필요</li> </ul> <h3 id="conclusion">Conclusion</h3> <ul> <li> <p>4DGS framework for <code class="language-plaintext highlighter-rouge">real-time</code> <code class="language-plaintext highlighter-rouge">dynamic</code> scene rendering</p> </li> <li>efficient deformation field network to model motions and shape-deformation <ul> <li>Spatial-temporal structure encoder :<br/> adjacent Gaussians가 비슷하게 encode되도록 spatial-temporal 정보를 encode</li> <li>Multi-head Gaussian deformation decoder :<br/> position, scaling, rotation을 각각 modeling</li> </ul> </li> <li>dynamic scenes 모델링 뿐만 아니라<br/> 4D object tracking 및 editing에도 활용 가능</li> </ul> <h2 id="code-flow">Code Flow</h2> <ul> <li>TBD</li> </ul> <h2 id="question">Question</h2> <ul> <li>Q1 : 본 논문을 한 문장으로 요약하자면,<br/> 3DGS를 dynamic scene에 적용하고자 할 때 4D 정보를 효율적으로 encode하기 위해 2D planes로 scene을 표현하는 HexPlane 기법을 이용하겠다!인데,<br/> 본 논문이 novelty가 있는지 의구심이 듭니다.</li> <li> <p>A1 : 3DGS 논문 자체가 나온 지 얼마 안 돼서<br/> 기존 논문(HexPlane) 아이디어를 3DGS에 적용하는 논문들이 아직까지는 많이 채택되는 것 같다.</p> </li> <li>Q2 : 본 포스팅에서 코드 리뷰는 encoder (HexPlane) 쪽만 진행하였는데,<br/> Multi-head Gaussian deformation decoder로 position, scaling, rotation 변화량을 구해서<br/> Deformed(변형된) 3DGS를 구하는 부분의 코드도 보고 싶습니다.</li> <li>A2 : 포스팅의 “Extremely Tiny Multi-head Gaussian Deformation Decoder” 부분에 해당 내용을 추가하였습니다.</li> </ul> <h2 id="appendix">Appendix</h2> <ul> <li>TBD</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="GS"/><category term="4d"/><category term="dynamic"/><category term="rendering"/><summary type="html"><![CDATA[4D Gaussian Splatting for Real-Time Dynamic Scene Rendering (CVPR 2024)]]></summary></entry><entry><title type="html">Lagrange Multiplier Method</title><link href="https://semyeong-yu.github.io/blog/2024/Lagrange/" rel="alternate" type="text/html" title="Lagrange Multiplier Method"/><published>2024-09-14T11:00:00+00:00</published><updated>2024-09-14T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Lagrange</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Lagrange/"><![CDATA[<p>본 포스팅 출처 : <a href="https://untitledtblog.tistory.com/96">Link</a></p> <h3 id="lagrange-multiplier-method">Lagrange Multiplier Method</h3> <ul> <li> <p>언제? :<br/> multi-variate function을 optimize할 때<br/> <code class="language-plaintext highlighter-rouge">constraint</code>가 존재할 경우<br/> 최적점의 필요조건을 찾기 위해<br/> Lagrange Multiplier Method 사용</p> </li> <li> <p>핵심 아이디어 :<br/> 주어진 function \(f\) 와 constraint \(g_{i}\) 에 대해<br/> \(f\) 와 \(g_{i}\) 의 <code class="language-plaintext highlighter-rouge">접점 (경계)</code>에 \(f\) 의 최댓(솟)값이 존재할 수도 있다!<br/> 그리고 접점에서는 \(\nabla f\) 를 \(\nabla g_{i}\) 들의 linear comb.로 표현할 수 있다!<br/> (다만, 접점은 극점이므로 반드시 최댓값 또는 최솟값이 존재하는 건 아니다)</p> </li> </ul> <h3 id="equality-constraint">Equality Constraint</h3> <ul> <li>\(g_{i}\) 가 등식일 경우 (e.g. \(g_{i} = 1 - \phi_{i}^T\phi_{i} = 0\)) :<br/> 접점에서 gradient가 평행하므로 (이에 대한 수식 증명은 참고한 포스팅 <a href="https://untitledtblog.tistory.com/96">Link</a> 에 있음)<br/> \(\nabla f = \sum_{i=1}^N \lambda_{i} \nabla g_{i}\) 로부터<br/> 아래처럼 풀면 된다<br/> (단, \(\lambda \neq 0\))<br/> (단, \(\nabla f\) 과 \(\nabla g_{i}\) 은 평행할 뿐 방향은 반대여도 됨) <ul> <li>방법 1)<br/> \(\nabla f + \sum_{i=1}^N \lambda_{i} \nabla g_{i} = 0\) 와 \(g_{i} = 0\) 을 연립하여 풀면 된다</li> <li>방법 2)<br/> Equivalently,<br/> \(L = f + \sum_{i=1}^N \lambda_{i} g_{i}\) 에 대해<br/> \(L\) 의 극소(대)점을 찾으면 된다<br/> 즉, \(f, g_{i}\) 가 \(x_{j}\) 에 대한 함수일 경우<br/> \(\frac{\partial L}{\partial x_{j}} = 0\) 과 \(\frac{\partial L}{\partial \lambda_{i}} = 0\) 을 연립하여 풀면 된다</li> </ul> </li> </ul> <h3 id="inequality-constraint">Inequality Constraint</h3> <p>등식 constraint일 때의 Lagrange Multiplier Method는 완전히 이해했는데,<br/> 부등식 constraint일 때의 Lagrange Multiplier Method는 아직 이해 못함.<br/> 추후에 고칠(이해할) 필요 있음. TBD</p> <ul> <li>부등식 constraint일 경우 <code class="language-plaintext highlighter-rouge">KKT (Karush-Kuhn-Tucker) 조건</code>을 만족해야 한다 <ul> <li>1) \(f\) 는 모든 variable (e.g. \(x, y\))에 대해 differentiable</li> <li>2) \(\lambda_{i} \nabla g_{i} = 0\)</li> <li>3) \(\lambda{i} \geq 0\)<br/> (만약 \(\lambda_{i} \lt 0\) 일 경우 \(\nabla f\) 와 \(\nabla g_{i}\) 가 평행하지만 방향이 반대라는 의미이므로 두 함수의 최적점이 서로 반대 방향에 위치하여 constraint를 만족할 수 없다)<br/> (따라서 \(\lambda_{i} \geq 0\) 이어야만 (\(\nabla f\) 방향과 \(\nabla g_{i}\) 방향이 일치해야만) \(\nabla f\) 를 \(\nabla g_{i}\) 들의 linear comb.로 표현 가능한지 아닌지를 판정할 수 있다)</li> </ul> </li> <li>\(g_{i}\) 가 부등식일 경우 (e.g. \(g_{i} = 1 - \phi_{i}^T\phi_{i} \leq 0\)) :<br/> \(\lambda_{i} \nabla g_{i} = 0\) <ul> <li>\(\nabla g_{i} = 0\) 일 경우 :<br/> constraint \(g_{i} \leq 0\) 을 항상 만족하므로<br/> \(\nabla f \geq 0\) 을 푸는 문제로 바꿔 쓸 수 있다<br/> (constraint 없이 \(f\) 만 최적화하면 됨!)</li> <li>\(\lambda_{i} = 0\) 일 경우 :<br/> \(\nabla f\) 를 \(\nabla g_{i}\) 들의 linear comb.로 표현 불가능하다는 의미이므로<br/> 비교하는 두 gradient가 평행하지 않다<br/> 따라서 gradient 방향에 따라 constraint 만족하는 지 여부가 달라지므로<br/> \(\nabla g_{i} \gt 0\) 인 경우와 \(\nabla g_{i} \lt 0\) 인 경우를 모두 따져봐서<br/> 어떤 경우(방향)가 constraint를 만족하는지 확인해야 한다</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="math"/><category term="Lagrange"/><category term="min"/><category term="max"/><category term="constraint"/><summary type="html"><![CDATA[find min(max) with constraint]]></summary></entry></feed>