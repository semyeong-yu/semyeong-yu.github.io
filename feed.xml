<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://semyeong-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://semyeong-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-21T08:59:23+00:00</updated><id>https://semyeong-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html"></title><link href="https://semyeong-yu.github.io/blog/2025/2025-01-22-MonST3R/" rel="alternate" type="text/html" title=""/><published>2025-01-21T08:59:23+00:00</published><updated>2025-01-21T08:59:23+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/2025-01-22-MonST3R</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/2025-01-22-MonST3R/"><![CDATA[<h2 id="monst3r---a-simple-approach-for-estimating-geometry-in-the-presence-of-motion">MonST3R - A Simple Approach for Estimating Geometry in the Presence of Motion</h2> <h4 id="junyi-zhang-charles-herrmann-junhwa-hur-varun-jampani-trevor-darrell-forrester-cole-deqing-sun-ming-hsuan-yang">Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, Ming-Hsuan Yang</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2410.03825">https://arxiv.org/abs/2410.03825</a><br/> project website :<br/> <a href="https://monst3r-project.github.io/">https://monst3r-project.github.io/</a></p> </blockquote> <h2 id="contribution">Contribution</h2> <p>TBD</p>]]></content><author><name></name></author></entry><entry><title type="html">Dynamic Gaussian Marbles</title><link href="https://semyeong-yu.github.io/blog/2025/GaussianMarbles/" rel="alternate" type="text/html" title="Dynamic Gaussian Marbles"/><published>2025-01-16T12:00:00+00:00</published><updated>2025-01-16T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/GaussianMarbles</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/GaussianMarbles/"><![CDATA[<h2 id="dynamic-gaussian-marbles-for-novel-view-synthesis-of-casual-monocular-videos">Dynamic Gaussian Marbles for Novel View Synthesis of Casual Monocular Videos</h2> <h4 id="colton-stearns-adam-harley-mikaela-uy-florian-dubost-federico-tombari-gordon-wetzstein-leonidas-guibas">Colton Stearns, Adam Harley, Mikaela Uy, Florian Dubost, Federico Tombari, Gordon Wetzstein, Leonidas Guibas</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2406.18717">https://arxiv.org/abs/2406.18717</a><br/> project website :<br/> <a href="https://geometry.stanford.edu/projects/dynamic-gaussian-marbles.github.io/">https://geometry.stanford.edu/projects/dynamic-gaussian-marbles.github.io/</a></p> </blockquote> <h2 id="contribution">Contribution</h2> <ul> <li>Dynamic 4D Gaussian Splatting : <ul> <li>이전까지는 multiple simultaneous viewpoints of a scene 세팅 (dense multi-camera setup)에서의 recon. 논문들이 많았음<br/> \(\rightarrow\)<br/> 평소의 casual <code class="language-plaintext highlighter-rouge">monocular video</code>로 4D recon.을 수행해보자!</li> <li>input에 multi-view info.가 없는 underconstrained monocular video더라도<br/> prior (<code class="language-plaintext highlighter-rouge">careful optimization strategy</code> 및 <code class="language-plaintext highlighter-rouge">off-the-shelf depth and motion estimation</code> 및 <code class="language-plaintext highlighter-rouge">geometry-based regularization</code>) 이용해서<br/> 적절한 constraint를 복원할 수 있다!</li> </ul> </li> <li>Dynamic Gaussian Marbles :<br/> monocular setting의 어려움을 해결하기 위해 GS에서 세 가지 사항을 변경<br/> 이를 통해 Gaussian trajectories를 학습할 수 있음 <ul> <li>isotropic Gaussian Marbles :<br/> <code class="language-plaintext highlighter-rouge">isotropic</code> Gaussian을 사용함으로써<br/> Gaussian의 <code class="language-plaintext highlighter-rouge">degrees of freedom을 줄이고</code><br/> <code class="language-plaintext highlighter-rouge">local shape보다는 motion과 apperance</code> 표현하는 데 더 집중하도록 제한</li> <li>hierarchical divide-and-conquer learning strategy :<br/> time 길이가 어느 정도 짧아야 잘 포착할 수 있으므로 <br/> long video를 short <code class="language-plaintext highlighter-rouge">subsequences로 나누고 optimize by iteratively merging the subsequences</code><br/> (long-sequence tracking 대신 인접한 subsequences를 붙이는 task로!) <ul> <li>procedure :<br/> 아래의 과정을 반복하며 locality와 global coherence를 모두 챙김! <ul> <li><code class="language-plaintext highlighter-rouge">motion estimation</code> :<br/> \(G^{b}\) 의 frame을 \(G^{a}\) 의 trajectory에 하나씩 더해 가며 motion estimation을 수행하므로 <d-cite key="Dynamic3DGS">[1]</d-cite> 처럼 <code class="language-plaintext highlighter-rouge">locality</code>와 smoothness로부터 benefit</li> <li><code class="language-plaintext highlighter-rouge">merge</code></li> <li><code class="language-plaintext highlighter-rouge">global adjustment</code> : <d-cite key="4DGS">[2]</d-cite> 처럼 <code class="language-plaintext highlighter-rouge">global coherence</code>라는 benefit</li> </ul> </li> </ul> </li> <li>prior :<br/> monocular video로도 recon. 잘 수행하기 위해 prior 이용 <ul> <li><code class="language-plaintext highlighter-rouge">image(2D)-space prior</code> : SAM (Rendering loss-segmentation), CoTracker (Tracking loss), DepthAnything (Rendering loss-depthmap)</li> <li><code class="language-plaintext highlighter-rouge">geometry(3D)-space prior</code> : regularization of Gaussian trajectories with rigidity (Isometry loss) and Chamfer priors (3D Alignment loss)</li> </ul> </li> </ul> </li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li> <p>Gaussian Splatting :<br/> TBD</p> </li> <li> <p>Dynamic Gaussian Splatting :<br/> TBD</p> </li> <li> <p>Other Dynamic Nerual Scene Representations :<br/> TBD</p> </li> </ul> <h2 id="method">Method</h2> <h3 id="overview">Overview</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/1.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/1.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/1.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/1.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/3.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/3.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="dynamic-gaussian-marbles">Dynamic Gaussian Marbles</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/2.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/2.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> simpler Gaussian Marble의 경우에만 generalize well to novel view </div> <ul> <li>Gaussian marble : <ul> <li><code class="language-plaintext highlighter-rouge">isotropic</code> :<br/> \(R = I\) and \(S = s \in R^{1}\) <ul> <li>anisotropic Gaussian은 expensive할 뿐만 아니라<br/> underconstrained monocular cam. setting에서는 오히려 degrees of freedom 많으면 poor하다는 걸 실험적으로 발견</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">semantic instance</code> :<br/> assign each Gaussian marble to semantic instance \(y \in N\) by SAM-driven TrackAnything</li> <li><code class="language-plaintext highlighter-rouge">dynamic trajectory</code> :<br/> trajectory \(\Delta X \in R^{T \times 3}\) : a sequence of translations which maps marble’s position change at each timestep</li> </ul> </li> </ul> <h3 id="divide-and-conquer-motion-estimation">Divide-and-Conquer Motion Estimation</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/3.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/3.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Training Procedure : <ul> <li>Step 1) <code class="language-plaintext highlighter-rouge">initialization for each frame</code><br/> initialize Gaussian marbles \([G_{11}, G_{22}, \ldots, G_{TT}]\) for each frame<br/> (initial marbles \(G_{ii}\) have trajectory length 1) <ul> <li>Step 1-1) obtain prior (<code class="language-plaintext highlighter-rouge">depthmap</code> and <code class="language-plaintext highlighter-rouge">segmentation</code>)<br/> obtain monocular (LiDAR) depthmap and segmentation from SAM-driven TrackAnything <d-cite key="TrackAnything">[3]</d-cite></li> <li>Step 1-2) <code class="language-plaintext highlighter-rouge">unproject</code> from 2D to 3D<br/> unproject the depthmap into point cloud<br/> perform outlier removal and downsampling</li> <li>Step 1-3) initialize Gaussian marbles and trajectory <ul> <li>Gaussian marbles : <ul> <li>mean \(\mu \in R^{3}\) : Step 1-2)에서 얻은 pcd</li> <li>color \(c \in R^{3}\) : pixel color (pixel-aligned Gaussians)</li> <li><code class="language-plaintext highlighter-rouge">instance class</code> \(y \in R^{1}\) : Step 1-1)에서 얻은 segmentation</li> <li>scale \(s \in R^{1}\) and opacity \(\alpha \in R^{1}\) : 3DGS 논문에서 했던대로 초기화</li> </ul> </li> <li>trajectory : <ul> <li><code class="language-plaintext highlighter-rouge">trajectory</code> : \(\Delta X = [\boldsymbol 0] \in R^{T \times 3}\)</li> </ul> </li> </ul> </li> </ul> </li> <li>Step 2) bottom-up divide-and-conquer merge<br/> merge short-trajectories into longer trajectories<br/> e.g. \(G = [G_{12}, G_{34}, G_{56}, G_{78}] \rightarrow G = [G_{14}, G{58}]\) <ul> <li>Step 2-1) <code class="language-plaintext highlighter-rouge">motion estimation</code> <ul> <li>Step 2-1-1) make a pair b.w. adjacent marbles <ul> <li>adjacent Gaussian marble set끼리 a pair로 묶음<br/> e.g. \([(G_{12}^{a}, G_{34}^{b}), (G_{56}^{a}, G_{78}^{b})]\)</li> <li>\(G^{a}\) 는 merge할 prev. frames’ Gaussians이고,<br/> \(G^{b}\) 는 merge할 next frames’ Gaussians</li> </ul> </li> <li>Step 2-1-2) \(G^{a}\) 의 trajectory 확장 <ul> <li>goal :<br/> \(G_{12}^{a}\) 의 trajectory인 \(\Delta X = [\Delta X_{1}, \Delta X_{2}]\) 는 이미 학습되어 merge된 motion이고,<br/> \(G_{12}^{a}\) 의 trajectory와 \(G_{34}^{b}\) 의 frame \(3\) 을 잇는 motion \(\Delta X_{3}\) 을 학습해야 함!</li> <li>constant-velocity assumption에 따라 trajectory를 \(\Delta X = [\Delta X_{1}, \Delta X_{2}, \Delta X_{3}^{init}]\) 로 확장</li> </ul> </li> <li>Step 2-1-3) trajectory optimization <ul> <li>\(G_{12}^{a}\) 를 frame \(3\) 에 render한 뒤<br/> \(\Delta X_{3}\) 이 frame \(3\) 으로의 motion을 잘 반영하도록 \(\Delta X_{3}\) 을 업데이트<br/> (\(\eta\) 번 반복 by 아래에서 설명할 Loss)</li> </ul> </li> <li>Step 2-1-4) repeat <ul> <li>Step 2-1-2), Step 2-1-3)을 반복<br/> until \(G^{a}\) 의 trajectory가 \(G_{34}^{b}\) 내 모든 frames를 커버할 때까지</li> <li>e.g. \(G^{a}\) 의 trajectory를 \(\Delta X = [\Delta X_{1}, \Delta X_{2}, \Delta X_{3}, \Delta X_{4}^{init}]\) 로 확장한 뒤<br/> \(G^{a}\) 를 frame \(4\) 에 render한 뒤<br/> \(\Delta X_{4}\) 가 frame \(4\) 으로의 motion을 잘 반영하도록 \(\Delta X_{4}\) 을 업데이트</li> </ul> </li> </ul> </li> <li>Step 2-2) <code class="language-plaintext highlighter-rouge">merge</code> <ul> <li>motion estimation을 거치고 나면 \(G_{ij}^{a}\) 와 \(G_{ij}^{b}\) 가 같은 frame subsequence \([i, j]\) 를 recon.할 것이므로<br/> merge by just union \(G_{ij} = G_{ij}^{a} \cup G_{ij}^{b}\)<br/> (set size 2배 됨)</li> <li>computational load 줄이기 위해<br/> opacity 또는 scale이 너무 작은 Gaussians는 drop하고,<br/> random downsampling 수행하여<br/> set size를 constant하게 유지</li> </ul> </li> <li>Step 2-3) <code class="language-plaintext highlighter-rouge">global adjustment</code> <ul> <li>merge로 합치고 나서도 still optimized라는 보장이 없기 때문에<br/> newly merged Gaussians를 모두 jointly optimize</li> <li>merged set가 \(G_{ij}\) 라고 했을 때<br/> \([i, j]\) 내 a frame을 randomly sampling하고<br/> \(G_{ij}\) 의 모든 Gaussians를 해당 frame에 render한 뒤<br/> Gaussian \(c, s, \alpha, \Delta X\) 을 업데이트<br/> (\(\beta\) 번 반복)</li> <li>그럼 merged Gaussians \(G_{ij}\) 가 global Gaussians로 인정받을 수 있음!</li> </ul> </li> </ul> </li> </ul> </li> <li>Inference Procedure : <ul> <li>learned Gaussian trajectories 이용해서<br/> render (roll out) into specific timestep \(t\)</li> </ul> </li> </ul> <h3 id="loss">Loss</h3> <ul> <li><code class="language-plaintext highlighter-rouge">Tracking</code> Loss :<br/> \(L_{track} = \sum_{p \in P} \sum_{g \in N(p_{i})} \alpha_{i}^{'} \| D_{i} \| \mu_{i}^{'} - p_{i} \| - D_{j} \| \mu_{j}^{'} - p_{j} \| \|\)<br/> where \(\mu_{i}^{'}\) and \(D_{i}\) : mean and depth of projected 2D Gaussian<br/> where \(P\) : tracked points by CoTracker <d-cite key="CoTracker">[4]</d-cite><br/> where \(N(p_{i})\) : tracked point \(p_{i}\) 와의 the nearest 3D Gaussians<br/> where \(\alpha_{i}^{'}\) : Gaussian’s opacity <ul> <li>goal :<br/> <code class="language-plaintext highlighter-rouge">2D point track</code>인 CoTracker <d-cite key="CoTracker">[4]</d-cite> (2D prior)를 사용하여<br/> <code class="language-plaintext highlighter-rouge">Gassian marble trajectories</code>를 regularize</li> <li>Step 1)<br/> \(G^{b}\) 의 frame \(j\) 로의 \(\Delta X_{j}\) 를 optimize하고자 할 때,<br/> CoTracker <d-cite key="CoTracker">[4]</d-cite> 를 이용하여 frames \([j - w , j + w]\) (\(w = 12\))에서의 point tracks \(P\) 를 estimate<br/> (from 2D frame to 2D frame)<br/> (일종의 GT로 사용)</li> <li>Step 2)<br/> a source frame \(i \in [j - w , j + w]\) 을 randomly sampling</li> <li>Step 3)<br/> Gaussian marble trajectory \(\Delta X\) 로부터 frame \(i\) 와 frame \(j\) 에서의 3DGS position을 sampling하고<br/> 3DGS를 2D Gaussian in image plane으로 project시켜 2D mean, depth, covariance 구함</li> <li>Step 4)<br/> Step 1)의 tracked point \(p_{i \rightarrow j}\) 와 가장 가까운 \(K = 32\) 개의 Step 3)의 2D Gaussians를 구한 뒤<br/> <code class="language-plaintext highlighter-rouge">2D tracked point와 2D Gaussian 사이의 거리</code>가 frame \(i\), \(j\) 에서 거의 <code class="language-plaintext highlighter-rouge">일정하게 유지되도록</code> loss term 걸어줌<br/> (for <code class="language-plaintext highlighter-rouge">temporal consistency</code>)</li> </ul> </li> <li>Rendering Loss : <ul> <li><code class="language-plaintext highlighter-rouge">image</code> rendering하여<br/> GT image와의 L1 loss 및 LPIPS loss 구함</li> <li><code class="language-plaintext highlighter-rouge">disparity map</code> rendering하여<br/> initial disparity estimation과의 L1 loss 구함</li> <li><code class="language-plaintext highlighter-rouge">segmentation map</code> rendering하여<br/> SAM(off-the-shelf instance segmentation)과의 L1 loss 구함</li> </ul> </li> <li>Geometry Loss : <ul> <li><code class="language-plaintext highlighter-rouge">Local Isometry</code> Loss :<br/> \(L_{iso-local} = \sum_{g^{a} \in G} \sum_{g^{b} \in N(g^{a})} | \| \mu_{i}^{a} - \mu_{i}^{b} \| - \| \mu_{j}^{a} - \mu_{j}^{b} \| |\) <ul> <li>goal :<br/> prev. works <d-cite key="Dynamic3DGS">[2]</d-cite>, <d-cite key="DynamicPointFields">[5]</d-cite> 에서처럼<br/> Gaussian marbles가 <code class="language-plaintext highlighter-rouge">locally rigid motion</code>을 따르도록 regularize</li> <li>Step 1)<br/> \(G^{b}\) 의 frame \(j\) 로의 \(\Delta X_{j}\) 를 optimize하고자 할 때,<br/> 우선 a source timestep \(i \in [j - 1 , j + 1]\) 을 randomly sampling</li> <li>Step 2)<br/> 3DGS \(g^{a} \in G\) 및 이와 가까운 3DGS들 \(g^{b} \in N(g^{a})\) 에 대해<br/> <code class="language-plaintext highlighter-rouge">nearest 3D Gaussians끼리의 거리</code>가 frame \(i\), \(j\) 에서 거의 <code class="language-plaintext highlighter-rouge">일정하게 유지되도록</code> loss term 걸어줌</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Instance Isometry</code> Loss :<br/> \(L_{iso-instance} = \sum_{g^{a} \in G} \sum_{g^{b} \in Y(g^{a})} | \| \mu_{i}^{a} - \mu_{i}^{b} \| - \| \mu_{j}^{a} - \mu_{j}^{b} \| |\) <ul> <li>goal :<br/> 각 semantic instance가 일관적으로 움직이도록 regularize</li> <li>Step 1)<br/> \(G^{b}\) 의 frame \(j\) 로의 \(\Delta X_{j}\) 를 optimize하고자 할 때,<br/> 우선 a source timestep \(i \in [j - 1 , j + 1]\) 을 randomly sampling</li> <li>Step 2)<br/> 3DGS \(g^{a} \in G\) 및 이와 semantic label이 같은 3DGS들 \(g^{b} \in Y(g^{a})\) 에 대해<br/> <code class="language-plaintext highlighter-rouge">semantic label이 같은 3D Gaussians끼리의 거리</code>가 frame \(i\), \(j\) 에서 거의 <code class="language-plaintext highlighter-rouge">일정하게 유지되도록</code> loss term 걸어줌</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">3D Alignment</code> Loss :<br/> \(L_{chamfer} = \sum_{g^{1} \in G^{1}} \text{min}_{g^{2} \in G^{2}} \| \mu^{1} - \mu^{2} \| + \sum_{g^{2} \in G^{2}} \text{min}_{g^{1} \in G^{1}} \| \mu^{1} - \mu^{2} \|\) <ul> <li>goal :<br/> merge하고나서 <code class="language-plaintext highlighter-rouge">Global Adjustment</code>할 때 a frame에 전부 rendering해서 optimize하므로 <code class="language-plaintext highlighter-rouge">projected 2D image plane 상에서는 align</code> 되어 있음<br/> 그런데 merge하고나서 <code class="language-plaintext highlighter-rouge">3D space 상에서도 align</code>할 필요 있음<br/> (만약에 3D alignment 하지 않으면 3D and novel-view 상에서 <code class="language-plaintext highlighter-rouge">cloudy artifacts</code> 생김)<br/> (off-the-shelf depth estimation이 time에 따라 inconsistent할 경우 이와 같은 상황 발생)</li> <li>Step 1)<br/> 두 pcd 집합을 서로 가깝게 만드는 Chamfer loss를 적용할 건데,<br/> merge할 Gaussian set \(G^{a}\) 와 \(G^{b}\) 는 scene의 명확히 서로 다른 부분을 관측하고 있으므로<br/> 둘 사이에 Chamfer loss를 바로 적용하면 안 됨</li> <li>Step 2)<br/> set \(G_{12}^{a}\) 와 \(G_{34}^{b}\) 를 single frame의 subsets \([G_{1}^{a}, G_{2}^{a}, G_{3}^{b}, G_{4}^{b}]\) 로 나눔<br/> where \(G_{1}^{a}\) contains Gaussians initialized from frame \(1\)</li> <li>Step 3)<br/> 해당 subsets list를 random shuffle한 뒤<br/> 맨 앞의 25%는 set \(G^{1}\) 으로 묶고, 다음 25%는 set \(G^{2}\) 로 묶음<br/> (\(G^{1}\) 과 \(G^{2}\) 가 <code class="language-plaintext highlighter-rouge">scene의 어떤 부분을 보고 있는지 명확히 정해지지 않도록 randomness 부여</code>)<br/> (만약 이렇게 randomness 부여하지 않는다면 observed scene content difference에 overfitting될 수 있음 <code class="language-plaintext highlighter-rouge">???</code>)</li> <li>Step 4)<br/> \(G^{1}\) 과 \(G^{2}\) 에 대해 2-way Chamfer distance 계산</li> </ul> </li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <h3 id="dataset">Dataset</h3> <ul> <li>training :<br/> 아래의 두 가지 datasets는 multi-view info.를 포함하고 있으므로<br/> monocular setting을 모방하기 위해<br/> training and evaluation protocol을 수정 <code class="language-plaintext highlighter-rouge">어떻게 ???</code> <ul> <li>NVIDIA Dynamic Scenes Dataset :<br/> 7 videos</li> <li>DyCheck iPhone Dataset</li> </ul> </li> <li>test : <ul> <li>Total-Recon Dataset</li> <li>Davis Dataset</li> <li>YouTube-VOS Dataset</li> <li>real-world videos</li> </ul> </li> </ul> <h3 id="implementation">Implementation</h3> <ul> <li>Implementation : <ul> <li>TBD</li> </ul> </li> <li>Runtime and Memory : <ul> <li>TBD</li> </ul> </li> </ul> <h3 id="results">Results</h3> <ul> <li>Dynamic Novel View Synthesis : <ul> <li>TBD</li> </ul> </li> <li>Tracking and Editing : <ul> <li>TBD</li> </ul> </li> </ul> <h3 id="ablation-study">Ablation Study</h3> <p>TBD</p> <h2 id="conclusion">Conclusion</h2> <ul> <li>Limitation : <ul> <li>TBD</li> </ul> </li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br/> Gaussian의 motion을 학습하는 것이라면 없던 object가 등장하거나 원래 있던 object가 frame 밖으로 벗어나는 경우에도 잘 대응할 수 있는지?</p> </li> <li> <p>A1 :<br/> TBD</p> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="Gaussian"/><category term="Marble"/><category term="degree"/><category term="freedom"/><category term="dynamic"/><category term="view"/><category term="synthesis"/><summary type="html"><![CDATA[Dynamic Gaussian Marbles for Novel View Synthesis of Casual Monocular Videos (SIGGRAPH 2024)]]></summary></entry><entry><title type="html">Feed Forward Bullet Time Reconstruction</title><link href="https://semyeong-yu.github.io/blog/2025/BTimer/" rel="alternate" type="text/html" title="Feed Forward Bullet Time Reconstruction"/><published>2025-01-10T12:00:00+00:00</published><updated>2025-01-10T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/BTimer</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/BTimer/"><![CDATA[<h2 id="feed-forward-bullet-time-reconstruction-of-dynamic-scenes-from-monocular-videos">Feed Forward Bullet Time Reconstruction of Dynamic Scenes from Monocular Videos</h2> <h4 id="hanxue-liang-jiawei-ren-ashkan-mirzaei-antonio-torralba-ziwei-liu-igor-gilitschenski-sanja-fidler-cengiz-oztireli-huan-ling-zan-gojcic-jiahui-huang">Hanxue Liang, Jiawei Ren, Ashkan Mirzaei, Antonio Torralba, Ziwei Liu, Igor Gilitschenski, Sanja Fidler, Cengiz Oztireli, Huan Ling, Zan Gojcic, Jiahui Huang</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2412.03526">https://arxiv.org/abs/2412.03526</a><br/> project website :<br/> <a href="https://research.nvidia.com/labs/toronto-ai/bullet-timer/">https://research.nvidia.com/labs/toronto-ai/bullet-timer/</a></p> </blockquote> <h2 id="contribution">Contribution</h2> <ul> <li>model : <ul> <li><code class="language-plaintext highlighter-rouge">motion-aware</code> <code class="language-plaintext highlighter-rouge">feed-forward</code> model for real-time recon. and novel-view-synthesis of <code class="language-plaintext highlighter-rouge">dynamic</code> scenes</li> <li>obtain <code class="language-plaintext highlighter-rouge">scalability</code> and <code class="language-plaintext highlighter-rouge">generalization</code> by using both static and dynamic scene datasets<br/> (static and dynamic recon.에 모두 사용 가능)</li> <li>Procedure : <ul> <li>Step 1) pre-train on large static scene dataset</li> <li>Step 2) video duration or FPS에 구애받지 않고 scale effectively across datasets</li> <li>Step 3) output multi-view volumetric video representation</li> </ul> </li> <li>recon. a bullet-time scene within 150ms with SOTA performance on a single GPU<br/> from 12 context frames of \(256 \times 256\) resolution</li> <li>bullet (target) timestamp \(t_{b}\) 를 원하는 each timestamp in video로 설정하면<br/> full video를 recon.할 수 있으므로<br/> 각 frame을 <code class="language-plaintext highlighter-rouge">parallel</code>하게 recon. 가능</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/2m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/2m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/2m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/2m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">BulletTimer</code> (Novelty 1.) :<br/> main model <ul> <li>recon. at <code class="language-plaintext highlighter-rouge">arbitrary</code> target (bullet) timestamp and <code class="language-plaintext highlighter-rouge">arbitrary</code> novel-view<br/> by adding <code class="language-plaintext highlighter-rouge">bullet-time embedding</code> to all the context (input) frames<br/> and <code class="language-plaintext highlighter-rouge">aggregating</code> pred. from all the context frames</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">NTE Module</code> (Novelty 2.) :<br/> <code class="language-plaintext highlighter-rouge">pre-processing</code> (FPI) <ul> <li><code class="language-plaintext highlighter-rouge">fast motion</code>에 대응하기 위해<br/> model에 feed하기 전에<br/> <code class="language-plaintext highlighter-rouge">intermediate (interpolated) frames를 predict</code></li> <li>inference할 때<br/> arbitrary target (bullet) timestamp에 대해<br/> recon.할 수 있도록 도움</li> </ul> </li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li> <p>Dynamic scene recon. from monocular video :<br/> still challenging<br/> due to inherently ill-posed (해가 무수히 많음) nature of dynamic recon. from limited observations</p> </li> <li> <p>Static scene recon. :</p> <ul> <li>optimization-based (per-scene) :<br/> NeRF, HyperNeRF</li> <li>learning-based (feed-forward) :<br/> MonoNeRF, GS-LRM</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/5m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/5m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/5m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/5m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Dynamic scene recon. :<br/> dynamic scene은 complex motion 때문에 ambiguity 존재<br/> 이를 해소하는 데 도움될 data prior 필요 <ul> <li>optimization-based (per-scene) : <ul> <li>use contraint (data prior) like depth and optical flow <d-cite key="33">[1]</d-cite>, <d-cite key="36">[2]</d-cite>, <d-cite key="37">[3]</d-cite>, <d-cite key="68">[4]</d-cite><br/> \(\rightarrow\)<br/> given data와 위의 data prior 간의 싱크를 맞추는 게 challenging <d-cite key="34">[5]</d-cite>, <d-cite key="63">[6]</d-cite></li> <li>per-scene approach는 time-consuming and thus scale 어렵</li> </ul> </li> <li>learning-based (<code class="language-plaintext highlighter-rouge">feed-forward</code>) : <ul> <li>directly predict recon. in feed-forward manner<br/> so, can <code class="language-plaintext highlighter-rouge">learn strong inherent prior directly from data</code> <d-cite key="7">[7]</d-cite>, <d-cite key="10">[8]</d-cite>, <d-cite key="12">[9]</d-cite>, <d-cite key="25">[10]</d-cite>, <d-cite key="53">[11]</d-cite></li> <li>근데 dynamic scene 모델링하기 복잡하고 4D supervision data 부족해서 적용하는데 한계 있었음</li> <li>지금 시점 기준 L4GM <d-cite key="53">[11]</d-cite> 이 유일한 feed-forward dynamic recon. model인데,<br/> synthetic object-centric dataset으로 훈련돼서<br/> fixed camera view-point와 multi-view supervision을 필요로 한다는 한계와<br/> real-world scene에 generalize하기 어렵다는 한계가 있었음</li> </ul> </li> </ul> </li> <li>Feed-Forward Dynamic scene recon. : <ul> <li>본 논문은<br/> <code class="language-plaintext highlighter-rouge">pixel-aligned 3DGS</code> <d-cite key="79">[12]</d-cite> 를 기반으로<br/> novel BulletTimer and NTE module 제안</li> </ul> </li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li>Dynamic 3D Representation : <ul> <li>TBD</li> </ul> </li> <li>Novel-View-Synthesis : <ul> <li>TBD</li> </ul> </li> <li>Feed-Forward Reconstruction : <ul> <li>TBD</li> </ul> </li> </ul> <h2 id="overview">Overview</h2> <ul> <li> <p>notation :<br/> context frames \(I_{c} \subset I\)<br/> camera poses \(P_{c} \subset P\)<br/> context timestamps \(T_{c} \subset T\)<br/> bullet timestamp \(t_{b} \in [\text{min}(T_{c}), \text{max}(T_{c})]\)<br/> recon. at timestamp \(t \notin T\) by NTE module</p> </li> <li> <p>Architecture :</p> <ul> <li>Training :<br/> BTimer와 NTE Module을 별도로 각각 train<br/> (not end-to-end)</li> <li>Inference :<br/> NTE Module로 pre-process한 뒤<br/> BTimer 사용</li> </ul> </li> </ul> <h2 id="method">Method</h2> <h3 id="btimer-bullet-timer">BTimer (Bullet Timer)</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/3m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/3m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/3m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/3m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Model Design : <ul> <li>encode (input) :<br/> \(i\)-th frame \(I_{i} \in I_{c}\) 을 \(8 \times 8\) 짜리 patches로 나눈 뒤<br/> \(j\)-th patch에 대해<br/> per-patch input token \(f_{ij} |_{j=1}^{HW / 64} = f_{ij}^{rgb} + f_{ij}^{pose} + f_{i}^{time}\) 만든 뒤<br/> concatenate input tokens from all context frames<br/> and feed into Transformer <ul> <li><code class="language-plaintext highlighter-rouge">image</code> encoder :<br/> GS-LRM <d-cite key="79">[12]</d-cite> 에서 영감을 받아,<br/> <code class="language-plaintext highlighter-rouge">ViT</code> model을 backbone으로 사용</li> <li><code class="language-plaintext highlighter-rouge">camera pose</code> encoder :<br/> <code class="language-plaintext highlighter-rouge">camera Plucker embedding</code> <d-cite key="70">[13]</d-cite></li> <li><code class="language-plaintext highlighter-rouge">time</code> encoder :<br/> PE (Positional Encoding) 및 linear layer를 거쳐<br/> \(t_{i}\) 와 \(t_{b}\) 를 각각 \(f_{i}^{ctx}\) 와 \(f_{i}^{bullet}\) 으로 encode한 뒤<br/> \(f_{i}^{time} = f_{i}^{ctx} + f_{i}^{bullet}\) <ul> <li><code class="language-plaintext highlighter-rouge">context (input)</code> timestamp \(t_{i}\) from context (input) frame \(I_{i}\)</li> <li><code class="language-plaintext highlighter-rouge">bullet (target)</code> timestamp \(t_{b}\) that is <code class="language-plaintext highlighter-rouge">shared</code> across context (input) frames</li> </ul> </li> </ul> </li> <li>decode (output) :<br/> transformer의 per-patch output token \(f_{ij}^{out}\) 을<br/> <code class="language-plaintext highlighter-rouge">per-patch 3DGS param. at bullet timestamp</code> \(G_{ij} \in R^{8 \times 8 \times 12}\) 로 regression <ul> <li>each Gaussian has 12 param. as color \(c \in R^{3}\), scale \(s \in R^{3}\), rotation unit quaternion \(q \in R^{4}\), opacity \(\sigma \in R\), and ray distance \(\tau \in R\)</li> <li>3D position is obtained by pixel-aligned unprojection \(\mu = o + \tau d\)<br/> (\(o\) and \(d\) are obtained from camera pose \(P_{i}\))</li> </ul> </li> </ul> </li> <li> <p>Loss :<br/> \(L_{RGB} = L_{MSE} + \lambda L_{LPIPS}\) with \(\lambda = 0.5\)</p> </li> <li>Timestamp :<br/> context (input) frames와 bullet (target supervision) frame 을 잘 고르는 게 중요 <ul> <li><code class="language-plaintext highlighter-rouge">In-context Supervision</code> : <ul> <li>bullet timestamp is randomly selected from context frames<br/> \(t_{b} \in T_{c}\)</li> <li>model이 context timestamp에 대해 정확히 recon. 가능하도록</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Interpolation Supervision</code> : <ul> <li>bullet timestamp lies between two adjacent context frames<br/> \(t_{b} \notin T_{c}\)</li> <li>model이 dynamic parts를 interpolate할 수 있도록</li> <li>pixel-aligned 3DGS의 inductive bias 때문에<br/> motion이 복잡하고 빠를 때 intermediate timestamp에 대해 예측 잘 못 함<br/> \(\rightarrow\)<br/> 먼저 NTE module의 도움을 받아 pre-process한 뒤<br/> BTimer 사용</li> <li>local minimum 방지 및 view 간 consistency 상승</li> </ul> </li> </ul> </li> <li>Inference : <ul> <li>bullet (target) timestamp \(t_{b}\) 를 원하는 each timestamp in video로 설정하면<br/> full video를 recon.할 수 있으므로<br/> 각 frame을 <code class="language-plaintext highlighter-rouge">parallel</code>하게 recon. 가능</li> <li><code class="language-plaintext highlighter-rouge">???</code><br/> For a video longer than the number of training context views \(| I_{c} |\),<br/> at timestamp \(t\), apart from including this exact timestamp and setting \(t_{b} = t\),<br/> we uniformly distribute the remaining \(| I_{c} | − 1\) required context frames across the whole duration of the video<br/> to form the input batch with \(| I_{c} |\) frames</li> </ul> </li> </ul> <h3 id="nte-module-novel-time-enhancer">NTE Module (Novel Time Enhancer)</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/4m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/4m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/4m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/4m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>NTE Module Design :<br/> decoder-only LVSM <d-cite key="27">[13]</d-cite> 에서 영감을 받아,<br/> BTimer model과 구조 똑같지만,<br/> I/O가 다름 <ul> <li>input : <ul> <li>context frame : <ul> <li>context (input) timestamp embedding<br/> (BTimer model과 달리 bullet timestamp embedding은 안 넣음)</li> <li>camera pose Plucker embedding</li> <li>context (input) frame</li> </ul> </li> <li>intermediate frame : <ul> <li>bullet (target) timestamp embedding</li> <li>target camera pose Plucker embedding</li> </ul> </li> </ul> </li> <li>output : <ul> <li>Transformer의 per-patch output tokens 중 target token을<br/> unpatchify and project directly to RGB values by linear layer<br/> \(\rightarrow\)<br/> RGB frame for any bullet (target) timestamp<br/> (this RGB frame은 NTE module network의 direct output이고, 3DGS로 rendering한 게 아님!!)<br/> \(\rightarrow\)<br/> NTE Module의 output은<br/> BTimer에서 bullet timestamp의 image로 쓰임</li> </ul> </li> <li>Implementation : <ul> <li>LVSM <d-cite key="27">[13]</d-cite> 에서처럼<br/> 안정적인 훈련을 위해 <code class="language-plaintext highlighter-rouge">QK-norm</code> 사용<br/> (Q와 K의 내적 과정에서 값이 너무 크거나 작으면 gradient explode or vanish 발생할 수 있으므로<br/> Q와 K를 normalize)</li> <li>target token에 attention할 수 있도록<br/> <code class="language-plaintext highlighter-rouge">masked attention</code> 사용</li> <li> <d-cite key="50">[14]</d-cite> <p>에서처럼<br/> 빠른 inference를 위해 <code class="language-plaintext highlighter-rouge">KV-Cache</code> 사용<br/> (training할 때는 전체 input sequence에 대해 K, Q, V를 계산하지만,<br/> inference할 때는 prev. token에서 계산한 K, V를 cache에 저장한 채 매번 Q만 새로 계산함으로써 input sequence 전체에 대해 K, V를 매번 계산할 필요 없어 계산 비용 감소)</p> </li> <li>NTE Module has negligible overhead on runtime</li> </ul> </li> </ul> </li> <li> <p>Loss :<br/> \(L_{RGB} = L_{MSE} + \lambda L_{LPIPS}\) with \(\lambda = 0.5\)</p> </li> <li>BTimer and NTE Module : <ul> <li>NTE Module로 직접 RGB image 예측하여<br/> novel-view-synthesis 할 수 있긴 한데, 그럼 성능 안 좋음<br/> (Ablation Study에 있음)</li> <li>feed-forward transformer (NTE Module)로 FPI pre-process한 뒤<br/> <code class="language-plaintext highlighter-rouge">feed-forward transformer</code> (BTimer)로 data info. 포착하여<br/> <code class="language-plaintext highlighter-rouge">3DGS param. 예측</code>한 뒤<br/> 3DGS rasterization으로 novel-view-synthesis</li> </ul> </li> </ul> <h3 id="curriculum-training-at-scale">Curriculum Training at Scale</h3> <ul> <li>Generalizability : <ul> <li>data 다양성이 많을수록 model generalizability가 높아짐<br/> static dataset은 많이 존재하고<br/> dynamic dataset은 적게 존재하지만 motion awareness 및 temporal consistency 확보 가능</li> <li>본 논문의 model인 BTimer는<br/> generalizable to both static and dynamic scenes <ul> <li>static scene : equalize all \(t_{b}\)</li> <li>dynamic scene : recon. at arbitrary bullet \(t_{b}\)</li> <li>different domain에서는 different model 필요로 하는<br/> GS-LRM <d-cite key="79">[12]</d-cite> or MVSplat <d-cite key="10">[8]</d-cite> 과는 다름</li> </ul> </li> </ul> </li> <li>Curriculum Training : <ul> <li>Stage 1) <code class="language-plaintext highlighter-rouge">Low-res to High-res Static Pretraining</code> <ul> <li>static dataset으로 pre-train <ul> <li>both synthetic and real-world</li> <li>390K training samples</li> <li>normalize datasets to be bounded in \(10^{3}\) cube</li> <li>종류 : <ul> <li>Objaverse</li> <li>RE10K</li> <li>MVImgNet</li> <li>DL3DV</li> </ul> </li> </ul> </li> <li>no time embedding<br/> (static scene이니까)</li> <li>data distribution이 복잡하기 때문에<br/> coarse 세팅 (low-resol.(\(128 \times 128\)) and few-view(\(| I_{c} | = 4\)))에서 시작해서<br/> 점점 fine 세팅 (high-resol.(\(256 \times 256 \rightarrow 512 \times 512\)))으로 train</li> </ul> </li> <li>Stage 2) <code class="language-plaintext highlighter-rouge">Dynamic Scene Co-training</code> <ul> <li>dynamic dataset으로 fine-tuning <ul> <li>종류 : <ul> <li>Kubric</li> <li>PointOdyssey</li> <li>DynamicReplica</li> <li>Spring</li> </ul> </li> </ul> </li> <li>4D dynamic dataset이 부족하기 때문에<br/> 안정적인 훈련을 위해<br/> static dataset을 함께 사용하여 co-training</li> <li>Internet video로부터 camera pose를 매기는 customized pipeline 구축하여<br/> real-world data에 대한 robustness 향상 <ul> <li>먼저 PANDA-70M dataset에서 random select한 video를 20s 길이의 clips로 자르기</li> <li>SAM으로 video의 dynamic objects를 mask out</li> <li>DROID-SLAM으로 video camera pose를 estimate</li> <li>reprojection error 측정하여 low-quality의 video 및 pose는 필터링</li> <li>최종적으로 obtain 40K clips with high-quality camera trajectories</li> </ul> </li> </ul> </li> <li>Stage 3) <code class="language-plaintext highlighter-rouge">Long-context Window Fine-tuning</code> <ul> <li>NTE Module 말고 BTimer model에만 적용</li> <li>context (input) image 수를<br/> \(| I_{c} | = 4\) 에서 \(| I_{c} | = 12\) 로 늘려서<br/> long video recon.하는 데 도움</li> </ul> </li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <h3 id="implementation">Implementation</h3> <ul> <li>Implementation : <ul> <li>Backbone Transformer :<br/> FlashAttention-3 <d-cite key="13">[15]</d-cite> and FlexAttention <d-cite key="24">[16]</d-cite></li> <li>3DGS Rasterization :<br/> gsplat library <d-cite key="74">[17]</d-cite></li> <li>Training Schedule : <ul> <li>BTimer :<br/> totally 4 days on 64 A100 GPUs <ul> <li>Stage 1)<br/> \(128^{2}\) resol. 90K iter. init lr \(4 \times 10^{-4}\)<br/> \(\rightarrow\)<br/> \(256^{2}\) resol. 90K iter. init lr \(2 \times 10^{-4}\)<br/> \(\rightarrow\)<br/> \(512^{2}\) resol. 50K iter. init lr \(1 \times 10^{-4}\)</li> <li>Stage 2)<br/> 10K iter.</li> <li>Stage 3)<br/> 5K iter.</li> </ul> </li> <li>NTE Module : <ul> <li>Stage 1)<br/> \(128^{2}\) resol. 140K iter. init lr \(4 \times 10^{-4}\)<br/> \(\rightarrow\)<br/> \(256^{2}\) resol. 60K iter. init lr \(2 \times 10^{-4}\)<br/> \(\rightarrow\)<br/> \(512^{2}\) resol. 30K iter. init lr \(1 \times 10^{-4}\)</li> <li>Stage 2)<br/> 20K iter.</li> </ul> </li> </ul> </li> <li>Inference time : <ul> <li>BTimer : <ul> <li>20 ms for 4-view \(256^{2}\) recon.</li> <li>150 ms for 12-view \(256^{2}\) recon.</li> <li>4.2 s for 12-view \(512 \times 896\) recon.</li> </ul> </li> <li>NTE : <ul> <li>0.44 s for 4-view \(512 \times 896\) recon. w/o KV cache</li> </ul> </li> </ul> </li> </ul> </li> </ul> <h3 id="results">Results</h3> <ul> <li>Dynamic Novel-View-Synthesis (Quantitative) : <ul> <li>DyCheck Benchmark <d-cite key="22">[18]</d-cite> : <ul> <li>dataset :<br/> DyCheck iPhone dataset (7 dynamic scenes by 3 synchronized cameras)</li> <li>baseline :<br/> TiNeuVox, NSFF, T-NeRF, Nerfies, HyperNeRF, PGDVS, direct depth warp <ul> <li>BTimer는 per-scene optimization method에 competitive performance 달성</li> <li>BTimer는 consistent depth estimate 없이도 PGDVS보다 성능 좋음</li> </ul> </li> </ul> </li> <li>NVIDIA Dynamic Scene Benchmark <d-cite key="75">[19]</d-cite> : <ul> <li>dataset :<br/> NVIDIA Dynamic Scene dataset (9 dynamic scenes by 12 forward-facing synchronized cameras)</li> <li>baseline :<br/> HyperNeRF, DynNeRF, NSFF, RoDynRF, MonoNeRF, 4D-GS, Casual-FVS <ul> <li>feed-forward 방식이므로 optimization time 필요 없음</li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/6m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/6m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/6m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/6m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Dynamic Novel-View-Synthesis (Qualitative) : <ul> <li>test on real-world scene 위해<br/> DAVIS dataset의 monocular videos 이용하고,<br/> customized pipeline으로 camera pose estimate해서 사용</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/8m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/8m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/8m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/8m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/7m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/7m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/7m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/7m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Static Novel-View-Synthesis : <ul> <li>RealEstate10K Benchmark : <ul> <li>baseline : pixelSplat, MVSplat, GPNR, GS-LRM</li> </ul> </li> <li>Tanks &amp; Temples Benchmark :<br/> from InstantSplat Benchmark <ul> <li>baseline : GS-LRM (SOTA)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/10m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/10m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/10m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/10m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> single dataset(Ours-Static)보다 mixed-dataset(Ours-Full) 사용하는 게 generalization 및 성능 훨씬 좋음 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/9m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/9m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/9m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/9m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="ablation-study">Ablation Study</h3> <ul> <li>Ablation 1) Context Frames : <ul> <li>train할 때 context frames 더 많이 쓰면 3DGS prediction이 progressively 많아지므로 more complete scene recon. 가능</li> <li>inference할 때 서로 멀리 떨어진 context frames를 arbitrarily 골라서 커버하는 view 범위 넓힘</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/12m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/12m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/12m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/12m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Ablation 2) Curriculum Training : <ul> <li>Stage 1)<br/> single dataset 말고 multiple dataset 써야<br/> geometry와 sharp detail 잡는 데 도움</li> <li>Stage 2)<br/> static scene을 섞어서 co-train해야<br/> geometry 및 rich detail 잡는 데 도움</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/11m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/11m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/11m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/11m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Ablation 3) Interpolation Supervision : <ul> <li>temporal and multi-view consistency 챙기는 데 도움</li> <li>bullet timestamp가 input frames에 없을 때를 훈련하지 않으면<br/> white-edge artifacts 생김<br/> (interpolation loss를 cheat하려고 camera에 너무 가까운 3DGS를 만들기 때문)</li> </ul> </li> <li>Ablation 4) NTE Module : <ul> <li>motion이 빠르고 복잡할 때 도움<br/> (ghosting artifacts 해소)</li> <li>BTimer 없이<br/> 3D info. 쓰지 않는 NTE Module만으로 novel-view-synthesis 수행하면<br/> input camera trajectory와 먼 novel-view에 대해서는 잘 recon. 못 함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/13m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/13m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/13m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/13m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="conclusion">Conclusion</h2> <ul> <li>Limitation : <ul> <li><code class="language-plaintext highlighter-rouge">geometry</code> : <ul> <li>SOTA depth prediction model <d-cite key="71">[20]</d-cite> 만큼 정확하게<br/> geometry (depth map)을 recover하지는 않음</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">memory issue</code> :<br/> transformer를 사용하다보니 memory 많이 소요 <ul> <li>need 3 days on 64 A100 GPU (40GB VRAM)</li> <li>up to \(512 \times 904\) spatial resol.</li> <li>up to 12 context frames</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">pose</code> : <ul> <li>need camera pose param.</li> <li>future work :<br/> DUSt3R, NoPoSplat처럼 pose-free일 순 없을까?</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">non-generative</code> : <ul> <li>본 논문은 feed-forward Transformer 모델이고<br/> generative model이 아니기 때문에<br/> cannot generate unseen region<br/> (unseen view를 예측하는 view extrapolation 불가능)</li> <li>future work :<br/> generative prior 사용하여 view extrapolation 수행</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">novelty</code> : <ul> <li>사실 arbitrary bullet timestamp를 input token에 추가한 뒤<br/> 모든 input frames를 transformer에 때려넣고 원하는 bullet timestamp에서의 frame을 뽑아내는 video interpolation 방식으로 보이고,<br/> 다만 차이점은 각 frame image를 transformer output으로 구하는 게 아니라 각 frame의 3DGS param.를 transformer output으로 구하는 것이고..<br/> 모델 자체의 novelty보다는 implementation을 잘 해서 결과 좋게 낸 것 같다..</li> <li>(static, dynamic) data를 많이 쓰고 stage 별 training을 통해 높은 performance를 달성할 수 있었고<br/> feed-forward 방식을 통해 빠른 속도를 달성</li> </ul> </li> </ul> </li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br/> NTE Module이 마지막에 linear layer로 RGB value를 예측함으로써<br/> pixel-space에서 RGB image at bullet timestamp 를 interpolate하고<br/> 이를 BTimer에 사용하는데,<br/> latent-space에서 interpolation 다룬 뒤 BTimer에 넘기면 더 성능 좋아질 수 있지 않을까?</p> </li> <li> <p>A1 :<br/> TBD</p> </li> <li> <p>Q2 :<br/> NTE Module이 예측한 pixel-space RGB image가 BTimer의 input으로 들어가는데,<br/> NTE Module output이 부정확하면 drift 연쇄적으로 BTimer의 결과에도 악영향 미칠 거 같아.<br/> refinement, uncertainty(confidence) 등으로 NTE Module output의 부정확성을 감소시켜 성능 높일 수 있을까?</p> </li> <li> <p>A2 :<br/> TBD</p> </li> <li> <p>Q3 :<br/> limitation 중에 unseen view는 recon.하지 못한다는 게 있는데 (view extrapolation 불가능)<br/> 본 논문이 generalizability를 가진다는 말은<br/> static and dynamic unseen dataset (scene)에 대응할 수 있어서인거지?</p> </li> <li> <p>A3 :<br/> TB</p> </li> <li> <p>Q4 :<br/> BTimer와 NTE Module을 각각 별도로 train하므로 not end-to-end인데<br/> end-to-end training할 수는 없을까?</p> </li> <li> <p>A4 :<br/> TBD</p> </li> <li> <p>Q5 :<br/> pixelSplat에서는 a pair of images를 transformer의 input으로 넣어 둘의 관계를 파악하여 static scene recon.하는데<br/> dynamic scene recon.에서는 motion 정보를 캡처해야 하기 때문에<br/> transformer의 input으로 두 장이 아니라 여러 장의 image를 넣어주어야 하는거야?</p> </li> <li> <p>A5 :<br/> TBD</p> </li> <li> <p>Q6 :<br/> interpolation supervision으로 context (input) frame이 아닌 그 사이의 frame에 대해 rendering할 때 GT는 무엇으로 두나요?</p> </li> <li> <p>A6 :<br/> context (input) frame의 image와 camera pose를 직접 interpolate하여 사용 <code class="language-plaintext highlighter-rouge">???</code></p> </li> <li> <p>Q7 :<br/> 다른 논문들을 보면 4DGS처럼 canonical time에 대한 시간에 따른 Gaussian 변화량을 MLP로 학습하거나,<br/> 또는 Dynamic Gaussian Marbles처럼 prev. frame의 GS가 next frame의 GS에 미치는 영향을 학습하기 위해 global adjustment해서 gaussian trajectory를 학습함으로써<br/> GS끼리 정보를 주고받습니다.<br/> 본 논문에서는 모든 input frames를 BTimer에 parallel하게 넣어준 뒤 bullet (target) timestamp마다 3DGS param.를 따로 뽑아내는데<br/> 그럼 3DGS끼리는 정보를 공유하지 않는 건가요?</p> </li> <li> <p>A7 :<br/> 네, 일단 BTimer 이 논문에서는 모든 input frames를 BTimer에 parallel하게 때려넣은 뒤 self-attention에 의존해서 t를 포함한 frames 간의 관계를 학습하는 것 같습니다.</p> </li> <li> <p>Q8 :<br/> 어차피 3DGS끼리 정보를 공유하지 않는 거면 굳이 3DGS를 사용한 이유가 있나요?</p> </li> <li> <p>A8 :<br/> novel-view-synthesis task에서 novel camera pose에 대한 image를 뽑아내려면 3D info.를 이용해야 recon.이 잘 될 것이기 때문에 3DGS를 이용합니다.<br/> 논문에서 언급되어 있듯이 NTE Module만을 이용해서 from 2D to 2D로 novel-view-synthesis task를 수행하면 quality가 좋지 않았다고 합니다.</p> </li> <li> <p>Q9 :<br/> camera pose의 영향을 많이 받을 것 같아요. 만약에 input frame 3에서 보였던 물체가 frame 밖을 벗어나거나 occlusion 때문에 input frame 4에서 안 보이게 되었을 때에도 잘 recon.하려면 prev. frame의 3D info. 정보를 결합해서 반영하는 식이어야 할 것 같은데, 각 bullet timestamp의 3D info.끼리 어떻게 relate되는지에 대한 내용이 없으니까 이와 같은 상황에 잘 대응할 수 있는지 궁금합니다.</p> </li> <li> <p>A9 :<br/> TBD</p> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="general"/><category term="dynamic"/><category term="GS"/><category term="view"/><category term="synthesis"/><summary type="html"><![CDATA[Feed Forward Bullet Time Reconstruction of Dynamic Scenes from Monocular Videos (CVPR 2025)]]></summary></entry><entry><title type="html">CUDA Programming</title><link href="https://semyeong-yu.github.io/blog/2024/CUDA/" rel="alternate" type="text/html" title="CUDA Programming"/><published>2024-12-29T12:00:00+00:00</published><updated>2024-12-29T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/CUDA</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/CUDA/"><![CDATA[<blockquote> <p>Lecture :<br/> 24F EE514 Parallel Computing<br/> by KAIST Minsoo Rhu <a href="https://sites.google.com/view/kaist-via">VIA Research Group</a></p> </blockquote> <h2 id="spmd-programming">SPMD Programming</h2> <ul> <li>GPGPU Programming : <ul> <li>serial part : in CPU C code (host)</li> <li>parallel part : in GPU SPMD kernel code (device)</li> </ul> </li> <li>SPMD (Single Program Multiple Data) : <ul> <li>grid (kernel) \(\supset\) block (SM) \(\supset\) warp \(\supset\) thread <ul> <li>gridDim : grid 내 block 개수</li> <li>blockIdx : block index</li> <li>blockDim : block 내 thread 개수</li> <li>threadIdx : thread index</li> </ul> </li> <li>보통<br/> 1 warp = 32 threads<br/> 1 block = 256 threads</li> <li>block 내 threads끼리 shared memory를 공유</li> </ul> </li> <li>memory address space : <ul> <li>1 address에 1 Byte를 저장하므로<br/> memory address가 32-bit일 때<br/> \(2^{32}\) Byte 저장 가능</li> <li>linear memory address space를 implement하는 것은 복잡</li> </ul> </li> <li>Shared Memory Model <ul> <li>shared var. in shared address space에 저장함으로써 threads끼리 communicate</li> <li>atomicity : threads끼리 겹치지 않도록 mutual exclusion <ul> <li>semaphore</li> <li>mutex : \(\text{LOCK(mylock); //critical section UNLOCK(mylock);}\)</li> <li>atomic : \(\text{atomic{//critical section}}\) 또는 \(\text{atomicAdd(x, 10);}\)</li> </ul> </li> <li>efficient implementation을 위해 hardware support 필요<br/> processor 수가 많으면 costly할 수 있음</li> <li>e.g. OpenMP</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-CUDA/2m.PNG-480.webp 480w,/assets/img/2024-12-29-CUDA/2m.PNG-800.webp 800w,/assets/img/2024-12-29-CUDA/2m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-CUDA/2m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Message Passing Model <ul> <li>thread는 각자 private address space를 가지고 있고<br/> threads끼리 message 주고받음으로써 communicate</li> <li>system-wide load/store를 위한 hardware implementation 필요 없음</li> <li>e.g. Open MPI</li> </ul> </li> </ul> <h2 id="cuda-programming">CUDA Programming</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-CUDA/3m.PNG-480.webp 480w,/assets/img/2024-12-29-CUDA/3m.PNG-800.webp 800w,/assets/img/2024-12-29-CUDA/3m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-CUDA/3m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>CUDA APIs : <ul> <li>\(\text{cudaMalloc()}\) : device(GPU) global memory에 allocate</li> <li>\(\text{cudaFree()}\) : device(GPU) global memory free</li> <li>\(\text{cudaMemcpy()}\) : data transfer between host(CPU) and device(GPU)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-CUDA/4m.PNG-480.webp 480w,/assets/img/2024-12-29-CUDA/4m.PNG-800.webp 800w,/assets/img/2024-12-29-CUDA/4m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-CUDA/4m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>CUDA Function : <ul> <li>\(\text{__global__}\) :<br/> kernel function 정의 (host에서 call해서 device에서 execute) 정의<br/> return void</li> </ul> </li> </ul> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// define kernel func.</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">vecAddKernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span>
<span class="p">{</span>
  <span class="c1">// global rank</span>
  <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>

<span class="c1">// n : global rank</span>
<span class="n">dim3</span> <span class="nf">DimGrid</span><span class="p">((</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mi">256</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="c1">// 256 threads per block</span>
<span class="c1">// DimBlock.x = 256</span>
<span class="n">dim3</span> <span class="nf">DimBlock</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>

<span class="c1">// call kernel func.</span>
<span class="c1">// kernel func.&lt;&lt;#block, #thread&gt;&gt;(param.)</span>
<span class="n">vecAddKernel</span><span class="o">&lt;&lt;</span><span class="n">DimGrid</span><span class="p">,</span> <span class="n">DimBlock</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">d_A</span><span class="p">,</span> <span class="n">d_B</span><span class="p">,</span> <span class="n">d_C</span><span class="p">,</span> <span class="n">n</span><span class="p">);</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-CUDA/5m.PNG-480.webp 480w,/assets/img/2024-12-29-CUDA/5m.PNG-800.webp 800w,/assets/img/2024-12-29-CUDA/5m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-CUDA/5m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>limitation : bottlenecked by global memory bandwidth<br/> \(\rightarrow\)<br/> solution : scratchpad memory (shared memory) <ul> <li>cache : transparent to programmer (it just works)</li> <li>scratchpad : programmer has to manually manage data movement</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-CUDA/6m.PNG-480.webp 480w,/assets/img/2024-12-29-CUDA/6m.PNG-800.webp 800w,/assets/img/2024-12-29-CUDA/6m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-CUDA/6m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>CUDA Variable : <ul> <li>\(\text{int LocalVar;}\) :<br/> <code class="language-plaintext highlighter-rouge">register</code>에 저장하여 thread 혼자서 사용</li> <li>\(\text{(__device__) __shared__ int SharedVar;}\) :<br/> <code class="language-plaintext highlighter-rouge">shared memory</code>에 저장하여 block 내 threads끼리 공유</li> <li>\(\text{__device__ int GlobalVar;}\) :<br/> <code class="language-plaintext highlighter-rouge">global memory</code>에 저장하여 grid 내 모든 threads가 공유</li> <li>\(\text{(__device__) __constant__ int SharedVar;}\) :<br/> <code class="language-plaintext highlighter-rouge">constant memory</code>에 저장하여 grid 내 모든 threads가 공유</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-CUDA/7m.PNG-480.webp 480w,/assets/img/2024-12-29-CUDA/7m.PNG-800.webp 800w,/assets/img/2024-12-29-CUDA/7m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-CUDA/7m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="tiled-matrix-multiplication">Tiled Matrix Multiplication</h2> <ul> <li>Matrix Multiplication without shared memory :<br/> each thread has to access global memory,<br/> so performance is bottlenecked by global memory bandwidth</li> </ul> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">MatrixMulKernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">M</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">P</span><span class="p">,</span> <span class="kt">int</span> <span class="n">Width</span><span class="p">){</span>
  <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>

  <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

  <span class="k">if</span> <span class="p">((</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">Width</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">col</span> <span class="o">&lt;</span> <span class="n">Width</span><span class="p">)){</span>
    <span class="kt">float</span> <span class="n">value</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="c1">// each thread computes one element of output matrix</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">Width</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">){</span>
      <span class="n">value</span> <span class="o">+=</span> <span class="n">M</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">Width</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">N</span><span class="p">[</span><span class="n">k</span> <span class="o">*</span> <span class="n">Width</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>
    <span class="p">}</span>
    
    <span class="n">P</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">Width</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span><span class="p">;</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <ul> <li>Tiling Algorithm : <ul> <li>17p TBD</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="others"/><category term="CUDA"/><category term="GPU"/><category term="kernel"/><category term="parallel"/><summary type="html"><![CDATA[.cu coding]]></summary></entry><entry><title type="html">Quark</title><link href="https://semyeong-yu.github.io/blog/2024/Quark/" rel="alternate" type="text/html" title="Quark"/><published>2024-12-23T12:00:00+00:00</published><updated>2024-12-23T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Quark</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Quark/"><![CDATA[<h2 id="quark---real-time-high-resolution-and-general-neural-view-synthesis">Quark - Real-time, High-resolution, and General Neural View Synthesis</h2> <h4 id="john-flynn-michael-broxton-lukas-murmann-lucy-chai-matthew-duvall-clément-godard-kathryn-heal-srinivas-kaza-stephen-lombardi-xuan-luo-supreeth-achar-kira-prabhu-tiancheng-sun-lynn-tsai-ryan-overbeck">John Flynn, Michael Broxton, Lukas Murmann, Lucy Chai, Matthew DuVall, Clément Godard, Kathryn Heal, Srinivas Kaza, Stephen Lombardi, Xuan Luo, Supreeth Achar, Kira Prabhu, Tiancheng Sun, Lynn Tsai, Ryan Overbeck</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2411.16680">https://arxiv.org/abs/2411.16680</a><br/> project website :<br/> <a href="https://quark-3d.github.io/">https://quark-3d.github.io/</a><br/> reference :<br/> Presentation of https://charlieppark.kr from 3D-Nerd Community</p> </blockquote> <h2 id="contribution">Contribution</h2> <ul> <li>Architecture : <ul> <li>3D space에서 ray를 쏘거나(NeRF) Gaussian list를 구해서(3DGS) alpha-compositing하는 게 아니라<br/> <code class="language-plaintext highlighter-rouge">layered RGB image(or depth map)</code>를 구해서 alpha-compositing</li> <li><code class="language-plaintext highlighter-rouge">target view가 어떤 input view에 얼만큼 attention해야 하는지</code>를 <code class="language-plaintext highlighter-rouge">iteratively refine</code>하여<br/> Blend Weights를 구해서 input view들을 interpolate하는 방식</li> <li>refinement로 Blend Weights 구해서<br/> input images를 blend하여 layered RGB images를 구하므로<br/> input view가 멀리멀리 sparse하게 떨어져 있어야<br/> recon.할 때 모든 영역 커버 가능</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Generalizable</code> : <ul> <li>pre-trained model 가져온 뒤<br/> pre-trained model이 학습하지 못했던 <code class="language-plaintext highlighter-rouge">unseen scene</code>에 대해<br/> <code class="language-plaintext highlighter-rouge">fine-tuning 없이</code> <code class="language-plaintext highlighter-rouge">refinement</code>로<br/> layered depth map 쫘르륵 얻어내서 novel view recon. 가능!</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Real-time</code> <code class="language-plaintext highlighter-rouge">Reconstruction</code> and Rendering : <ul> <li>3DGS에서는 real-time rendering이었는데<br/> 본 논문은 recon. 자체도 real-time<br/> (inference하는 데 총 33ms at 1080p with single A100 GPU)</li> </ul> </li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li>Generalizable : <ul> <li>IBRNet :<br/> rendering 시간은 오래 걸리지만 generalizable</li> <li>ENeRF :<br/> cost volume, depth-guided sampling, volume rendering 사용</li> <li>GPNR :<br/> 2-view VFT, Epipolar Transformer 사용</li> <li>CO3D - NeRFormer :<br/> 반복 between attention on feature-dim. and attention on ray-direction-dim.</li> </ul> </li> <li>Quark의 직계 조상 paper : <ul> <li>DeepView <d-cite key="DeepView">[1]</d-cite> : <ul> <li>MPI (여러 depth에 대해 image를 중첩한 multi-plane image)</li> <li>한계 : input view와 target view 간의 camera 이동이 크면 안 됨</li> </ul> </li> <li>Immersive light field video with a layered mesh representation <d-cite key="Immersive">[2]</d-cite> : <ul> <li>MSI (여러 depth에 대해 곡면 image를 중첩한 multi-spherical image) (= layered mesh)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/2m.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/2m.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/2m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-23-Quark/2m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="overview">Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/3m.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/3m.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/3m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-23-Quark/3m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>I/O : <ul> <li>input : sparse multi-view images (\(\in R^{M \times H \times W \times 3}\))<br/> (sensitive to view selection)<br/> (pose 정보 필요)</li> <li>output : novel view image</li> <li>Quark는 pretrained model (pretrained with 8 input views of scenes(Spaces, RFF, Nex-Shiny, and SWORD)) 가져와서<br/> unseen scene에 대한 refinement로 novel target view synthesis 가능 (generalizable) <ul> <li>Spaces : Quark의 직계 조상 격인 DeepView에서 사용한 dataset</li> <li>RFF : NeRF에서 사용한 Real Forward Facing dataset</li> <li>Nex-Shiny : NeX에서 사용한 shiny object이 포함된 dataset</li> <li>SWORD : real-world scene dataset</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/1m.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/1m.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/1m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-23-Quark/1m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Architecture :<br/> U-Net style <ul> <li>Encoder :<br/> Obtain feature pyramid \(I_{\downarrow 8}, I_{\downarrow 4}, I_{\downarrow 2}, I_{\downarrow 0}\) from input image</li> <li>Iterative Updates : <ul> <li>pre-trained model을 가져와서 학습하는데,<br/> layered depth map을 업데이트하는 방법은<br/> gradient descent 이용한 <code class="language-plaintext highlighter-rouge">fine-tuning이 아니라</code><br/> input view feature 이용한 <code class="language-plaintext highlighter-rouge">refinement</code>임!!</li> <li>U-Net skip-connection과 비슷하지만 <code class="language-plaintext highlighter-rouge">Update &amp; Fuse 단계가 novel</code><br/> (아래에서 별도로 설명)</li> </ul> </li> <li>Upsample &amp; Activate : <ul> <li>image resolution으로 upsample한 뒤<br/> Layered Depth Map at target view 구함 <ul> <li>Depth \(d \in R^{L \times H \times W \times 1}\)<br/> (이 때, depth map은 linear in disparity (가까운 high-freq. 영역에서 더 촘촘히))</li> <li>Opacity \(\sigma \in R^{L \times H \times W \times 1}\)</li> <li>Blend Weights \(\beta \in R^{L \times H \times W \times M}\)<br/> by attention softmax weight</li> </ul> </li> </ul> </li> <li>Rendering : <ul> <li>input images \(\in R^{M \times H \times W \times 3}\) 를 Layered Depth Map (target view)로 back-project한 뒤<br/> Blend Weights \(\beta\) 로 input images를 blend해서 per-layer RGB 얻음</li> <li>Opacity \(\sigma\) 로 per-layer RGB를 alpha-compositing해서 final RGB image at target view 얻고,<br/> Opacity \(\sigma\) 로 Depth \(d\) 를 alpha-compositing해서 Depth Map 얻음</li> <li>training할 때는 stadard differentiable rendering 사용하지만<br/> inference할 때는 1080p resol. at 1.3 ms per frame 위해 CUDA-optimized renderer 사용</li> </ul> </li> </ul> </li> </ul> <h2 id="method">Method</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/4m.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/4m.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/4m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-23-Quark/4m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Update &amp; Fuse : <ul> <li>Step 1) Render to Input Views <ul> <li>from <code class="language-plaintext highlighter-rouge">layer space (target view)</code> to <code class="language-plaintext highlighter-rouge">image space (input view)</code><br/> (feature pyramid \(I_{\downarrow k}\) 와 합치기 위해!)</li> <li>feature volume \(V^{(n)}\)<br/> \(\rightarrow\) obtain appearance \(a\), density \(\sigma\), depth map \(d\)<br/> (depth map \(d = \delta + \text{tanh}\) 는 depth anchor \(\delta\) 근처의 depth)<br/> \(\rightarrow\) project from target-view into input-view by \(P_{\theta}\)<br/> \(\rightarrow\) obtain rendered feature \(\tilde I\) by alpha-compositing \(O\) at input-view<br/> (\(\tilde I\) : intermediate LDM(layered depth map))</li> </ul> </li> <li>Step 2) Update Block <ul> <li><code class="language-plaintext highlighter-rouge">rendered feature</code> \(\tilde I\) 를<br/> <code class="language-plaintext highlighter-rouge">feature pyramid</code> \(I_{\downarrow k}\), <code class="language-plaintext highlighter-rouge">input view-direction</code> \(\gamma\) 등 input image에 대한 정보와 섞음 <ul> <li>input view-direction 넣어줄 때 Ray Encoding \(\gamma\) 수행 : <ul> <li>obtain difference vector (아래 그림 참고)<br/> (input view가 target view에서 멀리 떨어져 있을수록 값이 큼)<br/> \(\rightarrow\) tanh and Sinusoidal PE</li> <li>tanh 사용하므로<br/> difference vector가 0 근처일 때<br/> 즉, input view가 target view 근처일 때 gradient 많이 반영</li> <li>input view’s ray가 frustum 밖으로 벗어나더라도<br/> near, far plane과의 교점을 구할 수 있으므로<br/> Ray Encoding 가능</li> <li>view-direction 넣어줘야<br/> view-dependent color 만들 수 있고<br/> reflection, non-lambertian surface 잘 구현 가능</li> </ul> </li> </ul> </li> </ul> </li> <li>Step 3) Back-project <ul> <li>from <code class="language-plaintext highlighter-rouge">image space (input view)</code> to <code class="language-plaintext highlighter-rouge">layer space (target view)</code><br/> (feature volume \(V^{(n)}\) 과 합치기 위해!)</li> <li>back-project from input-view into target-view by \(P_{\theta}^{T} (I, d)\)<br/> \(\rightarrow\) obtain residual feature \(\Delta\)</li> </ul> </li> <li>Step 4) One-to-Many Attention <ul> <li><code class="language-plaintext highlighter-rouge">feature volume</code> \(V^{(n)}\) 을 <code class="language-plaintext highlighter-rouge">query</code>로,<br/> Step 1~3)에서 얻은 <code class="language-plaintext highlighter-rouge">residual feature</code> \(\Delta\) 를 <code class="language-plaintext highlighter-rouge">key, value</code>로 하여<br/> One-to-Many attention 수행하여<br/> updated feature volume \(V^{(n+1)}\) 얻음<br/> Then, target view가 input view의 feature들을 aggregate하여 이용할 수 있게 됨!!<br/> 즉, target view가 어떤 input view에 얼만큼 attention해야 하는지! <ul> <li><code class="language-plaintext highlighter-rouge">query</code> : <code class="language-plaintext highlighter-rouge">target view</code> 정보 at target view space</li> <li><code class="language-plaintext highlighter-rouge">key, value</code> : <code class="language-plaintext highlighter-rouge">input view</code> 정보 at target view space</li> <li><code class="language-plaintext highlighter-rouge">One-to-Many attention</code> : <ul> <li>cross-attention과 비슷하지만<br/> <code class="language-plaintext highlighter-rouge">redundant matrix multiplication 없애서</code><br/> complexity 줄여서<br/> real-time reconstruction에 기여!</li> <li>\(\text{MultiHead}(Q, K, V) = \text{concat}(\text{head}_{1}, \cdots, \text{head}_{h}) W^{O}\)<br/> where \(\text{head}_{i} = \text{Attention}(QW_{i}^{Q}, KW_{i}^{K}, VW_{i}^{V})\) 식을 써보면<br/> \(W_{i}^{Q} (W_{i}^{K})^{T}\) 항에서 \(W^{Q}\) 와 \(W^{K}\) 가 redundant 하고<br/> \(\text{concat}(\cdots W_{i}^{V}) W^{O}\) 항에서 \(W^{V}\) 와 \(W^{O}\) 가 redundant 하므로<br/> \(\text{head}_{i} = \text{Attention}(QW_{i}^{Q}, K, V)\) 로 바꿔서<br/> \(W^{Q}\) 와 \(W^{O}\) 만 사용</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/5m.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/5m.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/5m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-23-Quark/5m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> difference vector for input view-direction (Ray Encoding) </div> <h2 id="result">Result</h2> <ul> <li>Training : <ul> <li>Dataset : Spaces, RFF, Nex-Shiny, SWORD</li> <li>Loss : \(\text{10 * L1} + \text{LPIPS}\)</li> <li>Input : 8 views (randomly sampled from 16 views nearest to object)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/6m.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/6m.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/6m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-23-Quark/6m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Inference time : recon.까지 포함해서 총 33ms at 1080p single A100 GPU</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/7m.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/7m.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/7m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-23-Quark/7m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Generalizable method와의 비교 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/8m.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/8m.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/8m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-23-Quark/8m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/9m.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/9m.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/9m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-23-Quark/9m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Non-Generalizable method와의 비교 </div> <h2 id="discussion">Discussion</h2> <ul> <li>Limitation : <ul> <li>view selection :<br/> training할 때 sparse(8개) input views를 사용하는데, <code class="language-plaintext highlighter-rouge">view selection</code>에 매우 민감 (중요함) (heuristic)</li> <li>Blend Weights :<br/> target view RGB image를 rendering하기 전에 input RGB images를 blend하는데, <ul> <li><code class="language-plaintext highlighter-rouge">view dependency</code>를 잘 캡처 못한다<br/> \(\rightarrow\) Ray Encoding으로 해소하긴 함</li> <li>input images의 <code class="language-plaintext highlighter-rouge">focal length</code>가 각기 다르면 잘 recon.하지 못한다</li> </ul> </li> <li>sparse input :<br/> real-time rendering 뿐만 아니라 real-time recon. 위해<br/> 적은 수(8 ~ 16)의 <code class="language-plaintext highlighter-rouge">sparse inputs</code> 사용</li> <li>light, shadow 고려 X</li> <li>conv. network를 일반화에 사용했을 때 생기는 깨지는 artifacts 발생 (홈페이지 영상 참고)</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="general"/><category term="view"/><category term="synthesis"/><summary type="html"><![CDATA[Real-time, High-resolution, and General Neural View Synthesis (SIGGRAPH 2024)]]></summary></entry><entry><title type="html">PhysGaussian</title><link href="https://semyeong-yu.github.io/blog/2024/PhysGaussian/" rel="alternate" type="text/html" title="PhysGaussian"/><published>2024-12-20T12:00:00+00:00</published><updated>2024-12-20T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/PhysGaussian</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/PhysGaussian/"><![CDATA[<h2 id="physgaussian---physics-integrated-3d-gaussians-for-generative-dynamics">PhysGaussian - Physics-Integrated 3D Gaussians for Generative Dynamics</h2> <h4 id="tianyi-xie-zeshun-zong-yuxing-qiu-xuan-li-yutao-feng-yin-yang-chenfanfu-jiang">Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, Chenfanfu Jiang</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2311.12198">https://arxiv.org/abs/2311.12198</a><br/> project website :<br/> <a href="https://xpandora.github.io/PhysGaussian/">https://xpandora.github.io/PhysGaussian/</a><br/> code :<br/> <a href="https://github.com/XPandora/PhysGaussian">https://github.com/XPandora/PhysGaussian</a><br/> reference :<br/> <a href="https://xoft.tistory.com/101">https://xoft.tistory.com/101</a></p> </blockquote> <h2 id="contribution">Contribution</h2> <ul> <li><code class="language-plaintext highlighter-rouge">Physics Simulation</code>을 <code class="language-plaintext highlighter-rouge">3DGS</code>에 결합 : <ul> <li>3DGS에 부피, 질량, 속도 (Physics) property를 부여하여<br/> 3DGS의 covariance 및 rotation matrix가 시간에 따라 물리 법칙에 따라 변화</li> <li>MPM simulation 장점과 3DGS rendering 장점을 결합하여<br/> unified simulation-rendering pipeline 제시</li> </ul> </li> </ul> <p>결과 영상이 재밌음!</p> <h2 id="overview">Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-20-PhysGaussian/1m.PNG-480.webp 480w,/assets/img/2024-12-20-PhysGaussian/1m.PNG-800.webp 800w,/assets/img/2024-12-20-PhysGaussian/1m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-20-PhysGaussian/1m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Step 1) <code class="language-plaintext highlighter-rouge">3DGS optimization</code><br/> Anisotropic Loss를 추가하여 3DGS를 둥글둥글하게 만듦</li> <li>Step 2) <code class="language-plaintext highlighter-rouge">3DGS Internel Filling</code><br/> Object 내부 공간을 3DGS로 채워서 continuum(연속체)로 만듦</li> <li>Step 3) <code class="language-plaintext highlighter-rouge">Physics Integration</code> <ul> <li>Dynamics :<br/> 3DGS에 부피, 질량 부여하여<br/> 시간에 따라 Continuum Mechanics(연속체 역학)이라는 물리 법칙 따르도록 함</li> <li>Kinematics :<br/> Gaussian Evolution, SH Transform을 통해<br/> 시간에 따른 물리적인 변화를 3DGS로 모델링</li> </ul> </li> </ul> <h2 id="backgrounds-on-continuum-mechanics">Backgrounds on Continuum Mechanics</h2> <ul> <li>Conservation of Mass (질량 보존 법칙) :<br/> 시간 \(t\) 가 바뀌어도 infinitesimal region 내 질량은 항상 일정하게 유지된다!!<br/> \(\int_{B_{\epsilon}^{t}} \rho (x, t) = \int_{B_{\epsilon}^{0}} \rho (\phi^{-1}(x, t), 0)\) <ul> <li>\(B_{\epsilon}^{t}\) : infinitesimal region at \(t\)</li> <li>\(\rho(x, t)\) : density field at \(x, t\)</li> <li>\(x = \phi(x_{0}, t)\) : deformation map from \(x_{0}, 0\) to \(x, t\)</li> </ul> </li> <li>Conservation of Momentum (운동량 보존 법칙) :<br/> 시간 \(t\) 가 바뀌어도 물질의 운동량은 변하지 않는다!!<br/> 운동량 변화량 : \(\rho(x, t) \overset{\cdot}{v}(x, t) = \nabla \cdot \sigma(x, t) + f^{ext}\) <ul> <li>\(\overset{\cdot}{v}(x, t)\) : 가속도 field at \(x, t\)</li> <li>\(\sigma = \frac{1}{det(F)}\frac{\partial \psi}{\partial F}F^{E}(F^{E})^{T}\) : Cauchy stress tensor (물체 내부에서 발생하는 응력)<br/> where \(\psi(F)\) : hyperelastic energy density<br/> where deformation field gradient \(F = F^{E} F^{P}\) <ul> <li>\(F^{E}\) : elastic part (탄성)<br/> 물체에 stress를 가해서 조직에 구조적인 변형이 발생한 후,<br/> stress를 제거했을 때 원래 상태로 되돌아가는 성질</li> <li>\(F^{P}\) : plastic part (소성)<br/> 물체에 stress를 가해서 조직에 구조적인 변형이 발생한 후,<br/> stress가 탄성 범위를 넘어가서<br/> stress를 제거하더라도 원래 상태로 되돌아오지 않는 성질</li> </ul> </li> <li>\(f^{ext}\) : external force per unit volume</li> </ul> </li> </ul> <h2 id="mpm-material-point-method">MPM (Material Point Method)</h2> <ul> <li>핵심 :<br/> <code class="language-plaintext highlighter-rouge">particle과 grid 간에 운동량이 상호작용</code>하여<br/> 이 과정에서 질량과 운동량이 보존되어<br/> <code class="language-plaintext highlighter-rouge">simulation</code> 했을 때 현실과 비슷 <ul> <li>Lagrangian Particle Domain :<br/> particle의 위치, 질량, 운동량, 응력, 부피, 외력 등을 모델링하고<br/> particle 별로 추적하여 update</li> <li>Eulerian Grid Domain :<br/> 공간을 3D grid로 나누어서 grid를 통해 particle 이동<br/> cell의 크기가 작아질수록 정확도는 올라가지만 연산속도는 느려짐</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-20-PhysGaussian/2m.PNG-480.webp 480w,/assets/img/2024-12-20-PhysGaussian/2m.PNG-800.webp 800w,/assets/img/2024-12-20-PhysGaussian/2m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-20-PhysGaussian/2m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Procedure : <ul> <li>Particle to Node :<br/> each particle의 물리량을 grid 상의 adjacent 8 nodes로 분배</li> <li>Nodal Solution :<br/> 집계된 힘을 이용해서 each node의 \(a = \frac{F}{m}\), \(v\) 를 update</li> <li>Node to Particle :<br/> each node의 \(a, v\) 를 particle로 전파 by weighted sum</li> <li>Update Particles :<br/> each particle의 \(a, v\) 이용해서 새로운 particle 위치 갱신</li> </ul> </li> </ul> <p>실험 예시 : <a href="https://vimeo.com/267058393">SIGGRAPH2018</a></p> <h2 id="physics-integrated-3dgs">Physics-Integrated 3DGS</h2> <p>그렇다면 어떻게 물리 법칙을 3DGS에 적용할까??</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-20-PhysGaussian/7m.PNG-480.webp 480w,/assets/img/2024-12-20-PhysGaussian/7m.PNG-800.webp 800w,/assets/img/2024-12-20-PhysGaussian/7m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-20-PhysGaussian/7m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>방법 1) deformation gradient \(F_{p}\) 로 approx. <ul> <li>local affine transformation of deformation map \(\phi\) :<br/> \(\tilde \phi (X, t) = x_{p} + F_{p} (X - X_{p})\) <ul> <li>\(X\) : arbitrary point</li> <li>\(X_{p}\) : particle \(p\) 의 initial point</li> <li>\(x_{p}\) : particle \(p\) 의 current point</li> <li>\(F_{p}\) : 점이 어떻게 이동하는지에 대한 deformation gradient matrix<br/> (물리 법칙 적용)</li> </ul> </li> <li>3DGS position, covariance matrix 변화 :<br/> By approx. deformation map,<br/> \(x_{p}(t) = \tilde \phi (X_{p}, t)\)<br/> \(\Sigma_{p}(t) = F_{p}(t) \Sigma_{p} F_{p}(t)^{T}\)</li> <li>Gaussian 수식 변화 :<br/> \(G_{p}(x, t) = e^{-\frac{1}{2}(x-x_{p})^{T}(F_{p}(t) \Sigma_{p} F_{p}(t)^{T})^{-1}(x-x_{p})}\)</li> <li>grid 부피를 particle 수로 나누어서 각 particle 부피 \(V_{p}^{0}\) 를 초기화하고<br/> 이로써 각 particle(Gaussian)은 질량 \(m_{p} = \phi_{p} V_{p}\) 를 가지게 되고<br/> MPM Simulation을 바탕으로 Gaussian이 물리 법칙을 따름</li> <li>아래의 이유로 Physics와 3DGS의 결합은 자연스러움 <ul> <li>Gaussian itself가<br/> Continuum의 discretized form으로 간주되므로<br/> 직접 simulation 가능</li> <li>물리 법칙에 의해 변형된 Deformed Gaussian은<br/> 3DGS rasterization에 의해<br/> 직접 rendering 가능</li> <li>따라서 WS2(What you see is What you simulate) 달성</li> </ul> </li> </ul> </li> <li>방법 2) <code class="language-plaintext highlighter-rouge">incremental update</code> <ul> <li>deformation gradient \(F_{p}\) 에 의존하지 않고<br/> Langrangian framework(MPM simulation)에 더 잘 맞는<br/> Gaussian Kinematic(운동학) 방법 제시</li> <li>computational fluid dynamics (전산 유체 역학)에 따라 <ul> <li>covariance matrix :<br/> covariance matrix는 discretize되어<br/> \(\Sigma_{p}(t) = F_{p}(t) \Sigma_{p} F_{p}(t)^{T}\)<br/> 대신<br/> \(\Sigma_{p}^{n+1} = \Sigma_{i}^{n} + \Delta t \overset{\cdot}{\Sigma_{p}^{n}} = \Sigma_{i}^{n} + \Delta t (\nabla v_{p} \Sigma_{p}^{n} + \Sigma_{p}^{n} \nabla v_{p}^{T})\)</li> <li>rotation matrix :<br/> 마찬가지로 \(R_{p}^{0} = I\) 에서 출발해서 비슷하게 update 가능</li> <li>즉, <code class="language-plaintext highlighter-rouge">covariance matrix와 rotation matrix가 물리 법칙을 따르면서 incrementally update되도록 설계</code>!!</li> </ul> </li> <li>위의 수식을 통해 deformation gradient \(F_{p}\) 를 직접 구하지 않더라도<br/> Gaussian covariance를 \(t^{n}\) 에서 \(t^{n+1}\) 으로 incremental update 가능</li> </ul> </li> </ul> <h2 id="orientation-of-sh">Orientation of SH</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-20-PhysGaussian/3m.PNG-480.webp 480w,/assets/img/2024-12-20-PhysGaussian/3m.PNG-800.webp 800w,/assets/img/2024-12-20-PhysGaussian/3m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-20-PhysGaussian/3m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>SH는 view direction에 따른 color를 모델링하는, hard-coding되어 있는 함수이다<br/> 따라서 시간 \(t\) 에 따라 particle(Gaussian)이 rotate하면 색깔이 전혀 달라지므로<br/> view direction에 particle(Gaussian)의 역회전을 적용 <ul> <li>particle(Gaussian)의 회전 정보 :<br/> surface orientation을 사용한 <a href="https://arxiv.org/abs/2201.08845">Point-NeRF</a> 와 달리<br/> 방법 1)의 경우 polar decomposition을 통해 deformation gradient \(F_{p} = R_{p}S_{p}\) 에서 \(R_{p}\) 추출해서 사용<br/> 방법 2)의 경우 polar decomposition을 통해 \((I + \Delta t v_{p}) R_{p}^{n}\) 에서 \(R_{p}^{n+1}\) 추출해서 사용</li> </ul> </li> </ul> <h2 id="internal-filling">Internal Filling</h2> <ul> <li>recon. Gaussians는 surface 근처에 분포하는 경향이 있으므로<br/> object의 내부 구조는 비어 있는 채로 surface에 가려져 있음<br/> \(\rightarrow\)<br/> object의 deformation이 클 경우 내부가 노출될 수도 있고<br/> 질량을 가지는 물리 법칙에 따르는 volumetric object으로 만들기 위해<br/> 비어 있는 내부 영역도 particles(Gaussians)로 채워야 함</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-20-PhysGaussian/4m.PNG-480.webp 480w,/assets/img/2024-12-20-PhysGaussian/4m.PNG-800.webp 800w,/assets/img/2024-12-20-PhysGaussian/4m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-20-PhysGaussian/4m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Internal Filling : <ul> <li>Step 1)<br/> discretize<br/> from continuous <code class="language-plaintext highlighter-rouge">3D opacity field</code> \(d(x) = \sum_{p} \sigma_{p} e^{-\frac{1}{2}(x-x_{p})^{T}\Sigma_{p}^{-1}(x-x_{p})}\)<br/> into discrete 3D grid</li> <li>Step 2)<br/> low opacity(\(\sigma_{i} \lt \sigma_{th}\))를 가지는 grid에서<br/> high opacity(\(\sigma_{j} \gt \sigma_{th}\))를 가지는 grid로<br/> ray가 통과할 때<br/> 이를 intersection이라고 하자</li> <li>Step 3)<br/> 아래 두 가지 조건을 만족할 때 object 내부에 있다고 간주하고 3DGS 생성 <ul> <li>Condition 1) :<br/> 3D grid 상에서 6 axes 방향으로 ray casting한 뒤<br/> object 내부에 있는 grid의 경우 항상 surface와 intersect할 것이므로<br/> intersection 개수가 6개인지 체크하여 candidate grids 선택</li> <li>Condition 2) :<br/> candidate grids를 refine하기 위해<br/> additional ray를 casting하여 intersection 개수 체크</li> </ul> </li> <li>Step 4)<br/> object 내부에 채워 넣은 gaussian들도 3D 상에서 visualize할 필요가 있을 수 있음<br/> internal-filled particle(Gaussian)의 경우<br/> opacity \(\sigma_{p}\) 와 color \(C_{p}\) 는 closest Gaussian의 것을 물려받고<br/> covariance matrix는 \(\text{diag}(r_{p}^{2}, r_{p}^{2}, r_{p}^{2})\) 으로 initialize<br/> where \(r_{p}\) : particle radius from its volume \(V_{p}^{0} = \frac{4 \pi r_{p}^{3}}{3}\)<br/> (본 논문의 저자는 시도하지 않았지만 internal filling을 위해 generative model을 사용하면 more realistic results 가능할 듯)</li> </ul> </li> </ul> <h2 id="anisotropy-regularizer">Anisotropy Regularizer</h2> <ul> <li>3DGS가 너무 얇을 경우<br/> large deformation일 때 Gaussian이 object surface의 바깥쪽으로 튀어나와<br/> <code class="language-plaintext highlighter-rouge">plush artifacts</code> 발생 가능</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-20-PhysGaussian/9m.PNG-480.webp 480w,/assets/img/2024-12-20-PhysGaussian/9m.PNG-800.webp 800w,/assets/img/2024-12-20-PhysGaussian/9m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-20-PhysGaussian/9m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> chair에 Twist deformation 가했을 때 생기는 plush artifacts </div> <ul> <li>\(L_{aniso} = \frac{1}{| P |} \sum_{p \in P} \text{max}(\frac{\text{max}(S_{p})}{\text{min}(S_{p})}, r) - r\)<br/> where \(S_{p}\) : scale matrix of 3DGS <ul> <li>\(\frac{\text{max}(S_{p})}{\text{min}(S_{p})} \leq r\)<br/> 즉, 장축과 단축의 길이 비가 threshold \(r\) 을 넘지 않도록<br/> 3DGS를 <code class="language-plaintext highlighter-rouge">둥글둥글하게</code> 만듦</li> </ul> </li> </ul> <h2 id="experiments">Experiments</h2> <ul> <li> <p>Dataset :<br/> InstantNGP, NerfStudio, DroneDeployNeRF, \(\cdots\)</p> </li> <li> <p>Resource :<br/> 24-core 3.50GHz Intel i9-10920X machine with Nvidia RTX 3090 GPU</p> </li> <li>MPM Simulation : <ul> <li>MPM :<br/> <a href="https://zeshunzong.github.io/reduced-order-mpm/">SIGGRAPH2023</a></li> <li>simulation region :<br/> simulation region을 manually 선택하여 \(2 \times 2 \times 2\) cube로 normalize한 뒤 3D dense crid로 discretize</li> <li>particle :<br/> controlled movement(흔들리는 여우 얼굴 등)를 보일 specific particles만 선택적으로 velocities 수정하고<br/> 나머지 particles는 물리 법칙을 따르는 natural motion</li> </ul> </li> <li>Qualitative Results :<br/> <a href="https://xpandora.github.io/PhysGaussian/">Video</a> 를 보면<br/> Simulation 할 때 <ul> <li>Fox의 경우<br/> 물체의 원래 형태로 되돌아가는 Elasticity (탄성) 성질을 적용</li> <li>Plane의 경우<br/> 물체의 원래 형태로 되돌아가지 않는 Metal (금속) 성질을 적용</li> <li>Ruins의 경우<br/> Sand 효과 (granular-level frictional effect based on Druker-Prager plastic model)를 적용</li> <li>Toast의 경우<br/> MPM Simulation에 따라 큰 deformation이 발생하면 입자가 여러 그룹으로 분리되는 Fracture</li> <li>Jam의 경우<br/> Paste 효과 (non-Newtonian fluid based on Herschel Bulkley plastic model)를 적용</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-20-PhysGaussian/5m.PNG-480.webp 480w,/assets/img/2024-12-20-PhysGaussian/5m.PNG-800.webp 800w,/assets/img/2024-12-20-PhysGaussian/5m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-20-PhysGaussian/5m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Quantitative Results : <ul> <li>deformation에 대한 GT를 만들기 위해<br/> BlenderNeRF로 scene 합성한 뒤 lattice deformation tool로 Bend 및 Twist</li> <li>3가지 model과 비교 <ul> <li><a href="https://arxiv.org/abs/2205.04978">NeRF-Editing</a> : <ul> <li><a href="https://arxiv.org/abs/2106.10689">NeuS</a> 로 추출한 surface mesh를 이용해서 NeRF 를 변형하는데,<br/> surface recon.에 초점이 맞춰진 연구여서 volumetric simulation과 결합했을 때<br/> rendering 퀄리티가 낮았음</li> <li>deformation이 extracted surface mesh와 dilated cage mesh의 정밀도에 의존하는데<br/> mesh가 지나치게 크면 경계가 공백이 될 수 있음</li> </ul> </li> <li><a href="https://arxiv.org/abs/2309.13101">Deforming-NeRF</a> : <ul> <li>고해상도 deformation cage mesh를 사용해서 변형하여 향상된 결과 보이지만<br/> interpolation 과정에서 local detail을 filtering하면서 성능 낮아짐</li> </ul> </li> <li><a href="https://arxiv.org/abs/2303.05512">PAC-NeRF</a> : <ul> <li>단순한 object, texture를 표현하도록 디자인되어<br/> particle representation을 통해 flexible하지만 rendering 퀄리티는 여전히 높지 않음</li> </ul> </li> </ul> </li> <li>Ours :<br/> zero-order info.(deformation map)와 first-order info.(deformation gradient)를 모두 활용하였으므로<br/> deformation 후에도 높은 성능 보임</li> <li>Ablation Study : <ul> <li>Fixed Covariance :<br/> 3DGS에 translation만 적용하여<br/> covariance는 그대로 사용</li> <li>Rigid Covariance :<br/> 3DGS에 rigid transformation 적용하여<br/> covariance를 수정하여 물리 법칙을 따르도록</li> <li>Fixed Harmonics :<br/> SH에서 view direction을 rotate하지 않음</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-20-PhysGaussian/6m.PNG-480.webp 480w,/assets/img/2024-12-20-PhysGaussian/6m.PNG-800.webp 800w,/assets/img/2024-12-20-PhysGaussian/6m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-20-PhysGaussian/6m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 논문에서 언급한 기법들을 적용하지 않을 경우 Gaussian이 surface를 제대로 덮지 않아 artifacts 발생 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-20-PhysGaussian/8m.PNG-480.webp 480w,/assets/img/2024-12-20-PhysGaussian/8m.PNG-800.webp 800w,/assets/img/2024-12-20-PhysGaussian/8m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-20-PhysGaussian/8m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> E는 elasticity(탄성도), v는 poission ratio(volume 보존 정도) </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-20-PhysGaussian/10m.PNG-480.webp 480w,/assets/img/2024-12-20-PhysGaussian/10m.PNG-800.webp 800w,/assets/img/2024-12-20-PhysGaussian/10m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-20-PhysGaussian/10m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 당겼을 때 Physics-based Ours는 물리 법칙에 따라 volume을 잘 보존하지만, Geometry-based NeRF-Editing은 volume 보존하지 않음 </div> <h2 id="conclusion">Conclusion</h2> <ul> <li>Limitation : <ul> <li>그림자 고려 안 함</li> <li>material param.를 manually 정해주어야 함<br/> (GS segmentation과 differentiable MPM simulator를 결합하여 video로부터 param. 자동 assign 가능하긴 함)</li> </ul> </li> <li>Future Work : <ul> <li>more versatile materials like liquid 다루기</li> <li>more intuitive user control 포함하기</li> <li>LLM 기술 적용하기</li> <li>geometry-aware 3DGS recon. 결합하여 generative dynamics (생성 동역학) 향상시키기</li> </ul> </li> <li>마무리하며..<br/> 3DGS와 전혀 다른 분야를 통합하는 논문들이 종종 나오는데<br/> 이 논문도 결과가 재미있게 나온 논문이었다!!</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="Physics"/><category term="Simulation"/><category term="MPM"/><category term="3DGS"/><summary type="html"><![CDATA[Physics-Integrated 3D Gaussians for Generative Dynamics (CVPR 2024)]]></summary></entry><entry><title type="html">MASt3R</title><link href="https://semyeong-yu.github.io/blog/2024/MASt3R/" rel="alternate" type="text/html" title="MASt3R"/><published>2024-11-21T12:00:00+00:00</published><updated>2024-11-21T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/MASt3R</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/MASt3R/"><![CDATA[<h2 id="grounding-image-matching-in-3d-with-mast3r">Grounding Image Matching in 3D with MASt3R</h2> <h4 id="vincent-leroy-yohann-cabon-jérôme-revaud">Vincent Leroy, Yohann Cabon, Jérôme Revaud</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2406.09756">https://arxiv.org/abs/2406.09756</a><br/> project website :<br/> <a href="https://europe.naverlabs.com/blog/mast3r-matching-and-stereo-3d-reconstruction/">https://europe.naverlabs.com/blog/mast3r-matching-and-stereo-3d-reconstruction/</a><br/> code :<br/> <a href="https://github.com/naver/mast3r">https://github.com/naver/mast3r</a><br/> reference :<br/> <a href="https://xoft.tistory.com/100">https://xoft.tistory.com/100</a></p> </blockquote> <h3 id="contribution">Contribution</h3> <ul> <li>DUSt3R : <ul> <li>많은 연산을 필요로 하는 <code class="language-plaintext highlighter-rouge">SfM 생략</code> (pose-free)</li> <li>transformer 기반으로<br/> <code class="language-plaintext highlighter-rouge">2D(img pixel)-to-3D(point map)</code> mapping 예측하여<br/> <code class="language-plaintext highlighter-rouge">regression-based</code> 3D recon. 수행</li> <li>predicted pointmap 기반으로<br/> intrinsic/extrinsic camera param. 추정 가능</li> </ul> </li> <li>MASt3R : <ul> <li>DUSt3R 후속 논문으로,<br/> DUSt3R을 활용하여 <code class="language-plaintext highlighter-rouge">Image Matching에 특화</code>시킴 <ul> <li><code class="language-plaintext highlighter-rouge">Image Matching</code> 문제를 <code class="language-plaintext highlighter-rouge">3D 상에서</code> 풂</li> <li>quality 향상 및 속도 개선 및 많은 images 수 커버 가능</li> </ul> </li> </ul> </li> </ul> <h3 id="image-matching">Image Matching</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/1m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/1m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/1m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/1m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Image Matching 문제를 3D 상에서 풀기 때문에<br/> 2개 image의 공통 영역이 매우 적더라도 Image Matching 잘 수행 가능!</p> </li> <li> <p>Image Matching 기법의 문제 및 해결 :</p> <ul> <li>문제 1)<br/> 전통적인 Image Matching은 keypoint을 추출하여 local-invariant descriptor로 변형한 뒤<br/> feature space에서 거리를 비교하여 Matching을 수행했음<br/> 조명 변화와 시점 변화에도 정확했고 적은 keypoint 수로도 [ms] 단위로 Matching 가능했음<br/> 하지만 geometry context는 고려하지 못했고<br/> 반복 패턴이나 low-texture 영역에서는 잘 수행하지 못했음</li> <li>해결 1)<br/> local descriptor 대신 global descriptor를 이용하는 SuperGlue(2020) 기법</li> <li>문제 2)<br/> SuperGlue(2020)의 경우 keypoint descriptor가 충분히 encode되지 않으면 matching 도중에 global text를 활용할 수 없었음</li> <li>해결 2)<br/> keypoint 대신 image 전체를 한 번에 matching하는 dense holistic matching 기법<br/> thanks to global attention<br/> e.g. LoFTR(2021) : 반복 패턴 및 low-texture 영역에 robust하고 dense correspondence 만들 수 있음</li> <li>문제 3)<br/> LoFTR(2021)은 Map-free localization benchmark의 VCRE 평가에서 낮은 성능<br/> 현실적으로 Image Matching task는 같은 3D point에 대응되는 pixel을 찾는 문제인데 지금까지 전통적인 matching 기법들은 전부 2D 상에서 이루어졌기 때문</li> <li>해결 3)<br/> 2D pixel - 3D point correspondence 다루는 DUSt3R 활용</li> <li>문제 4)<br/> DUSt3R은 3D recon.을 목적으로 만들어졌기 때문에<br/> 시점 변화에는 강인하지만 Image Matching에서는 비교적 부정확</li> <li>해결 4)<br/> MASt3R(본 논문)에서는 DUSt3R을 Image Matching에 특화하는 방법에 대해 다룸!</li> </ul> </li> </ul> <h3 id="dust3r-framework">DUSt3R Framework</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/2m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/2m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/2m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/2m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>DUSt3R과 달라진 점 : <ul> <li>DUSt3R에서는 3D recon.이 목적이었기 때문에<br/> view-point가 달라지더라도 같은 크기의 물체를 recon.해야 함<br/> 즉, scale-invariant하도록 만들기 위해<br/> 각 view-point에서 averaged depth 값으로 나누어 normalize해주었음<br/> \(\rightarrow\)<br/> MASt3R에서는 서로 다른 scale의 images인 상태에서 image matching task를 수행해야 하므로<br/> (scale을 고려해야 하므로)<br/> regression loss에서 <code class="language-plaintext highlighter-rouge">scale(depth) normalization 파트를 없앰</code></li> </ul> </li> <li>Loss : <ul> <li><code class="language-plaintext highlighter-rouge">regression loss</code> :<br/> \(L_{regr} (v, i) = \| \frac{1}{z} X_{i}^{v, 1} - \frac{1}{\bar z} \bar X_{i}^{v, 1} \|\) <ul> <li>\(i\) : each point, \(v\) : each view</li> <li>\(z = \bar z\) : averaged depth of GT point</li> </ul> </li> <li>confidence loss :<br/> \(L_{conf} = \sum_{v \in \{ 1, 2 \}} \sum_{i \in D^{v}} C_{i}^{v, 1} L_{regr}(v, i) - \alpha \text{log} C_{i}^{v, 1}\) <ul> <li>\(C_{i}^{v, 1}\) : confidence score<br/> 물체인 부분에서는 3D point를 비교적 정확히 예측할 수 있으므로 confidence가 높고,<br/> 하늘 또는 반투명인 부분에서는 3D point를 정확하게 예측할 수 없으므로 confidence가 낮게 나옴</li> <li>\(C_{i}^{v, 1} L_{regr}(v, i)\) :<br/> confidence가 큰 <code class="language-plaintext highlighter-rouge">(확실한) point</code>에서는 GT와의 <code class="language-plaintext highlighter-rouge">regression loss</code> \(L_{regr}\) 가 더 <code class="language-plaintext highlighter-rouge">작도록</code></li> <li>\(- \alpha \text{log} C_{i}^{v, 1}\) : regularization term<br/> <code class="language-plaintext highlighter-rouge">confidence</code> \(C_{i}^{v, 1}\) 값이 <code class="language-plaintext highlighter-rouge">너무 작아지지 않도록</code></li> </ul> </li> <li>matching loss (<code class="language-plaintext highlighter-rouge">cross-entropy classification loss</code>) :<br/> \(L_{match} = - \sum_{(i, j) \in \hat M} (\text{log} \frac{s_{\tau} (i, j)}{\sum_{k \in P^{1}} s_{\tau} (k, j)} + \text{log} \frac{s_{\tau} (i, j)}{\sum_{k \in P^{2}} s_{\tau} (i, k)})\)<br/> 다음 section에서 언급할 예정</li> <li>total loss :<br/> \(L_{tot} = L_{conf} + \beta L_{match}\)</li> </ul> </li> </ul> <h3 id="matching-prediction-head">Matching Prediction Head</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/2m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/2m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/2m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/2m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>기존 DUSt3R의 Head output :<br/> extreme한 view-point 변화에도 robust <ul> <li>per-pixel <code class="language-plaintext highlighter-rouge">Pointmap</code> \(X_{i}^{v, 1} \in R^{H \times W \times 3}\)</li> <li>per-pixel <code class="language-plaintext highlighter-rouge">Confidence</code> score \(C_{i}^{v, 1} \in R^{H \times W}\)</li> </ul> </li> <li>MASt3R의 new Head output : <ul> <li>per-pixel <code class="language-plaintext highlighter-rouge">Local Feature</code> \(D_{i}^{v} \in R^{H \times W \times d}\) (\(d = 24\))</li> </ul> </li> <li>Fast NN :<br/> Fast Reciprocal Matching by Nearest Neighborhood<br/> (다음 section에서 설명 예정) <ul> <li>predicted <code class="language-plaintext highlighter-rouge">Pointmap</code> 이용하여 <code class="language-plaintext highlighter-rouge">Geometrical matching</code> 수행 <ul> <li>2개의 image에 대한 3D pointmap을 겹쳤을 때 align되도록<br/> <code class="language-plaintext highlighter-rouge">3D 공간 상에서 pixel correspondence</code>를 찾음</li> <li>\(X^{2, k}\) 중에 3D point \(X_{i}^{1, k}\) 와 가장 가까운 3D point가 \(X_{j}^{2, k}\) 이고,<br/> 동시에 \(X^{1, k}\) 중에 3D point \(X_{j}^{2, k}\) 와 가장 가까운 3D point가 \(X_{i}^{1, k}\) 일 때<br/> 두 pixel \(i, j\) 사이에 correspondence 있다고 함</li> </ul> </li> <li>predicted <code class="language-plaintext highlighter-rouge">Local Feature</code> 이용하여 <code class="language-plaintext highlighter-rouge">Feature-based matching</code> 수행</li> <li>얘네들 어떻게 합치는지 <code class="language-plaintext highlighter-rouge">???</code></li> </ul> </li> <li>Loss : <ul> <li>matching loss :<br/> constrastive learning에 사용되는 infoNCE Loss를 변형하여<br/> \(L_{match} = - \sum_{(i, j) \in \hat M} (\text{log} \frac{s_{\tau} (i, j)}{\sum_{k \in P^{1}} s_{\tau} (k, j)} + \text{log} \frac{s_{\tau} (i, j)}{\sum_{k \in P^{2}} s_{\tau} (i, k)})\)<br/> where \(s_{\tau} (i, j) = \text{exp}(- \tau D_{i}^{1 T} D_{j}^{2})\)<br/> 근데 \(s_{\tau} (i, j)\) 는 similarity score이니까 \(\text{exp}\) 안에 - 가 없어야 할 것 같음! 오타인가? 아니면 내가 잘못 생각하고 있나?<br/> <code class="language-plaintext highlighter-rouge">?????</code> <ul> <li>cross-entropy classification loss 꼴<br/> (regression loss 꼴 아님)<br/> \(\rightarrow\)<br/> 정확히 correct pixel pair (<code class="language-plaintext highlighter-rouge">positive sample</code>) \((i, j)\) 에 대해서는 \(s_{\tau} (i, j)\) 이 높아지고<br/> nearby pixel (<code class="language-plaintext highlighter-rouge">negative sample</code>) \((i+1, j)\) 에 대해서는 \(s_{\tau} (i+1, j)\) 이 낮아지도록 설계하여<br/> <code class="language-plaintext highlighter-rouge">nearby pixel로 regression하는 게 아니라</code> <code class="language-plaintext highlighter-rouge">정확한 correct pixel pair를 분류</code>하도록 하므로<br/> high-precision image matching 가능</li> <li>positive sample :<br/> pixel correspondence가 있는 pixel pair (2개 image에서 모두 나타나고 3D point가 일치하는 pixel pair)<br/> 1번 image의 \(i\)-th pixel이 2번 image의 \(j\)-th pixel과 correspondence 있다면<br/> \((i, j) \in \hat M = \{ (i, j) | \hat X_{i}^{1, 1} = \hat X_{j}^{2, 1} \}\)<br/> where \(X^{v, 1}\) : 1번 view-point를 중심좌표계로 두고 \(v\) 번 view에서 보이는 3D point 좌표</li> <li>negative sample :<br/> positive sample들을 모은 뒤<br/> \(\hat M\) 에서 대응되지 않는 pixel pair</li> <li>\(P^{1} = \{ i | (i, j) \in \hat M \}\) and \(P^{2} = \{ j | (i, j) \in \hat M \}\)<br/> 따라서 log 안의 분자는 positive sample의 score에 해당하고<br/> log 안의 분모는 negative sample의 score에 해당</li> </ul> </li> </ul> </li> </ul> <h3 id="fast-reciprocal-matching">Fast Reciprocal Matching</h3> <p>그렇다면 위에서 positive sample \(M\) 은 어떻게 찾을까?</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/4m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/4m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/4m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/4m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Fast Reciprocal Matching :<br/> feature space \(D\) 에서 reciprocal matching 수행 <ul> <li>기존 matching :<br/> Nearest Neighbor 기법 사용하여<br/> \(D^{1}\) 중에 \(D_{j}^{2}\) 와 가장 유사한 pixel이 \(D_{i}^{1}\) 이고,<br/> 동시에 \(D^{2}\) 중에 \(D_{i}^{1}\) 와 가장 유사한 pixel이 \(D_{j}^{2}\) 일 때<br/> 해당 \((i, j)\) pair에는 pixel correspondence가 있다고 함 <ul> <li>complexity \(O(W^{2} H^{2})\)</li> </ul> </li> <li>Fast Reciprocal matching :<br/> <code class="language-plaintext highlighter-rouge">연산 줄이기 위해 image의 부분 pixel들로 matching</code> 진행 <ul> <li>Step 1)<br/> image 1 에서 \(k\) 개의 pixel을 uniform sampling하여 \(U^{0}\) 로 표기</li> <li>Step 2)<br/> 기존 matching 방법대로 Nearest Neighbor 기법 사용하여<br/> mapping from \(U^{0}\) to \(V^{1}\) 진행<br/> (\(V^{t+1}\) : image 2에서 \(U^{t}\) 와 가장 유사한 pixel들의 집합)</li> <li>Step 3)<br/> 기존 matching 방법대로 Nearest Neighbor 기법 사용하여<br/> 다시 mapping from \(V^{1}\) to \(U^{1}\) 진행<br/> (\(U^{t+1}\) : image 1에서 \(V^{t+1}\) 와 가장 유사한 pixel들의 집합)</li> <li>Step 4)<br/> 만약 \(U^{t}\) 와 \(U^{t+1}\) 이 같다면 reciprocal pair로 저장<br/> \(M_{k}^{t} = \{ (i, j) | i \in U^{t+1}, j \in V^{t+1} \}\)</li> <li>Step 5)<br/> 또 다른 \(k\) 개의 pixel을 uniform sampling하여 Step 1) ~ Step 4)를 반복<br/> 이 때, 이전 loop에서 matching된 \(U^{t}\) 는 빼서 계산하므로 (\(U^{t+1} = U^{t+1} \setminus U^{t}\))<br/> 점점 un-converged point \(| U^{t} |\) 가 줄어들어 수렴</li> <li>Step 6)<br/> 최종적으로 reciprocal pair 집합 (positive sample) 만들 수 있음<br/> \(M_{k} = \cup_{t} M_{k}^{t}\)</li> <li>complexity \(O(kWH)\)<br/> (모든 pixel 조합을 비교하지 않음)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/5m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/5m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/5m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/5m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/6m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/6m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/6m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/6m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="coarse-to-fine-matching">Coarse-to-Fine Matching</h3> <ul> <li>Coarse-to-Fine Matching :<br/> <code class="language-plaintext highlighter-rouge">연산 줄이기 위해</code><br/> <code class="language-plaintext highlighter-rouge">저해상도 image pair에서 Fast Reciprocal Matching 수행하여 집중해야 할 영역(window pair)을 찾고,</code><br/> <code class="language-plaintext highlighter-rouge">고해상도 window pair에서 Fine Matching 수행하여 fine pixel correpondence 얻음</code><br/> low-resolution algorithm으로 high-resolution images를 match하기 위한 기법 <ul> <li>Step 1)<br/> \(k\) 배 subsampling하여 two downscaled images에서 Fast Reciprocal Matching 수행<br/> coarse pixel pair 집합을 \(M_{k}^{0}\) 으로 표기</li> <li>Step 2)<br/> 원본 고해상도 image 위에 grid of overlapping window \(\in R^{w \times 4}\) 를 만든 뒤<br/> (each window crop measures 512 pixels in its largest dimension <code class="language-plaintext highlighter-rouge">?????</code>)<br/> (인접한 windows overlap by 50%)<br/> coarse pixel pair \(M_{k}^{0}\) 를 가장 많이 포함하는 window pair \((w_{1}, w_{2}) \in W^{1} \times W^{2}\) 찾음</li> <li>Step 3)<br/> coarse pixel pair \(M_{k}^{0}\) 의 90%가 커버될 때까지<br/> greedy fashion으로 window pair를 추가</li> <li>Step 4)<br/> 최종적으로 each window pair를 두 이미지로 보고,<br/> each window pair에 대해 각각 matching 수행하여 fine pixel pair 집합 구함<br/> Then they are finally mapped back to the original image coordinates and concatenated,<br/> thus providing dense full-resolution matching</li> </ul> </li> </ul> <h3 id="experiments">Experiments</h3> <ul> <li>Map-free Localization</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/7m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/7m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/7m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/7m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/9m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/9m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/9m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/9m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/8m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/8m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/8m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/8m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Multi-view Relative Pose Estimation</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/10m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/10m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/10m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/10m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Visual Localization</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/11m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/11m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/11m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/11m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Multi-view 3D Reconstruction</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/12m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/12m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/12m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/12m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/13m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/13m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/13m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/13m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="future-work">Future Work</h3> <ul> <li>pose 없이 2D-to-3D 수행하는 DUSt3R, MASt3R에<br/> 3DGS를 적용한 NoPoSplat <a href="https://noposplat.github.io/">Link</a></li> </ul> <h3 id="question">Question</h3> <ul> <li> <p>Q1 :<br/> DUSt3R와 달리 MASt3R에서는 averaged depth 값으로 나누지 않는 이유를 다시 설명해주실 수 있을까요?</p> </li> <li>A1 : <ul> <li>DUSt3R에서는 3D recon.이 목적이었기 때문에<br/> view-point가 달라지더라도 같은 크기의 물체를 recon.해야 함<br/> 즉, scale-invariant하도록 만들기 위해<br/> 각 view-point에서 averaged depth 값으로 나누어 normalize해주었음</li> <li>MASt3R에서는 서로 다른 scale의 images인 상태 그 자체에서 image matching task를 수행해야 하므로<br/> (scale을 고려해야 하므로)<br/> regression loss에서 scale(depth) normalization 파트를 없앰</li> </ul> </li> <li> <p>Q2 :<br/> 3D 상에서 matching을 수행하므로 image 1에서 잘 보이는 부분이 image 2에서 잘 보이지 않더라도 잘 matching된다고 하셨는데,<br/> 왜 3D 상에서 matching을 수행한다고 하는지 이해가 되지 않습니다.</p> </li> <li>A2 :<br/> Fast Reciprocal Matching 기법으로 두 가지 matching을 수행하는데, <ul> <li>predicted 3D pointmap을 이용한 geometrical matching :<br/> 2개의 image에 대한 3D pointmap을 겹쳤을 때 align되도록<br/> 3D 공간 상에서 pixel correspondence를 찾음 <ul> <li>\(X^{2, k}\) 중에 3D point \(X_{i}^{1, k}\) 와 가장 가까운 3D point가 \(X_{j}^{2, k}\) 이고,<br/> 동시에 \(X^{1, k}\) 중에 3D point \(X_{j}^{2, k}\) 와 가장 가까운 3D point가 \(X_{i}^{1, k}\) 일 때<br/> 두 pixel \(i, j\) 사이에 correspondence 있다고 함</li> </ul> </li> <li>predicted local feature를 이용한 feature-based matching</li> </ul> </li> <li> <p>Q3 :<br/> geometrical matching 결과랑 feature-based matching 결과를 어떻게 합치나요?</p> </li> <li> <p>A3 :<br/> 그 부분은 아직 살펴보지 못해서 코드를 한 번 봐야 알 수 있을 것 같습니다. 알아본 뒤 블로그 포스팅에 업데이트해놓도록 하겠습니다.<br/> TBD <code class="language-plaintext highlighter-rouge">?????</code></p> </li> <li> <p>Q4 :<br/> \(U^{t} = U^{t+1}\) 이면 reciprocal pair로 저장하는데<br/> 만약 실패한 pixels가 많으면 결국 complexity가 \(O(kWH)\) 로 낮아지지 않을 것 같은데<br/> complexity \(O(kWH)\) 가 어떻게 달성되나요?</p> </li> <li> <p>A4 :<br/> TBD <code class="language-plaintext highlighter-rouge">?????</code></p> </li> <li> <p>Q5 :<br/> Fast Reciprocal Matching에서 \(U^{t} \rightarrow V^{t+1}\) 와 \(V^{t+1} \rightarrow U^{t+1}\) 을 어떻게 정의하나요?<br/> \(U^{t} \rightarrow V^{t+1}\) 는 Nearest Neighbor mapping이고 \(V^{t+1} \rightarrow U^{t+1}\) 는 transformer로 학습된 mapping 으로 진행하는 식인가요?</p> </li> <li>A5 :<br/> 아니요, \(U^{t} \rightarrow V^{t+1}\) 와 \(V^{t+1} \rightarrow U^{t+1}\) 둘 다 Nearest Neighbor mapping입니다.<br/> \(D^{1}\) 중에 \(D_{j}^{2}\) 와 가장 유사한 pixel이 \(D_{i}^{1}\) 이고,<br/> 동시에 \(D^{2}\) 중에 \(D_{i}^{1}\) 와 가장 유사한 pixel이 \(D_{j}^{2}\) 일 때<br/> 해당 \((i, j)\) pair에는 pixel correspondence가 있다고 합니다.<br/> 즉, \(i\) 랑 가장 가까운 게 \(j\) 이더라도, \(j\) 랑 가장 가까운 게 \(i\) 가 아닐 수도 있다는 의미입니다.<br/> 따라서 \(i \in U^{t}\) 와 가장 가까운 게 \(j \in V^{t+1}\) 이고, \(j \in V^{t+1}\) 와 가장 가까운 게 \(i \in U^{t+1}\) 일 때 reciprocal pair로 저장(correspondence 존재)합니다.</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="point"/><category term="regression"/><category term="pose"/><category term="free"/><summary type="html"><![CDATA[Grounding Image Matching in 3D with MASt3R]]></summary></entry><entry><title type="html">DUSt3R</title><link href="https://semyeong-yu.github.io/blog/2024/DUSt3R/" rel="alternate" type="text/html" title="DUSt3R"/><published>2024-11-19T12:00:00+00:00</published><updated>2024-11-19T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/DUSt3R</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/DUSt3R/"><![CDATA[<h2 id="dust3r---geometric-3d-vision-made-easy-cvpr-2024">DUSt3R - Geometric 3D Vision Made Easy (CVPR 2024)</h2> <h4 id="shuzhe-wang-vincent-leroy-yohann-cabon-boris-chidlovskii-jerome-revaud">Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, Jerome Revaud</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2312.14132">https://arxiv.org/abs/2312.14132</a><br/> project website :<br/> <a href="https://europe.naverlabs.com/research/publications/dust3r-geometric-3d-vision-made-easy/">https://europe.naverlabs.com/research/publications/dust3r-geometric-3d-vision-made-easy/</a><br/> code :<br/> <a href="https://github.com/naver/dust3r">https://github.com/naver/dust3r</a><br/> reference :<br/> <a href="https://xoft.tistory.com/83">https://xoft.tistory.com/83</a></p> </blockquote> <h3 id="contribution">Contribution</h3> <ul> <li> <p>MVS(Multi-View Stereo) 분야에서는 일반적으로 camera param.를 알아야 해서<br/> SfM(Structure from Motion)을 사용해서 camera param. estimaton을 하지만<br/> 이는 많은 연산 필요</p> </li> <li> <p>DUSt3R :</p> <ul> <li>많은 연산을 필요로 하는 <code class="language-plaintext highlighter-rouge">SfM 생략</code> (pose-free)</li> <li>transformer 기반으로<br/> <code class="language-plaintext highlighter-rouge">2D(img pixel)-to-3D(point map)</code> mapping 예측하여<br/> <code class="language-plaintext highlighter-rouge">regression-based</code> 3D recon. 수행</li> <li>1번 view를 기준으로 2번 view의 3D points를 <code class="language-plaintext highlighter-rouge">상대적으로 align</code>하므로<br/> (3D point의 절대적인 위치를 추정하는 게 아니므로)<br/> intrinsic/extrinsic <code class="language-plaintext highlighter-rouge">camera param. 몰라도</code> ok</li> <li>predicted pointmap 기반으로<br/> intrinsic/extrinsic camera param. 추정 가능</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/1m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/1m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/1m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/1m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="algorithm">Algorithm</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/2m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/2m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/2m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/2m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Algorithm : <ul> <li>Step 1) input<br/> image 2장</li> <li>Step 2) ViT encoder<br/> 두 images의 feature 비교하기 위해<br/> <code class="language-plaintext highlighter-rouge">Siamese</code> (shared weight) 구조 사용</li> <li>Step 3) Transformer decoder<br/> 두 features의 관계를 학습하여<br/> aligned pointmap 만들기 위해<br/> <code class="language-plaintext highlighter-rouge">self-attention and cross-attention</code> 수행</li> <li>Step 4) Head output<br/> per-pixel <code class="language-plaintext highlighter-rouge">Pointmap</code> \(X_{i}^{v, 1} \in R^{W \times H \times 3}\)<br/> and<br/> per-pixel <code class="language-plaintext highlighter-rouge">Confidence</code> score \(C_{i}^{v, 1} \in R^{W \times H}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/3m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/3m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/3m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/3m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>1번 camera : base view, 2번 camera : reference view<br/> \(G_{i}^{1}\) : 1번 view feature의 Transformer Decoder에서 \(i\)-th Block<br/> \(G_{i}^{2}\) : 2번 view feature의 Transformer Decoder에서 \(i\)-th Block</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/4m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/4m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/4m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/4m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Pointmap :<br/> \(X^{1, 1}\) : 1번 view-point를 중심좌표계로 두고 1번 view에서 보이는 3D point 좌표<br/> \(X^{2, 1}\) : 1번 view-point를 중심좌표계로 두고 2번 view에서 보이는 3D point 좌표</p> </li> <li>Confidence score :<br/> \(C_{i}^{v, 1}\) : 1번 view 시점을 기준으로 \(v\) 번 view에서 보이는 \(i\)-th 3D point의 confidence score <ul> <li>물체인 부분에서는 3D point를 비교적 정확히 예측할 수 있으므로 confidence가 높고,<br/> 하늘 또는 반투명인 부분에서는 3D point를 정확하게 예측할 수 없으므로 confidence가 낮게 나옴</li> <li>\(C_{i}^{v, 1} = 1 + \text{exp}(\tilde C_{i}^{v, 1}) \gt 1\) 로 설정하여<br/> 하나의 view에만 존재해서 추정하기 어려운 3D point의 경우에는 extrapolate할 수 있도록 <code class="language-plaintext highlighter-rouge">???</code></li> </ul> </li> <li>1번 view를 기준으로 2번 view의 3D points를 <code class="language-plaintext highlighter-rouge">상대적으로 align</code>하므로<br/> 3D point의 <code class="language-plaintext highlighter-rouge">절대적인 위치를 추정하는 게 아니므로</code><br/> intrinsic/extrinsic <code class="language-plaintext highlighter-rouge">camera param. 몰라도</code> ok</li> </ul> <h3 id="loss">Loss</h3> <ul> <li>regression loss :<br/> \(L_{regr} (v, i) = \| \frac{1}{z} X_{i}^{v, 1} - \frac{1}{\bar z} \bar X_{i}^{v, 1} \|\) <ul> <li>\(i\) : each point, \(v\) : each view</li> <li>\(z = \text{norm}(X^{1, 1}, X^{2, 1})\) : averaged depth of prediction point</li> <li>\(\bar z = \text{norm}(\bar X^{1, 1}, \bar X^{2, 1})\) : averaged depth of GT point</li> <li> <table> <tbody> <tr> <td>$$\text{norm}(X^{1, 1}, X^{2, 1}) = \frac{1}{</td> <td>D^{1}</td> <td>+</td> <td>D^{2}</td> <td>} \sum_{v \in { 1, 2 }} \sum_{i \in D^{v}} | X_{i}^{v, 1} |$$ : 모든 depth 값에 대한 평균</td> </tr> </tbody> </table> </li> </ul> </li> <li>final loss :<br/> \(L_{conf} = \sum_{v \in \{ 1, 2 \}} \sum_{i \in D^{v}} C_{i}^{v, 1} L_{regr}(v, i) - \alpha \text{log} C_{i}^{v, 1}\) <ul> <li>\(C_{i}^{v, 1} L_{regr}(v, i)\) :<br/> confidence가 큰 <code class="language-plaintext highlighter-rouge">(확실한) point</code>에서는 GT와의 <code class="language-plaintext highlighter-rouge">regression loss</code> \(L_{regr}\) 가 더 <code class="language-plaintext highlighter-rouge">작도록</code></li> <li>\(- \alpha \text{log} C_{i}^{v, 1}\) : regularization term<br/> <code class="language-plaintext highlighter-rouge">confidence</code> \(C_{i}^{v, 1}\) 값이 <code class="language-plaintext highlighter-rouge">너무 작아지지 않도록</code></li> </ul> </li> </ul> <h3 id="experiments">Experiments</h3> <ul> <li>Model<br/> CroCo pre-trained model 사용하여 weight initialization <ul> <li>encoder : ViT-Large</li> <li>decoder : ViT-Base</li> <li>head : DPT (ViT를 Depth Estimation에 적용한 연구)</li> </ul> </li> </ul> <h3 id="downstream---stereo-pixel-matching">Downstream - stereo pixel matching</h3> <ul> <li>2개의 image에 대한 3D pointmap을 겹쳤을 때 align되도록<br/> <code class="language-plaintext highlighter-rouge">3D 공간 상에서 pixel correspondence</code>를 찾음 <ul> <li>\(X^{2, k}\) 중에 3D point \(X_{i}^{1, k}\) 와 가장 가까운 3D point가 \(X_{j}^{2, k}\) 이고,<br/> 동시에 \(X^{1, k}\) 중에 3D point \(X_{j}^{2, k}\) 와 가장 가까운 3D point가 \(X_{i}^{1, k}\) 일 때<br/> 두 pixel \(i, j\) 사이에 correspondence 있다고 함</li> <li>모든 pixel에 대해 correspondence가 생기지는 않음</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/5m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/5m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/5m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/5m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="downstream---camera-intrinsic-estimation">Downstream - camera intrinsic estimation</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/6m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/6m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/6m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/6m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>camera intrinsic :<br/> camera intrinsic을 추정한다는 것은<br/> 보통 sclaing matrix, 즉 focal length를 추정한다는 얘기임 <ul> <li>2D translation : principal point의 위치<br/> (보통 이미지의 정가운데)</li> <li>2D shear : 카메라가 기울어진 정도<br/> (보통 카메라는 기울어져 있지 않으므로 shear matrix는 고려 X)</li> <li>2D scaling : focal length</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/7m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/7m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/7m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/7m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>위의 수식 설명 : <ul> <li>focal length는 3D point를 2D image plane으로 projection시킬 때 사용됨<br/> Weiszfeld algorithm을 이용해서 2D 상에서 위의 반복 최적화 문제를 풀면<br/> 해당 optimal <code class="language-plaintext highlighter-rouge">focal length</code>를 가질 때 <code class="language-plaintext highlighter-rouge">2D image와 3D point가 align</code>됨</li> <li>camera-coordinate에서 최적화 수행<br/> where pixel-coordinate : 2D \((i, j) \in ([0, W], [0, H])\) (좌상단이 원점)<br/> where camera-coordinate : 2D $$i^{‘}, j^{‘} \in ([-\frac{W}{2}, \frac{W}{2}], [-\frac{H}{2}, \frac{H}{2}]) (정중앙이 원점)<br/> where world-coordinate : 3D</li> </ul> </li> </ul> <h3 id="downstream---camera-extrinsic-estimation">Downstream - camera extrinsic estimation</h3> <ul> <li>Relative Pose Estimation : <ul> <li>방법 1)<br/> 위에서 언급한 2D pixel matching과 intrinsic estimation을 수행한 뒤<br/> Eight-Point algorithm 등 이용해서<br/> epipolar(essential) matrix와 relative pose를 추정</li> <li>방법 2)<br/> 서로 다른 view 시점에서 보이는 3D pointmap이 동일해지도록<br/> SVD-based procrustes alignment algorithm 이용해 3D 상에서 반복 최적화 문제를 풀어서<br/> optimal rotation matrix \(R\), translation vector \(t\), scale factor \(\sigma\) 추정 <ul> <li>procrustes alignment algorithm은 noise 및 outlier에 민감하므로<br/> 주어진 3D point와 corresponding 2D point를 바탕으로 camera pose를 추정하는 PnP algorithm과<br/> random sampling 방식의 RANSAC (Random Sample Consensus) algorithm 이용해서 위 수식의 해를 찾음<br/> <code class="language-plaintext highlighter-rouge">?????</code></li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/8m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/8m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/8m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/8m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Absolute Pose Estimation (visual localization) : <ul> <li>방법 1)<br/> 위에서 언급한 2D pixel matching과 instrinsic estimation을 수행한 뒤<br/> PnP RANSAC algorithm 이용해서 optimal rotation matrix 및 translation vector 추정</li> <li>방법 2)<br/> GT pointmap을 이용<br/> 즉, 위에서 언급한 Relative Pose Estimation을 수행할 때<br/> 해당 GT로 scale을 맞춰서 진행</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/9m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/9m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/9m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/9m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Experiment on Absolute Pose Estimation : <ul> <li>test dataset :<br/> 7Scenes, Cambridge Landmark<br/> (training에 사용되지 않은 dataset)</li> <li>각 값은 translation error (cm) / rotation error (degree)</li> <li>방식 :<br/> query image가 주어지면<br/> 가장 관련 있는 image를 test dataset에서 찾아<br/> 2개 image 간의 pixel을 matching하여 Absolute Camera Pose 계산<br/> (근데 query image와 test dataset image 간의 GT camera pose가 존재하나 <code class="language-plaintext highlighter-rouge">???</code>)</li> <li>비교 :<br/> FM(feature matching 기법), E2E(end-to-end learning 기법)과 비교했을 때<br/> SOTA 성능은 아니지만<br/> DUSt3R이 visual localization을 목적으로 학습되지 않았는데도 오차가 작다는 것을 확인할 수 있음</li> </ul> </li> </ul> <h3 id="downstream---global-alignment">Downstream - Global Alignment</h3> <ul> <li>Global Alignment :<br/> 3장 이상의 images로부터 예측한 Pointmap을 3D space에서 align하는 방법 <ul> <li>여러 장의 images를 다루기 위해<br/> <code class="language-plaintext highlighter-rouge">Graph</code> 만듦 (각 image가 vertex이고, 같은 visual contents를 공유하고 있으면 edge)</li> <li>DUSt3R 이용해서<br/> 모든 edge pair에 대해 Pointmap \(X_{i}^{v, 1} \in R^{W \times H \times 3}\) 과 Confidence score \(C_{i}^{v, 1} \in R^{W \times H}\) 계산</li> <li>여러 장의 images로 3D 상에서 반복 최적화 문제 풀어서<br/> 여러 장의 images로부터 얻은 Pointmap들이 3D 상에서 align되도록 함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/10m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/10m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/10m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/10m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>위의 수식 설명 :<br/> 3D 상에서 위의 반복 최적화 문제를 풀어서<br/> optimal \(\xi_{i}^{v}, \sigma_{e}, P_{e}\) 얻으면<br/> \(N\) 개의 images로 얻은 \(N\) 개의 <code class="language-plaintext highlighter-rouge">3D Pointmap을 align</code>하여<br/> Global pointmap을 얻을 수 있음<br/> 코드 : <a href="https://github.com/naver/dust3r/blob/01b2f1d1e6c6c386f95a007406defb5b8a5d2042/dust3r/cloud_opt/optimizer.py">Code</a> <ul> <li>\(C_{i}^{v, e}\) : confidence score from DUSt3R prediction<br/> (image \(e\) 의 view 시점을 기준으로, image \(v\) view에서 보이는 \(i\)-th pixel에 대응되는 값)</li> <li>\(X_{i}^{v, e}\) : pointmap from DUSt3R prediction</li> <li>\(\xi_{i}^{v}\) : global pointmap in world-coordinate</li> <li>\(\sigma_{e}\) : edge로 연결되어 있는 2개 images 간의 scale factor<br/> (0이 되는 것을 방지하기 위해 \(\prod_{e} \sigma_{e} = 1\) 로 설계)</li> <li>\(P_{e}\) : edge로 연결되어 있는 2개 images 간의 relative pose</li> </ul> </li> <li>위의 방법은<br/> <code class="language-plaintext highlighter-rouge">전통적인 SfM bundle adjustment 방법과 달리</code><br/> <code class="language-plaintext highlighter-rouge">빠르고 단순하게 regression(gradient descent)-based</code>로 반복 최적화 문제 풂 <ul> <li>bundle adjustment :<br/> 2D reprojection error 최소화</li> <li>본 논문 :<br/> 2D reprojection 뿐만 아니라 3D projection error을 같이 최소화</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/13m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/13m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/13m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/13m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="downstream---depth-estimation">Downstream - Depth Estimation</h3> <ul> <li>Monocular Depth Estimation</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/11m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/11m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/11m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/11m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Multi-view Depth Estimation</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/12m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/12m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/12m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/12m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="downstream---dense-3d-reconstruction">Downstream - Dense 3D reconstruction</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/14m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/14m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/14m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/14m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="limitation">Limitation</h3> <ul> <li>각 downstream task에서 SOTA 급은 아니지만<br/> 그래도 pose 없이 높은 성능 이뤘다는 데에 의미가 있음</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="point"/><category term="regression"/><category term="pose"/><category term="free"/><summary type="html"><![CDATA[Geometric 3D Vision Made Easy (CVPR 2024)]]></summary></entry><entry><title type="html">Deblurring 3D Gaussian Splatting</title><link href="https://semyeong-yu.github.io/blog/2024/Deblurring3DGS/" rel="alternate" type="text/html" title="Deblurring 3D Gaussian Splatting"/><published>2024-10-30T12:00:00+00:00</published><updated>2024-10-30T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Deblurring3DGS</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Deblurring3DGS/"><![CDATA[<h2 id="deblurring-3d-gaussian-splatting-eccv-2024">Deblurring 3D Gaussian Splatting (ECCV 2024)</h2> <h4 id="byeonghyeon-lee-howoong-lee-xiangyu-sun-usman-ali-eunbyung-park">Byeonghyeon Lee, Howoong Lee, Xiangyu Sun, Usman Ali, Eunbyung Park</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2401.00834">https://arxiv.org/abs/2401.00834</a><br/> project website :<br/> <a href="https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/">https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/</a><br/> code :<br/> <a href="https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting">https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting</a></p> </blockquote> <blockquote> <p>핵심 :</p> <ol> <li>defocus blur 구현 :<br/> MLP로 covariance(rotation, scaling)의 변화량을 모델링해서<br/> covariance를 키워서<br/> defocus-blurred image 얻음</li> <li>camera motion blur 구현 :<br/> MLP로 position 및 covariance의 변화량을 모델링해서<br/> M개의 3DGS sets를 만든 뒤<br/> 이걸로 만든 M개의 sharp imgs를 average해서<br/> camera-motion-blurred image 얻음</li> <li>위의 MLP를 training에서만 사용하므로<br/> still real-time rendering at inference</li> <li>sparse point clouds 보상하기 위해 points 추가<br/> 또한 먼 거리에 있는 3DGS는 덜 prune out</li> </ol> </blockquote> <h3 id="introduction">Introduction</h3> <ul> <li>3DGS : <ul> <li>novel-view로 inference할 때<br/> NeRF는 새로운 각도를 MLP에 넣어야만 color, opacity 얻을 수 있지만<br/> 3DGS는 spherical harmonics, explicit 기법이라 새로운 각도에 대해서도 바로 color, opacity 얻을 수 있어서<br/> volume rendering이 빠름</li> <li>differentiable splatting-based rasterization with parallelism</li> </ul> </li> <li>본 논문 : <ul> <li>핵심 : <ul> <li>각 3DGS의 <code class="language-plaintext highlighter-rouge">covariance</code>를 수정하여 <code class="language-plaintext highlighter-rouge">blur(adjacent pixels의 혼합)를 모델링하는 작은 MLP</code> 사용</li> <li>training 시에는 MLP output 곱해서 blurry image를 생성하고<br/> inference 시에는 MLP 사용하지 않아서 real-time으로 sharp image 생성</li> </ul> </li> <li>문제 : <ul> <li>3DGS는 initial point cloud에 많이 의존하는데<br/> given images가 <code class="language-plaintext highlighter-rouge">blurry</code>하면 SfM은 유효한 feature를 식별하지 못해서 <code class="language-plaintext highlighter-rouge">매우 적은 수의 point</code> cloud를 추출함</li> <li>심지어 depth가 크면 SfM은 맨 끝에 있는 점을 거의 추출하지 않음</li> </ul> </li> <li>해결 : <ul> <li>sparse point cloud를 방지하고자<br/> <code class="language-plaintext highlighter-rouge">N-nearest-neighbor interpolation으로 points 추가</code></li> <li>먼 거리의 평면에 많은 Gaussian을 유지하기 위해<br/> <code class="language-plaintext highlighter-rouge">위치에 따라 Gaussian pruning</code></li> </ul> </li> <li>contribution :<br/> SOTA qualtiy인데 훨씬 빠른 rendering speed (\(\gt 200\) FPS)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/1m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/1m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/1m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/1m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Overall Architecture </div> <h3 id="related-works">Related Works</h3> <ul> <li>Image Deblurring : <ul> <li>\(g(x) = \sum_{s \in S_{h}} h(x, s) f(x) + n(x)\)<br/> where \(g(x)\) : blurry image and \(f(x)\) : latent sharp image<br/> where \(h(x, s)\) : blur kernel or PSF (Point Spread Function)<br/> where \(n(x)\) : additive white Gaussian noise (occurs in nature images)</li> <li>지금까지 2D image deblurring은 많이 연구되어 왔는데<br/> 3D scene deblurring은 3D view consistency 부족 때문에 연구하기 어려웠음</li> </ul> </li> <li>Fast NeRF : <ul> <li>방법 1)<br/> use additional data-structure to reduce the size and number of MLP layers<br/> but, fail to reach real-time view synthesis <ul> <li>grid-based :<br/> Hexplane, TensoRF, K-planes, Mip-grid, Masked wavelet representation, Direct voxel grid optimization, F2-nerf</li> <li>hash-based :<br/> InstantNGP, Zip-nerf</li> </ul> </li> <li>방법 2)<br/> trained param.을 faster representation으로 bake해서 real-time rendering <ul> <li>Baking neural radiance fields, Merf, Bakedsdf</li> </ul> </li> </ul> </li> <li>Deblurring NeRF :<br/> 자세한 건 <a href="https://semyeong-yu.github.io/blog/2024/DeblurNeRF/">Link</a> 참조 <ul> <li>DoF-NeRF <d-cite key="DofNeRF">[1]</d-cite> : <ul> <li>단점 :<br/> train하기 위해 all-in-focus image와 blurry image 모두 필요<br/> (all-in-focus image : 화면 전체가 초점이 맞춰져 있는 image)</li> </ul> </li> <li>Deblur-NeRF <d-cite key="DeblurNeRF">[2]</d-cite> : <ul> <li>장점 :<br/> train할 때 all-in-focus image 필요 없음</li> <li>핵심 :<br/> additional small MLP 사용해서<br/> per-pixel blur kernel 예측</li> </ul> </li> <li>DP-NeRF <d-cite key="DpNeRF">[3]</d-cite> and PDRF <d-cite key="PDRF">[4]</d-cite> : <ul> <li>Deblur-NeRF 발전시킴</li> </ul> </li> <li>Hybrid <d-cite key="Hybrid">[5]</d-cite> and Sharp-NeRF <d-cite key="SharpNeRF">[6]</d-cite> and BAD-NeRF <d-cite key="BADNeRF">[7]</d-cite> : <ul> <li>camera motion blur와 defocus blur 중 하나만 다룸</li> </ul> </li> </ul> </li> <li>Deblurring NeRF 요약 : <ul> <li>deblur task 잘 수행하지만<br/> NeRF 자체가 rendering time이 오래 걸림<br/> \(\rightarrow\)<br/> real-time differentiable rasterizer 이용하는<br/> 3DGS로 deblur task 수행하자!</li> </ul> </li> </ul> <h3 id="background">Background</h3> <ul> <li> <p>3DGS <a href="https://semyeong-yu.github.io/blog/2024/GS/">Link</a> 참고</p> </li> <li> <p>Blur :</p> <ul> <li>Defocus Blur :<br/> 렌즈의 <code class="language-plaintext highlighter-rouge">초점이 맞지 않아서</code> 흐려진 경우<br/> e.g. 인물 사진에서 인물만 초점이 맞고 배경은 흐릿한 경우</li> <li>Camera Motion Blur :<br/> 셔터가 열려 있는 동안 카메라가 움직이거나 피사체가 <code class="language-plaintext highlighter-rouge">움직여서</code> 흐려진 경우<br/> e.g. 달리는 자동차를 촬영한 경우</li> </ul> </li> </ul> <h3 id="defocus-blur">Defocus Blur</h3> <ul> <li>Motivation : <ul> <li>Defocus Blur는 일반적으로<br/> 실제 image와 PSF(point spread func.)(2D Gaussian function) 간의<br/> convolution으로 모델링<br/> 즉, a pixel이 주위 pixels에 영향을 미칠 경우 blur</li> <li>여기서 영감을 받아<br/> <code class="language-plaintext highlighter-rouge">covariance(크기)가 큰 3DGS는 Blur</code>를 유발하고<br/> <code class="language-plaintext highlighter-rouge">covariance(크기)가 작은 3DGS는 Sharp</code> image에 기여한다고 가정<br/> (covariance(dispersion)가 클수록 Gaussian이 더 많은 pixels에 걸쳐 있으니까<br/> 더 많은 이웃한 pixels 간의 interference 표현 가능)</li> <li>그렇다면 covariance \(\Sigma = R S S^{T} R^{T}\) 를 변경하여 Blur를 모델링해야겠다!</li> </ul> </li> <li>Defocus Blur를 모델링하는 MLP :<br/> \((\delta r_{j}, \delta s_{j}) = F_{\theta}(\gamma(x_{j}), r_{j}, s_{j}, \gamma(v))\)<br/> where input : \(j\)-th Gaussian’s position, rotation, scale, view-direction<br/> where output : \(j\)-th Gaussian’s rotation change, scale change<br/> (\(\gamma\) : positional encoding) <ul> <li>transformed 3DGS : <ul> <li>rotation quaternion : \(\hat r_{j} = r_{j} \cdot \text{min}(1.0, \lambda_{s} \delta r_{j} + (1 - \lambda_{s}))\)</li> <li>scaling : \(\hat s_{j} = s_{j} \cdot \text{min}(1.0, \lambda_{s} \delta s_{j} + (1 - \lambda_{s}))\) <ul> <li>\(\cdot\) : element-wise multiplication</li> <li>\(\lambda_{s}\) 로 scale하고 \((1 - \lambda_{s})\) 로 shift : for optimization stability <code class="language-plaintext highlighter-rouge">???</code></li> <li>rotation 및 scaling 변화량의 <code class="language-plaintext highlighter-rouge">최솟값을 1로 clip</code> :<br/> \(\hat s_{j} \geq s_{j}\) 이므로 transformed 3DGS는 <code class="language-plaintext highlighter-rouge">더 큰 covariance</code>를 가져서<br/> <code class="language-plaintext highlighter-rouge">Defocus Blur</code>의 근본 원인인 주변 정보의 interference을 모델링할 수 있게 됨</li> </ul> </li> </ul> </li> <li>inference :<br/> scaling factor로 covariance 변화시키는 게 blur kernel의 역할을 하므로<br/> <code class="language-plaintext highlighter-rouge">training</code> 시에는 <code class="language-plaintext highlighter-rouge">transformed 3DGS</code>가 <code class="language-plaintext highlighter-rouge">blurry</code> image를 생성하지만<br/> <code class="language-plaintext highlighter-rouge">inference</code> 시에는 MLP를 사용하지 않은 <code class="language-plaintext highlighter-rouge">기존 3DGS</code>가 <code class="language-plaintext highlighter-rouge">sharp</code> image를 생성<br/> \(\rightarrow\)<br/> training할 때는 MLP forwarding과 간단한 element-wise multiplication만 추가 비용이고,<br/> inference할 때는 MLP를 사용하지 않아 Vanilla-3DGS와 모든 단계가 동일하므로<br/> <code class="language-plaintext highlighter-rouge">추가 비용 없이 real-time rendering</code> 가능</li> </ul> </li> </ul> <h3 id="selective-blurring">Selective Blurring</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/2m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/2m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/2m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/2m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>초점에 의한 Defocus Blur는 <code class="language-plaintext highlighter-rouge">영역마다 흐린 수준이 다름</code><br/> 본 논문에서는 <code class="language-plaintext highlighter-rouge">각 3DGS마다</code> 다르게 \(\delta_{r}, \delta_{s}\) 를 추정하므로<br/> Gaussian의 covariance를 선택적으로 확대시킬 수 있어서<br/> 영역에 따라 다르게 blurring 할 수 있으므로<br/> <code class="language-plaintext highlighter-rouge">pixel 단위의 blurring</code>을 보다 유연하게 모델링 가능 <ul> <li>defocus blur가 심한 영역에 있는 3DGS는 \(\delta_{s}\) 가 더 크도록</li> <li>당연히 단일 유형의 Gaussian Blur kernel을 써서 평균 blurring을 모델링하는 것보다<br/> 본 논문에서처럼 3DGS마다 다른 Blur kernel을 적용하여 pixel 단위 blurring을 모델링하는 게 더 좋음!</li> </ul> </li> </ul> <h3 id="camera-motion-blur">Camera motion Blur</h3> <ul> <li> <p>셔터가 열려 있는 exposure time 동안<br/> camera movement가 있으면<br/> light intensities from multipe sources가 inter-mixed되어<br/> Camera motion Blur 발생</p> </li> <li> <p>Camera motion Blur를 모델링하는 MLP :<br/> \({(\delta x_{j}^{(i)}, \delta r_{j}^{(i)}, \delta s_{j}^{(i)})}_{i=1}^{M} = F_{\theta}(\gamma(x_{j}), r_{j}, s_{j}, \gamma(v))\)</p> <ul> <li>transformed 3DGS : <ul> <li>3D position : \(\hat x_{j}^{(i)} = x_{j} + \lambda_{p} \delta x_{j}^{(i)}\) (shift)</li> <li>rotation quaternion : \(\hat r_{j}^{(i)} = r_{j} \cdot \delta r_{j}^{(i)}\) (element-wise multiplication)</li> <li>scaling : \(\hat s_{j}^{(i)} = s_{j} \cdot \delta s_{j}^{(i)}\) (element-wise multiplication) <ul> <li>Camera motion Blur의 경우<br/> Defocus Blur와 달리 covariance를 무조건 키워야 되는 게 아니므로<br/> min-clip by 1.0 없음</li> </ul> </li> </ul> </li> <li>Camera motion Blur :<br/> \(I_{b} = \frac{1}{M} \sum_{i=1}^{M} I_{i}\) <ul> <li>셔터가 열려 있는 동안 카메라가 움직이는 각 discrete moment는<br/> 각 3DGS set에 대응됨</li> <li>\(j\)-th Gaussian 의 <code class="language-plaintext highlighter-rouge">camera movement</code>를 나타내기 위해<br/> <code class="language-plaintext highlighter-rouge">M개의 auxiliary 3DGS sets</code> 만들어서<br/> <code class="language-plaintext highlighter-rouge">M개의 clean images</code> rendering해서<br/> <code class="language-plaintext highlighter-rouge">average</code>해서 camera-motion-blurred image 얻음</li> </ul> </li> <li>inference :<br/> 마찬가지로 <code class="language-plaintext highlighter-rouge">inference</code> 시에는 MLP를 사용하지 않은 <code class="language-plaintext highlighter-rouge">기존 3DGS</code>가 clean image를 생성<br/> \(\rightarrow\)<br/> inference할 때는 MLP로 \(M\)-개의 3DGS sets 만들지 않고<br/> Vanilla-3DGS와 모든 단계가 동일하므로<br/> <code class="language-plaintext highlighter-rouge">추가 비용 없이 real-time rendering</code> 가능</li> </ul> </li> </ul> <h3 id="compensation-for-sparse-point-cloud">Compensation for Sparse Point Cloud</h3> <ul> <li> <p>문제 1)<br/> 3DGS는 initial point cloud에 많이 의존하는데<br/> given input multi-view images가 <code class="language-plaintext highlighter-rouge">blurry</code>하면<br/> SfM은 유효한 feature를 식별하지 못해서<br/> 매우 적은 수의 <code class="language-plaintext highlighter-rouge">sparse point cloud</code>를 추출함</p> </li> <li> <p>해결 :</p> <ul> <li>sparse point cloud를 방지하고자<br/> \(N_{st}\) iter. 후에 \(N_{p}\)-개의 points를 uniform \(U(\alpha, \beta)\) 에서 sampling하여 추가<br/> where \(\alpha\) : 기존 point cloud 위치의 최솟값<br/> where \(\beta\) : 기존 point cloud 위치의 최댓값</li> <li>새로운 point의 <code class="language-plaintext highlighter-rouge">색상은 KNN(K-Nearest-Neighbor) interpolation</code>으로 할당</li> <li>새로운 points를 uniform 분포에서 sampling해서 <code class="language-plaintext highlighter-rouge">빈 공간에 불필요한 points</code>가 생길 수 있으므로<br/> nearest neighbor까지의 거리가 threshold \(t_{d}\) 를 초과하는 points는 <code class="language-plaintext highlighter-rouge">폐기</code></li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/3m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/3m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/3m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/3m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/12m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/12m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/12m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/12m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 가운데는 without adding points, 오른쪽은 with adding extra points </div> <ul> <li> <p>문제 2)<br/> 심지어 depth of field가 크면<br/> SfM은 맨 끝에 있는 점을 거의 추출하지 않음</p> </li> <li> <p>해결 :<br/> Deblur-NeRF dataset은 forward-facing scene으로만 구성되어 있으므로<br/> dataset에 기록된 <code class="language-plaintext highlighter-rouge">z-axis 값</code>은 <code class="language-plaintext highlighter-rouge">relative depth</code> from any viewpoint라고 볼 수 있음</p> <ul> <li>방법 1) 먼 거리에 있는 3DGS 수 늘리기<br/> 먼 거리의 평면에 있는 3DGS에 대해 denisfy<br/> \(\rightarrow\)<br/> 과도한 densification은 Blur 모델링을 방해하고 추가 계산 비용 필요</li> <li>방법 2) <code class="language-plaintext highlighter-rouge">먼 거리에 있는 3DGS는 덜 prune out</code><br/> pruning threshold를 깊이에 따라 다르게 scaling<br/> as \(t_{p}, 0.9 t_{p}, \cdots , \frac{1}{w_{p}} t_{p}\)<br/> (먼 거리의 3DGS일수록 낮은 threshold) <br/> \(\rightarrow\)<br/> real-time rendering을 고려했을 때<br/> 유연한 pruning으로도 먼 거리의 3DGS sparsity를 보상하기에 충분하다는 걸 경험적으로 발견</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/4m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/4m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/4m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/4m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="experiment">Experiment</h3> <ul> <li>Setting : <ul> <li>dataset : Deblur-NeRF dataset <ul> <li>have both synthetic and real images</li> <li>has camera motion blur or defocus blur</li> </ul> </li> <li>GPU : NVIDIA RTX 4090 GPU (24GB)</li> <li>optimzier : Adam</li> <li>iter. : \(20,000\)</li> <li>Blur를 모델링하는 small MLP : <ul> <li>lr : \(1e^{-3}\)</li> <li>hidden layer : 4 <ul> <li>3 layers : shared</li> <li>1 layer : head for each \(\delta\)</li> </ul> </li> <li>hidden unit : 64</li> <li>activation : ReLU</li> <li>initialization : Xavier</li> <li>scaling factor for \(\delta\) : \(\lambda_{s}, \lambda_{p} = 1 e^{-2}\)</li> </ul> </li> <li>sparse point cloud를 보상하기 위해 <ul> <li>\(N_{st} = 2,500\) iter. 후에 \(N_{p}\) 개의 point 추가<br/> \(N_{p}\) 는 기존 point cloud 규모에 비례하며 최대 200,000개</li> <li>색상은 \(K = 4\) 의 KNN interpolation으로 할당</li> <li>nearest neighbor까지의 거리가 \(t_{d} = 2\) 을 초과하는 point는 폐기</li> </ul> </li> <li>먼 거리에 있는 3DGS는 덜 pruning하기 위해<br/> pruning threshold를 깊이에 따라 다르게 scaling <ul> <li>pruning threshold \(t_{p} = 5 e^{-3}\) and densification threshold \(2 e^{-4}\)<br/> for real defocus blur dataset</li> <li>pruning threshold \(t_{p} = 1 e^{-2}\) and densification threshold \(5 e^{-4}\)<br/> for real camera motion blur dataset</li> <li>pruning threshold multiplier \(w_{p} = 3\)</li> </ul> </li> <li>camera motion blur를 구현하기 위해<br/> \(M = 5\) 개의 3DGS sets 만들어서<br/> \(M = 5\) 개의 clean images를 average</li> </ul> </li> </ul> <h3 id="results">Results</h3> <ul> <li>Results : <ul> <li><code class="language-plaintext highlighter-rouge">SOTA deblurring NeRF</code>만큼 <code class="language-plaintext highlighter-rouge">PSNR</code> 높음</li> <li><code class="language-plaintext highlighter-rouge">3DGS</code>만큼 <code class="language-plaintext highlighter-rouge">FPS</code> 높음</li> <li>비교 대상으로 쓰인 논문들 : <ul> <li>Deblur-NeRF, Sharp-NeRF, DP-NeRF, PDRF</li> <li>original 3DGS</li> <li>Restormer로 input training images 먼저 deblur한 뒤 original 3DGS</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/5m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/5m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/5m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/5m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/6m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/6m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/6m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/6m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> real-world Defocus Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/7m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/7m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/7m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/7m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> real-world Defocus Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/8m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/8m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/8m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/8m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> synthesized Defocus Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/9m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/9m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/9m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/9m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> synthesized Defocus Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/13m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/13m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/13m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/13m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> real-world Camera motion Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/14m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/14m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/14m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/14m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> real-world Camera motion Blur Dataset </div> <h3 id="ablation-study">Ablation Study</h3> <ul> <li>Ablation Study : <ul> <li>Extra points allocation</li> <li>Depth-based pruning</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/10m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/10m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/10m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/10m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Extra points allocation </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/11m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/11m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/11m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/11m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Depth-based pruning </div> <h3 id="limitation-and-future-work">Limitation and Future Work</h3> <ul> <li>Limitation : <ul> <li>volumetric rendering 기반의 NeRF-based deblurring 기법들을<br/> rasterization 기반의 3DGS에 적용하기 어렵<br/> \(\rightarrow\)<br/> MLP로 <code class="language-plaintext highlighter-rouge">world-space</code>에서의 rays 또는 kernels를 변형하는 대신<br/> MLP로 <code class="language-plaintext highlighter-rouge">rasterized image space</code>에서의 kernels를 변형하면<br/> Deblurring 3DGS 구현 가능<br/> \(\rightarrow\)<br/> 하지만 kernel interpolation 방향으로 가면<br/> pixel interpolation은 추가 비용이 발생하며<br/> 3DGS의 geometry를 implicitly 변형하는 것일 뿐이므로<br/> 해당 방법은 3DGS로 blur를 모델링하는 최적의 방법이 아닐 것이다<br/> 이를 개선하기 위한 future works 필요</li> </ul> </li> </ul> <h3 id="code-review">Code Review</h3> <ul> <li>blur kernel 함수 :<br/> Defocus Blur 및 Camera motion Blur <ul> <li>정의 : <a href="https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/scene/blur_kernel.py#L74">https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/scene/blur_kernel.py#L74</a></li> <li>호출 : <a href="https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/gaussian_renderer/__init__.py#L101">https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/gaussian_renderer/<strong>init</strong>.py#L101</a></li> </ul> </li> <li>sparse point cloud 보상하기 위해 add points : <ul> <li><a href="https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/scene/gaussian_model.py#L444">https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/scene/gaussian_model.py#L444</a></li> </ul> </li> </ul> <h3 id="question">Question</h3> <ul> <li> <p>Q1 :<br/> small MLP는 어떤 architecture로 구성되어 있나요?<br/> Dust3R 기반의 논문들을 보면<br/> transformer 등 pre-trained complex model 가져와서 쓰는데<br/> feed-forward 방식으로 학습하므로<br/> 빠르면서도 성능이 좋습니다. 이를 적용할 수 있지 않을까요?</p> </li> <li> <p>A1 :<br/> 일단 본 논문에서는 fast training 유지하기 위해 shallow MLP가 simply fc layers로 구성되어 있습니다<br/> 말씀해주신대로 simple shallow MLP 대신 더 좋은 network 쓰면 성능이 더 좋아질 것 같다고 생각하는데,<br/> deblurring task를 다룬 본 논문 이후의 논문들을 아직 읽어보지 않아서<br/> 혹시 읽어보고 좋은 아이디어 있다면 공유하도록 하겠습니다.</p> </li> <li> <p>Q2 :<br/> 본 논문이 deblurring task를 위해 pre-trained 3DGS를 가져와서 fine-tuning하는 것인가요?</p> </li> <li> <p>A2 :<br/> 아닙니다. 기존 3DGS에 blur를 모델링하는 MLP만 추가해서 scratch부터 training하고,<br/> 이로써 input image가 더러워도(blurry하더라도) clean image를 rendering할 수 있게 됩니다.<br/> 그리고 기존 3DGS와 같이 per-scene 방식으로 학습하는 것으로 알고 있습니다.</p> </li> <li> <p>Q3 :<br/> blurry input image를 dataset에서 미리 빼버리면 deblurring을 해야 하는 상황이 없어지잖아요<br/> 이처럼 제가 생각하기에는 굳이 deblurring을 해야 하나 라는 생각이 듭니다.</p> </li> <li> <p>A3 :<br/> 일단 deblurring이라는 게 super-resolution처럼 하나의 task로 생각할 수 있습니다<br/> input image가 blurry 할 수 있는데 말씀해주신대로 이를 dataset에서 미리 뺀다는 것 자체가 manual effort를 필요로 합니다 (이를 model이 대신 해준다면 좋겠죠)<br/> 그리고 만약 주어진 모델이 deblurring을 수행할 수 있다면 다른 모델의 앞단에 쓰여서 blur를 제거하는 pre-processing 용도로도 쓰일 수 있습니다.<br/> 이로써 input images가 현실에서 있을 법한 더러운(blurry) 이미지더라도 상관 없이 input으로 쓸 수 있습니다.<br/> 2D image 또는 video를 deblurring하는 논문들은 이미 많이 있는데<br/> 3D scene deblurring의 경우에는 3D view consistency 때문에 어려움이 있었습니다.<br/> 그러다가 3DGS 등장 이후로 처음 3DGS deblurring을 시도한 논문이 본 논문이라고 보시면 될 것 같습니다.</p> </li> <li> <p>Q4 :<br/> 그렇다면 deblurring task라는 게 uncertainty를 해결하는 것이라고 볼 수 있을까요? 아니면 이것과는 별개의 task로 봐야 할까요?</p> </li> <li> <p>A4 :<br/> (3D recon. 및 novel view synthesis에서 uncertainty라는 용어가 자주 등장하는데, 관련 논문들을 아직 많이 읽어보지 않아서 확실하게 답변드리지 못하겠습니다.)</p> </li> <li> <p>Q5 :<br/> dataset에 있는 image들이 blurry하지 않고 clean(sharp) 하더라도<br/> camera explore 하면서 novel view에 대해 rendering을 하다보면 rendered image에 blur가 생길 수 있을 것 같은데<br/> deblurring이라는 게 이러한 blur도 제거해주나요?</p> </li> <li> <p>A5 :<br/> 일단 본 논문에서 deblur를 하는 원리는 covariance를 조정하는 MLP로 blur를 모델링하여<br/> 해당 MLP(blur 담당)를 사용하지 않는 inference에서는 deblurred image가 rendering되는 것입니다<br/> 하지만 input이 blurry해서 생긴 blur가 아니라 rendering하다보니 생기는 artifacts로서의 blur의 경우라면<br/> 해당 MLP가 artifacts로서의 blur도 잘 모델링해줄지는 모르겠습니다. 더 찾아봐야 할 것 같습니다.</p> </li> <li> <p>Q6 :<br/> 혹시 본 논문을 읽으면서 생각해보셨던 limitation이 있을까요? 논문에 적혀있는 것 말고 개인적인 생각이 있으신지 궁금합니다.<br/> 저는 뭔가 본 논문의 알고리즘이 artificial하다는 생각이 들었습니다.</p> </li> <li> <p>A6 :<br/> (개인적으로 생각해본 limitation 답변 못 드림 ㅠㅠ 앞으로는 논문 읽을 때 novelty 말고도 limitation이 무엇일지 생각하는 습관 길러보자!)<br/> 기존 deblur nerf에서는 deblur kernel을 이용해서 여러 ray를 쏴서 2D 상에서 pixel들을 interpolate해서 blur를 모델링하는데<br/> deblurring 3DGS에서는 3D 상에서 Gaussian covariance를 키우는 방식으로 interpolate를 비슷하게 구현했다는 논리(가정)이고<br/> 결과적으로 성능이 좋게 나왔으니 본인들 주장(가정)이 맞았다 인 것 같아서 말씀해주신대로 artificial한 느낌이 들긴 하네요</p> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="3DGS"/><category term="deblur"/><summary type="html"><![CDATA[ECCV 2024]]></summary></entry><entry><title type="html">EE534 Pattern Recognition Final</title><link href="https://semyeong-yu.github.io/blog/2024/Pattern2/" rel="alternate" type="text/html" title="EE534 Pattern Recognition Final"/><published>2024-10-28T11:00:00+00:00</published><updated>2024-10-28T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Pattern2</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Pattern2/"><![CDATA[<blockquote> <p>Lecture :<br/> 24F EE534 Pattern Recognition<br/> by KAIST Munchurl Kim <a href="https://www.viclab.kaist.ac.kr/">VICLab</a></p> </blockquote> <h2 id="chapter-5-linear-discriminant-functions">Chapter 5. Linear Discriminant Functions</h2> <h3 id="linearly-non-separable-svm">Linearly Non-Separable SVM</h3> <ul> <li>new constraint :<br/> \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}\)<br/> \(\xi_{i}\) 를 도입하여 이제는 inside margin or misclassified 도 가능하지만 대신 \(C \sum_{i=1}^{N} \xi_{i}\) 를 loss에 넣어서 큰 \(\xi_{i}\) 값을 penalize <ul> <li>\(\xi = 0\) : outside margin or support vector</li> <li>\(0 \lt \xi \leq 1\) : inside margin (correctly classified, but margin violation)</li> <li>\(\xi \gt 1\) : misclassified</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/2m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/2m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/2m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/2m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>방법 1) 1-norm-soft-margin <ul> <li>constrained primal form :<br/> minimize \(J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \xi_{i}\)<br/> subject to \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}\) and \(\xi_{i} \geq 0\) <ul> <li>unconstrained primal form :<br/> 이 때 위의 두 가지 constraints는 \(\xi_{i} = \text{max}(0, 1 - y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}))\) 로 하나로 합칠 수 있음<br/> 따라서<br/> minimize \(J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \text{max}(0, 1 - y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}))\)</li> <li>regularization param. \(C\) : <ul> <li>small \(C\) : 큰 \(\xi_{i}\) 값도 허용하므로 margin 커짐</li> <li>large \(C\) : 큰 \(\xi_{i}\) 값은 허용 안 하므로 margin 작아짐</li> <li>\(C = \infty\) : non-zero \(\xi_{i}\) 값 허용 안 하므로 hard margin (no sample inside margin)<br/> (Linearly Separable SVM 에 해당함)</li> </ul> </li> </ul> </li> <li>Lagrangian :<br/> minimize \(L(\boldsymbol w, w_{0}, \xi, \boldsymbol \lambda, \boldsymbol \mu) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \xi_{i} - \sum_{i}^{N} \mu_{i} \xi_{i} - \sum_{i}^{N} \lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (1 - \xi_{i}))\)<br/> subject to \(\xi_{i}, \mu_{i}, \lambda_{i} \geq 0\) <ul> <li> \[\nabla_{\boldsymbol w} L = 0 \rightarrow \boldsymbol w = \sum_{i=1}^{N} \lambda_{i} y_{i} \boldsymbol x_{i}\] </li> <li> \[\nabla_{w_{0}} L = 0 \rightarrow \sum_{i=1}^{N} \lambda_{i} y_{i} = 0\] </li> <li> \[\nabla_{\xi_{i}} L = 0 \rightarrow C - \mu_{i} - \lambda_{i} = 0\] </li> </ul> </li> <li>KKT condition 중 slackness condition : <ul> <li> \[\lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (1 - \xi_{i})) = 0\] </li> <li> \[\mu_{i} \xi_{i} = 0\] </li> </ul> </li> <li>dual form :<br/> 위의 세 가지 식을 대입하여 \(\boldsymbol w, w_{0}, \xi_{i}, \mu_{i}\) 를 소거하면<br/> maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}\)<br/> subject to \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\) and \(0 \leq \lambda_{i} \leq C\)</li> <li>Summary : <ul> <li>Step 1) optimal \(\lambda_{i}^{\ast}\) 구하기<br/> \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\) and \(0 \leq \lambda_{i} \leq C\) 이용해서<br/> \(\nabla_{\lambda_{i}} L = 0\) 으로 아래의 dual form 풀어서<br/> (maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}\))<br/> optimal \(\lambda_{i}\) 얻음</li> <li>Step 2) optimal \(\boldsymbol w^{\ast}, w_{0}^{\ast}\) 구하기 <ul> <li>\(\boldsymbol w^{\ast} = \sum_{i=1}^{N_{s}} \lambda_{i}^{\ast} y_{i} x_{i}\)<br/> (\(N_{s}\) : support vector 개수)<br/> (hyperplane 결정할 때는 \(\lambda_{i} \gt 0\) 중에 \(\xi = 0\) 인 support vectors만 고려!!)</li> <li>\(w_{0}^{\ast} = \frac{1}{y_{j}} - \sum_{i=1}^{N_{s}} \lambda_{i}^{\ast} y_{i} x_{i}^{T} x_{j} = \frac{1}{y_{j}} - \boldsymbol w^{\ast T} x_{j}\)<br/> (support vector \(x_{j}\) 1개 사용)<br/> 또는<br/> \(w_{0}^{\ast} = \frac{1}{N_{s}} \sum_{j=1}^{N_{s}} (\frac{1}{y_{j}} - \sum_{i=1}^{N_{s}} \lambda_{i}^{\ast} y_{i} x_{i}^{T} x_{j}) = \frac{1}{N_{s}} \sum_{j=1}^{N_{s}} (\frac{1}{y_{j}} - \boldsymbol w^{\ast T} x_{j})\)<br/> (support vector \(x_{j}\) \(N_{s}\)-개 모두 사용하여 average value)</li> </ul> </li> <li>Tip : hard margin (no sample inside margin) 의 경우<br/> 육안으로 어떤 sample이 support vector일지 판단 가능하다면<br/> complementary slackness condition (\(\lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (1 - \xi_{i})) = 0\)) 에서<br/> support vector만 \(\lambda_{i} \gt 0\) 이므로<br/> 연립해서 \(\boldsymbol w^{\ast}, w_{0}^{\ast}\) 바로 구할 수 있음</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/1m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/1m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/1m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/1m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>방법 2) 2-norm-soft-margin <ul> <li>차이점 1) primal form<br/> minimize \(J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \xi_{i}\)<br/> subject to \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}\) and \(\xi_{i} \geq 0\)<br/> 대신<br/> minimize \(J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + \frac{1}{2} C \sum_{i=1}^{N} \xi_{i}^{2}\)<br/> subject to \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}\) and \(\xi_{i} \geq 0\)</li> <li>차이점 2) Lagrangian<br/> \(\nabla_{\xi_{i}} L(\boldsymbol w, w_{0}, \boldsymbol \xi, \boldsymbol \lambda, \boldsymbol \mu) = 0\) 했을 때<br/> \(C - \mu_{i} - \lambda_{i} = 0\)<br/> 대신<br/> \(C \xi_{i} - \mu_{i} - \lambda_{i} = 0\)</li> <li>차이점 3) dual form<br/> maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}\)<br/> subject to \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\) and \(0 \leq \lambda_{i} \leq C\)<br/> 대신<br/> maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j} - \frac{1}{2C} \sum_{i=1}^{N} (\lambda_{i} + \mu_{i})^{2}\)<br/> subject to \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\) and \(0 \leq \lambda_{i} \leq C\)</li> </ul> </li> <li>Remark : <ul> <li>Linearly Non-Separable SVM에서<br/> \(C \rightarrow \infty\) 하면 Linearly Separable SVM<br/> e.g. non-linear에서는 \(0 \leq \lambda_{i} \leq C\) 인데, linear에서는 \(0 \leq \lambda_{i} \lt \infty\)</li> <li>SVM의 한계 :<br/> high computational complexity<br/> (SVM training은 주로 batch mode로 진행되어 memory를 많이 필요로 하므로<br/> training dataset을 subset으로 나눠서 training 진행)</li> <li>지금까지는 SVM for two-category만 살펴봤는데,<br/> M-class 의 경우 M개의 discriminant function \(g_{i}(x)\) 를 design하여<br/> assign \(x\) to class \(w_{i}\) if \(i = \text{argmax}_{k} g_{k}(x)\)</li> </ul> </li> </ul> <h3 id="v-svm">v-SVM</h3> <ul> <li>v-SVM : <ul> <li>hyperplane<br/> \(\boldsymbol w^{T} \boldsymbol x_{i} + w_{0} = \pm 1\)<br/> 대신<br/> \(\boldsymbol w^{T} \boldsymbol x_{i} + w_{0} = \pm \rho\)<br/> where \(\rho \geq 0\) : var. to be optimized</li> <li>margin<br/> margin은 \(\frac{2 \rho}{\| w \|}\) 이므로<br/> margin을 maximize하려면<br/> \(\| w \|\) minimize 뿐만 아니라 \(\rho\) maximize하면 되므로<br/> 둘 다 primal form loss term에 넣음</li> <li>primal form<br/> minimize \(J(\boldsymbol w, \xi, \rho) = \frac{1}{2} \| \boldsymbol w \|^{2} - v \rho + \frac{1}{N} \sum_{i=1}^{N} \xi_{i}\)<br/> subject to \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq \rho - \xi_{i}\) and \(\xi_{i} \geq 0\) and \(\rho \geq 0\)</li> <li>Lagrangian<br/> \(L(\boldsymbol w, w_{0}, \boldsymbol \xi, \rho, \boldsymbol \lambda, \boldsymbol \mu, \delta) = \frac{1}{2} \| \boldsymbol w \|^{2} - v \rho + \frac{1}{N} \sum_{i=1}^{N} \xi_{i} - \sum_{i}^{N} \mu_{i} \xi_{i} - \sum_{i}^{N} \lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (\rho - \xi_{i})) - \delta \rho\) <ul> <li>\(\nabla_{\boldsymbol w} L = 0\) 했을 때<br/> \(\boldsymbol w = \sum_{i=1}^{N} \lambda_{i} y_{i} \boldsymbol x_{i}\)</li> <li>\(\nabla_{w_{0}} L = 0\) 했을 때<br/> \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\)</li> <li>\(\nabla_{\xi_{i}} L = 0\) 했을 때<br/> \(\mu_{i} + \lambda_{i} = \frac{1}{N}\)</li> <li>\(\nabla_{\rho} L = 0\) 했을 때<br/> \(\sum_{i=1}^{N} \lambda_{i} - \delta = v\)</li> </ul> </li> <li>KKT condition 중 complementary slackness<br/> For \(\lambda_{i} \geq 0\) and \(\mu_{i} \geq 0\) and \(\delta \geq 0\), <ul> <li> \[\lambda_{i} (y_{i}(\boldsymbol w^{T} \boldsymbol x + w_{0}) - (\rho - \xi_{i})) = 0\] </li> <li> \[\mu_{i} \xi_{i} = 0\] </li> <li> \[\delta \rho = 0\] </li> </ul> </li> <li>dual form<br/> maximize \(L(\lambda) = - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}\)<br/> subject to \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\) and \(0 \leq \lambda_{i} \leq \frac{1}{N}\) and \(\sum_{i=1}^{N} \lambda_{i} = \delta + v \geq v\) <ul> <li>\(\lambda\) 만 explicitly 남아 있고,<br/> margin var. \(\rho\) 와 slack var. \(\xi_{i}\) 는 constraint의 bounds에 implicitly 들어 있음</li> <li>v-SVM에서는 \(\sum_{i=1}^{N} \lambda_{i}\) term이 없으므로<br/> optimal \(\lambda_{i}\) 는 quadratically homogeneous solution</li> <li>새로운 constraint \(\sum_{i=1}^{N} \lambda_{i} \geq v\) 있음</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/3m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/3m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/3m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/3m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Remark <ul> <li>v-SVM의 경우 \(0 \leq v \leq 1\) 이어야 optimizable</li> <li>C-SVM에 비해 v-SVM은<br/> error rate와 support vector 수 bound 측면에서 장점 <code class="language-plaintext highlighter-rouge">???</code></li> <li>\(\rho \gt 0\) 일 때 \(\delta = 0\) 이므로<br/> \(\sum_{i=1}^{N} \lambda_{i} = v\)</li> <li>loss (error)에 기여하는 애들은<br/> \(\xi_{i} \gt 0\), 즉 \(\mu_{i} = 0\), 즉 \(\lambda_{i} = \frac{1}{N}\) 이다<br/> 따라서 error rate = \(\sum_{i=1}^{N_{error}} \lambda_{i} = \frac{N_{error}}{N} \leq \sum_{i=1}^{N} \lambda_{i} = v\)<br/> 즉, error rate \(\frac{N_{error}}{N} \leq v\) 이고<br/> total number errors \(N_{error} \leq N v\)</li> <li>Since \(0 \lt \lambda_{i} \lt 1\) for support vector \(i\),<br/> \(v = \sum_{i=1}^{N} \lambda_{i} = \sum_{i=1}^{N_{s}} \lambda_{i} \leq \sum_{i=1}^{N_{s}} \frac{1}{N} = \frac{N_{s}}{N}\)<br/> 즉, \(vN \leq N_{s}\)<br/> (\(\lambda_{i} \gt 0\) 중에 \(\xi = 0\) 인 support vectors만 고려하면 \(\sum_{i=1}^{N} \lambda_{i} = \sum_{i=1}^{N_{s}}\) !!)</li> <li>\(\frac{N_{error}}{N} \leq v \leq \frac{N_{s}}{N}\) 이므로<br/> \(v\) optimize하면 error rate와 support vector 개수도 bound 알 수 있음</li> <li>support vector 수 \(N_{s}\) 는 classifier performance에 있어서 매우 중요<br/> (\(N_{s}\) 가 클수록 inner product 많이 계산해야 돼서 computational cost 높아짐)<br/> (\(N_{s}\) 가 크면 training set 이외의 data에 대한 performance가 제한되어 poor generalization)</li> </ul> </li> </ul> <h3 id="kernel-method-for-svm">Kernel Method for SVM</h3> <ul> <li> <p>discriminant function :<br/> \(x\) 의 inner product 꼴<br/> \(g(\boldsymbol x) = \boldsymbol w^{T} \boldsymbol x + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \boldsymbol x_{i}^{T} \boldsymbol x + w_{0}\)</p> </li> <li> <p>Cover’s theorem :<br/> non-linearly separable D-dim. space는<br/> linearly separable space of high enough dim. 으로 transform 될 수 있다<br/> (separating hyperplane의 optimality는 관심사 아님)</p> </li> <li> <p>Kernel Method for SVM :<br/> discriminant function \(g(\boldsymbol x) = \boldsymbol w^{T} \boldsymbol x + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \boldsymbol x_{i}^{T} \boldsymbol x + w_{0}\) 에서<br/> kernel \(K(\boldsymbol x_{i}, \boldsymbol x) = \boldsymbol x_{i}^{T} \boldsymbol x\)<br/> (inner product b.w. support vector and input vector)<br/> 대신<br/> 다른 kernel \(K(\boldsymbol x_{i}, \boldsymbol x) = \Phi(\boldsymbol x_{i})^{T} \Phi(\boldsymbol x)\) 을 써서<br/> non-linearly separable samples도 분류해보자!</p> <ul> <li>Step 1)<br/> input vector \(\boldsymbol x\) 와 training samples \(\boldsymbol x_{i}\) 를 <code class="language-plaintext highlighter-rouge">high-dim.으로 project</code> by function \(\Phi(\cdot)\)</li> <li>Step 2)<br/> transformed vector \(\Phi (\boldsymbol x)\) 와 \(\Phi (\boldsymbol x_{i})\) 에 대해 linear SVM 적용<br/> \(g(\boldsymbol x) = \boldsymbol w^{T} \Phi (\boldsymbol x) + w_{0}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/4m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/4m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/4m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/4m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Kernel Trick</code> :<br/> \(\boldsymbol x_{i}^{T} \boldsymbol x_{j}\) 대신 \(K(\boldsymbol x_{i}, \boldsymbol x_{j}) = \Phi(\boldsymbol x_{i})^{T} \Phi(\boldsymbol x_{j})\) 쓰면 됨!! <ul> <li>optimization of dual form :<br/> maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}\) <br/> 대신<br/> maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} K(\boldsymbol x_{i}, \boldsymbol x_{j})\)<br/> where \($K(\boldsymbol x_{i}, \boldsymbol x_{j}) = \Phi(\boldsymbol x_{i})^{T} \Phi(\boldsymbol x_{j})\)</li> <li>hyperplane :<br/> \(g(\boldsymbol x) = \boldsymbol w^{T} \boldsymbol x + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \boldsymbol x_{i}^{T} \boldsymbol x + w_{0} = 0\)<br/> 대신<br/> \(g(\boldsymbol x) = \boldsymbol w^{T} \Phi(\boldsymbol x) + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \Phi(\boldsymbol x_{i})^{T} \Phi(\boldsymbol x) + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} K(\boldsymbol x_{i}, \boldsymbol x) + w_{0} = 0\)<br/> where \(\boldsymbol w = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \Phi(\boldsymbol x_{i})\)</li> </ul> </li> <li>Remark : <ul> <li>polynomial learning machine, radial-basis function network, two-layer perceptron(single hidden layer) 와 같은<br/> kernel-based learning machine을 만들 때<br/> support vector learning algorithm을 사용 <ul> <li>polynomial :<br/> \(K(x, z) = (x^{T} z + 1)^{q}\) for \(q \gt 0\)</li> <li>radial-basis function :<br/> \(K(x, z) = \text{exp}(-\frac{\| x - z \|^{2}}{\sigma^{2}})\)</li> <li>hyperbolic tangent :<br/> \(K(x, z) = \text{tanh}(\beta x^{T} z + \gamma)\) where typical value is \(\beta = 2\) and \(\gamma = 1\)</li> </ul> </li> </ul> </li> <li>문제 풀이 예시 :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/6m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/6m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/6m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/6m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/5m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/5m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/5m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/5m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/7m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/7m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/7m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/7m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/8m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/8m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/8m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/8m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/9m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/9m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/9m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/9m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="chapter-6-multilayer-neural-networks">Chapter 6. Multilayer Neural Networks</h2> <ul> <li>activation function : <ul> <li>unipolar sigmoid :<br/> \(\phi (x) = \frac{1}{1 + exp(-x)}\)<br/> \(\phi^{'} (x) = \phi (x) (1 - \phi (x))\)</li> <li>bipolar sigmoid (tanh) :<br/> \(\phi (x) = \text{tanh} (x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\)<br/> \(\phi^{'} (x) = 1 - \text{tanh}^{2} (x) = 1 - \phi^{2} (x)\) <ul> <li>tanh가 sigmoid보다 gradient 더 커서 같은 \(\eta\) 일 때 학습 더 많이 함</li> </ul> </li> <li>ReLU</li> </ul> </li> <li>weight initialization : <ul> <li>zero-mean uniform distribution \(U(0, \sigma^{2})\)<br/> where \(\sigma^{2}\) is chosen so that std of induced local fields of neurons lie in the linear transition interval of sigmoid activation function</li> </ul> </li> <li>weight update :<br/> \(w_{ji}(n+1) = w_{ji}(n) + \eta \delta_{j} (n) y_{i} (n) + \alpha (w_{ji} (n) - w_{ji} (n-1))\)<br/> where \(\eta\) : learning rate<br/> where \(\alpha\) : momentum constant<br/> (momentum in back-prop has stabilizing effect when gradient has oscillate in sign)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/11m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/11m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/11m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/11m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="back-propagation-algorithm">Back-propagation Algorithm</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/10m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/10m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/10m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/10m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="issues-on-neural-networks">Issues on Neural Networks</h3> <ul> <li>Stopping criteria : <ul> <li>Euclidean norm of gradient reaches sufficiently small threshold</li> <li>absolute rate of change in average squared error per epoch is sufficiently small</li> <li>generalization performance (tested after each iter.) has peaked</li> </ul> </li> <li>Weight Update : <ul> <li>sample-by-sample mode :<br/> weights are updated after presenting each training sample<br/> \(w_{ji}(n+1) = w_{ji}(n) + \eta \delta_{j} (n) y_{i} (n) + \alpha (w_{ji} (n) - w_{ji} (n-1))\) <ul> <li>very sensitive to each sample so that the weight update term is very noisy</li> </ul> </li> <li>batch mode :<br/> weights are updated after presenting entire set of training samples<br/> \(w_{ji}(t+1) = w_{ji}(t) + \eta \frac{1}{N} \sum_{n=1}^{N} \delta_{j} (n) y_{i} (n) + \alpha (w_{ji} (t) - w_{ji} (t-1))\)</li> </ul> </li> <li>k-fold cross validation :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/12m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/12m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/12m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/12m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/13m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/13m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/13m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/13m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Normalization : Whitening <ul> <li>mean removal</li> <li>de-correlation</li> <li>scaling for equal covariance<br/> (then input var. in training set becomes uncorrelated)<br/> (then gradient descent converges faster)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/14m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/14m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/14m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/14m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Gradient Vanish 해결 방법 : <ul> <li>방법 1) Batch Normalization<br/> Batch Normalization을 해서 input이 \(0\) 주위의 가파른 부분에 머무르도록 함<br/> 근데 sigmoid의 경우 gradient가 [0, 0.25] 이고, tanh의 경우 gradient가 [0, 1] 이므로<br/> 여전히 gradient vanishing 문제 발생</li> <li>방법 2) ReLU activation<br/> ReLU의 gradient는 0 또는 1이므로<br/> gradient value 1의 경우 gradient vanishing 문제 없음</li> <li>방법 3) Residual Network<br/> skip connection 사용하여<br/> non-linear activation 통과하지 않고 바로 gradient가 흘러들어갈 수 있도록 함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/15m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/15m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/15m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/15m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/16m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/16m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/16m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/16m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Bias-Variance dilemma : <ul> <li>prediction error의 종류로는 bias, variance, irreducible error가 있음<br/> MSE \(E_{x, y, D}[\epsilon^{2} (x)] = E_{x, y, D}[(h_{D}(x) - y)^{2}]\)<br/> \(= E_{x, D}[(h_{D}(x) - \bar h(x))^{2}] + E_{x}[(\bar h(x) - \bar y(x))^{2}] + E_{x, y}[(\bar y(x) - y)^{2}]\) <ul> <li>variance of model : \(E_{x, D}[(h_{D}(x) - \bar h(x))^{2}]\)<br/> where \(h_{D}(x)\) : model output<br/> where \(\bar h(x)\) : model mean <ul> <li>different test dataset으로 테스트했을 때 변하는 정도</li> <li>particular training dataset에 overfitting된 정도</li> </ul> </li> <li>bias of model : \(\bar h(x) - \bar y(x)\)<br/> where \(\bar y(x)\) : label mean <ul> <li>data를 아무리 많이 학습시켜도 model 특성 때문에 발생하는 inherent error<br/> e.g. linear classifier is biased to a particular kind of solution</li> </ul> </li> <li>data noise : \(E_{x, y}[(\bar y(x) - y)^{2}]\) <ul> <li>ambiguity due to data distribution and feature representation</li> </ul> </li> </ul> </li> <li>trade-off : <ul> <li>too simple model : high bias<br/> \(\rightarrow\) 해결법 : pick more complex model</li> <li>too complex model : high variance<br/> \(\rightarrow\) naive 해결법 : use more data, but it requires high cost</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/17m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/17m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/17m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/17m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>SSE (Sum of Squared Errors) :<br/> 위의 유도 결과에서<br/> 두 번째 term은 model weight와 무관하므로<br/> back-propagation은 첫 번째 term만 minimize<br/> 즉, \(y_{k}(x; W) \approx P(w_{k} | x)\)<br/> 이 때, 잘 approx.하려면 model이 충분한 layers, neurons를 가지고 있어야 함 <ul> <li>\(y_{k}(x; W)\) : model output</li> <li> <table> <tbody> <tr> <td>$$P(w_{k}</td> <td>x)$$ : true posteriori probability (Bayes linear discriminant function)</td> </tr> </tbody> </table> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/18m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/18m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/18m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/18m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="chapter-7-stochastic-methods-for-pattern-classification">Chapter 7. Stochastic Methods for Pattern Classification</h2> <ul> <li>Learning : <ul> <li>deterministic learning :<br/> 무조건 loss 작아지는 방향으로 이동<br/> e.g. gradient descent</li> <li>stochastic learning :<br/> 작은 확률이겠지만 energy 커지는 방향으로 이동하는 것도 허용<br/> \(\rightarrow\) local minima에 빠지는 문제 해결하여 global minimum에 도달 가능 e.g. Simulated Annealing, Boltzmann Machine</li> </ul> </li> <li>Boltzmann Machine : <ul> <li>neuron : <ul> <li>visible neuron : \(\alpha = \{ \alpha^{i}, \alpha^{o} \}\)</li> <li>hidden neuron : \(\beta\)</li> </ul> </li> <li>probability \(P(\alpha) = \sum_{\beta} P(\alpha \beta) = \sum_{\beta} \frac{e^{- E_{\alpha \beta} / T}}{Z} = \sum_{\beta} \frac{e^{- E_{\alpha \beta} / T}}{\sum_{\alpha \beta} e^{- E_{\alpha \beta} / T}}\)<br/> where energy \(E_{\gamma} = - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} w_{ij} s_{i} s_{j}\)<br/> where \(w_{ij} = w_{ji}\) is symmetric with \(w_{ii} = 0\)<br/> 즉, energy \(E_{\gamma}\) 가 낮을수록 configuration \(\alpha\) 일 확률 \(P(\alpha)\) 가 높음</li> <li>goal :<br/> \(P(\alpha)\) 가 \(Q(\alpha)\) 와 최대한 가까워지도록<br/> weight \(w_{ij}\) 를 학습 <ul> <li>marginal (estimated) distribution of generated samples \(P(\alpha)\) :<br/> network가 free-running일 때 (all neurons가 자유롭게 업데이트될 수 있을 때)<br/> visible neuron이 \(\alpha\) 일 확률</li> <li>observed (desired) distribution of training samples \(Q(\alpha)\) :<br/> network의 visible neuron이 \(\alpha\) 로 clamped되었을 때<br/> visible neuron이 \(\alpha\) 일 확률</li> <li>KL-divergence \(D_{KL} (Q(\alpha), P(\alpha)) = \sum_{\alpha} Q(\alpha) \text{log} \frac{Q(\alpha)}{P(\alpha)}\) 를 minimize하면<br/> \(\Delta w_{ij} = - \eta \frac{\partial D_{KL}}{\partial w_{ij}} = \eta \sum_{\alpha} \frac{Q(\alpha)}{P(\alpha)} \frac{\partial P(\alpha)}{\partial w_{ij}} = \frac{\eta}{2T}(\sum_{\alpha \beta} Q(\alpha) P(\beta | \alpha) s_{i} s_{j} - \sum_{\alpha^{'} \beta^{'}} P(\alpha^{'} \beta^{'}) s_{i} s_{j}) = \frac{\eta}{2T}(E_{Q}[s_{i} s_{j}]_{\text{clamped by} \alpha} - E[s_{i} s_{j}]_{\text{free}})\)<br/> (pf는 아래에)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/19m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/19m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/19m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/19m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Boltzmann Machine with I-O Association : <ul> <li>neuron : <ul> <li>visible neuron : <ul> <li>input neuron : \(\alpha\)</li> <li>output neuron : \(\gamma\)</li> </ul> </li> <li>hidden neuron : \(\beta\)</li> </ul> </li> <li>goal :<br/> \(P(\gamma | \alpha)\) 가 \(Q(\gamma | \alpha)\) 와 최대한 가까워지도록<br/> weight \(w_{ij}\) 를 학습 <ul> <li>KL-divergence \(D_{KL} (Q(\gamma | \alpha), P(\gamma | \alpha)) = D_{KL} (Q(\alpha \gamma), P(\alpha \gamma)) - D_{KL} (Q(\alpha), P(\alpha))\) 를 minimize하면<br/> \(\Delta w_{ij} = \frac{\eta}{2T}(E_{Q}[s_{i} s_{j}]_{\text{clamped by} \alpha \gamma} - E[s_{i} s_{j}]_{\text{clamped by} \alpha})\)<br/> (pf는 아래에)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/20m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/20m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/20m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/20m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/21m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/21m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/21m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/21m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/22m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/22m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/22m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/22m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Restricted Boltzmann Machine (RBM) :<br/> Boltzmann Machine with bi-partite graph of visible and hidden units <ul> <li>probability \(P(v) = \sum_{h} P(v, h) = \sum_{h} \frac{e^{- E(v, h)}}{Z}\)<br/> where (intractable) partition function \(Z = \sum_{v, h} e^{- E(v, h)}\)<br/> where energy \(E(v, h) = - h^{T} W v - b^{T} v - c^{T} h\) for \(T=1\)</li> <li>goal :<br/> training dataset의 distribution을 학습!! <ul> <li>훈련 끝나고나면 해당 distribution을 따르는 new sample을 generate할 수 있음!! image inpainting에도 적용 가능</li> <li>label과의 joint distribution을 학습하여 classification 수행 가능!! feed-forward layer를 initialize할 때 RBM weight 사용 가능</li> <li>feature extractor 역할 수행 가능!!</li> </ul> </li> </ul> </li> <li>Training RBM 수식 유도 : <ul> <li>weight \(\theta = [W, b, c]\) 에 대해<br/> \(\hat \theta = \text{argmax}_{\theta} \text{ln} P(v | \theta) = \text{argmax}_{\theta} \text{ln} \sum_{h} \frac{e^{- E(v, h | \theta)}}{Z} = \text{argmax}_{\theta} \text{ln} \sum_{h} e^{- E(v, h | \theta)} - \text{ln} Z\)<br/> \(\rightarrow\)<br/> \(\frac{\partial - \text{ln} P(v | \theta)}{\partial \theta} = \cdots = \sum_{h} P(h | v, \theta) \frac{\partial E(v, h | \theta)}{\partial \theta} - \sum_{v, h} P(v, h | \theta) \frac{\partial E(v, h | \theta)}{\partial \theta} = E_{h \sim P(h | v, \theta)}[\frac{\partial E(v, h | \theta)}{\partial \theta}] - E_{(v, h) \sim P(v, h | \theta)}[\frac{\partial E(v, h | \theta)}{\partial \theta}]\) <ul> <li>오른쪽 expectation 식 :<br/> model distribution \(P(v, h | \theta)\) 의 모든 경우의 수에 대해 expectation \(E[\cdot]\) 계산하려면 \(2^{m+n}\) 로 costly하므로<br/> MCMC (Markov chain Monte Carlo) 기법으로 해당 distribution에서 <code class="language-plaintext highlighter-rouge">Gibbs sampling</code> 수행하여<br/> 오른쪽 expectation 식을 sample mean으로 approx.</li> </ul> </li> <li>RBM의 경우 visible units끼리, hidden units끼리는 connection 없으므로<br/> given visible units에 대해 hidden units끼리는 <code class="language-plaintext highlighter-rouge">conditionally independent</code>하므로<br/> 같은 layer에 있는 units는 쉽게 jointly(동시에) Gibbs sampling 가능<br/> \(\rightarrow\)<br/> <code class="language-plaintext highlighter-rouge">Gibbs sampling</code>은 두 단계로 요약 가능 (block Gibbs sampling) <ul> <li>Step 1)<br/> sample \(h\) based on \(P(h | v)\)</li> <li>Step 2)<br/> sample \(v\) based on \(P(v | h)\)</li> </ul> </li> <li>Conditional Distribution :<br/> proof는 아래 아래 사진에 <ul> <li> \[P(h_{i} = 1 | v) = \sigma (\sum_{j=1}^{m} w_{ij} v_{j} + c_{i}) = \sigma (\boldsymbol w_{i} \cdot \boldsymbol v + c_{i})\] </li> <li> \[P(v_{j} = 1 | h) = \sigma (\sum_{i=1}^{n} w_{ij} h_{i} + b_{j}) = \sigma (\boldsymbol w_{j} \cdot \boldsymbol h + b_{j})\] </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/27m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/27m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/27m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/27m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/23m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/23m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/23m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/23m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Training RBM 수식 유도 (continued) : <ul> <li>goal :<br/> \(P(v | \theta)\) 가 \(Q(v)\) 와 최대한 가까워지도록<br/> weight \(W, b, c\) 를 학습 <ul> <li></li> <li>Gradient of Log-Likelihood :<br/> (proof는 아래 사진에)<br/> Let’s define<br/> \(\boldsymbol h (\boldsymbol v_{0}) = P(\boldsymbol h = \boldsymbol 1 | \boldsymbol v_{0}, \theta) = \sigma(\boldsymbol W \boldsymbol v_{0} + \boldsymbol b)\) <br/> \(\boldsymbol v_{0} \in S\) : clamped training sample where \(S\) is training dataset<br/> \(\boldsymbol v_{k}\) : generated sample by RBM<br/> KL-divergence \(D_{KL} (Q(v), P(v | \theta)) = \sum_{v} Q(v) \text{log} \frac{Q(v)}{P(v | \theta)}\) 를 minimize하려면 <ul> <li> \[\Delta W \propto \frac{1}{N} \sum_{\boldsymbol v_{0} \in S} \boldsymbol h (\boldsymbol v_{0}) \boldsymbol v_{0}^{T} - \boldsymbol h (\boldsymbol v_{k}) \boldsymbol v_{k}^{T}\] </li> <li> \[\Delta b \propto \frac{1}{N} \sum_{\boldsymbol v_{0} \in S} \boldsymbol v_{0}^{T} - \boldsymbol v_{k}^{T}\] </li> <li> \[\Delta c \propto \frac{1}{N} \sum_{\boldsymbol v_{0} \in S} \boldsymbol h (\boldsymbol v_{0}) - \boldsymbol h (\boldsymbol v_{k})\] </li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/24m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/24m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/24m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/24m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>k-step Contrastive Divergence :<br/> model expectation은 exponential cost 가지므로<br/> sampling from RBM distribution<br/> 대신<br/> k-step sampling from Gibbs chain initialized with training data<br/> Then, \(\frac{\partial - \text{ln} P(v | \theta)}{\partial \theta} \approx \sum_{h} P(h | v_{0}) \frac{\partial E(v_{0}, h)}{\partial \theta} - \sum_{h} P(h | v_{k}) \frac{\partial E(v_{k}, h)}{\partial \theta}\)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/26m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/26m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/26m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/26m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Training RBM Summary : <ul> <li>Step 1) <code class="language-plaintext highlighter-rouge">Positive Phase</code><br/> training sample \(\boldsymbol v_{0}\) 에서 시작하여<br/> sample \(h_{i}\) from \(Q(h_{i} = 1 | v_{j}) = \sigma (\boldsymbol w_{i} \cdot \boldsymbol v + c_{i})\)<br/> \(\rightarrow\)<br/> \(h_{i} = \begin{cases} 1 &amp; \text{if} &amp; \text{rand}(0, 1) \lt Q(h_{i}=1 | v_{j}) \\ 0 &amp; \text{O.W.} &amp; \text{} \end{cases}\)</li> <li>Step 2) <code class="language-plaintext highlighter-rouge">Negative Phase</code> (Recon. Phase)<br/> sample \(v_{j}\) from \(P(v_{j} = 1 | h_{i}) = \sigma (\boldsymbol w_{j} \cdot \boldsymbol h + b_{j})\)<br/> \(\rightarrow\)<br/> \(v_{j} = \begin{cases} 1 &amp; \text{if} &amp; \text{rand}(0, 1) \lt P(v_{j}=1 | h_{i}) \\ 0 &amp; \text{O.W.} &amp; \text{} \end{cases}\)</li> <li>Step 3) 위의 과정을 k-step 반복<br/> \(\boldsymbol v_{0}\) 으로부터 \(\boldsymbol h(\boldsymbol v_{0})\) 얻고<br/> \(\cdots\) (Gibbs sampling k-step 반복) \(\cdots\)<br/> \(\boldsymbol v_{k}\) 으로부터 \(\boldsymbol h(\boldsymbol v_{k})\) 얻음</li> <li>Step 4) <code class="language-plaintext highlighter-rouge">Update Weights</code> <ul> <li> \[\Delta W \propto \frac{1}{N} \sum_{\boldsymbol v_{0} \in S} \boldsymbol h (\boldsymbol v_{0}) \boldsymbol v_{0}^{T} - \boldsymbol h (\boldsymbol v_{k}) \boldsymbol v_{k}^{T}\] </li> <li> \[\Delta b \propto \frac{1}{N} \sum_{\boldsymbol v_{0} \in S} \boldsymbol v_{0}^{T} - \boldsymbol v_{k}^{T}\] </li> <li> \[\Delta c \propto \frac{1}{N} \sum_{\boldsymbol v_{0} \in S} \boldsymbol h (\boldsymbol v_{0}) - \boldsymbol h (\boldsymbol v_{k})\] </li> </ul> </li> </ul> </li> <li>RBM Code :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/25m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/25m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/25m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/25m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Example :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/28m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/28m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/28m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/28m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/29m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/29m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/29m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/29m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="chapter-8-non-metric-methods-for-pattern-classification">Chapter 8. Non-metric Methods for Pattern Classification</h2> <p>training data 전체에 부합하는 parametric boundary 찾는 대신<br/> feature space를 여러 tree level에서 sub-regions로 나눠서 classify independently</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/30m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/30m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/30m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/30m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Decision Tree :<br/> not unique for partition <ul> <li>probability that \(\boldsymbol x \in u(t)\) has class \(w_{j}\) :<br/> \(N(t=5) = 7\), \(N_{1} (t=5) = 2\), \(N_{2} (t=5) = 5\) 에 대해<br/> \(P(w_{1} | t=5) = \frac{2}{7}\), \(P(w_{2} | t=5) = \frac{5}{7}\)</li> <li>node impurity and splitting :<br/> \(I(t) = \phi (P(w_{1} | t), \cdots, P(w_{c} | t))\) where \(\sum_{i=1}^{c} P(w_{i} | t) = 1\) <ul> <li>maximum, minimum : <ul> <li> <table> <tbody> <tr> <td>$$P(w_{1}</td> <td>t) = \cdots = P(w_{c}</td> <td>t) = \frac{1}{c}$$ 일 때 maximum 값 (가장 impure)</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$$P(w_{j}</td> <td>t) = 1\(and\)P(w_{i}</td> <td>t) = 0\(for all\)i \neq j$$ 일 때 minimum 값 (가장 pure)</td> </tr> </tbody> </table> </li> </ul> </li> <li>종류 : <ul> <li> <table> <tbody> <tr> <td>Gini criterion : $$I(t) = 1 - \sum_{i} P(w_{i}</td> <td>t)^{2}$$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>Entropy : $$I(t) = - \sum_{i} P(w_{i}</td> <td>t) \text{log}<em>{2} P(w</em>{i}</td> <td>t)$$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>Classification error : $$I(t) = 1 - \text{max}<em>{i} P(w</em>{i}</td> <td>t) = \frac{N_{minor}(t)}{N(t)}$$</td> </tr> </tbody> </table> </li> </ul> </li> <li>parent node에서 child node로 옮겼을 때 <code class="language-plaintext highlighter-rouge">impurity가 많이 감소할수록 잘 split</code>한 것임<br/> 즉, \(\Delta I(t) = I(t) - (I(t_{L}) \frac{N(t_{L})}{N(t)} + I(t_{R}) \frac{N(t_{R})}{N(t)})\) 가 클수록 better split<br/> e.g. \(I(t) = 1 - \text{max}_{i} P(w_{i} | t)\) 를 사용할 경우 위의 그림에서 \(\Delta I(t=3) = \frac{3}{13} - (\frac{1}{4} \frac{4}{13} + \frac{0}{9} \frac{9}{13}) = \frac{2}{13}\)</li> <li>terminating splitting : <ul> <li>stopping rule :<br/> stop splitting the node if \(\Delta I(t) \lt \tau\)</li> <li>pruning :<br/> 일단 (nearly) pure class 될 때까지 tree 늘린 뒤<br/> subtree를 terminal node로 대체</li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/31m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/31m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/31m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/31m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/32m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/32m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/32m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/32m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Pruning Decision Tree : <ul> <li>Let’s define <ul> <li>\(l(t), r(t)\) : node \(t\) 의 left, right child (\(l(t) = r(t) = 0\) for terminal node \(t\))</li> <li>\(\tilde T\) : set of terminal(leaf) nodes of tree \(T\)</li> <li> \[P(t) = \frac{N(t)}{N(t=1)}\] </li> <li> <table> <tbody> <tr> <td>$$P(w_{j}</td> <td>t) = \frac{N_{j}(t)}{N(t)}$$</td> </tr> <tr> <td>where $$\sum_{j=1}^{c} P(w_{j}</td> <td>t) = 1$$</td> </tr> </tbody> </table> </li> <li>\(P_{L}(t) = \frac{P(l(t))}{P(t)} = \frac{N(l(t))}{N(t)}\)<br/> \(P_{R}(t) = \frac{P(r(t))}{P(t)} = \frac{N(r(t))}{N(t)}\)<br/> where \(P_{L}(t) + P_{R}(t) = 1\)</li> </ul> </li> <li>subtree and pruned subtree : <ul> <li><code class="language-plaintext highlighter-rouge">subtree</code> : 하나의 node에서 출발해서 its child node들을 가져오는데 양쪽 다 가져와야 함 (위 그림 참고)</li> <li><code class="language-plaintext highlighter-rouge">pruned subtree</code> \(T_{1} \leq T\) : tree \(T\) 와 root node가 같아야 함 (위 그림 참고)</li> </ul> </li> <li> <table> <tbody> <tr> <td>tree \(T\) has class labels $${ w_{j}(t)</td> <td>t \in \tilde T }\(and disjoint partition\){ u(t)</td> <td>t \in \tilde T }$</td> </tr> </tbody> </table> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/33m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/33m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/33m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/33m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Pruning Algorithm : <ul> <li>Let’s define <ul> <li>misclassification rate at <code class="language-plaintext highlighter-rouge">node</code> \(t\) :<br/> \(R(t) = \frac{M(t)}{N(t=1)} = \frac{M(t)}{N(t)} \frac{N(t)}{N(t=1)} = r(t) p(t)\)<br/> (node \(t\) 가 <code class="language-plaintext highlighter-rouge">terminal node라고 생각</code>)<br/> where \(r(t) = \frac{M(t)}{N(t)}\) : classification error at node \(t\)<br/> where \(p(t) = \frac{N(t)}{N(t=1)}\) : frequency at node \(t\)</li> <li>total misclassification rate for <code class="language-plaintext highlighter-rouge">tree</code> \(T\) :<br/> \(R(T) = \sum_{t \in \tilde T} R(t) = \frac{1}{N(t=1)} \sum_{t \in \tilde T} M(t)\)<br/> (for <code class="language-plaintext highlighter-rouge">all terminal nodes</code> of tree \(T\))</li> <li>cost-complexity at node \(t\) :<br/> \(R_{\alpha}(t) = R(t) + \alpha\)</li> <li>cost-complexity for tree \(T\) :<br/> \(R_{\alpha}(T) = \sum_{t \in \tilde T} R_{\alpha} (t) = R(T) + \alpha | \tilde T |\)</li> <li>cost-complexity for subtree \(T_{t}\) :<br/> \(R_{\alpha}(T_{t}) = \sum_{t \in \tilde T_{t}} R_{\alpha} (t) = R(T_{t}) + \alpha | \tilde T_{t} |\)<br/> where \(T_{t}\) : node \(t\) 를 <code class="language-plaintext highlighter-rouge">root node</code>로 하는 subtree</li> <li>\(\alpha = g(t)\) s.t. \(R_{\alpha}(t) = R_{\alpha}(T_{t})\) :<br/> \(R_{\alpha}(t) = R_{\alpha}(T_{t})\) 이도록 하는 \(\alpha\)<br/> 즉, \(R(t) + \alpha = R(T_{t}) + \alpha | \tilde T_{t} |\) 이도록 하는 \(\alpha\) 즉, \(\alpha = g(t) = \frac{R(t) - R(T_{t})}{| \tilde T_{t} | - 1}\)</li> </ul> </li> <li>Pruning : <ul> <li>\(g(t)\) 가 <code class="language-plaintext highlighter-rouge">최소</code>인 node \(t\) 를 prune!!<br/> \(\alpha = g(t) = \frac{R(t) - R(T_{t})}{| \tilde T_{t} | - 1}\) 가 최소라는 말은, <ul> <li>case 1)<br/> node \(t\) 가 terminal node이든 (\(R(t)\)), node \(t\) 아래로 children이 있든 (\(R(T_{t})\)) 차이가 크지 않아서 prune해도 ok<br/> 즉, \(R(t) - R(T_{t})\) 가 작다</li> <li>case 2)<br/> node \(t\) 아래로 children이 너무 많아서 (large \(| \tilde T_{t} |\)) prune하는 게 나음</li> </ul> </li> <li>prune할수록 \(| \tilde T_{t} |\) 가 작아지니까<br/> tree는 작아지고<br/> cost \(\alpha = g(t)\) 는 커짐</li> <li>test set 또는 cross-validation 이용해서<br/> training set에 너무 overfitting된 nodes를 pruning함으로써<br/> better generalization</li> </ul> </li> </ul> </li> <li>Pruning Example :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/34m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/34m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/34m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/34m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/35m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/35m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/35m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/35m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="chapter-9-algorithm-independent-machine-learning">Chapter 9. Algorithm-Independent Machine Learning</h2> <ul> <li>Resampling : <ul> <li>Comparison : <ul> <li>Sampling :<br/> select groups within population</li> <li>Resampling :<br/> accuracy 높이고 uncertainty of estimate quantify하기 위해<br/> reselect new samples, that can provide more info. about population, based on one observed sample<br/> and estimate population(distribution) param. multiple times from data</li> </ul> </li> <li>종류 :<br/> resampling 기법 중 유명한 건 <ul> <li>jackknife</li> <li>bootstrap</li> </ul> </li> </ul> </li> </ul> <p>How to estimate bias and variance of estimator(statistic)?</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/36m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/36m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/36m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/36m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Jackknife : <ul> <li>bias 줄이기 위해<br/> remove samples from available dataset<br/> and recalculate the estimator</li> <li>may fail in estimating non-smooth statistic<br/> (smooth statistic : small change in data causes small change in statistic)</li> <li>\(i\)-th jackknife sample :<br/> \(i\)-th observation 제거<br/> \(x_{i} = (x_{1}, \cdots, x_{i-1}, x_{i+1}, \cdots, x_{n})\)</li> <li>estimate again by \(i\)-th jackknife sample :<br/> estimate \(\hat \theta_{(i)} = g(x_{i})\) based on \(x_{i}\)</li> <li>\(i\)-th pseudo-sample :<br/> \(\tilde \theta_{(i)} = n \hat \theta - (n-1) \hat \theta_{(i)}\)</li> <li>jackknife estimate of bias (\(\hat b_{jack}(\hat \theta)\)) :<br/> \(\hat b_{jack}(\hat \theta) = (n-1)(\hat \theta_{(\cdot)} - \hat \theta)\)<br/> where \(\hat \theta_{(\cdot)} = \frac{1}{n} \sum_{i=1}^{n} \hat \theta_{(i)}\)</li> <li>bias-corrected jackknife estimate :<br/> \(\hat \theta_{jack} = \hat \theta - \hat b_{jack}(\hat \theta) = n \hat \theta - (n-1) \hat \theta_{(\cdot)} = \frac{1}{n} \sum_{i=1}^{n} \tilde \theta_{i}\)<br/> Then bias-corrected \(\hat \theta_{jack}\) 은 \(\hat \theta\) 에 비해 bias 작음</li> <li>jackknife estimate of variance (\(\hat v_{jack}(\hat \theta)\)) :<br/> \(\hat v_{jack} (\hat \theta) = \hat v (\hat \theta_{jack}) = \frac{s_{jack}^{2}}{n} = \frac{1}{n(n-1)} \sum_{i=1}^{n} (\tilde \theta_{(i)} - \frac{1}{n} \sum_{j=1}^{n} \tilde \theta_{(j)})^{2} = \frac{n-1}{n} \sum_{i=1}^{n} (\hat \theta_{(i)} - \hat \theta_{(\cdot)})^{2} = \frac{(n-1)^{2}}{n} s^{2}\)<br/> where \(s_{jack}^{2}\) is sample variance of \(n\) pseudo-samples<br/> where \(s\) is sample variance of \(n\) estimates<br/> (\(Var(\tilde \theta_{(i)}) \approx Var(\sqrt{n} \hat \theta)\))</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/37m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/37m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/37m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/37m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/38m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/38m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/38m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/38m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Bootstrap : <ul> <li>sampling distribution (\(\neq\) sample distribution) :<br/> distribution of estimated statistics from different samples from the same population<br/> By central limit theorem, sampling distribution is normal and allows for probabilistic predictions about outcomes <ul> <li>precision : by standard deviation of sampling distribution</li> <li>accuracy : by bias</li> </ul> </li> <li>bootstrap distribution :<br/> <code class="language-plaintext highlighter-rouge">population 대신 a sample set itself</code>로부터 <code class="language-plaintext highlighter-rouge">sampling n points with replacement</code> 수행하여 distribution 구함<br/> (population으로부터 추가 samples generate하지 않아도 new samples (sampling distribution) 얻는 과정을 모방)</li> <li>estimate by \(b\)-th bootstrap sample : \(\hat \theta^{\ast (b)}\)</li> <li>bootstrap estimate of bias (\(b_{boot} (\hat \theta)\)) :<br/> \(b_{boot} (\hat \theta) = \hat \theta^{\ast (\cdot)} - \hat \theta\)<br/> where \(\hat \theta^{\ast (\cdot)} = \frac{1}{B} \sum_{b=1}^{B} \hat \theta^{\ast (b)}\)</li> <li>bootstrap estimate of variance (\(Var_{boot} [\hat \theta]\)) :<br/> \(Var_{boot} [\hat \theta] = \frac{1}{B} \sum_{b=1}^{B} (\hat \theta^{\ast (b)} - \hat \theta^{\ast (\cdot)})^{2}\)<br/> (As \(B \rightarrow \infty\), \(Var_{boot}[\hat \theta] \rightarrow Var[\hat \theta]\))</li> <li>Implementation : Ensemble learning <ul> <li>Step 1)<br/> multiple subsets(size \(n\)) are selected with replacement from original dataset(size \(n\))</li> <li>Step 2)<br/> A base model is created on each subset</li> <li>Step 3)<br/> Each independent model is learned in parallel with each subset</li> <li>Step 4)<br/> Final prediction by combining predictions of each model</li> </ul> </li> <li>장단점 : <ul> <li>장점 : <ul> <li>sampling distribution for statistics의 shape, spread, bias 정도를 estimate하기 좋은 방법</li> <li>small samples로도 잘 작동<br/> (jackknife는 \(n\) 개의 \(\hat \theta_{(i)}\) 구하려고 반복해야 했음)</li> <li>\(B\) 커질수록 bias, var. 측면에서 더 좋지만 computational load 커짐</li> </ul> </li> <li>단점 : <ul> <li>non-smooth model일 때는 잘 작동 X</li> <li>아래의 경우에는 사용 안 함 <ul> <li>data가 너무 작아서 population value와 가깝지 않을 때</li> <li>data에 outlier가 너무 많을 때</li> <li>dependent data일 때 (e.g. time series data)<br/> (bootstrap은 data independence를 가정)</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li>Ensemble learning에는 Bagging과 Boosting의 two types 있음 <ul> <li>공통점 : <ul> <li>Ensemble learning <ul> <li>to get 1 strong learner from N homogeneous weak learners by averaging or majority voting<br/> (each weak learner는<br/> one target class에 대해서는 accurately predict하지만,<br/> all target classes에 대해서는 not generalized (not optimal) to accurately predict)</li> </ul> </li> </ul> </li> <li>차이점 :<br/> individual weak learner의 training phase에 차이가 있음 <ul> <li><code class="language-plaintext highlighter-rouge">Bagging</code> (= Bootstrap aggregating) :<br/> learn from each other independently in parallel, and combine them by averaging or voting <ul> <li>aim to decrease variance (not bias)<br/> (overfitting 문제 해소)</li> <li>combine predictions that belong to the same type</li> <li>models combine할 때 equal vote</li> <li>each model is independent</li> <li>different training subsets : random sample from entire training dataset<br/> by resampling with replacement</li> <li>classifier가 unstable (e.g. decision tree) (high variance) 하더라도 improve in almost all cases</li> <li>built and trained in parallel</li> <li>e.g. random forest model</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Boosting</code> (Arcing = Adaptive resampling and combining) :<br/> learn iteratively and sequentially and adaptively<br/> first round에는 training examples를 equal weight로 반영한 뒤<br/> round마다 misclassified training examples(by current weak hypothesis) 에 더 많은 weight를 부여하도록 update하여<br/> next round에서 weak learner가 hard-to-classify examples에 더 집중하도록 함 <ul> <li>aim to decrease bias (not variance)</li> <li>combine predictions that belong to the different types</li> <li>models combine할 때 weighted vote by their performance (accuracy)</li> <li>new model is influenced by the performance of previous model</li> <li>every new training subset : weighted sample to focus on misclassified examples by previous models (more often chosen)<br/> by adaptive resampling 그로써 weak learner can be converted to strong learner</li> <li>classifier가 stable하고 simple할 때 (high bias) 적용</li> <li>built and trained sequentially</li> <li>e.g. AdaBoost</li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/39m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/39m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/39m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/39m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="chapter-10-unsupervised-learning">Chapter 10. Unsupervised Learning</h2> <ul> <li>unsupervised learning :<br/> hard since no answer and no correct accuracy measure to check<br/> but, needed since annotating is costly and we often don’t know how many classes there are <ul> <li>parametric unsupervised learning : <ul> <li>sample comes from population that follows fixed param. probability distribution</li> <li>GMM(Gaussian Mixture Model) and EM(Expectation-Maximization)</li> </ul> </li> <li>non-parametric unsupervised learning : <ul> <li>clustering</li> <li>distribution-free, so do not require assumption about population distribution</li> </ul> </li> </ul> </li> <li> <p>K-means clustering :<br/> discriminative approach</p> </li> <li>GMM(Gaussian Mixture Model) :<br/> generative approach <ul> <li>mixture density :<br/> \(p(x_{k} | \theta) = \sum_{j=1}^{c} P(w_{j}) p(x_{k} | w_{j}, \theta_{j})\)<br/> where \(\sum_{j=1}^{c} P(w_{j}) = 1\)<br/> (gaussian일 경우 \(p(x_k | w_{i}, \theta_{i}) = N(x_k | w_{i}, \mu_{i}, \Sigma_{i}) = (2 \pi)^{-\frac{d}{2}} | \Sigma_{i} |^{-\frac{1}{2}} \text{exp}(-\frac{1}{2}(x_k - \mu_{i})^T \Sigma_{i}^{-1} (x_k - \mu_{i}))\))</li> <li>matrix derivative :<br/> \(\frac{d}{dx}(Ax) = A\)<br/> \(\frac{d}{dx}(y^TAx) = A^Ty\)<br/> \(\frac{d}{dx}(x^TAx) = (A+A^T)x\)<br/> \(\frac{d}{dA}(x^TAx) = xx^T\)<br/> \(\frac{\partial |A|}{\partial A} = (\text{adj}(A))^T = |A|(A^{-1})^T\)<br/> \(\frac{\partial \text{ln}|A|}{\partial A} = (A^{-1})^T = (A^T)^{-1}\) where \(|A| = \frac{1}{|A^{-1}|}\)</li> <li>MLE on \(\theta\) :<br/> \(L = \text{ln} p(D | \theta) = \text{ln} \prod_{k=1}^{n} p(x_{k} | \theta) = \sum_{k=1}^{n} \text{ln} p(x_{k} | \theta)\)<br/> \(\rightarrow\)<br/> data point index \(k\), class index \(i\) 에 대해<br/> \(\nabla_{\theta_{i}} L = \cdots = \sum_{k=1}^{n} \gamma_{ki} \nabla_{\theta_{i}} \text{ln} p(x_{k} | w_{i}, \theta_{i}) = 0\)<br/> where \(\gamma_{ki} = \frac{p(x_{k} | w_{i}, \theta_{i}) P(w_{i})}{\sum_{j=1}^{c} p(x_{k} | w_{j}, \theta_{j}) P(w_{j})} = \frac{p(x_{k} | w_{i}, \theta_{i}) P(w_{i})}{p(x_{k} | \theta)} = P(w_{i} | x_{k}, \theta_{i})\) : posterior probability (responsibility of mixture component \(i\) for data \(x_{k}\))<br/> (data \(x_{k}\) 가 cluster \(i\) 에 속할 확률) <ul> <li>Case 1) unknown \(\theta_{i} = \mu_{i}\) :<br/> \(\sum_{k=1}^{n} \gamma_{ki} \nabla_{\mu_{i}} \text{ln} p(x_{k} | w_{i}, \theta_{i}) = 0\)<br/> \(\rightarrow\)<br/> \(\hat \mu_{i} = \frac{1}{\sum_{k=1}^{n} \gamma_{ki}} \sum_{k=1}^{n} \gamma_{ki} x_{k} = \frac{1}{n_{i}} \sum_{k=1}^{n} \gamma_{ki} x_{k}\) : weighted mean of data points where posterior is weight<br/> where \(n_{i} = \sum_{k=1}^{n} \gamma_{ki}\) : effective(expected) number of points assigned to cluster \(i\)</li> <li>Case 2) unknown \(\mu_{i}\) and \(\Sigma_{i}\) :<br/> \(\sum_{k=1}^{n} \gamma_{ki} \nabla_{\Sigma_{i}^{-1}} \text{ln} p(x_{k} | w_{i}, \theta_{i}) = 0\)<br/> \(\rightarrow\)<br/> \(\hat \Sigma_{i} = \frac{1}{\sum_{k=1}^{n} \gamma_{ki}} \sum_{k=1}^{n} \gamma_{ki} (x_{k} - \hat \mu_{i})(x_{k} - \hat \mu_{i})^{T} = \frac{1}{n_{i}} \sum_{k=1}^{n} \gamma_{ki} (x_{k} - \hat \mu_{i})(x_{k} - \hat \mu_{i})^{T}\)</li> <li>Case 3) unknown \(\mu_{i}\) and \(\Sigma_{i}\) and \(P(w_{i})\) :<br/> \(\hat P(w_{i}) = \frac{n_{i}}{n}\)<br/> (proof는 아래에)<br/> (\(P(w_{i})\) 와 \(\gamma_{ki}\) 는 서로 dependent,<br/> so, \(\hat \mu_{i}, \hat \Sigma_{i}, \hat P(w_{i})\) 는 no closed-form solution<br/> so, need repetitive steps)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/40m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/40m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/40m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/40m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>EM (Expectation-Maximization) :<br/> repeat E-step and M-step until log-likelihood \(L\) converges <ul> <li>E-step :<br/> for each data point \(x_{k}\), estimate posterior \(\gamma_{ki}\) and assign \(x_{k}\) to the class \(\text{max}_{i} \gamma_{ki}\)</li> <li>M-step :<br/> update parameter \(\hat \theta\)<br/> e.g. \(\hat \mu_{i}, \hat \Sigma_{i}, \hat P(w_{i})\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/41m.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/41m.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/41m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-28-Pattern2/41m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="cheet-sheet-for-final-exam">Cheet Sheet for Final Exam</h2>]]></content><author><name></name></author><category term="cv-tasks"/><category term="cv"/><summary type="html"><![CDATA[Lecture Summary (24F)]]></summary></entry></feed>