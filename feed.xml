<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://semyeong-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://semyeong-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-08-04T14:15:50+00:00</updated><id>https://semyeong-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">MipNeRF</title><link href="https://semyeong-yu.github.io/blog/2024/MipNeRF/" rel="alternate" type="text/html" title="MipNeRF"/><published>2024-08-03T01:03:00+00:00</published><updated>2024-08-03T01:03:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/MipNeRF</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/MipNeRF/"><![CDATA[<h2 id="mip-nerf-a-multiscale-representation-for-anti-aliasing-neural-radiance-fields">Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</h2> <h4 id="jonathan-t-barron-ben-mildenhall-matthew-tancik-peter-hedman-ricardo-martin-brualla-pratul-p-srinivasan">Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2103.13415">https://arxiv.org/abs/2103.13415</a><br/> project website :<br/> <a href="https://jonbarron.info/mipnerf/">https://jonbarron.info/mipnerf/</a><br/> pytorch code :<br/> <a href="https://github.com/bebeal/mipnerf-pytorch">https://github.com/bebeal/mipnerf-pytorch</a><br/> <a href="https://github.com/google/mipnerf">https://github.com/google/mipnerf</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>ray-tracing하여 point-encoding 대신 cone-tracing하여 region-encoding 이므로 frustum의 모양과 크기 정보를 encode할 수 있어서 scale 반영 가능</li> <li>IPE 단계에서 <code class="language-plaintext highlighter-rouge">high variance (distant view)</code>일 때 <code class="language-plaintext highlighter-rouge">high freq.를 attenuate</code> (pre-filtering) 하여<br/> <code class="language-plaintext highlighter-rouge">임의의 continuous-space scale</code>을 가지는 scene에 대해 <code class="language-plaintext highlighter-rouge">anti-aliased</code> representation 학습 가능<br/> \(\rightarrow\) multi-resolution dataset에 대해 성능 대폭 향상<br/> \(\rightarrow\) scale-aware하므로 <code class="language-plaintext highlighter-rouge">single MLP</code> 하나만으로 충분하여 빠르고 가벼움</li> <li>camera center로부터 각 pixel로 3D cone을 쏜 다음,<br/> <code class="language-plaintext highlighter-rouge">frustum을 multi-variate Gaussian으로 근사</code>한 뒤,<br/> Gaussian 내 좌표를 positional encoding한 것 (PE-basis-lifted Gaussian)에 대해 expected value \(E \left[ \gamma (x) \right]\) 계산<br/> 주의 : frustum이 Gaussian 분포를 따르는 게 아니라, frustum 내부의 mean, variance 값을 먼저 구한 뒤 해당 mean, variance 값을 갖는 Gaussian으로 frustum을 대신(근사)할 수 있다고 생각!</li> </ol> </blockquote> <h2 id="introduction">Introduction</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/2-480.webp 480w,/assets/img/2024-08-03-MipNeRF/2-800.webp 800w,/assets/img/2024-08-03-MipNeRF/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>기존 NeRF의 문제점 : <ul> <li>rendering 위해 sampling할 때 <code class="language-plaintext highlighter-rouge">single ray</code> per pixel 쏴서 composite 하므로<br/> dataset에 있는 물체의 크기(resolution)가 일정하지 않을 때<br/> multi-scales images에 대해 학습하더라도</li> <li><code class="language-plaintext highlighter-rouge">blurry</code> rendering in <code class="language-plaintext highlighter-rouge">close-up</code> views<br/> (because 가까이서 찍어서 zoom-out하면 물체 in <code class="language-plaintext highlighter-rouge">high resolution</code>)</li> <li><code class="language-plaintext highlighter-rouge">aliased</code>(계단) rendering in <code class="language-plaintext highlighter-rouge">distant</code> views<br/> (because 멀리서 찍어서 zoom-in하면 물체 in <code class="language-plaintext highlighter-rouge">low resolution</code>)</li> <li>그렇다고 multiple rays per pixel through its footprint로 brute-force super-sampling(offline rendering)하는 것은 정확하긴 하겠지만 too costly 비현실적</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Minmap</code> Approach :<br/> classic 컴퓨터 그래픽스 분야에서 rendering할 때 aliasing을 없애기 위한 <code class="language-plaintext highlighter-rouge">pre-filtering</code> 기법<br/> 본 논문인 Mip-NeRF가 여기서 영감을 얻음<br/> signal(e.g. image)을 diff. <code class="language-plaintext highlighter-rouge">downsampling scales</code>로 나타낸 뒤 pixel footprint를 근거로 ray에 사용하기 위한 <code class="language-plaintext highlighter-rouge">적절한 scale을 고른다</code><br/> render time에 할 복잡할 일을 pre-computation phase에 미리 하는 것일 뿐이긴 하지만, 주어진 texture마다 한 번만 minmap을 만들면 된다는 장점이 있다</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/4-480.webp 480w,/assets/img/2024-08-03-MipNeRF/4-800.webp 800w,/assets/img/2024-08-03-MipNeRF/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Mip-NeRF : <ul> <li>represent pre-filtered scene at <code class="language-plaintext highlighter-rouge">continuous space of scales</code></li> <li>ray 대신 <code class="language-plaintext highlighter-rouge">conical frustum</code> 사용해서 <code class="language-plaintext highlighter-rouge">anti-aliased</code> rendering with fine details</li> <li>multi-scale variant of dataset에 대해 평균 error rate 60% 감소</li> <li>NeRF가 hierarchical sampling을 위해 coarse and fine MLP를 분리했다면, Mip-NeRF는 <code class="language-plaintext highlighter-rouge">scale-aware</code>하므로 <code class="language-plaintext highlighter-rouge">single multi-scale MLP만으로 충분</code><br/> 따라서 NeRF보다 7% 빠르고, param. 수는 절반이고, sampling도 더 효율적</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/1-480.webp 480w,/assets/img/2024-08-03-MipNeRF/1-800.webp 800w,/assets/img/2024-08-03-MipNeRF/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> ray 대신 cone을 쏘고, point-encoding 대신 frustum region-encoding </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/3-480.webp 480w,/assets/img/2024-08-03-MipNeRF/3-800.webp 800w,/assets/img/2024-08-03-MipNeRF/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>차이 : 기존 NeRF는 <code class="language-plaintext highlighter-rouge">a single point</code>를 encode하고, Mip-NeRF는 <code class="language-plaintext highlighter-rouge">a region of space</code>를 encode</p> </li> <li> <p>기존 NeRF :<br/> camera center로부터 각 pixel로 ray를 하나 쏜 다음 point sampling한 뒤 positional encoding<br/> point-sampled feature는 ray가 보는 <code class="language-plaintext highlighter-rouge">volume의 모양과 크기를 무시</code>하는 것임<br/> 예를 들어 training할 때 camera1로부터 t 사이의 간격이 평균 10cm로 학습된 scene에 대해<br/> camera2로 inference를 할 때 t 사이의 간격이 평균 1cm로 sampling된다면<br/> 10개의 점은 같은 point-based feature를 갖게 되어 scale을 고려하지 못함<br/> 이러한 ambiguity가 기존 NeRF의 성능 하락의 요인</p> </li> <li> <p>Mip-NeRF :<br/> volume 정보를 반영하기 위해 camera center로부터 각 pixel로 3D cone을 쏜 다음, 3D point 및 그 주위의 Gaussian region을 encode하기 위해 IPE</p> </li> <li> <p>IPE (<code class="language-plaintext highlighter-rouge">integrated positional encoding</code>) :<br/> region을 encode하기 위한 방식<br/> <code class="language-plaintext highlighter-rouge">frustum을 multi-variate Gaussian으로 근사</code>한 뒤,<br/> Gaussian 내 좌표를 positional encoding한 것 (PE-basis-lifted Gaussian)에 대해 expected value \(E \left[ \gamma (x) \right]\) 계산</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/5-480.webp 480w,/assets/img/2024-08-03-MipNeRF/5-800.webp 800w,/assets/img/2024-08-03-MipNeRF/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="related-work">Related Work</h2> <h3 id="anti-aliasing-in-rendering">Anti-aliasing in Rendering</h3> <blockquote> <p><code class="language-plaintext highlighter-rouge">anti-aliasing</code>을 위한 고전적인 방법으로는 두 가지가 있다.</p> </blockquote> <ol> <li><code class="language-plaintext highlighter-rouge">supersampling</code> : <ul> <li>rendering할 때 <code class="language-plaintext highlighter-rouge">multiple rays per pixel</code>을 쏴서 Nyquist frequency에 가깝게 sampling rate를 높임 (super-sampling)</li> <li><code class="language-plaintext highlighter-rouge">expensive</code> as runtime increases linearly with the super-sampling rate, so used only in <code class="language-plaintext highlighter-rouge">offline</code> rendering</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">pre-filtering</code> : <ul> <li>target sampling rate에 맞춰서 Nyquist frequency를 줄이기 위해 scene에 <code class="language-plaintext highlighter-rouge">lowpass-filter</code>를 씌운 버전 사용</li> <li>scene을 미리 <code class="language-plaintext highlighter-rouge">downsampling multi-scales</code> (sparse voxel octree 또는 minmap)로 나타낸 뒤, <code class="language-plaintext highlighter-rouge">ray 대신 cone</code>을 추적하여 cone과 scene이 만나는 곳의 cone’s footprint에 대응되는 적절한 scale을 골라서 사용 (<code class="language-plaintext highlighter-rouge">target sampling rate에 맞는 적절한 scale</code>)</li> <li>scene에 filter 씌운 버전을 한 번만 미리 계산하면 되므로, better for <code class="language-plaintext highlighter-rouge">real-time</code> rendering</li> </ul> </li> </ol> <blockquote> <p>그런데 아래의 이유로 고전적인 multi-scale representation은 적용 불가능</p> <ul> <li>input scene의 <code class="language-plaintext highlighter-rouge">geometry를 미리 알 수 없으므로</code> pre-filtering 할 수가 없어서<br/> 대신 pre-filtering 방식 자체를 training할 때 학습해야 한다</li> <li>input scene의 <code class="language-plaintext highlighter-rouge">scale이 continuous</code>하므로 a fixed number of scales (discrete)과 상황이 다르다</li> </ul> </blockquote> <p>\(\rightarrow\) 결론 : 따라서 Mip-NeRF는 training하는 동안, <code class="language-plaintext highlighter-rouge">임의의 scale</code>에 대해 query 받을 수 있는, scene의 <code class="language-plaintext highlighter-rouge">pre-filtered representation</code>을 학습한다.</p> <h3 id="scene-representations-for-view-synthesis">Scene Representations for View Synthesis</h3> <ul> <li> <p>만약 images of scene are captured <code class="language-plaintext highlighter-rouge">densely</code> 라면, novel view synthesis 위해, intermediate representation of scene을 reconstruct하지 않고 <code class="language-plaintext highlighter-rouge">light field interpolation</code> 기법 사용</p> </li> <li>만약 images of scene are captured <code class="language-plaintext highlighter-rouge">sparsely</code> 라면, novel view synthesis 위해, <code class="language-plaintext highlighter-rouge">explicit representation</code> of the scene’s 3D geometry and appearance를 reconstruct <ul> <li><code class="language-plaintext highlighter-rouge">polygon-mesh-based</code> representation (<code class="language-plaintext highlighter-rouge">discrete, classic</code>) :<br/> with either diffuse or view-dependent textures<br/> can be stored efficiently, but mesh geometry optimization에 gradient descent를 이용하는 것은 불연속점 및 극소점 때문에 어렵</li> <li><code class="language-plaintext highlighter-rouge">volumetric</code> representation : <ul> <li><code class="language-plaintext highlighter-rouge">voxel-grid-based</code> representation (<code class="language-plaintext highlighter-rouge">discrete, classic</code>) : deep learning으로 학습 가능, but 고해상도의 scene에는 부적합</li> <li><code class="language-plaintext highlighter-rouge">coordinate-based</code> neural representation (<code class="language-plaintext highlighter-rouge">continuous, recent</code>) : 3D 좌표를 그 위치에서의 scene의 특징(e.g. volume density, radiance)으로 mapping하는 continuous function을 MLP로 예측 <ul> <li>implicit surface representation</li> <li><code class="language-plaintext highlighter-rouge">implicit volumetric NeRF representation</code></li> </ul> </li> </ul> </li> </ul> </li> <li>문제점 :<br/> 그동안 view synthesis를 할 때 sampling 및 aliasing에는 덜 주목했다<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">classic discrete</code> representation의 경우 앞에서 소개한 minmap, octree와 같은 고전적인 multi-scale pre-filtering 기법을 쓰면 aliasing 없이 rendering 가능하다<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">coordinate-based</code> representation의 경우 scale이 continuous하므로 고전적인 anti-aliasing 기법은 사용할 수 없다<br/> \(\rightarrow\) sparse voxel octree 기반의 multi-scale representation으로 implicit surface의 continuous neural representation을 구현한 논문 <d-cite key="continuouspriori">[1]</d-cite> 있긴 하지만, scene geometry의 priori를 알아야 한다는 제약이 있다<br/> \(\rightarrow\) Mip-NeRF는 training하는 동안, <code class="language-plaintext highlighter-rouge">임의의 scale</code>에 대해 query 받을 수 있는, scene의 <code class="language-plaintext highlighter-rouge">anti-aliased (pre-filtered)</code> representation을 학습한다.</li> </ul> <h2 id="method">Method</h2> <h3 id="cone-tracing-and-positional-encoding">Cone Tracing and Positional Encoding</h3> <ul> <li>Cone Tracing : <ul> <li>Let \(d\) is cone direction vector from \(o\) to image plane</li> <li>Let \(\hat r =\) radius at image plane \(o + d\) \(= \frac{2}{\sqrt{12}}\) of pixel-width<br/> so that image plane에서의 cone의 \(x, y\) 축 variance가 pixel’s footprint의 variance와 같아지도록<br/> \(\hat r\)은 ray의 radius 변화율, 즉 frustum의 넓이를 결정</li> <li>\(t \in [t_0, t_1]\) 일 때 conical frustum 내의 \(x\)는 아래 범위의 값을 가질 때 indicator function \(F(x, o, d, \hat r, t_0, t_1)=1\)이다<br/> \(F(x, o, d, \hat r, t_0, t_1) = 1 \left\{ (t_0 \lt \frac{d^T(x-o)}{\| d \|^2} \lt t_1) \land (\frac{d^T(x-o)}{\| d \| \| x-o \|} \gt \frac{1}{\sqrt{1+(\frac{\hat r}{\| d \|})^2}}) \right\}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/6-480.webp 480w,/assets/img/2024-08-03-MipNeRF/6-800.webp 800w,/assets/img/2024-08-03-MipNeRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Region Encoding :<br/> conical frustum 내에 있는 모든 좌표 \(x\)에 대해 직접<br/> expected value \(E \left[ \gamma (x) \right] = \frac{\int \gamma (x) F(x, o, d, \hat r, t_0, t_1) dx}{\int F(x, o, d, \hat r, t_0, t_1) dx}\) 계산하면<br/> region을 encode할 수 있는데<br/> 여기서 분자의 적분식은 closed-form solution이 없음<br/> \(\rightarrow\) 직접 계산하지 말고<br/> <code class="language-plaintext highlighter-rouge">conical-frustum을 multi-variate Gaussian으로 근사</code>한 뒤<br/> Gaussian 내에 있는 모든 좌표 \(x\)에 대해<br/> expected value \(E \left[ \gamma (x) \right]\) 계산</p> </li> <li>frustum을 multi-variate Gaussian으로 근사 : <ul> <li>conical-frustum은 대칭적인 원이기 때문에<br/> \(o, d\) 뿐만 아니라 아래의 3가지 정보만 알면 Gaussian을 특정할 수 있다 <ul> <li><code class="language-plaintext highlighter-rouge">mean distance along ray from o</code> \(\mu_{t}\)</li> <li><code class="language-plaintext highlighter-rouge">variance along ray</code> \(\sigma_{t}^2\)</li> <li><code class="language-plaintext highlighter-rouge">variance perpendicular to ray</code> \(\sigma_{r}^2\)</li> </ul> </li> <li>Let mid-point \(t_{\mu} = \frac{t_0+t_1}{2}\)<br/> Let half-width \(t_{\sigma}=\frac{t_1-t_0}{2}\)</li> <li>아래 수식의 유도과정은 하위에 별도로 정리함<br/> \(\mu_{t} = t_{\mu} + \frac{2t_{\mu}t_{\sigma}^2}{3t_{\mu}^2+t_{\sigma}^2}\)<br/> \(\sigma_{t}^2 = \frac{t_{\sigma}^2}{3} - \frac{4t_{\sigma}^4(12t_{\mu}^2-t_{\sigma}^2)}{15(3t_{\mu}^2+t_{\sigma}^2)^2}\)<br/> \(\sigma_{r}^2 = \hat r^2 (\frac{t_{\mu}^2}{4} + \frac{5t_{\sigma}^2}{12} - \frac{4t_{\sigma}^4}{15(3t_{\mu}^2+t_{\sigma}^2)})\)</li> <li>위의 3가지 param.를 가지는 Gaussian은 <code class="language-plaintext highlighter-rouge">t-coordinate</code>에서 정의했는데<br/> 아래 수식에 의해 <code class="language-plaintext highlighter-rouge">world-coordinate</code>으로 변환할 수 있다<br/> \(\mu = o + \mu_{t}d\)<br/> \(\Sigma = \sigma_{t}^2(dd^T) + \sigma_{r}^2(I-\frac{dd^T}{\| d \|^2})\)<br/> where \(dd^T =\) \(d\) 의 outer product은 \(d\) 방향으로의 투영을 의미하는 rank-1 matrix<br/> where \(I-\frac{dd^T}{\| d \|^2}\) 는 \(\frac{d}{\| d\ \|}\) 와 수직인 subspace로의 투영을 의미하는 rank-2 matrix</li> </ul> </li> <li>Integrated Positional Encoding (IPE) : <ul> <li>목표 : 위에서 계산한 \(\mu, \Sigma\) 의 Gaussian 내에 있는 모든 좌표 \(x\)에 대해 expected value \(E \left[ \gamma (x) \right]\) 계산</li> <li>우선 <code class="language-plaintext highlighter-rouge">PE (positional-encoding) basis</code> P를 재정의<br/> \(P = \begin{pmatrix} \begin{matrix} 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2\end{matrix} &amp; \cdots &amp; \begin{matrix} 2^{L-1} &amp; 0 &amp; 0 \\ 0 &amp; 2^{L-1} &amp; 0 \\ 0 &amp; 0 &amp; 2^{L-1}\end{matrix} \end{pmatrix}^T\)<br/> \(\gamma (x) = \begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}\)</li> <li>\(E \left[ \gamma (x) \right]\) 는 expectation over \(\gamma (x) = \begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}\) 이므로<br/> \(x\) in Gaussian of \(\mu, \Sigma\) \(\rightarrow\) \(\gamma (x)\) in Gaussian of \(\mu_{r}, \Sigma_{r}\) 로 변환해야 한다<br/> 즉, <code class="language-plaintext highlighter-rouge">PE basis P로 lift</code>한 뒤의 mean과 covariance를 구해야 한다<br/> Since \(Cov[Ax, By] = A Cov[x, y] B^T\),<br/> \(\mu_{r} = P \mu\)<br/> \(\Sigma_{r} = P \Sigma P^T\)</li> <li>최종적으로 \(E \left[ \gamma (x) \right]\) , 즉 <code class="language-plaintext highlighter-rouge">expectation over lifted multi-variate Gaussian</code> of \(\mu_{r}, \Sigma_{r}\) 을 구하면 된다<br/> Since \(E_{k \sim N(\mu, \sigma^2)}[e^{itk}] = exp(i \mu t - \frac{1}{2} \sigma^2 t^2)\) and \(sin(k) = \frac{e^{ik}-e^{-ik}}{2i}\),<br/> \(E_{k \sim N(\mu, \sigma^2)}[sin(k)] = sin(\mu)exp(-\frac{1}{2}\sigma^2)\) and \(E_{k \sim N(\mu, \sigma^2)}[cos(k)] = cos(\mu)exp(-\frac{1}{2}\sigma^2)\) for each axis-k<br/> (positional-encoding은 각 dim.을 independently encode하므로 marginal distribution of \(\gamma (x)\) 에 의존)<br/> \(\rightarrow\)<br/> \(\gamma (\mu, \Sigma) = E_{x \sim N(\mu, \Sigma)} [\gamma (x)] = E_{Px \sim N(\mu_{r}, \Sigma_{r})} [\begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}]\)<br/> \(= \begin{bmatrix} sin(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \\ cos(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \end{bmatrix}\)<br/> where \(\circledast\) is element-wise multiplication</li> <li>\(diag(\Sigma_{r})\) 만 필요하므로 \(\Sigma_{r}\) 전부 계산하지 말고 <code class="language-plaintext highlighter-rouge">efficiently diagonal만 계산</code><br/> \(diag(\Sigma_{r}) = diag(P \Sigma P^T) = \left[ diag(\Sigma), 4 diag(\Sigma), \ldots , 4^{L-1}diag(\Sigma) \right]^T\)<br/> where 3d-vector \(diag(\Sigma) = \sigma_{t}^2(d \circledast d) + \sigma_{r}^2(1-\frac{d \circledast d}{\| d \|^2})\)<br/> diagonal만 직접 계산하면, IPE feature는 PE feature랑 비슷하게 cost 소모</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/7-480.webp 480w,/assets/img/2024-08-03-MipNeRF/7-800.webp 800w,/assets/img/2024-08-03-MipNeRF/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>IPE vs PE : <ul> <li>PE :<br/> point를 encode<br/> 0~L까지의 <code class="language-plaintext highlighter-rouge">모든 frequencies에 대해 동일하게</code> encode<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">high-freq.</code> PE features are <code class="language-plaintext highlighter-rouge">aliased</code><br/> (PE period가 interval width보다 작은 경우 PE over interval oscillates repeatedly)</li> <li>IPE :<br/> interval region을 integrate하여 encode<br/> IPE feature를 만드는 데 사용된 interval \(t \in [t_0, t_1]\) width보다 period가 작은 <code class="language-plaintext highlighter-rouge">high freq.</code>의 경우 <code class="language-plaintext highlighter-rouge">attenuate</code>하여 <code class="language-plaintext highlighter-rouge">anti-aliasing</code><br/> by \(exp(-\frac{1}{2}(\omega \sigma)^2)\) term</li> <li>위와 같은 특성 덕분에 IPE는 interval 내 공간의 모양과 크기를 smoothly encode할 수 있는 anti-aliased PE 기법이다!</li> <li>high freq.는 IPE 단계 자체에서 attenuate되므로 <code class="language-plaintext highlighter-rouge">L을 hyper-param.로 두지 않고 extremely large fixed-value</code>로 두면 된다</li> </ul> </li> <li>IPE의 의미 :<br/> 이게 Mip-NeRF의 핵심!! <ul> <li>수식 :<br/> PE-basis P 는 다양한 frequency \(\omega\) 로 구성되어 있고<br/> 각 element는 \(E_{x \sim N(\mu, \Sigma)} [\gamma_{\omega} (x)] = sin(\omega \mu) exp(-\frac{1}{2}(\omega \sigma)^2)\)</li> <li>distant view :<br/> <code class="language-plaintext highlighter-rouge">distant views (low-resolution)</code>, 즉 멀리 있는 <code class="language-plaintext highlighter-rouge">wide frustum</code> (high variance \(\sigma\))의 경우에는<br/> <code class="language-plaintext highlighter-rouge">detail-info.</code> (high freq. \(\omega\))는 <code class="language-plaintext highlighter-rouge">training에 사용하지 않겠다</code><br/> \(\rightarrow\) more attenuation for high \(\sigma\) and high \(\omega\)</li> <li>close view :<br/> <code class="language-plaintext highlighter-rouge">close views (high-resolution)</code>, 즉 가까이 있는 <code class="language-plaintext highlighter-rouge">narrow frustum</code> (low variance \(\sigma\))의 경우에는<br/> <code class="language-plaintext highlighter-rouge">detail-info.</code> (high freq. \(\omega\))를 training할 때 좀 더 <code class="language-plaintext highlighter-rouge">허용</code></li> <li>위와 같이 scale을 반영할 수 있으므로 blurry 및 aliased rendering 문제 해결 가능!</li> </ul> </li> <li>수식 <code class="language-plaintext highlighter-rouge">Summary</code> : <ul> <li>frustum을 근사하는 multi-variate Gaussian의 mean, variance \(\mu, \sigma\) 를 구한다</li> <li>PE-basis P로 lift한 Gaussian의 mean, variance \(\mu_{r}, \Sigma_{r}\) 를 구한다<br/> \(P = \begin{pmatrix} \begin{matrix} 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2\end{matrix} &amp; \cdots &amp; \begin{matrix} 2^{L-1} &amp; 0 &amp; 0 \\ 0 &amp; 2^{L-1} &amp; 0 \\ 0 &amp; 0 &amp; 2^{L-1}\end{matrix} \end{pmatrix}^T\)<br/> \(\gamma (x) = \begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}\)<br/> \(\mu_{r} = P \mu\) and \(\Sigma_{r} = P \Sigma P^T\)</li> <li>\(E_{x \sim N(\mu_{r}, \Sigma_{r})} [\gamma (x)] = \begin{bmatrix} sin(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \\ cos(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \end{bmatrix}\)<br/> (efficiently \(\Sigma_{r}\) 의 diagonal만 직접 계산)</li> </ul> </li> </ul> <h3 id="conical-frustum-integral-derivation">Conical Frustum Integral Derivation</h3> <ul> <li> <p>우선 <code class="language-plaintext highlighter-rouge">Cartesian-coordinate</code>에서 <code class="language-plaintext highlighter-rouge">conical-coordinate</code>으로 변환<br/> \((x, y, z) = \varphi (r, t, \theta) = t \cdot (r cos \theta , r sin \theta , 1)\)<br/> where \(\theta \in [0, 2 \pi)\) and \(t \geq 0\) and \(\| r \| \leq \hat r\)<br/> \(\rightarrow\)<br/> \(dx dy dz = \| det(D \varphi) \| dr dt d\theta\)<br/> \(= \begin{vmatrix} t cos\theta &amp; t sin\theta &amp; 0 \\ r cos\theta &amp; r sin\theta &amp; 1 \\ - rt sin\theta &amp; rt cos\theta &amp; 0 \end{vmatrix} dr dt d\theta\)<br/> \(= (rt^2cos^2\theta + rt^2sin\theta) dr dt d\theta\)<br/> \(= rt^2 dr dt d\theta\)</p> </li> <li> <p>Thus the probability density function for points uniformly sampled from the conical frustum is rt2/V . The first moment of t is: 정리할 차례 <code class="language-plaintext highlighter-rouge">????</code></p> </li> </ul> <h3 id="architecture">Architecture</h3> <ul> <li>아래 내용들을 제외하고는 NeRF의 Architecture와 동일 <ul> <li>ray-tracing 대신 cone-tracing</li> <li>PE 대신 IPE</li> <li>point-encoding 이므로 \(n\)개의 구간에 대해 \(n\)개의 point sampling<br/> \(\rightarrow\)<br/> interval(region)-encoding 이므로 \(n\)개의 구간을 위해 \(n+1\)개의 point sampling</li> <li>PE feature로는 scale을 반영할 수 없으므로 두 가지 MLP (coarse-MLP, fine-MLP) 이용해서 hierarchical sampling<br/> (coarse-MLP에서는 \(N_c=64\) points per ray, fine-MLP에서는 \(N_c+N_f=64+128\) points per ray)<br/> \(\rightarrow\)<br/> IPE feature 자체가 scale을 반영할 수 있으므로 MLP 하나를 반복해서 써서 hierarchical sampling<br/> (한 번은 \(N_c=128\) points per ray, 그 다음은 \(N_f=128\) points per ray)<br/> NeRF와 MipNeRF의 공정한 비교를 위해 같은 수(총 256개)의 point를 사용</li> <li>hierarchical sampling에서 piecewise-constant PDF of normalized \(w\) 에 따라 fine-sampling 하기 전에<br/> weight \(w_k\) 를 바로 사용하지 않고<br/> 2-tap MaxBlur filter 를 적용한 weight의 wide and smooth upper bound 를 사용<br/> \(w_k^{\ast} = \frac{1}{2}(max(w_{k-1}, w_k) + max(w_k, w_{k+1})) + \alpha\)<br/> where 빈 공간에서도 일부 samples 추출되도록 보장하기 위해 \(\alpha=0.01\) 설정</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/8-480.webp 480w,/assets/img/2024-08-03-MipNeRF/8-800.webp 800w,/assets/img/2024-08-03-MipNeRF/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> single MLP 쓰니까 coarse loss와 fine loss 간의 balance 맞추기 위해 hyperparam. gamma = 0.1로 설정 </div> <ul> <li>MaxBlur filter : <ul> <li>MaxPool 대신 MaxBlurPool 쓰면 aliasing 감소 효과</li> <li>MipNeRF에서 weight에 MaxBlur filter 쓰는 이유 :<br/> scene content는 아무래도 연속적으로 존재하니까<br/> 인접한 samples 간의 weight \(w\) 가 갑작스럽게 변하거나 불연속적인 outlier 를 제외하여 smoothing 해주는 역할</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/10-480.webp 480w,/assets/img/2024-08-03-MipNeRF/10-800.webp 800w,/assets/img/2024-08-03-MipNeRF/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/9-480.webp 480w,/assets/img/2024-08-03-MipNeRF/9-800.webp 800w,/assets/img/2024-08-03-MipNeRF/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Setting :<br/> implementation on JaxNeRF<br/> 1 million iter., Adam optimizer, batch_size = 4096, lr from \(5 \cdot 10^{-4}\) to \(5 \cdot 10^{-6}\)</li> </ul> <h2 id="result">Result</h2> <ul> <li>multi-scale dataset에 대해 NeRF보다 error rate 60% 감소</li> <li>single-scale dataset에 대해 NeRF보다 error rate 17% 감소</li> <li>NeRF의 param.의 절반이고, NeRF보다 7$ 빠름</li> <li>brute-force super-sampling한 버전보다 22배 빠른데 accuracy 비슷</li> </ul> <h2 id="question">Question</h2> <ul> <li>Q1 : distant view (scene content in low-resolution)일 때 IPE의 \(exp(-\frac{1}{2}(\omega \sigma)^2)\) term에 의해 high freq.를 attenuate하여 anti-aliasing 가능한 건 이해했는데,<br/> close view (scene content in high-resolution)일 때 blurry rendering은 어떻게 해결??</li> <li>A1 : 위에서 “Method - Cone Tracing and Positional Encoding - IPE의 의미”에 설명해둠</li> <li>Q2 : image plane에서의 cone의 \(x, y\) 축 variance가 pixel’s footprint의 variance와 같아지도록<br/> \(\hat r =\) radius at image plane \(o + d\) \(= \frac{2}{\sqrt{12}}\) of pixel-width 로 설정한다는데 이 부분이 이해가 되지 않습니다</li> <li>A2 : uniform distribution을 가정했을 때 pixel의 square variance는 \(\frac{w^2}{12}\) 이고, cone at image plane의 circle variance는 \(\frac{\hat r^2}{4}\) 이므로 variance 값이 같으려면 \(\hat r = \frac{2}{\sqrt{12}} \times w\)</li> <li>Q3 : \(E \left[ \gamma (x) \right] = \frac{\int \gamma (x) F(x, o, d, \hat r, t_0, t_1) dx}{\int F(x, o, d, \hat r, t_0, t_1) dx}\) 에서 분자의 적분식을 closed-form으로 계산할 수 없어서 conical frustum을 multi-variate Gaussian으로 근사했다는데,<br/> conical frustum의 모양과 크기 범위에 대한 parameter가 주어진다면 frustum 내부의 점 \(x\) 에 sin 및 cos을 씌운 \(\gamma (x)\) 의 경우 \(x\) 에 대해 공간 적분할 수 있지 않나요?</li> <li>A3 : frustum 내에 있는 모든 좌표에 \(\gamma\) 를 씌워서 공간 적분하는 것 자체가 말도 안 되게 복잡한 식이라 closed-form solution이 없기 때문에 frustum의 mean과 variance를 구해서 Gaussian으로 근사해서 expected value 구합니다</li> <li>Q4 : 논문을 보면 frustum을 multi-variate Gaussian으로 근사하기 위해서는 먼저 \(F(x, o, d, \hat r, t_0, t_1)\) 의 mean, covariance를 계산해야 한다고 쓰여있던데<br/> appendix를 보면 indicator function인 F의 mean과 covariance가 아니라 conical frustum의 \(r, t, \theta\) 범위를 이용해서 공간 적분해서 \(t, x, y\) 축의 mean과 variance를 계산하지 않나요?</li> <li>A4 : 맞습니다. 논문에서 \(F(x, o, d, \hat r, t_0, t_1)\) 의 mean, covariance를 계산해야 한다고 언급되어 있는 것은 단순히 frustum 내부 범위에 속해있는 지점에 대해 적분을 통해 mean, variance를 구해야 한다는 뜻인 것 같습니다.</li> <li>Q5 : NeRF에서 rendering할 때는 EWA volume splatting과 같은 좌표계 변환을 고려하지 않아도 되나요?</li> <li>A5 : NeRF에서는 ray를 따라 MLP의 output을 alpha-compositing하여 직접 pixel 값을 얻어내므로 ray를 쓰기 위해 cam-to-world coordinate 변환만 필요하고, projection에 의한 non-linear 좌표계 변환과는 관련이 없다.<br/> 반면, Gaussian Splatting에서는 rendering할 때 3D Gaussian 자체를 직접 projection해서 쓰기 때문에 3D Gaussian covariance matrix on world-coordinate을 2D Gaussian covariance matrix on image-coordinate (ray-space)으로 projection해야 하므로 non-linear 좌표계 변환이 필요하다. 이를 위해 EWA volume splatting에 따라 non-linear transformation을 Taylor approx.하여 local affine transformation으로서 Jacobian을 사용한다</li> <li>Q6 : camera origin과 pixel 중심을 잇는 ray가 image plane에 수직이 아닌 pixel의 경우 \(\hat r\) 과 \(d\) 를 어떻게 정의하지?</li> <li>A6 : \(d\) 는 camera origin부터 pixel 중심까지의 거리 vector이고,<br/> cone 단면의 \(\hat r\)은 \(d\) 와 수직인 방향으로 \(\frac{2}{\sqrt{12}}\) of pixel-width 이므로<br/> cone 단면이 image plane 위에 있지 않은 꼴이 됨</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="rendering"/><category term="3d"/><category term="multiscale"/><category term="antialiasing"/><summary type="html"><![CDATA[A Multiscale Representation for Anti-Aliasing Neural Radiance Fields]]></summary></entry><entry><title type="html">Vim, Pycharm Debug Shortcut</title><link href="https://semyeong-yu.github.io/blog/2024/vim/" rel="alternate" type="text/html" title="Vim, Pycharm Debug Shortcut"/><published>2024-08-01T11:00:00+00:00</published><updated>2024-08-01T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/vim</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/vim/"><![CDATA[<h2 id="vim">Vim</h2> <p>vimtutor : 튜토리얼<br/> vim 파일명 : 노말모드 실행</p> <h3 id="mode">Mode</h3> <ul> <li> <p>입력모드 :<br/> i : 노말모드 &gt; 입력모드 (커서 앞)<br/> I : 노말모드 &gt; 입력모드 (문장 시작)<br/> a : 노말모드 &gt; 입력모드 (커서 뒤)<br/> A : 노말모드 &gt; 입력모드 (문장 끝)</p> </li> <li> <p>노말모드 :<br/> esc : 입력모드 &gt; 노말모드</p> </li> <li> <p>명령모드 :<br/> ‘:’ : 노말모드 &gt; 명령모드</p> </li> </ul> <h3 id="입력모드">입력모드</h3> <p>파일 수정</p> <h3 id="명령모드">명령모드</h3> <p>:q : 종료<br/> :q! : 강제 종료<br/> :w : 저장<br/> :w! : 강제 저장<br/> :wq : 저장 후 종료<br/> :wq! : 강제 저장 후 종료</p> <h3 id="노말모드">노말모드</h3> <h4 id="navigation">Navigation</h4> <ul> <li> <p>커서 :<br/> hjkl : 커서 이동</p> </li> <li> <p>line :<br/> 0 : line 앞<br/> $ : line 뒤<br/> 20G : 20번째 line 앞</p> </li> <li> <p>문단 :<br/> { : 문단 시작<br/> } : 문단 끝</p> </li> <li> <p>단어 :<br/> w : 다음 단어 앞<br/> 3w : 3번째 다음 단어 앞<br/> e : 다음 단어 뒤<br/> b : 이전 단어 앞<br/> 3b : 3번째 이전 단어 앞</p> </li> <li> <p>화면, 파일 :<br/> H : 화면 위<br/> M : 화면 중간<br/> L : 화면 끝<br/> gg : 파일 앞<br/> G : 파일 끝</p> </li> <li> <p>스크롤링 :<br/> Ctrl+u : 위로 스크롤링<br/> Ctrl+d : 아래로 스크롤링</p> </li> </ul> <h4 id="비주얼선택-잘라내기-복사-붙여넣기">비주얼(선택), 잘라내기, 복사, 붙여넣기</h4> <ul> <li> <p>비주얼(선택) 모드 :<br/> v : 비주얼(선택) 모드<br/> Ctrl+v : 블럭 단위 비주얼(선택) 모드<br/> v + hjkl : 드래그 선택<br/> v aw : 단어 1개 선택</p> </li> <li> <p>잘라내기 :<br/> x : 글자 잘라내기<br/> dd : line 잘라내기</p> </li> <li> <p>복사 :<br/> y : 복사<br/> yy : line 복사</p> </li> <li> <p>붙여넣기 :<br/> p : 붙여넣기<br/> “p 혹은 *p : 클립보드 붙여넣기</p> </li> </ul> <h4 id="반복-되감기-앞감기">반복, 되감기, 앞감기</h4> <ul> <li>. : 이전 명령 반복</li> <li>u : undo (되돌리기)</li> <li>Ctrl+r : redo</li> </ul> <h4 id="command--object-조합">Command + Object 조합</h4> <ul> <li> <p>예시 :<br/> d 3w : 다음 단어 3개 잘라내기<br/> d 2j : 아래 2줄 잘라내기<br/> c i[ : 대괄호 안에 있는 것을 변경</p> </li> <li> <p>Command :<br/> d : 잘라내기 (delete)<br/> y : 복사 (yank)<br/> c : 변경 (change)<br/> v : 선택 (visual)<br/> Ctrl+v : 블럭 단위 선택</p> </li> <li> <p>Object :<br/> 3w : 다음 단어 3개<br/> 3b : 이전 단어 3개<br/> aw : 단어 1개<br/> ap : 문단 1개<br/> as : line 1개<br/> i” : “ “ 안에 있는 것<br/> ip : 문단 안에 있는 것<br/> i{ : 중괄호 안에 있는 것<br/> i( : 소괄호 안에 있는 것<br/> a( : 소괄호 포함 모든 것<br/> a[ : 대괄포 포함 모든 것<br/> f( : 현재부터 소괄호(포함)까지<br/> t( : 현재부터 소괄호(미포함)까지<br/> /abc : 현재부터 abc(미포함)까지 (드래그 표시로 확인 가능)</p> </li> </ul> <h4 id="검색">검색</h4> <ul> <li>/<단어> : <단어> 검색 후 n 누르면 밑으로 계속 검색</단어></단어></li> <li>?<단어> : <단어> 검색 후 n 누르면 위로 계속 검색</단어></단어></li> <li>n : 계속 검색</li> </ul> <h2 id="pycharm-debug">Pycharm Debug</h2> <ul> <li> <p>실행 :<br/> Ctrl+F5 : 그냥 실행<br/> F9 : break point 설정<br/> F5 또는 우상단 벌레 버튼 : 디버깅 모드 실행 (첫 번째 break point 직전에서 멈춤)</p> </li> <li> <p>디버깅 모드 :<br/> F10 : 코드 한 줄 실행<br/> F11 : 함수 안으로 이동<br/> Shift+F11 : 함수 밖(호출 위치)로 이동<br/> F5 : 다음 breakpoint 직전에서 멈춤<br/> Shift+F5 또는 우상단 정지 버튼 : 디버깅 모드 해제</p> </li> </ul>]]></content><author><name></name></author><category term="cv-tasks"/><category term="vim"/><category term="pycharm"/><category term="debug"/><summary type="html"><![CDATA[vim, pycharm debug shortcut]]></summary></entry><entry><title type="html">Structure-from-Motion Revisited (COLMAP)</title><link href="https://semyeong-yu.github.io/blog/2024/Colmap/" rel="alternate" type="text/html" title="Structure-from-Motion Revisited (COLMAP)"/><published>2024-07-31T11:00:00+00:00</published><updated>2024-07-31T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Colmap</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Colmap/"><![CDATA[<h2 id="structure-from-motion-revisited">Structure-from-Motion Revisited</h2> <h4 id="johannes-l-schonberger-jan-michael-frahm">Johannes L. Schonberger, Jan-Michael Frahm</h4> <blockquote> <p>paper :<br/> <a href="chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://openaccess.thecvf.com/content_cvpr_2016/papers/Schonberger_Structure-From-Motion_Revisited_CVPR_2016_paper.pdf">chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://openaccess.thecvf.com/content_cvpr_2016/papers/Schonberger_Structure-From-Motion_Revisited_CVPR_2016_paper.pdf</a><br/> referenced blog :<br/> <a href="https://xoft.tistory.com/88">https://xoft.tistory.com/88</a></p> </blockquote> <h3 id="colmap">COLMAP</h3> <ul> <li>COLMAP :<br/> SfM (Structure from Motion)과 MVS (Multi-View Stereo)를 수행하는 library</li> <li><code class="language-plaintext highlighter-rouge">SfM</code> : <ul> <li>input : images</li> <li>output : camera parameter(intrinsic, extrinsic), 3D point cloud</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">MVS</code> : <ul> <li>input : SfM의 output</li> <li>output : 3D model reconstruction</li> </ul> </li> </ul> <h3 id="sfm-history">SfM History</h3> <ul> <li> <p>SfM 시초 :<br/> “3d model acquisition from extended image sequences”, 1996.<br/> “Structure from motion without correspondence”, CVPR, 2000.<br/> “Automatic camera recovery for closed or open image sequences”, ECCV, 1998.<br/> “Relative 3d reconstruction using multiple uncalibrated images”, IJR, 1995.<br/> “Visual modeling with a hand-held camera”, IJCV, 2004</p> </li> <li> <p>internet images로 3D reconstruction 수행 :<br/> “Multi-view matching for unordered image sets, or How do I organize my holiday snaps?”, ECCV, 2002.<br/> “Photo tourism: exploring photo collections in 3d”, ACM TOG, 2006.<br/> “Detailed real-time urban 3d reconstruction from video”, IJCV, 2008.</p> </li> <li>input images 수 늘리는 연구 : <ul> <li>수천장 처리 : “Building rome in a day”, ICCV, 2009.</li> <li>수백만장 처리 : “Building Rome on a Cloudless Day”, ECCV, 2010.<br/> “Towards linear-time incremental structure from motion”, 3DV, 2013.<br/> “From Single Image Query to Detailed 3D Reconstruction”, CVPR, 2015.<br/> “From Dusk Till Dawn: Modeling in the Dark”, CVPR, 2016.</li> <li>수억장 처리 : “Reconstructing the World* in Six Days *(As Captured by the Yahoo 100 Million Image Dataset)”, CVPR, 2015.</li> </ul> </li> <li>SfM 연구 발전 방향 :<br/> incremental, hierarchical, global 으로 총 3가지 발전 방향이 있었고,<br/> 그 중 images를 sequentially 처리하는 incremental SfM이 가장 인기 있었지만<br/> robustness, accuracy, completeness, scalability 관점에서 general SfM을 만들기 어려웠음<br/> \(\rightarrow\) 근데 이를 본 논문 (COLMAP)에서 해결!!</li> </ul> <h3 id="incremental-sfm">Incremental SfM</h3> <ul> <li>images</li> <li>correspondence search <ul> <li><code class="language-plaintext highlighter-rouge">feature extraction</code> :<br/> image마다 geometric-radiometric-invariant feature 추출<br/> e.g. SIFT <a href="https://velog.io/@everyman123/SIFT-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0">link</a> (scale-rotation-invariant feature extraction)<br/> e.g. SURF, HOG, BRIEF, ORB 등 <a href="https://ggommappooh.tistory.com/entry/%EC%BB%B4%ED%93%A8%ED%84%B0-%EB%B9%84%EC%A0%84-Feature-Descriptors">link</a></li> <li><code class="language-plaintext highlighter-rouge">matching</code> :<br/> extracted feature를 바탕으로 2 images에서 같은 scene part 찾기<br/> image마다 feature를 비교하므로 time complexity \(O(N_I^2 N_F^2)\)<br/> e.g. “Building Rome in a day”, ICCV, 2009.<br/> e.g. “Building Rome on a Cloudless Day”, ECCV, 2010.<br/> e.g. “Vocmatch: Efficient multiview correspondence for structure from motion”, ECCV, 2014.<br/> e.g. “Reconstructing the World in Six Days (As Captured by the Yahoo 100 Million Image Dataset)”, CVPR, 2015.<br/> e.g. “MatchMiner: Efficient Spanning Structure Mining in Large Image Collections”, ECCV, 2012.<br/> e.g. “PAIGE: PAirwise Image Geometry Encoding for Improved Efficiency in Structure-from-Motion”, CVPR, 2015.<br/> e.g. “Towards linear-time incremental structure from motion”, 3DV, 2013.<br/> e..g CNN-based SuperGlue(2019)<br/> e.g. ViT-based LoFTR(2021)</li> <li><code class="language-plaintext highlighter-rouge">geometric verification</code> : <ul> <li>matching 결과를 보장하기 위한 검증 과정 (걸러냄)</li> <li>GRIC과 같은 기법을 통해 어떤 geometry model로 검증할 지 결정<br/> geometry model 예시 : Fundametal Matrix, Trifocal Tensor, Projective Matrix, Calibration Matrix, Rigid Transformation, Affine Transformation</li> <li>Epipolar Geometry로 검증하는 예시 : Fundamental Matrix로 relative camera pose를 추정한 뒤 camera1의 feature points를 camera2로 projection 했을 때 matching points로 잘 mapping 되는지 검증<br/> Fundamental Matrix 계산하기 위해 eight-point, five-point, RANSAC, LMedS, QDEGSAC 등의 기법 사용 가능</li> <li>output : scene graph (verified image pair 및 correpondence map)</li> </ul> </li> </ul> </li> <li>incremental reconstruction <ul> <li><code class="language-plaintext highlighter-rouge">initialization</code> :<br/> 최초로 등록한 2개의 images 선택 (매우 중요한 단계임)<br/> 여러 camera로부터 많이 overlap되는 scene을 가진 images로 선택하면, Bundle Adjustment 단계에서 overlap part가 반복적으로 최적화되면서 reconstruction 성능이 높아지지만 연산 시간도 늘어남</li> <li><code class="language-plaintext highlighter-rouge">image registration</code> : <ul> <li>camera pose를 world-coordinate에 등록한다</li> <li>initial 2 images의 경우 : fundamental matrix를 알고 있으므로 intrinsic param.와 extrinsic param. (camera pose)을 추정할 수 있다</li> <li>이후 images의 경우 : 이미 등록된 image들과의 feature correspondence와 PnP 알고리즘을 사용하여 intrinsic param.와 extrinsic param.를 추정할 수 있다</li> <li>PnP 알고리즘 : 3D points 위치와 projected 2D points 위치를 기반으로 camera pose를 추정</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">triangulation</code> :<br/> ddd</li> <li><code class="language-plaintext highlighter-rouge">bundle adjustment</code> :<br/> ddd</li> <li><code class="language-plaintext highlighter-rouge">outlier filtering</code> :<br/> RANSAC 및 minimal pose solver 등의 기법을 사용하여 outlier를 걸러냄<br/> RANSAC : dataset의 randomly selected points에 대해 Fundamental Matrix 등 geometry model을 추정한 뒤 해당 model이 dataset에 얼마나 잘 부합하는지를 반복적으로 검증</li> </ul> </li> <li>3D reconstruction</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="SfM"/><category term="pose"/><category term="3d"/><summary type="html"><![CDATA[SfM library]]></summary></entry><entry><title type="html">Normalized Device Coordinates</title><link href="https://semyeong-yu.github.io/blog/2024/NDC/" rel="alternate" type="text/html" title="Normalized Device Coordinates"/><published>2024-07-30T15:00:00+00:00</published><updated>2024-07-30T15:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/NDC</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/NDC/"><![CDATA[<h2 id="ndc-normalized-device-coordinates">NDC: Normalized Device Coordinates</h2> <blockquote> <p>referenced blog :<br/> <a href="https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#background">https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#background</a></p> </blockquote> <h3 id="motivation">Motivation</h3> <p>NeRF에서<br/> MLP의 input은 3D world-coordinate이고,<br/> MLP의 output인 \(c, \sigma\) 를 accumulate해서 2D pixel-coordinate을 채운다<br/> 이 때, LLFF (Local Light Field Fusion) dataset 에 있는<br/> <code class="language-plaintext highlighter-rouge">unbounded (in single direction) 3D world-coordinate</code>의 scene 정보를<br/> <code class="language-plaintext highlighter-rouge">bounded 3D NDC space</code>로 project하면<br/> <code class="language-plaintext highlighter-rouge">MLP를 효율적으로 쓸 수 있다</code><br/> NDC space로의 projection 과정을 수식적으로 알아보고자 한다.</p> <h3 id="from-world-coordinate-to-ndc-to-pixel-coordinate">From world-coordinate To NDC To pixel-coordinate</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-30-NDC/1-480.webp 480w,/assets/img/2024-07-30-NDC/1-800.webp 800w,/assets/img/2024-07-30-NDC/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-30-NDC/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>camera transformation : <ul> <li><code class="language-plaintext highlighter-rouge">3D world-coordinate</code> (canonical-coordinate)<br/> \(\rightarrow\)<br/> <code class="language-plaintext highlighter-rouge">3D camera-coordinate</code></li> <li>extrinsic matrix \(\begin{bmatrix} R &amp; t \\ 0 &amp; 1 \end{bmatrix}\)</li> </ul> </li> <li>projection transformation : <ul> <li><code class="language-plaintext highlighter-rouge">3D camera-coordinate</code><br/> \(\rightarrow\)<br/> <code class="language-plaintext highlighter-rouge">3D NDC (normalized-device-coordinate)</code> (canonical view volume)</li> <li>normalized-device-coordinate (NDC) :<br/> <code class="language-plaintext highlighter-rouge">camera 원점이 중앙에 있는</code> \([-1, 1]^3\) cube (<code class="language-plaintext highlighter-rouge">정육면체</code>)</li> <li>frustum \(\rightarrow\) 직육면체 \(\rightarrow\) 정육면체<br/> consists of perspective projection and then orthographic projection<br/> z-axis 방향 바꾸기 포함</li> </ul> </li> <li>viewport transformation : <ul> <li><code class="language-plaintext highlighter-rouge">3D NDC</code><br/> \(\rightarrow\)<br/> <code class="language-plaintext highlighter-rouge">2D pixel-coordinate</code></li> <li>\([-1, 1]^3\) 의 NDC를 flatten하여 2 \(\times\) 2 square를 raster image로 mapping</li> <li>intrinsic matrix \(\begin{bmatrix} f_x &amp; s &amp; W/2 \\ 0 &amp; f_y &amp; H/2 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\) (초점거리 곱하고 원점 좌상단 이동)<br/> y-axis 방향 바꾸기 포함</li> </ul> </li> </ul> <h3 id="projection-transformation">Projection Transformation</h3> <blockquote> <p>Step 1. <code class="language-plaintext highlighter-rouge">Perspective Projection</code></p> </blockquote> <ul> <li>frustum을 bounded cuboid로 변환<br/> bound :<br/> \(x \in [l, r]\) where \(l \lt 0\), \(r \gt 0\)<br/> \(y \in [b, t]\) where \(b \lt 0\), \(t \gt 0\)<br/> \(z \in [f, n]\) where \(f \lt 0\), \(n \lt 0\)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-30-NDC/2-480.webp 480w,/assets/img/2024-07-30-NDC/2-800.webp 800w,/assets/img/2024-07-30-NDC/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-30-NDC/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-30-NDC/3-480.webp 480w,/assets/img/2024-07-30-NDC/3-800.webp 800w,/assets/img/2024-07-30-NDC/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-30-NDC/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 3D camera-coordinate </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-30-NDC/4-480.webp 480w,/assets/img/2024-07-30-NDC/4-800.webp 800w,/assets/img/2024-07-30-NDC/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-30-NDC/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>\(z = n\) plane은 그대로 냅두고, 직육면체 꼴이 되도록 그 뒤 plane 변환<br/> camera를 통과하는 any line은 z-axis에 평행한 line이 됨</p> </li> <li> <p>perspective projection matrix :<br/> \(P_{per} = \begin{bmatrix} n &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; n &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; n+f &amp; -nf \\ 0 &amp; 0 &amp; 1 &amp; 0 \end{bmatrix}\)<br/> \(P_{per} \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix} = \begin{bmatrix} nX \\ nY \\ (n+f)Z - nf \\ Z \end{bmatrix}\)<br/> <code class="language-plaintext highlighter-rouge">?????</code></p> </li> </ul> <blockquote> <p>Step 2. <code class="language-plaintext highlighter-rouge">Orthographic Projection</code></p> </blockquote> <ul> <li> <p>corner (l, b, n)이 원점이 되도록 shift한 뒤,<br/> \([0, r-l] \times [0, t-b] \times [f-n, 0]\) 의 직육면체를 \([0, 2] \times [0, 2] \times [-2, 0]\) 의 정육면체로 scale한 뒤,<br/> center (1, 1, -1)이 원점이 되도록 \([-1, 1]^3\) 으로 shift</p> </li> <li> <p>orthographic projection matrix :<br/> \(M_{orth} = \begin{pmatrix} I_{3 \times 3} &amp; \begin{matrix} -1 \\ -1 \\ 1 \end{matrix} \\ 0_{1 \times 3} &amp; 1 \end{pmatrix} \begin{pmatrix} \begin{matrix} \frac{2}{r-l} &amp; 0 &amp; 0 \\ 0 &amp; \frac{2}{t-b} &amp; 0 \\ 0 &amp; 0 &amp; \frac{2}{n-f} \end{matrix} &amp; 0_{3 \times 1} \\ 0_{1 \times 3} &amp; 1 \end{pmatrix} \begin{pmatrix} I_{3 \times 3} &amp; \begin{matrix} -l \\ -b \\ -n \end{matrix} \\ 0_{1 \times 3} &amp; 1 \end{pmatrix}\)<br/> \(= \begin{bmatrix} \frac{2}{r-l} &amp; 0 &amp; 0 &amp; -\frac{r+l}{r-l} \\ 0 &amp; \frac{2}{t-b} &amp; 0 &amp; -\frac{t+b}{t-b} \\ 0 &amp; 0 &amp; \frac{2}{n-f} &amp; -\frac{n+f}{n-f} \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix}\)</p> </li> </ul> <blockquote> <p>Step 3. <code class="language-plaintext highlighter-rouge">Projection Matrix</code></p> </blockquote> <p>Since perspective projection matrix is scalable,<br/> \(M_{proj} = M_{orth} (- P_{per})\)<br/> \(= \begin{bmatrix} \frac{2}{r-l} &amp; 0 &amp; 0 &amp; -\frac{r+l}{r-l} \\ 0 &amp; \frac{2}{t-b} &amp; 0 &amp; -\frac{t+b}{t-b} \\ 0 &amp; 0 &amp; \frac{2}{n-f} &amp; -\frac{n+f}{n-f} \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix} \begin{bmatrix} -n &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; -n &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; -n-f &amp; nf \\ 0 &amp; 0 &amp; -1 &amp; 0 \end{bmatrix}\)<br/> \(= \begin{bmatrix} -\frac{2n}{r-l} &amp; 0 &amp; \frac{r+l}{r-l} &amp; 0 \\ 0 &amp; -\frac{2n}{t-b} &amp; \frac{t+b}{t-b} &amp; 0 \\ 0 &amp; 0 &amp; -\frac{n+f}{n-f} &amp; \frac{2nf}{n-f} \\ 0 &amp; 0 &amp; -1 &amp; 0 \end{bmatrix}\)</p> <p>camera-coordinate에서 \(z \in [f, n]\) where \(f \lt 0\), \(n \lt 0\) 이었는데,<br/> NDC에서는 z-axis의 방향이 반대이므로<br/> \(f \lt 0\), \(n \lt 0\) 대신 \(f = -f \gt 0\), \(n = -n \gt 0\) 를 대입하면,<br/> \(M_{proj} = \begin{bmatrix} \frac{2n}{r-l} &amp; 0 &amp; \frac{r+l}{r-l} &amp; 0 \\ 0 &amp; \frac{2n}{t-b} &amp; \frac{t+b}{t-b} &amp; 0 \\ 0 &amp; 0 &amp; -\frac{n+f}{n-f} &amp; -\frac{2nf}{n-f} \\ 0 &amp; 0 &amp; -1 &amp; 0 \end{bmatrix}\)</p> <p>OpenGL과 같은 graphics frameworks에서는 보통<br/> \(M_{proj}X\) 를 \(M_{proj}X\) 의 fourth entry로 나눴을 때 \(M_{proj}X\) 의 Z 값이 양수가 되도록 하기 때문에 (아래 Step 4의 NDC 참고)<br/> 조금 수정하면<br/> 최종적인 projection matrix는<br/> \(M_{proj} = \begin{bmatrix} \frac{2n}{r-l} &amp; 0 &amp; \frac{r+l}{r-l} &amp; 0 \\ 0 &amp; \frac{2n}{t-b} &amp; \frac{t+b}{t-b} &amp; 0 \\ 0 &amp; 0 &amp; -\frac{n+f}{f-n} &amp; -\frac{2nf}{f-n} \\ 0 &amp; 0 &amp; -1 &amp; 0 \end{bmatrix}\)</p> <p>camera frustum은 보통 symmetric하므로 \(l = -r\), \(b = -t\) 라 했을 때<br/> projection matrix는<br/> \(M_{proj} = \begin{bmatrix} \frac{n}{r} &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; \frac{n}{t} &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; -\frac{f+n}{f-n} &amp; -\frac{2nf}{f-n} \\ 0 &amp; 0 &amp; -1 &amp; 0 \end{bmatrix}\)</p> <blockquote> <p>Step 4. from <code class="language-plaintext highlighter-rouge">camera-coordinate</code> to <code class="language-plaintext highlighter-rouge">NDC</code></p> </blockquote> <ul> <li> <p>camera-coordinate :<br/> \(\boldsymbol X = \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}\)</p> </li> <li> <p>NDC :<br/> \(\begin{bmatrix} -\frac{n}{r}\frac{X}{Z} \\ -\frac{n}{t}\frac{Y}{Z} \\ \frac{f+n}{f-n} + \frac{2nf}{f-n}\frac{1}{Z} \\ 1 \end{bmatrix} = \boldsymbol x \sim M_{proj} \boldsymbol X = \begin{bmatrix} \frac{n}{r}X \\ \frac{n}{t}Y \\ -\frac{f+n}{f-n}Z -\frac{2nf}{f-n} \\ -Z \end{bmatrix}\)<br/> Let \(a_x = -\frac{n}{r}\)<br/> \(a_y = -\frac{n}{t}\)<br/> \(a_z = \frac{f+n}{f-n}\)<br/> \(b_z = \frac{2nf}{f-n}\)<br/> Then \(\boldsymbol x = \begin{bmatrix} a_x\frac{X}{Z} \\ a_y\frac{Y}{Z} \\ a_z + \frac{b_z}{Z} \\ 1 \end{bmatrix} = \begin{bmatrix} a_x\frac{X}{Z} \\ a_y\frac{Y}{Z} \\ a_z + \frac{b_z}{Z} \end{bmatrix}\)</p> </li> </ul> <h3 id="projection-in-nerf-ray">Projection in NeRF ray</h3> <p>any 3D points on ray \(r = o + td\)를<br/> NDC space (camera 원점이 중앙에 있는 \([-1, 1]^3\) cube) 로 projection하면<br/> 3D points on projected ray \(r^{\ast} = o^{\ast} + t^{\ast} d^{\ast}\) 가 된다<br/> 위에서 유도한 Projection Matrix 를 사용하면<br/> \(\boldsymbol x = \begin{bmatrix} a_x\frac{o_x + td_x}{o_z + td_z} \\ a_y\frac{o_y + td_y}{o_z + td_z} \\ a_z + \frac{b_z}{o_z + td_z} \end{bmatrix} = \begin{bmatrix} o_x^{\ast} + t^{\ast} d_x^{\ast} \\ o_y^{\ast} + t^{\ast} d_y^{\ast} \\ o_z^{\ast} + t^{\ast} d_z^{\ast} \end{bmatrix}\)</p> <p>먼저 projected 원점 좌표를 구해보자<br/> \(t = t^{\ast} = 0\) 를 대입하면<br/> \(o^{\ast} = \begin{bmatrix} o_x^{\ast} \\ o_y^{\ast} \\ o_z^{\ast} \end{bmatrix} = \begin{bmatrix} a_x\frac{o_x}{o_z} \\ a_y\frac{o_y}{o_z} \\ a_z + \frac{b_z}{o_z} \end{bmatrix}\)</p> <p>다음으로 projected t와 d를 구해보자<br/> \(\begin{bmatrix} t^{\ast} d_x^{\ast} \\ t^{\ast} d_y^{\ast} \\ t^{\ast} d_z^{\ast} \end{bmatrix} = \begin{bmatrix} a_x\frac{o_x + td_x}{o_z + td_z} \\ a_y\frac{o_y + td_y}{o_z + td_z} \\ a_z + \frac{b_z}{o_z + td_z} \end{bmatrix} - \begin{bmatrix} o_x^{\ast} \\ o_y^{\ast} \\ o_z^{\ast} \end{bmatrix}\)<br/> \(= \begin{bmatrix} a_x\frac{o_x + td_x}{o_z + td_z} - a_x\frac{o_x}{o_z} \\ a_y\frac{o_y + td_y}{o_z + td_z} - a_y\frac{o_y}{o_z} \\ a_z + \frac{b_z}{o_z + td_z} - (a_z + \frac{b_z}{o_z}) \end{bmatrix}\)<br/> \(= \begin{bmatrix} a_x\frac{td_z}{o_z + td_z}(\frac{d_x}{d_z} - \frac{o_x}{o_z}) \\ a_y\frac{td_z}{o_z + td_z}(\frac{d_y}{d_z} - \frac{o_y}{o_z}) \\ -b_z\frac{td_z}{o_z + td_z}\frac{1}{o_z} \end{bmatrix}\)<br/> \(= \frac{td_z}{o_z + td_z} \begin{bmatrix} a_x(\frac{d_x}{d_z} - \frac{o_x}{o_z}) \\ a_y(\frac{d_y}{d_z} - \frac{o_y}{o_z}) \\ -b_z\frac{1}{o_z} \end{bmatrix}\)</p> <blockquote> <p>Result</p> </blockquote> <p>ray \(r = o + td\)를<br/> NDC space (camera 원점이 중앙에 있는 \([-1, 1]^3\) cube) 로 projection 했을 때<br/> projected ray \(r^{\ast} = o^{\ast} + t^{\ast} d^{\ast}\) 는 아래와 같이 구할 수 있다</p> <p>\(o^{\ast} = \begin{bmatrix} o_x^{\ast} \\ o_y^{\ast} \\ o_z^{\ast} \end{bmatrix} = \begin{bmatrix} a_x\frac{o_x}{o_z} \\ a_y\frac{o_y}{o_z} \\ a_z + \frac{b_z}{o_z} \end{bmatrix}\)<br/> and<br/> \(t^{\ast} = \frac{td_z}{o_z + td_z} = 1 - \frac{o_z}{o_z + td_z}\)<br/> and<br/> \(d^{\ast} = \begin{bmatrix} d_x^{\ast} \\ d_y^{\ast} \\ d_z^{\ast} \end{bmatrix} = \begin{bmatrix} a_x(\frac{d_x}{d_z} - \frac{o_x}{o_z}) \\ a_y(\frac{d_y}{d_z} - \frac{o_y}{o_z}) \\ -b_z\frac{1}{o_z} \end{bmatrix}\)</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">Ray Projection to NDC 장점</code></p> </blockquote> <p>ray에서 \(t \in [0, \infty)\) 였다면 projected ray에서 \(t^{\ast} \in [0, 1)\)</p> <p>LLFF dataset에서<br/> camera에서 출발한 ray가 아무 object도 “hit”하지 않는다면 \(t = \infty\)일텐데,<br/> NDC (bounded cube)로 warp한다면 \(t^{\ast} \in [0, 1)\) 이므로<br/> MLP 효율적으로 쓸 수 있음</p> <blockquote> <p>Projection transformation 한계</p> </blockquote> <p>LLFF dataset과 같이 <code class="language-plaintext highlighter-rouge">single</code> direction으로만 unbounded된 camera frustum, 즉 front-facing scene에 대해서만 적용 가능하고<br/> unbounded 360 scene에 대해서는 기본 NeRF가 잘 수행 못함<br/> \(\rightarrow\) MipNeRF360 등 NeRF 후속 연구에서 해결됨</p> <blockquote> <p>특정 case</p> </blockquote> <p>\(f_{cam}\)이 camera의 focal length이고,<br/> \(W, H\)가 image plane의 width, height in pix 일 때<br/> image plane이 정확히 camera frustum의 near plane에 있고<br/> camera frustum의 far plane을 infinity로 확장하도록<br/> camera를 설정하면,<br/> \(z = -n = f_{cam} \lt 0\), \(r = \frac{W}{2}\), \(t = \frac{H}{2}\), \(z = -f \rightarrow -\infty\) 이므로</p> <p>\(a_x = -\frac{n}{r} = \frac{f_{cam}}{\frac{W}{2}}\)<br/> \(a_y = -\frac{n}{t} = \frac{f_{cam}}{\frac{H}{2}}\)<br/> \(\lim_{f \rightarrow \infty} a_z = \lim_{f \rightarrow \infty} \frac{f+n}{f-n} = 1\)<br/> \(\lim_{f \rightarrow \infty} b_z = \lim_{f \rightarrow \infty} \frac{2nf}{f-n} = 2n\)<br/> 이므로</p> <p>ray \(r = o + td\) 를 NDC로 projection했을 때<br/> projected ray \(r^{\ast} = o^{\ast} + t^{\ast} d^{\ast}\) 에서<br/> \(o^{\ast} = \begin{bmatrix} \frac{f_{cam}}{\frac{W}{2}}\frac{o_x}{o_z} \\ \frac{f_{cam}}{\frac{H}{2}}\frac{o_y}{o_z} \\ 1 + \frac{2n}{o_z} \end{bmatrix}\)<br/> and<br/> \(t^{\ast} = \frac{td_z}{o_z + td_z} = 1 - \frac{o_z}{o_z + td_z}\)<br/> and<br/> \(d^{\ast} = \begin{bmatrix} \frac{f_{cam}}{\frac{W}{2}}(\frac{d_x}{d_z} - \frac{o_x}{o_z}) \\ \frac{f_{cam}}{\frac{H}{2}}(\frac{d_y}{d_z} - \frac{o_y}{o_z}) \\ -2n\frac{1}{o_z} \end{bmatrix}\)</p> <h3 id="ndc-projection-in-nerf-code">NDC projection in NeRF code</h3> <p><a href="https://github.com/yenchenlin/nerf-pytorch">NeRF-Pytorch</a> 기준으로<br/> run_nerf_helpers.py의 ndc_rays()에서 구현</p> <pre><code class="language-Python">def ndc_rays(H, W, focal, near, rays_o, rays_d):
    # shift ray origins to near plane
    t = -(near + rays_o[...,2]) / rays_d[...,2]
    rays_o = rays_o + t[...,None] * rays_d
    
    # projection
    o0 = -1. / (W / (2. * focal)) * rays_o[...,0] / rays_o[...,2]
    o1 = -1. / (H / (2. * focal)) * rays_o[...,1] / rays_o[...,2]
    o2 =  1. +  2. * near / rays_o[...,2]

    d0 = -1. / (W / (2. * focal)) * (rays_d[...,0]/rays_d[...,2] - rays_o[...,0]/rays_o[...,2])
    d1 = -1. / (H / (2. * focal)) * (rays_d[...,1]/rays_d[...,2] - rays_o[...,1]/rays_o[...,2])
    d2 = -2. * near / rays_o[...,2]
    
    rays_o = torch.stack([o0,o1,o2], -1)
    rays_d = torch.stack([d0,d1,d2], -1)
    
    return rays_o, rays_d
</code></pre>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="NDC"/><category term="3d"/><summary type="html"><![CDATA[How NDC Works for Ray]]></summary></entry><entry><title type="html">State Space Model</title><link href="https://semyeong-yu.github.io/blog/2024/SSM/" rel="alternate" type="text/html" title="State Space Model"/><published>2024-07-18T15:00:00+00:00</published><updated>2024-07-18T15:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/SSM</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/SSM/"><![CDATA[<h2 id="state-space-model">State Space Model</h2> <blockquote> <p>참고 논문 :<br/> <a href="https://arxiv.org/abs/2406.07887">https://arxiv.org/abs/2406.07887</a><br/> 참고 강연 :<br/> by NVIDIA Wonmin Byeon</p> </blockquote> <h3 id="abstract">Abstract</h3> <ul> <li>Large Language Models (LLMs) are usually based on <code class="language-plaintext highlighter-rouge">Transformer</code> architectures. <ul> <li>Transformer-based models 장점 :<br/> highly <code class="language-plaintext highlighter-rouge">parallelizable</code><br/> can model <code class="language-plaintext highlighter-rouge">massive amounts of data</code></li> <li>Transformer-based models 단점 :<br/> significant <code class="language-plaintext highlighter-rouge">computational overhead</code> due to the <code class="language-plaintext highlighter-rouge">quadratic self-attention</code> calculations, especially on longer sequences<br/> large inference-time <code class="language-plaintext highlighter-rouge">memory requirements</code> from the <code class="language-plaintext highlighter-rouge">key-value cache</code></li> </ul> </li> <li>More recently, <code class="language-plaintext highlighter-rouge">State Space Models (SSM)</code> like Mamba have been shown to have fast parallelizable training and inference as an alternative of Transformer.<br/> In this talk, I present the strengths and weaknesses of <code class="language-plaintext highlighter-rouge">Mamba, Mamba-2, and Transformer models</code> at larger scales. I also introduce a <code class="language-plaintext highlighter-rouge">hybrid architecture consisting of Mamba-2, attention, and MLP layers</code>.<br/> While pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks that require <code class="language-plaintext highlighter-rouge">strong copying</code> or <code class="language-plaintext highlighter-rouge">in-context learning</code> abilities.<br/> In contrast, the hybrid model closely matches or exceeds the Transformer on all standard and long-context tasks and is predicted to be up to 8x faster when generating tokens at inference time.</li> </ul> <h3 id="is-attention-all-we-need">Is Attention All We Need?</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/4-480.webp 480w,/assets/img/2024-07-18-SSM/4-800.webp 800w,/assets/img/2024-07-18-SSM/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Transformer : <ul> <li>fast training due to parallelization</li> <li>slow inference for long sequence(context) <ul> <li>key-value cache can improve speed, but increase GPU memory</li> </ul> </li> </ul> </li> <li>RNN : <ul> <li>slow training due to no parallelization</li> <li>fast inference because scale linearly with sequence length</li> </ul> </li> <li>Mamba : <ul> <li>fast training</li> <li>fast inference because scale linearly with sequence length and can deal with unbounded context</li> </ul> </li> <li> <p>SSM or RNN :<br/> state = fixed-sized vector (compression)<br/> high efficiency, but low performance</p> </li> <li>Transformer :<br/> cache of entire history (no compression)<br/> high performance, but low efficiency</li> </ul> <h3 id="mamba-linear-time-sequence-modeling-with-selective-state-spaces">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</h3> <ul> <li>SSM</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/1-480.webp 480w,/assets/img/2024-07-18-SSM/1-800.webp 800w,/assets/img/2024-07-18-SSM/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Selective SSM :<br/> matrix B, C and step size are dependent on the input</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/2-480.webp 480w,/assets/img/2024-07-18-SSM/2-800.webp 800w,/assets/img/2024-07-18-SSM/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Parallel scan :<br/> The order does not matter through the associative property, so can calculate sequences in part and iteratively combine them</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/3-480.webp 480w,/assets/img/2024-07-18-SSM/3-800.webp 800w,/assets/img/2024-07-18-SSM/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Hardware-aware implementation :<br/> minimize copying between RAMs</li> </ul> <h3 id="mamba-2">Mamba-2</h3> <ul> <li>Mamba에서 Main Bottleneck이 Parallel scan 부분이었는데,<br/> Mamba-2는 divide input into chunks 등 architecture 개선으로 이를 해결하고자 했음</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/13-480.webp 480w,/assets/img/2024-07-18-SSM/13-800.webp 800w,/assets/img/2024-07-18-SSM/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="limitations-of-mamba">Limitations of Mamba</h3> <ul> <li>Poor at MMLU and Phonebook task<br/> 아래를 요구하는 task에 대해서는 Mamba가 잘 못함 <ul> <li>in-context learning</li> <li>info. routing between tokens</li> <li>copying from the context (bad on long-context tasks)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/5-480.webp 480w,/assets/img/2024-07-18-SSM/5-800.webp 800w,/assets/img/2024-07-18-SSM/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/6-480.webp 480w,/assets/img/2024-07-18-SSM/6-800.webp 800w,/assets/img/2024-07-18-SSM/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="hybrid-architecture-of-mamba-and-transformer">Hybrid Architecture of Mamba and Transformer</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/7-480.webp 480w,/assets/img/2024-07-18-SSM/7-800.webp 800w,/assets/img/2024-07-18-SSM/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Our Hybrid Mamba-Transformer Model <ul> <li>Minimize the number of Attention Layers and Maximize the number of MLPs</li> <li>Does not necessarily need Rotary Position Embedding (RoPE)</li> <li>evenly spread attention and MLP layers</li> <li>Place Mamba layer at the beginning, so has no position embedding</li> <li>Group-Query Attention (GQA) makes more efficient</li> <li>Global Attention makes better performance</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/11-480.webp 480w,/assets/img/2024-07-18-SSM/11-800.webp 800w,/assets/img/2024-07-18-SSM/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Mamba-2 Hybrid<br/> Inference Speed is fast<br/> Now, states in Mamba can understand longer history!</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/12-480.webp 480w,/assets/img/2024-07-18-SSM/12-800.webp 800w,/assets/img/2024-07-18-SSM/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Attention Layer is bottleneck at Hybrid model,<br/> so Context Length가 길어질수록 Speedup 증가율은 줄어듬</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/8-480.webp 480w,/assets/img/2024-07-18-SSM/8-800.webp 800w,/assets/img/2024-07-18-SSM/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/9-480.webp 480w,/assets/img/2024-07-18-SSM/9-800.webp 800w,/assets/img/2024-07-18-SSM/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="summary">Summary</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/10-480.webp 480w,/assets/img/2024-07-18-SSM/10-800.webp 800w,/assets/img/2024-07-18-SSM/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 왼쪽부터 4K, 16K, 32K-based models </div> <p>Mamba-2 Hybrid는 Transformer와 달리 Quadratic calculation까지 필요 없고 inference 빠름<br/> but, Attention Layer가 Bottleneck이듯이 해결해야 할 사항들이 남아 있어 앞으로도 발전 가능성 있음</p>]]></content><author><name></name></author><category term="cv-tasks"/><category term="SSM"/><category term="Mamba"/><summary type="html"><![CDATA[SSM]]></summary></entry><entry><title type="html">3D Gaussian Splatting</title><link href="https://semyeong-yu.github.io/blog/2024/GS/" rel="alternate" type="text/html" title="3D Gaussian Splatting"/><published>2024-07-11T10:00:00+00:00</published><updated>2024-07-11T10:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/GS</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/GS/"><![CDATA[<h2 id="3d-gaussian-splatting-for-real-time-radiance-field-rendering">3D Gaussian Splatting for Real-Time Radiance Field Rendering</h2> <h4 id="bernhard-kerbl-georgios-kopanas-thomas-leimkühler-george-drettakis">Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, George Drettakis</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2308.04079">https://arxiv.org/abs/2308.04079</a><br/> project website :<br/> <a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/</a><br/> code :<br/> <a href="https://github.com/graphdeco-inria/gaussian-splatting">https://github.com/graphdeco-inria/gaussian-splatting</a><br/> referenced blog :<br/> <a href="https://xoft.tistory.com/51">https://xoft.tistory.com/51</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>DD</li> </ol> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/2-480.webp 480w,/assets/img/2024-07-11-GS/2-800.webp 800w,/assets/img/2024-07-11-GS/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="abstract">Abstract</h2> <ul> <li>novel 3D Gaussian scene representation with real-time differentiable renderer<br/> <code class="language-plaintext highlighter-rouge">수많은 3D Gaussian이 모여 scene을 구성</code>하고 있다!</li> <li>Very Fast rendering (\(\geq\) 100 FPS) :<br/> real-time as \(\geq\) 30 FPS<br/> rasterization이 optimization의 main bottleneck인데, 3DGS는 fast rasterization 가짐</li> <li>Higher Quality than SOTA Mip-NeRF360(2022)</li> <li>Faster Training than SOTA InstantNGP(2022)</li> </ul> <h2 id="introduction">Introduction</h2> <h3 id="why-3d-gaussian">Why 3D Gaussian?</h3> <p>3D scene representation 방법</p> <ol> <li><code class="language-plaintext highlighter-rouge">Mesh or Point</code> <ul> <li>explicit</li> <li>good for fast GPU/CUDA-based rasterization(3D \(\rightarrow\) 2D)</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">NeRF</code> method <ul> <li>implicit (MLP로 geometry 및 appearance를 표현)</li> <li>ray marching</li> <li>continuous coordinate-based representation</li> <li>interpolate values stored in voxels, hash grids, or points</li> <li>But,,, continuous ray로부터 discrete points를 뽑아 내는 <code class="language-plaintext highlighter-rouge">stochastic sampling</code> for rendering 때문에 <code class="language-plaintext highlighter-rouge">연산량이 많고 noise</code> 생김</li> <li>MLP는 dot product 및 더하기(kernel regression)의 특성상 <code class="language-plaintext highlighter-rouge">orthogonality</code>를 흐리기 때문에 high-freq. output을 잘 표현할 수 없어서 따로 미리 positional encoding을 수행</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">3D Gaussian</code> method <ul> <li>explicit (3D Gaussian으로 geometry를, SH coeff.로 appearance를 표현)</li> <li>differentiable volumetric representation</li> <li>efficient rasterization(projection and \(\alpha\)-blending)</li> <li>3D Gaussian(ellipsoid)이나 SH coeff.라는 explicit 표현 자체가 <code class="language-plaintext highlighter-rouge">orthogonality</code>를 잘 살리기 때문에 high-freq. output 잘 표현 가능</li> </ul> </li> </ol> <h3 id="rendering-nerf-vs-3dgs">Rendering (NeRF vs 3DGS)</h3> <ul> <li>NeRF : <ul> <li>ray per pixel 쏴서 coarse(stratified) and fine(PDF) sampling하고,</li> <li>MLP로 sampled points의 color 및 volume density를 구하고,</li> <li>이 값들을 volume rendering 식으로 summation</li> </ul> </li> <li>3DGS : <ul> <li>image를 tile(14 \(\times\) 14 pixel)들로 나누고,</li> <li>tile마다 Gaussian을 Depth에 따라 정렬한 뒤</li> <li>앞에서부터 뒤로 \(\alpha\)-blending</li> </ul> </li> </ul> <h2 id="related-work">Related Work</h2> <p>생략 (추후에 다시 볼 수도)</p> <h2 id="overview">Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/1-480.webp 480w,/assets/img/2024-07-11-GS/1-800.webp 800w,/assets/img/2024-07-11-GS/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For unbounded and complete scenes,<br/> For 1080p high resolution and real-time(\(\geq\) 30 fps) rendering,</p> <ol> <li><code class="language-plaintext highlighter-rouge">input</code> : <ul> <li>Most point-based methods require <code class="language-plaintext highlighter-rouge">MVS</code>(Multi-View Stereo) data,<br/> but 3DGS only needs <code class="language-plaintext highlighter-rouge">SfM points</code> for initialization</li> <li>COLMAP 등 SfM(Structure-from-Motion) camera calibration으로 얻은 <code class="language-plaintext highlighter-rouge">sparse point cloud</code>에서 시작해서<br/> scene을 3D Gaussians로 나타냄으로써<br/> <code class="language-plaintext highlighter-rouge">empty space에서의 불필요한 계산을 하지 않도록</code> continuous volumetric radiance fields 정보를 저장</li> <li>NeRF-synthetic dataset의 경우 bg가 없어서 3DGS random initialization으로도 좋은 퀄리티 달성</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">optimization</code> interleaved with <code class="language-plaintext highlighter-rouge">adaptive density control</code> : <ul> <li>optimize 4 parameters :<br/> 3D position(mean), anisotropic covariance, opacity, and spherical harmonic coeff.(color)<br/> <code class="language-plaintext highlighter-rouge">highly anisotropic volumetric splats</code>는 <code class="language-plaintext highlighter-rouge">fine structures</code>를 compact하게 나타낼 수 있음!!<br/> <code class="language-plaintext highlighter-rouge">spherical harmonics</code>를 통해 <code class="language-plaintext highlighter-rouge">directional appearance(color)</code>를 잘 나타낼 수 있음!!<d-cite key="Plenoxels">[1]</d-cite>, <d-cite key="InstantNGP">[2]</d-cite></li> <li>adaptive density control :<br/> gradient 기반으로 Gaussian 형태를 변화시키기 위해, add and occasionally remove 3D Gaussians during optimization</li> </ul> </li> <li>differentiable visibility-aware <code class="language-plaintext highlighter-rouge">real-time rendering</code> :<br/> perform \(\alpha\)-blending of <code class="language-plaintext highlighter-rouge">anisotropic splats</code> respecting visibility order<br/> by fast <code class="language-plaintext highlighter-rouge">GPU sorting</code> algorithm and <code class="language-plaintext highlighter-rouge">tile-based rasterization</code>(projection and \(\alpha\)-blending)<br/> 한편, accumulated \(\alpha\) values를 tracking함으로써 <code class="language-plaintext highlighter-rouge">Gaussians 수에 제약 없이</code> 빠른 backward pass도 가능</li> </ol> <hr/> <h3 id="pseudo-code">Pseudo-Code</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/3-480.webp 480w,/assets/img/2024-07-11-GS/3-800.webp 800w,/assets/img/2024-07-11-GS/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>빨간 박스 : initialization<br/> 파란 박스 : optimization<br/> 초록 박스 : 특정 iter.마다 Gaussian을 clone, split, remove</p> <h2 id="differentiable-3d-gaussian-splatting">Differentiable 3D Gaussian Splatting</h2> <h3 id="3d-gaussian">3D Gaussian</h3> <ul> <li> <p><code class="language-plaintext highlighter-rouge">differentiable</code> volumetric representation의 특성을 가지고 있으면서도 빠른 rendering을 위해 <code class="language-plaintext highlighter-rouge">unstructured and explicit</code>한 게 무엇이 있을까?<br/> \(\rightarrow\) 3D Gaussian !!</p> </li> <li> <p>a point를 a small planar circle with a normal이라고 가정하는 이전 Point-based rendering 논문들 <d-cite key="Point1">[3]</d-cite> <d-cite key="Point2">[4]</d-cite> 과 달리<br/> <code class="language-plaintext highlighter-rouge">SfM points는 sparse해서 normals(법선)를 estimate하기 어려울</code> 뿐만 아니라, estimate 한다 해도 very noisy normals를 optimize하는 것은 매우 어렵<br/> \(\rightarrow\) normals 필요 없는 3D Gaussians !!<br/> k-dim. Gaussian : \(G(\boldsymbol x) = (2\pi)^{-\frac{k}{2}}det(\Sigma)^{-\frac{1}{2}}e^{-\frac{1}{2}(\boldsymbol x - \boldsymbol \mu)^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu)}\)</p> </li> </ul> <h2 id="parameters-to-train">Parameters to train</h2> <ol> <li><code class="language-plaintext highlighter-rouge">scale vector</code> \(s\) and <code class="language-plaintext highlighter-rouge">quaternion</code> \(q\) for <code class="language-plaintext highlighter-rouge">covariance matrix</code></li> <li><code class="language-plaintext highlighter-rouge">spherical harmonics</code>(SH) coeff. for <code class="language-plaintext highlighter-rouge">color</code></li> <li><code class="language-plaintext highlighter-rouge">opacity</code> \(\alpha\)</li> <li><code class="language-plaintext highlighter-rouge">3D position</code> for <code class="language-plaintext highlighter-rouge">mean</code></li> </ol> <h3 id="parameter-1-covariance-matrix">Parameter 1. Covariance matrix</h3> <blockquote> <p>covariance matrix and scale vector(scale) and quaternion(rotation)</p> </blockquote> <ul> <li>covariance matrix는 positive semi-definite \(x^T M x \geq 0\) for all \(x \in R^n\)이어야만 physical meaning을 가지는데,<br/> \(\Sigma\) 를 직접 바로 optimize하면 invalid covariance matrix가 될 수 있음<br/> 그렇다면!!</li> </ul> <p>\(\Sigma\) 가 <code class="language-plaintext highlighter-rouge">symmetric</code> and <code class="language-plaintext highlighter-rouge">positive semi-definite</code>이도록 \(\Sigma = R S S^T R^T\) 로 정의해서<br/> \(\Sigma\) 대신 <code class="language-plaintext highlighter-rouge">x,y,z-axis scale</code>을 나타내는 <code class="language-plaintext highlighter-rouge">3D vector</code> \(s\) 와 <code class="language-plaintext highlighter-rouge">rotation</code>을 나타내는 <code class="language-plaintext highlighter-rouge">4D quaternion</code> \(q\) 를 optimize 하자!!<br/> quaternion에 대한 설명은 <a href="https://semyeong-yu.github.io/blog/2024/Quaternion">Quaternion</a> 블로그 참고!!</p> <ul> <li><code class="language-plaintext highlighter-rouge">scale</code> 3D vector \(s\) <code class="language-plaintext highlighter-rouge">초기값</code> :<br/> <a href="https://github.com/graphdeco-inria/gaussian-splatting/blob/b2ada78a779ba0455dfdc2b718bdf1726b05a1b6/scene/gaussian_model.py#L134C1-L134C1">GaussianModel().create_from_pcd()</a><br/> SfM sparse point cloud의 각 점에 대해 가장 가까운 점 3개까지의 거리의 평균을 각 axis(\(x, y, z\))별로 구한 것을 3 \(\times\) 1 \(s\)라 할 때<br/> normalize 효과를 위해 log, sqrt 씌운 뒤<br/> 3 \(\times\) 1 \(log(\sqrt{s})\) 의 값을 3번 복사하여 3 \(\times\) 3 scale matrix \(S\)를 초기화 <pre><code class="language-Python">dist2 = torch.clamp_min(distCUDA2(torch.from_numpy(np.asarray(pcd.points)).float().cuda()), 0.0000001)
scales = torch.log(torch.sqrt(dist2))[...,None].repeat(1, 3)
</code></pre> </li> <li> <p><code class="language-plaintext highlighter-rouge">scale</code> 3D vector \(s\) <code class="language-plaintext highlighter-rouge">activation function</code> :<br/> smooth gradient 얻기 위해 exponential activation function을 씌움</p> </li> <li><code class="language-plaintext highlighter-rouge">quaternion</code> \(q\) <code class="language-plaintext highlighter-rouge">초기값</code> :<br/> 각 점에 대해 \(\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}\) 으로 quaternion을 초기화하고<br/> 이를 이용하여 rotation matrix \(R\) 초기화 <pre><code class="language-Python">rots = torch.zeros((fused_point_cloud.shape[0], 4), device="cuda")
rots[:, 0] = 1
</code></pre> </li> <li><code class="language-plaintext highlighter-rouge">anisotropic covariance</code>는 다양한 모양의 geometry를 나타내기 위해 optimize하기에 적합!</li> </ul> <blockquote> <p>param. gradient 직접 유도</p> </blockquote> <p>training할 때 automatic differentiation으로 인한 <code class="language-plaintext highlighter-rouge">overhead를 방지</code>하기 위해 <code class="language-plaintext highlighter-rouge">param. gradient를 직접 유도</code>함!</p> <p>Appendix A. <code class="language-plaintext highlighter-rouge">?????</code></p> <blockquote> <p>EWA volume splatting (2001) :<br/> world-to-camera 는 linear transformation 이지만,<br/> <code class="language-plaintext highlighter-rouge">camera-to-image (projection)</code> 는 <code class="language-plaintext highlighter-rouge">non-linear transformation</code> 이다!!</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/4-480.webp 480w,/assets/img/2024-07-11-GS/4-800.webp 800w,/assets/img/2024-07-11-GS/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 위 그림 : camera coordinate / 아래 그림 : image coordinate (ray space) </div> <ul> <li><code class="language-plaintext highlighter-rouge">world</code> coordinate (3D) : <ul> <li> \[\boldsymbol u = \begin{bmatrix} u_0 \\ u_1 \\ u_2 \end{bmatrix}\] </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">camera</code> coordinate (3D) : <ul> <li>\(\boldsymbol t = \begin{bmatrix} t_0 \\ t_1 \\ t_2 \end{bmatrix}\)<br/> \(= W \boldsymbol u + d\)<br/> where \(W\) : <code class="language-plaintext highlighter-rouge">viewing transformation</code> affine matrix from world coordinate to camera coordinate</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">image</code> coordinate (2D) : <ul> <li>\(\boldsymbol x = \begin{bmatrix} x_0 \\ x_1 \\ x_2 \end{bmatrix}\)<br/> \(= \phi(\boldsymbol t) = \begin{bmatrix} \frac{t_0}{t_2} \\ \frac{t_1}{t_2} \\ \| (t_0, t_1, t_2)^T \| \end{bmatrix}\)</li> <li>function \(\phi\) 는 non-linear하므로 Affine transformation이 불가능하다.</li> <li><code class="language-plaintext highlighter-rouge">Local Affine (Linear) transform으로 Approx.</code>하기 위해 \(\boldsymbol t = \boldsymbol t_{k}\) 에서의 <code class="language-plaintext highlighter-rouge">Taylor Approx.</code>를 이용하면,<br/> \(\phi_{k}(\boldsymbol t) = \phi(\boldsymbol t_{k}) + \boldsymbol J_{k} \cdot (\boldsymbol t - \boldsymbol t_{k})\)<br/> where<br/> \(\boldsymbol J_{k} = \frac{d\phi}{d \boldsymbol t}(\boldsymbol t_{k}) = \begin{bmatrix} \frac{d\phi}{d \boldsymbol t_{0}}(\boldsymbol t_{k}) &amp; \frac{d\phi}{d \boldsymbol t_{1}}(\boldsymbol t_{k}) &amp; \frac{d\phi}{d \boldsymbol t_{2}}(\boldsymbol t_{k}) \end{bmatrix} = \begin{bmatrix} \frac{1}{t_{k, 2}} &amp; 0 &amp; -\frac{t_{k, 0}}{t_{k, 2}^2} \\ 0 &amp; \frac{1}{t_{k, 2}} &amp; -\frac{t_{k, 1}}{t_{k, 2}^2} \\ \frac{t_{k, 0}}{l} &amp; \frac{t_{k, 1}}{l} &amp; \frac{t_{k, 2}}{l} \end{bmatrix}\)<br/> and ray distance \(l = \| (t_{k, 0}, t_{k, 1}, t_{k, 2})^T \|\)<br/> Here, \(J\) : <code class="language-plaintext highlighter-rouge">Jacobian</code>(각 axis로 편미분한 matrix) of the <code class="language-plaintext highlighter-rouge">affine approx.</code> of the <code class="language-plaintext highlighter-rouge">projective transformation</code> from camera coordinate to image coordinate</li> <li>즉, camera coordinate에서 임의의 좌표 \(\boldsymbol t_{k}\) 주변에 존재하는 입력 좌표 \(\boldsymbol t\)에 대해서는 image coordinate으로의 affine(linear) transformation이 충족된다.</li> <li>Gaussian Splatting 논문의 경우 <code class="language-plaintext highlighter-rouge">Gaussian의 중심점</code>을 \(\boldsymbol t_{k}\) 로 두면 그 주변의 \(\boldsymbol t\)에 대해서는 Jacobian을 이용한 affine(linear) transformation 가능!</li> </ul> </li> </ul> <blockquote> <p><code class="language-plaintext highlighter-rouge">Projection</code> of 3D Gaussian <code class="language-plaintext highlighter-rouge">covariance</code> to 2D</p> </blockquote> <ul> <li> <p><code class="language-plaintext highlighter-rouge">world coordinate</code> :<br/> \(\Sigma\) : 3 \(\times\) 3 covariance matrix of 3D Gaussian</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">image coordiante</code> (z=1) :<br/> \(\Sigma^{\ast} = J W \Sigma W^T J^T\) : covariance matrix of 2D splat</p> <ul> <li>Step 1. world-to-camera (<code class="language-plaintext highlighter-rouge">affine</code>) :<br/> \(\boldsymbol u \rightarrow W \boldsymbol u + d\)</li> <li>Step 2. camera-to-image (<code class="language-plaintext highlighter-rouge">local affine approx.</code>) :<br/> Projection<br/> \(W \boldsymbol u + d \rightarrow \phi_{k}(W \boldsymbol u + d) = x_k + \boldsymbol J_{k} W \boldsymbol u + \boldsymbol J_{k} (d - \boldsymbol t_{k})\)<br/> 상수 부분을 제외하면 \(\boldsymbol x = \boldsymbol J_{k} W \boldsymbol u\)</li> <li>Step 3. covariance 특성 :<br/> \(Cov[Ax] = E[(Ax - E[Ax])(Ax - E[Ax])^T]\)<br/> \(= E[A(x - E[x])(x - E[x])^TA^T] = A Cov[x] A^T\)</li> <li>Step 4. <code class="language-plaintext highlighter-rouge">world-to-image covariance</code> :<br/> \(\boldsymbol u \rightarrow \boldsymbol J_{k} W \boldsymbol u\) 이므로<br/> \(\Sigma \rightarrow \boldsymbol J \boldsymbol W \Sigma \boldsymbol W^T \boldsymbol J^T\)</li> <li>Step 5. <code class="language-plaintext highlighter-rouge">covariance dimension reduction</code> :<br/> 추가로, \(\boldsymbol J \boldsymbol W \Sigma \boldsymbol W^T \boldsymbol J^T\) 로 계산한 \(\Sigma^{\ast}\) 는 3-by-3 matrix 인데,<br/> 3D Gaussian을 한쪽 축으로 적분하면 2D Gaussian과 동일한 값을 가지게 되므로<br/> 3-by-3 covariance matrix의 3번째 행과 열의 값을 버린<br/> 2-by-2 matrix를 projected 2D covariance matrix 로 사용하면 됨!</li> </ul> </li> </ul> <h3 id="parameter-2-spherical-harmonicssh-coeff">Parameter 2. Spherical Harmonics(SH) coeff.</h3> <ul> <li><code class="language-plaintext highlighter-rouge">Spherical Harmonics</code> (SH) :<br/> spherical coordinate 에서 <code class="language-plaintext highlighter-rouge">각도</code> (\(\theta, \phi\))를 입력받아 <code class="language-plaintext highlighter-rouge">구의 표면 위치에서의 값</code>을 출력하는 함수<br/> spherical coordinate 에서 라플라스 방정식을 풀면 아래 수식과 같음</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/5-480.webp 480w,/assets/img/2024-07-11-GS/5-800.webp 800w,/assets/img/2024-07-11-GS/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/6-480.webp 480w,/assets/img/2024-07-11-GS/6-800.webp 800w,/assets/img/2024-07-11-GS/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/7-480.webp 480w,/assets/img/2024-07-11-GS/7-800.webp 800w,/assets/img/2024-07-11-GS/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> l이 같은 함수들은 same band l에 있다고 말함 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/8-480.webp 480w,/assets/img/2024-07-11-GS/8-800.webp 800w,/assets/img/2024-07-11-GS/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 가로축 : theta, 세로축 : phi, 채도 : SH magnitude, 색상 : SH phase </div> <ul> <li> <p>SH coeff. <code class="language-plaintext highlighter-rouge">초기값</code> :<br/> 0-band SH (\(\theta, \phi\) 와 관계없는 view-independent color) 의 경우 SfM으로 얻은 point cloud의 RGB color값과 RGB2SH 이용하여 초기화<br/> 다른 band의 경우 0으로 초기화</p> </li> <li>SH 의 역할 : <ul> <li>SH에서 band 수를 제한해서 쓴다는 것은 높은 band (high freq. 또는 detail info.)는 자른다는 의미이므로 <code class="language-plaintext highlighter-rouge">smoothing</code> 역할</li> <li>적은 비용(coeff. 몇 개만 사용)으로 SH function을 <code class="language-plaintext highlighter-rouge">approx.</code></li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">SH coeff.</code>로 <code class="language-plaintext highlighter-rouge">color</code> 나타내는 법 :<br/> Fourier Series 에서처럼,<br/> SH coeff. \(k_{l}^{m}\) 의 optimal 값을 구해서<br/> \(k_{l}^{m}\) 와 \(Y_l^m(\theta, \phi)\) 의 weighted sum!<br/> \(C = \Sigma_{l=0}^{l_{max}} \Sigma_{m=-l}^{l} k_l^m Y_l^m(\theta, \phi)\)<br/> 즉, <code class="language-plaintext highlighter-rouge">trainable parameter</code> : SH coeff.인 \(k_{l}^{m}\)<br/> (<code class="language-plaintext highlighter-rouge">light source</code>마다 SH coeff. \(k_{l}^{m}\) 다르므로 find optimal value)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/9-480.webp 480w,/assets/img/2024-07-11-GS/9-800.webp 800w,/assets/img/2024-07-11-GS/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="parameter-3-opacity">Parameter 3. opacity</h3> <ul> <li> <p>opacity \(\alpha\) <code class="language-plaintext highlighter-rouge">초기값</code> :<br/> 임의의 실수값으로 초기화<br/> inverse_sigmoid(0.1 * torch.ones(…))</p> </li> <li> <p>opacity \(\alpha\) <code class="language-plaintext highlighter-rouge">range</code> :<br/> \(\alpha \in [0, 1)\) 이므로<br/> 마지막에 sigmoid activation function을 씌워서 smooth gradient를 얻음</p> </li> </ul> <h3 id="parameter-4-3d-positionmean">Parameter 4. 3D position(mean)</h3> <h2 id="fast-differentiable-rasterizer-for-gaussians">Fast Differentiable Rasterizer for Gaussians</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/10-480.webp 480w,/assets/img/2024-07-11-GS/10-800.webp 800w,/assets/img/2024-07-11-GS/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Tile Rasterizer</p> </blockquote> <ul> <li> <p>기능 : 3D Gaussians로 구성된 3D model을 특정 camera pose에 대해 2D rendering</p> </li> <li><code class="language-plaintext highlighter-rouge">input</code> : <ul> <li>image의 rendering할 width, height</li> <li>3D Gaussian의 xyz-mean, covariance in world-coordinate</li> <li>3D Gaussian의 color, opacity</li> <li>current camera pose</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Frustum Culling</code> :<br/> 주어진 camera pose에서 view frustum을 그려서<br/> view frustum과 교차하는 확률이 99% confidence interval 범위 밖에 있는 3D Gaussians는 제거(culling)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/16-480.webp 480w,/assets/img/2024-07-11-GS/16-800.webp 800w,/assets/img/2024-07-11-GS/16-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Guard Band</code> :<br/> 아래의 경우 projected 2D covariance 계산이 불안정하기 때문에 개별적으로 제거 <ul> <li><code class="language-plaintext highlighter-rouge">view frustum의 near plane에 가까이 있는</code> Gaussian의 경우,<br/> EWA Volume Splatting에서 언급된 cam-to-img projection <code class="language-plaintext highlighter-rouge">nonlinearity</code>가 심하기 때문에<br/> projection matrix를 Jacobian으로 approx.한 값에 더 큰 artifact가 생김</li> <li>view frustum 밖에 멀리 떨어진 경우 <code class="language-plaintext highlighter-rouge">?????</code><br/> 코드에서는 이 경우는 빼버렸음 (주석 처리)<br/> (diff-gaussian-rasterization/cuda_rasterizer/auxiliary.h)</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Create Tiles</code> :<br/> <code class="language-plaintext highlighter-rouge">CUDA 병렬 처리</code>를 위해<br/> \(w \times h\)의 image를 \(16 \times 16\) pixel의 tiles로 쪼갬</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/11-480.webp 480w,/assets/img/2024-07-11-GS/11-800.webp 800w,/assets/img/2024-07-11-GS/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p><code class="language-plaintext highlighter-rouge">Parallelism</code> :<br/> <code class="language-plaintext highlighter-rouge">tile마다</code> 개별 <code class="language-plaintext highlighter-rouge">CUDA thread</code> block으로 실행하여<br/> forward/backward processing, data loading/sharing을 병렬처리<br/> (여러 threads가 Gaussian points를 shared memory에 collaboratively load)<br/> (VRAM과 DRAM 사이의 이동은 overhead 발생하기 때문에 <code class="language-plaintext highlighter-rouge">VRAM</code>에서 모두 처리해버릴 수 있도록 <code class="language-plaintext highlighter-rouge">CUDA Functions</code>(.cu)를 직접 짬!)</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Duplicate with Keys</code> :</p> <ul> <li><code class="language-plaintext highlighter-rouge">view-space-depth</code>와 <code class="language-plaintext highlighter-rouge">tile-ID</code>를 이용하여 tile마다 각 Gaussian의 key를 생성<br/> tile-ID 쪽이 MSB<br/> view-space-depth 쪽이 LSB<br/> 각 Gaussian의 value는 Gaussian’s index</li> <li><code class="language-plaintext highlighter-rouge">CUDA 병렬처리</code> 덕분에 2D Gaussian 하나가 3개의 tiles에 걸쳐 있다면, 3개의 2D Gaussians로 복제(<code class="language-plaintext highlighter-rouge">instance화</code>)되는 것처럼 작동</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/12-480.webp 480w,/assets/img/2024-07-11-GS/12-800.webp 800w,/assets/img/2024-07-11-GS/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Sort by Keys</code> : <ul> <li>tile마다 Depth 기준으로 <code class="language-plaintext highlighter-rouge">Radix Sort</code></li> <li>여기서 한 번 sort 하고 나면 끝!! 추가로 per-pixel sorting 할 필요 없음</li> <li>pixel-wise sorting이 아니라 Gaussians sort라서 \(\alpha\)-blending approx.이긴 한데, <code class="language-plaintext highlighter-rouge">splats가 각 pixel size 정도가 되기 때문에</code> 해당 approx. 오차는 무시 가능!</li> <li>쨌든 이 덕분에 visible artifacts 없이 training, rendering performance 베리베리 굳</li> </ul> </li> </ul> <pre><code class="language-Python">from collections import deque
# 양방향에서 삽입/삭제 가능한 queue형 자료구조

# 1의 자릿수 기준으로 정렬한 뒤
# 10의 자릿수 기준으로 정렬한 뒤
# ...
def radixSort():
    nums = list(map(int, input().split(' ')))
    buckets = [deque() for _ in range(10)] # 각 자릿수(0~9)에 대응되는 10개의 empty deque()
    
    max_val = max(nums)
    queue = deque(nums) # 정렬할 숫자들
    digit = 1 # 정렬 기준이 되는 자릿수
    
    while (max_val &gt;= digit): # 가장 큰 수의 자릿수일 때까지만 실행
        while queue:
            num = queue.popleft() # 정렬할 숫자
            buckets[(num // digit) % 10].append(num) # 각 자릿수(0~9)에 따라 buckets에 num을 넣는다.
        
        # 해당 정렬 기준 자릿수에서 buckets에 다 넣었으면, buckets에 담겨있는 순서대로 꺼내와서 정렬한다.
        for bucket in buckets:
            while bucket:
                queue.append(bucket.popleft())

        digit *= 10 # 정렬 기준이 되는 자릿수 증가시키기
    
    print(list(queue))
</code></pre> <ul> <li> <p><code class="language-plaintext highlighter-rouge">Identify Tile Ranges</code> :<br/> tile별 Gaussian list를 효율적으로 관리하기 위해<br/> tile마다 Gaussian list 범위 식별</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Get Tile Ranges</code> :<br/> 모든 tile에 대해 Gaussian list 범위 읽어옴</p> </li> <li>\(\alpha\)-Blending in Order (<code class="language-plaintext highlighter-rouge">forward process</code>) : <ul> <li>tile별 CUDA 병렬처리에 의해 각 pixel에 대해 <code class="language-plaintext highlighter-rouge">color</code> 및 <code class="language-plaintext highlighter-rouge">opacity</code> \(\alpha\) 값을 Gaussian list의 <code class="language-plaintext highlighter-rouge">앞에서 뒤로</code> accumulate</li> <li>i-th tile에 있는 pixels 중 a pixel’s accumulated opacity 값이 target saturation threshold를 넘어서면, 해당 i-th thread STOP (유일한 STOP 조건)</li> <li><code class="language-plaintext highlighter-rouge">Gaussian의 개수를 제한하지 않음</code>으로써 scene-specific hyper-param. tuning 없이 arbitrary depth complexity를 가지는 scene을 커버 가능<br/> (GPU Radix Sort 덕분에 parallelism(병렬) 및 amortized(분할상환) 가능하여 Gaussian 개수 늘릴 수 있었음)</li> <li><code class="language-plaintext highlighter-rouge">기존 기법들은 pixel마다 정렬이 필요</code>해서 inefficient했지만<br/> 본 논문은 tile별 CUDA 병렬처리 덕분에 efficient<br/> (e.g. NeRF : ray per pixel 쏴서 accumulate per pixel for volume rendering <code class="language-plaintext highlighter-rouge">???</code>)</li> </ul> <p>\(c = \alpha_{1}c_{1} + (1 - \alpha_{1})(\alpha_{2}c_{2} + (1 - \alpha_{2})(\cdots)) = \alpha_{1}c_{1} + (1 - \alpha_{1})\alpha_{2}c_{2} + (1 - \alpha_{1})(1 - \alpha_{2})(\cdots) = \cdots = \sum_{i=1}^{N}(\alpha_{i}c_{i}\prod_{j=1}^{i-1}(1-\alpha_{j}))\) where \(\alpha_{0} = 0\)</p> </li> <li><code class="language-plaintext highlighter-rouge">Backward process</code> : <ul> <li>각 tile의 Gaussian list에 대해 Gaussian의 <code class="language-plaintext highlighter-rouge">opacity 비율에 따라</code> <code class="language-plaintext highlighter-rouge">뒤에서 앞으로</code> gradient update</li> <li>직접 backward gradient update 식을 구해서 이용</li> <li>backward process를 위해 <d-cite key="Point1">[3]</d-cite>처럼 <code class="language-plaintext highlighter-rouge">pixel마다</code> global memory에 blended points list를 저장할 수도 있지만<br/> dynamic memory management overhead가 생기기 때문에<br/> forward process에서 <code class="language-plaintext highlighter-rouge">tile마다</code> 구했던 range 및 sorted Gaussian list를 <code class="language-plaintext highlighter-rouge">재사용</code></li> <li>\(\alpha\)-blending으로 합쳤던 각 Gaussian으로 gradient backpropagation을 해주려면<br/> \(\alpha\)-blending 각 step에서의 accumulated opacity 값이 필요한데,<br/> 이를 따로 list에 저장해두고 훑는 게 아니라,<br/> \(\alpha\)-blending을 할 때 그때그때 각 Gaussian point에 지금까지의 accumulated opacity 값인 \(\alpha_{l}\)을 저장해두고,<br/> \(\alpha\)-blending final step에서의 total accumulated opacity 값 \(\alpha_{N}\)만 backward process에 넘겨 주면<br/> backward process할 때 \(\alpha_{N}\)를 \(\alpha_{l}\)로 나눈 값을 gradient 계산에 사용 <code class="language-plaintext highlighter-rouge">?????</code></li> <li>0으로 나눠지는 경우를 방지하기 위해 \(\alpha\) 값이 \(\frac{1}{255}\)보다 작다면 blending update 안 함</li> <li>rasterization 중에 blending 값이 0.9999를 초과하기 전에 STOP<br/> <code class="language-plaintext highlighter-rouge">?????</code></li> </ul> </li> <li> <p>Primitives :<br/> 본 논문의 Gaussians는 <code class="language-plaintext highlighter-rouge">Euclidean space</code>에 <code class="language-plaintext highlighter-rouge">primitives</code>를 남김 <code class="language-plaintext highlighter-rouge">?????</code><br/> \(\rightarrow\) <d-cite key="Plenoxels">[1]</d-cite>, <d-cite key="MipNeRF360">[10]</d-cite>과 달리 distant or large Gaussians 처리를 위해 space compaction, warping, or projection 할 필요가 없음 <code class="language-plaintext highlighter-rouge">?????</code></p> </li> <li>Efficient Rasterization : <ul> <li>Pulsar 논문<d-cite key="Pulsar">[5]</d-cite> 에서처럼<br/> an entire image에 대해 가장 작은 원소(<code class="language-plaintext highlighter-rouge">primitives</code>)를 미리 정렬(<code class="language-plaintext highlighter-rouge">pre-sort</code>)하여 <code class="language-plaintext highlighter-rouge">primitives = Gaussians ?????</code><br/> pixel-wise sorting 비용을 절감</li> <li>differentiable</li> <li>arbitrary number of Gaussians에 대해 backpropagation 가능<br/> with low additional memory : O(1) per pixel</li> <li>2D projection 가능</li> </ul> </li> </ul> <h2 id="optimization-with-adaptive-density-control-of-3d-gaussians">Optimization with Adaptive Density Control of 3D Gaussians</h2> <h3 id="optimization">Optimization</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/13-480.webp 480w,/assets/img/2024-07-11-GS/13-800.webp 800w,/assets/img/2024-07-11-GS/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Loss :<br/> predicted image와 GT image를 비교하는<br/> <code class="language-plaintext highlighter-rouge">L1 loss</code> 및 <code class="language-plaintext highlighter-rouge">D-SSIM loss</code><br/> D-SSIM : Directional Structural Similarity Index Measure</p> </li> <li> <p>3D Gaussian의 xyz-mean에 대해서만 <d-cite key="Plenoxels">[1]</d-cite>에서처럼 <code class="language-plaintext highlighter-rouge">standard exponential decay scheduling</code> 사용</p> </li> <li>Adam optimizer로 네 가지 param. 업데이트 <ul> <li>3D xyz-mean</li> <li>3D covariance</li> <li>color</li> <li>opacity</li> </ul> </li> <li>optimization 세부 사항 : <ul> <li>연산을 <code class="language-plaintext highlighter-rouge">low resol.부터 warm-up</code> :<br/> 목적 : model이 효율적으로 coarse info.부터 학습하도록 하여 <code class="language-plaintext highlighter-rouge">stability</code> 향상<br/> 초기에 4배 작은 image로 optimization 진행하고 250, 500 iter.에서 2배씩 upsampling</li> <li>Spherical Harmonics <code class="language-plaintext highlighter-rouge">low band부터 warm-up</code> :<br/> 목적 : 처음부터 high band로 detail까지 학습하려고 하면<br/> scene의 corner를 촬영하거나 inside-out 방식(카메라가 촬영 대상의 내부에 위치하여 바깥쪽을 촬영) 때문에<br/> <code class="language-plaintext highlighter-rouge">놓친 angular 영역이 있을 경우 SH의 0-band coeff. (base or diffuse color)가 부적절</code>하게 만들어질 수 있어서<br/> 처음에는 0-band coeff.를 optimize하고 매 1000 iter.마다 band 수 늘려서 4-band coeff.까지 optimization</li> </ul> </li> </ul> <h3 id="adaptive-density-control-of-gaussians">Adaptive Density Control of Gaussians</h3> <p>optimization of 4 param.의 경우 매 iter.마다 update하지만,<br/> Adaptive Density Control of Gaussians의 경우 <code class="language-plaintext highlighter-rouge">100 iter.마다</code> update</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/14-480.webp 480w,/assets/img/2024-07-11-GS/14-800.webp 800w,/assets/img/2024-07-11-GS/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Remove</code> :<br/> \(\alpha\) 값이 threshold보다 작거나<br/> world-space에서 크기가 매우 크거나<br/> view-space에서 footprint가 매우 큰 경우<br/> 3D Gaussians 제거</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/15-480.webp 480w,/assets/img/2024-07-11-GS/15-800.webp 800w,/assets/img/2024-07-11-GS/15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Gaussians가 scene을 제대로 표현 못 하는 중<br/> \(\rightarrow\) scene을 제대로 표현하기 위해선 Gaussian position을 크게 옮겨야 함<br/> \(\rightarrow\) view-space positional gradient \(\Delta_{p} L\)가 큼<br/> \(\rightarrow\) under/over-reconstruction 상황이므로 clone/split을 통해 정확한 위치에 Gaussian이 분포하도록 하자</p> </li> <li><code class="language-plaintext highlighter-rouge">Split</code> :<br/> <code class="language-plaintext highlighter-rouge">over-reconstruction</code>의 경우 3D Gaussians split <ul> <li>split : 1개의 Gaussian을 <code class="language-plaintext highlighter-rouge">2개로 분리</code>하고 각 scale을 줄인 후 <code class="language-plaintext highlighter-rouge">기존 3D Gaussian의 PDF</code>에 따라 sampling하여 배치<br/> Gaussians의 수는 증가하지만, total volume은 유지</li> <li>조건 1. <code class="language-plaintext highlighter-rouge">view-space positional gradient</code> \(\Delta_{p} L\)의 avg. magnitude \(\geq\) threshold \(\tau_{pos}\)</li> <li>조건 2. <code class="language-plaintext highlighter-rouge">covariance</code>가 큼</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Clone</code> :<br/> <code class="language-plaintext highlighter-rouge">under-reconstruction</code>의 경우 3D Gaussians clone <ul> <li>clone : <code class="language-plaintext highlighter-rouge">같은 크기로 copy</code> 후 <code class="language-plaintext highlighter-rouge">positional gradient 방향</code>에 배치<br/> total volume 및 Gaussians의 수 모두 증가</li> <li>조건 1. <code class="language-plaintext highlighter-rouge">view-space positional gradient</code> \(\Delta_{p} L\)의 avg. magnitude \(\geq\) threshold \(\tau_{pos}\)</li> <li>조건 2. <code class="language-plaintext highlighter-rouge">covariance</code>가 작음</li> </ul> </li> <li>3000 iter.마다 \(\alpha\) <code class="language-plaintext highlighter-rouge">알파 값을 주기적으로 0으로 초기화</code> 하면 전체 Gaussian 조절에 큰 도움이 됨! <ul> <li>효과 1. volumetric 기법의 특성상 <code class="language-plaintext highlighter-rouge">camera와 가까운 영역</code>에서 많은 <code class="language-plaintext highlighter-rouge">floater</code>들이 생겨서 Gaussian density가 증가하는데, 이를 제거해주는 역할<br/> floater 해결 관련 논문 : <d-cite key="floater1">[6]</d-cite> <d-cite key="floater2">[7]</d-cite> <d-cite key="floater3">[8]</d-cite></li> <li>효과 2. <code class="language-plaintext highlighter-rouge">큰 Gaussian들이 중첩</code>되어 있는 case를 제거해주는 역할</li> </ul> </li> </ul> <h2 id="results">Results</h2> <h3 id="implementation">Implementation</h3> <ul> <li> <p>custom CUDA kernel :<br/> tile-based rasterization을 위해<br/> custom CUDA kernel를 추가하여 사용 like <d-cite key="Point1">[3]</d-cite>, <d-cite key="Plenoxels">[1]</d-cite>, <d-cite key="superfast">[9]</d-cite></p> </li> <li> <p>Radix Sort :<br/> fast Radix Sort를 위해 NVIDIA CUB sorting routines <d-cite key="radixsort">[11]</d-cite> 사용</p> </li> <li> <p>interactive image viewer :<br/> open-source SIBR <a href="https://gitlab.inria.fr/sibr/sibr_core">SIBR</a> 이용해서<br/> interactive image-rendering viewer 만듬 (frame rate 측정에 사용)</p> </li> </ul> <h3 id="evaluation">Evaluation</h3> <ul> <li>Dataset :<br/> bounded indoor scenes와 unbounded outdoor scenes 전부 커버 <ul> <li>synthetic Blender dataset (Nerf) :<br/> have exhaustive set of bounded views with exact camera param.<br/> \(\rightarrow\) SOTA result even with 100K uniformly random initialization</li> <li>Mip-Nerf360 dataset</li> <li>Tanks&amp;Temples dataset</li> <li>Hedman et al. dataset</li> </ul> </li> <li>Metrics : <ul> <li>PSNR</li> <li>L-PIPS</li> <li>SSIM (D-SSIM)</li> </ul> </li> <li>Comparison : <ul> <li><code class="language-plaintext highlighter-rouge">Quality</code> : NeRF 계열 중 SOTA인 <code class="language-plaintext highlighter-rouge">Mip-Nerf360</code> <d-cite key="MipNeRF360">[10]</d-cite>과 비교 <ul> <li>끝까지 훈련시켰을 때 비슷한 quality 보이고,</li> <li>training speed는 35-45 min. versus 48 hours</li> </ul> </li> <li>Traning/Rendering <code class="language-plaintext highlighter-rouge">Speed</code> : NeRF 계열 중 SOTA인 <code class="language-plaintext highlighter-rouge">InstantNGP</code> <d-cite key="InstantNGP">[2]</d-cite>, <code class="language-plaintext highlighter-rouge">Plenoxels</code> <d-cite key="Plenoxels">[1]</d-cite> 과 비교 <ul> <li>speed SOTA인 <d-cite key="InstantNGP">[2]</d-cite> , <d-cite key="Plenoxels">[1]</d-cite> 과 비슷한 quality 가질 때까지 training 5-10 min.밖에 안 걸리고,</li> <li>훈련 더 하면 <d-cite key="InstantNGP">[2]</d-cite>, <d-cite key="Plenoxels">[1]</d-cite>보다 더 좋은 quality 가짐</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/19-480.webp 480w,/assets/img/2024-07-11-GS/19-800.webp 800w,/assets/img/2024-07-11-GS/19-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/19.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/17-480.webp 480w,/assets/img/2024-07-11-GS/17-800.webp 800w,/assets/img/2024-07-11-GS/17-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/17.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 7K iter.으로도 꽤 좋은 결과 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/18-480.webp 480w,/assets/img/2024-07-11-GS/18-800.webp 800w,/assets/img/2024-07-11-GS/18-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/18.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Comparison : <ul> <li>Compactness :<br/> anisotropic 3D Gaussians는<br/> scene representation 뿐만 아니라<br/> complex shape with a lower number of param.을 모델링하는 데도 쓰일 수 있음 <ul> <li>space carving으로 얻은 <d-cite key="Point3">[12]</d-cite> 의 initial point cloud에서 시작했을 때 <d-cite key="Point3">[12]</d-cite> 의 PSNR 값은 2-4 min.만에 넘겨버림</li> <li>또한, <d-cite key="Point3">[12]</d-cite> 의 point cloud의 4분의 1만큼만 써도 작은 model size로도 <d-cite key="Point3">[12]</d-cite> 의 PSNR 넘겨버림</li> </ul> </li> </ul> </li> </ul> <blockquote> <p>Space Carving :</p> <ul> <li>설명 : 여러 camera에 대해 voxel-space에서 object 있는 부분만 남기고 깎아내는 기법</li> <li>이유 : 3D reconstruction을 할 때 color 정보만으로 segmentation 가능할 정도로 background는 simple할수록 좋기 때문</li> <li>한계 : 빛, 그림자 같은 정보는 사용하지 않기 때문에 fg/bg 판단만 가능하다. 따라서 lidar처럼 camera에 depth-detection 메커니즘이 없을 경우 물체 내부의 구멍 같은 건 reconstruct 불가능</li> </ul> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/20-480.webp 480w,/assets/img/2024-07-11-GS/20-800.webp 800w,/assets/img/2024-07-11-GS/20-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/20.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="ablation-study">Ablation Study</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/21-480.webp 480w,/assets/img/2024-07-11-GS/21-800.webp 800w,/assets/img/2024-07-11-GS/21-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/21.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> PSNR score for Ablation Study </div> <ul> <li><code class="language-plaintext highlighter-rouge">Intialization (SfM)</code> : <ul> <li>uniformly sample a cube (random initialization w/o SfM points) :<br/> 주로 <code class="language-plaintext highlighter-rouge">background</code> 퀄리티 저하<br/> training view가 충분하지 않은 영역에서는 optimization으로 제거할 수 없는 <code class="language-plaintext highlighter-rouge">floater</code> 많이 발생<br/> \(\rightarrow\) synthetic NeRF dataset의 경우 bg가 없고 have exhaustive set of bounded views with exact input camera param. 이므로 random initializatino으로도 성능 굳</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/22-480.webp 480w,/assets/img/2024-07-11-GS/22-800.webp 800w,/assets/img/2024-07-11-GS/22-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/22.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Densification (clone, split)</code> : <ul> <li>Split : <code class="language-plaintext highlighter-rouge">background</code> reconstruction에 중요한 역할</li> <li>Clone : <code class="language-plaintext highlighter-rouge">thin</code> structure reconstruction에 중요한 역할</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/23-480.webp 480w,/assets/img/2024-07-11-GS/23-800.webp 800w,/assets/img/2024-07-11-GS/23-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/23.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Unlimited depth complexity of splats with gradients</code> : <ul> <li>Limited-BW :<br/> 각 tile의 Gaussian list에서 앞에서부터 N개까지만 gradient 전파할 경우<br/> Pulsar <d-cite key="Pulsar">[5]</d-cite>에서의 값의 2배인 N=10으로 했는데도 unstable optimization 초래</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/24-480.webp 480w,/assets/img/2024-07-11-GS/24-800.webp 800w,/assets/img/2024-07-11-GS/24-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/24.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> left: N=10 / right: N=inf </div> <ul> <li><code class="language-plaintext highlighter-rouge">Anisotropic Covariance</code> : <ul> <li>isotropic convariance :<br/> single scala value (radius of 3D Gaussian)를 optimize할 경우<br/> 같은 Gaussian 개수를 쓰더라도 <code class="language-plaintext highlighter-rouge">align with surfaces</code> 잘 하지 못해서 <code class="language-plaintext highlighter-rouge">fine</code> structure 잘 나타내지 못함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/25-480.webp 480w,/assets/img/2024-07-11-GS/25-800.webp 800w,/assets/img/2024-07-11-GS/25-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/25.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Spherical Harmonics</code> : <ul> <li>color 나타낼 때 <code class="language-plaintext highlighter-rouge">view-dependent</code> effect 담당</li> </ul> </li> </ul> <h2 id="discussion">Discussion</h2> <h3 id="limitations--future-work">Limitations &amp; Future Work</h3> <ul> <li><code class="language-plaintext highlighter-rouge">training view가 부족한 영역</code>에서는 여전히 <code class="language-plaintext highlighter-rouge">floater</code>, <code class="language-plaintext highlighter-rouge">elongated(길쭉한) artifacts</code>, <code class="language-plaintext highlighter-rouge">splotchy(얼룩진) Gaussians</code> 등 artifacts 발생 (Mip-NeRF360 등 prev. methods도 마찬가지)<br/> \(\rightarrow\) regularization으로 alleviate 가능</li> <li><code class="language-plaintext highlighter-rouge">view-dependent appearance</code>가 나타나는 영역에서는 large Gaussian 만들 때 <code class="language-plaintext highlighter-rouge">guard band</code> 등의 이유로 <code class="language-plaintext highlighter-rouge">popping</code> artifacts 발생<br/> \(\rightarrow\) better culling과 regularization으로 alleviate 가능</li> <li>Gaussians <code class="language-plaintext highlighter-rouge">depth-order</code> 갑자기 바뀔 수 있음<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">anti-aliasing</code>으로 해결 가능</li> <li>urban dataset처럼 very <code class="language-plaintext highlighter-rouge">large scene</code>에 대해서는 <code class="language-plaintext highlighter-rouge">position learning-rate</code>를 줄이는 게 도움됨</li> <li>prev. point-based methods에 비해서는 compact하긴 하지만, NeRF-based methods에 비해서는 memory consumption이 훨씬 큼<br/> e.g. large scene을 학습할 때 최대 GPU memory consumption은 20GB를 넘김<br/> \(\rightarrow\) InstantNGP에서처럼 optimization 과정을 low-level implementation 하면 괜찮<br/> e.g. scene을 rendering할 때도 model 저장하는 데 몇백MB, rasterizer 저장하는 데 30-500MB 필요<br/> \(\rightarrow\) memory consumption을 줄이기 위한 추후 개선 필요 (point-clouds compression technique <d-cite key="pointcompress">[13]</d-cite>을 적용해볼 수 있을 듯)</li> <li>3D Gaussians를 mesh reconstruction에 사용할 수 있는지 연구가 진행된다면 본 논문이 정확히 volumetric 과 surface representation 사이 어디에 위치해있는지를 이해할 수 있음</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/26-480.webp 480w,/assets/img/2024-07-11-GS/26-800.webp 800w,/assets/img/2024-07-11-GS/26-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/26.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> left(Mip-NeRF360): floaters and grainy(오돌토돌한, 거친) appearance / right(3DGS): low-detail bg from coarse Gaussians </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/27-480.webp 480w,/assets/img/2024-07-11-GS/27-800.webp 800w,/assets/img/2024-07-11-GS/27-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/27.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> training에서 많이 보지 못한 view의 경우 left(Mip-NeRF360), right(3DGS) 모두 artifacts 발생 </div> <h3 id="conclusion">Conclusion</h3> <ul> <li><code class="language-plaintext highlighter-rouge">3D Gaussian</code> :<br/> volumetric rendering의 특성을 살림과 동시에 fast splat-based rasterization 가능<br/> continuous representation이어야만 fast, high-quality radiance field training 가능하다는 기존 통념을 반전시킴</li> <li><code class="language-plaintext highlighter-rouge">CUDA</code> Implementation :<br/> training time의 80%는 Pytorch code (for 가독성)<br/> rasterization만 optimized CUDA kernels (for real-time)<br/> \(\rightarrow\) InstantNGP <d-cite key="InstantNGP">[2]</d-cite>처럼 optimization 나머지 부분도 전부 CUDA로 옮기면 훨씬 speedup 가능</li> <li><code class="language-plaintext highlighter-rouge">real-time rasterization by GPU</code> :<br/> rasterization이 main bottleneck인데<br/> GPU 힘으로 real-time rasterization pipeline 구현한 게<br/> 기존 volumetric ray-marching NeRF-based 기법보다 faster training, rendering 가능했던 비결</li> <li>Higher Quality than SOTA Mip-NeRF360(2022)</li> <li>Faster Training than SOTA InstantNGP(2022)</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="gaussian"/><category term="splatting"/><category term="rendering"/><category term="3d"/><category term="view"/><category term="synthesis"/><summary type="html"><![CDATA[3D GS for Real-Time Radiance Field Rendering]]></summary></entry><entry><title type="html">Pytorch Tensor</title><link href="https://semyeong-yu.github.io/blog/2024/TorchTensor/" rel="alternate" type="text/html" title="Pytorch Tensor"/><published>2024-07-09T15:00:00+00:00</published><updated>2024-07-09T15:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/TorchTensor</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/TorchTensor/"><![CDATA[<h2 id="pytorch-tensor-summary">Pytorch Tensor Summary</h2> <h3 id="python-문법">Python 문법</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/2-480.webp 480w,/assets/img/2024-07-09-TorchTensor/2-800.webp 800w,/assets/img/2024-07-09-TorchTensor/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="create-and-access-tensor">Create and Access Tensor</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/3-480.webp 480w,/assets/img/2024-07-09-TorchTensor/3-800.webp 800w,/assets/img/2024-07-09-TorchTensor/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/4-480.webp 480w,/assets/img/2024-07-09-TorchTensor/4-800.webp 800w,/assets/img/2024-07-09-TorchTensor/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="tensor-slice-indexing">Tensor slice indexing</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/5-480.webp 480w,/assets/img/2024-07-09-TorchTensor/5-800.webp 800w,/assets/img/2024-07-09-TorchTensor/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="tensor-listtensor--boolean-indexing">Tensor list(tensor) &amp; boolean indexing</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/6-480.webp 480w,/assets/img/2024-07-09-TorchTensor/6-800.webp 800w,/assets/img/2024-07-09-TorchTensor/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/7-480.webp 480w,/assets/img/2024-07-09-TorchTensor/7-800.webp 800w,/assets/img/2024-07-09-TorchTensor/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="gather-scatter_add-indexing">gather, scatter_add indexing</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/8-480.webp 480w,/assets/img/2024-07-09-TorchTensor/8-800.webp 800w,/assets/img/2024-07-09-TorchTensor/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="where-condition">where condition</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/9-480.webp 480w,/assets/img/2024-07-09-TorchTensor/9-800.webp 800w,/assets/img/2024-07-09-TorchTensor/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="timing">Timing</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/10-480.webp 480w,/assets/img/2024-07-09-TorchTensor/10-800.webp 800w,/assets/img/2024-07-09-TorchTensor/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="reshape-permute">Reshape, Permute</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/11-480.webp 480w,/assets/img/2024-07-09-TorchTensor/11-800.webp 800w,/assets/img/2024-07-09-TorchTensor/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="element-wise-reduction-operation-concatenation">element-wise, reduction operation, concatenation</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/12-480.webp 480w,/assets/img/2024-07-09-TorchTensor/12-800.webp 800w,/assets/img/2024-07-09-TorchTensor/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="matrix-operation">Matrix operation</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/13-480.webp 480w,/assets/img/2024-07-09-TorchTensor/13-800.webp 800w,/assets/img/2024-07-09-TorchTensor/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/14-480.webp 480w,/assets/img/2024-07-09-TorchTensor/14-800.webp 800w,/assets/img/2024-07-09-TorchTensor/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="broadcasting">Broadcasting</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/15-480.webp 480w,/assets/img/2024-07-09-TorchTensor/15-800.webp 800w,/assets/img/2024-07-09-TorchTensor/15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="in-place-gpu">In-place, GPU</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/16-480.webp 480w,/assets/img/2024-07-09-TorchTensor/16-800.webp 800w,/assets/img/2024-07-09-TorchTensor/16-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="cv-tasks"/><category term="pytorch"/><summary type="html"><![CDATA[import torch]]></summary></entry><entry><title type="html">Pytorch Basic Code (DDP)</title><link href="https://semyeong-yu.github.io/blog/2024/PytorchBasic/" rel="alternate" type="text/html" title="Pytorch Basic Code (DDP)"/><published>2024-07-08T11:00:00+00:00</published><updated>2024-07-08T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/PytorchBasic</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/PytorchBasic/"><![CDATA[<h2 id="pytorch-basic-code-distributeddataparallel-ver">Pytorch Basic Code (DistributedDataParallel ver.)</h2> <h3 id="deal-with-json-image-csv">Deal with json, image, csv</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/3-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/3-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">import os
import json
import pandas as pd
import csv
from PIL import Image
import cv2

# read json
if os.path.exists(json_path):
    with open(json_path, "r") as f:
        data = json.load(f)
    f.close()

# read image 방법 1.
img = Image.open(image_path).convert('RGB') # PIL image object in range [0, 255]
img.show()

# read image 방법 2.
img = cv2.imread(image_path) # np.ndarray of shape (H, W, C) in range [0, 255] in BGR mode
cv2.imshow('Image', img)
cv2.waitKey(0)
cv2.destroyAllWindows()

# read csv 방법 1. general case
data = pd.read_csv(csv_path, sep="|", index_col=0, skiprows=[1], na_values=['?', 'nan']).values # 0-th column (1-th row는 제외) ('?'와 'nan'은 결측값으로 인식)

# read csv 방법 2. special case : csv가 row별로 dictionary 형태일 때
if os.path.exists(csv_path):
    with open(csv_path, "r") as f:
        reader = csv.DictReader(f, delimiter=",")
        data = [{key : value for key, value in row.items()} for row in reader] # row별로 읽음

# write to csv
dataset = [] # list of dictionaries
dataset.append({"id":id, "w":w, "h":h, "class":i})
pd.DataFrame(dataset).to_csv(output_path, index=False) # output_path : ".../dataset.csv"
</code></pre> <h3 id="convert-to-tensor">Convert to Tensor</h3> <p>data.py의 CustomDataset(torch.utils.data.Dataset)에서 image는 <code class="language-plaintext highlighter-rouge">shape (C, H, W) tensor</code>여야 하기 때문에<br/> PIL.Image.open() 또는 cv2.imread()로 얻은<br/> PIL image object 또는 np.ndarray를 적절한 shape 및 range의 tensor로 변환해주어야 한다</p> <ul> <li>PIL image object \(\rightarrow\) Tensor <ul> <li>torchvision.transforms.ToTensor()</li> <li>np.array(), torch.tensor()</li> <li>getdata(), torch.tensor()</li> </ul> </li> <li>np.ndarray \(\rightarrow\) Tensor <ul> <li>torch.tensor()</li> </ul> </li> </ul> <pre><code class="language-Python">PIL_img = Image.open(image_path).convert('RGB') # PIL image object of size (W, H) in range [0, 255]

# 방법 1. torchvision.transforms.ToTensor()
transform = torchvision.transforms.Compose([
    torchvision.transforms.Resize((256, 256)),
    torchvision.transforms.ToTensor() # convert to tensor in range [0., 1.]
])
img = transform(PIL_img) # tensor of shape (C, H, W) in range [0., 1.]

# 방법 2. np.array(), torch.tensor()
img = np.array(PIL_img) # np.ndarray of shape (H, W, C) in range [0., 255.]
img = torch.tensor(img.transpose((2, 0, 1)).astype(float)).mul_(1.0) / 255.0 # tensor of shape (C, H, W) in range [0., 1.]

# 방법 3. getdata(), torch.tensor()
img_data = PIL_img.getdata()
img = torch.tensor(img_data, dtype=torch.float32) # tensor of shape (H*W*C,) in range [0, 255]
img = img.view(PIL_img.size[1], PIL_img.size[0], 3).permute(2, 0, 1) / 255.0 # tensor of shape (C, H, W) in range [0., 1.]
</code></pre> <pre><code class="language-Python">img = cv2.imread(image_path) # np.ndarray of shape (H, W, C) in range [0., 255.]

img = torch.tensor(img.transpose((2, 0, 1)).astype(float)).mul_(1.0) / 255.0 # tensor of shape (C, H, W) in range [0., 1.]
</code></pre> <h3 id="create-dataset">Create Dataset</h3> <ul> <li><code class="language-plaintext highlighter-rouge">data augmentation</code> : <ul> <li>Resize</li> <li>ToTensor</li> <li>RandomHorizontalFlip</li> <li>RandomVerticalFlip</li> <li>Normalize</li> <li>RandomRotation</li> <li>RandomAffine <ul> <li>shear</li> <li>scale (zoom-in/out)</li> </ul> </li> <li>RandomResizedCrop</li> <li>ColorJitter</li> <li>GaussianBlur</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/4-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/4-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">import os
import torch
from torch.utils.data import Dataset
import cv2
import numpy as np
import glob
import random

# Create Dataset
class CustomDataset(Dataset):
    def __init__(self, args, mode):
        # lazy-loading :
        # load할 data가 너무 크다면 __init__()에서는 load할 파일명만 저장해놓고 __getitem__()에서 필요할 때마다 load
        self.args = args
        self.mode = mode
        
        if mode == 'train':
            self.data_path = os.path.join(args.data_path, 'train_blur')
        elif mode == 'val':
            self.data_path = os.path.join(args.data_path, 'val_blur')
        elif mode == 'test':
            self.data_path = os.path.join(args.data_path, 'test_blur')
        
        # a list of data/train_blur/*.png
        self.blur_path_list = sorted(glob.glob(os.path.join(self.data_path, '*.png')))
        
        # a list of data/train_sharp/*.png
        self.sharp_path_list = [os.path.normpath(path.replace('blur', 'sharp') for path in self.blur_path_list)]

    def __getitem__(self, idx):
        # should return float tensor!!
        blur_path = self.blur_path_list[idx]
        # np.ndarray of shape (H, W, C) in range [0, 255]
        blur_img = cv2.imread(blur_path) 

        if self.mode == 'train':
            sharp_path = self.sharp_path_list[idx]
            sharp_img = cv2.imread(sharp_path)
            
            # np.ndarray of shape (pat, pat, C) where pat is patch_size
            blur_img, sharp_img = self.augment(self.get_random_patch(blur_img, sharp_img)) 
            
            # tensor of shape (C, pat, pat) in range [0, 1]
            return self.np2tensor(blur_img), self.np2tensor(sharp_img) 
        
        elif self.mode == 'val':
            sharp_path = self.sharp_path_list[idx]
            sharp_img = cv2.imread(sharp_path)
            return self.np2tensor(blur_img), self.np2tensor(sharp_img)
        
        elif self.mode == 'test':
            return self.np2tensor(blur_img), blur_path

    def np2tensor(self, x):
        # input : shape (H, W, C) / range [0, 255]
        # output : shape (C, H, W) / range [0, 1]
        ts = (2, 0, 1)
        x = torch.tensor(x.transpose(ts).astype(float)).mul_(1.0) # _ : in-place
        x = x / 255.0 # normalize
        return x

    def get_random_patch(self, blur_img, sharp_img):
        H, W, C = blur_img.shape # shape (H, W, C)

        pat = self.args.patch_size # pat : patch size
        iw = random.randrange(0, W - pat + 1) # iw : range [0, W - pat]
        ih = random.randrange(0, H - pat + 1) # ih : range [0, H - pat]

        blur_img = blur_img[ih:ih + pat, iw:iw + pat, :] # shape (pat, pat, C)
        sharp_img = sharp_img[ih:ih + pat, iw:iw + pat, :]

        return blur_img, sharp_img # shape (pat, pat, C)

    def augment(self, blur_img, sharp_img):
        # random horizontal flip
        if random.random() &lt; 0.5:
            blur_img = blur_img[:, ::-1, :] # Width-axis를 flip
            sharp_img = sharp_img[:, ::-1, :]
            '''
            flow-mask pair의 경우 C-dim.이 3 = 2(optical flow x, y) + 1(occlusion mask) 이므로
            shape (T, H, W, 3)의 flow-mask pair를 horizontal flip을 하려면
            flow = flow[:, :, ::-1, :]
            flow[:, :, :, 0] *= -1
            '''
            
        # random vertical flip
        if random.random() &lt; 0.5:
            blur_img = blur_img[::-1, :, :] # Height-axis를 flip
            sharp_img = sharp_img[::-1, :, :]
            '''
            flow = flow[:, ::-1, :, :]
            flow[:, :, :, 1] *= -1
            '''

        return blur_img, sharp_img

    def __len__(self):
        return len(self.path_list)
</code></pre> <h3 id="dataloader">DataLoader</h3> <ul> <li>DataParallel(DP) vs DistributedDataParallel(DDP) :<br/> <a href="https://tkayyoo.tistory.com/27#tktag2">Pytorch DP and DDP</a> <ul> <li>DataParallel(DP) :<br/> single-process<br/> multi-thread<br/> single-machine</li> <li>DistributedDataParallel(DDP) :<br/> multi-process<br/> single-machine과 multi-machine 모두 가능</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">rank</code> : <ul> <li>전체 distributed system에서 process 순서</li> <li>4-CPU system이 2개 있을 경우<br/> rank = machine 번호(0~1) * machine 당 process 개수(4) + process 번호(0~3)</li> <li>rank = 0인 process에 대해서만 wandb로 train log 출력</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">world_size</code> : <ul> <li>전체 distributed system에서 총 process 개수</li> <li>4-GPU system이 2개 있을 경우<br/> world_size = machine 개수(2) * machine 당 process 개수(4)</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">torch.distributed.init_process_group()</code> : <ul> <li>분산 학습 환경 초기화 : 분산 학습하는 각 process 간의 통신을 설정</li> <li>backend : ‘gloo’ for CPU, ‘nccl’ for GPU, ‘mpi’ for 고성능</li> <li>init_method : 각 process가 서로 탐색하는 방법(url)<br/> 예시 : ‘env://’, f’tcp://127.0.0.1:11203’</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">torch.utils.data.distributed.DistributedSampler()</code> : <ul> <li>world_size(총 process 개수)만큼 dataset을 분할하여 모든 process가 동일한 양의 dataset을 갖도록 함</li> <li>DistributedSampler는 각 epoch마다 dataset을 무작위로 분할</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">torch.utils.data.DataLoader()</code> : <ul> <li>shuffle=False :<br/> 보통 training일 때는 일반화를 위해 shuffle=True로 두지만<br/> 분산 학습을 할 때는 같은 epoch 내에서<br/> 각 process가 서로 다른 dataset을 처리하기 위해 (중복 방지)<br/> shuffle=False로 설정</li> <li>num_workers : cpu data load할 때 multi-processing core 개수</li> <li>pin_memory=True :<br/> data load한 장치(CPU)에서 GPU로 data를 옮길 때<br/> host memory가 아닌 CPU의 page-locked memory로 할당하고<br/> GPU는 이를 참조하여 복사하므로 전송 시간을 단축<br/> pin_memory=True와 non_blocking=True는 함께 사용</li> <li>drop_last=True : 나눠떨어지지 않는 마지막 batch를 버림</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">_collate_fn(input)</code> : <ul> <li>DataLoader()에서 1개의 batch로 묶을 때 사용하는 custom 전처리 함수</li> <li>input : <ul> <li>1개의 batch에 해당하는 입력</li> <li>Dataset(torch.utils.data.Dataset)의 <strong>getitem</strong>(self, idx)이 return img, target 형태일 때<br/> [(img1, target1), (img2, target2), …]의 형태</li> </ul> </li> <li>output : <ul> <li>for iter, (x, y) in enumerate(dataloader): 의 x, y</li> </ul> </li> <li>예 : 길이가 다른 input들을 batch로 묶기 위해 padding, tokenization<br/> img의 경우 CustomDataset()에서 augmentation으로 shape (C, H, W)로 통일해줬다면 (N, C, H, W)로 묶을 수 있지만,<br/> object detection task에서 target의 경우 n_box가 image마다 다르므로 N batch로 묶기 위해 padding해주어야 함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/5-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/5-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
import torch.distributed as dist
from datetime import timedelta

def _collate_fn(samples):
    # ...

# main_worker : 각 process가 실행하는 함수
def main_worker(process_id, args):

    rank = args.machine_id * args.num_processes + process_id
    
    world_size = args.num_machines * args.num_processes
    
    dist.init_process_group(backend='nccl', init_method='env://', world_size=world_size, rank=rank, timeout=timedelta(300))
    
    ###################################################################
    
    # for epoch in range ... 밖에서
    train_dataset = CustomDataset(args, 'train')
    '''
    train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transforms.Compose(
        [
        transforms.ToTensor(), 
        transforms.Normalize((0.1302,), (0.3069,))
        ]))  
    '''

    # machine 당 process 수로 나눔
    batch_size = int(args.batch_size / args.num_processes) 
    num_workers = int(args.num_workers / args.num_processes)

    # for epoch in range ... 안에서
    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)

    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=_collate_fn, pin_memory=True, drop_last=True, sampler=train_sampler)
</code></pre> <h3 id="train">Train</h3> <ul> <li><code class="language-plaintext highlighter-rouge">torch.multiprocessing.spawn()</code> : <ul> <li>main_worker : 각 process가 실행하는 함수</li> <li>nprocs : machine 당 process 개수인 4로 설정<br/> main_worker()의 첫 번째 argument는 process_id인 0~3이 됨</li> <li>args : main_worker()에 추가로 전달할 tuple 형태의 argument</li> <li>join</li> <li>daemon</li> <li>start_method</li> </ul> </li> </ul> <ol> <li><code class="language-plaintext highlighter-rouge">model</code> initialize, and set cuda, and parallelize <ul> <li>nn.parallel.DistributedDataParallel :<br/> 각 model 복사본은 각자의 optimizer를 이용해 gradient를 구하고<br/> rank=0의 process와 통신하여 gradient의 평균을 구해서 backpropagation 진행<br/> GIL(global interpreter lock)의 제약을 해결</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">wandb</code> init <ul> <li>wandb.init() : wandb 초기화<br/> vars(args)는 args 객체의 <strong>dict</strong> 속성을 반환<br/> {‘transforms’ : ‘BaseTransform’, ‘crop_size’ : 224}과 같이 반환</li> <li>wandb.watch() : wandb 기록<br/> 모든 param.의 gradient를 기록<br/> arg.log_interval-번째 batch마다 log 기록</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">optimizer, scheduler</code> initialize</li> <li>load <code class="language-plaintext highlighter-rouge">checkpoint</code></li> <li><code class="language-plaintext highlighter-rouge">train</code> with <code class="language-plaintext highlighter-rouge">barrier</code> <ul> <li>torch.distributed.barrier() :<br/> 분산 학습 환경에서<br/> 모든 process가 이 장벽에 도달할 때까지 대기하여<br/> 모든 process가 synchronize된 상태에서 훈련이 진행되도록 함</li> <li>torch.cuda.empty_cache() :<br/> 더 이상 사용하지 않는 tensor들을 GPU cached memory에서 해제<br/> 장점 : GPU memory 확보<br/> 단점 : 너무 자주 호출하면 메모리 할당/해제에 따른 성능 저하 발생</li> <li>train :<br/> args.accumulation_steps만큼 loss를 누적한 뒤 backward<br/> args.accumulation_steps마다 gradient 및 measurement 초기화, rank=0 logging</li> <li>validation :<br/> with torch.no_grad(): 로 gradient 누적 안 함!</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">wandb</code> and <code class="language-plaintext highlighter-rouge">distributed</code> finish</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/6-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/6-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">from importlib import import_module
import torch
import torch.nn as nn
import torch.multiprocessing as mp
from utils import *
from tqdm import tqdm
import wandb

def main():
    args = arg_parse()
    fix_seed(args.random_seed)
    
    # rank=0인 process를 실행하는 system의 IP 주소
    # rank=0인 system이 모든 backend 통신을 설정!
    os.environ['MASTER_ADDR'] = '127.0.0.1'
    # 해당 system에서 사용 가능한 PORT           
    os.environ['MASTER_PORT'] = '8892'

    mp.spawn(main_worker, nprocs=args.num_processes, args=(args,))
    # DDP가 아니라면, main.py에 def main_worker()의 내용을 넣고, train.py에 class Runner 만들자
    '''
    class Runner:
        def __init__(self, args, model):
            self.args = args
            self.model = model
            pass
        def train(self, dataloader, epoch):
            pass
        def validate(self, dataloader, epoch):
            pass
        def test(self, dataloader):
            pass
    '''

def main_worker(process_id, args):
    global best_acc
    best_acc = 0.0

    # 1. model initialize, and set cuda, and parallelize
    model = MyFMANet()

    torch.cuda.set_device(process_id)
    model.cuda(process_id)
    criterion = nn.NLLLoss().cuda(process_id) # criterion = nn.CrossEntropyLoss(reduction='mean').cuda(process_id)

    model = nn.parallel.DistributedDataParallel(model, device_ids=[process_id])

    # 2. wandb init
    if rank == 0:
        wandb.init(project=args.prj_name, name=f"{args.exp_name}", entity="semyeongyu", config=vars(args))
        wandb.watch(model, log='all', log_freq=args.log_interval)

    # 3. optimizer, scheduler initialize
    optimizer = getattr(import_module("torch.optim"), args.optimizer)(model.parameters(), lr=args.lr, betas=(args.b1, args.b2), weight_decay=args.weight_decay)
    scheduler = getattr(import_module("torch.optim.lr_scheduler"), args.lr_scheduler)(optimizer, T_max=args.period, eta_min=0, last_epoch=-1, verbose=True)
    # T_max : 주기 1번 도는 데 걸리는 최대 iter. 수 / eta_min : lr의 최솟값 / last_epoch : 학습 시작할 때의 epoch

    # 4. load checkpoint
    if args.resume_from:
        start_epoch, model, optimizer, scheduler = load_checkpoint(args.checkpoint_path, model, optimizer, scheduler, rank)
    else:
        start_epoch = 0

    # 5. train with barrier
    dist.barrier()

    for epoch in range(start_epoch, args.n_epochs):
        train_sampler.set_epoch(epoch) # train_sampler가 epoch끼리 동일하게 data 분할하는 것을 방지하기 위해

        optimizer.zero_grad() # epoch마다 gradient 초기화

        train_loss = train(train_loader, model, criterion, optimizer, scheduler, epoch, args)

        dist.barrier()

        if rank == 0:
            val_acc, val_loss = validate(val_loader, model, criterion, epoch, args)
            
            # best acc일 때마다 save checkpoint
            if (best_top1 &lt; val_acc):
                best_top1 = val_acc # best_top1은 global var.
                save_checkpoint(
                    {
                        'epoch': epoch,
                        'model': model.state_dict(),
                        'best_top1': best_top1,
                        'optimizer': optimizer.state_dict(),
                        'scheduler': scheduler.state_dict()
                    }, os.path.join(args.checkpoint_dir, args.exp_name), f"{epoch}_{round(best_top1, 4)}.pt"
                )
        
        torch.cuda.empty_cache() 
    
    # 6. wandb and distributed finish
    if rank == 0:
        wandb.run.finish()

    dist.destroy_process_group()
</code></pre> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/7-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/7-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">def train(train_loader, model, criterion, optimizer, scheduler, epoch, args):
    model.train()
    train_acc, train_loss = AverageMeter(), AverageMeter() 
    # measurement of acc and loss

    pbar = tqdm(enumerate(train_loader), total=len(train_loader))
    for step, (x, y_gt) in pbar:
        x, y_gt = x.cuda(non_blocking=True), y_gt.cuda(non_blocking=True) 
        # cuda device에 올려야 함
        # pin_memory=True와 non_blocking=True는 함께 사용

        # forward
        y_pred = model(x)

        # loss divided by accumulation_steps
        loss = criterion(y_pred, y_gt) / args.accumulation_steps

        # gradient 누적
        loss.backward()
        
        # measurement
        train_acc.update(
            topk_accuracy(y_pred.clone().detach(), y_gt).item(), x.size(0))
        train_loss.update(loss.item() * args.accumulation_steps, x.size(0))

        # args.accumulation_steps만큼 loss를 누적한 뒤 평균값으로 backward
        if (step+1) % args.accumulation_steps == 0:
            # gradient clipping
            nn.utils.clip_grad_norm_(model.parameters(), max_norm=args.max_norm)

            # backward
            optimizer.step()
            scheduler.step()

            # gradient 초기화
            optimizer.zero_grad()

            # logging
            dist.barrier()
            if rank == 0:
                # wandb log
                wandb.log(
                    {
                        "Training Loss": round(train_loss.avg, 4),
                        "Training Accuracy": round(train_acc.avg, 4),
                        "Learning Rate": optimizer.param_groups[0]['lr']
                    }
                )

                # tqdm log
                description = f'Epoch: {epoch+1}/{args.n_epochs} || Step: {(step+1)//args.accumulation_steps}/{len(train_loader)//args.accumulation_steps} || Training Loss: {round(train_loss.avg, 4)}'
                pbar.set_description(description)

                # measurement 초기화
                train_loss.init()
                train_acc.init()
    
    return train_loss.avg
</code></pre> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/8-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/8-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">def validate(val_loader, model, criterion, epoch, args):
    model.eval()
    val_acc, val_loss = AverageMeter(), AverageMeter() # measurement of acc and loss

    pbar = tqdm(enumerate(val_loader), total=len(val_loader))
    with torch.no_grad(): # validation은 gradient 누적 안 함!!
        for step, (x, y_gt) in pbar:
            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)
            
            # forward
            y_pred = model(x)
            loss = criterion(y_pred, y_gt)
            
            # measurement
            val_acc.update(topk_accuracy(y_pred.clone().detach(), y_gt).item(), x.size(0)) 
            val_loss.update(loss.item(), x.size(0))

            # tqdm log
            description = f'Epoch: {epoch+1}/{args.n_epochs} || Step: {step+1}/{len(val_loader)} || Validation Loss: {round(loss.item(), 4)} || Validation Accuracy: {round(val_acc.avg, 4)}'
            pbar.set_description(description)

    # wandb log
    wandb.log(
        {
            'Validation Loss': round(val_loss.avg, 4),
            'Validation Accuracy': round(val_acc.avg, 4)
        }
    )

    return val_acc.avg, val_loss.avg
</code></pre> <h3 id="utils">Utils</h3> <ul> <li><code class="language-plaintext highlighter-rouge">argument parser</code></li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/9-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/9-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">import argparse

def arg_parse():
    parser = argparse.ArgumentParser()

    parser.add_argument("--transforms", type=str, default="BaseTransform")
    parser.add_argument("--crop_size", type=int, default=224)

    args = parser.parse_args()

    return args
</code></pre> <ul> <li><code class="language-plaintext highlighter-rouge">seed</code></li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/10-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/10-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">import random
import torch
import numpy as np

def fix_seed(random_seed):
    torch.manual_seed(random_seed)
    torch.cuda.manual_seed(random_seed)
    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(random_seed)
    random.seed(random_seed)
</code></pre> <ul> <li><code class="language-plaintext highlighter-rouge">checkpoint</code> : dictionary of elements below <ul> <li>epoch</li> <li>model.state_dict()</li> <li>best_acc</li> <li>optimizer.state_dict()</li> <li>scheduler.state_dict()</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/11-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/11-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">def save_checkpoint(checkpoint, saved_dir, file_name):
    os.makedirs(saved_dir, exist_ok=True)
    output_path = os.path.join(saved_dir, file_name)
    torch.save(checkpoint, output_path) 
    # checkpoint : dictionary

def load_checkpoint(checkpoint_path, model, optimizer, scheduler, rank=-1):
    # checkpoint_path : ".../240325.pt"
    if rank != -1: # 분산학습 yes
        map_location = {"cuda:%d" % 0 : "cuda:%d" % rank}
        checkpoint = torch.load(checkpoint_path, map_location=map_location)
    else: # 분산학습 no
        checkpoint = torch.load(checkpoint_path)

    start_epoch = checkpoint['epoch']

    model.load_state_dict(checkpoint['model'])
    
    if optimizer is not None:
        optimizer.load_state_dict(checkpoint['optimizer'])
    
    if scheduler is not None:
        scheduler.load_state_dict(checkpoint['scheduler'])

    return start_epoch, model, optimizer, scheduler
</code></pre> <ul> <li><code class="language-plaintext highlighter-rouge">augmentation</code></li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/12-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/12-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">import albumentations as A
from albumentations.pytorch.transforms import ToTensorV2

class BaseTransform(object):
    def __init__(self, crop_size = 224):
        self.transform = A.Compose(
            [   
                A.RandomResizedCrop(crop_size, crop_size),
                A.HorizontalFlip(),
                A.Normalize(),
                ToTensorV2() 
# albumentations에서는 normalize 이후에 ToTensorV2를 사용해줘야 함 (여기서 어차피 shape (C,H,W)로 변경)
            ]
        )

    def __call__(self, img):
# BaseTransform()은 nn.Module을 상속한 게 아니므로 forward를 구현해도 __call__과 연결되어 있지 않음
# 따라서 __call__()을 직접 구현해줘야 함
        return self.transform(image=img)
</code></pre> <ul> <li><code class="language-plaintext highlighter-rouge">measurement</code></li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/13-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/13-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">class AverageMeter(object):
    def __init__(self):
        self.init()
    
    def init(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val*n
        self.count += n
        self.avg = self.sum / self.count

def topk_accuracy(pred, gt, k=1):
    # pred : shape (N, class)
    # gt : shape (N,)
    _, pred_topk = pred.topk(k, dim=1)
    n_correct = torch.sum(pred_topk.squeeze() == gt)

    return n_correct / len(gt)
</code></pre> <h3 id="multi-attention">Multi-Attention</h3> <ul> <li>FMA-Net (2024) <d-cite key="FMANet">[1]</d-cite>의 Multi-Attention 구현<br/> 출처 : <a href="https://github.com/KAIST-VICLab/FMA-Net">FMA-Net Code</a></li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/2-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/2-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Multi-Attention :</p> <ul> <li><code class="language-plaintext highlighter-rouge">CO(center-oriented)</code> attention :<br/> better align \(\tilde F_{w}^{i}\) to \(F_{c}^{0}\) (center feature map of initial temporally-anchored feature)</li> <li><code class="language-plaintext highlighter-rouge">DA(degradation-aware)</code> attention :<br/> \(\tilde F_{w}^{i}\) becomes globally adaptive to spatio-temporally variant degradation by using degradation kernels \(K^{D}\)</li> </ul> </blockquote> <ul> <li> <p>CO attention :<br/> \(Q=W_{q} F_{c}^{0}\)<br/> \(K=W_{k} \tilde F_{w}^{i}\)<br/> \(V=W_{v} \tilde F_{w}^{i}\)<br/> \(COAttn(Q, K, V) = softmax(\frac{QK^{T}}{\sqrt{d}})V\)<br/> 실험 결과, \(\tilde F_{w}^{i}\)가 자기 자신(self-attention)이 아니라 \(F_{c}^{0}\)과의 relation에 집중할 때 better performance</p> </li> <li> <p>DA attention :<br/> CO attention과 비슷하지만,<br/> Query 만들 때 \(F_{c}^{0}\) 대신 \(k^{D, i}\) 사용<br/> \(\tilde F_{w}^{i}\) becomes globally adaptive to spatio-temporally-variant degradation<br/> \(k^{D, i} \in R^{H \times W \times C}\) : degradation features adjusted by conv. with \(K^{D}\) (motion-aware spatio-temporally-variant degradation kernels) 에 대해<br/> \(Q=W_{q} k^{D, i}\)<br/> DA attention은 \(Net^{D}\) 말고 \(Net^{R}\) 에서만 사용</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">nn.Conv2d()</code> :</p> <ul> <li> \[H_{out} = \lfloor 1 + \frac{H_{in} + 2 \times pad - dilation \times (K-1) - 1}{stride} \rfloor\] </li> <li>argument : <ul> <li>groups :<br/> shape (\(C_{in}\), \(C_{out}\), K, K) 대신 \(C_{in}\), \(C_{out}\) 을 groups-개로 쪼개서<br/> shape (\(\frac{C_{in}}{groups}\), \(\frac{C_{out}}{groups}\), K, K)를 groups-번 실행하여 concat</li> </ul> </li> <li>variable : <ul> <li>weight : shape (\(C_{out}\), \(\frac{C_{in}}{groups}\), K, K)</li> <li>bias : shape (\(C_{out}\),)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/14-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/14-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">import torch
import torch.nn as nn

class Attention(nn.Module):
    # Restormer (CVPR 2022) transposed-attention block
    # original source code: https://github.com/swz30/Restormer
    def __init__(self, dim, n_head, bias):
        super(Attention, self).__init__()
        self.n_head = n_head # multi-head for channel dim.
        self.temperature = nn.Parameter(torch.ones(n_head, 1, 1)) 
        # multi-head 별로 scale factor를 parameterize

        # W_q
        self.q = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)
        self.q_dwconv = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, groups=dim, bias=bias)

        # W_kv
        self.kv_conv = nn.Conv2d(dim, dim*2, kernel_size=1, bias=bias)
        self.kv_dwconv = nn.Conv2d(dim*2, dim*2, kernel_size=3, stride=1, padding=1, groups=dim*2, bias=bias)

        # W_o
        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)

    def forward(self, x, f):
        # first input x : shape (N, C, H, W) -&gt; makes key and value
        # second input f : shape (N, C, H, W) -&gt; makes query
        N, C, H, W = x.shape

        # Apply W_q and W_kv
        q = self.q_dwconv(self.q(f)) # query q : shape (N, C, H, W)
        kv = self.kv_dwconv(self.kv_conv(x)) # kv : shape (N, 2*C, H, W)
        k, v = kv.chunk(2, dim=1) # key k and value v : shape (N, C, H, W)

        # Multi-Head Attention
        q = einops.rearrange(q, 'b (head c) h w -&gt; b head c (h w)', head=self.n_head)
        # query q : shape (N, C, H, W) -&gt; shape (N, M, C/M, H * W)
        k = einops.rearrange(k, 'b (head c) h w -&gt; b head c (h w)', head=self.n_head)
        # key k : shape (N, C, H, W) -&gt; shape (N, M, C/M, H * W)
        v = einops.rearrange(v, 'b (head c) h w -&gt; b head c (h w)', head=self.n_head)
        # value v : shape (N, C, H, W) -&gt; shape (N, M, C/M, H * W)

        # matrix mul.을 할 spatial dim.을 normalize
        q = torch.nn.functional.normalize(q, dim=-1)
        k = torch.nn.functional.normalize(k, dim=-1)

        '''
        - q @ k.transpose(-2, -1) = similarity :
          shape (N, M, C/M, C/M)
        - self.temperature = scale factor for each head :
          shape (M, 1, 1) -&gt; shape (N, M, C/M, C/M) 
        '''
        attn = (q @ k.transpose(-2, -1)) * self.temperature 
        attn = attn.softmax(dim=-1) # convert to probability distribution

        out = (attn @ v) # shape (N, M, C/M, H*W)
        
        # Multi-Head Attention - concatenation
        out = einops.rearrange(out, 'b head c (h w) -&gt; b (head c) h w', head=self.n_head, h=H, w=W) 
        # shape (N, C, H, W)

        # Apply W_o
        out = self.project_out(out) # shape (N, C, H, W)

        return out

class LayerNorm(nn.Module):
    def __init__(self, normalized_shape):
        super(LayerNorm, self).__init__()
        
        # learnable param.
        self.weight = nn.Parameter(torch.ones(normalized_shape)) # shape (C,)
        self.bias = nn.Parameter(torch.zeros(normalized_shape)) # shape (C,)
    
    def forward(self, x):
        # x : shape (N, C, H, W)
        # LayerNorm : dim. C에 대해 normalize
        mu = x.mean(1, keepdim=True)
        sigma = x.var(1, keepdim=True, unbiased=False)
        return (x - mu) / torch.sqrt(sigma + 1e-5) * self.weight + self.bias

class MultiAttentionBlock(nn.Module):
    def __init__(self, dim, n_head, ffn_expansion_factor, bias, is_DA):
        super(MultiAttentionBlock, self).__init__()
        self.norm1 = LayerNorm(dim)
        # center-oriented attention
        self.co_attn = Attention(dim, n_head, bias) 
        self.norm2 = LayerNorm(dim)
        self.ffn1 = FeedForward(dim, bias)

        if is_DA:
            self.norm3 = LayerNorm(dim)
            # degradation-aware attention
            self.da_attn = Attention(dim, n_head, bias) 
            self.norm4 = LayerNorm(dim)
            self.ffn2 = FeedForward(dim, bias)

    def forward(self, Fw, F0_c, Kd):
        Fw = Fw + self.co_attn(self.norm1(Fw), F0_c)
        Fw = Fw + self.ffn1(self.norm2(Fw))

        if Kd is not None:
            Fw = Fw + self.da_attn(self.norm3(Fw), Kd)
            Fw = Fw + self.ffn2(self.norm4(Fw))

        return Fw
</code></pre>]]></content><author><name></name></author><category term="cv-tasks"/><category term="pytorch"/><summary type="html"><![CDATA[Dataset, DataLoader, Train, Attention, ...]]></summary></entry><entry><title type="html">Solar LLM with Langchain</title><link href="https://semyeong-yu.github.io/blog/2024/LLM/" rel="alternate" type="text/html" title="Solar LLM with Langchain"/><published>2024-07-03T14:00:00+00:00</published><updated>2024-07-03T14:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/LLM</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/LLM/"><![CDATA[<h2 id="solar-llm-by-upstage">Solar LLM by Upstage</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-03-LLM/1-480.webp 480w,/assets/img/2024-07-03-LLM/1-800.webp 800w,/assets/img/2024-07-03-LLM/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-03-LLM/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="solar-llm-as-personalized-llm">Solar LLM as Personalized LLM</h3> <ul> <li> <p>Referenced Github :<br/> <a href="https://github.com/UpstageAI/cookbook/tree/main">UpstageAI-cookbook</a><br/> <a href="https://github.com/semyeong-yu/LLM-cookbook">UpstageAI-cookbook</a><br/> <a href="https://github.com/iamtaewan/solarllm-oracle-cookbook">OracleDB-cookbook</a></p> </li> <li> <p>Solar mini LLM :<br/> small-size<br/> best LLM for fine-tuning<br/> can be used as personalized LLM</p> </li> <li> <p>Future of AI Ecosystem Hierarchy :<br/> Domain-specific and self-fine-tuned LLMs<br/> Solar LLM O/S<br/> O/S<br/> AI chips</p> </li> <li> <p>Langchain :<br/> LLM과 application의 통합을 간소화하는 SDK</p> </li> <li> <p>핵심 기능 : 앞으로 아래에서 배울 예정!!</p> <ul> <li>LLM 사용 (query, context)</li> <li>Groundedness Check (팩트체크)</li> <li>Layout Analyzer (PDF 또는 img에서 정보 추출)</li> <li>Embedding and DB vector store (embedding vector를 DB에 저장)</li> <li>Define Custom Tools (img 생성, 뉴스 검색, 스케쥴 관리 등)</li> </ul> </li> </ul> <h3 id="chat">Chat</h3> <pre><code class="language-Python">from langchain_upstage import ChatUpstage

llm = ChatUpstage()
llm.invoke("What's the best season to get to Korean?") # invoke llm

llm = ChatUpstage(model="solar-1-mini-chat-ja")
llm.invoke("ソーラーLLM、こんにちは。ソーラーLLM、こんにちは。")
</code></pre> <h3 id="few-shot-learning---chain">Few-shot Learning - Chain</h3> <pre><code class="language-Python"># 1. use Chat Prompt Template
from langchain_core.prompts import ChatPromptTemplate

# 2. 농담조로 말하도록 Few-shot Learning
chat_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant."),
        ("human", "What is the capital of France?"),
        ("ai", "I know of it. It's Paris!!"),
        ("human", "What about Korea?"),
    ]
)

# 3. define and invoke chain
from langchain_core.output_parsers import StrOutputParser

chain = chat_prompt | llm | StrOutputParser()
chain.invoke({})
</code></pre> <pre><code class="language-Python"># 1. use Prompt Template
from langchain_core.prompts import PromptTemplate

# 2. 한 번에 답을 내도록 (Standard Prompting) Few-shot Learning
# 한 번에 답을 내려다보니 llm이 답 틀리게 내놓음
prompt_template = PromptTemplate.from_template(
    """
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?

A: The answer is 11.

Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?

A: the answer is
"""
)

# 3. define and invoke chain
chain = prompt_template | llm | StrOutputParser()
chain.invoke({})
</code></pre> <pre><code class="language-Python"># 1. use Prompt Template
from langchain_core.prompts import PromptTemplate

# 2. 설명하면서 답을 내도록 (Chain-of-Thought Prompting) Few-shot Learning
# 설명하면서 답을 내니 llm이 알맞게 답을 내놓음
prompt_template = PromptTemplate.from_template(
    """
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?

A: Roger started with 5 balls. 2 cans of 3 tennis balls
each is 6 tennis balls. 5 + 6 = 11. The answer is 11.

Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?"""
)

# 3. define and invoke chain
chain = prompt_template | llm | StrOutputParser()
chain.invoke({})
</code></pre> <h3 id="zero-shot-learning---chain">Zero-shot Learning - Chain</h3> <pre><code class="language-Python">from langchain_core.prompts import PromptTemplate

# Zero-shot, 즉 예시를 주지 않고
# "Let's think step by step"이라는 마법의 한 문장만 써줬는데도
# 답 잘 내놓음
prompt_template = PromptTemplate.from_template(
    """
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?

A: The answer is 11.

Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?

A: Let's think step by step.
"""
)
chain = prompt_template | llm | StrOutputParser()
chain.invoke({})
</code></pre> <h3 id="divide-and-conquer">Divide-and-Conquer</h3> <ul> <li>Please provide three questions from the following text</li> </ul> <p>보다는</p> <ul> <li>Please extract three keywords from the following text 한 다음<br/> Please provide one question from the following text regarding “Depth up-scaling (DUS)”</li> </ul> <h3 id="prompt-반복">Prompt 반복</h3> <p>python f-string과 비슷한 원리</p> <pre><code class="language-Python">from langchain_core.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template(
    "Tell me a {adjective} joke about {content}."
)
# prompt_template.format(adjective="funny", content="chickens")

chain = prompt_template | llm | StrOutputParser()
chain.invoke({"adjective": "funny", "content": "chickens"})
</code></pre> <h3 id="keep-message-history-in-langchain-prompts">Keep Message History in LangChain Prompts</h3> <p>MessagesPlaceholder</p> <pre><code class="language-Python">from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# General Chat form with Message History
rag_with_history_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant."),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{input}"),
    ]
)

from langchain_core.messages import AIMessage, HumanMessage

# Message History argument
history = [
    HumanMessage("What is the capital of France?"),
    AIMessage("It's Paris!!"),
]

chain = rag_with_history_prompt | llm | StrOutputParser()
chain_result = chain.invoke({"history": history, "input": "What about Korea?"})
print(chain_result)
</code></pre> <h3 id="groundedness-check-with-langchain">Groundedness Check with LangChain</h3> <p>Groundedness Check :<br/> 답(answer)이 주어진 문맥(context)과 일맥상통하는지 (구라가 아닌지) <code class="language-plaintext highlighter-rouge">팩트 체크</code>!</p> <pre><code class="language-Python">from langchain_upstage import UpstageGroundednessCheck

groundedness_check = UpstageGroundednessCheck()

answer = chain.invoke(
    {
        "question": "What is DUS?",
        "Context": context,
    }
)
print("Potential answer: ", answer)

gc_result = groundedness_check.invoke({"context": context, "answer": answer})
print("GC check result: ", gc_result)
if gc_result.lower().startswith("grounded"):
    print("Groundedness check passed")
else:
    print("Groundedness check failed")
</code></pre> <h3 id="pdf-loader-context로-사용">PDF Loader (Context로 사용)</h3> <p>PDF에 있는 내용을 읽어와서 Context로 사용!</p> <pre><code class="language-Python">from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("pdfs/solar_sample.pdf")
docs = loader.load()  # or layzer.lazy_load()
print(docs[0].page_content[:1000])
</code></pre> <h3 id="layout-analyzer-context로-사용">Layout Analyzer (Context로 사용)</h3> <p>LLM이 받아들이기 좋은 형태로 문서를 읽기 위해<br/> Extract layouts, tables, and figures from any document to .html file<br/> Maximize RAG performance (RAG는 이후에 설명 예정)</p> <pre><code class="language-Python">! pip3 install -qU  markdownify  langchain-upstage  requests
import os
from langchain_upstage import UpstageLayoutAnalysisLoader
from IPython.display import display, HTML

os.environ["UPSTAGE_API_KEY"] = "UPSTAGE_API_KEY"

loader = UpstageLayoutAnalysisLoader("invoice.png", split="page", use_ocr=True)
# For improved memory efficiency, consider using the lazy_load method to load documents page by page.
pages = loader.load()  # or loader.lazy_load()
for page in pages:
    print(page)

loader = UpstageLayoutAnalysisLoader("pdfs/solar_sample.pdf", output_type="html")
docs = layzer.load() # or loader.lazy_load()
display(HTML(docs[0].page_content[:5000]))
</code></pre> <h3 id="rag-retrieval-augmented-generation">RAG: Retrieval Augmented Generation</h3> <ul> <li>RAG (Retrieval Augmented Generation) : <ul> <li><code class="language-plaintext highlighter-rouge">pdf, html 등 주어진 파일에서 query와 관련 있는 부분만 검색해서 context로서 사용</code>!</li> <li>Large language models (LLMs) have a limited context size</li> <li>Not all context is relevant to a given question</li> <li><code class="language-plaintext highlighter-rouge">Relevant context is retrieved(검색) from external data sources</code> and added to the prompt</li> <li>LLM generates a response based on this augmented context prompt</li> <li>RAG is particularly useful for Question Answering on custom datasets</li> <li>Query \(\rightarrow\) Retrieve (Search) \(\rightarrow\) Augmented Prompt \(\rightarrow\) LLM \(\rightarrow\) Answer</li> </ul> </li> <li>Chunking, Splitting : <ul> <li>Fixed-size chunking : split text into equal-sized chunks based on character or token count</li> <li>Semantic chunking : split text based on semantic boundaries like sentences, paragraphs, or sections</li> <li>Hierarchical chunking : create chunks at multiple levels of granularity (The ideal chunk size depends on the embedding model, retrieval use-case, and downstream task)</li> </ul> </li> </ul> <pre><code class="language-Python">from langchain_upstage import UpstageLayoutAnalysisLoader
from langchain_community.retrievers import BM25Retriever
from langchain_text_splitters import (
    Language,
    RecursiveCharacterTextSplitter,
)

layzer = UpstageLayoutAnalysisLoader(
    "pdfs/kim-tse-2008.pdf", use_ocr=True, output_type="html"
)
docs = layzer.load()

text_splitter = RecursiveCharacterTextSplitter.from_language(
    chunk_size=1000, chunk_overlap=100, language=Language.HTML
)
splits = text_splitter.split_documents(docs)

retriever = BM25Retriever.from_documents(splits)

query = "What is bug classficiation?"
context_docs = retriever.invoke("bug") # keyword search
chain.invoke({"question": query, "Context": context_docs})
</code></pre> <h3 id="keyword-search-대신-semantic-search-with-embedding-space">Keyword Search 대신 Semantic Search with Embedding space</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-03-LLM/2-480.webp 480w,/assets/img/2024-07-03-LLM/2-800.webp 800w,/assets/img/2024-07-03-LLM/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-03-LLM/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Solar-Embedding-1-Large (v1.0)<br/> Convert unstructured text data into embedding vectors</p> <pre><code class="language-Python">from langchain_upstage import UpstageEmbeddings
from langchain_chroma import Chroma
from langchain_upstage import UpstageEmbeddings
from langchain.docstore.document import Document

embeddings_model = UpstageEmbeddings(
  api_key="UPSTAGE_API_KEY", 
  model="solar-embedding-1-large"
)
embeddings = embeddings_model.embed_documents(
    [
        "What is the best season to visit Korea?",
    ]
)
query_result = embeddings.embed_query("What does Sam do?") # vector

sample_text_list = [
    "Korea is a beautiful country to visit in the spring.",
    "The best time to visit Korea is in the fall.",
    "Best way to find bug is using unit test.",
    "Python is a great programming language for beginners.",
    "Sung Kim is a great teacher.",
    "맛있는 좋은 과일을 많이 먹어 볼까?"
]

sample_docs = [Document(page_content=text) for text in sample_text_list]

vectorstore = Chroma.from_documents(
    documents=sample_docs,
    embedding=UpstageEmbeddings(model="solar-embedding-1-large"),
)

retriever = vectorstore.as_retriever()
result_docs = retriever.invoke("When to visit Korea?")
print(result_docs[0].page_content[:100])
</code></pre> <h3 id="rag-summary">RAG Summary</h3> <ol> <li>load doc</li> <li>chunking, splits</li> <li>embedding, indexing (vector store)</li> <li>retrieval, augmenting context (find Top-k most similar doc chunks in vector store with the query embedding)</li> <li>invoke chain based on the augmented context by retrieval</li> </ol> <pre><code class="language-Python">from langchain_upstage import UpstageLayoutAnalysisLoader
from langchain_text_splitters import (
    Language,
    RecursiveCharacterTextSplitter,
)
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_upstage import ChatUpstage

# 1. load doc
layzer = UpstageLayoutAnalysisLoader("pdfs/kim-tse-2008.pdf", output_type="html")
docs = layzer.load()

# 2. chunking &amp; splits
text_splitter = RecursiveCharacterTextSplitter.from_language(
    chunk_size=1000, chunk_overlap=100, language=Language.HTML
)
splits = text_splitter.split_documents(docs)

# 3. Embedding &amp; indexing
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=UpstageEmbeddings(model="solar-embedding-1-large"),
)

# 4. retrieve
retriever = vectorstore.as_retriever()
result_docs = retriever.invoke("What is Bug Classification?")
print(len(result_docs))
print(result_docs[0].page_content[:100])

# 5. invoke chain based on the augmented context by retrieval 
llm = ChatUpstage()

prompt_template = PromptTemplate.from_template(
    """
    Please provide most correct answer from the following context. 
    If the answer is not present in the context, please write "The information is not present in the context."
    ---
    Question: {question}
    ---
    Context: {Context}
    """
)

chain = prompt_template | llm | StrOutputParser()
chain.invoke({"question": "What is bug classficiation?", "Context": result_docs})
</code></pre> <h3 id="oracle-db를-persistent-memory로-쓰기">Oracle DB를 persistent memory로 쓰기</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-03-LLM/3-480.webp 480w,/assets/img/2024-07-03-LLM/3-800.webp 800w,/assets/img/2024-07-03-LLM/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-03-LLM/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>매번 100만 개의 pdf를 load해서 index할 수는 없는 일!</p> <ol> <li>load doc</li> <li>chunking, splits</li> <li>embedding, indexing (<code class="language-plaintext highlighter-rouge">vector store with Oracle DB</code>)</li> <li>retrieval, augmenting context</li> <li>split한 text가 이미 Oracle DB vector store에 있는지 체크</li> <li>없다면 embedding, indexing again</li> <li>invoke chain based on the augmented context by retrieval</li> </ol> <pre><code class="language-Python">! pip3 install -qU  markdownify  langchain-upstage rank_bm25

import oracledb
from langchain_upstage import UpstageLayoutAnalysisLoader

# 0. connect to Oracle DB
username=os.environ["DB_USER"]
password=os.environ["DB_PASSWORD"]
dsn=os.environ["DSN"]

con = oracledb.connect(user=username, password=password, dsn=dsn)

try: 
    conn23c = oracledb.connect(user=username, password=password, dsn=dsn)
    print("Connection successful!", conn23c.version)
except Exception as e:
    print("Connection failed!")

# 1. load doc
layzer = UpstageLayoutAnalysisLoader("pdfs/kim-tse-2008.pdf", output_type="html")
docs = layzer.load()

# 2. chunking &amp; splits
text_splitter = RecursiveCharacterTextSplitter.from_language(
    chunk_size=1000, chunk_overlap=100, language=Language.HTML
)
splits = text_splitter.split_documents(docs)

# check if text is in the vector store
def is_in_vectorstore(vectorstore, text):
    search_results = vectorstore.get(ids=[text])
    if search_results and search_results["ids"]:
        return True
    else:
        return False

# 3. Embedding &amp; indexing 방법 1.
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=UpstageEmbeddings(model="solar-embedding-1-large"),
)

# 3. Embedding &amp; indexing 방법 2.

knowledge_base = OracleVS.from_documents(docs, upstage_embeddings, client=conn23c, table_name="text_embeddings2", distance_strategy=DistanceStrategy.DOT_PRODUCT)

# result_chunks = knowledge_base.similarity_search(user_question)

vectorstore = OracleVS(client=conn23c, embedding_function=upstage_embeddings, table_name="text_embeddings2", distance_strategy=DistanceStrategy.DOT_PRODUCT) # create vector store

oraclevs.create_index(
    client=conn23c,
    vector_store=vectorstore,
    params={
        "idx_name": "ivf_idx1",
        "idx_type": "IVF",
    },
) # index 추가

# 4. retrieve
retriever = vectorstore.as_retriever()

unique_splits = [
    split for split in splits if not is_in_vectorstore(vectorstore, split.page_content)
]
print(len(unique_splits))

# 5. split한 text가 이미 Oracle DB vector store에 있는지 체크
# 6. 없다면 embedding, indexing again
if len(unique_splits) &gt; 0:
    vectorstore = Chroma.from_documents(
        ids=[split.page_content for split in unique_splits],
        persist_directory="./chroma_db",
        documents=unique_splits,
        embedding=UpstageEmbeddings(model="solar-embedding-1-large"),
)

# 7. invoke chain based on the augmented context by retrieval 
llm = ChatUpstage()

prompt_template = PromptTemplate.from_template(
    """
    Please provide most correct answer from the following context. 
    If the answer is not present in the context, please write "The information is not present in the context."
    ---
    Question: {question}
    ---
    Context: {Context}
    ---
    Output: please, response in Korean
    """
)

chain = ({"context": retriever, "question": RunnablePassthrough()} | prompt_template | llm | StrOutputParser())
response = chain.invoke("What is bug classficiation?")
</code></pre> <h3 id="smart-rag">Smart RAG</h3> <p>local vector store에 검색했을 때</p> <ul> <li>내가 아는 건 <code class="language-plaintext highlighter-rouge">local RAG</code> 로 처리</li> <li>내가 모르는 건 <code class="language-plaintext highlighter-rouge">external search</code> 로 처리</li> </ul> <pre><code class="language-Python"># 주어진 context만으로 주어진 question에 답변할 수 있는지 판단
# RAG or Search?
def is_in(question, context):
    is_in_conetxt = """As a helpful assistant, 
please use your best judgment to determine if the answer to the question is within the given context. 
If the answer is present in the context, please respond with "yes". 
If not, please respond with "no". 
Only provide "yes" or "no" and avoid including any additional information. 
Please do your best. Here is the question and the context:
---
CONTEXT: {context}
---
QUESTION: {question}
---
OUTPUT (yes or no):"""

    is_in_prompt = PromptTemplate.from_template(is_in_conetxt)
    chain = is_in_prompt | ChatUpstage() | StrOutputParser()

    response = chain.invoke({"context": context, "question": question})
    print(response)
    return response.lower().startswith("yes")
</code></pre> <pre><code class="language-Python"># Smart RAG, Self-Improving RAG
import os
from tavily import TavilyClient

def smart_rag(question, context):
    if not is_in(question, context):
        print("Searching in tavily")
        tavily = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])
        context = tavily.search(query=question)

    chain = prompt_template | llm | StrOutputParser()
    return chain.invoke({"context": context, "question": question})
</code></pre> <pre><code class="language-Python">smart_rag("What is DUS?", solar_summary)
# 질문에 대한 답변이 solar_summary에 있는 내용이므로 RAG  
# yes  
# 'The answer to the question "What is DUS?" is:\n\nDepth Up-Scaling (DUS)'
</code></pre> <pre><code class="language-Python">smart_rag("How to get to Seoul from SF?", solar_summary)
# solar_summary에 없는 내용이므로 Search  
# no  
# Searching in tavily
# 'The answer to "How to get to Seoul from SF?" is:\n\n1. Fly from San Francisco (SFO) to Seoul (ICN) with airlines such as ANA, Japan Airlines, Asiana Airlines, Korean Air, and United Airlines.\n2. Take a train from Incheon Int\'l Airport T1 to Seoul Station.\n3. Take the BART from Civic Center / UN Plaza to Milpitas and then fly from San Jose (SJC) to Incheon (ICN).\n\nPlease note that the cheapest flights from San Francisco to Seoul start at $453 with AIR PREMIA.'
</code></pre> <h3 id="smart-rag-with-tools">Smart RAG with Tools</h3> <ol> <li>Define <code class="language-plaintext highlighter-rouge">Custom Tools</code></li> <li>Create a list of tools</li> <li>Bind the tools to LLM</li> </ol> <p>특정 task (산수 계산 혹은 뉴스기사 검색 등) 맞춤형으로<br/> custom tools를 정의함으로써<br/> LLM 답변의 질을 높일 수 있음!</p> <pre><code class="language-Python">! pip3 install -qU  markdownify  langchain-upstage rank_bm25

from langchain_core.tools import tool
import requests
import os
from tavily import TavilyClient

# external API to search
tavily = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])

# 1. Define Custom Tools
@tool
def add(a: int, b: int) -&gt; int:
    """Adds a and b."""
    return a + b

@tool
def solar_paper_search(query: str) -&gt; str:
    """Query for research paper about solarllm, dus, llm and general AI.
    If the query is about DUS, Upstage, AI related topics, use this.
    """
    return solar_summary

@tool
def internet_search(query: str) -&gt; str:
    """This is for query for internet search engine like Google.
    Query for general topics.
    """
    return tavily.search(query=query)

@tool
def get_news(topic: str) -&gt; str:
    """Get latest news about a topic.
    If users are more like recent news, use this.
    """
    # https://newsapi.org/v2/everything?q=tesla&amp;from=2024-04-01&amp;sortBy=publishedAt&amp;apiKey=API_KEY
    # change this to request news from a real API
    news_url = f"https://newsapi.org/v2/everything?q={topic}&amp;apiKey={os.environ['NEWS_API_KEY']}"
    respnse = requests.get(news_url)
    return respnse.json()

# 2. Create a list of tools 
tools = [add, solar_paper_search, internet_search, get_news]

# 3. Bind the tools to LLM
llm_with_tools = llm.bind_tools(tools)

llm_with_tools.invoke("What is Solar LLM?").tool_calls
# 출력 : [{'name': 'solar_paper_search', 'args': {'query': 'Solar LLM'}, 'id': 'cb1687d2-7c6a-45dc-8287-19376c335cd4'}]
llm_with_tools.invoke("What's best place in Seoul?").tool_calls
# 출력 : [{'name': 'internet_search', 'args': {'query': 'best place in Seoul'}, 'id': '1f86d563-de15-460a-abc0-0e644e284518'}]
</code></pre> <pre><code class="language-Python"># Smart RAG, Self-Improving RAG
import os
from tavily import TavilyClient

def call_tool_func(tool_call):
    tool_name = tool_call["name"].lower()
    if tool_name not in globals():
        print("Tool not found", tool_name)
        return None
    selected_tool = globals()[tool_name]
    return selected_tool.invoke(tool_call["args"])

def tool_rag(question):
    for _ in range(3): # try 3 times
        tool_calls = llm_with_tools.invoke(question).tool_calls
        if tool_calls:
            break
        else:
            print("try again")

    if not tool_calls:
        return "I'm sorry, I don't have an answer for that."
    
    print(tool_calls)
    context = ""
    for tool_call in tool_calls:
        context += str(call_tool_func(tool_call))

    chain = prompt_template | llm | StrOutputParser()
    return chain.invoke({"context": context, "question": question})

tool_rag("What is Solar llm?")
# 출력 : [{'name': 'solar_paper_search', 'args': {'query': 'What is Solar llm?'}, 'id': 'cb291b01-a1aa-4839-84a8-a473f4eb0920'}] 'Solar llm is a large language model (LLM) with 10.7 billion parameters.'
tool_rag("What is news about Tesla?")
# 출력 : [{'name': 'get_news', 'args': {'topic': 'Tesla'}, 'id': 'aade5002-b9e2-4a23-92d7-fd66f12cfeb6'}] "The news about Tesla is that the company has issued a voluntary recall for nearly 4,000 Cybertrucks due to a fault with the accelerator pedal that could get trapped, pushing the car to full speed."
</code></pre> <h3 id="fine-tuning-with-predibase">Fine-tuning with Predibase</h3> <p>CFT (Continued Fine-Tuning) : feedback database에 기반하여 계속 fine-tuning</p> <pre><code class="language-Python">adapter = pb.adapters.create(
  config=FinetuningConfig(
    base_model = "solar-1-mini-chat-240612",
    epochs = 1, # default: 3
    rank = 1, # default: 16
  ),
  dataset = pb_dataset, # also accepts the dataset name as str
  repo = repo,
  description = "initial model with defaults"
)
</code></pre>]]></content><author><name></name></author><category term="generative"/><category term="generative"/><category term="LLM"/><summary type="html"><![CDATA[Upstage Solar LLM as Personalized LLM]]></summary></entry><entry><title type="html">3D Rotation-Quaternion</title><link href="https://semyeong-yu.github.io/blog/2024/Quaternion/" rel="alternate" type="text/html" title="3D Rotation-Quaternion"/><published>2024-07-01T14:00:00+00:00</published><updated>2024-07-01T14:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Quaternion</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Quaternion/"><![CDATA[<h2 id="lecture-06-3d-rotations-and-complex-representations-cmu-15-462662">Lecture 06: 3D Rotations and Complex Representations (CMU 15-462/662)</h2> <blockquote> <p>referenced video :<br/> <a href="https://www.youtube.com/watch?v=YF5ZUlKxSgE&amp;list=PL9_jI1bdZmz2emSh0UQ5iOdT2xRHFHL7E&amp;index=7">3D Rotations and Quaternion</a><br/> referenced blog :<br/> <a href="https://blog.naver.com/hblee4119/223188806834">Quaternion</a></p> </blockquote> <h2 id="3d-rotation">3D Rotation</h2> <ul> <li>2D rotation에서는 order of rotations 노상관, but<br/> 3D rotation에서는 <code class="language-plaintext highlighter-rouge">order of rotations 중요</code></li> </ul> <h2 id="gimbal-lock">Gimbal Lock</h2> <ul> <li>Gimbal Lock :<br/> Euler angles \(\theta_{x}, \theta_{y}, \theta_{z}\) 로 회전시킬 때 두 축이 맞물려서 <code class="language-plaintext highlighter-rouge">한 축이 소실</code>되는 상황</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-Quaternion/2-480.webp 480w,/assets/img/2024-07-01-Quaternion/2-800.webp 800w,/assets/img/2024-07-01-Quaternion/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-Quaternion/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 1 -&gt; 2번째 그림 : x축(초록) 회전 / 2 -&gt; 3번째 그림 : z축(파랑) 회전 / 3 -&gt; 4번째 그림 : y축(빨강) 회전 </div> <ul> <li>위의 그림에 따르면 Euler angles는 \(x\)(초록), \(y\)(빨강), \(z\)(파랑) 순으로 <code class="language-plaintext highlighter-rouge">상속관계</code>여서<br/> \(x\)축(초록)을 회전시키면 그의 자식들인 \(y, z\)축(빨강, 파랑)도 같이 회전한다.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-Quaternion/3-480.webp 480w,/assets/img/2024-07-01-Quaternion/3-800.webp 800w,/assets/img/2024-07-01-Quaternion/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-Quaternion/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>이 때, <code class="language-plaintext highlighter-rouge">Gimbal Lock</code>은 위의 그림과 같이<br/> <code class="language-plaintext highlighter-rouge">상속관계에서의 2번째 축(빨강)이 -90도 혹은 90도 회전</code>했을 때<br/> <code class="language-plaintext highlighter-rouge">상속관계에서의 1번째 축(초록)과 3번째 축(파랑)이 겹쳐서</code> 같은 방향으로 회전하기 때문에 발생한다.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-Quaternion/1-480.webp 480w,/assets/img/2024-07-01-Quaternion/1-800.webp 800w,/assets/img/2024-07-01-Quaternion/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-Quaternion/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>예를 들어, 만약 \(\theta_{y} = \frac{\pi}{2}\) 로 고정한다면<br/> \(R_x R_y R_z = \begin{bmatrix} 0 &amp; 0 &amp; 1 \\ sin(\theta_{x}+\theta_{z}) &amp; cos(\theta_{x}+\theta_{z}) &amp; 0 \\ - cos(\theta_{x}+\theta_{z}) &amp; sin(\theta_{x}+\theta_{z}) &amp; 0 \end{bmatrix}\)<br/> 이므로 \(\theta_{x}, \theta_{z}\) 값(자유도=2)과 관계없이 <code class="language-plaintext highlighter-rouge">특정 하나의 axis에 대한 회전(자유도=1)으로 제약 생겨버림</code>!</li> </ul> <h2 id="quaternion">Quaternion</h2> <ul> <li> <p>Euler angles vs Quaternion :<br/> Euler angles는 상속관계이므로 한 번에 계산이 불가능하여 순서대로 회전시켜야 하고, 짐벌락 현상이 발생할 수 있지만<br/> Quaternion은 <code class="language-plaintext highlighter-rouge">한 번에 계산 가능</code>하여 <code class="language-plaintext highlighter-rouge">동시에 회전</code>시킬 수 있으며, 짐벌락 현상이 없다!</p> </li> <li>2D rotation : <ul> <li>real, rectangular form : 2D rotation matrix 복잡</li> <li>complex, polar form : 단순히 크기 곱하고, 각도 더하고!</li> </ul> </li> <li>3D rotation : <ul> <li>real, xyz form : 3D rotation matrix 복잡</li> <li>quaternion : only need <code class="language-plaintext highlighter-rouge">FOUR</code> coordinates!(one real, three imaginary)<br/> \(H\) = span(\(\{1, i, j, k\}\))<br/> \(q = a + bi + cj + dk \in H\)<br/> \(i^2 = j^2 = k^2 = ijk = -1\) \(\leftarrow\) <code class="language-plaintext highlighter-rouge">new property!</code><br/> \(ij = k\), \(ji = -k\)<br/> \(jk = i\), \(kj = -i\)<br/> \(ki = j\), , \(ik = -j\)</li> </ul> </li> <li>Quaternion : <ul> <li>distributive and associative</li> <li><code class="language-plaintext highlighter-rouge">not commutative</code> : \(qp \neq pq\) for \(q, p \in H\)</li> <li>quaternion is <code class="language-plaintext highlighter-rouge">a pair of scalar and vector</code><br/> \(q = a + bi + cj + dk\)<br/> \(= (a, \boldsymbol u) = (a, (b, c, d)) \in H\)<br/> where \(a \in Re(H) = R\) and \(\boldsymbol u \in Im(H) = R^3\)</li> <li><code class="language-plaintext highlighter-rouge">quaternion product</code> :<br/> \((a, \boldsymbol u)(b, \boldsymbol v) = (ab - \boldsymbol u \cdot \boldsymbol v, a \boldsymbol v + b \boldsymbol u + \boldsymbol u \times \boldsymbol v)\)<br/> \(\boldsymbol u \boldsymbol v = \boldsymbol u \times \boldsymbol v - \boldsymbol u \cdot \boldsymbol v\)</li> <li><code class="language-plaintext highlighter-rouge">quaternion conjugate</code> :<br/> \(q = (w, x, y, z)\)<br/> \(q^{\ast} = (w, -x, -y, -z)\)<br/> \(\| q \| = \sqrt{w^2 + x^2 + y^2 + z^2}\)<br/> \(q \cdot q^{\ast} = (w, x, y, z) \cdot (w, -x, -y, -z) = w^2 + x^2 + y^2 + z^2 = \| q \|^2\)<br/> \(q^{-1} = \frac{q^{\ast}}{\| q \|^2} = \frac{q^{\ast}}{q \cdot q^{\ast}} = \frac{1}{q}\)<br/> \((q_1 q_2)^{\ast} = q_2^{\ast} q_1^{\ast}\)</li> </ul> </li> <li>3D Transformations via Quaternions : <ul> <li><code class="language-plaintext highlighter-rouge">3D Rotation</code> : \(q x \bar q\) \(\leftrightarrow\) \(x\)를 \(u\)에 대해 \(\theta\)만큼 회전<br/> for \(q = cos(\frac{\theta}{2}) + sin(\frac{\theta}{2})u\)<br/> where pure imaginary 3D vector \(x, u \in Im(H) = R^3\)<br/> where unit quaternion \(q \in H = (R, R^3)\) where \(\| q \|^2 = 1\)<br/> where \(\bar q\) 는 \(q\)의 conjugate</li> <li><code class="language-plaintext highlighter-rouge">Interpolating Rotation</code> :<br/> interpolating Euler angles는 strange-looking paths 및 non-uniform rotation speed를 야기할 수 있음<br/> 대신 Quaternion으로 나타내면,<br/> <code class="language-plaintext highlighter-rouge">spherical linear interpolation (SLERP)</code> :<br/> Slerp(\(q_0, q_1, t\)) = \(q_0(q_0^{-1} q_1)^t\)<br/> where \(t \in [0, 1]\)</li> <li>Generating Coordinates for <code class="language-plaintext highlighter-rouge">Texture Maps</code> :<br/> (hyper)complex numbers는 <code class="language-plaintext highlighter-rouge">angle-preserving(conformal)</code> maps에 쓰임!<br/> texture에서 angle-preserving 특성은 사람 눈으로 보기에 매우 그럴 듯하게 보이게 함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-01-Quaternion/4-480.webp 480w,/assets/img/2024-07-01-Quaternion/4-800.webp 800w,/assets/img/2024-07-01-Quaternion/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-01-Quaternion/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Beyond Quaternion … :<br/> <code class="language-plaintext highlighter-rouge">Lie algebras</code> and <code class="language-plaintext highlighter-rouge">Lie Groups</code> 으로도 3D rotations를 나타낼 수 있으며,<br/> 특히 <code class="language-plaintext highlighter-rouge">statistics(averages) of rotations</code> 를 구할 때 매우 유용!<br/> <code class="language-plaintext highlighter-rouge">exponential map</code> : axis/angle \(\rightarrow\) rotation matrix<br/> <code class="language-plaintext highlighter-rouge">logarithmic map</code> : rotation matrix \(\rightarrow\) axis/angle</p> </li> <li> <p>4 \(\times\) 1 <code class="language-plaintext highlighter-rouge">quaternion</code> \(q\) 으로 3 \(\times\) 3 <code class="language-plaintext highlighter-rouge">rotation matrix</code> 만드는 방법 : <a href="https://github.com/graphdeco-inria/gaussian-splatting/blob/b2ada78a779ba0455dfdc2b718bdf1726b05a1b6/utils/general_utils.py#L78">build_rotation(r)</a></p> <pre><code class="language-Python">def build_rotation(r):
  norm = torch.sqrt(r[:,0]*r[:,0] + r[:,1]*r[:,1] + r[:,2]*r[:,2] + r[:,3]*r[:,3])
  q = r / norm[:, None] # use normalized quaternion

  R = torch.zeros((q.size(0), 3, 3), device='cuda')

  r = q[:, 0]
  x = q[:, 1]
  y = q[:, 2]
  z = q[:, 3]

  R[:, 0, 0] = 1 - 2 * (y*y + z*z)
  R[:, 0, 1] = 2 * (x*y - r*z)
  R[:, 0, 2] = 2 * (x*z + r*y)
  R[:, 1, 0] = 2 * (x*y + r*z)
  R[:, 1, 1] = 1 - 2 * (x*x + z*z)
  R[:, 1, 2] = 2 * (y*z - r*x)
  R[:, 2, 0] = 2 * (x*z - r*y)
  R[:, 2, 1] = 2 * (y*z + r*x)
  R[:, 2, 2] = 1 - 2 * (x*x + y*y)
  return R
</code></pre> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="quaternion"/><category term="rotation"/><summary type="html"><![CDATA[Quaternion for Rotation Matrix]]></summary></entry></feed>