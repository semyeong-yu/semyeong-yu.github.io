<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://semyeong-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://semyeong-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-04-15T20:03:02+00:00</updated><id>https://semyeong-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">NeRF</title><link href="https://semyeong-yu.github.io/blog/2024/NeRF/" rel="alternate" type="text/html" title="NeRF"/><published>2024-04-10T21:00:00+00:00</published><updated>2024-04-10T21:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/NeRF</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/NeRF/"><![CDATA[<h1 id="nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h1> <h4 id="ben-mildenhall-pratul-psrinivasan-matthew-tancik">Ben Mildenhall, Pratul P.Srinivasan, Matthew Tancik</h4> <h2 id="introduction">Introduction</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/3-480.webp 480w,/assets/img/2024-04-10-NeRF/3-800.webp 800w,/assets/img/2024-04-10-NeRF/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="pipeline">Pipeline</h4> <ul> <li> <p>(a) march camera rays to generate sampling of 5D coordinates</p> </li> <li> <p>(b) represents volumetric static scene by optimizing continuous 5D function(fully-connected network)</p> </li> </ul> <ol> <li>input: single continuous <code class="language-plaintext highlighter-rouge">5D coordinate</code><br/> 3D location \(x, y, z\)<br/> 2D direction \(\theta, \phi\)</li> <li>output:<br/> <code class="language-plaintext highlighter-rouge">volume density</code> (differential opacity) (how much radiance is accumulated by a ray)<br/> <code class="language-plaintext highlighter-rouge">view-dependent RGB color</code> (emitted radiance) \(c = (r, g, b)\)</li> </ol> <ul> <li> <p>(c) synthesizes novel view by classic <code class="language-plaintext highlighter-rouge">volume rendering</code> techniques(differentiable) to accumulate(project)(composite) the color/density samples into 2D image along rays</p> </li> <li> <p>(d) loss between synthesized and GT observed images</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/2-480.webp 480w,/assets/img/2024-04-10-NeRF/2-800.webp 800w,/assets/img/2024-04-10-NeRF/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Pipeline of NeRF architecture </div> <h4 id="problem--solution">Problem &amp; Solution</h4> <p>Problem :</p> <ol> <li>not sufficiently high-resolution representation</li> <li>inefficient in the number of samples per camera ray</li> </ol> <p>Solution :</p> <ol> <li>input <code class="language-plaintext highlighter-rouge">positional encoding</code> for MLP to represent higher frequency function</li> <li><code class="language-plaintext highlighter-rouge">hierarchical sampling</code> to reduce the number of queries</li> </ol> <h4 id="contribution">Contribution</h4> <ul> <li>represent continuous scenes as 5D neural radiance fields with basic MLP to render high-resolution novel views</li> <li>differentiable volume rendering + hierarchical sampling</li> <li>positional encoding to map input 5D coordinate into higher dim. space for high-frequency scene representation</li> <li>overcome the storage costs of discretized voxel grids by encoding continuous volume into network’s parameters<br/> =&gt; require only storage costs of sampled volumetric representations</li> </ul> <h2 id="related-work">Related Work</h2> <h4 id="neural-3d-shape-representation">Neural 3D shape representation</h4> <ul> <li> <p>deep networks that map \(xyz\) coordinates to signed distance functions or occupancy fields<br/> =&gt; limit : need GT 3D geometry</p> </li> <li> <p>Niemeyer et al.<br/> =&gt; input : find directly surface intersection for each ray (can calculate exact derivatie)<br/> =&gt; output : diffuse color at each ray intersection location</p> </li> <li> <p>Sitzmann et al.<br/> =&gt; input : each 3D coordinate<br/> =&gt; output : feature vector and RGB color at each 3D coordinate<br/> =&gt; rendering by RNN that marches along each ray to decide where the surface is.</p> </li> </ul> <blockquote> <p>Limit : oversmoothed renderings, so limited to simple shapes with low geometric complexity</p> </blockquote> <h4 id="view-synthesis-and-image-based-rendering">View synthesis and image-based rendering</h4> <ul> <li> <p>Given dense sampling of views, novel view synthesis is possible by simple light field sample interpolation</p> </li> <li> <p>Given sparser sampling of views, there are 2 ways :<br/> mesh-based representation and volumetric representation</p> </li> <li> <p>Mesh-based representation with either diffuse(난반사) or view-dependent appearance :<br/> Directly optimize mesh representations by differentiable rasterizers or pathtracers so that we reproject and reconstruct images</p> </li> </ul> <blockquote> <p>Limit :<br/> gradient-based optimization is often difficult because of local minima or poor loss landscape<br/> needs a template mesh with fixed topology for initialization, which is unavailable in real-world</p> </blockquote> <ul> <li>Volumetric representation :<br/> well-suited for gradient-based optimization and less distracting artifacts<br/> train : predict a sampled volumetric representation (voxel grids) from input images<br/> test : use alpha-(or learned-)compositing along rays to render novel views<br/> +) alpha-compositing : 여러 frame을 합쳐서 하나의 image로 합성하는 과정으로, 각 이미지 픽셀마다 알파 값(불투명도 값)(0~1)이 있어 겹치는 부분의 알파 값 및 픽셀 값을 결정<br/> CNN compensates discretization artifacts from low resolution voxel grids or CNN allows voxel grids to vary on input time</li> </ul> <blockquote> <p>Limit :<br/> good results, but limited by poor time, space complexity due to discrete sampling<br/> +) discrete sampling : rendering high resol. image =&gt; finer sampling of 3D space</p> </blockquote> <blockquote> <p>Author’s solution :<br/> encode <code class="language-plaintext highlighter-rouge">continuous</code> volume into network’s parameters<br/> =&gt; higher quality rendering + require only storage cost of those <code class="language-plaintext highlighter-rouge">sampled</code> volumetric representations</p> </blockquote> <h2 id="neural-radiance-field-scene-representation">Neural Radiance Field Scene Representation</h2> <p>represent continuous scene by 5D MLP : (x, d) =&gt; (c, \(\sigma\))</p> <p>Here, there are 2 key-points!</p> <blockquote> <p>multiview consistent :<br/> c is dependent on both x and d, but \(\sigma\) is only dependent on location x</p> </blockquote> <ul> <li>3D coordinate x =&gt; 8 fc-layers =&gt; volume-density and 256-dim. feature vector</li> </ul> <blockquote> <p>Lambertian reflection : diffuse(난반사) vs Specular reflection : 전반사</p> </blockquote> <blockquote> <p>non-Lambertian effects : view-dependent color change to represent specular reflection</p> </blockquote> <ul> <li>feature vector is concatenated with direction d =&gt; 1 fc-layer =&gt; view-dependent RGB color</li> </ul> <h2 id="volume-rendering-with-radiance-fields">Volume Rendering with Radiance Fields</h2> <h4 id="ray-from-input-image-pre-processing">Ray from input image (pre-processing)</h4> <p>We use <code class="language-plaintext highlighter-rouge">Ray</code> to synthesize <code class="language-plaintext highlighter-rouge">continuous</code>-viewpoint images from <code class="language-plaintext highlighter-rouge">discrete</code> input images</p> <blockquote> <p>\(r(t) = o + td\)<br/> o : the location of camera<br/> d : viewing direction</p> </blockquote> <blockquote> <p>How to calculate viewing direction d??</p> <ul> <li>pixel coordinate : \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)</li> <li>normalized coordinate by intrinsic matrix :<br/> \(\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\) = \(K^{-1}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\) = \(\begin{bmatrix} 1/f_x &amp; 0 &amp; W/2 \\ 0 &amp; 1/f_y &amp; H/2 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)<br/> Since y, z have opposite direction between the real-world coordinate and pixel coordinate, we multiply (-1)<br/> \(\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\) = \(\begin{bmatrix} 1/f_x &amp; 0 &amp; W/2 \\ 0 &amp; -1/f_y &amp; H/2 \\ 0 &amp; 0 &amp; -1 \end{bmatrix}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)<br/> Here, focal length in intrinsic matrix K is usually calculated using camear angle \(\alpha\) as \(\tan{\alpha / 2} = \frac{h/2}{f}\)</li> <li>3D coordinate by extrinsic matrix :<br/> For extrinsic matrix \([R \vert t']\),<br/> \(o = t'\)<br/> \(d = R * \begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\)<br/> Therefore, we can obtain \(r(t) = o + td\)</li> </ul> </blockquote> <h4 id="volume-rendering-from-mlp-output">Volume Rendering from MLP output</h4> <p>We use differential classical volume rendering</p> <blockquote> <p>idea : 빛이 들어올 확률이 클수록, 물체의 밀도가 높을수록 2D 이미지 픽셀에서 색상 채널의 값이 크게 나타나므로<br/> pixel’s color along a ray is equal to the integral of (transmittance) x (volume density) x (color) for all points along one ray.</p> </blockquote> <p>For ray \(r\) traced through desired virtual camera and near, far bounds \(t_n\), \(t_f\),<br/> expected color of ray \(r\) = \(C(r) = \int_{t_n}^{t_f} T(t) \sigma (r(t)) c(r(t), d) dt\)</p> <ul> <li>\(T(t) = \exp(- \int_{t_n}^{t} \sigma (r(s)) ds)\) :<br/> accumulated transmittance along ray from \(t_n\) to \(t\)<br/> the probability that ray travels from \(t_n\) to \(t\) without hitting any particle<br/> 투과도가 클수록 투명<br/> T(t) is the solution of \(\frac{dy}{dt} = - \sigma (x)y\)<br/> since T(t)’s rate of decrease is proportional to T(t) itself and volume density</li> <li>\(\sigma (r(t))\) : volume density along the ray (learned by MLP)</li> <li>\(c(r(t), d)\) : object’s color along the ray (learned by MLP)</li> </ul> <p>To apply the equation to our model, we have to do sampling from continuous ray to discrete points<br/> Instead of deterministic quadrature(typically used for rendering voxel grids, but may limit resolution),<br/> author divides a ray \(\left[t_n, t_f\right]\) into N = 64 bins(intervals), and chooses one point \(t_i\) for each bin by uniform sampling<br/> \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N}(t_f - t_n), t_n + \frac{i}{N}(t_f - t_n)\right]\)<br/> Although we use discrete N samples, stratified sampling(층화 표집) enables MLP to be evaluated at continuous positions by optimization</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/5-480.webp 480w,/assets/img/2024-04-10-NeRF/5-800.webp 800w,/assets/img/2024-04-10-NeRF/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>And then discretized version for N samples :<br/> expected color \(\hat{C}(r) = \sum_{i=1}^{N} T_i (1 - \exp(- \sigma_{i} \delta_{i})) c_i\)</p> <ul> <li>\(\hat{C}(r)\) : differentiable about (\(\sigma_{i}, c_i\))</li> <li>\(\delta_{j} = t_{j+1} - t_j\) : the distance between adjacent samples</li> <li>\(\sigma (r(t)) dt ~~ =&gt; ~~ \sigma_{j} \delta_{j} = 1 - \exp(-\sigma_{j} \delta_{j})\) by Taylor Series Expansion</li> <li> \[T(t) = \exp(- \int_{t_n}^{t} \sigma (r(s)) ds) ~~ =&gt; ~~ T_i = \exp(- \sum_{j=1}^{i-1} \sigma_{j} \delta_{j})\] </li> <li> \[c(r(t), d) ~~ =&gt; ~~ c_i\] </li> </ul> <p>Here, in volume rendering, author uses <code class="language-plaintext highlighter-rouge">volume density</code><br/> for 불투명도 == opacity == extinction coefficient == alpha value for alpha-compositing</p> <p>Final version :<br/> expected color \(\hat{C}(r) = \sum_{i=1}^{N} T_i \alpha_{i} c_i\)<br/> where \(T_i = \prod_{j=1}^{i-1} (1-\alpha_{j})\) and \(\alpha_{j} = 1 - \exp(-\sigma_{j} \delta_{j})\)<br/> which reduces to traditional alpha-compositing problem</p> <h2 id="optimizing-a-neural-radiance-field">Optimizing a Neural Radiance Field</h2> <h4 id="positional-encoding-pre-processing">Positional encoding (pre-processing)</h4> <p>If we use input directly, MLP is biased to learn low-frequency function (oversmoothed appearance)<br/> If we map input into higher dim. space, MLP can fit data with high-frequency variation<br/> \(r : R \rightarrow R^{2L}\) <br/> \(r(p) = (sin(2^0\pi p), cos(2^0\pi p), \cdots, sin(2^{L-1}\pi p), cos(2^{L-1}\pi p))\)<br/> \(L=10\) for \(r(x)\) where x has three coordinates<br/> \(L=4\) for \(r(d)\) where d has three components of the cartesian viewing direction unit vector</p> <h4 id="hierarchical-volume-sampling">Hierarchical volume sampling</h4> <p>Densely evaluating N points by stratified sampling is inefficient<br/> =&gt; We don’t need much sampling at free space or occluded regions<br/> =&gt; Hierarchical sampling enables us to allocate more samples to regions we expect to contain visible content</p> <p>We simultaneously optimize 2 networks with different sampling : <code class="language-plaintext highlighter-rouge">coarse</code> and <code class="language-plaintext highlighter-rouge">fine</code></p> <blockquote> <p>coarse sampling \(N_c\)개 : 위에서 배웠던 내용<br/> author divides a ray into \(N_c\) = 64 bins(intervals), and chooses one point \(t_i\) for each bin by uniform sampling<br/> \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N_c}(t_f - t_n), t_n + \frac{i}{N_c}(t_f - t_n)\right]\)</p> </blockquote> <blockquote> <p>fine sampling \(N_f\)개 : 새로운 내용<br/> coarse sampling model’s output is a weighted sum of all coarse-sampled colors<br/> \(\hat{C}(r) = \sum_{i=1}^{N_c} T_i \alpha_{i} c_i = \sum_{i=1}^{N_c} w_i c_i\)<br/> where we define \(w_i = T_i \alpha_{i} = T_i (1 - \exp(-\sigma_{i} \delta_{i}))\) for \(i=1,\cdots,N_c\)<br/> =&gt; Given the output of <code class="language-plaintext highlighter-rouge">coarse</code> network, we try more informed sampling where samples are biased toward the relevant parts of the volume<br/> =&gt; We sample \(N_f\)=128 <code class="language-plaintext highlighter-rouge">fine</code> points following a piecewise-constant PDF of normalized \(\frac{w_i}{\sum_{j=1}^{N_c} w_j}\)<br/> =&gt; Here, we use Inverse CDF Method for sampling fine points</p> </blockquote> <blockquote> <p>Inverse transform sampling = Inverse CDF Method :<br/> =&gt; PDF (probability density function) : \(f_X(x)\)<br/> =&gt; CDF (cumulative distribution function) : \(F_X(x) = P(X \leq x) = \int_{-\infty}^x f_X(x) dx\)<br/> idea : 모든 확률 분포의 CDF는 Uniform distribution을 따른다!<br/> =&gt; 따라서 CDF의 y값을 Uniform sampling하면, 그 y에 대응되는 x에 대한 (특정 PDF를 따르는) sampling을 구현할 수 있다!<br/> =&gt; 즉, CDF의 역함수를 계산할 수 있다면, 기본 난수 생성기인 Uniform sampling을 이용해서 확률 분포 X에 대한 sampling을 할 수 있다!<br/> \(F_X(x)\) ~ \(U\left[0, 1\right]\)<br/> \(X\) ~ \(F^{-1}(U\left[0, 1\right])\)</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/4-480.webp 480w,/assets/img/2024-04-10-NeRF/4-800.webp 800w,/assets/img/2024-04-10-NeRF/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We evaluate <code class="language-plaintext highlighter-rouge">coarse</code> network using \(N_c\)=64 points per ray<br/> We evaluate <code class="language-plaintext highlighter-rouge">fine</code> network using \(N_c+N_f\)=64+128=192 points per ray where we sample 128 points following PDF of <code class="language-plaintext highlighter-rouge">coarse</code> sampled points<br/> In result, we use a total of 64+192=256 samples per ray to compute the final rendering color \(C(r)\)</p> <h4 id="implementation-details--loss">Implementation details &amp; Loss</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/6-480.webp 480w,/assets/img/2024-04-10-NeRF/6-800.webp 800w,/assets/img/2024-04-10-NeRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>Prepare RGB images, corresponding camera poses, intrinsic parameters and scene bounds (use COLMAP structure-from-motion package to estimate these parameters)</li> <li>From H x W input image, randomly sample a batch of 4096 pixels</li> <li>calculate continuous ray from each pixel \(r(t) = o + kd\)</li> <li>coarse sampling of \(N_c\)=64 points per each ray \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N_c}(t_f - t_n), t_n + \frac{i}{N_c}(t_f - t_n)\right]\)</li> <li>positional encoding \(r(x)\) and \(r(d)\) for input</li> <li>obtain volume density \(\sigma\) by MLP with \(r(x, y, z)\) as input</li> <li>obtain color \(c\) by MLP with \(r(x, y, z)\) and \(r(\theta, \phi)\) as input</li> <li>obtain rendering color of each ray by volume rendering \(\hat{C}(r) = \sum_{i=1}^{N} T_i (1 - \exp(- \sigma_{i} \delta_{i})) c_i\) from two networks ‘coarse’ and ‘fine’</li> <li>compute loss</li> <li>Adam optimizer with learning rate from \(5x10^{-4}\) to \(5x10^{-5}\)</li> <li>optimization for a single scene typically takes around 100-300k iterations (1~2 days) to converge on a single NVIDIA V100 GPU</li> </ol> <p>Here, we use L2 norm for loss<br/> \(L = \sum_{r \in R} \left[{\left\|\hat{C_c}(r)-C(r)\right\|}^2+{\left\|\hat{C_f}(r)-C(r)\right\|}^2\right]\)<br/> \(C(r)\) : GT pixel RGB color<br/> \(\hat{C_c}(r)\) : rendering RGB color from coarse network : to allocate better samples in fine network<br/> \(\hat{C_f}(r)\) : rendering RGB color from fine network : our goal</p> <h2 id="results">Results</h2> <h4 id="datasets">Datasets</h4> <p>synthetic data, llff data, deep voxel data</p> <h4 id="comparisons">Comparisons</h4>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="rendering"/><category term="3d"/><summary type="html"><![CDATA[representing scenes as neural radiance fields for view synthesis]]></summary></entry><entry><title type="html">Unsupervised Learning</title><link href="https://semyeong-yu.github.io/blog/2024/unsupervised-learning/" rel="alternate" type="text/html" title="Unsupervised Learning"/><published>2024-03-01T20:00:00+00:00</published><updated>2024-03-01T20:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/unsupervised-learning</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/unsupervised-learning/"><![CDATA[<p>뮌헨공대 (Technical University of Munich)에서 공부한<br/> [IN2375 Computer Vision - Detection, Segmentation and Tracking] 컴퓨터비전 노트 정리</p> <h3 id="unsupervised-learning">Unsupervised Learning</h3> <p>특징 : fast / costly (much computation)</p> <h3 id="self-supervised-learning">Self-supervised learning</h3> <blockquote> <p>Evaluation</p> </blockquote> <p>fine-tuning / linear probing / k-NN classification</p> <blockquote> <p>Pretext task</p> </blockquote> <p>사용자가 직접 정의한 새로운 different task (goal task와 관련 있어야 함)</p> <blockquote> <p>Contrastive Learning</p> </blockquote> <p>아이디어 :</p> <p>positive pair (image vs augmented image)와 negative pair (image vs other image) 만들어서 triplet loss 사용 또는 maximize contrastive score (아래 사진 참고)</p> <ul> <li>SimCLR (Simple framework for Contrastive Learning) :</li> </ul> <p>two augmented images에 대해 maximize similarity b.w. feature maps</p> <p>장점 : simple</p> <p>단점 : negative samples 수가 많을수록 gradient bias가 줄어드므로<br/> large batchsize가 필요한데 그러면 large computation 필요</p> <ul> <li>MoCo (Momentum Contrast) :</li> </ul> <ol> <li>memory queue를 통해 GPU memory footprint 작아서 large batchsize 가능</li> <li>dynamic update of momentum encoder로 성능 향상</li> </ol> <blockquote> <p>Non-Contrastive Learning</p> </blockquote> <p>other images (negative pair) 쓰지 않음</p> <ul> <li>DINO (self-distillation with no labels) :<br/> ViT 등에서 robust latent embedding 만드는 데 사용</li> </ul> <p>entering : teacher sharpening할 때 collapse되는 것을 방지하기 위해 moving average of center를 빼줌</p> <p>sharpening : teacher가 student보다 lower temperature로 sharpened</p> <ul> <li>DINOv2 :</li> </ul> <p>DINO보다 2배 faster, 3배 less memory by noisy student, adaptive resolution, data curation, …</p> <ul> <li>MAE (Masked Autoencoders) :</li> </ul> <p>high masking ratio 필요</p> <p>masked patches를 reconstruct한 뒤 이에 대해서만 loss 계산</p> <blockquote> <p>multi-view assumption</p> </blockquote> <p>view(crop) provides enough info. for downstream task</p> <blockquote> <p>PAC (pixel-adaptive conv.) layer</p> </blockquote> <p>conv.를 하기 전에 spatially varying kernel을 곱함</p> <h3 id="downstream-applications">Downstream Applications</h3> <blockquote> <p>semantic segmentation 에서 KNN correspondence</p> </blockquote> <p>two image에 대해 “segmentation correspondence”가 “feature(DINO) correspondence”를 모방하도록 학습</p> <blockquote> <p>motion-based VOS</p> </blockquote> <p>network G : image &amp; optical flow -&gt; object mask</p> <p>network I : masked optical flow &amp; image -&gt; reconstruct optical flow</p> <blockquote> <p>Contrastive Random Walk (object tracking)</p> </blockquote> <p>아래 사진 참고.</p> <p>cycle consistency loss (cross-entropy loss b.w. label_t=0 and label_t=T)</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-unsupervised-learning/img97-480.webp 480w,/assets/img/2024-03-01-unsupervised-learning/img97-800.webp 800w,/assets/img/2024-03-01-unsupervised-learning/img97-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-unsupervised-learning/img97.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-unsupervised-learning/img102-480.webp 480w,/assets/img/2024-03-01-unsupervised-learning/img102-800.webp 800w,/assets/img/2024-03-01-unsupervised-learning/img102-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-unsupervised-learning/img102.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-unsupervised-learning/img107-480.webp 480w,/assets/img/2024-03-01-unsupervised-learning/img107-800.webp 800w,/assets/img/2024-03-01-unsupervised-learning/img107-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-unsupervised-learning/img107.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-unsupervised-learning/img112-480.webp 480w,/assets/img/2024-03-01-unsupervised-learning/img112-800.webp 800w,/assets/img/2024-03-01-unsupervised-learning/img112-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-unsupervised-learning/img112.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container>]]></content><author><name></name></author><category term="cv-tasks"/><category term="vision"/><category term="unsupervised"/><summary type="html"><![CDATA[Unsupervised Learning]]></summary></entry><entry><title type="html">Semi-Supervised Learning</title><link href="https://semyeong-yu.github.io/blog/2024/semisupervised-learning/" rel="alternate" type="text/html" title="Semi-Supervised Learning"/><published>2024-03-01T19:00:00+00:00</published><updated>2024-03-01T19:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/semisupervised-learning</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/semisupervised-learning/"><![CDATA[<p>뮌헨공대 (Technical University of Munich)에서 공부한<br/> [IN2375 Computer Vision - Detection, Segmentation and Tracking] 컴퓨터비전 노트 정리</p> <h3 id="semi-supervised-learning">Semi-supervised Learning</h3> <blockquote> <p>아이디어</p> </blockquote> <p>use both labelled and unlabelled data</p> <blockquote> <p>assumption</p> </blockquote> <ul> <li>smoothness : if two inputs are close, their labels are same</li> <li>low density : decision boundary should pass through region with low density</li> <li>manifold : data come from multiple low-dim. manifolds if data points share same manifold, their labels are same</li> </ul> <blockquote> <p>unsupervised pre-processing</p> </blockquote> <p>feature extraction</p> <blockquote> <p>wrapper method 중 self-training</p> </blockquote> <p>OnAVOS는 slow이므로 first frame/new frame 대신<br/> offline으로 labelled/unlabelled data 사용</p> <p>initial prediction이 중요하므로 미리 train strong baseline on labelled set</p> <blockquote> <p>energy minimization (low-density assumption 적용)</p> </blockquote> <p>minimize \(-p(x_i)logp(x_i)\) = entropy of class distribution of each pixel \(x_i\)</p> <blockquote> <p>VAN (virtual adversarial network) (smoothness assumption 적용)</p> </blockquote> <p>labelled set : true posterior(gt) 와 adversarial 추가한 image의 prediction 비교</p> <p>unlabelled set : 기존 image의 prediction 과 adversarial 추가한 image의 prediction 비교</p> <blockquote> <p>Domain Alignment</p> </blockquote> <p>GAN의 원리 사용하여 unlabelled real data와 labelled synthetic data의 distribution을 비슷하게</p> <blockquote> <p>Consistency Regularization</p> </blockquote> <p>image에 transformation을 가하더라도 robust하게 consistent prediction을 하도록 consistency loss 추가</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-semisupervised-learning/img112-480.webp 480w,/assets/img/2024-03-01-semisupervised-learning/img112-800.webp 800w,/assets/img/2024-03-01-semisupervised-learning/img112-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-semisupervised-learning/img112.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-semisupervised-learning/img117-480.webp 480w,/assets/img/2024-03-01-semisupervised-learning/img117-800.webp 800w,/assets/img/2024-03-01-semisupervised-learning/img117-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-semisupervised-learning/img117.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-semisupervised-learning/img122-480.webp 480w,/assets/img/2024-03-01-semisupervised-learning/img122-800.webp 800w,/assets/img/2024-03-01-semisupervised-learning/img122-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-semisupervised-learning/img122.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container>]]></content><author><name></name></author><category term="cv-tasks"/><category term="vision"/><category term="semisupervised"/><summary type="html"><![CDATA[Semi-Supervised Learning]]></summary></entry><entry><title type="html">Transformer</title><link href="https://semyeong-yu.github.io/blog/2024/transformer/" rel="alternate" type="text/html" title="Transformer"/><published>2024-03-01T18:00:00+00:00</published><updated>2024-03-01T18:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/transformer</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/transformer/"><![CDATA[<p>뮌헨공대 (Technical University of Munich)에서 공부한<br/> [IN2375 Computer Vision - Detection, Segmentation and Tracking] 컴퓨터비전 노트 정리</p> <h3 id="ideas-in-transformer">Ideas in Transformer</h3> <blockquote> <p>positional encoding</p> </blockquote> <p>sequential model에서는 위치 정보를 temporal 차원으로 펼쳐서 나타내서 vanishing gradient 등의 문제가 생기는데, transformer는 “positional encoding”이라는 하나의 module에서 위치 정보를 부여할 수 있다!</p> <blockquote> <p>attention</p> </blockquote> <p>input data를 “한 번에 넣어서” 어디에 더 가중치를 부여할 지 계산!</p> <blockquote> <p>multi-head attention</p> </blockquote> <p>attention 구조는 auto-regressive 하지 않으므로 dimension을 쪼개서 “efficient parallel computation”을 가능하도록 하는 구조!</p> <h3 id="attention">Attention</h3> <blockquote> <p>transformer</p> </blockquote> <p>learn non-local (global) info. in the same layer in parallel</p> <blockquote> <p>self-attention</p> </blockquote> <p>\(Q = XW_q\) : 질문</p> <p>\(K = XW_k\) : 답변</p> <p>\(V = XW_v\) : 표현</p> \[Y = softmax(QK^T/sqrt(n))V W_o\] <p>\(softmax(QK^T/sqrt(n))\) : query와 key가 얼마나 유사한지에 대한 attention weight 그 자체 : 어디를 봐야 돼?<br/> \(V\) : weighted-sum을 할 단어 : 어떤 단어가 와야 돼?</p> <p>softmax 안의 값이 너무 커지면 vanishing gradient 문제 발생하므로 sqrt(n)으로 나눔</p> <p>memory complexity : \(O(T^2)\) / runtime complexity : \(O(nT^2)\)</p> <blockquote> <p>multi-head attention</p> </blockquote> <p>Q, K, V의 dimension을 Z개로 쪼개서 따로 attention한 뒤 concat으로 합치고 \(W_o\) 곱함</p> <p>장점 :</p> <ul> <li>쪼개서 attention 여러 번 하므로 more complex interaction 포착 가능</li> <li>runtime complexity는 그대로지만 parallel하게 수행하므로 실질적으론 less computation / faster</li> </ul> <h3 id="transformer">Transformer</h3> <blockquote> <p>ViT (Vision Transformer)</p> </blockquote> <p>for classification</p> <p>extra [class] embedding 추가한 뒤 positional encoding of embedded patches</p> <p>단점 : ViT는 CNN의 inductive bias (locality / 2D neighborhood structure / translation invariance) 없으므로 must be pre-trained on large JFT dataset</p> <blockquote> <p>DETR (Deformable Transformer)</p> </blockquote> <p>for detection</p> <ol> <li> <p>transformer encoder with CNN features</p> </li> <li> <p>transformer decoder with object queries (= learnable positional encoding)</p> </li> <li> <p>predict class &amp; BB in parallel</p> </li> </ol> <p>+) positional encoding은 Q, K에 적용</p> <p>+) decoder에서 object queries는 Q 역할, encoder output은 K, V 역할</p> <p>loss :</p> <ul> <li> <p>BB : no object가 아닌 BB에 대해 (1 - gIoU) + (L1 loss)<br/> +) gIoU : IoU=0이어서 vanishing gradient 문제 발생하는 것 방지</p> </li> <li> <p>class : cross-entropy loss</p> </li> </ul> <p>장점 : accurate / simple / NMS 필요 X<br/> 단점 : computation, memory 소모 / slow</p> <blockquote> <p>Swin Transformer</p> </blockquote> <p>아이디어 : computation 이득을 위해 # of tokens (resolution)을 줄여보자</p> <ol> <li>처음부터 여러 resolution의 hierarchy를 만들자</li> <li>low resolution의 경우 alternate the layout of local attention windows 로 aggregate global context</li> <li>stage 지날 때마다 patch merging 으로 reduce # of tokens</li> </ol> <p>장점 :<br/> efficient, accurate (SOTA)<br/> CNN의 inductive bias + Transformer 이므로 ViT와 달리 미리 pre-train할 필요 X</p> <blockquote> <p>MaskFormer</p> </blockquote> <p>Panoptic FCN처럼 compute kernel(mask), but<br/> Panoptic FCN과 달리<br/> learnable queries with Transformer로 kernel(mask) 계산</p> <p>stuff, thing 이라는 개념 대신 N개의 queries</p> <p>단점 : small object일 때 잘 작동 X</p> <blockquote> <p>Mask2Former</p> </blockquote> <p>pixel decoder에 있는 multiple scale의 feature map을 Transformer decoder에 사용하여 SOTA</p> <blockquote> <p>masked attention</p> </blockquote> <p>N개의 learnable query를 이용해서 predict mask of shape (N, H, W), and apply masked-attention (fg area에 대해서만 attention 하기 위해) : masked localized feature 만으로 attention 해도 충분하고 오히려 더 efficient</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-transformer/img82-480.webp 480w,/assets/img/2024-03-01-transformer/img82-800.webp 800w,/assets/img/2024-03-01-transformer/img82-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-transformer/img82.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-transformer/img87-480.webp 480w,/assets/img/2024-03-01-transformer/img87-800.webp 800w,/assets/img/2024-03-01-transformer/img87-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-transformer/img87.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-transformer/img92-480.webp 480w,/assets/img/2024-03-01-transformer/img92-800.webp 800w,/assets/img/2024-03-01-transformer/img92-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-transformer/img92.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container>]]></content><author><name></name></author><category term="cv-tasks"/><category term="vision"/><category term="transformer"/><summary type="html"><![CDATA[Transformer]]></summary></entry><entry><title type="html">Video Segmentation</title><link href="https://semyeong-yu.github.io/blog/2024/video-segmentation/" rel="alternate" type="text/html" title="Video Segmentation"/><published>2024-03-01T17:00:00+00:00</published><updated>2024-03-01T17:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/video-segmentation</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/video-segmentation/"><![CDATA[<p>뮌헨공대 (Technical University of Munich)에서 공부한<br/> [IN2375 Computer Vision - Detection, Segmentation and Tracking] 컴퓨터비전 노트 정리</p> <h3 id="motion-based-vos">Motion-based VOS</h3> <blockquote> <p>Motion-based VOS</p> </blockquote> <p>장점 : fast, accurate to obtain optical flow<br/> 단점 : optical flow만으로는 occlusion 및 illumination change에 취약</p> <blockquote> <p>assumption</p> </blockquote> <ul> <li>brightness constancy : brightness in the small region remains the same</li> <li>spatial coherence : neighboring points in 2D are in same surface, so have similar 3D moton</li> <li>temporal persistence : motion changes gradually over time</li> </ul> <p>+) assumption 1., 2. 이용해서 classic Lucas-Kanade method로 optical flow motion (u, v) 구할 수 있음</p> <blockquote> <p>FlowNet (optical flow)</p> </blockquote> <ul> <li>architecture : two images를 siamese network에 넣음</li> <li>correlation layer : spatial similarity between two feature maps</li> <li>refinement : deconv. 만 하는 게 아니라 upsampled prediction과 skip-connection을 더함 for high-quality upsampling</li> </ul> <blockquote> <p>SegFlow (joint of optical flow and segment)</p> </blockquote> <p>optical flow output과 segmentation output에 대해 둘 중 하나는 freeze하여 번갈아가며 optimize</p> <h3 id="appearance-only-vos">Appearance-only VOS</h3> <blockquote> <p>Appearance-based VOS</p> </blockquote> <ul> <li>장점 : appearance라 static image로 훈련 가능 / optical flow와 달리 occlusion에 대응 가능 / simple</li> <li>단점 : temporal consistency (X) / slow (model을 each frame에 independently 적용 at test)</li> </ul> <blockquote> <p>Appearance-only VOS</p> </blockquote> <p>아이디어 : model을 each frame에 independently 적용 at test</p> <blockquote> <p>OSVOS (One-shot VOS)</p> </blockquote> <p>test sequence의 “first frame”에 대해 fine-tune 하여 learn which object to segment</p> <p>단점 : drifting (appearance change에 적응 불가능)</p> <blockquote> <p>OnAVOS (Online Adaptation VOS)</p> </blockquote> <p>test sequence의 “every frame”에 대해 fine-tune 하여 online adaptation to appearance change</p> <p>every frame으로부터 pseudo-label을 추출하여 annotated sample에 추가</p> <p>장점 : appearance change에 적응 가능, so more accurate<br/> 단점 : slow / pseudo-label may be inaccurate</p> <blockquote> <p>MaskTrack</p> </blockquote> <p>MOT의 Tracktor와 비슷한 원리</p> <p>perturb mask by affine transformation or non-rigid deformation -&gt; recover original mask</p> <h3 id="metric-based-vos-appearance-based">Metric-based VOS (Appearance-based)</h3> <blockquote> <p>Pixel-wise Retrieval</p> </blockquote> <p>training 필요 없이 unsupervised method로 fast</p> <p>triplet loss로 fg pixel끼리는 가까이, bg pixel은 멀리 있도록 learn pixel-level embedding</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-video-segmentation/img72-480.webp 480w,/assets/img/2024-03-01-video-segmentation/img72-800.webp 800w,/assets/img/2024-03-01-video-segmentation/img72-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-video-segmentation/img72.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-video-segmentation/img77-480.webp 480w,/assets/img/2024-03-01-video-segmentation/img77-800.webp 800w,/assets/img/2024-03-01-video-segmentation/img77-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-video-segmentation/img77.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-video-segmentation/img82-480.webp 480w,/assets/img/2024-03-01-video-segmentation/img82-800.webp 800w,/assets/img/2024-03-01-video-segmentation/img82-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-video-segmentation/img82.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container>]]></content><author><name></name></author><category term="cv-tasks"/><category term="vision"/><category term="video"/><category term="segmentation"/><summary type="html"><![CDATA[vos(video object segmentation)]]></summary></entry><entry><title type="html">Image Segmentation</title><link href="https://semyeong-yu.github.io/blog/2024/image-segmentation/" rel="alternate" type="text/html" title="Image Segmentation"/><published>2024-03-01T16:00:00+00:00</published><updated>2024-03-01T16:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/image-segmentation</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/image-segmentation/"><![CDATA[<p>뮌헨공대 (Technical University of Munich)에서 공부한</p> <p>[IN2375 Computer Vision - Detection, Segmentation and Tracking] 컴퓨터비전 노트 정리</p> <h3 id="semantic-segmentation">Semantic Segmentation</h3> <blockquote> <p>K-means</p> </blockquote> <blockquote> <p>Spectral Clustering</p> </blockquote> <p>A : N x N similarity matrix<br/> \(D_{ii}\) = \(sum_j\)(\(a_{ij}\))<br/> L = D - A</p> <p>L의 eigenvectors K개를 column으로 갖는 U에 대해 U의 rows를 data points로 보고 clustering</p> <blockquote> <p>CRF (Conditional Random Fields)</p> </blockquote> <p>useful post-processing tool (segmentation boundary 향상)</p> <ul> <li>unary term : K[\(x_i\) != \(y_i\)]</li> <li>pairwise term : [\(x_i\) != \(x_j\)]\(w_{ij}\)</li> </ul> <p>max-flow min-cut theorem으로 clustering</p> <p>단점 : unbalanced isolated clusters</p> <blockquote> <p>Ncut (Normalized cut)</p> </blockquote> <p>minimize similarity b.w. different groups 뿐만 아니라 maximize similarity within each group까지 고려</p> <blockquote> <p>FCN</p> </blockquote> <p>마지막에 fc layer (단점 : parameter 수 많고, fixed input size, no inductive bias) 대신</p> <p>1 x 1 conv로 semantic segmentation</p> <blockquote> <p>high resolution</p> </blockquote> <p>장점 : high segmentation accuracy</p> <p>단점 :</p> <ul> <li>receptive field size 작음 -&gt; dilated convolution으로 보완 (skip N-1 pixels in between, and then receptive field size = N(K-1)+1)</li> <li>memory, FLOP 많이 소요 -&gt; encoder - decoder 구조로 보완 (skip-connection 쓰면 better upsampling quality)</li> </ul> <blockquote> <p>upsampling = transposed convolution = up-convolution</p> </blockquote> <p>!= deconvolution</p> <p>z = s - 1개의 0을 각 row, column 사이에 삽입 (unpooling)<br/> p’ = k - p - 1개만큼 padding<br/> s’ = 1으로 convolution</p> <p>문제 : checkboard artifacts -&gt; 해결 : kernel size와 stride를 잘 선택 또는 interpolation(resize)한 뒤 conv</p> <h3 id="instance-segmentation">Instance Segmentation</h3> <blockquote> <p>proposal-based (Mask R-CNN)</p> </blockquote> <p>object detection -&gt; segment and classify</p> <p>segmentation mask는 object detection보다 localization이 정확해야 하므로 RoI Pool 대신 RoI Align</p> <ul> <li>RoI Pool : BB alignment -&gt; bin alignment -&gt; SPP</li> <li>RoI Align : bilinear interpolation으로 feature value 계산</li> </ul> <p>+) Mask R-CNN 개선 by PointRend :<br/> bilinear upsampling 단점 : computation 많음 / boundary에서 fine detail 떨어짐</p> <p>-&gt; 해결 : bilinear sampling (discrete pixel -&gt; value) 대신<br/> trained network (continuous (x,y) -&gt; value)로 refine boundaries</p> <blockquote> <p>proposal-free (SOLOv2)</p> </blockquote> <p>semantic segmentation -&gt; group pixels</p> <p>instance 개수 몇 개일지 모르므로 predict the kernels G : S x S x D</p> <h3 id="panoptic-segmentation">Panoptic Segmentation</h3> <blockquote> <p>proposal-based (Panoptic FPN)</p> </blockquote> <ol> <li>FPN</li> <li>decoder CNN (semantic segmentation) 및 Mask R-CNN (instance segmentation)</li> <li>합침</li> </ol> <p>합칠 때 thing 우선이므로</p> <p>사실은 thing이라서 “other”로 labelling된 stuff 또는 small region stuff는 제거</p> <p>instance(class / BB reg / mask) / semantic 으로 총 4가지의 loss term</p> <blockquote> <p>proposal-free (Panoptic FCN)</p> </blockquote> <p>SOLOv2에서처럼 predict the kernels</p> <ol> <li>FPN</li> <li>position head (\(N_{thing} x H_i x W_i\) / \(N_{stuff} x H_i x W_i\)) 및 kernel head (\(C_{in} x Hi x Wi\))로 kernel fusion \((N_{thing}+N_{stuff}) x C_in x 1 x 1\)</li> </ol> <blockquote> <p>Panoptic Segmentation Evaluation</p> </blockquote> <p>PQ = SQ x RQ = MOTP x F1-score<br/> under unique matching theorem</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-image-segmentation/img52-480.webp 480w,/assets/img/2024-03-01-image-segmentation/img52-800.webp 800w,/assets/img/2024-03-01-image-segmentation/img52-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-image-segmentation/img52.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-image-segmentation/img57-480.webp 480w,/assets/img/2024-03-01-image-segmentation/img57-800.webp 800w,/assets/img/2024-03-01-image-segmentation/img57-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-image-segmentation/img57.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-image-segmentation/img62-480.webp 480w,/assets/img/2024-03-01-image-segmentation/img62-800.webp 800w,/assets/img/2024-03-01-image-segmentation/img62-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-image-segmentation/img62.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-image-segmentation/img67-480.webp 480w,/assets/img/2024-03-01-image-segmentation/img67-800.webp 800w,/assets/img/2024-03-01-image-segmentation/img67-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-image-segmentation/img67.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container>]]></content><author><name></name></author><category term="cv-tasks"/><category term="vision"/><category term="segmentation"/><summary type="html"><![CDATA[Segmentation]]></summary></entry><entry><title type="html">Object Tracking</title><link href="https://semyeong-yu.github.io/blog/2024/object-tracking/" rel="alternate" type="text/html" title="Object Tracking"/><published>2024-03-01T15:00:00+00:00</published><updated>2024-03-01T15:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/object-tracking</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/object-tracking/"><![CDATA[<p>뮌헨공대 (Technical University of Munich)에서 공부한<br/> [IN2375 Computer Vision - Detection, Segmentation and Tracking] 컴퓨터비전 노트 정리</p> <h3 id="general-bayesian-framework">General Bayesian Framework</h3> <blockquote> <p>setting</p> </blockquote> <p>\(x_k\) : internal state, hidden random var. e.g. BB</p> <p>\(z_k\) : measurement, observable random var. e.g. image data</p> <p>\(X_k\) = [\(x_1\), …, \(x_k\)]</p> <p>\(Z_k\) = [\(z_1\), …, \(z_k\)]</p> <blockquote> <p>assumptions</p> </blockquote> <p>likelihood p(\(z_k \vert x_k\))는 \(Z_{k-1}\) 에 무관</p> <p>temporal prior p(\(x_k \vert x_{k-1}\))는 \(Z_{k-1}\) 또는 \(X_{k-2}\) 에 무관</p> <blockquote> <p>Bayesian framework</p> </blockquote> <p>아래 사진에 있는 식과 같이 posteior p($x_k$ $\vert$ $Z_k$)를 recursively 구할 수 있음</p> <p>posterior mean = E(\(x_k \vert Z_k\))</p> <p>MAP(maximum a posterior) = \(argmax_{x_k} (p(x_k \vert Z_k))\)</p> <p>+) deep learning : learn MAP directly</p> <ul> <li>online tracking : computational overhead / drifting에 취약</li> <li>offline tracking : real-time (X) / 새로운 frame에 적응 (X)</li> </ul> <h3 id="single-object-tracking-online">Single Object Tracking (online)</h3> <blockquote> <p>GOTURN</p> </blockquote> <p>assumption : MAP at frame k approximates to \(x_k\)<br/> (즉, use position of BB in frame k-1 to crop in frame k)</p> <p>장점 : simple -&gt; real-time / end-to-end (can use large data)<br/> 단점 : simple temporal prior -&gt; one object possible / fast motion or occlusion (X) / template에 의존</p> <blockquote> <p>MDNet</p> </blockquote> <p>아이디어 : 매 training sequence(task)마다 domain-specific fc layers 따로 만들면 inefficient하므로 online adaptable fc layer 사용하여 appearance change에 적응</p> <p>architecture : 매 frame마다 아래의 과정 반복<br/> candidates -&gt; R-CNN으로 optimal state 찾기 -&gt; bias 추가로 collect pos/neg samples -&gt; fine-tuning으로 update fc layer</p> <p>장점 : fine-tuning / online adaptable appearance model이므로 change in appearance에 적응</p> <p>단점 : slow / strong assumption on temporal prior</p> <h3 id="multiple-object-tracking-online">Multiple Object Tracking (online)</h3> <blockquote> <p>Match</p> </blockquote> <p>predict next position using motion model (Kalman filter / LSTM, GRU)<br/> match (Hungarian matching O(N^3))</p> <blockquote> <p>Refine (Tracktor)</p> </blockquote> <p>매 frame마다 아래의 과정 반복<br/> copy boxes to frame k -&gt; regression (refine boxes) &amp; classification (kill if low conf) &amp; detection (find new BB)</p> <p>장점 : object detector 재사용 가능 / still image로 훈련된 object detector여도 괜찮 / regression is agnostic to ID or class</p> <p>단점 :</p> <ul> <li>no motion model, so fast motion or occlusion에 취약 -&gt; long term memory of tracks로 보완</li> <li>identity 없으므로 crowded space에 취약</li> </ul> <blockquote> <p>Metric learning for Re-ID</p> </blockquote> <p>small motion assumption이 필요한 motion model (IoU) 말고 more robust “appearance” model을 만들어보자!</p> <p>contrastive learning : positive &amp; negative pair 만들어서 siamese network를 통해 hinge 또는 triplet loss</p> <h3 id="multiple-object-tracking-offline--graph-based">Multiple Object Tracking (offline) : Graph-based</h3> <blockquote> <p>setting</p> </blockquote> <ul> <li> <p>goal : find maximum flow with minimum cost</p> </li> <li> <p>Bayesian framework로부터 cost 유도하기 : 아래 사진 참고<br/> likelihood -&gt; detection cost<br/> prior -&gt; entrance / transition / exit cost</p> </li> <li> <p>Markov formulation은 occlusion 설명 불가능 / not end-to-end</p> </li> </ul> <blockquote> <p>Message Passing Network</p> </blockquote> <ul> <li> <p>initialization :<br/> node : from BB<br/> edge : from BB coordinates 차이 및 reid distance 차이</p> </li> <li> <p>neural message passing :<br/> node-to-edge :<br/> (\(node_i\) at k-1), (\(node_j\) at k-1), (\(edge_{ij}\) at k-1), (\(edge_{ij}\) at 0) -&gt; \(edge_{ij}\) at k</p> </li> <li> <p>edge-to-node :<br/> (\(edge_{ij}\) at k for every neighbor j), (\(node_i\) at k-1) -&gt; \(node_i\) at k<br/> 이 때, node-permutation-invariant 이므로 set or sum 사용</p> </li> <li> <p>edge classification : 단순히 thresholding 뿐만 아니라 post-processing 필요</p> </li> <li> <p>loss :<br/> 아래 사진에서 w &gt; 1 이면 gt = 1 (active adge)일 때 틀리면 안 된다는 의미이므로 penalize FN</p> </li> </ul> <p>장점 : handle occlusions / end-to-end (cost is learned) / graph structure 자유 / SOTA for offline</p> <blockquote> <p>Multiple Object Tracking Evaluation</p> </blockquote> <p>MOTA / F1-score / MOTP</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-object-tracking/img27-480.webp 480w,/assets/img/2024-03-01-object-tracking/img27-800.webp 800w,/assets/img/2024-03-01-object-tracking/img27-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-object-tracking/img27.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-object-tracking/img32-480.webp 480w,/assets/img/2024-03-01-object-tracking/img32-800.webp 800w,/assets/img/2024-03-01-object-tracking/img32-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-object-tracking/img32.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-object-tracking/img37-480.webp 480w,/assets/img/2024-03-01-object-tracking/img37-800.webp 800w,/assets/img/2024-03-01-object-tracking/img37-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-object-tracking/img37.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-object-tracking/img42-480.webp 480w,/assets/img/2024-03-01-object-tracking/img42-800.webp 800w,/assets/img/2024-03-01-object-tracking/img42-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-object-tracking/img42.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-object-tracking/img47-480.webp 480w,/assets/img/2024-03-01-object-tracking/img47-800.webp 800w,/assets/img/2024-03-01-object-tracking/img47-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-object-tracking/img47.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container>]]></content><author><name></name></author><category term="cv-tasks"/><category term="vision"/><category term="tracking"/><summary type="html"><![CDATA[Tracking]]></summary></entry><entry><title type="html">Object Detection</title><link href="https://semyeong-yu.github.io/blog/2024/object-detection/" rel="alternate" type="text/html" title="Object Detection"/><published>2024-03-01T14:00:00+00:00</published><updated>2024-03-01T14:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/object-detection</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/object-detection/"><![CDATA[<p>뮌헨공대 (Technical University of Munich)에서 공부한<br/> [IN2375 Computer Vision - Detection, Segmentation and Tracking] 컴퓨터비전 노트 정리</p> <h3 id="old-approach">Old Approach</h3> <blockquote> <p>Template matching with sliding window</p> </blockquote> <p>loss : MSE(SSD) / NCC / ZNCC (difference between ‘image itself’ and template)</p> <p>단점 : self-occlusions / change in appearance / change in position / change in scale or aspect ratio</p> <blockquote> <p>Viola-Jones detector (1-stage)</p> </blockquote> <blockquote> <p>HOG (Histogram of Oriented Gradients) (1-stage)</p> </blockquote> <blockquote> <p>Proposals(RoIs) by selective search or edge box (2-stage)</p> </blockquote> <p>+) NMS에서 \(b_i\)와 비슷한 \(b_j\)들에 대해 confidence score를 비교하여 \(b_i\) 제거 여부를 결정하는데, 만약 \(IoU_{threshold}\)를 넘겨야 비교 후 제거 (N) 가능하다면 high \(IoU_{threshold}\) -&gt; less FN, more FP</p> <h3 id="detection-evaluation">Detection Evaluation</h3> <p>TP : positive(BB)라고 예측했는데 맞았다(true)<br/> FN : negative(no BB)라고 예측했는데 틀렸다(false)</p> <p>precision / recall / F1-score</p> <p>confindence score 순으로 P를 정렬한 뒤 AP(average precision)<br/> 여러 object category에 대해 평균 낸 게 mAP</p> <h3 id="r-cnn-2-stage">R-CNN (2-stage)</h3> <blockquote> <p>R-CNN : extract RoI -&gt; crop&amp;warp -&gt; CNN -&gt; SVM &amp; BB reg</p> </blockquote> <p>장점 : CNN / transfer learning</p> <p>단점 :</p> <ul> <li> <p>slow (~2k proposals per image are warped and forwarded each through CNN)<br/> -&gt; Fast R-CNN에서 SPP로 해결</p> </li> <li> <p>object proposal algorithm is flixed<br/> -&gt; Faster R-CNN에서 RPN으로 해결</p> </li> <li> <p>not end-to-end (CNN and SVM &amp; BB reg are trained separately)<br/> -&gt; Faster R-CNN에서 RPN으로 해결</p> </li> </ul> <blockquote> <p>Fast R-CNN : CNN -&gt; extract RoI -&gt; SPP (RoI Pooling) -&gt; fc -&gt; classifier &amp; BB reg</p> </blockquote> <p>SPP (= Spatial Pyramid Pooling) : fc layer 직전에 배치하면 any input size 가능</p> <p>R-CNN의 단점 1. 만 해결</p> <blockquote> <p>Faster R-CNN : CNN -&gt; RPN (loss 1., 2.) -&gt; RoI Pooling -&gt; fc -&gt; classifier &amp; BB reg (loss 3., 4.)</p> </blockquote> <p>RPN (= Region Proposal Network) : output shape (H, W, 5n)</p> <ul> <li>n anchors per location</li> <li>1 confidence score</li> <li>4 normalized anchor coordinates</li> </ul> <p>R-CNN의 단점 1., 2., 3. 모두 해결</p> <blockquote> <p>FPN (= Feature Pyramid Network) :</p> </blockquote> <p>define RPN on each level of FPN<br/> scale variance 문제 해소<br/> high scale pyramid에서 small object까지 detect하므로 more TP, FP<br/> But, 단점 : model complexity</p> <h3 id="1-stage">1-stage</h3> <blockquote> <p>YOLO (= You Only Look Once) : Faster R-CNN의 loss 3., 4.를 loss 1., 2.에 합치자!</p> </blockquote> <p>output shape (H, W, 5n) 대신 (S, S, (5+C)n)</p> <p>장점 : efficient, faster<br/> 단점 : less accurate (coarse grid resolution)<br/> single scale (small object detect 불가능, scale variation에 취약)</p> <blockquote> <p>SSD (= Single Shot Multibox Detector) : multi-scale을 사용하자!</p> </blockquote> <p>장점 : YOLO 단점 해결<br/> 단점 : still less accurate than two-stage detectors due to class imbalance<br/> data augmentation 중요</p> <blockquote> <p>class imbalance 문제 :</p> </blockquote> <p>two-stage detector의 경우 first stage에서 미리 negative anchor를 대부분 걸러낼 수 있지만<br/> one-stage detector는 그렇지 않아서 class imbalance 문제 발생</p> <p>대안 :</p> <ul> <li>hard negative mining : FP 오류 줄이기 위해 어려웠던 sample들 추가</li> <li>focal loss = $-(1-p)^r$ * $log(p)$ : 많이 존재하는 easy example(p ~ 1)은 $(1-p)^r$로 영향 작게 만들고, 적게 존재하는 hard example(p ~ 0)에 가중을 둠</li> </ul> <blockquote> <p>RetinaNet : 기존 1-stage 방법 + multi-scale(FPN) + focal loss</p> </blockquote> <blockquote> <p>accuracy : YOLO &lt; SSD &lt; two-stage detector &lt; RetinaNet</p> </blockquote> <blockquote> <p>spatial transformer :</p> </blockquote> <p>grid generator로 sampling with bilinear interpolation<br/> = localisation &amp; certain transformation</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-object-detection/img7-480.webp 480w,/assets/img/2024-03-01-object-detection/img7-800.webp 800w,/assets/img/2024-03-01-object-detection/img7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-object-detection/img7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-object-detection/img12-480.webp 480w,/assets/img/2024-03-01-object-detection/img12-800.webp 800w,/assets/img/2024-03-01-object-detection/img12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-object-detection/img12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-object-detection/img17-480.webp 480w,/assets/img/2024-03-01-object-detection/img17-800.webp 800w,/assets/img/2024-03-01-object-detection/img17-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-object-detection/img17.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-object-detection/img22-480.webp 480w,/assets/img/2024-03-01-object-detection/img22-800.webp 800w,/assets/img/2024-03-01-object-detection/img22-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-object-detection/img22.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-object-detection/img27-480.webp 480w,/assets/img/2024-03-01-object-detection/img27-800.webp 800w,/assets/img/2024-03-01-object-detection/img27-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-object-detection/img27.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container>]]></content><author><name></name></author><category term="cv-tasks"/><category term="vision"/><category term="detection"/><summary type="html"><![CDATA[Detection]]></summary></entry><entry><title type="html">Quantum Computing</title><link href="https://semyeong-yu.github.io/blog/2024/quantum-computing/" rel="alternate" type="text/html" title="Quantum Computing"/><published>2024-02-29T00:00:00+00:00</published><updated>2024-02-29T00:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/quantum-computing</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/quantum-computing/"><![CDATA[<p>뮌헨공대 (Technical University of Munich)에서 공부한</p> <p>[IN2381 Introduction to Quantum Computing] 양자컴퓨팅 노트 정리</p> <p><a href="../assets/file/2024-02-29-quantum-computing/lecture.pdf">lecture.pdf</a></p> <p><a href="../assets/file/2024-02-29-quantum-computing/exercise.pdf">exercise.pdf</a></p>]]></content><author><name></name></author><category term="quantum-computing"/><category term="quantum"/><category term="bloch"/><category term="qubit"/><summary type="html"><![CDATA[Introduction to Quantum Computing]]></summary></entry><entry><title type="html">vega lite</title><link href="https://semyeong-yu.github.io/blog/2024/vega-lite/" rel="alternate" type="text/html" title="vega lite"/><published>2024-01-27T21:00:00+00:00</published><updated>2024-01-27T21:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/vega-lite</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/vega-lite/"><![CDATA[<p>This is an example post with some <a href="https://vega.github.io/vega-lite/">vega lite</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">vega_lite
</span><span class="sb">{
  "$schema": "https://vega.github.io/schema/vega-lite/v5.json",
  "description": "A dot plot showing each movie in the database, and the difference from the average movie rating. The display is sorted by year to visualize everything in sequential order. The graph is for all Movies before 2019.",
  "data": {
    "url": "https://raw.githubusercontent.com/vega/vega/main/docs/data/movies.json"
  },
  "transform": [
    {"filter": "datum['IMDB Rating'] != null"},
    {"filter": {"timeUnit": "year", "field": "Release Date", "range": [null, 2019]}},
    {
      "joinaggregate": [{
        "op": "mean",
        "field": "IMDB Rating",
        "as": "AverageRating"
      }]
    },
    {
      "calculate": "datum['IMDB Rating'] - datum.AverageRating",
      "as": "RatingDelta"
    }
  ],
  "mark": "point",
  "encoding": {
    "x": {
      "field": "Release Date",
      "type": "temporal"
    },
    "y": {
      "field": "RatingDelta",
      "type": "quantitative",
      "title": "Rating Delta"
    },
    "color": {
      "field": "RatingDelta",
      "type": "quantitative",
      "scale": {"domainMid": 0},
      "title": "Rating Delta"
    }
  }
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-vega_lite">{
  "$schema": "https://vega.github.io/schema/vega-lite/v5.json",
  "description": "A dot plot showing each movie in the database, and the difference from the average movie rating. The display is sorted by year to visualize everything in sequential order. The graph is for all Movies before 2019.",
  "data": {
    "url": "https://raw.githubusercontent.com/vega/vega/main/docs/data/movies.json"
  },
  "transform": [
    {"filter": "datum['IMDB Rating'] != null"},
    {"filter": {"timeUnit": "year", "field": "Release Date", "range": [null, 2019]}},
    {
      "joinaggregate": [{
        "op": "mean",
        "field": "IMDB Rating",
        "as": "AverageRating"
      }]
    },
    {
      "calculate": "datum['IMDB Rating'] - datum.AverageRating",
      "as": "RatingDelta"
    }
  ],
  "mark": "point",
  "encoding": {
    "x": {
      "field": "Release Date",
      "type": "temporal"
    },
    "y": {
      "field": "RatingDelta",
      "type": "quantitative",
      "title": "Rating Delta"
    },
    "color": {
      "field": "RatingDelta",
      "type": "quantitative",
      "scale": {"domainMid": 0},
      "title": "Rating Delta"
    }
  }
}
</code></pre> <p>This plot supports both light and dark themes.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="charts"/><summary type="html"><![CDATA[this is what included vega lite code could look like]]></summary></entry></feed>