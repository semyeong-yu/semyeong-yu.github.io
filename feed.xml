<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://semyeong-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://semyeong-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-26T02:06:48+00:00</updated><id>https://semyeong-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Diffusion</title><link href="https://semyeong-yu.github.io/blog/2024/Diffusion/" rel="alternate" type="text/html" title="Diffusion"/><published>2024-06-25T15:00:00+00:00</published><updated>2024-06-25T15:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Diffusion</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Diffusion/"><![CDATA[<h2 id="diffusion">Diffusion</h2> <h4 id="diffusion-model">Diffusion Model</h4> <ul> <li>forward process : <code class="language-plaintext highlighter-rouge">fixed</code> Gaussian noise 더함</li> <li>reverse process : <code class="language-plaintext highlighter-rouge">learned</code> Gaussian noise 뺌 (mean, std를 학습)</li> </ul> <h4 id="likelihood">Likelihood</h4> <p>아래 둘 다 관측값 \(x\)가 나올 확률인데,</p> <ul> <li><code class="language-plaintext highlighter-rouge">probability</code> \(P(x | \theta)\) : <code class="language-plaintext highlighter-rouge">확률 분포가 고정</code>된 상태에서, <code class="language-plaintext highlighter-rouge">관측되는 사건이 변화</code>할 때의 확률<br/> 예: 선택 가능한 정수를 1~5로 제한(확률 분포 고정)했을 때, 관측 목표값이 1~5 중 한 개의 숫자(관측 사건 변화)일 경우</li> <li><code class="language-plaintext highlighter-rouge">likelihood</code> \(L(\theta | x)\) : <code class="language-plaintext highlighter-rouge">관측된 사건이 고정</code>된 상태에서, <code class="language-plaintext highlighter-rouge">확률 분포 몰라서 가정</code>할 때의 확률<br/> 예: 선택 가능한 정수를 1~5가 아니라 1~10 또는 4~50으로 바꾸면서(확률 분포 모름), 2가 관측될 확률을 계산(관측 사건 고정)할 경우<br/> 예: 어떤(모르는) 확률 분포를 따르는 task를 n회 반복 수행하여 관측했을 때 random var. 종류를 가정할 수도 있고 특정 random var.의 parameter를 가정할 수도 있다</li> </ul> <h4 id="markov-process">Markov process</h4> <ul> <li><code class="language-plaintext highlighter-rouge">Markov</code> process (= Markov chain = <code class="language-plaintext highlighter-rouge">memoryless</code> process) : Markov property를 가지는 discrete stochastic process<br/> \(P[s_{t+1}|s_t] = P[s_{t+1}|s_1, \ldots, s_t]\)</li> </ul> <h4 id="kl-divergence">KL-divergence</h4> <p>\(H(p, q) = - \sum p_i log q_i\) : 두 확률분포 p, q의 cross entropy<br/> (보통 \(p\)는 GT, \(q\)는 predicted)<br/> \(H(p) = - \sum p_i log p_i\) : p’s entropy (상수값)<br/> \(KL(p \| q) = H(p, q) - H(p) = \sum p_i log \frac{p_i}{q_i}\) : 두 확률분포 p, q의 차이<br/> \(H(p)\)는 상수값이므로 <code class="language-plaintext highlighter-rouge">KL-divergence minimize = cross entropy minimize</code><br/> \(KL(p \| q) \simeq \frac{1}{N} \sum_{n=1}^{N} {-log q(x_n | \theta) + log p(x_n)}\) :<br/> \(log p(x_n)\)은 \(\theta\)에 독립이므로 <code class="language-plaintext highlighter-rouge">KL-divergence minimize = negative log likelihood minimize = MLE</code></p> <p>KL-diverence 특성 :</p> <ol> <li>\(KL(p \| q) \geq 0\) : p = q일 때 최소</li> <li>\(KL(p \| q) \neq KL(q \| p)\) (asymmetric) : 거리 개념이 아님<br/> 거리 개념으로 쓰는 방법 : 2가지 KL-divergence를 평균내는 방식의 \(JSD(p \| q)\)</li> </ol> <h4 id="diffusion-algorithm">Diffusion Algorithm</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-06-25-Diffusion/1-480.webp 480w,/assets/img/2024-06-25-Diffusion/1-800.webp 800w,/assets/img/2024-06-25-Diffusion/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-06-25-Diffusion/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>forward process : \(q(X_t | X_{t-1}) = N(X_t ; \mu_{X_{t-1}}, \Sigma_{X_{t-1}}) = N(X_t ; \sqrt{1-\beta_t} \cdot X_{t-1}, \beta_t \cdot I)\)<br/> where \(\beta_t\) : noise 주입 정도 (상수값)<br/> t가 증가하면 \(\beta_t\)가 증가하여 다른 pixel(\(I\))을 선택하므로 noise가 강해진다</p> </li> <li> <table> <tbody> <tr> <td>backward process : image prior \(q(X_t)\)를 모르기 때문에 $$q(X_{t-1}</td> <td>X_t)$$를 계산할 수 없으므로</td> <td> </td> </tr> <tr> <td>Goal : $$q(X_{t-1}</td> <td>X_t)\(에 근사하는\)p_{\theta}(X_{t-1}</td> <td>X_t)$$ 학습</td> </tr> <tr> <td>즉, 확률분포 \(q\)에서 관측한 값 \(x\)로 likelihood $$p_{theta</td> <td>x}$$를 구했을 때 그 값이 최대가 되도록 하는 MLE Problem</td> <td> </td> </tr> </tbody> </table> </li> </ul> <table> <tbody> <tr> <td>$$E_q [D_{KL}(q(x_T</td> <td>x_0) | p(x_T)) + \sum_{t \gt 1} D_{KL}(q(x_{t-1}</td> <td>x_t, x_0) | p(x_{t-1} | x_t)) - log p_{\theta} (x_0</td> <td>x_1)]$$</td> </tr> <tr> <td>$$L_T = D_{KL}(q(x_T</td> <td>x_0) | p(x_T))$$ : ddd</td> <td> </td> <td> </td> </tr> <tr> <td>$$L_{t-1} = D_{KL}(q(x_{t-1}</td> <td>x_t, x_0) | p(x_{t-1} | x_t))$$ : ddd</td> <td> </td> <td> </td> </tr> <tr> <td>$$L_0 = - log p_{\theta} (x_0</td> <td>x_1)$$ : ddd</td> <td> </td> <td> </td> </tr> </tbody> </table> <blockquote> <p>출처 블로그 :<br/> <a href="https://xoft.tistory.com/32">Diffusion Model</a><br/> <a href="https://xoft.tistory.com/33?category=1156151">DDPM 수식 유도</a></p> </blockquote>]]></content><author><name></name></author><category term="generative"/><category term="diffusion"/><category term="generative"/><summary type="html"><![CDATA[Diffusion Study]]></summary></entry><entry><title type="html">NeRF-Code</title><link href="https://semyeong-yu.github.io/blog/2024/NeRFcode/" rel="alternate" type="text/html" title="NeRF-Code"/><published>2024-06-23T15:00:00+00:00</published><updated>2024-06-23T15:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/NeRFcode</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/NeRFcode/"><![CDATA[<h2 id="nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h2> <h4 id="ben-mildenhall-pratul-psrinivasan-matthew-tancik">Ben Mildenhall, Pratul P.Srinivasan, Matthew Tancik</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2003.08934">https://arxiv.org/abs/2003.08934</a><br/> project website :<br/> <a href="https://www.matthewtancik.com/nerf">https://www.matthewtancik.com/nerf</a><br/> pytorch code :<br/> <a href="https://github.com/yenchenlin/nerf-pytorch">https://github.com/yenchenlin/nerf-pytorch</a><br/> <a href="https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file">https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file</a><br/> tiny tensorflow code :<br/> <a href="https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb">https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb</a></p> </blockquote> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_rays_np</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">c2w</span><span class="p">):</span>
    <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">W</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">W</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">H</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">H</span><span class="p">))</span> <span class="c1"># pytorch's meshgrid has indexing='ij', so both i and j have shape (W, H)
</span>    <span class="n">i</span> <span class="o">=</span> <span class="n">i</span><span class="p">.</span><span class="nf">t</span><span class="p">()</span> <span class="c1"># width grid : shape (H, W)
</span>    <span class="n">j</span> <span class="o">=</span> <span class="n">j</span><span class="p">.</span><span class="nf">t</span><span class="p">()</span> <span class="c1"># height grid : shape (H, W)
</span>
    <span class="c1"># Apply intrinsic matrix
</span>    <span class="n">dirs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([(</span><span class="n">i</span><span class="o">-</span><span class="n">K</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span><span class="o">/</span><span class="n">K</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="p">(</span><span class="n">j</span><span class="o">-</span><span class="n">K</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span> <span class="o">/</span> <span class="n">K</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">i</span><span class="p">)],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># dirs : shape (H, W, 3) : H*W개의 3D rays
</span>    
    <span class="c1"># Apply extrinsic matrix
</span>    <span class="c1"># Rotate ray directions from camera frame to the world frame by applying dot product
</span>    <span class="n">rays_d</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dirs</span><span class="p">[...,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">c2w</span><span class="p">[:</span><span class="mi">3</span><span class="p">,</span> <span class="p">:</span><span class="mi">3</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># same with "rays_d = [c2w.dot(dir) for dir in dirs]"
</span>    <span class="c1"># dirs[..., np.newaxis, :] : shape (H, W, 1, 3) -&gt; (H, W, 3, 3) by broadcasting 
</span>    <span class="c1"># c2w[:3, :3] : shape (3, 3) -&gt; (H, W, 3, 3) by broadcasting
</span>    <span class="c1"># rays_d : shape (H, W, 3)
</span>    
    <span class="c1"># Translate camera frame's origin to the world frame. It is the origin of all rays.
</span>    <span class="n">rays_o</span> <span class="o">=</span> <span class="n">c2w</span><span class="p">[:</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">expand</span><span class="p">(</span><span class="n">rays_d</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="c1"># rays_o : shape (3, H*W) -&gt; (H, W, 3)
</span>    <span class="k">return</span> <span class="n">rays_o</span><span class="p">,</span> <span class="n">rays_d</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="rendering"/><category term="3d"/><summary type="html"><![CDATA[NeRF Code Review]]></summary></entry><entry><title type="html">MipNeRF</title><link href="https://semyeong-yu.github.io/blog/2024/MipNeRF/" rel="alternate" type="text/html" title="MipNeRF"/><published>2024-06-17T21:00:00+00:00</published><updated>2024-06-17T21:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/MipNeRF</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/MipNeRF/"><![CDATA[<h2 id="mip-nerf-a-multiscale-representation-for-anti-aliasing-neural-radiance-fields">Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</h2> <h4 id="jonathan-t-barron-ben-mildenhall-matthew-tancik-peter-hedman-ricardo-martin-brualla-pratul-p-srinivasan">Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2103.13415">https://arxiv.org/abs/2103.13415</a><br/> project website :<br/> <a href="https://jonbarron.info/mipnerf/">https://jonbarron.info/mipnerf/</a><br/> pytorch code :<br/> <a href="https://github.com/bebeal/mipnerf-pytorch">https://github.com/bebeal/mipnerf-pytorch</a><br/> <a href="https://github.com/google/mipnerf">https://github.com/google/mipnerf</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>Mip-NeRF는 training하는 동안, <code class="language-plaintext highlighter-rouge">임의의 scale</code>에 대해 query 받을 수 있는, scene의 <code class="language-plaintext highlighter-rouge">anti-aliased</code> <code class="language-plaintext highlighter-rouge">pre-filtered representation</code>을 학습한다.</li> <li>camera center로부터 각 pixel로 3D conical frustum을 쏜 다음, the <code class="language-plaintext highlighter-rouge">frustum을 multi-variate Gaussian으로 근사</code>한 뒤, Gaussian 내 좌표를 positional encoding한 것에 대해 <code class="language-plaintext highlighter-rouge">integral</code> \(E \left[ \gamma (x) \right]\) 계산</li> </ol> </blockquote> <h2 id="introduction">Introduction</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-06-17-MipNeRF/2-480.webp 480w,/assets/img/2024-06-17-MipNeRF/2-800.webp 800w,/assets/img/2024-06-17-MipNeRF/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-06-17-MipNeRF/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>기존 NeRF의 문제점 :<br/> rendering 위해 sampling할 때 <code class="language-plaintext highlighter-rouge">single ray</code> per pixel 쏴서 composite 하므로<br/> dataset images에 있는 물체의 크기가 일정하지 않을 때 (multiple resolutions) multi-scales images에 대해 학습하더라도<br/> high-resolution은 <code class="language-plaintext highlighter-rouge">blurry</code> rendering<br/> low-resolution은 <code class="language-plaintext highlighter-rouge">aliased</code> rendering<br/> 그렇다고 multiple rays per pixel through its footprint로 brute-force supersampling하는 것은 정확하긴 하겠지만 too costly 비현실적</p> </li> <li> <p>Minmapping Approach :<br/> classic 컴퓨터 그래픽스 분야에서 rendering할 때 aliasing을 없애기 위한 <code class="language-plaintext highlighter-rouge">pre-filtering</code> 기법<br/> 본 논문인 Mip-NeRF가 여기서 영감을 얻음<br/> signal(e.g. image)을 diff. downsampling scales로 나타낸 뒤 pixel footprint를 근거로 ray에 사용하기 위한 <code class="language-plaintext highlighter-rouge">적절한 scale을 고른다</code><br/> render time에 할 복잡할 일을 precomputation phase에 미리 하는 것일 뿐이긴 하지만, 주어진 texture마다 한 번만 minmap을 만들면 된다는 장점이 있다</p> </li> <li> <p>Mip-NeRF :</p> <ul> <li>represent pre-filtered scene at <code class="language-plaintext highlighter-rouge">continuous space of scales</code></li> <li>ray 대신 <code class="language-plaintext highlighter-rouge">conical frustum</code> 사용해서 <code class="language-plaintext highlighter-rouge">anti-aliased</code> rendering with fine details</li> <li>multiscale variant of dataset에 대해 평균 error rate 60% 감소</li> <li>NeRF가 hierarchical sampling을 위해 coarse and fine MLP를 분리했다면, Mip-NeRF는 <code class="language-plaintext highlighter-rouge">scale-aware</code>하므로 <code class="language-plaintext highlighter-rouge">single MLP만으로 충분</code><br/> 따라서 NeRF보다 7% 빠르고, param. 수는 절반</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-06-17-MipNeRF/1-480.webp 480w,/assets/img/2024-06-17-MipNeRF/1-800.webp 800w,/assets/img/2024-06-17-MipNeRF/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-06-17-MipNeRF/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>차이 : 기존 NeRF는 <code class="language-plaintext highlighter-rouge">a single point</code>를 encode하고, Mip-NeRF는 <code class="language-plaintext highlighter-rouge">a region of space</code>를 encode</p> </li> <li> <p>기존 NeRF :<br/> camera center로부터 각 pixel로 ray를 하나 쏜 다음 point sampling한 뒤 positional encoding</p> </li> <li> <p>Mip-NeRF :<br/> camera center로부터 각 pixel로 3D conical frustum을 쏜 다음, 3D point 및 그 주위의 Gaussian region을 encode하기 위해 <code class="language-plaintext highlighter-rouge">IPE(integrated positional encoding)</code><br/> IPE : the <code class="language-plaintext highlighter-rouge">frustum을 multi-variate Gaussian으로 근사</code>한 뒤, Gaussian 내 좌표를 positional encoding한 것에 대해 <code class="language-plaintext highlighter-rouge">integral</code> \(E \left[ \gamma (x) \right]\) 계산</p> </li> </ul> <h2 id="related-work">Related Work</h2> <h4 id="anti-aliasing-in-rendering">Anti-aliasing in Rendering</h4> <blockquote> <p><code class="language-plaintext highlighter-rouge">anti-aliasing</code>을 위한 고전적인 방법으로는 두 가지가 있다.</p> </blockquote> <ol> <li><code class="language-plaintext highlighter-rouge">supersampling</code> : <ul> <li>rendering할 때 <code class="language-plaintext highlighter-rouge">multiple rays per pixel</code>을 쏴서 Nyquist frequency에 가깝게 supersampling</li> <li><code class="language-plaintext highlighter-rouge">expensive</code> as runtime increases linearly with the supersampling rate, so used only in <code class="language-plaintext highlighter-rouge">offline</code> rendering</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">pre-filtering</code> : <ul> <li>target sampling rate에 맞춰서 Nyquist frequency를 줄이기 위해 scene에 <code class="language-plaintext highlighter-rouge">lowpass-filter</code>를 씌운 버전 사용</li> <li>scene을 미리 downsampling <code class="language-plaintext highlighter-rouge">multi-scales</code> (sparse voxel octree 또는 minmap)로 나타낸 뒤, <code class="language-plaintext highlighter-rouge">ray 대신 cone</code>을 추적하여 cone과 scene이 만나는 곳의 cone’s footprint에 대응되는 <code class="language-plaintext highlighter-rouge">적절한 scale</code>을 골라서 사용 (target sampling rate에 맞는 적절한 scale)</li> <li>scene에 filter 씌운 버전을 한 번만 미리 계산하면 되므로, better for <code class="language-plaintext highlighter-rouge">real-time</code> rendering</li> </ul> </li> </ol> <blockquote> <p>그런데 아래의 이유로 고전적인 multi-scale representation은 적용 불가능<br/> input scene의 geometry를 미리 알 수 없음<br/> input scene의 <code class="language-plaintext highlighter-rouge">scale이 continuous</code>하므로 a fixed number of scales (discrete)과 상황이 다름</p> </blockquote> <p>\(\rightarrow\) 결론 : 따라서 Mip-NeRF는 training하는 동안, <code class="language-plaintext highlighter-rouge">임의의 scale</code>에 대해 query 받을 수 있는, scene의 <code class="language-plaintext highlighter-rouge">pre-filtered representation</code>을 학습한다.</p> <h4 id="scene-representations-for-view-synthesis">Scene Representations for View Synthesis</h4> <ul> <li> <p>만약 images of scene are captured <code class="language-plaintext highlighter-rouge">densely</code> 라면, novel view synthesis 위해, intermediate representation of scene을 reconstruct하지 않고 <code class="language-plaintext highlighter-rouge">light field interpolation</code> 기법 사용</p> </li> <li>만약 images of scene are captured <code class="language-plaintext highlighter-rouge">sparsely</code> 라면, novel view synthesis 위해, <code class="language-plaintext highlighter-rouge">explicit representation</code> of the scene’s 3D geometry and appearance를 reconstruct <ul> <li><code class="language-plaintext highlighter-rouge">polygon-mesh-based</code> representation (<code class="language-plaintext highlighter-rouge">discrete, classic</code>) :<br/> with either diffuse or view-dependent textures<br/> can be stored efficiently, but mesh geometry optimization에 gradient descent를 이용하는 것은 불연속점 및 극소점 때문에 어렵</li> <li><code class="language-plaintext highlighter-rouge">volumetric</code> representation : <ul> <li><code class="language-plaintext highlighter-rouge">voxel-grid-based</code> representation (<code class="language-plaintext highlighter-rouge">discrete, classic</code>) : deep learning으로 학습 가능, but 고해상도의 scene에는 부적합</li> <li><code class="language-plaintext highlighter-rouge">coordinate-based</code> neural representation (<code class="language-plaintext highlighter-rouge">continuous, recent</code>) : 3D 좌표를 그 위치에서의 scene의 특징(e.g. volume density, radiance)으로 mapping하는 continuous function을 MLP로 예측 <ul> <li>implicit surface representation</li> <li><code class="language-plaintext highlighter-rouge">volumetric NeRF representation</code></li> </ul> </li> </ul> </li> </ul> </li> <li>문제점 :<br/> 그동안 view synthesis를 할 때 sampling 및 aliasing에는 덜 주목했다<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">classic discrete</code> representation의 경우 앞에서 소개한 minmap, octree와 같은 고전적인 multi-scale pre-filtering 기법을 쓰면 aliasing 없이 rendering 가능하다<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">coordinate-based</code> representation의 경우 scale이 continuous하므로 고전적인 anti-aliasing 기법은 사용할 수 없다<br/> \(\rightarrow\) sparse voxel octree 기반의 multi-scale representation으로 implicit surface의 continuous neural representation을 구현한 논문 <d-cite key="continuouspriori">[1]</d-cite> 있긴 하지만, scene geometry의 priori를 알아야 한다는 제약이 있다<br/> \(\rightarrow\) Mip-NeRF는 training하는 동안, <code class="language-plaintext highlighter-rouge">임의의 scale</code>에 대해 query 받을 수 있는, scene의 anti-aliased <code class="language-plaintext highlighter-rouge">pre-filtered representation</code>을 학습한다.</li> </ul> <h2 id="method">Method</h2> <h4 id="cone-tracing-and-positional-encoding">Cone Tracing and Positional Encoding</h4> <p>dd</p> <h4 id="architecture">Architecture</h4> <p>dd</p> <h2 id="results">Results</h2> <p>dd</p> <h2 id="conclusion">Conclusion</h2> <p>dd</p>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="rendering"/><category term="3d"/><category term="multiscale"/><category term="antialiasing"/><summary type="html"><![CDATA[A Multiscale Representation for Anti-Aliasing Neural Radiance Fields]]></summary></entry><entry><title type="html">SegmentAnything</title><link href="https://semyeong-yu.github.io/blog/2024/SegmentAnything/" rel="alternate" type="text/html" title="SegmentAnything"/><published>2024-05-29T14:00:00+00:00</published><updated>2024-05-29T14:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/SegmentAnything</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/SegmentAnything/"><![CDATA[<h3 id="segmentanything">SegmentAnything</h3> <h4 id="alexander-kirillov-eric-mintun-nikhila-ravi-hanzi-mao-chloe-rolland-laura-gustafson-tete-xiao-spencer-whitehead-alexander-c-berg-wan-yen-lo-piotr-dollár-ross-girshick">Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2304.02643">https://arxiv.org/abs/2304.02643</a><br/> 출처 : Vision study mkd님</p> </blockquote> <hr/> <h2 id="abstract">Abstract</h2> <ul> <li> <p>Task<br/> Promptable Image Segmentation</p> </li> <li> <p>Model Architecture<br/> image encoder + prompt encoder + mask decoder</p> </li> <li> <p>Generate Data (Data Engine)<br/> assisted-manual stage \(\rightarrow\) semi-automatic stage \(\rightarrow\) fully-automatic stage<br/> data ‘SA-1B’ : 1B masks with 11M images</p> </li> <li> <p>Enable Zero-Shot Generalization<br/> Zero-Shot transfer to various tasks</p> </li> <li> <p>Code Review</p> </li> </ul> <h2 id="overview">Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/2-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/2-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>prompt : mask를 생성할 대상을 지정<br/> point, BB, mask(rough area), text(preliminary) 중 하나</p> </li> <li> <p>valid masks : segmented mask를 하나가 아닌 3개 (whole, part, sub-part) 생성<br/> ambiguous prompt에 대응하기 위해, zero shot을 위해<br/> 3개의 masks 중 GT와 가장 유사한(confidence score가 가장 높은) mask의 loss만 사용</p> </li> </ul> <h2 id="model">Model</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/3-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/3-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Image Encoder : MAE (Masked AutoEncoder) 방식의 ViT<br/> MAE 요약 : 이미지를 grid로 나누고 patches 중 일부를 가린 뒤 원본을 복원하도록 학습하고, 학습이 끝난 후에는 encoder embedding만 사용<br/> ViT-H/16 : 14 \(\times\) 14 windowed attention and 4 global attention blocks</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/4-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/4-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Prompt Encoder :<br/> Mask (dense prompt) : conv. 거친 후 image embedding에 pixel-wise sum (mask가 없는 pixel의 경우 ‘no mask’ prompt 사용)<br/> Point (sparse prompt) : positional encoding + learned embedding(fg or bg)<br/> BB (sparse prompt) : positional encoding + learned embedding(top-left or bottom-right)<br/> text (sparse prompt) : by CLIP text encoder</p> </li> <li> <p>Loss :</p> <ol> <li>Mask loss : related to mask prediction<br/> 1-1. focal loss : \(L(p_{t}) = - (1-p_{t})^{r}log(p_{t})\) where \((1-p_{t})^{r}\) gives more weight to few hard examples (\(p_{t} \sim 0\))<br/> 1-2. dice loss : 1 - dice score where dice score = \(\frac{2 \times Area(A \cap B)}{Area(A) + Area(B)}\)</li> </ol> </li> </ul> <ol> <li>IoU loss : related to confidence score<br/> MSE loss</li> </ol> <h2 id="data--develop-data-engine-by-curriculum-learning">Data : Develop Data Engine by Curriculum Learning</h2> <ul> <li> <p>Assisted-manual stage :<br/> public segmentation dataset \(\rightarrow\) SAM \(\rightarrow\) pixel-wise manual augmentation \(\rightarrow\) re-train<br/> After re-training, the number of masks per image increased from 20 to 44 in average<br/> Collect 4.3M masks from 0.12M images</p> </li> <li> <p>Semi-automatic stage :<br/> dataset from previous stage (4.3M masks) \(\rightarrow\) SAM \(\rightarrow\) mask predict 실패한(제외된) object를 annotate \(\rightarrow\) re-train<br/> After re-training, the number of masks per image increased from 44 to 72 in average<br/> Collect 5.9M masks from 0.18M images (totally 4.3M + 5.9M = 10.2M masks)</p> </li> <li> <p>Fully-automatic stage :<br/> dataset from previous stage (10.2M masks) : image에 32 \(\times\) 32 grid points 찍음 \(\rightarrow\) SAM<br/> ambiguity-aware training (whole, part, sub-part 구분 가능)<br/> After filtering masks with high confidence score,<br/> Collect SA-1B dataset : 1.1B masks from 11M images (various HR masks)<br/> 99.1% is fully-automatically generated<br/> follow RAI (Responsible AI) : no bias and blur human faces</p> </li> </ul> <h2 id="task">Task</h2> <p>generalizable (zero-shot transfer to various tasks)</p> <ul> <li>Zero-Shot Transfer Tasks : <ol> <li>Zero-Shot Single Point Valid Mask Evaluation</li> <li>Zero-Shot Edge Detection</li> <li>Zero-Shot Object Proposals</li> <li>Zero-Shot Instance Segmentation</li> <li>Zero-Shot Text-to-Mask (CLIP)</li> </ol> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/4-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/4-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Zero-Shot Single Point Valid Mask Evaluation :<br/> point 찍었을 때 그에 해당하는 mask를 얼마나 잘 생성하는가<br/> use one most-confident mask<br/> compare with RITM model on 23 datasets</p> </li> <li> <p>Zero-Shot Edge Detection :</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/6-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/6-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>About filter : 블로그 맨 아랫 부분에 설명해놓음</p> <ul> <li> <p>Zero-Shot Object Proposals :<br/> mask 예측 후 object의 identity(class)를 얼마나 잘 맞추는가</p> </li> <li> <p>Zero-Shot Instance Segmentation :</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/7-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/7-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Zero-Shot Text-to-Mask :<br/> image \(\rightarrow\) CLIP \(\rightarrow\) image embedding as input<br/> text \(\rightarrow\) CLIP \(\rightarrow\) text embedding as SAM prompt</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/8-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/8-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>SAM’s latent space에서 similar mask embedding vectors within threshold를 추출한 결과 실제로도 semantically similar</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/9-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/9-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A query is indicated by magenta box : top row shows matches at a low threshold and bottom row shows matches at a high threshold </div> <h2 id="code-review">Code Review</h2> <p>다음에 해야지… 라고 미뤄둠..ㅎㅎ</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/10-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/10-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/11-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/11-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/11.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/12-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/12-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/13-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/13-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/13.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/14-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/14-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/14.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/15-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/15-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/15.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/16-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/16-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/16-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/16.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/17-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/17-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/17-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/17.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/18-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/18-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/18-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/18.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/19-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/19-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/19-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/19.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/20-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/20-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/20-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/20.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/21-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/21-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/21-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/21.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/22-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/22-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/22-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/22.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/23-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/23-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/23-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/23.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/24-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/24-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/24-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/24.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/25-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/25-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/25-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/25.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/26-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/26-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/26-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/26.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/27-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/27-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/27-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/27.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/28-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/28-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/28-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/28.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/29-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/29-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/29-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/29.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/30-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/30-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/30-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/30.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/31-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/31-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/31-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/31.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/32-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/32-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/32-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/32.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/33-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/33-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/33-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/33.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/34-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/34-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/34-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/34.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/35-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/35-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/35-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/35.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/36-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/36-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/36-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/36.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/37-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/37-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/37-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/37.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/38-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/38-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/38-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/38.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/39-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/39-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/39-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/39.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/40-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/40-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/40-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/40.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/41-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/41-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/41-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/41.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container>]]></content><author><name></name></author><category term="cv-tasks"/><category term="image"/><category term="segmentation"/><summary type="html"><![CDATA[Promptable Image Segmentation]]></summary></entry><entry><title type="html">Reconstruction and Synthesis 3D humans in 3D scenes</title><link href="https://semyeong-yu.github.io/blog/2024/Reconstruction_Synthesis_humans/" rel="alternate" type="text/html" title="Reconstruction and Synthesis 3D humans in 3D scenes"/><published>2024-05-13T14:00:00+00:00</published><updated>2024-05-13T14:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Reconstruction_Synthesis_humans</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Reconstruction_Synthesis_humans/"><![CDATA[<h3 id="reconstruction-and-synthesis-3d-humans-in-3d-scenes">Reconstruction and Synthesis 3D humans in 3D scenes</h3> <h4 id="introduction-to-siyu-tang-eth-zurich">Introduction to Siyu Tang (ETH Zurich)</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/14-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/14-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="lecture-summary">Lecture Summary</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/3-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/3-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>real human : How to reconstruct natural human motions in 3D scenes with a monocular camera?</li> <li>digital human</li> <li>virtual human</li> </ol> <h4 id="real-human">real human</h4> <p>Key : learn motion priors from high quality mocap datasets</p> <p>LEMO: Learning Motion Priors for 4D Human Body Capture in 3D Scenes. Zhang. Zhang. Bogo. Pollefeys. Tang. ICCV 2021 (Oral)<br/> It enforces smoothness in latent space</p> <ol> <li>physics-based prior<br/> data-based prior</li> <li>diffusion-based approach : robust for auto-encoder, but optimization may be not that fast<br/> reinforcement-learning-based approach : policy update can be fast</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/1-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/1-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/2-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/2-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="digital-human">digital human</h4> <p>In Ego-centric motion capture,<br/> <code class="language-plaintext highlighter-rouge">Collision score guided sampling</code></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/4-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/4-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="virtual-human">virtual human</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/5-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/5-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Key : Generate 0.25-second(8 frames) Motion Primitives for perpetual motion prediction</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/6-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/6-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/8-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/8-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/7-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/7-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <h4 id="both-real-and-virtual-human">Both real and virtual human</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/9-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/9-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Use both synthetic data and real data for 3D segmentation in Point Clouds</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/10-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/10-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/11-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/11-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <h4 id="egocentric-synthetic-data-generator">Egocentric Synthetic Data Generator</h4> <p>egocentric task : challenging</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/12-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/12-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="future-work">Future Work</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/13-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/13-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="reconstruction"/><category term="synthesis"/><category term="3D"/><category term="human"/><summary type="html"><![CDATA[SoC Colloquium lecture by Siyu Tang (ETH Zurich)]]></summary></entry><entry><title type="html">FMANet</title><link href="https://semyeong-yu.github.io/blog/2024/FMANet/" rel="alternate" type="text/html" title="FMANet"/><published>2024-05-02T14:00:00+00:00</published><updated>2024-05-02T14:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/FMANet</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/FMANet/"><![CDATA[<h3 id="fma-net--flow-guided-dynamic-filtering-and-iterative-feature-refinement-with-multi-attention-for-joint-video-super-resolution-and-deblurring">FMA-Net : Flow-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring</h3> <h4 id="geunhyuk-youk-jihyong-oh-munchurl-kim">Geunhyuk Youk, Jihyong Oh, Munchurl Kim</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2401.03707">https://arxiv.org/abs/2401.03707</a><br/> project website :<br/> <a href="https://kaist-viclab.github.io/fmanet-site/">https://kaist-viclab.github.io/fmanet-site/</a><br/> pytorch code :<br/> <a href="https://github.com/KAIST-VICLab/FMA-Net">https://github.com/KAIST-VICLab/FMA-Net</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> </blockquote> <hr/> <img-comparison-slider> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/9-480.webp 480w,/assets/img/2024-05-02-FMANet/9-800.webp 800w,/assets/img/2024-05-02-FMANet/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/10-480.webp 480w,/assets/img/2024-05-02-FMANet/10-800.webp 800w,/assets/img/2024-05-02-FMANet/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> <img-comparison-slider> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/11-480.webp 480w,/assets/img/2024-05-02-FMANet/11-800.webp 800w,/assets/img/2024-05-02-FMANet/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/12-480.webp 480w,/assets/img/2024-05-02-FMANet/12-800.webp 800w,/assets/img/2024-05-02-FMANet/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> <h2 id="abstract">Abstract</h2> <p><strong>Task : Joint learning of VSRDB (<code class="language-plaintext highlighter-rouge">video super-resolution and deblurring</code>)</strong></p> <ul> <li>restore HR video from blurry LR video<br/> challenging because should handle two types of degradation (SR and deblurring) simultaneously</li> <li>super-resolution : LR vs HR</li> <li>deblurring : blurry vs sharp</li> </ul> <p><strong>FGDF (<code class="language-plaintext highlighter-rouge">flow-guided dynamic filtering</code>)</strong></p> <ul> <li>precise estimation of both <code class="language-plaintext highlighter-rouge">spatio-temporally-variant</code> <code class="language-plaintext highlighter-rouge">degradation</code> and <code class="language-plaintext highlighter-rouge">restoration</code> kernels that are aware of motion trajectories (not stick to fixed positions)</li> <li>effectively <code class="language-plaintext highlighter-rouge">handle large motions with small-sized kernels</code> (naive dynamic filtering의 한계 극복)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/2-480.webp 480w,/assets/img/2024-05-02-FMANet/2-800.webp 800w,/assets/img/2024-05-02-FMANet/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>DCN (Deformable Conv.) : learn position-invariant \(n \times n\) filter coeff.<br/> vs<br/> DF (Dynamic filtering) : learn position-wise \(n \times n\) dynamic filter coeff.</p> <p>DF (Dynamic Filtering) : fixed surroundings<br/> vs<br/> FGDF (Flow Guided DF) : variable surroundings by learned optical flow</p> <p><strong>FRMA (<code class="language-plaintext highlighter-rouge">iterative feature refinement with multi-attention</code>)</strong></p> <p>refine features by iterative updates<br/> loss : TA (temporal anchor)<br/> multi-attention :</p> <ul> <li><code class="language-plaintext highlighter-rouge">center-oriented</code> attention (focus on target frame)</li> <li><code class="language-plaintext highlighter-rouge">degradation-aware</code> attention (use degradation kernels in globally adaptive manner)</li> </ul> <hr/> <h2 id="related-work">Related Work</h2> <p><strong>VSR (Video Super-Resolution)</strong></p> <p>Based on the number of input frames,</p> <ol> <li><code class="language-plaintext highlighter-rouge">sliding window</code>-based method : recover HR frames by using neighboring frames within a sliding window<br/> use CNN, optical flow estimation, deformable conv., or transformer focusing on temporal alignment<br/> vs</li> <li><code class="language-plaintext highlighter-rouge">recurrent</code>-based method : sequentially propagate the latent features of one frame to the next frame<br/> Chan et al. <d-cite key="vsr">[1]</d-cite> BasicVSR++ : combine bidirectional propagation of past and future frames into current frame features<br/> limit : gradient vanishing</li> </ol> <p><strong>DB (Video Deblurring)</strong></p> <p>Zhang et al. <d-cite key="adversarial">[2]</d-cite> 3D CNN<br/> Li et al. <d-cite key="groupshift">[3]</d-cite> grouped spatial-temporal shifts<br/> transformer-based : Restormer <d-cite key="restormer">[4]</d-cite>, Stripformer <d-cite key="stripformer">[5]</d-cite>, RVRT <d-cite key="rvrt">[6]</d-cite></p> <p><strong>Joint learning of VSRDB (not sequential cascade of VSR and DB)</strong></p> <p>Previous works are mostly designed for ISRDB</p> <p>Fang et al. <d-cite key="HOFFR">[7]</d-cite> HOFFR : the first deep-learning-based VSRDB<br/> limit : struggle to deblur spatially-variant motion blur because 2D CNN has spatially-equivariant and input-independent filters</p> <p><strong>Dynamic Filter Network</strong></p> <p>predict spatially-variant degradation or restoration kernels</p> <p>Zhou et al. <d-cite key="adaptivefilter">[8]</d-cite> :<br/> spatially adaptive deblurring filter for recurrent video deblurring<br/> Kim et al. <d-cite key="koalanet">[9]</d-cite> KOALAnet :<br/> blind SR predicts spatially-variant degradation and upsampling filters</p> <ul> <li>limit : apply dynamic filtering only to the reference frame (target position and its fixed surrounding neighbors), so cannot accurately exploit spatio-temporally-variant-motion info. from adjacent frames</li> <li>limit : if apply dynamic filtering to adjacent frames \(\rightarrow\) large-sized filters are required to capture large motions \(\rightarrow\) high computational complexity</li> <li>limit : <d-cite key="separableconv">[10]</d-cite> suggested two separable large 1D kernels to approximate a large 2D kernel \(\rightarrow\) does not capture fine detail, so inappropriate for video</li> </ul> <hr/> <h2 id="method">Method</h2> <p><strong>Overview</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/1-480.webp 480w,/assets/img/2024-05-02-FMANet/1-800.webp 800w,/assets/img/2024-05-02-FMANet/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>FMA-Net : VSRDB framework based on FGDF and FRMA<br/> allow for small-to-large motion representation learning</p> <ul> <li>input : <code class="language-plaintext highlighter-rouge">blurry LR sequence</code> \(X = \left\lbrace X_{c-N}:X_{c+N} \right\rbrace \in R^{T \times H \times W \times 3}\) where \(T=2N+1\) and \(c\) is a center frame index</li> <li>goal : predict <code class="language-plaintext highlighter-rouge">sharp HR center frame</code> \(\hat Y_{c} \in R^{sH \times sW \times 3}\) where \(s\) is SR scale factor</li> </ul> <ol> <li><code class="language-plaintext highlighter-rouge">degradation</code> learning network \(Net^{D}\) : learn <code class="language-plaintext highlighter-rouge">motion-aware</code> <code class="language-plaintext highlighter-rouge">spatio-temporally-variant</code> degradation kernels</li> <li><code class="language-plaintext highlighter-rouge">restoration</code> network \(Net^{R}\) : utilize these degradation kernels in a globally adaptive manner to restore center frame \(X_c\)</li> <li>\(Net^{D}\) and \(Net^{R}\) consist of FRMA blocks and FGDF module</li> </ol> <p><strong>FRMA block</strong></p> <p>pre-trained optical flow network : unstable for blurry frames and computationally expensive</p> <p>vs</p> <blockquote> <p>FRMA block :<br/> learn <code class="language-plaintext highlighter-rouge">self-induced</code> optical flow in a residual learning manner<br/> learn <code class="language-plaintext highlighter-rouge">multiple</code> optical flows with corresponding occlusion masks<br/> \(\rightarrow\) flow diversity enables to learn one-to-many relations b.w. pixels in a target frame and its neighbor frames<br/> \(\rightarrow\) beneficial since <code class="language-plaintext highlighter-rouge">blurry frame's pixel info. is spread due to light accumulation</code></p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/3-480.webp 480w,/assets/img/2024-05-02-FMANet/3-800.webp 800w,/assets/img/2024-05-02-FMANet/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Three features</p> <ol> <li>\(F \in R^{T \times H \times W \times C}\) :<br/> <code class="language-plaintext highlighter-rouge">temporally-anchored (unwarped)</code> feature at each frame index \(0 \sim T-1\)</li> <li>\(F_w \in R^{H \times W \times C}\) :<br/> <code class="language-plaintext highlighter-rouge">warped</code> feature</li> <li>\(\boldsymbol f = \left \lbrace f_{c \rightarrow c+t}^{j}, o_{c \rightarrow c+t}^{j} \right \rbrace _{j=1:n}^{t=-N:N} \in R^{T \times H \times W \times (2+1)n}\) :<br/> multi-<code class="language-plaintext highlighter-rouge">flow-mask</code> pairs<br/> \(f_{c \rightarrow c+t}^{j}\) : learnable optical flow<br/> \(o_{c \rightarrow c+t}^{j}\) : learnable occlusion mask (sigmoid for stability)<br/> \(n\) is the number of multi-flow-mask pairs from the center frame index \(c\) to each frame index<br/> <code class="language-plaintext highlighter-rouge">왜 dim. (2+1)???</code></li> </ol> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/16-480.webp 480w,/assets/img/2024-05-02-FMANet/16-800.webp 800w,/assets/img/2024-05-02-FMANet/16-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>(i+1)-th Feature Refinement : 위첨자로 표기<br/> <code class="language-plaintext highlighter-rouge">feature refine 식 기원??</code></p> <ol> <li>\(F^{i+1}\)=RDB(\(F^{i}\)) :<br/> RDB <d-cite key="RDB">[11]</d-cite></li> <li>\(\boldsymbol f^{i+1}\) = \(\boldsymbol f^{i}\) + Conv3d(concat(\(\boldsymbol f^{i}\), \(W\)(\(F^{i+1}\), \(\boldsymbol f^{i}\)), \(F_{c}^{0}\)))<br/> \(W\)(\(F^{i+1}\), \(\boldsymbol f^{i}\)) : warp \(F^{i+1}\) to center frame index \(c\) based on \(f^{i}\)<br/> \(W\) : occlusion-aware backward warping<br/> concat : along channel dim.<br/> \(F_{c}^{0} \in R^{H \times W \times C}\) : feature map at center frame index \(c\) of the initial feature \(F^{0} \in R^{T \times H \times W \times C}\)</li> <li>\(\tilde F_{w}^{i}\) = Conv2d(concat(\(F_{w}^{i}\), \(r_{4 \rightarrow 3}\)(\(W\)(\(F^{i+1}\), \(\boldsymbol f^{i+1}\)))))<br/> \(r_{4 \rightarrow 3}\) : reshape from \(R^{T \times H \times W \times C}\) to \(R^{H \times W \times TC}\) for feature aggregation</li> <li>\(F_w^{i+1}\) = Multi-Attn(\(\tilde F_{w}^{i}\), \(F_{c}^{0}\)(, \(k^{D, i}\)))</li> </ol> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/15-480.webp 480w,/assets/img/2024-05-02-FMANet/15-800.webp 800w,/assets/img/2024-05-02-FMANet/15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>RDB Network <d-cite key="RDB">[11]</d-cite> :<br/> TBD</p> </blockquote> <blockquote> <p>RRDB Network <d-cite key="rrdb">[15]</d-cite> :<br/> TBD</p> </blockquote> <blockquote> <p>Occlusion-Aware Backward Warping <d-cite key="warp">[12]</d-cite> <d-cite key="warpp">[13]</d-cite> <d-cite key="warppp">[14]</d-cite> :<br/> TBD</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/3-480.webp 480w,/assets/img/2024-05-02-FMANet/3-800.webp 800w,/assets/img/2024-05-02-FMANet/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Multi-Attention :</p> <ul> <li><code class="language-plaintext highlighter-rouge">CO(center-oriented)</code> attention :<br/> better align \(\tilde F_{w}^{i}\) to \(F_{c}^{0}\) (center feature map of initial temporally-anchored feature)</li> <li><code class="language-plaintext highlighter-rouge">DA(degradation-aware)</code> attention :<br/> \(\tilde F_{w}^{i}\) becomes globally adaptive to spatio-temporally variant degradation by using degradation kernels \(K^{D}\)</li> </ul> </blockquote> <ul> <li> <p>CO attention :<br/> \(Q=W_{q} F_{c}^{0}\)<br/> \(K=W_{k} \tilde F_{w}^{i}\)<br/> \(V=W_{v} \tilde F_{w}^{i}\)<br/> \(COAttn(Q, K, V) = softmax(\frac{QK^{T}}{\sqrt{d}})V\)<br/> 실험 결과, \(\tilde F_{w}^{i}\)가 자기 자신(self-attention)이 아니라 \(F_{c}^{0}\)과의 relation에 집중할 때 better performance</p> </li> <li> <p>DA attention :<br/> CO attention과 비슷하지만,<br/> Query 만들 때 \(F_{c}^{0}\) 대신 \(k^{D, i}\) 사용<br/> \(\tilde F_{w}^{i}\) becomes globally adaptive to spatio-temporally-variant degradation<br/> \(k^{D, i} \in R^{H \times W \times C}\) : degradation features adjusted by conv. with \(K^{D}\) (motion-aware spatio-temporally-variant degradation kernels) 에 대해<br/> \(Q=W_{q} k^{D, i}\)<br/> DA attention은 \(Net^{D}\) 말고 \(Net^{R}\) 에서만 사용</p> </li> </ul> <p><strong>FGDF</strong></p> <ul> <li> <p>spatio-temporal Dynamic Filter :<br/> \(y(p) = \sum_{t=-N}^{N} \sum_{k=1}^{n^2} F_{c+t}^{p}(p_k) x_{c+t}(p+p_k)\)<br/> where<br/> \(c\) : center frame index<br/> \(p_k \in \{ (- \lfloor \frac{n}{2} \rfloor, - \lfloor \frac{n}{2} \rfloor), \cdots , (\lfloor \frac{n}{2} \rfloor, \lfloor \frac{n}{2} \rfloor) \}\) : sampling offset for conv. with \(n \times n\) kernel<br/> \(F \in R^{T \times H \times W \times n^{2}}\) : predicted \(n \times n\) dynamic filter<br/> \(F^p \in R^{T \times n^{2}}\) : predicted \(n \times n\) dynamic filter at position p</p> </li> <li> <p>limit :<br/> fixed position (\(p\)) and fixed surrounding neighbors (\(p_k\))<br/> \(\rightarrow\) To capture large motion, require large-sized filter</p> </li> </ul> <blockquote> <p>solution : <code class="language-plaintext highlighter-rouge">FGDF</code><br/> kernels - dynamically generated / pixel-wise (position-wise) / variable surroundings guided by optical flow<br/> \(\rightarrow\) can handle large motion with relatively small-sized filter<br/> \(y(p) = \sum_{t=-N}^{N} \sum_{k=1}^{n^2} F_{c+t}^{p}(p_k) x_{c+t}^{\ast}(p+p_k)\)<br/> where <br/> \(x_{c+t}^{\ast} = W(x_{c+t}, \boldsymbol f_{c+t})\) : <code class="language-plaintext highlighter-rouge">warped input feature</code> based on \(\boldsymbol f_{c+t}\)<br/> \(\boldsymbol f_{c+t}\) : <code class="language-plaintext highlighter-rouge">flow-mask pair</code> from frame index \(c\) to \(c+t\)</p> </blockquote> <p><strong>Overall Architecture</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/1-480.webp 480w,/assets/img/2024-05-02-FMANet/1-800.webp 800w,/assets/img/2024-05-02-FMANet/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Degradation Network \(Net^{D}\)<br/> input : blurry LR sequence \(\boldsymbol X\) and sharp HR sequence \(\boldsymbol Y\)<br/> goal : <code class="language-plaintext highlighter-rouge">predict flow and degradation kernels in sharp HR sequence</code> \(\boldsymbol Y\)</p> <ol> <li>an image flow-mask pair \(\boldsymbol f^{Y}\)</li> <li>motion-aware spatio-temporally-variant degradation kernels \(K^{D}\)<br/> \(\rightarrow\) obtain blurry LR center frame \(\boldsymbol X_{c}\) from sharp HR counterpart \(\boldsymbol Y\)</li> </ol> </blockquote> <ul> <li> <p>step 1-1. initialize<br/> RRDB : <d-cite key="rrdb">[15]</d-cite><br/> \(\boldsymbol X \rightarrow\) 3D RRDB \(\rightarrow F^{0}\)</p> </li> <li> <p>step 1-2. initialize<br/> \(F_{w}^{0} = 0\), \(\boldsymbol f = \left \lbrace f_{c \rightarrow c+t}^{j} = 0, o_{c \rightarrow c+t}^{j} = 1 \right \rbrace _{j=1:n}^{t=-N:N}\)</p> </li> <li> <p>step 2. M FRMA blocks<br/> \(F^{0}, F_{w}^{0}, \boldsymbol f^{0} \rightarrow\) \(M\) FRMA blocks \(\rightarrow F^{M}, F_{w}^{M}, \boldsymbol f^{M}\)</p> </li> <li> <p>step 3-1. <code class="language-plaintext highlighter-rouge">an</code> image flow-mask pair \(\boldsymbol f^{Y} \in R^{T \times H \times W \times (2+1) 1}\)<br/> \(\boldsymbol f^{M} \rightarrow\) Conv3d \(\rightarrow \boldsymbol f^{Y}\)</p> </li> <li> <p>step 3-2. \(\hat X_{sharp}^{D}\) only used in Temporal Anchor (TA) loss<br/> \(F^{M} \rightarrow\) Conv3d \(\rightarrow \hat X_{sharp}^{D} \in R^{T \times H \times W \times 3}\) in image domain</p> </li> <li> <p>step 3-3. motion-aware spatio-temporally-variant degradation kernels \(K^{D} \in R^{T \times H \times W \times k_{d}^{2}}\)<br/> \(K^{D}\) = softmax(Conv3d(\(r_{3 \rightarrow 4}\)(\(F_{w}^{M}\))))<br/> where<br/> \(k_{d}\) : degradation kernel size<br/> sigmoid for normalization : all kernels have <code class="language-plaintext highlighter-rouge">positive</code> values, which mimics <code class="language-plaintext highlighter-rouge">blur generation process</code></p> </li> <li> <p>step 4. FGDF downsampling to predict blurry center frame \(\hat X_{c}\)<br/> \(\hat X_{c}\) = \(W(\boldsymbol Y, s (\boldsymbol f^{Y} \uparrow _{s}))\) \(\circledast K^{D} \downarrow _{s}\)<br/> where<br/> \(\uparrow\) : \(\times s\) bilinear upsampling<br/> \(W(\boldsymbol Y, s (\boldsymbol f^{Y} \uparrow _{s}))\) : warped sharp HR sequence based on an upsampled image flow-mask pair<br/> \(\circledast K^{D} \downarrow _{s}\) : FGDF with filter \(K^{D}\) with stride \(s\)</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/17-480.webp 480w,/assets/img/2024-05-02-FMANet/17-800.webp 800w,/assets/img/2024-05-02-FMANet/17-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/17.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/1-480.webp 480w,/assets/img/2024-05-02-FMANet/1-800.webp 800w,/assets/img/2024-05-02-FMANet/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Restoration Network \(Net^{R}\)<br/> input : blurry LR sequence \(\boldsymbol X\) and \(F^{M}, \boldsymbol f^{M}, K^{D}\) from \(Net^{D}\)<br/> goal : <code class="language-plaintext highlighter-rouge">predict flow and restoration kernels in blurry LR sequence</code> \(\boldsymbol X\)</p> <ol> <li>an image flow-mask pair \(\boldsymbol f^{X}\)</li> <li>restoration kernels \(K^{R}\)<br/> \(\rightarrow\) obtain sharp HR center frame \(\hat Y_{c}\) from blurry LR counterpart \(\boldsymbol X\)</li> </ol> </blockquote> <ul> <li> <p>step 1-1. initialize \(F^{0}\)<br/> RRDB : <d-cite key="rrdb">[15]</d-cite><br/> concat(\(\boldsymbol X\), \(F^{M}\) from \(Net^{D}\)) \(\rightarrow\) 3D RRDB \(\rightarrow\) \(F^{0}\)</p> </li> <li> <p>step 1-2. initialize \(F_{w}^{0}\), \(\boldsymbol f^{0}\)<br/> \(F_{w}^{0} = 0\), \(\boldsymbol f^{0} = \boldsymbol f^{M}\) from \(Net^{D}\)</p> </li> <li> <p>step 2-1. compute \(k^{D, i} \in R^{H \times W \times C}\) for DA attention</p> </li> <li> <p>step 2-2. M FRMA blocks<br/> \(F^{0}, F_{w}^{0}, \boldsymbol f^{0}, k^{D, i} \rightarrow\) \(M\) FRMA blocks \(\rightarrow F^{M}, F_{w}^{M}, \boldsymbol f^{M}\)</p> </li> <li> <p>step 3-1. <code class="language-plaintext highlighter-rouge">an</code> image flow-mask pair \(\boldsymbol f^{X} \in R^{T \times H \times W \times (2+1) 1}\)<br/> \(\boldsymbol f^{M} \rightarrow\) Conv3d \(\rightarrow \boldsymbol f^{X}\)</p> </li> <li> <p>step 3-2. \(\hat X_{sharp}^{R}\) only used in Temporal Anchor (TA) loss<br/> \(F^{M} \rightarrow\) Conv3d \(\rightarrow \hat X_{sharp}^{R} \in R^{T \times H \times W \times 3}\) in image domain</p> </li> <li> <p>step 3-3. motion-aware spatio-temporally-variant \(\times s\) upsampling and restoration kernels \(K^{R} \in R^{T \times H \times W \times s^{2} k_{r}^{2}}\)<br/> \(K^{R}\) = Normalize(Conv3d(\(r_{3 \rightarrow 4}\)(\(F_{w}^{M}\))))<br/> where<br/> \(k_{r}\) : restoration kernel size<br/> Normalize : w.r.t all kernels at temporally co-located positions over \(X\) (\(T\) dim.에 대해 normalize)</p> </li> <li> <p>step 3-4. high-frequency detail \(\hat Y_{r}\)<br/> \(F_{w}^{M} \rightarrow\) stacked conv. and pixel shuffle \(\rightarrow \hat Y_{r}\)</p> </li> <li> <p>step 4. FGDF upsampling to predict sharp center frame \(\hat Y_{c}\)<br/> \(\hat Y_{c}\) = \(\hat Y_{r}\) + \(W(\boldsymbol X, \boldsymbol f^{X})\) \(\circledast K^{D} \uparrow _{s}\)<br/> where<br/> \(W(\boldsymbol X, \boldsymbol f^{X})\) : warped blurry LR sequence based on an image flow-mask pair<br/> \(\circledast K^{D} \uparrow _{s}\) : \(\times s\) dynamic upsampling with kernel \(K^{R}\)</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/17-480.webp 480w,/assets/img/2024-05-02-FMANet/17-800.webp 800w,/assets/img/2024-05-02-FMANet/17-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/17.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Training</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/4-480.webp 480w,/assets/img/2024-05-02-FMANet/4-800.webp 800w,/assets/img/2024-05-02-FMANet/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Stage 1. Pre-train \(Net^{D}\)</p> <ul> <li>loss 1. <code class="language-plaintext highlighter-rouge">reconstruction loss</code> for blurry LR \(X_{c}\)<br/> \(\hat X_{c}\) \(\leftrightarrow\) \(X_{c}\)</li> <li>loss 2. <code class="language-plaintext highlighter-rouge">optical flow warping loss</code> (warping from c to c+t) in \(\boldsymbol Y\)<br/> \(W(Y_{t+c}, s (\boldsymbol f_{t+c}^{Y} \uparrow _{s}))\) \(\leftrightarrow\) \(Y_{c}\)</li> <li>loss 3. <code class="language-plaintext highlighter-rouge">optical flow refining loss</code> in \(\boldsymbol Y\)<br/> \(f^{Y}\) \(\leftrightarrow\) \(f_{RAFT}^{Y}\)<br/> where<br/> \(f^{Y}\) is image optical flow (no occlusion mask) contained in \(\boldsymbol f^{Y}\)<br/> \(f_{RAFT}^{Y}\) is pseudo-GT optical flow by pre-trained RAFT model <d-cite key="Raft">[16]</d-cite></li> <li>loss 4. <code class="language-plaintext highlighter-rouge">Temporal Anchor (TA) loss</code> for sharp LR \(X_{sharp}\)<br/> <code class="language-plaintext highlighter-rouge">It anchors and sharpens each feature w.r.t corresponding frame index</code><br/> \(\hat X_{sharp}^{D}\) \(\leftrightarrow\) \(X_{sharp}\)<br/> where<br/> sharp HR sequence \(\boldsymbol Y \rightarrow\) bicubic downsampling \(\rightarrow\) GT sharp LR sequence \(X_{sharp}\)<br/> \(\rightarrow\) keep each feature temporally anchored for the corresponding frame index<br/> \(\rightarrow\) constrain the solution space to distinguish warped and unwarped features<br/> <code class="language-plaintext highlighter-rouge">???</code></li> </ul> <blockquote> <p>RAFT: Recurrent all-pairs field transforms for optical flow <d-cite key="Raft">[16]</d-cite> :<br/> 핵심 아이디어 : TBD</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/5-480.webp 480w,/assets/img/2024-05-02-FMANet/5-800.webp 800w,/assets/img/2024-05-02-FMANet/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Stage 2. Jointly train \(Net^{D}\) and \(Net^{R}\)</p> <ul> <li>loss 1. <code class="language-plaintext highlighter-rouge">restoration loss</code> for sharp HR \(Y_{c}\)<br/> \(\hat Y_{c}\) \(\leftrightarrow\) \(Y_{c}\)</li> <li>loss 2. <code class="language-plaintext highlighter-rouge">optical flow warping loss</code> (warping from c to c+t) in \(\boldsymbol X\)<br/> Stage 1.의 loss 2.와 동일한 원리</li> <li>loss 3. <code class="language-plaintext highlighter-rouge">Temporal Anchor (TA) loss</code> for sharp LR \(X_{sharp}\)<br/> Stage 1.의 loss 4.와 동일한 원리</li> <li>loss 4. \(L_{D}\)<br/> Stage 1.의 loss들<br/> <code class="language-plaintext highlighter-rouge">왜 X optical flow에 대해선 RAFT loss 안 했지??</code></li> </ul> <hr/> <h2 id="results">Results</h2> <p><strong>Settings</strong></p> <p>LR patch size : 64 \(\times\) 64<br/> the number of FRMA blocks : \(M\) = 4<br/> the number of multi-flow-mask pairs : \(n\) = 9<br/> degradation and restoration kernel size : \(k_{d}\), \(k_{r}\) = 20, 5<br/> the number of frames in sequence : \(T\) = 3 (\(N\) = 1)<br/> ratio b.w. HR and LR : \(s\) = 4<br/> multi-attention block : utilize multi-Dconv head transposed attention (MDTA) and Gated-Dconv feed-forward network (GDFN) from Restormer <d-cite key="restormer">[4]</d-cite></p> <blockquote> <p>multi-Dconv head transposed attention and Gated-Dconv feed-forward network <d-cite key="restormer">[4]</d-cite> :<br/> TBD</p> </blockquote> <p><strong>Datasets and Evaluation Metrics</strong></p> <ul> <li> <p>Datasets :<br/> REDS dataset : train and test<br/> GoPro and YouTube dataset : test (generalization)<br/> \(\rightarrow\) spatially bicubic downsampling to make LR sequence and temporally downsampling to make lower fps sequence</p> </li> <li> <p>Evaluation Metrics :<br/> PSNR and SSIM for image quality<br/> tOF for temporal consistency</p> </li> </ul> <p><strong>Comparision with SOTA</strong></p> <blockquote> <p>SOTA methods (SR) :<br/> single-image SR : SwinIR <d-cite key="swinir">[17]</d-cite> and HAT <d-cite key="hat">[18]</d-cite><br/> video SR : BasicVSR++ <d-cite key="vsr">[1]</d-cite> and FTVSR <d-cite key="ftvsr">[19]</d-cite></p> </blockquote> <blockquote> <p>SOTA methods (DB) :<br/> single-image deblurring : Restormer <d-cite key="restormer">[4]</d-cite> and FFTformer <d-cite key="fftformer">[20]</d-cite><br/> video deblurring : RVRT <d-cite key="rvrt">[6]</d-cite> and GShiftNet <d-cite key="gshiftnet">[21]</d-cite></p> </blockquote> <blockquote> <p>SOTA methods (VSRDB) :<br/> HOFFR <d-cite key="HOFFR">[7]</d-cite></p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/18-480.webp 480w,/assets/img/2024-05-02-FMANet/18-800.webp 800w,/assets/img/2024-05-02-FMANet/18-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/18.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>VSRDB methods have superior performance compared to sequential cascade of SR and DB<br/> \(\rightarrow\) SR and DB tasks are highly inter-correlated</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/7-480.webp 480w,/assets/img/2024-05-02-FMANet/7-800.webp 800w,/assets/img/2024-05-02-FMANet/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/8-480.webp 480w,/assets/img/2024-05-02-FMANet/8-800.webp 800w,/assets/img/2024-05-02-FMANet/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <p><strong>Ablation Study</strong></p> <ul> <li>FGDF<br/> FGDF is better than conventional dynamic filtering for all ranges of motion magnitudes<br/> conventional dynamic filtering is especially not good for large motion</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/13-480.webp 480w,/assets/img/2024-05-02-FMANet/13-800.webp 800w,/assets/img/2024-05-02-FMANet/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> PSNR/tOF according to the average optical flow magnitude b.w. two consecutive frames </div> <ul> <li>Design of FMA-Net <ol> <li>the number of multi-flow-mask pairs \(n\) \(\propto\) performance</li> <li>motion info. from multi-flow-mask pairs \(\boldsymbol f\) is better than motion info. from DCN (Deformable Conv.) due to self-induced sharper optical flows and occlusion masks</li> <li>RAFT loss and TA loss</li> <li>two-stage (\(Net^{D} \rightarrow\) both) training is better than end-to-end training</li> <li>multi-attention (CO + DA) is better than self-attention + SFT(spatial feature transform) <d-cite key="SFT">[22]</d-cite></li> </ol> </li> </ul> <blockquote> <p>SFT (spatial feature transform) <d-cite key="SFT">[22]</d-cite><br/> ddd</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/14-480.webp 480w,/assets/img/2024-05-02-FMANet/14-800.webp 800w,/assets/img/2024-05-02-FMANet/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="conclusion">Conclusion</h2> <p>VSRDB framework based on FGDF and FRMA</p> <ul> <li>FRMA :<br/> iteratively update features (e.g. self-induced optical flow)<br/> multi-attention (CO + DA attention)</li> <li>FGDF :<br/> predict flow-mask pair with flow-guided dynamic filters \(K^{D}\) and \(K^{R}\) that are aware of motion<br/> can handle large motion</li> <li>TA loss :<br/> temporally anchors and sharpens unwarped features</li> <li>2-stage training :<br/> because, during multi-attention of \(Net^{R}\), warped feature \(F_{w}\) is adjusted by predicted degradation \(K^{D}\) from \(Net^{D}\) in globally adaptive manner</li> </ul> <h2 id="limitation">Limitation</h2> <ul> <li>2-stage approach has longer training time than end-to-end approach</li> <li>In extreme contidions such as object rotation, it is hard to predict accurate optical flow<br/> \(\rightarrow\) learnable homography parameters or quaternion representations can be one option to handle rotational motions</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/19-480.webp 480w,/assets/img/2024-05-02-FMANet/19-800.webp 800w,/assets/img/2024-05-02-FMANet/19-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/19.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="super-resolution"/><category term="super-resolution"/><category term="deblur"/><category term="flow"/><category term="dynamic"/><category term="attention"/><summary type="html"><![CDATA[Flow-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring]]></summary></entry><entry><title type="html">NeRF</title><link href="https://semyeong-yu.github.io/blog/2024/NeRF/" rel="alternate" type="text/html" title="NeRF"/><published>2024-04-10T21:00:00+00:00</published><updated>2024-04-10T21:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/NeRF</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/NeRF/"><![CDATA[<h2 id="nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h2> <h4 id="ben-mildenhall-pratul-psrinivasan-matthew-tancik">Ben Mildenhall, Pratul P.Srinivasan, Matthew Tancik</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2003.08934">https://arxiv.org/abs/2003.08934</a><br/> project website :<br/> <a href="https://www.matthewtancik.com/nerf">https://www.matthewtancik.com/nerf</a><br/> pytorch code :<br/> <a href="https://github.com/yenchenlin/nerf-pytorch">https://github.com/yenchenlin/nerf-pytorch</a><br/> <a href="https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file">https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file</a><br/> tiny tensorflow code :<br/> <a href="https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb">https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb</a><br/> referenced blog :<br/> <a href="https://csm-kr.tistory.com/64">https://csm-kr.tistory.com/64</a><br/> <a href="https://yconquesty.github.io/blog/ml/nerf/nerf_rendering.html#the-rendering-formula">https://yconquesty.github.io/blog/ml/nerf/nerf_rendering.html#the-rendering-formula</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>여러 각도의 camera center에서 each input image pixel 방향으로 ray(r=o+td)를 쏜다.</li> <li>ray를 discrete points로 sampling한다.</li> <li>3D coordinate x와 viewing direction d를 r(x)와 r(d)로 positional encoding한다.</li> <li>r(x)를 MLP에 넣어 volume density를 얻고 여기에 r(d)까지 넣어 RGB color를 얻는다.</li> <li>coarse network와 fine network(hierarchical sampling) 각각에서 volume density와 color를 이용한 volume rendering으로 ray마다 rendering pixel color를 구한다.</li> </ol> </blockquote> <h2 id="introduction">Introduction</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/3-480.webp 480w,/assets/img/2024-04-10-NeRF/3-800.webp 800w,/assets/img/2024-04-10-NeRF/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="pipeline">Pipeline</h4> <ul> <li> <p>(a) march camera rays and generate sampling of 5D coordinates</p> </li> <li> <p>(b) represents volumetric static scene by optimizing continuous 5D function(fully-connected network)</p> </li> </ul> <ol> <li>input: single continuous <code class="language-plaintext highlighter-rouge">5D coordinate</code><br/> 3D location \(x, y, z\)<br/> 2D direction \(\theta, \phi\)</li> <li>output:<br/> <code class="language-plaintext highlighter-rouge">volume density</code> (differential opacity) (how much radiance is accumulated by a ray)<br/> <code class="language-plaintext highlighter-rouge">view-dependent RGB color</code> (emitted radiance) \(c = (r, g, b)\)</li> </ol> <ul> <li> <p>(c) synthesizes novel view by classic <code class="language-plaintext highlighter-rouge">volume rendering</code> techniques(differentiable) to accumulate(project)(composite) the color/density samples into 2D image along rays</p> </li> <li> <p>(d) loss between synthesized and GT observed images</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/2-480.webp 480w,/assets/img/2024-04-10-NeRF/2-800.webp 800w,/assets/img/2024-04-10-NeRF/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Pipeline of NeRF architecture </div> <h4 id="problem--solution">Problem &amp; Solution</h4> <p>Problem :</p> <ol> <li>not sufficiently high-resolution representation</li> <li>inefficient in the number of samples per camera ray</li> </ol> <p>Solution :</p> <ol> <li>input <code class="language-plaintext highlighter-rouge">positional encoding</code> for MLP to represent higher frequency function</li> <li><code class="language-plaintext highlighter-rouge">hierarchical sampling</code> to reduce the number of queries</li> </ol> <h4 id="contribution">Contribution</h4> <ul> <li>represent continuous scenes as 5D neural radiance fields with basic MLP to render high-resolution novel views</li> <li>differentiable volume rendering + hierarchical sampling</li> <li>positional encoding to map input 5D coordinate into higher dim. space for high-frequency scene representation</li> <li>overcome the storage costs of discretized voxel grids by encoding continuous volume into network’s parameters<br/> =&gt; require only storage costs of sampled volumetric representations</li> </ul> <h2 id="related-work">Related Work</h2> <h4 id="neural-3d-shape-representation">Neural 3D shape representation</h4> <ul> <li> <p>deep networks that map \(xyz\) coordinates to signed distance functions or occupancy fields<br/> =&gt; limit : need GT 3D geometry</p> </li> <li> <p>Niemeyer et al.<br/> =&gt; input : find directly surface intersection for each ray (can calculate exact derivatie)<br/> =&gt; output : diffuse color at each ray intersection location</p> </li> <li> <p>Sitzmann et al.<br/> =&gt; input : each 3D coordinate<br/> =&gt; output : feature vector and RGB color at each 3D coordinate<br/> =&gt; rendering by RNN that marches along each ray to decide where the surface is.</p> </li> </ul> <blockquote> <p>Limit : oversmoothed renderings, so limited to simple shapes with low geometric complexity</p> </blockquote> <h4 id="view-synthesis-and-image-based-rendering">View synthesis and image-based rendering</h4> <ul> <li> <p>Given <code class="language-plaintext highlighter-rouge">dense sampling of views</code>, novel view synthesis is possible by <code class="language-plaintext highlighter-rouge">simple light field sample interpolation</code></p> </li> <li> <p>Given <code class="language-plaintext highlighter-rouge">sparser sampling of views</code>, there are 2 ways :<br/> mesh-based representation and volumetric representation</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Mesh-based</code> representation with either diffuse(난반사) or view-dependent appearance :<br/> Directly optimize mesh representations by differentiable rasterizers or pathtracers so that we reproject and reconstruct images</p> </li> </ul> <blockquote> <p>Limit :<br/> gradient-based optimization is often difficult because of <code class="language-plaintext highlighter-rouge">local minima or discontinuities or poor loss landscape</code><br/> mesh 구조를 유지하면서 <code class="language-plaintext highlighter-rouge">gradient-based optimization하는 게 어렵</code><br/> needs a <code class="language-plaintext highlighter-rouge">template mesh</code> with fixed topology for initialization, which is unavailable in real-world</p> </blockquote> <ul> <li><code class="language-plaintext highlighter-rouge">Volumetric</code> representation :<br/> well-suited for gradient-based optimization and less distracting artifacts<br/> train : predict a sampled volumetric representation (voxel grids) from input images<br/> test : use alpha-(or learned-)compositing along rays to render novel views<br/> (alpha-compositing : 아래 volume rendering section에서 설명 예정)<br/> CNN compensates discretization artifacts from low resolution voxel grids or CNN allows voxel grids to vary on input time</li> </ul> <blockquote> <p>Limit :<br/> good results, but limited by poor time, space complexity due to discrete sampling<br/> \(\rightarrow\) discrete sampling : rendering high resol. image =&gt; finer sampling of 3D space</p> </blockquote> <blockquote> <p>Author’s solution :<br/> encode <code class="language-plaintext highlighter-rouge">continuous</code> volume into network’s parameters<br/> =&gt; higher quality rendering + require only storage cost of those <code class="language-plaintext highlighter-rouge">sampled</code> volumetric representations</p> </blockquote> <h2 id="neural-radiance-field-scene-representation">Neural Radiance Field Scene Representation</h2> <p>represent continuous scene by 5D MLP : (x, d) =&gt; (c, \(\sigma\))</p> <p>Here, there are 2 key-points!</p> <blockquote> <p>multiview consistent :<br/> c is dependent on both x and d, but \(\sigma\) is only dependent on location x<br/> 3D coordinate x =&gt; 8 fc-layers =&gt; volume-density and 256-dim. feature vector</p> </blockquote> <blockquote> <p>Lambertian reflection : diffuse(난반사) vs Specular reflection : 전반사</p> </blockquote> <blockquote> <p>non-Lambertian effects : view-dependent color change to represent specular reflection<br/> feature vector is concatenated with direction d =&gt; 1 fc-layer =&gt; view-dependent RGB color</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/6-480.webp 480w,/assets/img/2024-04-10-NeRF/6-800.webp 800w,/assets/img/2024-04-10-NeRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="volume-rendering-with-radiance-fields">Volume Rendering with Radiance Fields</h2> <h4 id="ray-from-input-image-pre-processing">Ray from input image (pre-processing)</h4> <p>We use <code class="language-plaintext highlighter-rouge">Ray</code> to synthesize <code class="language-plaintext highlighter-rouge">continuous</code>-viewpoint images from <code class="language-plaintext highlighter-rouge">discrete</code> input images</p> <blockquote> <p>\(r(t) = o + td\)<br/> o : camera’s center of projection<br/> d : viewing direction<br/> t \(\in [ t_n , t_f ]\) : distance from camera center b.w. camera’s predefined near and far planes</p> </blockquote> <blockquote> <p>How to calculate viewing direction d??</p> <ul> <li>2D pixel coordinate : \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)</li> <li>2D normalized coordinate (\(z = 1\)) by intrinsic matrix :<br/> \(\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\) = \(K^{-1}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\) = \(\begin{bmatrix} 1/f_x &amp; 0 &amp; W/2 \\ 0 &amp; 1/f_y &amp; H/2 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)<br/> Since \(y, z\) have opposite direction between the real-world coordinate and pixel coordinate, we multiply (-1)<br/> \(\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\) = \(\begin{bmatrix} 1/f_x &amp; 0 &amp; W/2 \\ 0 &amp; -1/f_y &amp; H/2 \\ 0 &amp; 0 &amp; -1 \end{bmatrix}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)<br/> Here, focal length in intrinsic matrix K is usually calculated using camear angle \(\alpha\) as \(\tan{\alpha / 2} = \frac{h/2}{f}\)</li> <li>3D coordinate by extrinsic matrix :<br/> For extrinsic matrix \([R \vert t']\),<br/> \(o = t'\)<br/> \(d = R * \begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\)<br/> Therefore, we can obtain \(r(t) = o + td\)</li> </ul> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/9-480.webp 480w,/assets/img/2024-04-10-NeRF/9-800.webp 800w,/assets/img/2024-04-10-NeRF/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="volume-rendering-from-mlp-output">Volume Rendering from MLP output</h4> <p>We use differential classical volume rendering</p> <blockquote> <p>idea : 빛이 들어올 확률이 클수록, 물체의 밀도가 높을수록 2D 이미지 픽셀에서 색상 채널의 값이 크게 나타나므로<br/> pixel’s color along a ray is equal to the integral of (transmittance) x (volume density) x (color) for all points along one ray.</p> </blockquote> <p><code class="language-plaintext highlighter-rouge">transmittance가 클수록 투명해서 no color에 가까우니까 색상 채널의 값이 작게 나타나야 하는 것 아니야?????</code></p> <blockquote> <p>Let ray \(r\) (traced through desired virtual camera) have near and far bounds \(t_n, t_f\)<br/> expected color of ray \(r\) = \(C(r) = \int_{t_n}^{t_f} T(t) \sigma (r(t)) c(r(t), d) dt\)</p> </blockquote> <ul> <li>\(T(t) = \exp(- \int_{t_n}^{t} \sigma (r(s)) ds)\) :<br/> <code class="language-plaintext highlighter-rouge">transmittance</code><br/> transmittance = 투과도 = <code class="language-plaintext highlighter-rouge">CDF</code> that a ray <code class="language-plaintext highlighter-rouge">does not hit</code> any particles from \(z_0\) to \(z\)<br/> 투과도가 클수록 투명 (no color)</li> <li>\(\sigma (r(t))\) : <code class="language-plaintext highlighter-rouge">volume density</code> along the ray (learned by MLP)<br/> volume density = <code class="language-plaintext highlighter-rouge">opacity</code> = 불투명도 = <code class="language-plaintext highlighter-rouge">extinction coefficient</code> = <code class="language-plaintext highlighter-rouge">alpha value</code> for alpha-compositing</li> <li>\(c(r(t), d)\) : object’s <code class="language-plaintext highlighter-rouge">color</code> along the ray (learned by MLP)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/10-480.webp 480w,/assets/img/2024-04-10-NeRF/10-800.webp 800w,/assets/img/2024-04-10-NeRF/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/11-480.webp 480w,/assets/img/2024-04-10-NeRF/11-800.webp 800w,/assets/img/2024-04-10-NeRF/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>volume rendering 식 유도 과정</p> </blockquote> <p>occluding objects are modeled as spherical particles with radius \(r\)<br/> There are \(A \cdot \Delta z \cdot \rho (z)\)개의 particles in the slice where \(\rho (z)\) is particle density (the number of particles per unit volume)</p> <p>Since solid particles do not overlap for \(\Delta z \rightarrow 0\),<br/> \(A \cdot \Delta z \cdot \rho (z) \cdot \pi r^2\)만큼 area is occluded<br/> 즉, cross section \(A\)에서 \(\frac{A \cdot \Delta z \cdot \rho (z) \cdot \pi r^2}{A} = \pi r^2 \cdot \rho (z) \cdot \Delta z\)의 비율만큼 occluded</p> <p>If \(\frac{A \cdot \Delta z \cdot \rho (z) \cdot \pi r^2}{A}\)만큼 rays are occluded, the light intensity decreases as<br/> \(I(z + \Delta z) = (1 - \pi r^2 \rho (z) \Delta z) \times I(z)\)</p> <p>Then the light density difference \(\Delta I = I(z + \Delta z) - I(z) = - \pi r^2 \rho (z) \Delta z \cdot I(z)\)<br/> 즉, \(dI(z) = - \pi r^2 \rho (z) I(z) dz = - \sigma (z) I(z) dz\)<br/> where <code class="language-plaintext highlighter-rouge">volume density (or opacity)</code> is \(\sigma(z) = \pi r^2 \rho (z)\)<br/> It makes sense because particle area와 particle density(particle 수)가 클수록 ray 감소량 (volume density)이 커지기 때문<br/> ODE 풀면, \(I(z) = I(z_0)\exp(- \int_{z_0}^{z} \sigma (r(s)) ds)\)</p> <p>Let’s define transmittance \(T(z) = \exp(- \int_{z_0}^{z} \sigma (r(s)) ds)\)<br/> where \(I(z) = I(z_0)T(z)\) means the <code class="language-plaintext highlighter-rouge">remainning</code> intensity after rays travel from \(z_0\) to \(z\)<br/> where <code class="language-plaintext highlighter-rouge">transmittance</code> \(T(z)\) means <code class="language-plaintext highlighter-rouge">CDF</code> that a ray <code class="language-plaintext highlighter-rouge">does not hit</code> any particles from \(z_0\) to \(z\)</p> <p>If a ray passes empty space, there is no color<br/> If a ray hits particles, there exists color (<code class="language-plaintext highlighter-rouge">radiance is emitted</code>)<br/> Let’s define \(H(z) = 1 - T(z)\), which means CDF that a ray <code class="language-plaintext highlighter-rouge">hits</code> particles from \(z_0\) to \(z\)<br/> CDF를 미분하면 PDF이므로<br/> Then PDF is \(p_{hit}(z) = \frac{dH}{dz} = - \frac{dT}{dz} = \exp(- \int_{z_0}^{z} \sigma (r(s)) ds) \sigma (z) = T(z) \sigma (z)\)</p> <p>Let a random variable \(R\) be the emitted randiance.<br/> Then PDF \(p_R(ray) = P[R = c(z)] = p_{hit}(z) = T(z) \sigma (z)\)<br/> Then the <code class="language-plaintext highlighter-rouge">color of a pixel is expected radiance for ray</code> bounded from \(t_n\) to \(t_f\)<br/> \(C(ray) = E[R] = \int_{t_n}^{t_f} R \cdot p_R dz = \int_{t_n}^{t_f} c \cdot p_{hit} dz = \int_{t_n}^{t_f} T(z) \sigma (z) c(z) dz\)</p> <p>\(t_n, t_f = 0., 1.\) for scaled-bounded and front-facing scenes after conversion to <code class="language-plaintext highlighter-rouge">NDC (normalized device coordinates)</code><br/> <a href="https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#analysis">How NDC Works?</a></p> <blockquote> <p>To apply the equation to our model by numerical quadrature,<br/> we have to sample discrete points from continuous ray</p> </blockquote> <p>Instead of deterministic quadrature(typically used for rendering voxel grids, but may limit resolution),<br/> author divides a ray \(\left[t_n, t_f\right]\) into N = 64 bins(intervals), and chooses one point \(t_i\) for each bin by uniform sampling<br/> \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N}(t_f - t_n), t_n + \frac{i}{N}(t_f - t_n)\right]\)</p> <p>Although we use discrete N samples, <code class="language-plaintext highlighter-rouge">stratified sampling(층화 표집)</code> enables MLP to be evaluated at continuous positions by optimization</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/5-480.webp 480w,/assets/img/2024-04-10-NeRF/5-800.webp 800w,/assets/img/2024-04-10-NeRF/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Discretized version for N samples by Numerical Quadrature :<br/> expected color \(\hat{C}(r) = \sum_{i=1}^{N} T_i (1 - \exp(- \sigma_{i} \delta_{i})) c_i\)</p> </blockquote> <ul> <li>\(\hat{C}(r)\) : differentiable about (\(\sigma_{i}, c_i\))</li> <li>\(\delta_{j} = t_{j+1} - t_j\) : the distance between adjacent samples</li> <li>\(T(t) = \exp(- \int_{t_n}^{t} \sigma (r(s)) ds) ~~ \rightarrow ~~ T_i = \exp(- \sum_{j=1}^{i-1} \sigma_{j} \delta_{j})\) where T_1 = 1</li> <li>\(\sigma (r(t)) dt ~~ \rightarrow ~~ \sigma_{j} \delta_{j} = 1 - \exp(-\sigma_{j} \delta_{j})\) by Taylor Series Expansion</li> <li> \[c(r(t), d) ~~ \rightarrow ~~ c_i\] </li> </ul> <p>또는</p> <p>\(p_{hit}(z_i) = \frac{dH}{dz} |_{z_i} ~~ \rightarrow ~~ H(z_{i+1}) - H(z_i) = (1 - T(z_{i+1})) - (1 - T(z_i)) = T(z_i) - T(z_{i+1}) = e^{- \sum_{j=1}^{i-1} \sigma_{j} \delta_{j}} - e^{- \sum_{j=1}^{i} \sigma_{j} \delta_{j}} = T(z_i)(1 - e^{- \sigma_{i} \delta_{i}})\)<br/> Then the <code class="language-plaintext highlighter-rouge">color of a pixel is expected radiance for ray</code> bounded from \(t_n\) to \(t_f\)<br/> \(\hat{C}(ray) = E[R] = \int_{t_n}^{t_f} R \cdot p_R dz ~~ \rightarrow ~~ \sum_{i=1}^{N} c_i \cdot p_{hit}(z_i) dz = \sum_{i=1}^{N} c_i T_i (1 - \exp(- \sigma_{i} \delta_{i}))\)</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">Final version</code> :<br/> expected color \(\hat{C}(r) = \sum_{i=1}^{N} T_i \alpha_{i} c_i\)<br/> where \(T_i = \prod_{j=1}^{i-1} (1-\alpha_{j})\) and \(\alpha_{i} = 1 - \exp(-\sigma_{i} \delta_{i})\)<br/> which reduces to traditional <code class="language-plaintext highlighter-rouge">alpha-compositing</code> problem</p> </blockquote> <p>이 때, this volume rendering 식은 <code class="language-plaintext highlighter-rouge">differentiable</code>하므로 end-to-end learning 가능!!<br/> a sequence of samples \(\boldsymbol t = {t_1, t_2, \ldots, t_N}\)에 대해<br/> \(\frac{d\hat{C}}{dc_i} |_{\boldsymbol t} = T_i \alpha_{i}\) \(\frac{d\hat{C}}{d \sigma_{i}} |_{\boldsymbol t} = c_i \times (\frac{dT_i}{d \sigma_{i}} \alpha_{i} + \frac{d \alpha_{i}}{d \sigma_{i}} T_i) = c_i \times (0 + \delta_{i}e^{-\sigma_{i}\delta_{i}} T_i) = \delta_{i} T_i c_i e^{- \sigma_{i} \delta_{i}}\)</p> <blockquote> <p>alpha-compositing :<br/> 여러 frame을 합쳐서 하나의 image로 합성하는 과정으로, 각 frame pixel마다 alpha 값(불투명도 값)(0~1)이 있어 겹치는 부분의 pixel 값을 결정</p> </blockquote> <p>By divide-and-conquer approach (tail recursion),<br/> \(c = \alpha_{1}c_{1} + (1 - \alpha_{1})(\alpha_{2}c_{2} + (1 - \alpha_{2})(\cdots)) = \alpha_{1}c_{1} + (1 - \alpha_{1})\alpha_{2}c_{2} + (1 - \alpha_{1})(1 - \alpha_{2})(\cdots) = \cdots = \sum_{i=1}^{N}(\alpha_{i}c_{i}\prod_{j=1}^{i-1}(1-\alpha_{j}))\) where \(\alpha_{0} = 0\)</p> <p>If \(\alpha_{i} = 1 - \exp(-\sigma_{i} \delta_{i})\),<br/> NeRF volume rendering 식 \(\hat{C}(r) = \sum_{i=1}^{N} T_i \alpha_{i} c_i\)과<br/> alpha-compositing 식 \(c = \sum_{i=1}^{N}(\alpha_{i}c_{i}\prod_{j=1}^{i-1}(1-\alpha_{j}))\)은<br/> SAME!!</p> <h2 id="optimizing-a-neural-radiance-field">Optimizing a Neural Radiance Field</h2> <h4 id="positional-encoding-pre-processing">Positional encoding (pre-processing)</h4> <p>If we use input directly, MLP is biased to learn low-frequency function (oversmoothed appearance) (no detail)<br/> If we map input into <code class="language-plaintext highlighter-rouge">higher dim.</code> space which contains info. from low frequency to high frequency, MLP can fit data with <code class="language-plaintext highlighter-rouge">high-frequency variation</code><br/> Due to positional encoding, MLP can behave as <code class="language-plaintext highlighter-rouge">interpolation function</code> where \(L\) determines the bandwidth of the interpolation kernel <d-cite key="interpolation">[1]</d-cite><br/> \(r : R \rightarrow R^{2L}\) <br/> \(r(p) = (sin(2^0\pi p), cos(2^0\pi p), \cdots, sin(2^{L-1}\pi p), cos(2^{L-1}\pi p))\)<br/> \(L=10\) for \(r(x)\) where x has three coordinates<br/> \(L=4\) for \(r(d)\) where d has three components of the cartesian viewing direction unit vector</p> <h4 id="hierarchical-volume-sampling">Hierarchical volume sampling</h4> <p>Densely evaluating N points by stratified sampling is inefficient<br/> =&gt; We don’t need much sampling at free space or occluded regions<br/> =&gt; Hierarchical sampling enables us to allocate more samples to regions we expect to contain visible content</p> <p>We simultaneously optimize 2 networks with different sampling : <code class="language-plaintext highlighter-rouge">coarse</code> and <code class="language-plaintext highlighter-rouge">fine</code></p> <blockquote> <p>coarse sampling \(N_c\)개 : 위에서 배웠던 내용<br/> author divides a ray into \(N_c\) = 64 bins(intervals), and chooses one point \(t_i\) for each bin by uniform sampling<br/> \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N_c}(t_f - t_n), t_n + \frac{i}{N_c}(t_f - t_n)\right]\)</p> </blockquote> <blockquote> <p>fine sampling \(N_f\)개 : 새로운 내용<br/> coarse sampling model’s output is a <code class="language-plaintext highlighter-rouge">weighted sum of all coarse-sampled colors</code><br/> \(\hat{C}(r) = \sum_{i=1}^{N_c} T_i \alpha_{i} c_i = \sum_{i=1}^{N_c} w_i c_i\)<br/> where we define \(w_i = T_i \alpha_{i} = T_i (1 - \exp(-\sigma_{i} \delta_{i}))\) for \(i=1,\cdots,N_c\)<br/> =&gt; Given the output of <code class="language-plaintext highlighter-rouge">coarse</code> network, we try more informed (better) sampling where samples are biased toward the <code class="language-plaintext highlighter-rouge">relevant parts of the scene volume</code><br/> =&gt; We sample \(N_f\)=128 <code class="language-plaintext highlighter-rouge">fine</code> points following a <code class="language-plaintext highlighter-rouge">piecewise-constant PDF</code> of normalized \(\frac{w_i}{\sum_{j=1}^{N_c} w_j}\)<br/> =&gt; Here, we use <code class="language-plaintext highlighter-rouge">Inverse CDF Method</code> for sampling fine points</p> </blockquote> <blockquote> <p>Inverse transform sampling = Inverse CDF Method :<br/> =&gt; PDF (probability density function) : \(f_X(x)\)<br/> =&gt; CDF (cumulative distribution function) : \(F_X(x) = P(X \leq x) = \int_{-\infty}^x f_X(x) dx\)<br/> idea : 모든 확률 분포의 CDF는 Uniform distribution을 따른다!<br/> =&gt; 따라서 CDF의 y값을 Uniform sampling하면, 그 y에 대응되는 x에 대한 (특정 PDF를 따르는) sampling을 구현할 수 있다!<br/> =&gt; 즉, CDF의 역함수를 계산할 수 있다면, 기본 난수 생성기인 Uniform sampling을 이용해서 확률 분포 X에 대한 sampling을 할 수 있다!<br/> \(F_X(x)\) ~ \(U\left[0, 1\right]\)<br/> \(X\) ~ \(F^{-1}(U\left[0, 1\right])\)</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/4-480.webp 480w,/assets/img/2024-04-10-NeRF/4-800.webp 800w,/assets/img/2024-04-10-NeRF/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We evaluate <code class="language-plaintext highlighter-rouge">coarse</code> network using \(N_c\)=64 points per ray<br/> We evaluate <code class="language-plaintext highlighter-rouge">fine</code> network using \(N_c+N_f\)=64+128=192 points per ray where we sample 128 fine points following PDF of <code class="language-plaintext highlighter-rouge">coarse</code> sampled points<br/> In result, we use a total of 64+192=256 samples per ray to compute the final rendering color \(C(r)\)</p> <h4 id="implementation-details--loss">Implementation details &amp; Loss</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/6-480.webp 480w,/assets/img/2024-04-10-NeRF/6-800.webp 800w,/assets/img/2024-04-10-NeRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>Prepare RGB images, corresponding camera poses, intrinsic parameters and scene bounds (use COLMAP structure-from-motion package to estimate these parameters)</li> <li>From H x W input image, randomly sample a batch of 4096 pixels</li> <li>calculate continuous ray from each pixel \(r(t) = o + kd\)</li> <li>coarse sampling of \(N_c\)=64 points per each ray \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N_c}(t_f - t_n), t_n + \frac{i}{N_c}(t_f - t_n)\right]\)</li> <li>positional encoding \(r(x)\) and \(r(d)\) for input</li> <li>obtain volume density \(\sigma\) by MLP with \(r(x, y, z)\) as input</li> <li>obtain color \(c\) by MLP with \(r(x, y, z)\) and \(r(\theta, \phi)\) as input</li> <li>obtain rendering color of each ray by volume rendering \(\hat{C}(r) = \sum_{i=1}^{N} T_i (1 - \exp(- \sigma_{i} \delta_{i})) c_i\) from two networks ‘coarse’ and ‘fine’</li> <li>compute loss</li> <li>Adam optimizer with learning rate from \(5 \times 10^{-4}\) to \(5 \times 10^{-5}\)</li> <li>optimization for a single scene typically takes around 100-300k iterations (1~2 days) to converge on a single NVIDIA V100 GPU</li> </ol> <p>Here, we use L2 norm for loss<br/> \(L = \sum_{r \in R} \left[{\left\|\hat{C_c}(r)-C(r)\right\|}^2+{\left\|\hat{C_f}(r)-C(r)\right\|}^2\right]\)<br/> \(C(r)\) : GT pixel RGB color<br/> \(\hat{C_c}(r)\) : rendering RGB color from coarse network : to allocate better samples in fine network<br/> \(\hat{C_f}(r)\) : rendering RGB color from fine network : our goal<br/> \(R\) : the set of all pixels(rays) across all images</p> <h2 id="results">Results</h2> <h4 id="datasets">Datasets</h4> <p>Synthetic renderings of objects</p> <ul> <li>Diffuse Synthetic 360 : 4 Lambertian objects with simple geometry</li> <li>Realistic Synthetic 360 : 8 non-Lambertian objects with complicated geometry</li> </ul> <p>Real images of complex scenes</p> <ul> <li>Real Forward-Facing : 8 scenes captured with a handheld cellphone</li> </ul> <h4 id="measurement">Measurement</h4> <ul> <li>PSNR(Peak Signal-to-Noise Ratio) \(\uparrow\) : the ratio between the maximum possible power of a signal and the power of corrupting noise \(10\log_{10}\left(\frac{(MAX)^2}{MSE}\right)\)[dB]</li> <li>SSIM(Structural Similarity Index Map) \(\uparrow\) : compare image qualities in three ways: Lumincance(\(l\)), Contrast(\(c\)), Structural(\(s\))<br/> SSIM(x, y) = \([l(x,y)]^{\alpha}[c(x,y)]^{\beta}[s(x,y)]^{\gamma}=\frac{(2\mu_{x}\mu_{y}+C_1)(2\sigma_{xy}+C_2)}{(\mu_{x}^2+\mu_{y}^2+C_1)(\sigma_{x}^2+\sigma_{y}^2+C_2)}\) where \(l(x,y)=\frac{(2\mu_{x}\mu_{y}+C_1)}{\mu_{x}^2+\mu_{y}^2+C_1}\) and \(c(x,y)=\frac{(2\sigma_{x}\sigma_{y}+C_2)}{\sigma_{x}^2+\sigma_{y}^2+C_2}\) and \(s(x,y)=\frac{\sigma_{xy}+C_3}{\sigma_{x}\sigma_{y}+C_3}\)<br/> SSIM calculator :<br/> https://darosh.github.io/image-ssim-js/test/browser_test.html</li> <li>LPIPS \(\downarrow\)</li> </ul> <h4 id="comparisons">Comparisons</h4> <ul> <li>Neural Volumes (NV) :<br/> It synthsizes novel views of objects that lie entirely within a bounded volume in front of a distinct background.<br/> It optimizes 3D conv. network to predict a discretized RGB\(\alpha\) voxel grid and a 3D warp grid.<br/> It renders novel views by marching rays through the warped voxel grid</li> <li>Scene Representation Networks (SRN) :<br/> It represents continuous scene as an opaque surface.<br/> MLP maps each 3D coordinate to a feature vector, and we optimize RNN to predict the next step size along the ray using the feature vector.<br/> The feature vector from the final step is decoded into a color for that point on the surface. Note that SRN is followup to DeepVoxels by the same authors.</li> <li>Local Light Field Fusion (LLFF) :<br/> designed for producing novel views for well-sampled forward facing scenes<br/> trained 3D conv. network directly predicts a discretized frustum-sampled RGB\(\alpha\) grid (multiplane image), and then renders novel views by alpha-compositing and blending</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/7-480.webp 480w,/assets/img/2024-04-10-NeRF/7-800.webp 800w,/assets/img/2024-04-10-NeRF/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Test for scenes from author's new synthetic dataset </div> <p>LLFF exhibits banding and ghosting artifacts<br/> SRN produces blurry and distorted renderings<br/> NV cannot capture the details<br/> NeRF captures fine details in both geometry and appearance</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/8-480.webp 480w,/assets/img/2024-04-10-NeRF/8-800.webp 800w,/assets/img/2024-04-10-NeRF/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Test for read-world scenes </div> <p>LLFF may have repeated edges because of blending between multiple renderings<br/> NeRF also correctly reconstruct partially-occluded regions<br/> SRN does not capture any high-frequency fine detail</p> <h4 id="discussion">Discussion</h4> <h4 id="ablation-studies">Ablation Studies</h4> <h2 id="conclusion">Conclusion</h2> <p>prior : MLP outputs discretized voxel representations<br/> author : MLP outputs volume density and view-dependent emitted radiance</p> <h2 id="future-work">Future Work</h2> <p>efficiency :<br/> Rather than hierarchical sampling, there is still much more progress to be made for efficient optimization and rendering of neural radiance fields</p> <p>interpretability :<br/> voxel grids or meshes admits reasoning about the expected quality, but it is unclear to analyze these issues when we encode scenes into the weights of MLP</p>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="rendering"/><category term="3d"/><summary type="html"><![CDATA[representing scenes as neural radiance fields for view synthesis]]></summary></entry><entry><title type="html">SfMLearner</title><link href="https://semyeong-yu.github.io/blog/2024/SfMLearner/" rel="alternate" type="text/html" title="SfMLearner"/><published>2024-04-06T17:00:00+00:00</published><updated>2024-04-06T17:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/SfMLearner</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/SfMLearner/"><![CDATA[<h1 id="unsupervised-learning-of-depth-and-ego-motion-from-video">Unsupervised Learning of Depth and Ego-Motion from Video</h1> <h4 id="tinghui-zhou-matthew-brown-noah-snavely-david-g-lowe">Tinghui Zhou, Matthew Brown, Noah Snavely, David G. Lowe</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/1704.07813">https://arxiv.org/abs/1704.07813</a><br/> code :<br/> <a href="https://github.com/tinghuiz/SfMLearner">https://github.com/tinghuiz/SfMLearner</a></p> </blockquote> <h2 id="introduction">Introduction</h2> <ul> <li>SfM : Structure from Motion<br/> end-to-end unsupervised learning from monocular video (only one camera lens)</li> <li><code class="language-plaintext highlighter-rouge">single-view</code> depth estimation by per-pixel depth map</li> <li><code class="language-plaintext highlighter-rouge">multi-view</code> camera motion (= <code class="language-plaintext highlighter-rouge">ego-motion</code> = <code class="language-plaintext highlighter-rouge">pose</code>) by <code class="language-plaintext highlighter-rouge">6-DoF transformation matrices</code></li> <li><code class="language-plaintext highlighter-rouge">unsupervised</code> learning : 직접적인 GT data가 아니라 <code class="language-plaintext highlighter-rouge">view synthesis (reconstruction term)를 supervision</code>으로 씀</li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li>simultaneous estimation of structure and motion through deep learning</li> <li>end-to-end learning of transformation matrix without learning geometry explicitly</li> <li>learning of 3D single-view from registered 2D views</li> <li>unsupervised/self-supervised learning from video</li> </ul> <h2 id="method">Method</h2> <h4 id="approach">Approach</h4> <p>Assumption :<br/> Scenes, which we are interested in, are mostly rigid, so changes across different frames are dominated by camera motion</p> <h4 id="view-synthesis-as-supervision">View Synthesis as supervision</h4> <ul> <li>View Synthesis : as supervision of depth and pose (추후 설명 예정)</li> <li>loss function (reconstruction term) :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/1-480.webp 480w,/assets/img/2024-04-06-SfMLearner/1-800.webp 800w,/assets/img/2024-04-06-SfMLearner/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>\(p\) : index of target view’s pixel coordinates<br/> \(s\) : index of source views<br/> \(I_{t}(p)\) : target view<br/> \(\hat I_{s}(p)\) : source view warped to target coordinate frame (= reconstructed target view) using predicted depth \(\hat D_{t}\) and \(4 \times 4\) camera transformation matrix \(\hat T_{t \rightarrow s}\) and source view \(I_{s}\)</p> <ul> <li>pipeline for depth and pose estimation :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/2-480.webp 480w,/assets/img/2024-04-06-SfMLearner/2-800.webp 800w,/assets/img/2024-04-06-SfMLearner/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="differentiable-depth-image-based-rendering">Differentiable depth image-based rendering</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/3-480.webp 480w,/assets/img/2024-04-06-SfMLearner/3-800.webp 800w,/assets/img/2024-04-06-SfMLearner/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li> <p><code class="language-plaintext highlighter-rouge">Depth CNN</code>을 통해 <code class="language-plaintext highlighter-rouge">target view</code> (single view)로부터 <code class="language-plaintext highlighter-rouge">depth prediction</code> \(\hat D_{t}\) 얻기</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Pose CNN</code>을 통해 <code class="language-plaintext highlighter-rouge">target &amp; source view</code> (multi-view)로부터 \(4 \times 4\) <code class="language-plaintext highlighter-rouge">camera transformation matrix</code> \(\hat T_{t \rightarrow s}\) 얻기</p> </li> <li>target view의 pixels를 source view coordinate으로 <code class="language-plaintext highlighter-rouge">project</code>하기<br/> 값이 아니라 <code class="language-plaintext highlighter-rouge">대응되는 위치</code>를 구하기 위해<br/> projection할 때 depth와 pose 이용 <ul> <li>monocular camera이므로 두 카메라 사이의 상대적인 위치를 설명하는 \([R \vert t]\)는 고려 안함</li> <li>\(K^{-1}p_{t}\) : target view coordinate에서 2D 좌표 \(\rightarrow\) 3D 좌표</li> <li>\(\hat D_{t}(p_{t})K^{-1}p_{t}\) : target view의 3D depth map (= 2D depth \(\times\) 3D 좌표)<br/> full 3D volumetric은 아니고, surface만 나타내는 3D target</li> <li>\(\hat T_{t \rightarrow s} \hat D_{t}(p_{t})K^{-1}p_{t}\) : 3D depth map projected from target view to source view</li> <li>\(K \hat T_{t \rightarrow s} \hat D_{t}(p_{t})K^{-1}p_{t}\) : source view coordinate에서 3D 좌표 \(\rightarrow\) 2D 좌표<br/> <code class="language-plaintext highlighter-rouge">target view의 pixel 좌푯값을 source view의 좌푯값으로 project하는 데 중간에 depth map이 왜 필요한 거지???</code></li> </ul> </li> <li>source view coordinate에서 differentiable bilinear <code class="language-plaintext highlighter-rouge">interpolation</code>으로 value 얻은 뒤 <code class="language-plaintext highlighter-rouge">warp to target coordinate</code> (= <code class="language-plaintext highlighter-rouge">reconstructed target view</code>)<br/> source view의 pixel 값들을 이용해서 reconstruct target view</li> </ol> <h4 id="modeling-the-model-limitation">Modeling the model limitation</h4> <p>Assumption :</p> <ol> <li> <p>objects are static except camera (changes are dominated by camera motion)<br/> 물체들이 움직이지 않아야 Depth CNN과 Pose CNN이 같은 coordinate에 대해 project할 수 있다.</p> </li> <li> <p>there is no occlusion/disocclusion between target view and source view<br/> target view와 source views 중 하나라도 물체가 가려져서 안보인다면 projection 정보가 없어 학습에 문제가 된다.</p> </li> <li> <p>surface is Lambertain so that photo-consistency error is meaningful<br/> 어떤 방향에서 보든 표면이 isotropic 똑같은 밝기로 보인다고 가정 \(\rightarrow\) photo-consistency에 차이가 있을 경우 이는 다른 surface를 의미함</p> </li> </ol> <h4 id="overcoming-the-gradient-locality-at-loss-term">Overcoming the gradient locality at loss term</h4> <ol> <li> <p>To improve robustness, train additional network which predicts <code class="language-plaintext highlighter-rouge">explainability soft mask</code> \(\hat E_{s}\) (= <code class="language-plaintext highlighter-rouge">per-pixel weight</code>), and add it to reconstruction loss term.<br/> deep-learning model은 black-box이므로 explainablity는 중요한 요소</p> </li> <li> <p>trivial sol. \(\hat E_{s} = 0\)을 방지하기 위해, add <code class="language-plaintext highlighter-rouge">regularization</code> term that encourages nonzero prediction of \(\hat E_{s}\)</p> </li> <li> <p>직접 pixel intensity difference로 reconstruction loss를 얻으므로, GT depth &amp; pose로 project하여 얻은 \(p_{s}\) 가 low-texture region or far region에 있을 경우 training 방해 (common issue in motion estimation)<br/> \(\rightarrow\) 해결 1. use conv. encoder-decoder with small bottleneck<br/> \(\rightarrow\) 해결 2. add <code class="language-plaintext highlighter-rouge">multi-scale</code> and <code class="language-plaintext highlighter-rouge">smoothness loss</code> term<br/> (less sensitive to architecture choice, so 이 논문은 해결 2. 적용)</p> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/4-480.webp 480w,/assets/img/2024-04-06-SfMLearner/4-800.webp 800w,/assets/img/2024-04-06-SfMLearner/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> s : source view image index / p : target view pixel index </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/5-480.webp 480w,/assets/img/2024-04-06-SfMLearner/5-800.webp 800w,/assets/img/2024-04-06-SfMLearner/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> l : multi-scale / s : source view image index </div> <h4 id="network-architecture">Network Architecture</h4> <ul> <li>Network 1. <code class="language-plaintext highlighter-rouge">Single-view Depth CNN</code><br/> input : target view<br/> output : per-pixel depth map<br/> DispNet encoder-decoder architecture</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/6-480.webp 480w,/assets/img/2024-04-06-SfMLearner/6-800.webp 800w,/assets/img/2024-04-06-SfMLearner/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Network 2. <code class="language-plaintext highlighter-rouge">Multi-view Pose CNN</code> (아래 figure의 파란 부분)<br/> input : target view concatenated with all source views<br/> output : 6-DoF relative poses between target view and each source view<br/> (Pose CNN estimates <code class="language-plaintext highlighter-rouge">6 channels (3 Euler angles + 3D translation vector)</code> for each source view, and then it is converted to \(4 \times 4\) <code class="language-plaintext highlighter-rouge">transformation matrix</code>)</li> </ul> <p><code class="language-plaintext highlighter-rouge">어떻게 transformation matrix로 변환???</code></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/7-480.webp 480w,/assets/img/2024-04-06-SfMLearner/7-800.webp 800w,/assets/img/2024-04-06-SfMLearner/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/7-480.webp 480w,/assets/img/2024-04-06-SfMLearner/7-800.webp 800w,/assets/img/2024-04-06-SfMLearner/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Network 3. <code class="language-plaintext highlighter-rouge">Explainablity soft mask</code> (= <code class="language-plaintext highlighter-rouge">reconstruction weight per pixel</code>) (위의 figure의 빨간 부분)<br/> output : multi-scale explainability masks<br/> (it estimates <code class="language-plaintext highlighter-rouge">2 channels</code> for each source view at each prediction layer)</li> </ul> <p><code class="language-plaintext highlighter-rouge">weight per pixel인데 왜 2 channels are needed for explainability mask???</code></p> <h2 id="experiments">Experiments</h2> <p>Train : BN, Adam optimizer, monocular camera (one camera lens), resize input image<br/> Test : arbitrary input image size</p> <h4 id="single-view-depth-estimation">Single-view depth estimation</h4> <ul> <li>train model on the split (exclude frames from test sequences and exclude static scene’s pixels with mean optical flow magnitude &lt; 1)</li> <li>pre-trained on Cityscapes dataset / fine-tuned on KITTI dataset / test on Make3D dataset</li> <li>may improve if we also use left-right cycle consistency loss</li> <li>ablation study 결과, explainablity mask를 추가하고 fine-tuning하는 게 더 좋은 성능 도출</li> </ul> <h4 id="multi-view-pose-estimation">Multi-view pose estimation</h4> <ul> <li>trained on KITTI odometry(change in position over time by motion sensor) dataset</li> <li>measurement :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/8-480.webp 480w,/assets/img/2024-04-06-SfMLearner/8-800.webp 800w,/assets/img/2024-04-06-SfMLearner/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>ATE : Absolute Trajectory Error<br/> left/right turning magnitude : coordinate diff. in the side-direction between start and ending frame at test<br/> Mean Odom. : mean of car motion for 5-frame snippets from GT odometry dataset<br/> ORB-SLAM(full) : recover odometry using all frames for loop closure and re-localization<br/> ORB-SLAM(short) : Ours에서처럼, use 5-frame snippets as input<br/> \(\rightarrow\) 특히 small left/right turning magnitude (car is mostly driving forward) 상황에서 Ours가 ORB-SLAM(short)보다 성능 더 좋으므로 monocular SLAM system의 local estimation module을 Ours가 대체할 수 있을 것이라 예상​<br/> (<code class="language-plaintext highlighter-rouge">SLAM 논문 아직 안 읽어봄. 읽어보자.</code>)</p> <h4 id="visualizing-explainability-prediction">Visualizing Explainability Prediction</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/9-480.webp 480w,/assets/img/2024-04-06-SfMLearner/9-800.webp 800w,/assets/img/2024-04-06-SfMLearner/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> highlighted pixels at explainability mask : predicted to be unexplainable </div> <p>explainability = per-pixel weight (confidence 느낌) for reconstruction</p> <p>row 1 ~ 3 : due to motion (dynamic objects are unexplainable)<br/> row 4 ~ 5 : due to occlusion/visibility (disappeared objects are unexplainable)<br/> row 6 ~ 7 : due to other factors (e.g. depth CNN has low confidence on thin structures)</p> <h2 id="discussion">Discussion</h2> <h4 id="contribution">Contribution</h4> <ul> <li>end-to-end <code class="language-plaintext highlighter-rouge">unsupervised</code> learning from <code class="language-plaintext highlighter-rouge">monocular</code> sequences<br/> (기존에는 gt depth로 depth supervision 또는 calibrated stereo images로 pose supervision이었지만, 본 논문은 <code class="language-plaintext highlighter-rouge">view synthesis (reconstruction)을 supervision으로</code> 써서 unsupervised learning으로도 comparable performance 달성)</li> <li>depth CNN recognizes common structural features of objects, and pose CNN uses image correspondence with estimating camera motion</li> </ul> <h4 id="limitation">Limitation</h4> <ol> <li> <p><code class="language-plaintext highlighter-rouge">dynamic objects (X) / occlusion (X) / must be Lambertain surface / vast open scenes (X) / when objects are close to the front of camera (X) / thin structure (X)</code><br/> \(\rightarrow\) 위의 한계들을 개선하고자 explainablity mask (= per-pixel reconstruction confidence 느낌) 도입했지만, it is implicit consideration</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">assume that camera intrinsic K is given</code>, so not generalized to the random videos with unknown camera types</p> </li> <li> <p>predict simplified 3D depth map of <code class="language-plaintext highlighter-rouge">surface</code> (<code class="language-plaintext highlighter-rouge">not full 3D volumetric representation</code>)</p> </li> </ol> <p>중간중간에 있는 질문들은 아직 이해하지 못해서 남겨놓은 코멘트입니다.<br/> 추후에 다시 읽어보고 이해했다면 업데이트할 예정입니다.<br/> 혹시 알고 계신 분이 있으면 댓글로 남겨주시면 감사하겠습니다!</p>]]></content><author><name></name></author><category term="depth-estimation"/><category term="unsupervised"/><category term="depth"/><category term="ego"/><category term="motion"/><category term="video"/><summary type="html"><![CDATA[Unsupervised Learning of Depth and Ego-Motion from Video]]></summary></entry><entry><title type="html">Monocular Depth Estimation with Left Right Consistency</title><link href="https://semyeong-yu.github.io/blog/2024/Monocular_Depth_Estimation_LR_Consistency/" rel="alternate" type="text/html" title="Monocular Depth Estimation with Left Right Consistency"/><published>2024-04-05T17:00:00+00:00</published><updated>2024-04-05T17:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Monocular_Depth_Estimation_LR_Consistency</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Monocular_Depth_Estimation_LR_Consistency/"><![CDATA[<h1 id="unsupervised-monocular-depth-estimation-with-left-right-consistency">Unsupervised Monocular Depth Estimation with Left-Right Consistency</h1> <h4 id="clement-godard-oisin-mac-aodha-gabriel-j-brostow">Clement Godard, Oisin Mac Aodha, Gabriel J. Brostow</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/1609.03677">https://arxiv.org/abs/1609.03677</a><br/> referenced blog :<br/> <a href="https://blog.naver.com/dncks1107/223104039030">https://blog.naver.com/dncks1107/223104039030</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ul> <li>unsupervised mono (single image as input) depth estimation</li> <li>need for binocular stereo image pairs at training</li> <li>novel loss function : left-right consistency b.w. disparity maps</li> </ul> </blockquote> <h2 id="backgrounds">Backgrounds</h2> <h4 id="stereo-depth-estimation">Stereo Depth Estimation</h4> <p>인간은 물체를 두 개의 눈을 통해 바라보고 그 차이를 이용하여 대상까지의 거리를 예측한다. AI는 이러한 인간의 시각 시스템을 모방하여 stereo depth estimation을 통해 대상까지의 깊이를 추정할 수 있다.<br/> Stereo image란 카메라 두 대를 사용하여 찍은 두 이미지를 의미하고, disparity는 한 쌍의 stereo image 간의 pixel difference를 의미한다.<br/> <code class="language-plaintext highlighter-rouge">stereo depth estimation은 stereo image 한 쌍 (left image, right image)을 network의 input</code>으로 넣어 이미지 간의 disparity를 통해 depth를 추정하는 것이다.<br/> <code class="language-plaintext highlighter-rouge">stereo depth estimation의 경우, epipolar geometry라는 수학적 원리에 의해 depth를 계산하기 때문에 비교적 정확하지만 카메라와 물체 사이의 거리가 멀어질수록 불리해진다.</code></p> <h4 id="monocular-depth-estimation">Monocular Depth Estimation</h4> <p><code class="language-plaintext highlighter-rouge">monocular depth estimation은 위와 달리 하나의 image만을 network의 input</code>으로 넣어 depth를 추정하는 방법이다. 물론 test-phase에서 하나의 image를 input으로 넣겠다는 의미이고, 이 논문의 경우 training loss를 구할 때는 stereo image 쌍을 모두 이용하였다.<br/> <code class="language-plaintext highlighter-rouge">mono depth estimation의 경우, 믿을 만한 근거(epipolar geometry와 같은 수학적 원리)가 없기 때문에 정확도가 떨어지지만 하나의 image만 input으로 넣기 때문에 전처리 과정이 간단하고 메모리도 덜 필요로 하여, 어느 정도의 정확도만 확보된다면 실생활에서 적용 가능 범위가 더 넓다.</code></p> <h4 id="monocular-and-stereo-camera">Monocular and Stereo Camera</h4> <p><code class="language-plaintext highlighter-rouge">monocular camera</code> : 특정 시간 t에 한 개의 camera 렌즈를 사용<br/> <code class="language-plaintext highlighter-rouge">stereo camera</code> : 특정 시간 t에 6~7cm 떨어진 두 개의 camera 렌즈를 사용</p> <h2 id="abstract">Abstract</h2> <p>기존의 supervised depth estimation 방식은 성능은 좋지만, 구하기 어려운 pixel-wise ground-truth depth data를 대량으로 필요로 한다는 단점이 있었다. 그래서 본 논문의 저자는 ground-truth depth 정보가 없는 stereo image 쌍으로부터 pixel-level depth map을 합성하도록 훈련하는 unsupervised depth estimation 방식을 제안한다. 효과적인 표기를 위해 아래의 notation을 사용하자.<br/> \(I^{l}\) : left image<br/> \(I^{r}\) : right image<br/> \(d^{r}\) : disparity map from left to right<br/> \(d^{l}\) : disparity map from right to left</p> <p>그렇다면 stereo image \(I^{l}, I^{r}\)로부터 어떻게 depth를 추정할까? <code class="language-plaintext highlighter-rouge">image rectfiication</code>을 거친 뒤, depth를 직접 예측하는 게 아니라 우선 두 개의 <code class="language-plaintext highlighter-rouge">disparity map (dense correspondence field)</code> \(d^{r}, d^{l}\) 을 생성한다. 여기서 disparity map이란, image의 한 pixel이 다른 image의 어느 pixel에 대응하는지에 대한 정보를 의미한다. 이후 \(I^{l}\)과 \(d^{r}\)을 이용하여 \(I^{r \ast} = I^{l}(d^{r})\)을 reconstruct하고, \(I^{r}\)과 \(d^{l}\)을 이용하여 \(I^{l \ast} = I^{r}(d^{l})\)을 reconstruct 한 뒤, \(I^{r \ast}\)과 \(I^{r}\) 간의 reconstruction loss와 \(I^{l \ast}\)과 \(I^{l}\) 간의 <code class="language-plaintext highlighter-rouge">reconstruction loss</code>를 이용하여 모델을 학습시킨다. 그런데 reconstruction loss만 사용한다면 depth image의 quality가 저하된다고 한다. 따라서 본 논문의 저자는 ​\(d^{r}\)과 (projected \(d^{l}\)) = \(d^{l}(d^{r})\) 간의 차이도 고려하는 <code class="language-plaintext highlighter-rouge">left-right disparity consistency loss</code>라는 논문의 핵심 아이디어를 제안하였다.</p> <h2 id="contribution">Contribution</h2> <ul> <li>end-to-end unsupervised monocular depth estimation</li> <li>new training loss that enforces left-right disparity consistency</li> </ul> <h2 id="related-work">Related Work</h2> <h4 id="supervised-stereo-depth-estimation">supervised stereo depth estimation</h4> <p>DispNet (Mayer et al.) :<br/> directly predict the disparity for each pixel by regression loss<br/> 단점 : need lots of ground-truth disparity data and stereo image pairs, which are hard to obtain in real-world</p> <h4 id="unsupervised-depth-estimation">unsupervised depth estimation</h4> <p>Deep Stereo (Flynne et al.) :<br/> select pixels from nearby images and generate new views by using the relative pose of multiple cameras<br/> 단점 : At test phase, need several nearby posed images, which is not mono depth estimation</p> <p>Deep3D (Xie et al.) :<br/> make a distribution over all the possible disparities for each pixel and generate right view from an input left image by using image reconstruction loss<br/> 단점 : need much memory if there are lots of possible disparities. So, it is not scalable to bigger output resolutions</p> <p>Garg et al. :<br/> 본 논문과 유사하게 unsupervised mono depth estimation with image reconstruction loss<br/> 단점 : not fully differentiable (이를 보완하고자 Taylor approximation을 수행하긴 했지만 이는 more challenging to optimize)</p> <h2 id="method">Method</h2> <h4 id="depth-estimation-as-image-reconstruction">Depth Estimation as Image Reconstruction</h4> <p>핵심 아이디어 : calibrated binocular(stereo) camera로 같은 시간에 찍은 한 쌍의 stereo image가 주어졌을 때, <code class="language-plaintext highlighter-rouge">하나의 image로부터 다른 image를 reconstruct 할 수 있다면 그 장면의 3D 구조를 알 수 있다!</code></p> <p>우선 a stereo image pair에 대해 image rectification을 거친 뒤 만약 <code class="language-plaintext highlighter-rouge">disparity map을 얻었다면 아래의 도식에 의해 depth map으로 변환</code>할 수 있다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/1.JPG-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/1.JPG-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/1.JPG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/1.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>b : baseline distance between two camera centers (상수)<br/> f : camera focal length (상수)<br/> 가로로 뻗은 직선 : rectified image plane<br/> d : predicted disparity<br/> d^ : depth<br/> b : d^ = b - d : d^ - f 이므로 d^ (b - d) = b (d^ - f) 이고, 이를 정리하면 d^d = bf, 즉 <code class="language-plaintext highlighter-rouge">d^ = bf / d</code> 이다.<br/> 만약 disparity \(d = x_{r} - x_{l}\) 과 depth Z = d^를 얻었다면, 아래의 도식으로 X, Y 값도 얻을 수 있어서 3D point 좌표를 알 수 있다.<br/> <code class="language-plaintext highlighter-rouge">(아래의 도식은 뭘 말하는거지?)</code><br/> \(x = \frac{f \cdot X}{Z} + p_x\)</p> <h4 id="depth-estimation-network">Depth Estimation Network</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/2.JPG-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/2.JPG-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/2.JPG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/2.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>위의 figure에서 볼 수 있듯이 Naive 버전은 input left image와 align할 output reconstructed left image가 없다. 한편, No LR 버전은 align할 output reconstructed left image는 존재하지만, <code class="language-plaintext highlighter-rouge">left-right consistency가 보장되지 않기 때문에 'texture-copy' artifacts와 depth discontinuities(boundaries)에서의 errors가 생기는 문제가 있다.</code> 본 논문의 model은 disparity \(d^{r}, d^{l}\) 을 동시에 추론함으로써 이러한 문제들을 모두 해결하였다.<br/> 위의 figure에서 볼 수 있듯이 mono depth estimation이므로 CNN의 <code class="language-plaintext highlighter-rouge">input으로 left image만을 넣어서 disparity dr, dl 을 동시에 추론</code>하였다. 이를 통해 두 disparity 간의 consistency를 어느 정도 강제할 수 있고 결과적으로 더 정확한 depth estimation이 가능해진다. 참고로 <code class="language-plaintext highlighter-rouge">right image는 image reconstruction과 training loss를 구할 때만 사용</code>된다.<br/> disparity를 구한 뒤에는 <code class="language-plaintext highlighter-rouge">bilinear sampler와 backward mapping을 통해 image reconstruction</code>을 수행한다. 이 때, <code class="language-plaintext highlighter-rouge">STN(spatial transformer network)의 bilinear sampler를 이용하기 때문에 위의 일련의 과정은 fully convolutional and fully differentiable</code>하다.</p> <p>backward mapping :<br/> 결과 영상으로 mapping 되는 원본 영상에서의 좌표를 계산하여 해당 밝기값을 가져온다. 이 때, 원본 영상에서의 좌표는 실숫값이므로 bilinear interpolation (output pixel = the weighted sum of four input pixel)을 사용한다. 결과 영상의 각 pixel에 대해 값을 가져오므로 forward mapping에서의 hole 발생은 일어나지 않는다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/3-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/3-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/4-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/4-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>본 논문의 model은 크게 두 부분으로 나뉜다. : encoder (conv1~conv7b) and decoder (upconv7~)<br/> 본 논문의 model은 output으로서 disparity \(d^{r}, d^{l}\)을 동시에 추론하는데, 이를 <code class="language-plaintext highlighter-rouge">four different output scales</code>에 대해 반복한다.</p> <h4 id="train-loss">Train Loss</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/5-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/5-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/6-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/6-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>\(C_s\) : loss at output scale s</p> <blockquote> <p>\(C_{ap}^{l}\) :<br/> appearance matching loss for left image (<code class="language-plaintext highlighter-rouge">image reconstruction term</code>)<br/> How much \(I^{r}(d^{l})\) appears similar to \(I^{l}\)</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/7-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/7-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>이 때, <code class="language-plaintext highlighter-rouge">SSIM (Structural Similarity Index Measure)</code>는 두 images 간의 차이가 작을수록 1 에 가까운 값을 가지며, 정확한 정의는 아래를 참고하자.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/8-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/8-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>\(C_{ds}^{l}\) :<br/> disparity smoothness loss (<code class="language-plaintext highlighter-rouge">smoothness term</code>)<br/> How much \(d^{l}\) is smooth</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/9-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/9-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>real-world에서 depth가 급격하게 변하는 경우 image boundary 혹은 texture change가 있는 부분이므로 image plane에서도 해당 부분의 image gradient가 크게 나타난다. 따라서 image gradient가 큰 부분에서는 disparity (depth) 변화를 허용하지만, image gradient가 작은 부분에서는 disparity (depth)가 부드럽게 변하도록 하는 것이 disparity smoothness loss의 역할이다.</p> <blockquote> <p>\(C_{lr}^{l}\) :<br/> left-right consistency (<code class="language-plaintext highlighter-rouge">left-right disparity consistency term</code>)<br/> How much \(d^{l}\) and \(d^{r}(d^{l})\) are consistent</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/10-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/10-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>image reconstruction 뿐만 아니라 left-right disparity consistency까지 고려함으로써 <code class="language-plaintext highlighter-rouge">depth estimation의 accuracy</code>를 향상시킬 수 있다.</p> <h2 id="results--limitations">Results &amp; Limitations</h2> <h4 id="results">Results</h4> <p>Train : on Cityscapes and KITTI 2015 dataset using two different test splits<br/> Test : on other datasets like Make3D and CamVid</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/11-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/11-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/12-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/12-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><code class="language-plaintext highlighter-rouge">Post-processing</code> :<br/> original left image로부터 구한 disparity map을 \(d^{l}\) 라 하고,<br/> flipped left image로부터 구한 disparity map을 \(d^{l \ast}\) 라 하고,<br/> 이를 다시 flip한 걸 \(d^{l \ast \ast}\) 라 할 때,<br/> \(d^{l}\)의 경우 stereo disocclusions which create disparity ramps(경사) on both the left side of the image and the left of occluders 가 있을 수 있는데,<br/> \(d^{l \ast \ast}\)의 경우 disparity ramps are located on the right side of the image and the right of occluders 이므로<br/> <code class="language-plaintext highlighter-rouge">We combine both disparity maps to form the final disparity map</code> by assigning the first 5% on the left of the image using \(d^{l \ast \ast}\) and the last 5% on the right to the disparities from \(d^{l}\). The central part of the final disparity map is the average of \(d^{l \ast \ast}\) and \(d^{l}\).<br/> 이러한 post-processing을 통해 can reduce the effect of stereo disocclusions, and lead to better accuracy and less visual artifacts,<br/> but double the amount of test time<br/> (<code class="language-plaintext highlighter-rouge">stereo disocclusions의 영향을 줄이기 위한 post-processing 과정 아직 완벽하게 이해하지는 못했음</code>)</p> <h4 id="limitations">Limitations</h4> <ol> <li> <p>left-right consistency와 post-processing으로 quality 향상을 이룬 건 맞지만, <code class="language-plaintext highlighter-rouge">두 images에서 모두 안 보이는 occlusion region에서의 pixels 때문에 occlusion boundaries에서는 여전히 artifacts가 존재</code>한다. training phase에서 occclusion에 대해 explicitly reasoning하는 것으로 이 문제를 개선할 수는 있지만, supervised methods 또한 모든 pixels에 대해 항상 valid depth를 가지는 것은 아님에 주목할 필요가 있다.</p> </li> <li> <p>training phase에서 <code class="language-plaintext highlighter-rouge">rectified and temporally aligned (image rectification을 거치고 동시에 찍은) stereo image pairs가 필요</code>하다. 이 말은 즉슨, single-view dataset은 training에 쓸 수 없다. (fine-tune하는 것만 가능하다.)</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">image reconstruction term에 의존</code>한다. 이 말은 즉슨, <code class="language-plaintext highlighter-rouge">specular and transparent (거울 같이 반사하는 and 투명한) surfaces에서는 inconsistent depth</code>가 생긴다. 이는 더 정교한 similarity measures를 사용함으로써 개선될 수 있다.</p> </li> </ol> <h2 id="conclusion">Conclusion</h2> <ul> <li>unsupervised mono (single image as input) depth estimation \(\rightarrow\) no need for expensive GT depth</li> <li>need for binocular stereo image pairs at training</li> <li>novel loss function : left-right consistency b.w. disparity maps \(\rightarrow\) improve quality of depth map</li> <li>can generalize to unseen datasets</li> </ul> <h2 id="future-work">Future Work</h2> <ul> <li>extend to videos (can add temporal consistency)</li> <li>investigate sparse input as an alternative training signal<br/> (<code class="language-plaintext highlighter-rouge">?? 이해 못함</code>)</li> <li>our model estimates per-pixel depth, but it would be also interesting to predict the full occupancy of the scene<br/> (<code class="language-plaintext highlighter-rouge">?? 이해 못함</code>)</li> </ul> <p>중간중간에 있는 질문들은 아직 이해하지 못해서 남겨놓은 코멘트입니다.<br/> 추후에 다시 읽어보고 이해했다면 업데이트할 예정입니다.<br/> 혹시 알고 계신 분이 있으면 댓글로 남겨주시면 감사하겠습니다!</p>]]></content><author><name></name></author><category term="depth-estimation"/><category term="unsupervised"/><category term="monocular"/><category term="depth"/><category term="consistency"/><summary type="html"><![CDATA[Unsupervised Monocular Depth Estimation with Left-Right Consistency]]></summary></entry><entry><title type="html">Epipolar Geometry and Image Rectification</title><link href="https://semyeong-yu.github.io/blog/2024/Epipolar_Geometry_Image_Rectification/" rel="alternate" type="text/html" title="Epipolar Geometry and Image Rectification"/><published>2024-04-01T17:00:00+00:00</published><updated>2024-04-01T17:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Epipolar_Geometry_Image_Rectification</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Epipolar_Geometry_Image_Rectification/"><![CDATA[<h2 id="epipolar-geometry">Epipolar Geometry</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/1-480.webp 480w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/1-800.webp 800w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/2-480.webp 480w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/2-800.webp 800w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="image-plane--epipolar-plane--baseline--epipole--epipolar-line">image plane / epipolar plane / baseline / epipole / epipolar line</h3> <p>X : 3D point<br/> \(x_L, x_R\) : projected 2D point in left and right image<br/> 파란색 면 : image plane<br/> 초록색 면 : <code class="language-plaintext highlighter-rouge">epipolar plane</code><br/> \(O_L, O_R\) : center of left and right camera<br/> 직선 \(O_L O_R\) : <code class="language-plaintext highlighter-rouge">baseline</code><br/> epipolar pencil : set of epipolar planes<br/> <code class="language-plaintext highlighter-rouge">epipole</code> : intersection of baseline and image plane<br/> \(e_L, e_R\) : epipole of left and right camera<br/> <code class="language-plaintext highlighter-rouge">epipolar line</code> :</p> <ul> <li>intersection of image plane and epipolar plane</li> <li>빨간 선 \(l_R\) : 직선 \(x_R e_R\) (projected 2D point와 epipole은 epipolar line 위에 있다)</li> <li>left image plane 위의 같은 점 \(x_L\)로 project 되는 모든 3D points \(X, X_1, X_2, \cdots\)를 right image plane에 project했을 때 그려지는 선</li> </ul> <h4 id="normalized-coordinates-pixel-coordinates--intrinsic-extrinsic-parameters--homography-matrix--projection-matrix">normalized coordinates, pixel coordinates / intrinsic, extrinsic parameters / homography matrix / projection matrix</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/3.JPG-480.webp 480w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/3.JPG-800.webp 800w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/3.JPG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/3.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>real-world의 3D 좌표 \((X, Y, Z)\) 에 있는 물체를 카메라에 투영하기 위해 Z (= 깊이) 값을 1로 정규화한 평면을 <code class="language-plaintext highlighter-rouge">normalized plane</code>이라 하고, \((\frac{X}{Z}, \frac{Y}{Z}, 1)\)의 좌표값을 갖는다.<br/> 이를 image로서 나타내기 위해 초점거리를 곱해주고 원점을 정중앙에서 좌상단으로 바꿔서 normalized plane 상의 normalized coordinates \((\frac{X}{Z}, \frac{Y}{Z}, 1)\)을 <code class="language-plaintext highlighter-rouge">image plane 상의 pixel coordinates</code> \((\frac{X}{Z} \ast f_x - \frac{W}{2}, \frac{Y}{Z} \ast f_y - \frac{H}{2}, 1)\) 로 변환할 수 있는데,<br/> 이 때 곱하게 되는 행렬이 바로 intrinsic matrix (= calibration matrix) K 이다. 그리고 이렇게 intrinsic parameters를 구하는 과정을 <code class="language-plaintext highlighter-rouge">Camera Calibration</code> 이라 부른다.<br/> 한편, normalized coordinate \(p = (x, y, 1)\)에 대해 any \(\hat p = (X, Y, Z)\) where \((\frac{X}{Z}, \frac{Y}{Z}) = (x, y)\)를 <code class="language-plaintext highlighter-rouge">homogeneous coordinates</code> for \(p\) 라고 부른다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/10-480.webp 480w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/10-800.webp 800w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>그런데, 카메라의 각도 혹은 위치가 달라지면 맺히는 이미지 자체도 달라지기 때문에 intrinsic matrix를 곱하기 전에 camera의 rotation 및 translation을 먼저 고려해주어야 하는데, 이 때 곱하게 되는 행렬이 바로 extrinsic marix \([R \vert t]\) 이다.</p> <ul> <li> <p><code class="language-plaintext highlighter-rouge">K : intrinsic parameters</code> (3x3 <code class="language-plaintext highlighter-rouge">calibration</code> matrix) (초점거리 곱하고 원점 바꾸는 등 image에 projection하기 위한 카메라 자체의 특성)</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">R, t : extrinsic parameters</code> (3x3 <code class="language-plaintext highlighter-rouge">rotation</code>, 3x1 <code class="language-plaintext highlighter-rouge">translation</code> matrix) (두 카메라의 상대적인 pose(위치 및 각도))</p> </li> </ul> <p>즉, 정리하면 3D point [X, Y, Z]가 image plane [x, y]에 맺히는 projection 과정은 아래의 수식을 따른다.</p> <p>\(\begin{bmatrix} x \\ y \\ z \end{bmatrix}\) = \(\begin{bmatrix} f_x &amp; s &amp; -W/2 \\ 0 &amp; f_y &amp; -H/2 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\) \([R \vert t]\) \(\begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}\)</p> <p>\(s\) : skew due to sensor not mounted perpendicular to the optical axis</p> <ul> <li>\(x_L \Leftrightarrow x_R\) : homography matrix H (projection of 2D point to 2D point)<br/> \(x_R = H x_L\)</li> <li>\(X \Leftrightarrow x_L\) : projection matrix \(P_L\) (projection of 3D point to 2D point)<br/> \(x_L = P_LX\) where \(P_L = K_L[R \vert t]\)</li> </ul> <h3 id="fundamental-matrix">Fundamental matrix</h3> <ol> <li> \[x_R = H x_L\] </li> <li>\(e_R\)과 \(x_R\)은 직선 \(l_R\) 위에 있으므로 \(e_{R}^{T} l_R = 0\) and \(x_{R}^{T} l_R = 0\)<br/> 예를 들어, 직선 2x+y-2z = 0에 대해 \(l_R\)은 (2, 1, -2)이고, \(e_R\) 및 \(x_R\)은 직선 위에 있는 점 (x, y, z)이다.</li> <li>위의 1.과 2.로부터 \(l_R = e_R \circledast x_R = e_R \circledast H x_L = F x_L\)<br/> where F = fundamental matrix = \(e_R \circledast H\)</li> <li>위의 2.와 3.으로부터 \(x_{R}^{T} l_R = x_{R}^{T} F x_L = 0\)</li> <li>위의 2.와 3.으로부터 \(e_{R}^{T} l_R = e_{R}^{T} F x_L = 0\) 이고, 모든 \(x_L\)에 대해 \(e_{R}^{T} F = 0\)을 만족하므로 \(e_R\)은 F의 left null vector이다. (유사한 방법으로 \(e_L\)은 F의 right null vector이다.)</li> </ol> <p>즉, fundamental matrix와 관련된 식을 정리하면</p> <ul> <li><code class="language-plaintext highlighter-rouge">fundamental matrix</code> : \(F = e_R \circledast H\)</li> <li><code class="language-plaintext highlighter-rouge">correspondence condition</code> : \(x_{R}^{T} F x_L = 0\)</li> <li><code class="language-plaintext highlighter-rouge">epipolar line</code> : \(l_R = F x_L\)</li> <li><code class="language-plaintext highlighter-rouge">epipole</code> : \(e_{R}^{T} F = 0\) (\(e_R\)은 F의 left null vector)</li> </ul> <h3 id="essential-matrix">Essential matrix</h3> <p>essential matrix는 fundamental matrix의 specialization으로, pixel coordinates이 특별히 <code class="language-plaintext highlighter-rouge">calibrated camera들을 다루는 normalized image coordinates (K = I)인 경우</code>에 사용된다. 즉, K = I 여서 \(x^{\ast} = PX = K[R \vert t]X = [R \vert t]X\) 를 만족하는 \(x^{\ast}\)을 normalized coordinates에 있는 image point라 부른다.</p> <p>그리고 epipolar constraint란, vector \(x_L O_L\)과 vector \(x_R O_R\)과 vector \(O_L O_R\)이 같은 평면 epipolar plane 위에 있다는 것이다. 이를 normalized coordinates에서 생각하면, \(x_R^{\ast}\)과 \(Rx_L^{\ast}\)과 t가 <code class="language-plaintext highlighter-rouge">같은 평면 epipolar plane 위에 있다</code>는 뜻이므로 (그 이유는 아래의 Algebraic derivation을 참고하자) 이를 간단하게 수식으로 표현하면 \(x_R^{\ast T}(t \circledast Rx_L^{\ast}) = 0\) 이다.</p> <p>즉, essential matrix와 관련된 식을 정리하면</p> <ul> <li><code class="language-plaintext highlighter-rouge">essential matrix</code> : \(E = t \circledast R\)</li> <li><code class="language-plaintext highlighter-rouge">correspondence condition</code> : \(x_R^{\ast T} E x_L^{\ast} = 0\)</li> </ul> <h3 id="relationship-between-fundamental-matrix-and-essential-matrix">​Relationship between fundamental matrix and essential matrix</h3> <p>이제 intrinsic parameters인 calibration matrix <code class="language-plaintext highlighter-rouge">K를 이용하여 uncalibrated camera들을 다루는 general case로 확장</code>해보자. \(x_L^{\ast}\) 이 normalized coordinates에서의 image point였고, \(x_L\)은 일반적인 pixel coordinates에서의 image point라고 할 때,</p> <ol> <li>\(x_L = K_L x_L^{\ast}\) 이고, \(x_R = K_R x_R^{\ast}\) 이므로 \(x_L^{\ast} = K_L^{-1}x_L\) 이고, \(x_R^{\ast} = K_R^{-1}x_R\)</li> <li> \[x_R^{\ast T} E x_L^{\ast} = 0\] </li> <li>위의 1.과 2.로부터 \(x_R^{T} (K_R^{-T} E K_L{-1}) x_L = 0\)</li> </ol> <p>이 때, 위의 3.은 \(x_{R}^{T} F x_L = 0\) 꼴과 같으므로</p> <p>즉, fundamental matrix와 essential matrix 간의 관계식을 정리하면</p> <ul> <li><code class="language-plaintext highlighter-rouge">fundamental matrix</code> : \(F = e_R \circledast H\)</li> <li><code class="language-plaintext highlighter-rouge">essential matrix</code> : \(E = t \circledast R\)</li> <li><code class="language-plaintext highlighter-rouge">relationshiop</code> : \(F = K_R^{-T} E K_L^{-1} = K_R^{-T} t \circledast R K_L^{-1}\)<br/> (F는 \(K_L, K_R, R, t\) 만으로 표현 가능)</li> </ul> <p>​즉, fundamental matrix F는 각 camera의 calibration matrix \(K_L, K_R\)과 두 camera 사이의 상대적인 rotation R 및 translation t에 의존한다는 것을 알 수 있다.</p> <h3 id="algebraic-derivation">Algebraic derivation</h3> <p>\(F = K_R^{-T} E K_L^{-1} = K_R^{-T} t \circledast R K_L^{-1}\) 임을 조금 더 수학적으로 유도해보자.</p> <p>상대적인 카메라의 위치인 extrinsic parameters의 경우 <code class="language-plaintext highlighter-rouge">left camera에 world origin이 있다고 가정하여 이에 대해 상대적인 right camera의 위치를 R, t로 지정</code>하자.</p> <p>\(ax_L = K_L[I \vert 0]X\)<br/> \(bx_R = K_R[R \vert t]X\)<br/> (여기서 a, b는 단순히 scale factor)</p> <p>\(X = [x, y, z, 1]^T = [X^{\ast}, 1]^T\) 에 대해<br/> \(ax_L = K_{L}X^{\ast}\)<br/> \(bx_R = K_{R}(RX^{\ast}+t)\)</p> <p>\(X^{\ast} = aK_{L}^{-1}x_L\)을 \(bK_{R}^{-1}x_R = RX^{\ast}+t\)에 대입하면,</p> <ol> <li>\(bK_{R}^{-1}x_R = aRK_{L}^{-1}x_L + t\)<br/> 이 때, vector \(bK_{R}^{-1}x_R\)은 vector \(aRK_{L}^{-1}x_L\)와 vector t의 합이므로 기하학적으로 \(bK_{R}^{-1}x_R\)과 \(aRK_{L}^{-1}x_L\)과 t는 <code class="language-plaintext highlighter-rouge">같은 평면 위에 있다. (그리고 그 평면은 epipolar plane이다.)</code><br/> 따라서 vector \(v = t \circledast RK_{L}^{-1}x_L\) 는 epipolar plane에 수직이므로 \(b(K_{R}^{-1}x_R)^{T}v = a(RK_{L}^{-1}x_L)^{T}v + t^{T}v = 0\) 이라 쓸 수 있다.<br/> \(b(K_R^{-1}x_R)^{T}v = 0\)을 정리하면 \(x_R^{T} (K_R^{-T} (t \circledast R) K_L^{-1}) x_L = 0\) 이다.</li> <li>\(bx_R = aK_{R}RK_L^{-1}x_L + K_{R}t\)<br/> 이와 비슷하게 vector \(w = K_{R}t \circledast K_{R}RK_{L}^{-1}x_L\)는 \(bx_R = aK_{R}RK_{L}^{-1}x_L + K_{R}t\) 에 수직이므로 \(bx_{R}^{T}w = a(K_{R}RK_{L}^{-1}x_L)^{T}w + (K_{R}t)^{T}w = 0\) 이라 쓸 수 있다.<br/> \(bx_{R}^{T}w = 0\)을 정리하면 \(x_{R}^{T} (K_{R} t \circledast K_{R}RK_{L}^{-1}) x_L = 0\) 이다.</li> <li>위의 1., 2.에서 유도한 \(x_R^{T} (K_R^{-T} (t \circledast R) K_L^{-1}) x_L = 0\)과 \(x_{R}^{T} (K_{R} t \circledast K_{R}RK_{L}^{-1}) x_L = 0\)을 통해<br/> \(F = K_{R}^{-T} t \circledast R K_{L}^{-1}\) 임을 유도할 수 있다.<br/> (F 유도에 \(x_R^{T} (K_R^{-T} (t \circledast R) K_L^{-1}) x_L = 0\) 은 왜 필요한 거지..? <code class="language-plaintext highlighter-rouge">조금 더 공부 필요</code>)</li> </ol> <h2 id="image-rectification">Image Rectification</h2> <p><code class="language-plaintext highlighter-rouge">Image rectification은 주어진 images를 common image plane에 project하는 것</code>이다. 여러 각도에서 찍은 <code class="language-plaintext highlighter-rouge">이미지들 간에 매칭되는 점들을 쉽게 찾기 위해</code> computer stereo vision 분야에서 널리 사용되는 transform 기법이다. 이 때, 매칭되는 점들을 찾는 것은 위에서 설명한 epipolar geometry에 의해 수행된다.<br/> (<code class="language-plaintext highlighter-rouge">epipolar geometry 요약 : 한 image의 어떤 pixel에 매칭되는 3D 상의 점들은 다른 image의 epipolar line 위에 있다</code>.)</p> <blockquote> <p>parallel stereo cameras : e.g. 두 카메라 사이의 관계가 t = [T; 0; 0] (shifted in x-direction)<br/> 만약 두 image plane이 같은 평면 상에 있다면 optical axes가 parallel하여 <code class="language-plaintext highlighter-rouge">모든 epipolar line은 horizontal axis에 평행하고 epipoles는 infinite point in [1, 0, 0]T direction in homogeneous coordinates로 mapping되며, 매칭되는 점들이 같은 vertical coordinates를 가진다.</code> 그리고 이 때 매칭되는 점들을 찾는 것은 matching cost(minimize SSD or maximize normalized correlation)를 찾기 위해 horizontal scan만 하면 되므로 쉬운 문제이다.<br/> 예를 들어, left image의 어떤 pixel (\(x_l, y_l\))에 대응되는 점을 right image에서 찾는다고 가정하자. 이 때, right image에서의 horizontal scan이 \(x_l\) 의 위치(아래 오른쪽 사진의 빨간 점)을 넘어가서는 안 된다. right image에서 \(x_l\) 의 오른쪽에 matching point가 있다면 연장선을 그었을 때 3D point가 camera center의 뒤쪽에 있다는 것을 의미하기 때문이다.</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/4-480.webp 480w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/4-800.webp 800w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>그런데 horizontal scan을 하며 모든 pixel에 대해 matching cost를 일일이 찾는 건 inefficient하므로 positive matches &amp; negative matches 만들어서 classifier training 할 수 있다.<br/> (smaller patch size : more detail, but noisy)<br/> (bigger patch size : less detail, but smooth)<br/> 이를 통해 disparity map을 얻을 수 있고, depth map을 얻을 수 있다.</p> <p>For <code class="language-plaintext highlighter-rouge">post-processing, MRF(Markov Random Field) or CRF(Conditional Random Field)</code> : energy minimization</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/5-480.webp 480w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/5-800.webp 800w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/6-480.webp 480w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/6-800.webp 800w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>general stereo cameras : 두 카메라 사이의 관계가 [R|t]<br/> 하지만, 두 카메라는 보통 서로 rotate, translate 되어 있기 때문에 <code class="language-plaintext highlighter-rouge">두 image를 warp해서라도 두 image plane이 같은 평면 상에 있던 것처럼 (모든 epipolar line이 수평선이 되도록) 만들 필요가 있고, 이것이 바로 image rectification</code>이다.</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/7-480.webp 480w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/7-800.webp 800w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Image rectification은 두 images에 대해 동시에 수행되며, 일반적으로 셋 이상의 images에 대해서는 simultaneous rectification이 불가능하다.</p> <p>calibrated cameras에 대해서는 essential matrix가 두 camera 사이의 관계를 설명(\(x_{R}^{\ast T} E x_{L}^{\ast} = 0\))하고, uncalibrated cameras (general case)에 대해서는 fundamental matrix가 두 camera 사이의 관계를 설명(\(x_{R}^{T} F x_L = 0\))한다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/11-480.webp 480w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/11-800.webp 800w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>파란색 axis : camera coordinate<br/> 빨간색 axis : rectified camera coordinate<br/> <code class="language-plaintext highlighter-rouge">rectified camera coordinate</code> \(r_{1}, r_{2}, r_{3}\) 구하는 방법 :<br/> \(r_{1} = \frac{t}{\| t \|}\) where \(t\) is vector from camera 1 to camera 2<br/> \(r_{2} = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} \times r_{1}\) where \(\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}\) is z-axis from original camera coordinate<br/> \(r_{3} = r_{1} \times r_{2}\)</p> <p>Image rectification 알고리즘은 대표적으로 세 가지가 있다. : <code class="language-plaintext highlighter-rouge">planar, cylindrical, and polar rectification</code><br/> Image rectification을 수행하기 위해서는 projective transformation을 위해 homography matrix \(H_L, H_R\)를 찾아야 하는데, 여러 방법 중 하나를 아래에서 소개하겠다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/8.JPG-480.webp 480w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/8.JPG-800.webp 800w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/8.JPG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/8.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>우선 left camera에 world origin이 있다고 가정하여 이에 대해 상대적인 right camera의 위치를 R, t로 지정하자. 그러면 \(O_{L} = 0, O_{R} = -R^{t} t\) 라 쓸 수 있고, \(P_{L} = K_{L}[I \vert 0], P_{R} = K_{R}[R \vert t]\) 라 쓸 수 있다.<br/> <code class="language-plaintext highlighter-rouge">(왜 OR = -R^t t 이지?)</code></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/9.JPG-480.webp 480w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/9.JPG-800.webp 800w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/9.JPG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/9.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>첫 번째로, <code class="language-plaintext highlighter-rouge">epipole의 위치를 구한다.</code><br/> \(O_R\)을 left image plane에 project한 게 \(e_L\) 이고, \(O_L\)을 right image plane에 project한 게 \(e_R\) 이므로<br/> \(e_{L} = P_{L} [O_{R} ; 1] = P_{L} [-R^{t} t ; 1] = K_{L}[I \vert 0][-R^{t} t ; 1] = - K_{L} R^{t} t\)<br/> \(e_{R} = P_{R} [O_{L} ; 1] = P_{R} [0 ; 1] = K_{R}[R \vert t][0 ; 1] = K_{R} t\)</li> <li>두 번째로, <code class="language-plaintext highlighter-rouge">left image plane이 baseline에 평행해지도록 rotate시키는 projective transformation HL1 을 구한다. 이는 original optical axis와 desired optical axis 간의 외적</code>으로 구할 수 있다.</li> <li>세 번째로, <code class="language-plaintext highlighter-rouge">horizontal axis가 baseline 및 epipolar line과 평행해지도록 twist시키는 projective transformation</code> \(H_{L2}\) 를 구한다. 맞게 구했다면 twist 후 epipoles가 infinity in x-direction로 mapping되어야 한다.</li> <li>네 번째로, left image를 rectify하는 <code class="language-plaintext highlighter-rouge">최종 projective transformation</code> \(H_{L} = H_{L2}H_{L1}\) 을 구한다.</li> <li>다섯 번째로, 같은 방법으로 right image를 rectify하는 <code class="language-plaintext highlighter-rouge">최종 projective transformation</code> \(H_{R} = H_{R2}H_{R1}\)을 구한다. 여기서 주의할 점은, left image와 right image를 각각 \(H_{L1}\)과 \(H_{R1}\)으로 <code class="language-plaintext highlighter-rouge">rotate한 후에 optical axis가 서로 평행해야 한다</code>.<br/> One strategy is to pick a plane parallel to the line where the two original optical axes intersect to minimize distortion from the reprojection process. 또는 We simply define as \(H_{R} = H_{L} R^{t}\)<br/> (<code class="language-plaintext highlighter-rouge">위의 두 가지 strategy 이해 못 했음. 추가 공부 필요</code>)</li> <li>마지막으로, two images가 same resolution을 갖도록 <code class="language-plaintext highlighter-rouge">scale</code>해준다. 그러면 horizontal epipoles가 align되어 매칭되는 점들이 같은 <code class="language-plaintext highlighter-rouge">vertical coordinates를 가지므로 매칭되는 점들을 찾기 위해 horizontal scan만 하면 되는 쉬운 문제로 바뀐다</code>.<br/> 추가로, 꼭 \(K_{L}, K_{R}\) <code class="language-plaintext highlighter-rouge">intrinsic parameter를 모르더라도 a set of seven or more image-to-image correspondences만 알면 fundamental matrix와 epipoles를 계산할 수 있어서 image rectification을 수행할 수 있다고 한다.</code><br/> (<code class="language-plaintext highlighter-rouge">a set of seven or more correspondences로 fundamental matrix, epipole 구해서 rectification 하는 거 이해 못 했음. 추가 공부 필요</code>)<br/> (fundamental matrix와 epipole 알면 image rectification 수행 가능)</li> </ul> <blockquote> <p>참고 사이트 :<br/> <a href="https://blog.naver.com/hms4913/220043661788">https://blog.naver.com/hms4913/220043661788</a><br/> <a href="https://en.wikipedia.org/wiki/Image_rectification#cite_note-HARTLEY2003-9">https://en.wikipedia.org/wiki/Image_rectification#cite_note-HARTLEY2003-9</a><br/> <a href="https://csm-kr.tistory.com/64">https://csm-kr.tistory.com/64</a><br/> CSC420: Intro to Image Understanding 수업 내용</p> </blockquote> <p>중간중간에 있는 질문들은 아직 이해하지 못해서 남겨놓은 코멘트입니다.<br/> 추후에 다시 읽어보고 이해했다면 업데이트할 예정입니다.<br/> 혹시 알고 계신 분이 있으면 댓글로 남겨주시면 감사하겠습니다!</p>]]></content><author><name></name></author><category term="depth-estimation"/><category term="epipolar"/><category term="fundamental"/><category term="essential"/><category term="image"/><category term="rectification"/><summary type="html"><![CDATA[Epipolar Geometry & Image Rectification]]></summary></entry></feed>