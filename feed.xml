<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://semyeong-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://semyeong-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-15T10:01:51+00:00</updated><id>https://semyeong-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">SegmentAnything</title><link href="https://semyeong-yu.github.io/blog/2024/SegmentAnything/" rel="alternate" type="text/html" title="SegmentAnything"/><published>2024-05-29T14:00:00+00:00</published><updated>2024-05-29T14:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/SegmentAnything</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/SegmentAnything/"><![CDATA[<h3 id="segmentanything">SegmentAnything</h3> <h4 id="alexander-kirillov-eric-mintun-nikhila-ravi-hanzi-mao-chloe-rolland-laura-gustafson-tete-xiao-spencer-whitehead-alexander-c-berg-wan-yen-lo-piotr-dollár-ross-girshick">Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2304.02643">https://arxiv.org/abs/2304.02643</a></p> </blockquote> <hr/> <h2 id="abstract">Abstract</h2> <ul> <li> <p>Task<br/> Promptable Image Segmentation</p> </li> <li> <p>Model Architecture<br/> image encoder + prompt encoder + mask decoder</p> </li> <li> <p>Generate Data (Data Engine)<br/> assisted-manual stage \(\rightarrow\) semi-automatic stage \(\rightarrow\) fully-automatic stage<br/> data ‘SA-1B’ : 1B masks with 11M images</p> </li> <li> <p>Enable Zero-Shot Generalization<br/> Zero-Shot transfer to various tasks</p> </li> <li> <p>Code Review</p> </li> </ul> <h2 id="overview">Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/2-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/2-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>prompt : mask를 생성할 대상을 지정<br/> point, BB, mask(rough area), text(preliminary) 중 하나</p> </li> <li> <p>valid masks : segmented mask를 하나가 아닌 3개 (whole, part, sub-part) 생성<br/> ambiguous prompt에 대응하기 위해, zero shot을 위해<br/> 3개의 masks 중 GT와 가장 유사한(confidence score가 가장 높은) mask의 loss만 사용</p> </li> </ul> <h2 id="model">Model</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/3-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/3-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Image Encoder : MAE (Masked AutoEncoder) 방식의 ViT<br/> MAE 요약 : 이미지를 grid로 나누고 patches 중 일부를 가린 뒤 원본을 복원하도록 학습하고, 학습이 끝난 후에는 encoder embedding만 사용<br/> ViT-H/16 : 14 \(\times\) 14 windowed attention and 4 global attention blocks</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/4-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/4-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Prompt Encoder :<br/> Mask (dense prompt) : conv. 거친 후 image embedding에 pixel-wise sum (mask가 없는 pixel의 경우 ‘no mask’ prompt 사용)<br/> Point (sparse prompt) : positional encoding + learned embedding(fg or bg)<br/> BB (sparse prompt) : positional encoding + learned embedding(top-left or bottom-right)<br/> text (sparse prompt) : by CLIP text encoder</p> </li> <li> <p>Loss :</p> <ol> <li>Mask loss : related to mask prediction<br/> 1-1. focal loss : \(L(p_{t}) = - (1-p_{t})^{r}log(p_{t})\) where \((1-p_{t})^{r}\) gives more weight to few hard examples (\(p_{t} \sim 0\))<br/> 1-2. dice loss : 1 - dice score where dice score = \(\frac{2 \times Area(A \cap B)}{Area(A) + Area(B)}\)</li> </ol> </li> </ul> <ol> <li>IoU loss : related to confidence score<br/> MSE loss</li> </ol> <h2 id="data--develop-data-engine-by-curriculum-learning">Data : Develop Data Engine by Curriculum Learning</h2> <ul> <li> <p>Assisted-manual stage :<br/> public segmentation dataset \(\rightarrow\) SAM \(\rightarrow\) pixel-wise manual augmentation \(\rightarrow\) re-train<br/> After re-training, the number of masks per image increased from 20 to 44 in average<br/> Collect 4.3M masks from 0.12M images</p> </li> <li> <p>Semi-automatic stage :<br/> dataset from previous stage (4.3M masks) \(\rightarrow\) SAM \(\rightarrow\) mask predict 실패한(제외된) object를 annotate \(\rightarrow\) re-train<br/> After re-training, the number of masks per image increased from 44 to 72 in average<br/> Collect 5.9M masks from 0.18M images (totally 4.3M + 5.9M = 10.2M masks)</p> </li> <li> <p>Fully-automatic stage :<br/> dataset from previous stage (10.2M masks) : image에 32 \(\times\) 32 grid points 찍음 \(\rightarrow\) SAM<br/> ambiguity-aware training (whole, part, sub-part 구분 가능)<br/> After filtering masks with high confidence score,<br/> Collect SA-1B dataset : 1.1B masks from 11M images (various HR masks)<br/> 99.1% is fully-automatically generated<br/> follow RAI (Responsible AI) : no bias and blur human faces</p> </li> </ul> <h2 id="task">Task</h2> <p>generalizable (zero-shot transfer to various tasks)</p> <ul> <li>Zero-Shot Transfer Tasks : <ol> <li>Zero-Shot Single Point Valid Mask Evaluation</li> <li>Zero-Shot Edge Detection</li> <li>Zero-Shot Object Proposals</li> <li>Zero-Shot Instance Segmentation</li> <li>Zero-Shot Text-to-Mask (CLIP)</li> </ol> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/4-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/4-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Zero-Shot Single Point Valid Mask Evaluation :<br/> point 찍었을 때 그에 해당하는 mask를 얼마나 잘 생성하는가<br/> use one most-confident mask<br/> compare with RITM model on 23 datasets</p> </li> <li> <p>Zero-Shot Edge Detection :</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/6-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/6-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>About filter : 블로그 맨 아랫 부분에 설명해놓음</p> <ul> <li> <p>Zero-Shot Object Proposals :<br/> mask 예측 후 object의 identity(class)를 얼마나 잘 맞추는가</p> </li> <li> <p>Zero-Shot Instance Segmentation :</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/7-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/7-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Zero-Shot Text-to-Mask :<br/> image \(\rightarrow\) CLIP \(\rightarrow\) image embedding as input<br/> text \(\rightarrow\) CLIP \(\rightarrow\) text embedding as SAM prompt</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/8-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/8-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>SAM’s latent space에서 similar mask embedding vectors within threshold를 추출한 결과 실제로도 semantically similar</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/9-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/9-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A query is indicated by magenta box : top row shows matches at a low threshold and bottom row shows matches at a high threshold </div> <h2 id="code-review">Code Review</h2> <p>다음에 해야지… 라고 미뤄둠..ㅎㅎ</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/10-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/10-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/11-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/11-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/11.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/12-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/12-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/13-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/13-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/13.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/14-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/14-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/14.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/15-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/15-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/15.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/16-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/16-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/16-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/16.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/17-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/17-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/17-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/17.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/18-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/18-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/18-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/18.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/19-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/19-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/19-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/19.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/20-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/20-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/20-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/20.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/21-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/21-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/21-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/21.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/22-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/22-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/22-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/22.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/23-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/23-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/23-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/23.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/24-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/24-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/24-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/24.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/25-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/25-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/25-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/25.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/26-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/26-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/26-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/26.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/27-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/27-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/27-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/27.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/28-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/28-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/28-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/28.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/29-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/29-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/29-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/29.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/30-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/30-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/30-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/30.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/31-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/31-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/31-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/31.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/32-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/32-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/32-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/32.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/33-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/33-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/33-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/33.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/34-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/34-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/34-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/34.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/35-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/35-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/35-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/35.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/36-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/36-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/36-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/36.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/37-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/37-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/37-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/37.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/38-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/38-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/38-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/38.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/39-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/39-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/39-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/39.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/40-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/40-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/40-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/40.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-29-SegmentAnything/41-480.webp 480w,/assets/img/2024-05-29-SegmentAnything/41-800.webp 800w,/assets/img/2024-05-29-SegmentAnything/41-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-29-SegmentAnything/41.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container>]]></content><author><name></name></author><category term="cv-tasks"/><category term="image"/><category term="segmentation"/><summary type="html"><![CDATA[Promptable Image Segmentation]]></summary></entry><entry><title type="html">Reconstruction and Synthesis 3D humans in 3D scenes</title><link href="https://semyeong-yu.github.io/blog/2024/Reconstruction_Synthesis_humans/" rel="alternate" type="text/html" title="Reconstruction and Synthesis 3D humans in 3D scenes"/><published>2024-05-13T14:00:00+00:00</published><updated>2024-05-13T14:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Reconstruction_Synthesis_humans</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Reconstruction_Synthesis_humans/"><![CDATA[<h3 id="reconstruction-and-synthesis-3d-humans-in-3d-scenes">Reconstruction and Synthesis 3D humans in 3D scenes</h3> <h4 id="introduction-to-siyu-tang-eth-zurich">Introduction to Siyu Tang (ETH Zurich)</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/14-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/14-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="lecture-summary">Lecture Summary</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/3-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/3-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>real human : How to reconstruct natural human motions in 3D scenes with a monocular camera?</li> <li>digital human</li> <li>virtual human</li> </ol> <h4 id="real-human">real human</h4> <p>Key : learn motion priors from high quality mocap datasets</p> <p>LEMO: Learning Motion Priors for 4D Human Body Capture in 3D Scenes. Zhang. Zhang. Bogo. Pollefeys. Tang. ICCV 2021 (Oral)<br/> It enforces smoothness in latent space</p> <ol> <li>physics-based prior<br/> data-based prior</li> <li>diffusion-based approach : robust for auto-encoder, but optimization may be not that fast<br/> reinforcement-learning-based approach : policy update can be fast</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/1-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/1-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/2-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/2-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="digital-human">digital human</h4> <p>In Ego-centric motion capture,<br/> <code class="language-plaintext highlighter-rouge">Collision score guided sampling</code></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/4-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/4-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="virtual-human">virtual human</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/5-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/5-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Key : Generate 0.25-second(8 frames) Motion Primitives for perpetual motion prediction</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/6-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/6-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/8-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/8-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/7-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/7-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <h4 id="both-real-and-virtual-human">Both real and virtual human</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/9-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/9-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Use both synthetic data and real data for 3D segmentation in Point Clouds</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/10-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/10-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/11-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/11-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <h4 id="egocentric-synthetic-data-generator">Egocentric Synthetic Data Generator</h4> <p>egocentric task : challenging</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/12-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/12-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="future-work">Future Work</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/13-480.webp 480w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/13-800.webp 800w,/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-13-Reconstruction_Synthesis_3D_humans/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="reconstruction"/><category term="synthesis"/><category term="3D"/><category term="human"/><summary type="html"><![CDATA[SoC Colloquium lecture by Siyu Tang (ETH Zurich)]]></summary></entry><entry><title type="html">FMANet</title><link href="https://semyeong-yu.github.io/blog/2024/FMANet/" rel="alternate" type="text/html" title="FMANet"/><published>2024-05-02T14:00:00+00:00</published><updated>2024-05-02T14:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/FMANet</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/FMANet/"><![CDATA[<h3 id="fma-net--flow-guided-dynamic-filtering-and-iterative-feature-refinement-with-multi-attention-for-joint-video-super-resolution-and-deblurring">FMA-Net : Flow-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring</h3> <h4 id="geunhyuk-youk-jihyong-oh-munchurl-kim">Geunhyuk Youk, Jihyong Oh, Munchurl Kim</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2401.03707">https://arxiv.org/abs/2401.03707</a><br/> project website :<br/> <a href="https://kaist-viclab.github.io/fmanet-site/">https://kaist-viclab.github.io/fmanet-site/</a><br/> pytorch code :<br/> <a href="https://github.com/KAIST-VICLab/FMA-Net">https://github.com/KAIST-VICLab/FMA-Net</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>DD</li> <li>DD</li> <li>DD</li> <li>DD</li> <li>DD</li> </ol> </blockquote> <hr/> <img-comparison-slider> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/9-480.webp 480w,/assets/img/2024-05-02-FMANet/9-800.webp 800w,/assets/img/2024-05-02-FMANet/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/10-480.webp 480w,/assets/img/2024-05-02-FMANet/10-800.webp 800w,/assets/img/2024-05-02-FMANet/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> <img-comparison-slider> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/11-480.webp 480w,/assets/img/2024-05-02-FMANet/11-800.webp 800w,/assets/img/2024-05-02-FMANet/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/12-480.webp 480w,/assets/img/2024-05-02-FMANet/12-800.webp 800w,/assets/img/2024-05-02-FMANet/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> <h2 id="abstract">Abstract</h2> <p><strong>Task : Joint learning of VSRDB (<code class="language-plaintext highlighter-rouge">video super-resolution and deblurring</code>)</strong></p> <ul> <li>restore HR video from blurry LR video<br/> challenging because should handle two types of degradation (SR and deblurring) simultaneously</li> <li>super-resolution : LR vs HR</li> <li>deblurring : blurry vs sharp</li> </ul> <p><strong>FGDF (<code class="language-plaintext highlighter-rouge">flow-guided dynamic filtering</code>)</strong></p> <ul> <li>precise estimation of both spatio-temporally-variant <code class="language-plaintext highlighter-rouge">degradation</code> and <code class="language-plaintext highlighter-rouge">restoration</code> kernels that are aware of motion trajectories (not stick to fixed positions)</li> <li>effectively handle large motions with small-sized kernels (naive dynamic filtering의 한계 극복)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/2-480.webp 480w,/assets/img/2024-05-02-FMANet/2-800.webp 800w,/assets/img/2024-05-02-FMANet/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>DCN (Deformable Conv.) : learn position-invariant \(n \times n\) filter coeff.<br/> vs<br/> DF (Dynamic filtering) : learn position-wise \(n \times n\) dynamic filter coeff.</p> <p>DF (Dynamic Filtering) : fixed surroundings<br/> vs<br/> FGDF (Flow Guided DF) : variable surroundings by learned optical flow</p> <p><strong>FRMA (<code class="language-plaintext highlighter-rouge">iterative feature refinement with multi-attention</code>)</strong></p> <p>refine features by iterative updates<br/> loss : TA (temporal anchor)<br/> multi-attention :</p> <ul> <li>center-oriented attention (focus on target frame)</li> <li>degradation-aware attention (use degradation kernels in globally adaptive manner)</li> </ul> <hr/> <h2 id="related-work">Related Work</h2> <p><strong>VSR (Video Super-Resolution)</strong></p> <p>Based on the number of input frames,</p> <ol> <li>sliding window-based method : recover HR frames by using neighboring frames within a sliding window<br/> use CNN, optical flow estimation, deformable conv., or transformer focusing on temporal alignment<br/> vs</li> <li>recurrent-based method : sequentially propagate the latent features of one frame to the next frame<br/> Chan et al. <d-cite key="vsr">[1]</d-cite> BasicVSR++ : combine bidirectional propagation of past and future frames into current frame features<br/> limit : gradient vanishing</li> </ol> <p><strong>DB (Video Deblurring)</strong></p> <p>Zhang et al. <d-cite key="adversarial">[2]</d-cite> 3D CNN<br/> Li et al. <d-cite key="groupshift">[3]</d-cite> grouped spatial-temporal shifts<br/> transformer-based : Restormer <d-cite key="restormer">[4]</d-cite>, Stripformer <d-cite key="stripformer">[5]</d-cite>, RVRT <d-cite key="rvrt">[6]</d-cite></p> <p><strong>Joint learning of VSRDB (not sequential cascade of VSR and DB)</strong></p> <p>Previous works are mostly designed for ISRDB</p> <p>Fang et al. <d-cite key="HOFFR">[7]</d-cite> HOFFR :<br/> the first deep-learning-based VSRDB<br/> limit : struggle to deblur spatially-variant motion blur because 2D CNN has spatially-equivariant and input-independent filters</p> <p><strong>Dynamic Filter Network</strong></p> <p>predict spatially-variant degradation or restoration kernels</p> <p>Zhou et al. <d-cite key="adaptivefilter">[8]</d-cite> :<br/> spatially adaptive deblurring filter for recurrent video deblurring<br/> Kim et al. <d-cite key="koalanet">[9]</d-cite> KOALAnet :<br/> blind SR predicts spatially-variant degradation and upsampling filters</p> <p>limit : apply dynamic filtering only to the reference frame (target position and its fixed surrounding neighbors), so cannot accurately exploit spatio-temporally-variant-motion info. from adjacent frames<br/> limit : if apply dynamic filtering to adjacent frames \(\rightarrow\) large-sized filters are required to capture large motions \(\rightarrow\) high computational complexity<br/> limit : <d-cite key="separableconv">[10]</d-cite> suggested two separable large 1D kernels to approximate a large 2D kernel \(\rightarrow\) does not capture fine detail, so inappropriate for video</p> <hr/> <h2 id="method">Method</h2> <p><strong>Overview</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/1-480.webp 480w,/assets/img/2024-05-02-FMANet/1-800.webp 800w,/assets/img/2024-05-02-FMANet/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>FMA-Net : VSRDB framework based on FGDF and FRMA<br/> allow for small-to-large motion representation learning</p> <ul> <li>input : <code class="language-plaintext highlighter-rouge">blurry LR sequence</code> \(X = \left\lbrace X_{c-N}:X_{c+N} \right\rbrace \in R^{T \times H \times W \times 3}\) where \(T=2N+1\) and \(c\) is a center frame index</li> <li>goal : predict <code class="language-plaintext highlighter-rouge">sharp HR center frame</code> \(\hat Y_{c} \in R^{sH \times sW \times 3}\) where \(s\) is SR scale factor</li> </ul> <ol> <li>degradation learning network \(Net^{D}\) : learn <code class="language-plaintext highlighter-rouge">motion-aware</code> <code class="language-plaintext highlighter-rouge">spatio-temporally-variant</code> degradation kernels</li> <li>restoration network \(Net^{R}\) : utilize these degradation kernels in a <code class="language-plaintext highlighter-rouge">globally adaptive</code> manner to restore center frame \(X_c\)</li> <li>\(Net^{D}\) and \(Net^{R}\) consist of FRMA blocks and FGDF module</li> </ol> <p><strong>FRMA block</strong></p> <p>pre-trained optical flow network : unstable for blurry frames and computationally expensive</p> <p>vs</p> <blockquote> <p>FRMA block :<br/> learn <code class="language-plaintext highlighter-rouge">self-induced</code> optical flow in a residual learning manner<br/> learn <code class="language-plaintext highlighter-rouge">multiple</code> optical flows with corresponding occlusion masks<br/> \(\rightarrow\) flow diversity enables to learn one-to-many relations b.w. pixels in a target frame and its neighbor frames<br/> \(\rightarrow\) beneficial since blurry frame’s pixel info. is spread due to light accumulation</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/3-480.webp 480w,/assets/img/2024-05-02-FMANet/3-800.webp 800w,/assets/img/2024-05-02-FMANet/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Three features</p> <ol> <li>\(F \in R^{T \times H \times W \times C}\) :<br/> temporally-anchored (unwarped) feature at each frame index (\(0 \sim T-1\))</li> <li>\(F_w \in R^{H \times W \times C}\) :<br/> warped feature</li> <li>\(\boldsymbol f = \left \lbrace f_{c \rightarrow c+t}^{j}, o_{c \rightarrow c+t}^{j} \right \rbrace _{j=1:n}^{t=-N:N} \in R^{T \times H \times W \times (2+1)n}\) :<br/> multi-flow-mask pairs<br/> \(f_{c \rightarrow c+t}^{j}\) : learnable optical flow<br/> \(o_{c \rightarrow c+t}^{j}\) : learnable occlusion mask (sigmoid for stability)<br/> \(n\) is the number of multi-flow-mask pairs from the center frame index \(c\) to each frame index</li> </ol> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/16-480.webp 480w,/assets/img/2024-05-02-FMANet/16-800.webp 800w,/assets/img/2024-05-02-FMANet/16-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>(i+1)-th Feature Refinement : 위첨자로 표기<br/> <code class="language-plaintext highlighter-rouge">왜 아래의 식으로 update 하는지 이해 필요</code></p> <ol> <li>\(F^{i+1}\)=RDB(\(F^{i}\)) :<br/> RDB <d-cite key="RDB">[11]</d-cite></li> <li>\(\boldsymbol f^{i+1}\) = \(\boldsymbol f^{i}\) + Conv3d(concat(\(\boldsymbol f^{i}\), \(W\)(\(F^{i+1}\), \(\boldsymbol f^{i}\)), \(F_{c}^{0}\)))<br/> \(W\)(\(F^{i+1}\), \(\boldsymbol f^{i}\)) : warp \(F^{i+1}\) to center frame index \(c\) based on \(f^{i}\)<br/> \(W\) : occlusion-aware backward warping<br/> concat : along channel dim.<br/> \(F_{c}^{0} \in R^{H \times W \times C}\) : feature map at center frame index \(c\) of the initial feature \(F^{0} \in R^{T \times H \times W \times C}\)</li> <li>\(\tilde F_{w}^{i}\) = Conv2d(concat(\(F_{w}^{i}\), \(r_{4 \rightarrow 3}\)(\(W\)(\(F^{i+1}\), \(\boldsymbol f^{i+1}\)))))<br/> \(r_{4 \rightarrow 3}\) : reshape from \(R^{T \times H \times W \times C}\) to \(R^{H \times W \times TC}\) for feature aggregation</li> <li>\(F_w^{i+1}\) = Multi-Attn(\(\tilde F_{w}^{i}\), \(F_{c}^{0}\)(, \(k^{D, i}\)))</li> </ol> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/15-480.webp 480w,/assets/img/2024-05-02-FMANet/15-800.webp 800w,/assets/img/2024-05-02-FMANet/15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>RDB Network <d-cite key="RDB">[11]</d-cite> :<br/> ddd</p> </blockquote> <blockquote> <p>RRDB Network <d-cite key="rrdb">[15]</d-cite> :<br/> ddd</p> </blockquote> <blockquote> <p>Occlusion-Aware Backward Warping <d-cite key="warp">[12]</d-cite> <d-cite key="warpp">[13]</d-cite> <d-cite key="warppp">[14]</d-cite> :<br/> ddd</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/3-480.webp 480w,/assets/img/2024-05-02-FMANet/3-800.webp 800w,/assets/img/2024-05-02-FMANet/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Multi-Attention :</p> <ul> <li>CO(center-oriented) attention :<br/> better align \(\tilde F_{w}^{i}\) to \(F_{c}^{0}\) (center feature map of initial temporally-anchored feature)</li> <li>DA(degradation-aware) attention :<br/> \(\tilde F_{w}^{i}\) becomes globally adaptive to spatio-temporally variant degradation by using degradation kernels \(K^{D}\)</li> </ul> </blockquote> <ul> <li> <p>CO attention :<br/> \(Q=W_{q} F_{c}^{0}\)<br/> \(K=W_{k} \tilde F_{w}^{i}\)<br/> \(V=W_{v} \tilde F_{w}^{i}\)<br/> \(COAttn(Q, K, V) = softmax(\frac{QK^{T}}{\sqrt{d}})V\)<br/> 실험 결과, \(\tilde F_{w}^{i}\)가 자기 자신(self-attention)이 아니라 \(F_{c}^{0}\)과의 relation에 집중할 때 better performance</p> </li> <li> <p>DA attention :<br/> CO attention과 비슷하지만,<br/> Query 만들 때 \(F_{c}^{0}\) 대신 \(k^{D, i}\) 사용<br/> \(\tilde F_{w}^{i}\) becomes globally adaptive to spatio-temporally-variant degradation<br/> \(k^{D, i} \in R^{H \times W \times C}\) : degradation features adjusted by conv. with \(K^{D}\) (motion-aware spatio-temporally-variant degradation kernels) 에 대해<br/> \(Q=W_{q} k^{D, i}\)<br/> DA attention은 \(Net^{D}\) 말고 \(Net^{R}\) 에서만 사용</p> </li> </ul> <p><strong>FGDF</strong></p> <ul> <li> <p>spatio-temporal Dynamic Filter :<br/> \(y(p) = \sum_{t=-N}^{N} \sum_{k=1}^{n^2} F_{c+t}^{p}(p_k) x_{c+t}(p+p_k)\)<br/> where<br/> \(x \in R^{T \times H \times W}\) <code class="language-plaintext highlighter-rouge">?????</code><br/> \(y \in R^{H_{out} \times W_{out} \times C_{out}}\) where \(C_{out}\) is the number of filters<br/> \(c\) : center frame index<br/> \(p_k \in \{ (- \lfloor \frac{n}{2} \rfloor, - \lfloor \frac{n}{2} \rfloor), \cdots , (\lfloor \frac{n}{2} \rfloor, \lfloor \frac{n}{2} \rfloor) \}\) : sampling offset for conv. with \(n \times n\) kernel<br/> \(F \in R^{T \times H \times W \times n^{2}}\) : predicted \(n \times n\) dynamic filter<br/> \(F^p \in R^{T \times n^{2}}\) : predicted \(n \times n\) dynamic filter at position p</p> </li> <li> <p>limit :<br/> fixed position (\(p\)) and fixed surrounding neighbors (\(p_k\))<br/> \(\rightarrow\) To capture large motion, require large-sized filter</p> </li> </ul> <blockquote> <p>solution : FGDF<br/> kernels - dynamically generated / pixel-wise (position-wise) / variable surroundings guided by optical flow<br/> \(\rightarrow\) can handle large motion with relatively small-sized filter<br/> \(y(p) = \sum_{t=-N}^{N} \sum_{k=1}^{n^2} F_{c+t}^{p}(p_k) x_{c+t}^{\ast}(p+p_k)\)<br/> where <br/> \(x_{c+t}^{\ast} = W(x_{c+t}, \boldsymbol f_{c+t})\) : warped input feature based on \(\boldsymbol f_{c+t}\)<br/> \(\boldsymbol f_{c+t}\) : flow-mask pair from frame index \(c\) to \(c+t\)</p> </blockquote> <p><strong>Overall Architecture</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/1-480.webp 480w,/assets/img/2024-05-02-FMANet/1-800.webp 800w,/assets/img/2024-05-02-FMANet/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Degradation Network \(Net^{D}\)<br/> input : blurry LR sequence \(\boldsymbol X\) and sharp HR sequence \(\boldsymbol Y\)<br/> goal : <code class="language-plaintext highlighter-rouge">predict flow and degradation kernels in sharp HR sequence</code> \(\boldsymbol Y\)</p> <ol> <li>an image flow-mask pair \(\boldsymbol f^{Y}\)</li> <li>motion-aware spatio-temporally-variant degradation kernels \(K^{D}\)<br/> \(\rightarrow\) obtain blurry LR center frame \(\boldsymbol X_{c}\) from sharp HR counterpart \(\boldsymbol Y\)</li> </ol> </blockquote> <ul> <li> <p>step 1-1. initialize<br/> RRDB : <d-cite key="rrdb">[15]</d-cite><br/> \(\boldsymbol X \rightarrow\) 3D RRDB \(\rightarrow F^{0}\)</p> </li> <li> <p>step 1-2. initialize<br/> \(F_{w}^{0} = 0\), \(\boldsymbol f = \left \lbrace f_{c \rightarrow c+t}^{j} = 0, o_{c \rightarrow c+t}^{j} = 1 \right \rbrace _{j=1:n}^{t=-N:N}\)</p> </li> <li> <p>step 2. M FRMA blocks<br/> \(F^{0}, F_{w}^{0}, \boldsymbol f^{0} \rightarrow\) \(M\) FRMA blocks \(\rightarrow F^{M}, F_{w}^{M}, \boldsymbol f^{M}\)</p> </li> <li> <p>step 3-1. <code class="language-plaintext highlighter-rouge">an</code> image flow-mask pair \(\boldsymbol f^{Y} \in R^{T \times H \times W \times (2+1) 1}\)<br/> \(\boldsymbol f^{M} \rightarrow\) Conv3d \(\rightarrow \boldsymbol f^{Y}\)</p> </li> <li> <p>step 3-2. \(\hat X_{sharp}^{D}\) only used in Temporal Anchor (TA) loss<br/> \(F^{M} \rightarrow\) Conv3d \(\rightarrow \hat X_{sharp}^{D} \in R^{T \times H \times W \times 3}\) in image domain</p> </li> <li> <p>step 3-3. motion-aware spatio-temporally-variant degradation kernels \(K^{D} \in R^{T \times H \times W \times k_{d}^{2}}\)<br/> \(K^{D}\) = softmax(Conv3d(\(r_{3 \rightarrow 4}\)(\(F_{w}^{M}\))))<br/> where<br/> \(k_{d}\) : degradation kernel size<br/> sigmoid for normalization : all kernels have <code class="language-plaintext highlighter-rouge">positive</code> values, which mimics <code class="language-plaintext highlighter-rouge">blur generation process</code></p> </li> <li> <p>step 4. FGDF downsampling to predict blurry center frame \(\hat X_{c}\)<br/> \(\hat X_{c}\) = \(W(\boldsymbol Y, s (\boldsymbol f^{Y} \uparrow _{s}))\) \(\circledast K^{D} \downarrow _{s}\)<br/> where<br/> \(\uparrow\) : \(\times s\) bilinear upsampling<br/> \(W(\boldsymbol Y, s (\boldsymbol f^{Y} \uparrow _{s}))\) : warped sharp HR sequence based on an upsampled image flow-mask pair<br/> \(\circledast K^{D} \downarrow _{s}\) : FGDF with filter \(K^{D}\) with stride \(s\)</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/17-480.webp 480w,/assets/img/2024-05-02-FMANet/17-800.webp 800w,/assets/img/2024-05-02-FMANet/17-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/17.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/1-480.webp 480w,/assets/img/2024-05-02-FMANet/1-800.webp 800w,/assets/img/2024-05-02-FMANet/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Restoration Network \(Net^{R}\)<br/> input : blurry LR sequence \(\boldsymbol X\) and \(F^{M}, \boldsymbol f^{M}, K^{D}\) from \(Net^{D}\)<br/> goal : <code class="language-plaintext highlighter-rouge">predict flow and restoration kernels in blurry LR sequence</code> \(\boldsymbol X\)</p> <ol> <li>an image flow-mask pair \(\boldsymbol f^{X}\)</li> <li>restoration kernels \(K^{R}\)<br/> \(\rightarrow\) obtain sharp HR center frame \(\hat Y_{c}\) from blurry LR counterpart \(\boldsymbol X\)</li> </ol> </blockquote> <ul> <li> <p>step 1-1. initialize \(F^{0}\)<br/> RRDB : <d-cite key="rrdb">[15]</d-cite><br/> concat(\(\boldsymbol X\), \(F^{M}\) from \(Net^{D}\)) \(\rightarrow\) 3D RRDB \(\rightarrow\) \(F^{0}\)</p> </li> <li> <p>step 1-2. initialize \(F_{w}^{0}\), \(\boldsymbol f^{0}\)<br/> \(F_{w}^{0} = 0\), \(\boldsymbol f^{0} = \boldsymbol f^{M}\) from \(Net^{D}\)</p> </li> <li> <p>step 2-1. compute \(k^{D, i} \in R^{H \times W \times C}\) for DA attention</p> </li> <li> <p>step 2-2. M FRMA blocks<br/> \(F^{0}, F_{w}^{0}, \boldsymbol f^{0}, k^{D, i} \rightarrow\) \(M\) FRMA blocks \(\rightarrow F^{M}, F_{w}^{M}, \boldsymbol f^{M}\)</p> </li> <li> <p>step 3-1. <code class="language-plaintext highlighter-rouge">an</code> image flow-mask pair \(\boldsymbol f^{X} \in R^{T \times H \times W \times (2+1) 1}\)<br/> \(\boldsymbol f^{M} \rightarrow\) Conv3d \(\rightarrow \boldsymbol f^{X}\)</p> </li> <li> <p>step 3-2. \(\hat X_{sharp}^{R}\) only used in Temporal Anchor (TA) loss<br/> \(F^{M} \rightarrow\) Conv3d \(\rightarrow \hat X_{sharp}^{R} \in R^{T \times H \times W \times 3}\) in image domain</p> </li> <li> <p>step 3-3. motion-aware spatio-temporally-variant \(\times s\) upsampling and restoration kernels \(K^{R} \in R^{T \times H \times W \times s^{2} k_{r}^{2}}\)<br/> \(K^{R}\) = Normalize(Conv3d(\(r_{3 \rightarrow 4}\)(\(F_{w}^{M}\))))<br/> where<br/> \(k_{r}\) : restoration kernel size<br/> Normalize : w.r.t all kernels at temporally co-located positions over \(X\) <code class="language-plaintext highlighter-rouge">뭔 말인지 코드 한 번 보자</code></p> </li> <li> <p>step 3-4. high-frequency detail \(\hat Y_{r}\)<br/> \(F_{w}^{M} \rightarrow\) stacked conv. and pixel shuffle \(\rightarrow \hat Y_{r}\)</p> </li> <li> <p>step 4. FGDF upsampling to predict sharp center frame \(\hat Y_{c}\)<br/> \(\hat Y_{c}\) = \(\hat Y_{r}\) + \(W(\boldsymbol X, \boldsymbol f^{X})\) \(\circledast K^{D} \uparrow _{s}\)<br/> where<br/> \(W(\boldsymbol X, \boldsymbol f^{X})\) : warped blurry LR sequence based on an image flow-mask pair<br/> \(\circledast K^{D} \uparrow _{s}\) : \(\times s\) dynamic upsampling with kernel \(K^{R}\)</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/17-480.webp 480w,/assets/img/2024-05-02-FMANet/17-800.webp 800w,/assets/img/2024-05-02-FMANet/17-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/17.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Training</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/4-480.webp 480w,/assets/img/2024-05-02-FMANet/4-800.webp 800w,/assets/img/2024-05-02-FMANet/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Stage 1. Pre-train \(Net^{D}\)</p> <ul> <li>loss 1. <code class="language-plaintext highlighter-rouge">reconstruction loss</code> for blurry LR \(X_{c}\)<br/> \(\hat X_{c}\) \(\leftrightarrow\) \(X_{c}\)</li> <li>loss 2. <code class="language-plaintext highlighter-rouge">optical flow loss</code> (warping from c to c+t) in \(\boldsymbol Y\)<br/> \(W(Y_{t+c}, s (\boldsymbol f_{t+c}^{Y} \uparrow _{s}))\) \(\leftrightarrow\) \(Y_{c}\)</li> <li>loss 3. <code class="language-plaintext highlighter-rouge">optical flow loss</code> in \(\boldsymbol Y\)<br/> \(f^{Y}\) \(\leftrightarrow\) \(f_{RAFT}^{Y}\)<br/> where<br/> \(f^{Y}\) is image optical flow (no occlusion mask) contained in \(\boldsymbol f^{Y}\)<br/> \(f_{RAFT}^{Y}\) is pseudo-GT optical flow by pre-trained RAFT model <d-cite key="Raft">[16]</d-cite></li> <li>loss 4. <code class="language-plaintext highlighter-rouge">Temporal Anchor (TA) loss</code> for sharp LR \(X_{sharp}\)<br/> It anchors and sharpens each feature w.r.t corresponding frame index<br/> \(\hat X_{sharp}^{D}\) \(\leftrightarrow\) \(X_{sharp}\)<br/> where<br/> sharp HR sequence \(\boldsymbol Y \rightarrow\) bicubic downsampling \(\rightarrow\) GT sharp LR sequence \(X_{sharp}^{D}\)<br/> \(\rightarrow\) keep each feature temporally anchored for the corresponding frame index<br/> \(\rightarrow\) constrain the solution space to distinguish warped and unwarped features<br/> <code class="language-plaintext highlighter-rouge">위의 말 이해 안 감...!!ㅠ</code></li> </ul> <blockquote> <p>RAFT: Recurrent all-pairs field transforms for optical flow <d-cite key="Raft">[16]</d-cite> :<br/> 핵심 아이디어 : ddd</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/5-480.webp 480w,/assets/img/2024-05-02-FMANet/5-800.webp 800w,/assets/img/2024-05-02-FMANet/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Stage 2. Jointly train \(Net^{D}\) and \(Net^{R}\)</p> <ul> <li>loss 1. <code class="language-plaintext highlighter-rouge">restoration loss</code> for sharp HR \(Y_{c}\)<br/> \(\hat Y_{c}\) \(\leftrightarrow\) \(Y_{c}\)</li> <li>loss 2. <code class="language-plaintext highlighter-rouge">optical flow loss</code> (warping from c to c+t) in \(\boldsymbol X\)<br/> Stage 1.의 loss 2.와 동일한 원리</li> <li>loss 3. <code class="language-plaintext highlighter-rouge">Temporal Anchor (TA) loss</code> for sharp LR \(X_{sharp}\)<br/> Stage 1.의 loss 4.와 동일한 원리</li> <li>loss 4. \(L_{D}\)<br/> Stage 1.의 loss들</li> </ul> <hr/> <h2 id="results">Results</h2> <p><strong>Settings</strong></p> <p>LR patch size : 64 \(\times\) 64<br/> the number of FRMA blocks : \(M\) = 4<br/> the number of multi-flow-mask pairs : \(n\) = 9<br/> degradation and restoration kernel size : \(k_{d}\), \(k_{r}\) = 20, 5<br/> the number of frames in sequence : \(T\) = 3 (\(N\) = 1)<br/> ratio b.w. HR and LR : \(s\) = 4<br/> multi-attention block : utilize multi-Dconv head transposed attention (MDTA) and Gated-Dconv feed-forward network (GDFN) from Restormer <d-cite key="restormer">[4]</d-cite></p> <blockquote> <p>multi-Dconv head transposed attention and Gated-Dconv feed-forward network <d-cite key="restormer">[4]</d-cite> :<br/> ddd</p> </blockquote> <p><strong>Datasets and Evaluation Metrics</strong></p> <ul> <li> <p>Datasets :<br/> REDS dataset : train and test<br/> GoPro and YouTube dataset : test (generalization)<br/> \(\rightarrow\) spatially bicubic downsampling to make LR sequence and temporally downsampling to make lower fps sequence</p> </li> <li> <p>Evaluation Metrics :<br/> PSNR and SSIM for image quality<br/> tOF for temporal consistency</p> </li> </ul> <p><strong>Comparision with SOTA</strong></p> <blockquote> <p>SOTA methods (SR) :<br/> single-image SR : SwinIR <d-cite key="swinir">[17]</d-cite> and HAT <d-cite key="hat">[18]</d-cite><br/> video SR : BasicVSR++ <d-cite key="vsr">[1]</d-cite> and FTVSR <d-cite key="ftvsr">[19]</d-cite></p> </blockquote> <blockquote> <p>SOTA methods (DB) :<br/> single-image deblurring : Restormer <d-cite key="restormer">[4]</d-cite> and FFTformer <d-cite key="fftformer">[20]</d-cite><br/> video deblurring : RVRT <d-cite key="rvrt">[6]</d-cite> and GShiftNet <d-cite key="gshiftnet">[21]</d-cite></p> </blockquote> <blockquote> <p>SOTA methods (VSRDB) :<br/> HOFFR <d-cite key="HOFFR">[7]</d-cite></p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/18-480.webp 480w,/assets/img/2024-05-02-FMANet/18-800.webp 800w,/assets/img/2024-05-02-FMANet/18-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/18.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>VSRDB methods have superior performance compared to sequential cascade of SR and DB<br/> \(\rightarrow\) SR and DB tasks are highly inter-correlated</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/7-480.webp 480w,/assets/img/2024-05-02-FMANet/7-800.webp 800w,/assets/img/2024-05-02-FMANet/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/8-480.webp 480w,/assets/img/2024-05-02-FMANet/8-800.webp 800w,/assets/img/2024-05-02-FMANet/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <p><strong>Ablation Study</strong></p> <ul> <li>FGDF<br/> FGDF is better than conventional dynamic filtering for all ranges of motion magnitudes<br/> conventional dynamic filtering is especially not good for large motion</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/13-480.webp 480w,/assets/img/2024-05-02-FMANet/13-800.webp 800w,/assets/img/2024-05-02-FMANet/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> PSNR/tOF according to the average optical flow magnitude b.w. two consecutive frames </div> <ul> <li>Design of FMA-Net <ol> <li>the number of multi-flow-mask pairs \(n\) \(\propto\) performance</li> <li>motion info. from multi-flow-mask pairs \(\boldsymbol f\) is better than motion info. from DCN (Deformable Conv.) due to self-induced sharper optical flows and occlusion masks</li> <li>RAFT loss and TA loss</li> <li>two-stage (\(Net^{D} \rightarrow\) both) training is better than end-to-end training</li> <li>multi-attention (CO + DA) is better than self-attention + SFT(spatial feature transform) <d-cite key="SFT">[22]</d-cite></li> </ol> </li> </ul> <blockquote> <p>SFT (spatial feature transform) <d-cite key="SFT">[22]</d-cite><br/> ddd</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/14-480.webp 480w,/assets/img/2024-05-02-FMANet/14-800.webp 800w,/assets/img/2024-05-02-FMANet/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="conclusion">Conclusion</h2> <p>VSRDB framework based on FGDF and FRMA</p> <ul> <li>FRMA :<br/> iteratively update features (e.g. self-induced optical flow)<br/> multi-attention (CO + DA attention)</li> <li>FGDF :<br/> predict flow-mask pair with flow-guided dynamic filters \(K^{D}\) and \(K^{R}\) that are aware of motion<br/> can handle large motion</li> <li>TA loss :<br/> temporally anchors and sharpens unwarped features</li> <li>2-stage training :<br/> because, during multi-attention of \(Net^{R}\), warped feature \(F_{w}\) is adjusted by predicted degradation \(K^{D}\) from \(Net^{D}\) in globally adaptive manner</li> </ul> <h2 id="limitation">Limitation</h2> <ul> <li>2-stage approach has longer training time than end-to-end approach</li> <li>In extreme contidions such as object rotation, it is hard to predict accurate optical flow<br/> \(\rightarrow\) learnable homography parameters or quaternion representations can be one option to handle rotational motions</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-02-FMANet/19-480.webp 480w,/assets/img/2024-05-02-FMANet/19-800.webp 800w,/assets/img/2024-05-02-FMANet/19-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-05-02-FMANet/19.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="super-resolution"/><category term="super-resolution"/><category term="deblur"/><category term="flow"/><category term="dynamic"/><category term="attention"/><summary type="html"><![CDATA[Flow-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring]]></summary></entry><entry><title type="html">NeRF</title><link href="https://semyeong-yu.github.io/blog/2024/NeRF/" rel="alternate" type="text/html" title="NeRF"/><published>2024-04-10T21:00:00+00:00</published><updated>2024-04-10T21:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/NeRF</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/NeRF/"><![CDATA[<h1 id="nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h1> <h4 id="ben-mildenhall-pratul-psrinivasan-matthew-tancik">Ben Mildenhall, Pratul P.Srinivasan, Matthew Tancik</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2003.08934">https://arxiv.org/abs/2003.08934</a><br/> project website :<br/> <a href="https://www.matthewtancik.com/nerf">https://www.matthewtancik.com/nerf</a><br/> pytorch code :<br/> <a href="https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file">https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file</a><br/> tiny tensorflow code :<br/> <a href="https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb">https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb</a><br/> referenced blog :<br/> <a href="https://csm-kr.tistory.com/64">https://csm-kr.tistory.com/64</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>여러 각도에서 찍은 input images에서 ray(r=o+td)를 쏜다.</li> <li>ray를 discrete points로 sampling한다.</li> <li>3D coordinate x와 viewing direction d를 r(x)와 r(d)로 positional encoding한다.</li> <li>r(x)를 MLP에 넣어 volume density를 얻고 여기에 r(d)까지 넣어 RGB color를 얻는다.</li> <li>coarse network와 fine network(hierarchical sampling) 각각에서 volume density와 color를 이용한 volume rendering으로 ray마다 rendering pixel color를 구한다.</li> </ol> </blockquote> <h2 id="introduction">Introduction</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/3-480.webp 480w,/assets/img/2024-04-10-NeRF/3-800.webp 800w,/assets/img/2024-04-10-NeRF/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="pipeline">Pipeline</h4> <ul> <li> <p>(a) march camera rays and generate sampling of 5D coordinates</p> </li> <li> <p>(b) represents volumetric static scene by optimizing continuous 5D function(fully-connected network)</p> </li> </ul> <ol> <li>input: single continuous <code class="language-plaintext highlighter-rouge">5D coordinate</code><br/> 3D location \(x, y, z\)<br/> 2D direction \(\theta, \phi\)</li> <li>output:<br/> <code class="language-plaintext highlighter-rouge">volume density</code> (differential opacity) (how much radiance is accumulated by a ray)<br/> <code class="language-plaintext highlighter-rouge">view-dependent RGB color</code> (emitted radiance) \(c = (r, g, b)\)</li> </ol> <ul> <li> <p>(c) synthesizes novel view by classic <code class="language-plaintext highlighter-rouge">volume rendering</code> techniques(differentiable) to accumulate(project)(composite) the color/density samples into 2D image along rays</p> </li> <li> <p>(d) loss between synthesized and GT observed images</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/2-480.webp 480w,/assets/img/2024-04-10-NeRF/2-800.webp 800w,/assets/img/2024-04-10-NeRF/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Pipeline of NeRF architecture </div> <h4 id="problem--solution">Problem &amp; Solution</h4> <p>Problem :</p> <ol> <li>not sufficiently high-resolution representation</li> <li>inefficient in the number of samples per camera ray</li> </ol> <p>Solution :</p> <ol> <li>input <code class="language-plaintext highlighter-rouge">positional encoding</code> for MLP to represent higher frequency function</li> <li><code class="language-plaintext highlighter-rouge">hierarchical sampling</code> to reduce the number of queries</li> </ol> <h4 id="contribution">Contribution</h4> <ul> <li>represent continuous scenes as 5D neural radiance fields with basic MLP to render high-resolution novel views</li> <li>differentiable volume rendering + hierarchical sampling</li> <li>positional encoding to map input 5D coordinate into higher dim. space for high-frequency scene representation</li> <li>overcome the storage costs of discretized voxel grids by encoding continuous volume into network’s parameters<br/> =&gt; require only storage costs of sampled volumetric representations</li> </ul> <h2 id="related-work">Related Work</h2> <h4 id="neural-3d-shape-representation">Neural 3D shape representation</h4> <ul> <li> <p>deep networks that map \(xyz\) coordinates to signed distance functions or occupancy fields<br/> =&gt; limit : need GT 3D geometry</p> </li> <li> <p>Niemeyer et al.<br/> =&gt; input : find directly surface intersection for each ray (can calculate exact derivatie)<br/> =&gt; output : diffuse color at each ray intersection location</p> </li> <li> <p>Sitzmann et al.<br/> =&gt; input : each 3D coordinate<br/> =&gt; output : feature vector and RGB color at each 3D coordinate<br/> =&gt; rendering by RNN that marches along each ray to decide where the surface is.</p> </li> </ul> <blockquote> <p>Limit : oversmoothed renderings, so limited to simple shapes with low geometric complexity</p> </blockquote> <h4 id="view-synthesis-and-image-based-rendering">View synthesis and image-based rendering</h4> <ul> <li> <p>Given dense sampling of views, novel view synthesis is possible by simple light field sample interpolation</p> </li> <li> <p>Given sparser sampling of views, there are 2 ways :<br/> mesh-based representation and volumetric representation</p> </li> <li> <p>Mesh-based representation with either diffuse(난반사) or view-dependent appearance :<br/> Directly optimize mesh representations by differentiable rasterizers or pathtracers so that we reproject and reconstruct images</p> </li> </ul> <blockquote> <p>Limit :<br/> gradient-based optimization is often difficult because of local minima or poor loss landscape<br/> needs a template mesh with fixed topology for initialization, which is unavailable in real-world</p> </blockquote> <ul> <li>Volumetric representation :<br/> well-suited for gradient-based optimization and less distracting artifacts<br/> train : predict a sampled volumetric representation (voxel grids) from input images<br/> test : use alpha-(or learned-)compositing along rays to render novel views<br/> \(\rightarrow\) alpha-compositing : 여러 frame을 합쳐서 하나의 image로 합성하는 과정으로, 각 이미지 픽셀마다 알파 값(불투명도 값)(0~1)이 있어 겹치는 부분의 알파 값 및 픽셀 값을 결정<br/> CNN compensates discretization artifacts from low resolution voxel grids or CNN allows voxel grids to vary on input time</li> </ul> <blockquote> <p>Limit :<br/> good results, but limited by poor time, space complexity due to discrete sampling<br/> \(\rightarrow\) discrete sampling : rendering high resol. image =&gt; finer sampling of 3D space</p> </blockquote> <blockquote> <p>Author’s solution :<br/> encode <code class="language-plaintext highlighter-rouge">continuous</code> volume into network’s parameters<br/> =&gt; higher quality rendering + require only storage cost of those <code class="language-plaintext highlighter-rouge">sampled</code> volumetric representations</p> </blockquote> <h2 id="neural-radiance-field-scene-representation">Neural Radiance Field Scene Representation</h2> <p>represent continuous scene by 5D MLP : (x, d) =&gt; (c, \(\sigma\))</p> <p>Here, there are 2 key-points!</p> <blockquote> <p>multiview consistent :<br/> c is dependent on both x and d, but \(\sigma\) is only dependent on location x<br/> 3D coordinate x =&gt; 8 fc-layers =&gt; volume-density and 256-dim. feature vector</p> </blockquote> <blockquote> <p>Lambertian reflection : diffuse(난반사) vs Specular reflection : 전반사</p> </blockquote> <blockquote> <p>non-Lambertian effects : view-dependent color change to represent specular reflection<br/> feature vector is concatenated with direction d =&gt; 1 fc-layer =&gt; view-dependent RGB color</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/6-480.webp 480w,/assets/img/2024-04-10-NeRF/6-800.webp 800w,/assets/img/2024-04-10-NeRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="volume-rendering-with-radiance-fields">Volume Rendering with Radiance Fields</h2> <h4 id="ray-from-input-image-pre-processing">Ray from input image (pre-processing)</h4> <p>We use <code class="language-plaintext highlighter-rouge">Ray</code> to synthesize <code class="language-plaintext highlighter-rouge">continuous</code>-viewpoint images from <code class="language-plaintext highlighter-rouge">discrete</code> input images</p> <blockquote> <p>\(r(t) = o + td\)<br/> o : the location of camera<br/> d : viewing direction</p> </blockquote> <blockquote> <p>How to calculate viewing direction d??</p> <ul> <li>pixel coordinate : \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)</li> <li>normalized coordinate by intrinsic matrix :<br/> \(\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\) = \(K^{-1}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\) = \(\begin{bmatrix} 1/f_x &amp; 0 &amp; W/2 \\ 0 &amp; 1/f_y &amp; H/2 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)<br/> Since y, z have opposite direction between the real-world coordinate and pixel coordinate, we multiply (-1)<br/> \(\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\) = \(\begin{bmatrix} 1/f_x &amp; 0 &amp; W/2 \\ 0 &amp; -1/f_y &amp; H/2 \\ 0 &amp; 0 &amp; -1 \end{bmatrix}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)<br/> Here, focal length in intrinsic matrix K is usually calculated using camear angle \(\alpha\) as \(\tan{\alpha / 2} = \frac{h/2}{f}\)</li> <li>3D coordinate by extrinsic matrix :<br/> For extrinsic matrix \([R \vert t']\),<br/> \(o = t'\)<br/> \(d = R * \begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\)<br/> Therefore, we can obtain \(r(t) = o + td\)</li> </ul> </blockquote> <h4 id="volume-rendering-from-mlp-output">Volume Rendering from MLP output</h4> <p>We use differential classical volume rendering</p> <blockquote> <p>idea : 빛이 들어올 확률이 클수록(채도?), 물체의 밀도가 높을수록(명도?) 2D 이미지 픽셀에서 색상 채널의 값이 크게 나타나므로<br/> pixel’s color along a ray is equal to the integral of (transmittance) x (volume density) x (color) for all points along one ray.</p> </blockquote> <p>For ray \(r\) traced through desired virtual camera and near, far bounds \(t_n\), \(t_f\),<br/> expected color of ray \(r\) = \(C(r) = \int_{t_n}^{t_f} T(t) \sigma (r(t)) c(r(t), d) dt\)</p> <ul> <li>\(T(t) = \exp(- \int_{t_n}^{t} \sigma (r(s)) ds)\) :<br/> accumulated transmittance along ray from \(t_n\) to \(t\)<br/> the probability that ray travels from \(t_n\) to \(t\) without hitting any particle<br/> 투과도가 클수록 투명<br/> T(t) is the solution of \(\frac{dy}{dt} = - \sigma (x)y\)<br/> since T(t)’s rate of decrease is proportional to T(t) itself and volume density</li> <li>\(\sigma (r(t))\) : volume density along the ray (learned by MLP)</li> <li>\(c(r(t), d)\) : object’s color along the ray (learned by MLP)</li> </ul> <p>To apply the equation to our model, we have to do sampling from continuous ray to discrete points<br/> Instead of deterministic quadrature(typically used for rendering voxel grids, but may limit resolution),<br/> author divides a ray \(\left[t_n, t_f\right]\) into N = 64 bins(intervals), and chooses one point \(t_i\) for each bin by uniform sampling<br/> \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N}(t_f - t_n), t_n + \frac{i}{N}(t_f - t_n)\right]\)<br/> Although we use discrete N samples, stratified sampling(층화 표집) enables MLP to be evaluated at continuous positions by optimization</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/5-480.webp 480w,/assets/img/2024-04-10-NeRF/5-800.webp 800w,/assets/img/2024-04-10-NeRF/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>And then discretized version for N samples :<br/> expected color \(\hat{C}(r) = \sum_{i=1}^{N} T_i (1 - \exp(- \sigma_{i} \delta_{i})) c_i\)</p> <ul> <li>\(\hat{C}(r)\) : differentiable about (\(\sigma_{i}, c_i\))</li> <li>\(\delta_{j} = t_{j+1} - t_j\) : the distance between adjacent samples</li> <li>\(\sigma (r(t)) dt ~~ =&gt; ~~ \sigma_{j} \delta_{j} = 1 - \exp(-\sigma_{j} \delta_{j})\) by Taylor Series Expansion</li> <li> \[T(t) = \exp(- \int_{t_n}^{t} \sigma (r(s)) ds) ~~ =&gt; ~~ T_i = \exp(- \sum_{j=1}^{i-1} \sigma_{j} \delta_{j})\] </li> <li> \[c(r(t), d) ~~ =&gt; ~~ c_i\] </li> </ul> <p>Here, in volume rendering, author uses <code class="language-plaintext highlighter-rouge">volume density</code><br/> for 불투명도 == opacity == extinction coefficient == alpha value for alpha-compositing</p> <blockquote> <p>Final version :<br/> expected color \(\hat{C}(r) = \sum_{i=1}^{N} T_i \alpha_{i} c_i\)<br/> where \(T_i = \prod_{j=1}^{i-1} (1-\alpha_{j})\) and \(\alpha_{j} = 1 - \exp(-\sigma_{j} \delta_{j})\)<br/> which reduces to traditional alpha-compositing problem</p> </blockquote> <h2 id="optimizing-a-neural-radiance-field">Optimizing a Neural Radiance Field</h2> <h4 id="positional-encoding-pre-processing">Positional encoding (pre-processing)</h4> <p>If we use input directly, MLP is biased to learn low-frequency function (oversmoothed appearance)<br/> If we map input into higher dim. space, MLP can fit data with high-frequency variation<br/> \(r : R \rightarrow R^{2L}\) <br/> \(r(p) = (sin(2^0\pi p), cos(2^0\pi p), \cdots, sin(2^{L-1}\pi p), cos(2^{L-1}\pi p))\)<br/> \(L=10\) for \(r(x)\) where x has three coordinates<br/> \(L=4\) for \(r(d)\) where d has three components of the cartesian viewing direction unit vector</p> <h4 id="hierarchical-volume-sampling">Hierarchical volume sampling</h4> <p>Densely evaluating N points by stratified sampling is inefficient<br/> =&gt; We don’t need much sampling at free space or occluded regions<br/> =&gt; Hierarchical sampling enables us to allocate more samples to regions we expect to contain visible content</p> <p>We simultaneously optimize 2 networks with different sampling : <code class="language-plaintext highlighter-rouge">coarse</code> and <code class="language-plaintext highlighter-rouge">fine</code></p> <blockquote> <p>coarse sampling \(N_c\)개 : 위에서 배웠던 내용<br/> author divides a ray into \(N_c\) = 64 bins(intervals), and chooses one point \(t_i\) for each bin by uniform sampling<br/> \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N_c}(t_f - t_n), t_n + \frac{i}{N_c}(t_f - t_n)\right]\)</p> </blockquote> <blockquote> <p>fine sampling \(N_f\)개 : 새로운 내용<br/> coarse sampling model’s output is a weighted sum of all coarse-sampled colors<br/> \(\hat{C}(r) = \sum_{i=1}^{N_c} T_i \alpha_{i} c_i = \sum_{i=1}^{N_c} w_i c_i\)<br/> where we define \(w_i = T_i \alpha_{i} = T_i (1 - \exp(-\sigma_{i} \delta_{i}))\) for \(i=1,\cdots,N_c\)<br/> =&gt; Given the output of <code class="language-plaintext highlighter-rouge">coarse</code> network, we try more informed sampling where samples are biased toward the relevant parts of the volume<br/> =&gt; We sample \(N_f\)=128 <code class="language-plaintext highlighter-rouge">fine</code> points following a piecewise-constant PDF of normalized \(\frac{w_i}{\sum_{j=1}^{N_c} w_j}\)<br/> =&gt; Here, we use Inverse CDF Method for sampling fine points</p> </blockquote> <blockquote> <p>Inverse transform sampling = Inverse CDF Method :<br/> =&gt; PDF (probability density function) : \(f_X(x)\)<br/> =&gt; CDF (cumulative distribution function) : \(F_X(x) = P(X \leq x) = \int_{-\infty}^x f_X(x) dx\)<br/> idea : 모든 확률 분포의 CDF는 Uniform distribution을 따른다!<br/> =&gt; 따라서 CDF의 y값을 Uniform sampling하면, 그 y에 대응되는 x에 대한 (특정 PDF를 따르는) sampling을 구현할 수 있다!<br/> =&gt; 즉, CDF의 역함수를 계산할 수 있다면, 기본 난수 생성기인 Uniform sampling을 이용해서 확률 분포 X에 대한 sampling을 할 수 있다!<br/> \(F_X(x)\) ~ \(U\left[0, 1\right]\)<br/> \(X\) ~ \(F^{-1}(U\left[0, 1\right])\)</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/4-480.webp 480w,/assets/img/2024-04-10-NeRF/4-800.webp 800w,/assets/img/2024-04-10-NeRF/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We evaluate <code class="language-plaintext highlighter-rouge">coarse</code> network using \(N_c\)=64 points per ray<br/> We evaluate <code class="language-plaintext highlighter-rouge">fine</code> network using \(N_c+N_f\)=64+128=192 points per ray where we sample 128 points following PDF of <code class="language-plaintext highlighter-rouge">coarse</code> sampled points<br/> In result, we use a total of 64+192=256 samples per ray to compute the final rendering color \(C(r)\)</p> <h4 id="implementation-details--loss">Implementation details &amp; Loss</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/6-480.webp 480w,/assets/img/2024-04-10-NeRF/6-800.webp 800w,/assets/img/2024-04-10-NeRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>Prepare RGB images, corresponding camera poses, intrinsic parameters and scene bounds (use COLMAP structure-from-motion package to estimate these parameters)</li> <li>From H x W input image, randomly sample a batch of 4096 pixels</li> <li>calculate continuous ray from each pixel \(r(t) = o + kd\)</li> <li>coarse sampling of \(N_c\)=64 points per each ray \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N_c}(t_f - t_n), t_n + \frac{i}{N_c}(t_f - t_n)\right]\)</li> <li>positional encoding \(r(x)\) and \(r(d)\) for input</li> <li>obtain volume density \(\sigma\) by MLP with \(r(x, y, z)\) as input</li> <li>obtain color \(c\) by MLP with \(r(x, y, z)\) and \(r(\theta, \phi)\) as input</li> <li>obtain rendering color of each ray by volume rendering \(\hat{C}(r) = \sum_{i=1}^{N} T_i (1 - \exp(- \sigma_{i} \delta_{i})) c_i\) from two networks ‘coarse’ and ‘fine’</li> <li>compute loss</li> <li>Adam optimizer with learning rate from \(5 \times 10^{-4}\) to \(5 \times 10^{-5}\)</li> <li>optimization for a single scene typically takes around 100-300k iterations (1~2 days) to converge on a single NVIDIA V100 GPU</li> </ol> <p>Here, we use L2 norm for loss<br/> \(L = \sum_{r \in R} \left[{\left\|\hat{C_c}(r)-C(r)\right\|}^2+{\left\|\hat{C_f}(r)-C(r)\right\|}^2\right]\)<br/> \(C(r)\) : GT pixel RGB color<br/> \(\hat{C_c}(r)\) : rendering RGB color from coarse network : to allocate better samples in fine network<br/> \(\hat{C_f}(r)\) : rendering RGB color from fine network : our goal</p> <h2 id="results">Results</h2> <h4 id="datasets">Datasets</h4> <p>Synthetic renderings of objects</p> <ul> <li>Diffuse Synthetic 360 : 4 Lambertian objects with simple geometry</li> <li>Realistic Synthetic 360 : 8 non-Lambertian objects with complicated geometry</li> </ul> <p>Real images of complex scenes</p> <ul> <li>Real Forward-Facing : 8 scenes captured with a handheld cellphone</li> </ul> <h4 id="measurement">Measurement</h4> <ul> <li>PSNR(Peak Signal-to-Noise Ratio) \(\uparrow\) : the ratio between the maximum possible power of a signal and the power of corrupting noise \(10\log_{10}\left(\frac{(MAX)^2}{MSE}\right)\)[dB]</li> <li>SSIM(Structural Similarity Index Map) \(\uparrow\) : compare image qualities in three ways: Lumincance(\(l\)), Contrast(\(c\)), Structural(\(s\))<br/> SSIM(x, y) = \([l(x,y)]^{\alpha}[c(x,y)]^{\beta}[s(x,y)]^{\gamma}=\frac{(2\mu_{x}\mu_{y}+C_1)(2\sigma_{xy}+C_2)}{(\mu_{x}^2+\mu_{y}^2+C_1)(\sigma_{x}^2+\sigma_{y}^2+C_2)}\) where \(l(x,y)=\frac{(2\mu_{x}\mu_{y}+C_1)}{\mu_{x}^2+\mu_{y}^2+C_1}\) and \(c(x,y)=\frac{(2\sigma_{x}\sigma_{y}+C_2)}{\sigma_{x}^2+\sigma_{y}^2+C_2}\) and \(s(x,y)=\frac{\sigma_{xy}+C_3}{\sigma_{x}\sigma_{y}+C_3}\)<br/> SSIM calculator :<br/> https://darosh.github.io/image-ssim-js/test/browser_test.html</li> <li>LPIPS \(\downarrow\)</li> </ul> <h4 id="comparisons">Comparisons</h4> <ul> <li>Neural Volumes (NV) :<br/> It synthsizes novel views of objects that lie entirely within a bounded volume in front of a distinct background.<br/> It optimizes 3D conv. network to predict a discretized RGB\(\alpha\) voxel grid and a 3D warp grid.<br/> It renders novel views by marching rays through the warped voxel grid</li> <li>Scene Representation Networks (SRN) :<br/> It represents continuous scene as an opaque surface.<br/> MLP maps each 3D coordinate to a feature vector, and we optimize RNN to predict the next step size along the ray using the feature vector.<br/> The feature vector from the final step is decoded into a color for that point on the surface. Note that SRN is followup to DeepVoxels by the same authors.</li> <li>Local Light Field Fusion (LLFF) :<br/> designed for producing novel views for well-sampled forward facing scenes<br/> trained 3D conv. network directly predicts a discretized frustum-sampled RGB\(\alpha\) grid (multiplane image), and then renders novel views by alpha-compositing and blending</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/7-480.webp 480w,/assets/img/2024-04-10-NeRF/7-800.webp 800w,/assets/img/2024-04-10-NeRF/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Test for scenes from author's new synthetic dataset </div> <p>LLFF exhibits banding and ghosting artifacts<br/> SRN produces blurry and distorted renderings<br/> NV cannot capture the details<br/> NeRF captures fine details in both geometry and appearance</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/8-480.webp 480w,/assets/img/2024-04-10-NeRF/8-800.webp 800w,/assets/img/2024-04-10-NeRF/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-10-NeRF/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Test for read-world scenes </div> <p>LLFF may have repeated edges because of blending between multiple renderings<br/> NeRF also correctly reconstruct partially-occluded regions<br/> SRN does not capture any high-frequency fine detail</p> <h4 id="discussion">Discussion</h4> <h4 id="ablation-studies">Ablation Studies</h4> <h2 id="conclusion">Conclusion</h2> <p>prior : MLP outputs discretized voxel representations<br/> author : MLP outputs volume density and view-dependent emitted radiance</p> <h2 id="future-work">Future Work</h2> <p>efficiency :<br/> Rather than hierarchical sampling, there is still much more progress to be made for efficient optimization and rendering of neural radiance fields</p> <p>interpretability :<br/> voxel grids or meshes admits reasoning about the expected quality, but it is unclear to analyze these issues when we encode scenes into the weights of MLP</p>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="rendering"/><category term="3d"/><summary type="html"><![CDATA[representing scenes as neural radiance fields for view synthesis]]></summary></entry><entry><title type="html">SfMLearner</title><link href="https://semyeong-yu.github.io/blog/2024/SfMLearner/" rel="alternate" type="text/html" title="SfMLearner"/><published>2024-04-06T17:00:00+00:00</published><updated>2024-04-06T17:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/SfMLearner</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/SfMLearner/"><![CDATA[<h1 id="unsupervised-learning-of-depth-and-ego-motion-from-video">Unsupervised Learning of Depth and Ego-Motion from Video</h1> <h4 id="tinghui-zhou-matthew-brown-noah-snavely-david-g-lowe">Tinghui Zhou, Matthew Brown, Noah Snavely, David G. Lowe</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/1704.07813">https://arxiv.org/abs/1704.07813</a><br/> code :<br/> <a href="https://github.com/tinghuiz/SfMLearner">https://github.com/tinghuiz/SfMLearner</a></p> </blockquote> <h2 id="introduction">Introduction</h2> <ul> <li>SfM : Structure from Motion<br/> end-to-end unsupervised learning from monocular video (only one camera lens)</li> <li><code class="language-plaintext highlighter-rouge">single-view</code> depth estimation by per-pixel depth map</li> <li><code class="language-plaintext highlighter-rouge">multi-view</code> camera motion (= <code class="language-plaintext highlighter-rouge">ego-motion</code> = <code class="language-plaintext highlighter-rouge">pose</code>) by <code class="language-plaintext highlighter-rouge">6-DoF transformation matrices</code></li> <li><code class="language-plaintext highlighter-rouge">unsupervised</code> learning : 직접적인 GT data가 아니라 <code class="language-plaintext highlighter-rouge">view synthesis (reconstruction term)를 supervision</code>으로 씀</li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li>simultaneous estimation of structure and motion through deep learning</li> <li>end-to-end learning of transformation matrix without learning geometry explicitly</li> <li>learning of 3D single-view from registered 2D views</li> <li>unsupervised/self-supervised learning from video</li> </ul> <h2 id="method">Method</h2> <h4 id="approach">Approach</h4> <p>Assumption :<br/> Scenes, which we are interested in, are mostly rigid, so changes across different frames are dominated by camera motion</p> <h4 id="view-synthesis-as-supervision">View Synthesis as supervision</h4> <ul> <li>View Synthesis : as supervision of depth and pose (추후 설명 예정)</li> <li>loss function (reconstruction term) :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/1-480.webp 480w,/assets/img/2024-04-06-SfMLearner/1-800.webp 800w,/assets/img/2024-04-06-SfMLearner/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>\(p\) : index of target view’s pixel coordinates<br/> \(s\) : index of source views<br/> \(I_{t}(p)\) : target view<br/> \(\hat I_{s}(p)\) : source view warped to target coordinate frame (= reconstructed target view) using predicted depth \(\hat D_{t}\) and \(4 \times 4\) camera transformation matrix \(\hat T_{t \rightarrow s}\) and source view \(I_{s}\)</p> <ul> <li>pipeline for depth and pose estimation :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/2-480.webp 480w,/assets/img/2024-04-06-SfMLearner/2-800.webp 800w,/assets/img/2024-04-06-SfMLearner/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="differentiable-depth-image-based-rendering">Differentiable depth image-based rendering</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/3-480.webp 480w,/assets/img/2024-04-06-SfMLearner/3-800.webp 800w,/assets/img/2024-04-06-SfMLearner/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li> <p><code class="language-plaintext highlighter-rouge">Depth CNN</code>을 통해 <code class="language-plaintext highlighter-rouge">target view</code> (single view)로부터 <code class="language-plaintext highlighter-rouge">depth prediction</code> \(\hat D_{t}\) 얻기</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Pose CNN</code>을 통해 <code class="language-plaintext highlighter-rouge">target &amp; source view</code> (multi-view)로부터 \(4 \times 4\) <code class="language-plaintext highlighter-rouge">camera transformation matrix</code> \(\hat T_{t \rightarrow s}\) 얻기</p> </li> <li>target view의 pixels를 source view coordinate으로 <code class="language-plaintext highlighter-rouge">project</code>하기<br/> 값이 아니라 <code class="language-plaintext highlighter-rouge">대응되는 위치</code>를 구하기 위해<br/> projection할 때 depth와 pose 이용 <ul> <li>monocular camera이므로 두 카메라 사이의 상대적인 위치를 설명하는 \([R \vert t]\)는 고려 안함</li> <li>\(K^{-1}p_{t}\) : target view coordinate에서 2D 좌표 \(\rightarrow\) 3D 좌표</li> <li>\(\hat D_{t}(p_{t})K^{-1}p_{t}\) : target view의 3D depth map (= 2D depth \(\times\) 3D 좌표)<br/> full 3D volumetric은 아니고, surface만 나타내는 3D target</li> <li>\(\hat T_{t \rightarrow s} \hat D_{t}(p_{t})K^{-1}p_{t}\) : 3D depth map projected from target view to source view</li> <li>\(K \hat T_{t \rightarrow s} \hat D_{t}(p_{t})K^{-1}p_{t}\) : source view coordinate에서 3D 좌표 \(\rightarrow\) 2D 좌표<br/> <code class="language-plaintext highlighter-rouge">target view의 pixel 좌푯값을 source view의 좌푯값으로 project하는 데 중간에 depth map이 왜 필요한 거지???</code></li> </ul> </li> <li>source view coordinate에서 differentiable bilinear <code class="language-plaintext highlighter-rouge">interpolation</code>으로 value 얻은 뒤 <code class="language-plaintext highlighter-rouge">warp to target coordinate</code> (= <code class="language-plaintext highlighter-rouge">reconstructed target view</code>)<br/> source view의 pixel 값들을 이용해서 reconstruct target view</li> </ol> <h4 id="modeling-the-model-limitation">Modeling the model limitation</h4> <p>Assumption :</p> <ol> <li> <p>objects are static except camera (changes are dominated by camera motion)<br/> 물체들이 움직이지 않아야 Depth CNN과 Pose CNN이 같은 coordinate에 대해 project할 수 있다.</p> </li> <li> <p>there is no occlusion/disocclusion between target view and source view<br/> target view와 source views 중 하나라도 물체가 가려져서 안보인다면 projection 정보가 없어 학습에 문제가 된다.</p> </li> <li> <p>surface is Lambertain so that photo-consistency error is meaningful<br/> 어떤 방향에서 보든 표면이 isotropic 똑같은 밝기로 보인다고 가정 \(\rightarrow\) photo-consistency에 차이가 있을 경우 이는 다른 surface를 의미함</p> </li> </ol> <h4 id="overcoming-the-gradient-locality-at-loss-term">Overcoming the gradient locality at loss term</h4> <ol> <li> <p>To improve robustness, train additional network which predicts <code class="language-plaintext highlighter-rouge">explainability soft mask</code> \(\hat E_{s}\) (= <code class="language-plaintext highlighter-rouge">per-pixel weight</code>), and add it to reconstruction loss term.<br/> deep-learning model은 black-box이므로 explainablity는 중요한 요소</p> </li> <li> <p>trivial sol. \(\hat E_{s} = 0\)을 방지하기 위해, add <code class="language-plaintext highlighter-rouge">regularization</code> term that encourages nonzero prediction of \(\hat E_{s}\)</p> </li> <li> <p>직접 pixel intensity difference로 reconstruction loss를 얻으므로, GT depth &amp; pose로 project하여 얻은 \(p_{s}\) 가 low-texture region or far region에 있을 경우 training 방해 (common issue in motion estimation)<br/> \(\rightarrow\) 해결 1. use conv. encoder-decoder with small bottleneck<br/> \(\rightarrow\) 해결 2. add <code class="language-plaintext highlighter-rouge">multi-scale</code> and <code class="language-plaintext highlighter-rouge">smoothness loss</code> term<br/> (less sensitive to architecture choice, so 이 논문은 해결 2. 적용)</p> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/4-480.webp 480w,/assets/img/2024-04-06-SfMLearner/4-800.webp 800w,/assets/img/2024-04-06-SfMLearner/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> s : source view image index / p : target view pixel index </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/5-480.webp 480w,/assets/img/2024-04-06-SfMLearner/5-800.webp 800w,/assets/img/2024-04-06-SfMLearner/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> l : multi-scale / s : source view image index </div> <h4 id="network-architecture">Network Architecture</h4> <ul> <li>Network 1. <code class="language-plaintext highlighter-rouge">Single-view Depth CNN</code><br/> input : target view<br/> output : per-pixel depth map<br/> DispNet encoder-decoder architecture</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/6-480.webp 480w,/assets/img/2024-04-06-SfMLearner/6-800.webp 800w,/assets/img/2024-04-06-SfMLearner/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Network 2. <code class="language-plaintext highlighter-rouge">Multi-view Pose CNN</code> (아래 figure의 파란 부분)<br/> input : target view concatenated with all source views<br/> output : 6-DoF relative poses between target view and each source view<br/> (Pose CNN estimates <code class="language-plaintext highlighter-rouge">6 channels (3 Euler angles + 3D translation vector)</code> for each source view, and then it is converted to \(4 \times 4\) <code class="language-plaintext highlighter-rouge">transformation matrix</code>)</li> </ul> <p><code class="language-plaintext highlighter-rouge">어떻게 transformation matrix로 변환???</code></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/7-480.webp 480w,/assets/img/2024-04-06-SfMLearner/7-800.webp 800w,/assets/img/2024-04-06-SfMLearner/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/7-480.webp 480w,/assets/img/2024-04-06-SfMLearner/7-800.webp 800w,/assets/img/2024-04-06-SfMLearner/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Network 3. <code class="language-plaintext highlighter-rouge">Explainablity soft mask</code> (= <code class="language-plaintext highlighter-rouge">reconstruction weight per pixel</code>) (위의 figure의 빨간 부분)<br/> output : multi-scale explainability masks<br/> (it estimates <code class="language-plaintext highlighter-rouge">2 channels</code> for each source view at each prediction layer)</li> </ul> <p><code class="language-plaintext highlighter-rouge">weight per pixel인데 왜 2 channels are needed for explainability mask???</code></p> <h2 id="experiments">Experiments</h2> <p>Train : BN, Adam optimizer, monocular camera (one camera lens), resize input image<br/> Test : arbitrary input image size</p> <h4 id="single-view-depth-estimation">Single-view depth estimation</h4> <ul> <li>train model on the split (exclude frames from test sequences and exclude static scene’s pixels with mean optical flow magnitude &lt; 1)</li> <li>pre-trained on Cityscapes dataset / fine-tuned on KITTI dataset / test on Make3D dataset</li> <li>may improve if we also use left-right cycle consistency loss</li> <li>ablation study 결과, explainablity mask를 추가하고 fine-tuning하는 게 더 좋은 성능 도출</li> </ul> <h4 id="multi-view-pose-estimation">Multi-view pose estimation</h4> <ul> <li>trained on KITTI odometry(change in position over time by motion sensor) dataset</li> <li>measurement :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/8-480.webp 480w,/assets/img/2024-04-06-SfMLearner/8-800.webp 800w,/assets/img/2024-04-06-SfMLearner/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>ATE : Absolute Trajectory Error<br/> left/right turning magnitude : coordinate diff. in the side-direction between start and ending frame at test<br/> Mean Odom. : mean of car motion for 5-frame snippets from GT odometry dataset<br/> ORB-SLAM(full) : recover odometry using all frames for loop closure and re-localization<br/> ORB-SLAM(short) : Ours에서처럼, use 5-frame snippets as input<br/> \(\rightarrow\) 특히 small left/right turning magnitude (car is mostly driving forward) 상황에서 Ours가 ORB-SLAM(short)보다 성능 더 좋으므로 monocular SLAM system의 local estimation module을 Ours가 대체할 수 있을 것이라 예상​<br/> (<code class="language-plaintext highlighter-rouge">SLAM 논문 아직 안 읽어봄. 읽어보자.</code>)</p> <h4 id="visualizing-explainability-prediction">Visualizing Explainability Prediction</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/9-480.webp 480w,/assets/img/2024-04-06-SfMLearner/9-800.webp 800w,/assets/img/2024-04-06-SfMLearner/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-06-SfMLearner/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> highlighted pixels at explainability mask : predicted to be unexplainable </div> <p>explainability = per-pixel weight (confidence 느낌) for reconstruction</p> <p>row 1 ~ 3 : due to motion (dynamic objects are unexplainable)<br/> row 4 ~ 5 : due to occlusion/visibility (disappeared objects are unexplainable)<br/> row 6 ~ 7 : due to other factors (e.g. depth CNN has low confidence on thin structures)</p> <h2 id="discussion">Discussion</h2> <h4 id="contribution">Contribution</h4> <ul> <li>end-to-end <code class="language-plaintext highlighter-rouge">unsupervised</code> learning from <code class="language-plaintext highlighter-rouge">monocular</code> sequences<br/> (기존에는 gt depth로 depth supervision 또는 calibrated stereo images로 pose supervision이었지만, 본 논문은 <code class="language-plaintext highlighter-rouge">view synthesis (reconstruction)을 supervision으로</code> 써서 unsupervised learning으로도 comparable performance 달성)</li> <li>depth CNN recognizes common structural features of objects, and pose CNN uses image correspondence with estimating camera motion</li> </ul> <h4 id="limitation">Limitation</h4> <ol> <li> <p><code class="language-plaintext highlighter-rouge">dynamic objects (X) / occlusion (X) / must be Lambertain surface / vast open scenes (X) / when objects are close to the front of camera (X) / thin structure (X)</code><br/> \(\rightarrow\) 위의 한계들을 개선하고자 explainablity mask (= per-pixel reconstruction confidence 느낌) 도입했지만, it is implicit consideration</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">assume that camera intrinsic K is given</code>, so not generalized to the random videos with unknown camera types</p> </li> <li> <p>predict simplified 3D depth map of <code class="language-plaintext highlighter-rouge">surface</code> (<code class="language-plaintext highlighter-rouge">not full 3D volumetric representation</code>)</p> </li> </ol> <p>중간중간에 있는 질문들은 아직 이해하지 못해서 남겨놓은 코멘트입니다.<br/> 추후에 다시 읽어보고 이해했다면 업데이트할 예정입니다.<br/> 혹시 알고 계신 분이 있으면 댓글로 남겨주시면 감사하겠습니다!</p>]]></content><author><name></name></author><category term="depth-estimation"/><category term="unsupervised"/><category term="depth"/><category term="ego"/><category term="motion"/><category term="video"/><summary type="html"><![CDATA[Unsupervised Learning of Depth and Ego-Motion from Video]]></summary></entry><entry><title type="html">Monocular Depth Estimation with Left Right Consistency</title><link href="https://semyeong-yu.github.io/blog/2024/Monocular_Depth_Estimation_LR_Consistency/" rel="alternate" type="text/html" title="Monocular Depth Estimation with Left Right Consistency"/><published>2024-04-05T17:00:00+00:00</published><updated>2024-04-05T17:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Monocular_Depth_Estimation_LR_Consistency</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Monocular_Depth_Estimation_LR_Consistency/"><![CDATA[<h1 id="unsupervised-monocular-depth-estimation-with-left-right-consistency">Unsupervised Monocular Depth Estimation with Left-Right Consistency</h1> <h4 id="clement-godard-oisin-mac-aodha-gabriel-j-brostow">Clement Godard, Oisin Mac Aodha, Gabriel J. Brostow</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/1609.03677">https://arxiv.org/abs/1609.03677</a><br/> referenced blog :<br/> <a href="https://blog.naver.com/dncks1107/223104039030">https://blog.naver.com/dncks1107/223104039030</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ul> <li>unsupervised mono (single image as input) depth estimation</li> <li>need for binocular stereo image pairs at training</li> <li>novel loss function : left-right consistency b.w. disparity maps</li> </ul> </blockquote> <h2 id="backgrounds">Backgrounds</h2> <h4 id="stereo-depth-estimation">Stereo Depth Estimation</h4> <p>인간은 물체를 두 개의 눈을 통해 바라보고 그 차이를 이용하여 대상까지의 거리를 예측한다. AI는 이러한 인간의 시각 시스템을 모방하여 stereo depth estimation을 통해 대상까지의 깊이를 추정할 수 있다.<br/> Stereo image란 카메라 두 대를 사용하여 찍은 두 이미지를 의미하고, disparity는 한 쌍의 stereo image 간의 pixel difference를 의미한다.<br/> <code class="language-plaintext highlighter-rouge">stereo depth estimation은 stereo image 한 쌍 (left image, right image)을 network의 input</code>으로 넣어 이미지 간의 disparity를 통해 depth를 추정하는 것이다.<br/> <code class="language-plaintext highlighter-rouge">stereo depth estimation의 경우, epipolar geometry라는 수학적 원리에 의해 depth를 계산하기 때문에 비교적 정확하지만 카메라와 물체 사이의 거리가 멀어질수록 불리해진다.</code></p> <h4 id="monocular-depth-estimation">Monocular Depth Estimation</h4> <p><code class="language-plaintext highlighter-rouge">monocular depth estimation은 위와 달리 하나의 image만을 network의 input</code>으로 넣어 depth를 추정하는 방법이다. 물론 test-phase에서 하나의 image를 input으로 넣겠다는 의미이고, 이 논문의 경우 training loss를 구할 때는 stereo image 쌍을 모두 이용하였다.<br/> <code class="language-plaintext highlighter-rouge">mono depth estimation의 경우, 믿을 만한 근거(epipolar geometry와 같은 수학적 원리)가 없기 때문에 정확도가 떨어지지만 하나의 image만 input으로 넣기 때문에 전처리 과정이 간단하고 메모리도 덜 필요로 하여, 어느 정도의 정확도만 확보된다면 실생활에서 적용 가능 범위가 더 넓다.</code></p> <h4 id="monocular-and-stereo-camera">Monocular and Stereo Camera</h4> <p><code class="language-plaintext highlighter-rouge">monocular camera</code> : 특정 시간 t에 한 개의 camera 렌즈를 사용<br/> <code class="language-plaintext highlighter-rouge">stereo camera</code> : 특정 시간 t에 6~7cm 떨어진 두 개의 camera 렌즈를 사용</p> <h2 id="abstract">Abstract</h2> <p>기존의 supervised depth estimation 방식은 성능은 좋지만, 구하기 어려운 pixel-wise ground-truth depth data를 대량으로 필요로 한다는 단점이 있었다. 그래서 본 논문의 저자는 ground-truth depth 정보가 없는 stereo image 쌍으로부터 pixel-level depth map을 합성하도록 훈련하는 unsupervised depth estimation 방식을 제안한다. 효과적인 표기를 위해 아래의 notation을 사용하자.<br/> \(I^{l}\) : left image<br/> \(I^{r}\) : right image<br/> \(d^{r}\) : disparity map from left to right<br/> \(d^{l}\) : disparity map from right to left</p> <p>그렇다면 stereo image \(I^{l}, I^{r}\)로부터 어떻게 depth를 추정할까? <code class="language-plaintext highlighter-rouge">image rectfiication</code>을 거친 뒤, depth를 직접 예측하는 게 아니라 우선 두 개의 <code class="language-plaintext highlighter-rouge">disparity map (dense correspondence field)</code> \(d^{r}, d^{l}\) 을 생성한다. 여기서 disparity map이란, image의 한 pixel이 다른 image의 어느 pixel에 대응하는지에 대한 정보를 의미한다. 이후 \(I^{l}\)과 \(d^{r}\)을 이용하여 \(I^{r \ast} = I^{l}(d^{r})\)을 reconstruct하고, \(I^{r}\)과 \(d^{l}\)을 이용하여 \(I^{l \ast} = I^{r}(d^{l})\)을 reconstruct 한 뒤, \(I^{r \ast}\)과 \(I^{r}\) 간의 reconstruction loss와 \(I^{l \ast}\)과 \(I^{l}\) 간의 <code class="language-plaintext highlighter-rouge">reconstruction loss</code>를 이용하여 모델을 학습시킨다. 그런데 reconstruction loss만 사용한다면 depth image의 quality가 저하된다고 한다. 따라서 본 논문의 저자는 ​\(d^{r}\)과 (projected \(d^{l}\)) = \(d^{l}(d^{r})\) 간의 차이도 고려하는 <code class="language-plaintext highlighter-rouge">left-right disparity consistency loss</code>라는 논문의 핵심 아이디어를 제안하였다.</p> <h2 id="contribution">Contribution</h2> <ul> <li>end-to-end unsupervised monocular depth estimation</li> <li>new training loss that enforces left-right disparity consistency</li> </ul> <h2 id="related-work">Related Work</h2> <h4 id="supervised-stereo-depth-estimation">supervised stereo depth estimation</h4> <p>DispNet (Mayer et al.) :<br/> directly predict the disparity for each pixel by regression loss<br/> 단점 : need lots of ground-truth disparity data and stereo image pairs, which are hard to obtain in real-world</p> <h4 id="unsupervised-depth-estimation">unsupervised depth estimation</h4> <p>Deep Stereo (Flynne et al.) :<br/> select pixels from nearby images and generate new views by using the relative pose of multiple cameras<br/> 단점 : At test phase, need several nearby posed images, which is not mono depth estimation</p> <p>Deep3D (Xie et al.) :<br/> make a distribution over all the possible disparities for each pixel and generate right view from an input left image by using image reconstruction loss<br/> 단점 : need much memory if there are lots of possible disparities. So, it is not scalable to bigger output resolutions</p> <p>Garg et al. :<br/> 본 논문과 유사하게 unsupervised mono depth estimation with image reconstruction loss<br/> 단점 : not fully differentiable (이를 보완하고자 Taylor approximation을 수행하긴 했지만 이는 more challenging to optimize)</p> <h2 id="method">Method</h2> <h4 id="depth-estimation-as-image-reconstruction">Depth Estimation as Image Reconstruction</h4> <p>핵심 아이디어 : calibrated binocular(stereo) camera로 같은 시간에 찍은 한 쌍의 stereo image가 주어졌을 때, <code class="language-plaintext highlighter-rouge">하나의 image로부터 다른 image를 reconstruct 할 수 있다면 그 장면의 3D 구조를 알 수 있다!</code></p> <p>우선 a stereo image pair에 대해 image rectification을 거친 뒤 만약 <code class="language-plaintext highlighter-rouge">disparity map을 얻었다면 아래의 도식에 의해 depth map으로 변환</code>할 수 있다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/1.JPG-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/1.JPG-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/1.JPG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/1.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>b : baseline distance between two camera centers (상수)<br/> f : camera focal length (상수)<br/> 가로로 뻗은 직선 : rectified image plane<br/> d : predicted disparity<br/> d^ : depth<br/> b : d^ = b - d : d^ - f 이므로 d^ (b - d) = b (d^ - f) 이고, 이를 정리하면 d^d = bf, 즉 <code class="language-plaintext highlighter-rouge">d^ = bf / d</code> 이다.<br/> 만약 disparity \(d = x_{r} - x_{l}\) 과 depth Z = d^를 얻었다면, 아래의 도식으로 X, Y 값도 얻을 수 있어서 3D point 좌표를 알 수 있다.<br/> <code class="language-plaintext highlighter-rouge">(아래의 도식은 뭘 말하는거지?)</code><br/> \(x = \frac{f \cdot X}{Z} + p_x\)</p> <h4 id="depth-estimation-network">Depth Estimation Network</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/2.JPG-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/2.JPG-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/2.JPG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/2.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>위의 figure에서 볼 수 있듯이 Naive 버전은 input left image와 align할 output reconstructed left image가 없다. 한편, No LR 버전은 align할 output reconstructed left image는 존재하지만, <code class="language-plaintext highlighter-rouge">left-right consistency가 보장되지 않기 때문에 'texture-copy' artifacts와 depth discontinuities(boundaries)에서의 errors가 생기는 문제가 있다.</code> 본 논문의 model은 disparity \(d^{r}, d^{l}\) 을 동시에 추론함으로써 이러한 문제들을 모두 해결하였다.<br/> 위의 figure에서 볼 수 있듯이 mono depth estimation이므로 CNN의 <code class="language-plaintext highlighter-rouge">input으로 left image만을 넣어서 disparity dr, dl 을 동시에 추론</code>하였다. 이를 통해 두 disparity 간의 consistency를 어느 정도 강제할 수 있고 결과적으로 더 정확한 depth estimation이 가능해진다. 참고로 <code class="language-plaintext highlighter-rouge">right image는 image reconstruction과 training loss를 구할 때만 사용</code>된다.<br/> disparity를 구한 뒤에는 <code class="language-plaintext highlighter-rouge">bilinear sampler와 backward mapping을 통해 image reconstruction</code>을 수행한다. 이 때, <code class="language-plaintext highlighter-rouge">STN(spatial transformer network)의 bilinear sampler를 이용하기 때문에 위의 일련의 과정은 fully convolutional and fully differentiable</code>하다.</p> <p>backward mapping :<br/> 결과 영상으로 mapping 되는 원본 영상에서의 좌표를 계산하여 해당 밝기값을 가져온다. 이 때, 원본 영상에서의 좌표는 실숫값이므로 bilinear interpolation (output pixel = the weighted sum of four input pixel)을 사용한다. 결과 영상의 각 pixel에 대해 값을 가져오므로 forward mapping에서의 hole 발생은 일어나지 않는다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/3-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/3-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/4-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/4-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>본 논문의 model은 크게 두 부분으로 나뉜다. : encoder (conv1~conv7b) and decoder (upconv7~)<br/> 본 논문의 model은 output으로서 disparity \(d^{r}, d^{l}\)을 동시에 추론하는데, 이를 <code class="language-plaintext highlighter-rouge">four different output scales</code>에 대해 반복한다.</p> <h4 id="train-loss">Train Loss</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/5-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/5-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/6-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/6-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>\(C_s\) : loss at output scale s</p> <blockquote> <p>\(C_{ap}^{l}\) :<br/> appearance matching loss for left image (<code class="language-plaintext highlighter-rouge">image reconstruction term</code>)<br/> How much \(I^{r}(d^{l})\) appears similar to \(I^{l}\)</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/7-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/7-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>이 때, <code class="language-plaintext highlighter-rouge">SSIM (Structural Similarity Index Measure)</code>는 두 images 간의 차이가 작을수록 1 에 가까운 값을 가지며, 정확한 정의는 아래를 참고하자.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/8-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/8-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>\(C_{ds}^{l}\) :<br/> disparity smoothness loss (<code class="language-plaintext highlighter-rouge">smoothness term</code>)<br/> How much \(d^{l}\) is smooth</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/9-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/9-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>real-world에서 depth가 급격하게 변하는 경우 image boundary 혹은 texture change가 있는 부분이므로 image plane에서도 해당 부분의 image gradient가 크게 나타난다. 따라서 image gradient가 큰 부분에서는 disparity (depth) 변화를 허용하지만, image gradient가 작은 부분에서는 disparity (depth)가 부드럽게 변하도록 하는 것이 disparity smoothness loss의 역할이다.</p> <blockquote> <p>\(C_{lr}^{l}\) :<br/> left-right consistency (<code class="language-plaintext highlighter-rouge">left-right disparity consistency term</code>)<br/> How much \(d^{l}\) and \(d^{r}(d^{l})\) are consistent</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/10-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/10-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>image reconstruction 뿐만 아니라 left-right disparity consistency까지 고려함으로써 <code class="language-plaintext highlighter-rouge">depth estimation의 accuracy</code>를 향상시킬 수 있다.</p> <h2 id="results--limitations">Results &amp; Limitations</h2> <h4 id="results">Results</h4> <p>Train : on Cityscapes and KITTI 2015 dataset using two different test splits<br/> Test : on other datasets like Make3D and CamVid</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/11-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/11-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/12-480.webp 480w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/12-800.webp 800w,/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-05-Monocular_Depth_Estimation_LR_Consistency/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><code class="language-plaintext highlighter-rouge">Post-processing</code> :<br/> original left image로부터 구한 disparity map을 \(d^{l}\) 라 하고,<br/> flipped left image로부터 구한 disparity map을 \(d^{l \ast}\) 라 하고,<br/> 이를 다시 flip한 걸 \(d^{l \ast \ast}\) 라 할 때,<br/> \(d^{l}\)의 경우 stereo disocclusions which create disparity ramps(경사) on both the left side of the image and the left of occluders 가 있을 수 있는데,<br/> \(d^{l \ast \ast}\)의 경우 disparity ramps are located on the right side of the image and the right of occluders 이므로<br/> <code class="language-plaintext highlighter-rouge">We combine both disparity maps to form the final disparity map</code> by assigning the first 5% on the left of the image using \(d^{l \ast \ast}\) and the last 5% on the right to the disparities from \(d^{l}\). The central part of the final disparity map is the average of \(d^{l \ast \ast}\) and \(d^{l}\).<br/> 이러한 post-processing을 통해 can reduce the effect of stereo disocclusions, and lead to better accuracy and less visual artifacts,<br/> but double the amount of test time<br/> (<code class="language-plaintext highlighter-rouge">stereo disocclusions의 영향을 줄이기 위한 post-processing 과정 아직 완벽하게 이해하지는 못했음</code>)</p> <h4 id="limitations">Limitations</h4> <ol> <li> <p>left-right consistency와 post-processing으로 quality 향상을 이룬 건 맞지만, <code class="language-plaintext highlighter-rouge">두 images에서 모두 안 보이는 occlusion region에서의 pixels 때문에 occlusion boundaries에서는 여전히 artifacts가 존재</code>한다. training phase에서 occclusion에 대해 explicitly reasoning하는 것으로 이 문제를 개선할 수는 있지만, supervised methods 또한 모든 pixels에 대해 항상 valid depth를 가지는 것은 아님에 주목할 필요가 있다.</p> </li> <li> <p>training phase에서 <code class="language-plaintext highlighter-rouge">rectified and temporally aligned (image rectification을 거치고 동시에 찍은) stereo image pairs가 필요</code>하다. 이 말은 즉슨, single-view dataset은 training에 쓸 수 없다. (fine-tune하는 것만 가능하다.)</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">image reconstruction term에 의존</code>한다. 이 말은 즉슨, <code class="language-plaintext highlighter-rouge">specular and transparent (거울 같이 반사하는 and 투명한) surfaces에서는 inconsistent depth</code>가 생긴다. 이는 더 정교한 similarity measures를 사용함으로써 개선될 수 있다.</p> </li> </ol> <h2 id="conclusion">Conclusion</h2> <ul> <li>unsupervised mono (single image as input) depth estimation \(\rightarrow\) no need for expensive GT depth</li> <li>need for binocular stereo image pairs at training</li> <li>novel loss function : left-right consistency b.w. disparity maps \(\rightarrow\) improve quality of depth map</li> <li>can generalize to unseen datasets</li> </ul> <h2 id="future-work">Future Work</h2> <ul> <li>extend to videos (can add temporal consistency)</li> <li>investigate sparse input as an alternative training signal<br/> (<code class="language-plaintext highlighter-rouge">?? 이해 못함</code>)</li> <li>our model estimates per-pixel depth, but it would be also interesting to predict the full occupancy of the scene<br/> (<code class="language-plaintext highlighter-rouge">?? 이해 못함</code>)</li> </ul> <p>중간중간에 있는 질문들은 아직 이해하지 못해서 남겨놓은 코멘트입니다.<br/> 추후에 다시 읽어보고 이해했다면 업데이트할 예정입니다.<br/> 혹시 알고 계신 분이 있으면 댓글로 남겨주시면 감사하겠습니다!</p>]]></content><author><name></name></author><category term="depth-estimation"/><category term="unsupervised"/><category term="monocular"/><category term="depth"/><category term="consistency"/><summary type="html"><![CDATA[Unsupervised Monocular Depth Estimation with Left-Right Consistency]]></summary></entry><entry><title type="html">Epipolar Geometry and Image Rectification</title><link href="https://semyeong-yu.github.io/blog/2024/Epipolar_Geometry_Image_Rectification/" rel="alternate" type="text/html" title="Epipolar Geometry and Image Rectification"/><published>2024-04-01T17:00:00+00:00</published><updated>2024-04-01T17:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Epipolar_Geometry_Image_Rectification</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Epipolar_Geometry_Image_Rectification/"><![CDATA[<h2 id="epipolar-geometry">Epipolar Geometry</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/1.JPG-480.webp 480w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/1.JPG-800.webp 800w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/1.JPG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/1.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/2-480.webp 480w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/2-800.webp 800w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="image-plane--epipolar-plane--baseline--epipole--epipolar-line">image plane / epipolar plane / baseline / epipole / epipolar line</h3> <p>X : 3D point<br/> \(x_L, x_R\) : projected 2D point in left and right image<br/> 파란색 면 : image plane<br/> 초록색 면 : <code class="language-plaintext highlighter-rouge">epipolar plane</code><br/> \(O_L, O_R\) : center of left and right camera<br/> 직선 \(O_L O_R\) : <code class="language-plaintext highlighter-rouge">baseline</code><br/> epipolar pencil : set of epipolar planes<br/> <code class="language-plaintext highlighter-rouge">epipole</code> : intersection of baseline and image plane<br/> \(e_L, e_R\) : epipole of left and right camera<br/> <code class="language-plaintext highlighter-rouge">epipolar line</code> :</p> <ul> <li>intersection of image plane and epipolar plane</li> <li>빨간 선 \(l_R\) : 직선 \(x_R e_R\) (projected 2D point와 epipole은 epipolar line 위에 있다)</li> <li>left image plane 위의 같은 점 \(x_L\)로 project 되는 모든 3D points \(X, X_1, X_2, \cdots\)를 right image plane에 project했을 때 그려지는 선</li> </ul> <h4 id="normalized-coordinates-pixel-coordinates--intrinsic-extrinsic-parameters--homography-matrix--projection-matrix">normalized coordinates, pixel coordinates / intrinsic, extrinsic parameters / homography matrix / projection matrix</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/3.JPG-480.webp 480w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/3.JPG-800.webp 800w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/3.JPG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/3.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>real-world의 3D 좌표 (X, Y, Z) 에 있는 물체를 카메라에 투영하기 위해 Z (= 깊이) 값을 1로 정규화한 평면을 <code class="language-plaintext highlighter-rouge">normalized plane</code>이라 하고, \((\frac{X}{Z}, \frac{Y}{Z}, 1)\)의 좌표값을 갖는다. 이를 image로서 나타내기 위해 초점거리를 곱해주고 원점을 정중앙에서 좌상단으로 바꿔서 normalized plane 상의 normalized coordinates \((\frac{X}{Z}, \frac{Y}{Z}, 1)\)을 <code class="language-plaintext highlighter-rouge">image plane 상의 pixel coordinates</code> \((\frac{X}{Z} \ast f_x - \frac{W}{2}, \frac{Y}{Z} \ast f_y - \frac{H}{2}, 1)\) 로 변환할 수 있는데, 이 때 곱하게 되는 행렬이 바로 intrinsic matrix (= calibration matrix) K 이다. 그리고 이렇게 intrinsic parameters를 구하는 과정을 Camera Calibration 이라 부른다.</p> <p>그런데, 카메라의 각도 혹은 위치가 달라지면 맺히는 이미지 자체도 달라지기 때문에 intrinsic matrix를 곱하기 전에 camera의 rotation 및 translation을 먼저 고려해주어야 하는데, 이 때 곱하게 되는 행렬이 바로 extrinsic marix \([R \vert t]\) 이다.</p> <ul> <li> <p><code class="language-plaintext highlighter-rouge">K : intrinsic parameters</code> (3x3 calibration matrix) (초점거리 곱하고 원점 바꾸는 등 카메라 자체의 특성)</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">R, t : extrinsic parameters</code> (3x3 rotation, 3x1 translation matrix) (두 카메라의 상대적인 위치, 각도)</p> </li> </ul> <p>즉, 정리하면 3D point [X, Y, Z]가 image plane [x, y]에 맺히는 projection 과정은 아래의 수식을 따른다.</p> <p>\(\begin{bmatrix} x \\ y \\ z \end{bmatrix}\) = \(\begin{bmatrix} f_x &amp; 0 &amp; -W/2 \\ 0 &amp; f_y &amp; -H/2 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\) \([R \vert t]\) \(\begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}\)</p> <p>\(x_L \Leftrightarrow x_R\) : homography matrix H (projection of 2D point to 2D point)<br/> \(x_R = H x_L\)<br/> \(X \Leftrightarrow x_L\) : projection matrix \(P_L\)<br/> \(x_L = P_LX\) where \(P_L = K_L[R \vert t]\)</p> <h3 id="fundamental-matrix">Fundamental matrix</h3> <ol> <li> \[x_R = H x_L\] </li> <li>\(e_R\)과 \(x_R\)은 직선 \(l_R\) 위에 있으므로 \(e_{R}^{T} l_R = 0\) and \(x_{R}^{T} l_R = 0\)<br/> 예를 들어, 직선 2x+y-2z = 0에 대해 \(l_R\)은 (2, 1, -2)이고, \(e_R\) 및 \(x_R\)은 직선 위에 있는 점 (x, y, z)이다.</li> <li>위의 1.과 2.로부터 \(l_R = e_R \circledast x_R = e_R \circledast H x_L = F x_L\)<br/> where F = fundamental matrix = \(e_R \circledast H\)</li> <li>위의 2.와 3.으로부터 \(x_{R}^{T} l_R = x_{R}^{T} F x_L = 0\)</li> <li>위의 2.와 3.으로부터 \(e_{R}^{T} l_R = e_{R}^{T} F x_L = 0\) 이고, 모든 \(x_L\)에 대해 \(e_{R}^{T} F = 0\)을 만족하므로 \(e_R\)은 F의 left null vector이다. (유사한 방법으로 \(e_L\)은 F의 right null vector이다.)</li> </ol> <p>즉, fundamental matrix와 관련된 식을 정리하면</p> <ul> <li><code class="language-plaintext highlighter-rouge">fundamental matrix</code> : \(F = e_R \circledast H\)</li> <li><code class="language-plaintext highlighter-rouge">correspondence condition</code> : \(x_{R}^{T} F x_L = 0\)</li> <li><code class="language-plaintext highlighter-rouge">epipolar line</code> : \(l_R = F x_L\)</li> <li><code class="language-plaintext highlighter-rouge">epipole</code> : \(e_{R}^{T} F = 0\) (\(e_R\)은 F의 left null vector)</li> </ul> <h3 id="essential-matrix">Essential matrix</h3> <p>essential matrix는 fundamental matrix의 specialization으로, pixel coordinates이 특별히 <code class="language-plaintext highlighter-rouge">calibrated camera들을 다루는 normalized image coordinates (K = I)인 경우</code>에 사용된다. 즉, K = I 여서 \(x^{\ast} = PX = K[R \vert t]X = [R \vert t]X\) 를 만족하는 \(x^{\ast}\)을 normalized coordinates에 있는 image point라 부른다.</p> <p>그리고 epipolar constraint란, vector \(x_L O_L\)과 vector \(x_R O_R\)과 vector \(O_L O_R\)이 같은 평면 epipolar plane 위에 있다는 것이다. 이를 normalized coordinates에서 생각하면, \(x_R^{\ast}\)과 \(Rx_L^{\ast}\)과 t가 <code class="language-plaintext highlighter-rouge">같은 평면 epipolar plane 위에 있다</code>는 뜻이므로 (그 이유는 아래의 Algebraic derivation을 참고하자) 이를 간단하게 수식으로 표현하면 \(x_R^{\ast T}(t \circledast Rx_L^{\ast}) = 0\) 이다.</p> <p>즉, essential matrix와 관련된 식을 정리하면</p> <ul> <li><code class="language-plaintext highlighter-rouge">essential matrix</code> : \(E = t \circledast R\)</li> <li><code class="language-plaintext highlighter-rouge">correspondence condition</code> : \(x_R^{\ast T} E x_L^{\ast} = 0\)</li> </ul> <h3 id="relationship-between-fundamental-matrix-and-essential-matrix">​Relationship between fundamental matrix and essential matrix</h3> <p>이제 intrinsic parameters인 calibration matrix <code class="language-plaintext highlighter-rouge">K를 이용하여 uncalibrated camera들을 다루는 general case로 확장</code>해보자. \(x_L^{\ast}\) 이 normalized coordinates에서의 image point였고, \(x_L\)은 일반적인 pixel coordinates에서의 image point라고 할 때,</p> <ol> <li>\(x_L = K_L x_L^{\ast}\) 이고, \(x_R = K_R x_R^{\ast}\) 이므로 \(x_L^{\ast} = K_L^{-1}x_L\) 이고, \(x_R^{\ast} = K_R^{-1}x_R\)</li> <li> \[x_R^{\ast T} E x_L^{\ast} = 0\] </li> <li>위의 1.과 2.로부터 \(x_R^{T} (K_R^{-T} E K_L{-1}) x_L = 0\)</li> </ol> <p>이 때, 위의 3.은 \(x_{R}^{T} F x_L = 0\) 꼴과 같으므로</p> <p>즉, fundamental matrix와 essential matrix 간의 관계식을 정리하면</p> <ul> <li><code class="language-plaintext highlighter-rouge">fundamental matrix</code> : \(F = e_R \circledast H\)</li> <li><code class="language-plaintext highlighter-rouge">essential matrix</code> : \(E = t \circledast R\)</li> <li><code class="language-plaintext highlighter-rouge">relationshiop</code> : \(F = K_R^{-T} E K_L^{-1} = K_R^{-T} t \circledast R K_L^{-1}\)<br/> (F는 \(K_L, K_R, R, t\) 만으로 표현 가능)</li> </ul> <p>​즉, fundamental matrix F는 각 camera의 calibration matrix \(K_L, K_R\)과 두 camera 사이의 상대적인 rotation R 및 translation t에 의존한다는 것을 알 수 있다.</p> <h3 id="algebraic-derivation">Algebraic derivation</h3> <p>\(F = K_R^{-T} E K_L^{-1} = K_R^{-T} t \circledast R K_L^{-1}\) 임을 조금 더 수학적으로 유도해보자.</p> <p>상대적인 카메라의 위치인 extrinsic parameters의 경우 <code class="language-plaintext highlighter-rouge">left camera에 world origin이 있다고 가정하여 이에 대해 상대적인 right camera의 위치를 R, t로 지정</code>하자.</p> <p>\(ax_L = K_L[I \vert 0]X\)<br/> \(bx_R = K_R[R \vert t]X\)<br/> (여기서 a, b는 단순히 scale factor)</p> <p>\(X = [x, y, z, 1]^T = [X^{\ast}, 1]^T\) 에 대해<br/> \(ax_L = K_{L}X^{\ast}\)<br/> \(bx_R = K_{R}(RX^{\ast}+t)\)</p> <p>\(X^{\ast} = aK_{L}^{-1}x_L\)을 \(bK_{R}^{-1}x_R = RX^{\ast}+t\)에 대입하면,</p> <ol> <li>\(bK_{R}^{-1}x_R = aRK_{L}^{-1}x_L + t\)<br/> 이 때, vector \(bK_{R}^{-1}x_R\)은 vector \(aRK_{L}^{-1}x_L\)와 vector t의 합이므로 기하학적으로 \(bK_{R}^{-1}x_R\)과 \(aRK_{L}^{-1}x_L\)과 t는 <code class="language-plaintext highlighter-rouge">같은 평면 위에 있다. (그리고 그 평면은 epipolar plane이다.)</code><br/> 따라서 vector \(v = t \circledast RK_{L}^{-1}x_L\) 는 epipolar plane에 수직이므로 \(b(K_{R}^{-1}x_R)^{T}v = a(RK_{L}^{-1}x_L)^{T}v + t^{T}v = 0\) 이라 쓸 수 있다.<br/> \(b(K_R^{-1}x_R)^{T}v = 0\)을 정리하면 \(x_R^{T} (K_R^{-T} (t \circledast R) K_L^{-1}) x_L = 0\) 이다.</li> <li>\(bx_R = aK_{R}RK_L^{-1}x_L + K_{R}t\)<br/> 이와 비슷하게 vector \(w = K_{R}t \circledast K_{R}RK_{L}^{-1}x_L\)는 \(bx_R = aK_{R}RK_{L}^{-1}x_L + K_{R}t\) 에 수직이므로 \(bx_{R}^{T}w = a(K_{R}RK_{L}^{-1}x_L)^{T}w + (K_{R}t)^{T}w = 0\) 이라 쓸 수 있다.<br/> \(bx_{R}^{T}w = 0\)을 정리하면 \(x_{R}^{T} (K_{R} t \circledast K_{R}RK_{L}^{-1}) x_L = 0\) 이다.</li> <li>위의 1., 2.에서 유도한 \(x_R^{T} (K_R^{-T} (t \circledast R) K_L^{-1}) x_L = 0\)과 \(x_{R}^{T} (K_{R} t \circledast K_{R}RK_{L}^{-1}) x_L = 0\)을 통해<br/> \(F = K_{R}^{-T} t \circledast R K_{L}^{-1}\) 임을 유도할 수 있다.<br/> (F 유도에 \(x_R^{T} (K_R^{-T} (t \circledast R) K_L^{-1}) x_L = 0\) 은 왜 필요한 거지..? <code class="language-plaintext highlighter-rouge">조금 더 공부 필요</code>)</li> </ol> <h2 id="image-rectification">Image Rectification</h2> <p><code class="language-plaintext highlighter-rouge">Image rectification은 주어진 images를 common image plane에 project하는 것</code>이다. 여러 각도에서 찍은 <code class="language-plaintext highlighter-rouge">이미지들 간에 매칭되는 점들을 쉽게 찾기 위해</code> computer stereo vision 분야에서 널리 사용되는 transform 기법이다. 이 때, 매칭되는 점들을 찾는 것은 위에서 설명한 epipolar geometry에 의해 수행된다.<br/> (<code class="language-plaintext highlighter-rouge">epipolar geometry 요약 : 한 image의 어떤 pixel에 매칭되는 3D 상의 점들은 다른 image의 epipolar line 위에 있다</code>.)</p> <blockquote> <p>parallel stereo cameras : e.g. 두 카메라 사이의 관계가 t = [T; 0; 0] (shifted in x-direction)<br/> 만약 두 image plane이 같은 평면 상에 있다면 optical axes가 parallel하여 <code class="language-plaintext highlighter-rouge">모든 epipolar line은 horizontal axis에 평행하고 epipoles는 infinite point in [1, 0, 0]T direction in homogeneous coordinates로 mapping되며, 매칭되는 점들이 같은 vertical coordinates를 가진다.</code> 그리고 이 때 매칭되는 점들을 찾는 것은 matching cost(minimize SSD or maximize normalized correlation)를 찾기 위해 horizontal scan만 하면 되므로 쉬운 문제이다.<br/> 예를 들어, left image의 어떤 pixel (\(x_l, y_l\))에 대응되는 점을 right image에서 찾는다고 가정하자. 이 때, right image에서의 horizontal scan이 \(x_l\) 의 위치(아래 오른쪽 사진의 빨간 점)을 넘어가서는 안 된다. right image에서 \(x_l\) 의 오른쪽에 matching point가 있다면 연장선을 그었을 때 3D point가 camera center의 뒤쪽에 있다는 것을 의미하기 때문이다.</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/4-480.webp 480w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/4-800.webp 800w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>그런데 horizontal scan을 하며 모든 pixel에 대해 matching cost를 일일이 찾는 건 inefficient하므로 positive matches &amp; negative matches 만들어서 classifier training 할 수 있다.<br/> (smaller patch size : more detail, but noisy)<br/> (bigger patch size : less detail, but smooth)<br/> 이를 통해 disparity map을 얻을 수 있고, depth map을 얻을 수 있다.</p> <p>For <code class="language-plaintext highlighter-rouge">post-processing, MRF(Markov Random Field) or CRF(Conditional Random Field)</code> : energy minimization</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/5-480.webp 480w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/5-800.webp 800w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/6-480.webp 480w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/6-800.webp 800w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>general stereo cameras : 두 카메라 사이의 관계가 [R|t]<br/> 하지만, 두 카메라는 보통 서로 rotate, translate 되어 있기 때문에 <code class="language-plaintext highlighter-rouge">두 image를 warp해서라도 두 image plane이 같은 평면 상에 있던 것처럼 (모든 epipolar line이 수평선이 되도록) 만들 필요가 있고, 이것이 바로 image rectification</code>이다.</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/7-480.webp 480w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/7-800.webp 800w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Image rectification은 두 images에 대해 동시에 수행되며, 일반적으로 셋 이상의 images에 대해서는 simultaneous rectification이 불가능하다.</p> <p>calibrated cameras에 대해서는 essential matrix가 두 camera 사이의 관계를 설명(\(x_{R}^{\ast T} E x_{L}^{\ast} = 0\))하고, uncalibrated cameras (general case)에 대해서는 fundamental matrix가 두 camera 사이의 관계를 설명(\(x_{R}^{T} F x_L = 0\))한다.</p> <p>Image rectification 알고리즘은 대표적으로 세 가지가 있다. : <code class="language-plaintext highlighter-rouge">planar, cylindrical, and polar rectification</code><br/> Image rectification을 수행하기 위해서는 projective transformation을 위해 homography matrix \(H_L, H_R\)를 찾아야 하는데, 여러 방법 중 하나를 아래에서 소개하겠다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/8.JPG-480.webp 480w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/8.JPG-800.webp 800w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/8.JPG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/8.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>우선 left camera에 world origin이 있다고 가정하여 이에 대해 상대적인 right camera의 위치를 R, t로 지정하자. 그러면 \(O_{L} = 0, O_{R} = -R^{t} t\) 라 쓸 수 있고, \(P_{L} = K_{L}[I \vert 0], P_{R} = K_{R}[R \vert t]\) 라 쓸 수 있다.<br/> <code class="language-plaintext highlighter-rouge">(왜 OR = -R^t t 이지?)</code></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/9.JPG-480.webp 480w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/9.JPG-800.webp 800w,/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/9.JPG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-04-01-Epipolar_Geometry_Image_Rectification/9.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>첫 번째로, <code class="language-plaintext highlighter-rouge">epipole의 위치를 구한다.</code><br/> \(O_R\)을 left image plane에 project한 게 \(e_L\) 이고, \(O_L\)을 right image plane에 project한 게 \(e_R\) 이므로<br/> \(e_{L} = P_{L} [O_{R} ; 1] = P_{L} [-R^{t} t ; 1] = K_{L}[I \vert 0][-R^{t} t ; 1] = - K_{L} R^{t} t\)<br/> \(e_{R} = P_{R} [O_{L} ; 1] = P_{R} [0 ; 1] = K_{R}[R \vert t][0 ; 1] = K_{R} t\)</li> <li>두 번째로, <code class="language-plaintext highlighter-rouge">left image plane이 baseline에 평행해지도록 rotate시키는 projective transformation HL1 을 구한다. 이는 original optical axis와 desired optical axis 간의 외적</code>으로 구할 수 있다.</li> <li>세 번째로, <code class="language-plaintext highlighter-rouge">horizontal axis가 baseline 및 epipolar line과 평행해지도록 twist시키는 projective transformation</code> \(H_{L2}\) 를 구한다. 맞게 구했다면 twist 후 epipoles가 infinity in x-direction로 mapping되어야 한다.</li> <li>네 번째로, left image를 rectify하는 <code class="language-plaintext highlighter-rouge">최종 projective transformation</code> \(H_{L} = H_{L2}H_{L1}\) 을 구한다.</li> <li>다섯 번째로, 같은 방법으로 right image를 rectify하는 <code class="language-plaintext highlighter-rouge">최종 projective transformation</code> \(H_{R} = H_{R2}H_{R1}\)을 구한다. 여기서 주의할 점은, left image와 right image를 각각 \(H_{L1}\)과 \(H_{R1}\)으로 <code class="language-plaintext highlighter-rouge">rotate한 후에 optical axis가 서로 평행해야 한다</code>.<br/> One strategy is to pick a plane parallel to the line where the two original optical axes intersect to minimize distortion from the reprojection process. 또는 We simply define as \(H_{R} = H_{L} R^{t}\)<br/> (<code class="language-plaintext highlighter-rouge">위의 두 가지 strategy 이해 못 했음. 추가 공부 필요</code>)</li> <li>마지막으로, two images가 same resolution을 갖도록 <code class="language-plaintext highlighter-rouge">scale</code>해준다. 그러면 horizontal epipoles가 align되어 매칭되는 점들이 같은 <code class="language-plaintext highlighter-rouge">vertical coordinates를 가지므로 매칭되는 점들을 찾기 위해 horizontal scan만 하면 되는 쉬운 문제로 바뀐다</code>.<br/> 추가로, 꼭 \(K_{L}, K_{R}\) <code class="language-plaintext highlighter-rouge">intrinsic parameter를 모르더라도 a set of seven or more image-to-image correspondences만 알면 fundamental matrix와 epipoles를 계산할 수 있어서 image rectification을 수행할 수 있다고 한다.</code><br/> (<code class="language-plaintext highlighter-rouge">a set of seven or more correspondences로 fundamental matrix, epipole 구해서 rectification 하는 거 이해 못 했음. 추가 공부 필요</code>)<br/> (fundamental matrix와 epipole 알면 image rectification 수행 가능)</li> </ul> <blockquote> <p>참고 사이트 :<br/> <a href="https://blog.naver.com/hms4913/220043661788">https://blog.naver.com/hms4913/220043661788</a><br/> <a href="https://en.wikipedia.org/wiki/Image_rectification#cite_note-HARTLEY2003-9">https://en.wikipedia.org/wiki/Image_rectification#cite_note-HARTLEY2003-9</a><br/> <a href="https://csm-kr.tistory.com/64">https://csm-kr.tistory.com/64</a><br/> CSC420: Intro to Image Understanding 수업 내용</p> </blockquote> <p>중간중간에 있는 질문들은 아직 이해하지 못해서 남겨놓은 코멘트입니다.<br/> 추후에 다시 읽어보고 이해했다면 업데이트할 예정입니다.<br/> 혹시 알고 계신 분이 있으면 댓글로 남겨주시면 감사하겠습니다!</p>]]></content><author><name></name></author><category term="depth-estimation"/><category term="epipolar"/><category term="fundamental"/><category term="essential"/><category term="image"/><category term="rectification"/><summary type="html"><![CDATA[Epipolar Geometry & Image Rectification]]></summary></entry><entry><title type="html">Quantization</title><link href="https://semyeong-yu.github.io/blog/2024/Quantization/" rel="alternate" type="text/html" title="Quantization"/><published>2024-03-02T14:00:00+00:00</published><updated>2024-03-02T14:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Quantization</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Quantization/"><![CDATA[<h1 id="introduction"><strong>Introduction</strong></h1> <h3 id="the-increasing-demand-for-deep-learning-model-efficiency">The Increasing Demand for Deep-learning Model Efficiency</h3> <p>In the area of deep learning, where neural networks have remarkable capabilites to learn complex patterns from massive datasets, there has emerged an ongoing pursuit for model efficiency. We need to achieve quick inference speed and less memory consumption in order to apply our deep-learning models to a wider range of users and applications. Especially, there is an increasing demand for deploying our models on mobile devices or edge devices and running the models in real-time, but the devices have resource-constrained hardware. Therefore it became important to strike a balance between model accuracy and computational cost.</p> <h3 id="some-ways-to-achieve-faster-inference-speed">Some Ways to Achieve Faster Inference Speed</h3> <p>There are several ways to achieve faster inference speed of deep-learning model.</p> <ul> <li>Hardware accelerators : There are specialized hardware accelerators such as GPUs and TPUs which can be utilized to efficiently execute optimized deep-learning operations. We can achieve faster inference speed because the hardware accelerators are designed to utilize multi-threading or mult-processing to parallelize inference across multiple cores.</li> <li>Optimized kernels : Kernel optimization refers to the process of improving the performance of codes that form the core computational operations of a software application. There are optimized kernels such as cuDNN and Intel MKL-DNN to perform optimized deep-learning operations. For example, we can achieve faster inference speed by vectorization or hardware-specific assembly-level optimization.</li> <li>Model architecture : We can make a model of compact architecture such as NasNet, MobileNet, and FBNet. The models have more suitable architectures to deploy on mobile devices. Another example is that we can reduce computation by using bottleneck architecture like depth-wise separable convolution. By separating depth-wise convolution(= per channel convolution) and point-wise convolution(= 1x1 convolution), we extract many feature maps(as much as output channels) only at point-wise convolution with reduced computation. Eventually, we can reduce the number of parameters by about kernel_size x kernel_size times and we can achieve faster inference speed.</li> <li>Model network optimization : We can optimize the existing model architecture via some optimization techniques. For example, we can reduce computation by pruning unnecessary layers, channels, or filters, which results in faster inference speed.</li> <li>Quantization : We can optimize the existing model via reducing the precision of weights or activations. For example, we can reduce inference time and model size by quantizing weights or activations into lower bit-width.</li> </ul> <h3 id="the-reason-why-quantization-can-be-a-powerful-technique-among-them">The Reason why Quantization can be a Powerful Technique among them</h3> <p>It should be considered that the real world’s environment, on which our model is deployed, has limited resources. In real world, we cannot produce model with expensive hardware in large quantities, so we have to fix hardware at a proper price. Then our goal is to fit our model to the specific target hardware, usually for edge devices. In the case of kernel optimization, it has diminishing returns : Improving inference speed by optimizing computational operations in kernel can become increasingly difficult and may require more effort for relatively minor gains in performance. It’s because bottlenecks would exist at higher level such as I/O operations. Also, let us assume that we already designed the architecture of efficient small model as a backbone, but we need more improvement in inference time and model size.</p> <p>How can we improve a existing model with fixed hardware, kernel, and model architecture? In that case, eventually quantization can be a powerful technique for inference speedup. Since quantization is lossy compression, it is important to achieve inference speedup with minimal accuracy drop.</p> <h3 id="a-brief-explanation-of-quantization">A Brief Explanation of Quantization</h3> <p>Quantization is the process of reducing the precision of numerical values in neural network model : for example, from FP32 to INT8. By reducing the precision of weights or activations of deep-learning model, we can compress the model’s size and computational cost.</p> <p>We will discuss how quantization works and look through various quantization techniques such as Post-Training-Quantization and Quantization-Aware-Training. In addition, we are also going to discuss how we quantize a model on different frameworks such as Pytorch and ONNX.</p> <p>Nowadays, it is important to consider the balance between model accuracy and computational cost. By understanding the process of quantization, you will have the knowledge to use its potential and may efficiently bridge the gap between powerful AI models and resource-constrained real-world environments.</p> <h1 id="method">Method</h1> <h2 id="quantization">Quantization</h2> <h3 id="overview-of-quantization">Overview of Quantization</h3> <p>In general, we use FP32 (= 32-bit floating-point) representation in deep-learning models because it provides a high level of numerical precision at the backpropagation during the training phase. However, performing operations in high bit-depth can be slow during the inference phase when it is deployed on the small device with resource-constrained hardware.</p> <p>In the real world’s environment with resource-constrained hardware, we need small model size, small RAM bandwidth, and inference speedup with less accuracy drop. To achieve this goal, quantization can be a powerful technique.</p> <p>Quantization is to perform computation and storage at reduced precision using lower bits.</p> <p>We can quantize a model from FP32 to FP16, INT8, or INT4. Here, INT8 (= 8-bit integer) quantization is a common choice due to a balance between accuracy drop and efficiency improvement. By INT8 quantization, we can also utilize the advantages of modern specialized hardware accelerators such as NVIDIA GPU, TPU, and Qualcomm DSP so that they perform efficient INT8 arithmetic operations. If you quantize a model from FP32 to INT8, model size is typically reduced by 4 times and inference speed is improved by 2~4 times and required memory bandwidth is reduced by 2~4 times.</p> <h3 id="principle-of-quantization">Principle of Quantization</h3> <p>Let me explain the main principle of quantization.</p> <p>First, we specify the float range to be quantized and clip values outside the range.</p> <p>Then we take the quantization equation \(x_q=clip(round({x\over s})+z)\) for the Quantization Layer and the dequantization equation \(x=s(x_q-z)\) for the Dequantization Layer.</p> <p>Here, <strong>s</strong> is a scale factor which determines the range mapping and <strong>z</strong> is a zero-point integer such that \(x=0\) in FP32 corresponds to \(x_q=z\) in INT8.</p> <p>When we quantize weights or activations of a model by the above equation in the case of INT8 quantization, we have to map the range of FP32 precision into the range of INT8 precision as shown in the picture below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled1-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled1-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-02-Quantization/Untitled1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-02-Quantization/Untitled.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Scale Quantization (Symmetric) from FP32 to IN8 vs Affine Quantization (Asymmetric) from FP32 to INT8 </div> <p>There are two types of range-mapping techniques in quantization according to the way of choosing scale factor <strong>s</strong> and zero-point integer <strong>z.</strong></p> <ul> <li> <p>Affine Quantization Mapping : INT8 range is from -128 to 127, which is asymmetric.</p> \[s = \frac{\left| \beta\ - \alpha \right|}{2^{bit}-1}, z = - round(\frac{\alpha}{s})-2^{bit-1}\] <p>\(\alpha, \beta\) = min, max of original weight/activation values</p> <p>\(\alpha_q, \beta_q\) = min, max of quantized weight/activation values ( \(\beta_q-\alpha_q=2^{bit}-1\) )</p> <ul> <li>advantage : Affine quantization generally offers tighter clipping range since \(\alpha,\ \beta\) are assigned to min, max of observed values itself. This can result in good quantization resolution. Also, it is particularly useful for quantizing non-negative activations of which distribution is not symmetric around zero. You can calibrate the zero-point integer to match the data distribution in order to reduce quantization errors.</li> <li>disadvantage : Affine quantization needs extra computations with calibration of zero-point integer and requires hardware-specific tweaks.</li> </ul> </li> <li> <p>Scale Quantization Mapping : INT8 range is from -127 to 127, which is symmetric.</p> \[s = {\left| \beta \right| \over 2^{bit-1}-1}, z=0\] <p>\(\beta, \beta\) = min, max of original weight/activation values where \(\vert \alpha \vert \leq \vert \beta \vert\)</p> <p>\(\beta_q, \beta_q\) = min, max of quantized weight/activation values (\(\beta_q = 2^{bit-1}-1\))</p> <ul> <li>advantage : Symmetric quantization eliminates the need to calculate zero-point integer and it is simpler than asymmetric quantization. Thus, symmetric quantization is more hardware-friendly and produces higher speedup.</li> <li>disadvantage : For skewed signals like non-negative activations, this can result in bad quantization resolution because the clipping range includes negative values that never show up in the input.</li> </ul> </li> </ul> <p>To quantize each layer by MinMax, we need to know the value of \(\alpha,\ \beta\) to determine scale factor <strong>s</strong> and zero-point integer <strong>z.</strong> Thus, we insert observer into each layer of a model and the observers gather statistics from the activations and weights of a neural network during the forward pass of calibration process. These statistics are then used to determine scale-factor and zero-point integer.</p> <p>The above equations are based on MinMax observer, but there are also many other observers in Pytorch framework as shown in the picture below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled2-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled2-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-02-Quantization/Untitled2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled3-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled3-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-02-Quantization/Untitled3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Different observers to determine scale factor and zero-point integer </div> <h3 id="types-of-quantization">Types of Quantization</h3> <p>Quantization techniques can be grouped into two classes depending on to which stages of neural network model’s development pipeline they are applicable. One is PTQ (Post-Training Quantization) which is applicable after training is finished. The other is QAT (Quantization-Aware Training) which is applicable during training.</p> <ul> <li> <p>PTQ (Post-Training Quantization) : It is to quantize a model which was already trained in high precision. The quantization has nothing to do with training.</p> <p>If the clipping range of activation is determined during inference, it is called Post-Training Dynamic Quantization.</p> <p>If the clipping range of activation is determined before inference, it is called Post-Training Static Quantization.</p> </li> <li> <p>QAT (Quantization-Aware Training) : It is to fine-tune a model with integrating the quantization effects. The model is exposed to quantization during training by inserting observers and fake-quantization modules(e.g. QuantStub and DeQuantStub) in the forward/backward-pass. Here, fake-quantization modules mimic the behavior of quantized operations while working with full precision representations, allowing developers to simulate the effects of quantization during training and evaluation.</p> <p>QAT is more complicated than PTQ since it needs training process. However, QAT outperforms PTQ since the weights of the model is fine-tuned to the quantization task. Therefore, QAT may be an appropriate method for small models such as MobileNet since small models on edge device are more sensitive to quantization error.</p> </li> </ul> <h3 id="model-fusion">Model Fusion</h3> <p>In addition, we usually fuse modules of a model before quantization since it would have less accuracy drop : typically Conv2d-BatchNorm or Conv2d-ReLU or Conv2d-BatchNorm-ReLU or Linear-ReLU. It’s because the overall number of layers to be quantized and the number of operations are reduced if you fuse modules. This reduction of quantization overhead can mitigate the cumulative quantization error, resulting in a less accuracy drop.</p> <h3 id="code-implementation-with-torchaoquantization-library">code implementation with torch.ao.quantization library</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># One Example of Model Fusion : timm resnet18
# torch.ao.quantization.fuse_modules() is used     for PTQ
# torch.ao.quantization.fuse_modules_qat() is used for QAT
</span> 
<span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">fuse_modules_qat</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[[</span><span class="sh">"</span><span class="s">conv1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">bn1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">act1</span><span class="sh">"</span><span class="p">]],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">name1</span><span class="p">,</span> <span class="n">module1</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_children</span><span class="p">():</span>
    <span class="k">if</span> <span class="sh">"</span><span class="s">layer</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">name1</span> <span class="ow">and</span> <span class="n">module1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">name2</span><span class="p">,</span> <span class="n">module2</span> <span class="ow">in</span> <span class="n">module1</span><span class="p">.</span><span class="nf">named_children</span><span class="p">():</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">fuse_modules_qat</span><span class="p">(</span><span class="n">module2</span><span class="p">,</span> <span class="p">[[</span><span class="sh">"</span><span class="s">conv1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">bn1</span><span class="sh">"</span><span class="p">],</span> <span class="p">[</span><span class="sh">"</span><span class="s">conv2</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">bn2</span><span class="sh">"</span><span class="p">]],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">name3</span><span class="p">,</span> <span class="n">module3</span> <span class="ow">in</span> <span class="n">module2</span><span class="p">.</span><span class="nf">named_children</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">name3</span> <span class="o">==</span> <span class="sh">"</span><span class="s">downsample</span><span class="sh">"</span> <span class="ow">and</span> <span class="n">module3</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                    <span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">fuse_modules_qat</span><span class="p">(</span><span class="n">module3</span><span class="p">,</span> <span class="p">[[</span><span class="sh">"</span><span class="s">0</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">1</span><span class="sh">"</span><span class="p">]],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <h2 id="post-training-dynamic-quantization-in-pytorch">Post-Training Dynamic Quantization in Pytorch</h2> <p>If the clipping range of activation is determined during inference, it is called dynamic quantization. Only weights of a trained model are quantized before inference and activations of the model should be quantized dynamically during inference. So, observer which can compute quantization parameters in real-time manner should be used such as MinMax and Percentile.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled4-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled4-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-02-Quantization/Untitled4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Post-Training Dynamic Quantization </div> <ul> <li> <p>advantage :</p> <p>Inference accuarcy may be higher than Static Quantization since scale factor and zero-point integer of activations are determined in real-time during inference such that they fit our input data.</p> <p>Post-Training Dynamic Quantization is appropriate to dynamic models like LSTMs or BERT. It’s because the disbribution of activation values of dynamic models can vary significantly depending on the input data.</p> </li> <li> <p>disadvantage :</p> <p>The scale factor and zero-point integer of activations should be computed dynamically at inference runtime. This results in the increase of the cost of inference and has less improvement of inference latency than Static Quantization.</p> </li> </ul> <p>Note that inference of a quantized model is still executed on CPU for Pytorch framework. (For other frameworks, GPU may work.)</p> <h3 id="code-implementation-with-torchaoquantization-library-1">code implementation with torch.ao.quantization library</h3> <p>It is very simple to implement Post-Training Dynamic Quantization as shown below.</p> <p>You can specify submodules which will be quantized using “qconfig_spec” argument.</p> <p>That’s all!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">quantized_model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">quantize_dynamic</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">qconfig_spec</span><span class="o">=</span><span class="p">{</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">},</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">quint8</span><span class="p">)</span>
</code></pre></div></div> <h2 id="post-training-static-quantization-in-pytorch">Post-Training Static Quantization in Pytorch</h2> <p>If the clipping range of activation is determined before inference, it is called static quantization. Both weights and activations of a trained model are quantized before inference. Here, by calibration, observers observe the range of stored values to determine quantization parameters.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled5-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled5-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-02-Quantization/Untitled5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled6-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled6-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-02-Quantization/Untitled6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Post-Training Static Quantization - calibrate &amp; quantize </div> <ul> <li> <p>advantage :</p> <p>It is relatively easy to find scale factor and zero-point integer in advance before inference.</p> <p>Post-Training Static Quantization is appropriate to CNN models since their throughput is limited by memory bandwidth for activations and we can figure out the disbribution of activation values of CNN models during calibration.</p> </li> <li> <p>disadvantage :</p> <p>Smaller model like Mobile CNN is more sensitive to quantization errors, so Post-Training Quantization may have significant accuracy drop. It’s the moment when we need Quantization-Aware Training.</p> </li> </ul> <p>Note that inference of a quantized model is still executed on CPU for Pytorch framework. (For other frameworks like tflite, both CPU and GPU may work.)</p> <h3 id="code-implementation-with-torchaoquantization-library-2">code implementation with torch.ao.quantization library</h3> <ul> <li>Step 1. Module Fusion :</li> </ul> <p>Fuse modules for less accuracy drop</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">fuse_modules</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[[</span><span class="sh">"</span><span class="s">conv1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">bn1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">act1</span><span class="sh">"</span><span class="p">]],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">name1</span><span class="p">,</span> <span class="n">module1</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_children</span><span class="p">():</span>
    <span class="k">if</span> <span class="sh">"</span><span class="s">layer</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">name1</span> <span class="ow">and</span> <span class="n">module1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">name2</span><span class="p">,</span> <span class="n">module2</span> <span class="ow">in</span> <span class="n">module1</span><span class="p">.</span><span class="nf">named_children</span><span class="p">():</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">fuse_modules</span><span class="p">(</span><span class="n">module2</span><span class="p">,</span> <span class="p">[[</span><span class="sh">"</span><span class="s">conv1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">bn1</span><span class="sh">"</span><span class="p">],</span> <span class="p">[</span><span class="sh">"</span><span class="s">conv2</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">bn2</span><span class="sh">"</span><span class="p">]],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">name3</span><span class="p">,</span> <span class="n">module3</span> <span class="ow">in</span> <span class="n">module2</span><span class="p">.</span><span class="nf">named_children</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">name3</span> <span class="o">==</span> <span class="sh">"</span><span class="s">downsample</span><span class="sh">"</span> <span class="ow">and</span> <span class="n">module3</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                    <span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">fuse_modules</span><span class="p">(</span><span class="n">module3</span><span class="p">,</span> <span class="p">[[</span><span class="sh">"</span><span class="s">0</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">1</span><span class="sh">"</span><span class="p">]],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># insert torch.ao.quantization.QuantStub() layer 
#	and torch.ao.quantization.DeQuantStub() layer 
# at the beginning and end of forward() respectively.
</span></code></pre></div></div> <ul> <li>Step 2. Prepare :</li> </ul> <p>Insert observer and prepare the quantization process</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 'x86' or 'fbgemm' for server inference
# 'qnnpack' for mobile inference
</span><span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">quantized</span><span class="p">.</span><span class="n">engine</span> <span class="o">=</span> <span class="sh">'</span><span class="s">fbgemm</span><span class="sh">'</span> 
<span class="n">model</span><span class="p">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">get_default_qconfig</span><span class="p">(</span><span class="sh">'</span><span class="s">fbgemm</span><span class="sh">'</span><span class="p">)</span>
<span class="c1"># You can use different observers for quantized_model.qconfig
</span><span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <ul> <li>Step 3. Calibration :</li> </ul> <p>Forward-pass to determine the scale factor and zero-point integer based on the given input calibration dataset.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># forward pass of model in .eval() phase
# observer computes scale factor and zero-point integer by calibration
</span><span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cpu:0</span><span class="sh">"</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">gt</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">calibrate_loader</span><span class="p">):</span>
		<span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cpu:0</span><span class="sh">"</span><span class="p">))</span>
		<span class="n">gt</span> <span class="o">=</span> <span class="n">gt</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cpu:0</span><span class="sh">"</span><span class="p">))</span>
	  <span class="nf">model</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
</code></pre></div></div> <ul> <li>Step 4. Convert :</li> </ul> <p>Convert from FP32 to reduced precision based on the calibrated quantization parameters</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">convert</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <h3 id="code-implementation-with-pytorch_quantization-library">code implementation with pytorch_quantization library</h3> <ul> <li>Step 1. Initialize quantizable modules :</li> </ul> <p>pytorch_quantization library supports only the quantization of the layers shown below.</p> <ul> <li>QuantConv1d, QuantConv2d, QuantConv3d, QuantConvTranspose1d, QuantConvTranspose2d, QuantConvTranspose3d</li> <li>QuantLinear</li> <li>QuantAvgPool1d, QuantAvgPool2d, QuantAvgPool3d, QuantMaxPool1d, QuantMaxPool2d, QuantMaxPool3d</li> </ul> <p>If you want to quantize another layer, you should implement the quantized version of custom modules. (In my case, I implemented QuantHardswish and QuantConvReLU2d.)</p> <p>For the model instance that you create after quant_modules.initialize(), it automatically converts the default modules and custom modules into their quantizable version via monkey-patching.</p> <ul> <li>Limitation : pytorch_quantization library “immediately” converts the modules of a model into their quantized version as soon as the model is loaded after quant_modules.initialize(). Therefore the modules to be quantized must be the modules of a model to be loaded.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pytorch_quantization</span> <span class="kn">import</span> <span class="n">quant_modules</span>

<span class="n">custom_quant_modules</span> <span class="o">=</span> <span class="p">[(</span><span class="n">nn</span><span class="p">,</span> <span class="sh">"</span><span class="s">Hardswish</span><span class="sh">"</span><span class="p">,</span> <span class="n">QuantHardswish</span><span class="p">),</span> 
		<span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">intrinsic</span><span class="p">.</span><span class="n">modules</span><span class="p">.</span><span class="n">fused</span><span class="p">,</span> <span class="sh">"</span><span class="s">ConvReLU2d</span><span class="sh">"</span><span class="p">,</span> <span class="n">QuantConvReLU2d</span><span class="p">)]</span>
<span class="n">quant_modules</span><span class="p">.</span><span class="nf">initialize</span><span class="p">(</span><span class="n">custom_quant_modules</span><span class="o">=</span><span class="n">custom_quant_modules</span><span class="p">)</span>

<span class="c1"># create a model instance
# then modules are substituted into quantizable version 
# automatically via monkey-patching
</span></code></pre></div></div> <ul> <li>Step 2. Prepare :</li> </ul> <p>I utilized histogram-based calibration for activations, but you can also try another calibration method.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">quant_desc_input</span> <span class="o">=</span> <span class="nc">QuantDescriptor</span><span class="p">(</span><span class="n">calib_method</span><span class="o">=</span><span class="sh">'</span><span class="s">histogram</span><span class="sh">'</span><span class="p">)</span>
<span class="n">quant_nn</span><span class="p">.</span><span class="n">QuantConv2d</span><span class="p">.</span><span class="nf">set_default_quant_desc_input</span><span class="p">(</span><span class="n">quant_desc_input</span><span class="p">)</span>
<span class="n">quant_nn</span><span class="p">.</span><span class="n">QuantLinear</span><span class="p">.</span><span class="nf">set_default_quant_desc_input</span><span class="p">(</span><span class="n">quant_desc_input</span><span class="p">)</span>
<span class="n">QuantHardswish</span><span class="p">.</span><span class="nf">set_default_quant_desc_input</span><span class="p">(</span><span class="n">quant_desc_input</span><span class="p">)</span>
<span class="n">QuantConvReLU2d</span><span class="p">.</span><span class="nf">set_default_quant_desc_input</span><span class="p">(</span><span class="n">quant_desc_input</span><span class="p">)</span>
</code></pre></div></div> <ul> <li>Step 3. Calibration :</li> </ul> <p>Forward-pass to determine the scale factor and zero-point integer</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># If there is calibrator, disable quantization and enable calibrator
# Otherwise, disable the module itself
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
<span class="c1"># Enable calibrators to collect statistics
</span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_modules</span><span class="p">():</span>
    <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">quant_nn</span><span class="p">.</span><span class="n">TensorQuantizer</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">module</span><span class="p">.</span><span class="n">_calibrator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">module</span><span class="p">.</span><span class="nf">disable_quant</span><span class="p">()</span> <span class="c1"># use full precision data to calibrate
</span>            <span class="n">module</span><span class="p">.</span><span class="nf">enable_calib</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">module</span><span class="p">.</span><span class="nf">disable</span><span class="p">()</span>

<span class="c1"># Calibration  
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">ground_truth</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
    <span class="nf">model</span><span class="p">(</span><span class="n">image</span><span class="p">.</span><span class="nf">cuda</span><span class="p">())</span> <span class="c1"># forward pass of model in .eval() phase
</span>    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">7</span><span class="p">:</span>
        <span class="k">break</span>

<span class="c1"># If there is calibrator, enable quantization and disable calibrator
# Otherwise, enable the module itself
</span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_modules</span><span class="p">():</span>
    <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">quant_nn</span><span class="p">.</span><span class="n">TensorQuantizer</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">module</span><span class="p">.</span><span class="n">_calibrator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">module</span><span class="p">.</span><span class="nf">enable_quant</span><span class="p">()</span>
            <span class="n">module</span><span class="p">.</span><span class="nf">disable_calib</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">module</span><span class="p">.</span><span class="nf">enable</span><span class="p">()</span>

<span class="c1"># After calibration, quantizers obtain amax set, which is
# absolute maximum input value representable in the quantized space
# In default, amax for weight is per channel 
#         and amax for activation is per tensor.
</span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_modules</span><span class="p">():</span>
    <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">quant_nn</span><span class="p">.</span><span class="n">TensorQuantizer</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">module</span><span class="p">.</span><span class="n">_calibrator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">.</span><span class="n">_calibrator</span><span class="p">,</span> <span class="n">calib</span><span class="p">.</span><span class="n">MaxCalibrator</span><span class="p">):</span>
                <span class="n">module</span><span class="p">.</span><span class="nf">load_calib_amax</span><span class="p">(</span><span class="n">strict</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># method = "percentile" or "mse" or "entropy"
</span>								<span class="n">module</span><span class="p">.</span><span class="nf">load_calib_amax</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="sh">"</span><span class="s">percentile</span><span class="sh">"</span><span class="p">,</span> 
									<span class="n">percentile</span><span class="o">=</span><span class="mf">99.99</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> 
				<span class="c1"># You can check 
</span>				<span class="nf">print</span><span class="p">(</span><span class="sa">F</span><span class="sh">"</span><span class="si">{</span><span class="n">name</span><span class="si">:</span><span class="mi">40</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">module</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>		
</code></pre></div></div> <h2 id="qatquantization-aware-training-in-pytorch">QAT(Quantization-Aware Training) in Pytorch</h2> <p>It has similar steps with Post-Training Static Quantization, but the difference is that the weights of a model are updated via fine-tuning to quantization task. To emulate the quantization, we insert fake-quantization modules at the beginning and end of forward() function.</p> <ul> <li> <p>advantage :</p> <p>The weights of model is updated to fit the quantization task (a.k.a fine-tuning), so it has usually higher accuracy than Post-Training Quantization.</p> </li> <li> <p>disadvantage :</p> <p>It needs additional resources due to training process, so it is more complicated.</p> </li> </ul> <p>Note that training of a model can be executed on both CPU and GPU, but inference of a quantized model is still executed on CPU for Pytorch framework. To compare the inference time(latency) between original model and quantized model, I did inference on CPU for both models.</p> <h3 id="code-implementation-using-torchaoquantization-library">code implementation using torch.ao.quantization library</h3> <ul> <li>Step 1. Module Fusion :</li> </ul> <p>Fuse modules for less accuracy drop</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">fuse_modules_qat</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> 
																			<span class="p">[[</span><span class="sh">"</span><span class="s">conv1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">bn1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">act1</span><span class="sh">"</span><span class="p">]],</span> 
																			<span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">name1</span><span class="p">,</span> <span class="n">module1</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_children</span><span class="p">():</span>
    <span class="k">if</span> <span class="sh">"</span><span class="s">layer</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">name1</span> <span class="ow">and</span> <span class="n">module1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">name2</span><span class="p">,</span> <span class="n">module2</span> <span class="ow">in</span> <span class="n">module1</span><span class="p">.</span><span class="nf">named_children</span><span class="p">():</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">fuse_modules_qat</span><span class="p">(</span><span class="n">module2</span><span class="p">,</span> 
																					<span class="p">[[</span><span class="sh">"</span><span class="s">conv1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">bn1</span><span class="sh">"</span><span class="p">],</span> <span class="p">[</span><span class="sh">"</span><span class="s">conv2</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">bn2</span><span class="sh">"</span><span class="p">]],</span>
																				  <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">name3</span><span class="p">,</span> <span class="n">module3</span> <span class="ow">in</span> <span class="n">module2</span><span class="p">.</span><span class="nf">named_children</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">name3</span> <span class="o">==</span> <span class="sh">"</span><span class="s">downsample</span><span class="sh">"</span> <span class="ow">and</span> <span class="n">module3</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                    <span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">fuse_modules_qat</span><span class="p">(</span><span class="n">module3</span><span class="p">,</span> 
																													<span class="p">[[</span><span class="sh">"</span><span class="s">0</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">1</span><span class="sh">"</span><span class="p">]],</span> 
																													<span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># To emulate the quantization process,
# insert torch.ao.quantization.QuantStub() layer 
#	and torch.ao.quantization.DeQuantStub() layer 
# at the beginning and end of forward() respectively.
</span></code></pre></div></div> <ul> <li>Step 2. Prepare :</li> </ul> <p>Insert observer and prepare the quantization process</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 'x86' or 'fbgemm' for server inference
# 'qnnpack' for mobile inference
</span><span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">quantized</span><span class="p">.</span><span class="n">engine</span> <span class="o">=</span> <span class="sh">'</span><span class="s">fbgemm</span><span class="sh">'</span>
<span class="n">model</span><span class="p">.</span><span class="n">qconfig</span> <span class="o">=</span> 
	<span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">get_default_qat_qconfig</span><span class="p">(</span><span class="sh">'</span><span class="s">fbgemm</span><span class="sh">'</span><span class="p">)</span>
<span class="c1"># You can use different observers for quantized_model.qconfig
</span><span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">prepare_qat</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">(),</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <ul> <li>Step 3. Calibration + Fine-Tuning (Training) :</li> </ul> <p>First, enable the observers and fake-quantization modules.</p> <p>The observers and fake-quantization modules will compute the scale factor and zero-point integer during calibration.</p> <p>Second, fine-tune the model until loss converges (training).</p> <p>Here, note that you should finish the calibration around the beginning of epochs. So, disable the observers and freeze the BatchNorm stats around the beginning of epochs (epoch 4, 3 in my case) so that we can focus on updating the weights of the model with the observer’s fixed quantization parameters.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">quantized_model</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">enable_observer</span><span class="p">)</span>
<span class="n">quantized_model</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">enable_fake_quant</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
	<span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="nf">train_one_epoch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
	<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">train loss : {.8f} acc : {:.5f}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">acc</span><span class="p">))</span>
	
	<span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="nf">test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
	<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">test loss : {.8f} acc : {:.5f}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">acc</span><span class="p">))</span>
	
	<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">inference_mode</span><span class="p">():</span>
		<span class="k">if</span> <span class="n">epoch</span> <span class="o">&gt;=</span> <span class="mi">4</span><span class="p">:</span>
			<span class="n">quantized_model</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">disable_observer</span><span class="p">)</span>
		<span class="k">if</span> <span class="n">epoch</span> <span class="o">&gt;=</span> <span class="mi">3</span><span class="p">:</span>
			<span class="n">quantized_model</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">intrinsic</span><span class="p">.</span><span class="n">qat</span><span class="p">.</span><span class="n">freeze_bn_stats</span><span class="p">)</span>
</code></pre></div></div> <ul> <li>Step 4. Convert :</li> </ul> <p>Convert from FP32 to reduced precision based on the calibrated quantization parameters</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">convert</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div> <h2 id="quantization-in-onnx">Quantization in ONNX</h2> <h3 id="definition-of-onnx">Definition of ONNX</h3> <p>ONNX(Open Neural Network Exchange) is an open standard to facilitate interoperability between different DNN frameworks.</p> <h3 id="necessity-of-onnx">Necessity of ONNX</h3> <p>Deploying models on specific hardware can be challenging due to the difference in hardware’s architecture and runtime environment. ONNX deals with this challenge by providing a standardized way to represent deep-learning models so that they can be easily transferred across various frameworks and easily deployed on specific hardware device. It bridges the gap between model development on framework and model deployment on hardware.</p> <ul> <li>Framework-Agnostic : ONNX allows you to train deep-learning models in one framework such as PyTorch and TensorFlow, and then export them to the ONNX format. This enables you to choose the best framework which is familiar with you and suitable for the model development. And then you can deploy the model on different target hardware without extensive modifications.</li> <li>Hardware Optimization : Different hardware have varying architectures and optimizations. Here, hardware-specific optimizations are incorporated into onnxruntime, so onnxruntime allows the model to take full advantage of the underlying hardware-specific capabilities and perform inference efficiently on the target device. In detail, ONNX Runtime works with different hardware acceleration libraries through its extensible hardware-specific Execution Providers listed below.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled7-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled7-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-02-Quantization/Untitled7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="inference-in-onnx">Inference in ONNX</h3> <p>You can perform forward-pass of an ONNX model following the code below.</p> <h3 id="code-implementation-using-onnxruntime-library">code implementation using onnxruntime library</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">onnxruntime</span>

<span class="k">def</span> <span class="nf">benchmark</span><span class="p">(</span><span class="n">model_path</span><span class="p">):</span>
    <span class="n">session</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="p">.</span><span class="nc">InferenceSession</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
    <span class="n">input_name</span> <span class="o">=</span> <span class="n">session</span><span class="p">.</span><span class="nf">get_inputs</span><span class="p">()[</span><span class="mi">0</span><span class="p">].</span><span class="n">name</span>

    <span class="n">total</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">runs</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span> <span class="c1"># B, C, H, W
</span>    <span class="c1"># Warming up
</span>    <span class="n">_</span> <span class="o">=</span> <span class="n">session</span><span class="p">.</span><span class="nf">run</span><span class="p">([],</span> <span class="p">{</span><span class="n">input_name</span><span class="p">:</span> <span class="n">input_data</span><span class="p">})</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">runs</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">session</span><span class="p">.</span><span class="nf">run</span><span class="p">([],</span> <span class="p">{</span><span class="n">input_name</span><span class="p">:</span> <span class="n">input_data</span><span class="p">})</span>
        <span class="n">end</span> <span class="o">=</span> <span class="p">(</span><span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">end</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">end</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">ms</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">total</span> <span class="o">/=</span> <span class="n">runs</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Avg: </span><span class="si">{</span><span class="n">total</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">ms</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="quantization-in-onnx-1">Quantization in ONNX</h3> <p>Quantization in ONNX Runtime refers to INT8 or UINT8 linear quantization of an ONNX model. There are two ways to represent quantized ONNX models.</p> <ul> <li> <p>Operator-oriented (QOperator) :</p> <p>All the quantized operators have their own ONNX definitions like QLinearConv and MatMulInteger.</p> </li> <li> <p>Tensor-oriented (QDQ) :</p> <p>This format inserts &lt;tensor - QuantizeLinear - DequantizeLinear&gt; between the original operators to simulate the quantization and dequantization process. In the case of activations, QuantizeLinear layer is used for quantizing and DequantizeLinear layer is used for dequantizing respectively. In the case of weight, only DequantizeLinear layer is inserted.</p> <p>In Dynamic Quantization, a ComputeQuantizationParameters functions proto is inserted to calculate quantization parameters on the fly. In Static Quantization, QuantizeLinear and DeQuantizeLinear operators carry the quantization parameters (scale factor and zero-point integer) of activations or weights.</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled8-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled8-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-02-Quantization/Untitled8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Visualization of ONNX model via netron.app Left : QOperator format of a quantized ONNX model Right : QDQ format of a quantized ONNX model </div> <ul> <li>For more details, refer to the link below.</li> </ul> <p><a href="https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html">https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html</a></p> <h3 id="code-implementation-using-onnxruntime-library-cpu-qdq-format">code implementation using onnxruntime library (CPU, QDQ format)</h3> <p>I will introduce how to quantize an ONNX model using CPU hardware and QDQ format.</p> <p>It follows the link below.</p> <p><a href="https://github.com/microsoft/onnxruntime-inference-examples/tree/main/quantization/image_classification/cpu">https://github.com/microsoft/onnxruntime-inference-examples/tree/main/quantization/image_classification/cpu</a></p> <ul> <li>Step 1. Pre-process</li> </ul> <p>Pre-processing is to prepare ONNX model for better quantization. It consists of three optional steps : Symbolic shape inference, Model optimization, and ONNX shape inference.</p> <p>Both Symbolic shape inference and ONNX shape inference figure out the tensor shapes. Model optimization performs module fusion.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">onnxruntime</span>
<span class="kn">from</span> <span class="n">onnxruntime.quantization.shape_inference</span> <span class="kn">import</span> <span class="n">quant_pre_process</span>

<span class="c1"># whether you skip Model optimization since model size is greater than 2GB
</span><span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--skip_optimization</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># whether you skip ONNX shape inference
</span><span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--skip_onnx_shape</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># whether you skip Symbolic shape inference
</span><span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--skip_symbolic_shape</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--auto_merge</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--int_max</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">31</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--guess_output_rank</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--verbose</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--save_as_external_data</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--all_tensors_to_one_file</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--external_data_location</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--external_data_size_threshold</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>

<span class="nf">quant_pre_process</span><span class="p">(</span>
        <span class="n">input_onnxmodel_path</span><span class="p">,</span>
        <span class="n">output_onnxmodel_path</span><span class="p">,</span>
        <span class="n">args</span><span class="p">.</span><span class="n">skip_optimization</span><span class="p">,</span>
        <span class="n">args</span><span class="p">.</span><span class="n">skip_onnx_shape</span><span class="p">,</span>
        <span class="n">args</span><span class="p">.</span><span class="n">skip_symbolic_shape</span><span class="p">,</span>
        <span class="n">args</span><span class="p">.</span><span class="n">auto_merge</span><span class="p">,</span>
        <span class="n">args</span><span class="p">.</span><span class="n">int_max</span><span class="p">,</span>
        <span class="n">args</span><span class="p">.</span><span class="n">guess_output_rank</span><span class="p">,</span>
        <span class="n">args</span><span class="p">.</span><span class="n">verbose</span><span class="p">,</span>
        <span class="n">args</span><span class="p">.</span><span class="n">save_as_external_data</span><span class="p">,</span>
        <span class="n">args</span><span class="p">.</span><span class="n">all_tensors_to_one_file</span><span class="p">,</span>
        <span class="n">args</span><span class="p">.</span><span class="n">external_data_location</span><span class="p">,</span>
        <span class="n">args</span><span class="p">.</span><span class="n">external_data_size_threshold</span>
<span class="p">)</span>
</code></pre></div></div> <ul> <li>Step 2. Quantize</li> </ul> <p>Model optimization may also be performed during quantization by default for historical reasons. However, it’s highly recommended to perform model optimization during pre-process(Step 1) and turn off model optimization during quantization(Step 2) for the ease of debugging.</p> <p>(i) Dynamic Quantization :</p> <p>It calculates quantization parameters for activations dynamically</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">onnxruntime</span>
<span class="kn">from</span> <span class="n">onnxruntime.quantization</span> <span class="kn">import</span> <span class="n">QuantFormat</span><span class="p">,</span> <span class="n">QuantType</span><span class="p">,</span> <span class="n">quantize_dynamic</span>

<span class="c1"># Tensor-oriented QDQ format of quantized ONNX model
</span><span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--quant_format</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">QuantFormat</span><span class="p">.</span><span class="n">QDQ</span><span class="p">)</span>

<span class="c1"># You can use per-channel quantization if accuracy drop is significant
</span><span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--per_channel</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># If accuracy drop is significant, it may be caused by saturation (clamped)
# Then you can try reduce_range
# reduce_range == True : quantize weights with 7-bits. 
#                        It may improve accuracy for non-VNNI machine
</span><span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--reduce_range</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># nodes_to_exclude : specify nodes which you will freeze and will not quantize
</span>
<span class="nf">quantize_dynamic</span><span class="p">(</span>
        <span class="n">input_onnxmodel_path</span><span class="p">,</span>
        <span class="n">output_onnxmodel_path</span><span class="p">,</span>
        <span class="n">per_channel</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">per_channel</span><span class="p">,</span>
        <span class="n">reduce_range</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">reduce_range</span><span class="p">,</span>
        <span class="n">weight_type</span><span class="o">=</span><span class="n">QuantType</span><span class="p">.</span><span class="n">QInt8</span><span class="p">,</span> 
        <span class="n">nodes_to_exclude</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">/layer4/layer4.0/conv1/Conv</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/layer4/layer4.0/conv2/Conv</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/layer4/layer4.0/downsample/downsample.0/Conv</span><span class="sh">'</span><span class="p">,</span>  
                          <span class="sh">'</span><span class="s">/layer4/layer4.1/conv1/Conv</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/layer4/layer4.1/conv2/Conv</span><span class="sh">'</span><span class="p">,</span>
                          <span class="sh">'</span><span class="s">/fc/Gemm</span><span class="sh">'</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div></div> <p>(ii) Static Quantization :</p> <p>It calculates quantization parameters using calibration input data before inference.</p> <p>ONNX Runtime quantization tool supports three calibration methods: MinMax, Entropy and Percentile.</p> <p>ONNX Runtime quantization on CPU can run U8U8, U8S8, and S8S8. Here, U8S8 means that activation and weight are quantized to UINT8(unsigned) and INT8(signed) respectively. And S8S8 with QDQ is the default setting since it may have balance between performance and accuracy.</p> <p>Note that S8S8 with QOperator will be slow on x86-64 CPUs and should be avoided in general. Also, note that ONNX Runtime quantization on GPU only supports S8S8.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">onnxruntime</span>
<span class="kn">from</span> <span class="n">onnxruntime.quantization</span> <span class="kn">import</span> <span class="n">QuantFormat</span><span class="p">,</span> <span class="n">QuantType</span><span class="p">,</span> <span class="n">quantize_static</span>

<span class="c1"># Tensor-oriented QDQ format of quantized ONNX model
</span><span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--quant_format</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">QuantFormat</span><span class="p">.</span><span class="n">QDQ</span><span class="p">)</span>

<span class="c1"># You can use per-channel quantization if accuracy drop is significant
</span><span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--per_channel</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">True</span>

<span class="c1"># If accuracy drop is significant, it may be caused by saturation (clamped)
# Then you can try reduce_range or U8U8
# reduce_range == True : quantize weights with 7-bits. 
#                        It may improve accuracy for non-VNNI machine
</span><span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--reduce_range</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># nodes_to_exclude : specify nodes which you will freeze and will not quantize
</span>
<span class="c1"># Create a set of inputs called calibration data
</span><span class="n">dr</span> <span class="o">=</span> <span class="n">resnet18_data_reader</span><span class="p">.</span><span class="nc">ResNet18DataReader</span><span class="p">(</span>
        <span class="n">calibration_dir_path</span><span class="p">,</span> <span class="n">input_onnxmodel_path</span>
    <span class="p">)</span>

<span class="nf">quantize_static</span><span class="p">(</span>
        <span class="n">input_onnxmodel_path</span><span class="p">,</span>
        <span class="n">output_onnxmodel_path</span><span class="p">,</span>
        <span class="n">dr</span><span class="p">,</span>
        <span class="n">quant_format</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">quant_format</span><span class="p">,</span>
        <span class="n">per_channel</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">per_channel</span><span class="p">,</span>
        <span class="n">reduce_range</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">reduce_range</span><span class="p">,</span>
        <span class="n">activation_type</span><span class="o">=</span><span class="n">QuantType</span><span class="p">.</span><span class="n">QUInt8</span><span class="p">,</span>
        <span class="n">weight_type</span><span class="o">=</span><span class="n">QuantType</span><span class="p">.</span><span class="n">Int8</span><span class="p">,</span> 
        <span class="n">nodes_to_exclude</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">/layer4/layer4.0/conv1/Conv</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/layer4/layer4.0/act1/Relu</span><span class="sh">'</span><span class="p">,</span>
                          <span class="sh">'</span><span class="s">/layer4/layer4.0/conv2/Conv</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/layer4/layer4.0/downsample/downsample.0/Conv</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/layer4/layer4.0/Add</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/layer4/layer4.0/act2/Relu</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/layer4/layer4.1/conv1/Conv</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/layer4/layer4.1/act1/Relu</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/layer4/layer4.1/conv2/Conv</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/layer4/layer4.1/Add</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/layer4/layer4.1/act2/Relu</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/global_pool/pool/GlobalAveragePool</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/global_pool/flatten/Flatten</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">/fc/Gemm</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">extra_options</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">CalibMovingAverage</span><span class="sh">'</span><span class="p">:</span><span class="bp">True</span><span class="p">,</span> <span class="sh">'</span><span class="s">SmoothQuant</span><span class="sh">'</span><span class="p">:</span><span class="bp">True</span><span class="p">}</span>
<span class="p">)</span>
</code></pre></div></div> <ul> <li>Step 3. Debugging</li> </ul> <p>Quantization is a lossy compression, so it may drop a model’s accuracy. To improve the problematic parts, you can compare the weights and activations tensors between the original computation graph and the quantized comptuation graph. By debugging, you can identify where they differ most and avoid quantizing these nodes using “nodes_to_exclude” argument in Step 2. Quantize.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">onnxruntime.quantization.qdq_loss_debug</span> <span class="kn">import</span> <span class="p">(</span>
	<span class="n">collect_activations</span><span class="p">,</span> <span class="n">compute_activation_error</span><span class="p">,</span> <span class="n">compute_weight_error</span><span class="p">,</span>
	<span class="n">create_activation_matching</span><span class="p">,</span> <span class="n">create_weight_matching</span><span class="p">,</span>
	<span class="n">modify_model_output_intermediate_tensors</span><span class="p">)</span>

<span class="c1"># Comparing weights of float model vs qdq model
</span><span class="n">matched_weights</span> <span class="o">=</span> <span class="nf">create_weight_matching</span><span class="p">(</span><span class="n">float_model_path</span><span class="p">,</span> <span class="n">qdq_model_path</span><span class="p">)</span>
<span class="n">weights_error</span> <span class="o">=</span> <span class="nf">compute_weight_error</span><span class="p">(</span><span class="n">matched_weights</span><span class="p">)</span>
<span class="k">for</span> <span class="n">weight_name</span><span class="p">,</span> <span class="n">err</span> <span class="ow">in</span> <span class="n">weights_error</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Cross model error of </span><span class="sh">'</span><span class="si">{</span><span class="n">weight_name</span><span class="si">}</span><span class="sh">'</span><span class="s">: </span><span class="si">{</span><span class="n">err</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Augmenting models to save intermediate activations
</span><span class="nf">modify_model_output_intermediate_tensors</span><span class="p">(</span><span class="n">float_model_path</span><span class="p">,</span> <span class="n">aug_float_model_path</span><span class="p">)</span>
<span class="nf">modify_model_output_intermediate_tensors</span><span class="p">(</span><span class="n">qdq_model_path</span><span class="p">,</span> <span class="n">aug_qdq_model_path</span><span class="p">)</span>

<span class="c1"># Running the augmented floating point model to collect activations
</span><span class="n">dr</span> <span class="o">=</span> <span class="n">resnet18_data_reader</span><span class="p">.</span><span class="nc">ResNet18DataReader</span><span class="p">(</span>
        <span class="n">calibration_dir_path</span><span class="p">,</span> <span class="n">float_model_path</span>
<span class="p">)</span>
<span class="n">float_activations</span> <span class="o">=</span> <span class="nf">collect_activations</span><span class="p">(</span><span class="n">aug_float_model_path</span><span class="p">,</span> <span class="n">dr</span><span class="p">)</span>

<span class="c1"># Running the augmented qdq model to collect activations
</span><span class="n">dr</span><span class="p">.</span><span class="nf">rewind</span><span class="p">()</span>
<span class="n">qdq_activations</span> <span class="o">=</span> <span class="nf">collect_activations</span><span class="p">(</span><span class="n">aug_qdq_model_path</span><span class="p">,</span> <span class="n">dr</span><span class="p">)</span>

<span class="c1"># Comparing activations of float model vs qdq model
</span><span class="n">act_matching</span> <span class="o">=</span> <span class="nf">create_activation_matching</span><span class="p">(</span><span class="n">qdq_activations</span><span class="p">,</span> <span class="n">float_activations</span><span class="p">)</span>
<span class="n">act_error</span> <span class="o">=</span> <span class="nf">compute_activation_error</span><span class="p">(</span><span class="n">act_matching</span><span class="p">)</span>
<span class="k">for</span> <span class="n">act_name</span><span class="p">,</span> <span class="n">err</span> <span class="ow">in</span> <span class="n">act_error</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Cross model error of </span><span class="sh">'</span><span class="si">{</span><span class="n">act_name</span><span class="si">}</span><span class="sh">'</span><span class="s">: </span><span class="si">{</span><span class="n">err</span><span class="p">[</span><span class="sh">'</span><span class="s">xmodel_err</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">QDQ error of </span><span class="sh">'</span><span class="si">{</span><span class="n">act_name</span><span class="si">}</span><span class="sh">'</span><span class="s">: </span><span class="si">{</span><span class="n">err</span><span class="p">[</span><span class="sh">'</span><span class="s">qdq_err</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <ul> <li>Summary :</li> </ul> <p>Assume that you implemented Step 1. into <a href="http://preprocess.py">preprocess.py</a> and Step 2. into <a href="http://quantize.py">quantize.py</a> and Step 3. into <a href="http://debug.py">debug.py</a> with proper I/O. Then you can run them in terminal as shown below.</p> <p>(i) If you do not optimize ONNX model during quantization (recommended)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 1. Pre-process
</span><span class="n">python</span> <span class="n">preprocess</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="nb">input</span> <span class="n">original</span><span class="p">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">output</span> <span class="n">preprocess</span><span class="p">.</span><span class="n">onnx</span>
<span class="c1"># Step 2. Quantize without optimization
</span><span class="n">python</span> <span class="n">quantize</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="nb">input</span> <span class="n">preprocess</span><span class="p">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">output</span> <span class="n">quantized</span><span class="p">.</span><span class="n">onnx</span>
<span class="c1"># Step 3. Debug
</span><span class="n">python</span> <span class="n">debug</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">float_model</span> <span class="n">preprocess</span><span class="p">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">qdq_model</span> <span class="n">quantized</span><span class="p">.</span><span class="n">onnx</span>
</code></pre></div></div> <p>(ii) If you optimize ONNX model during quantization (default)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 1. Pre-process
</span><span class="n">python</span> <span class="n">preprocess</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="nb">input</span> <span class="n">original</span><span class="p">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">output</span> <span class="n">preprocess</span><span class="p">.</span><span class="n">onnx</span>
<span class="c1"># Step 2. Quantize with optimization
</span><span class="n">python</span> <span class="n">quantize</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="nb">input</span> <span class="n">preprocess</span><span class="p">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">output</span> <span class="n">quantized_2</span><span class="p">.</span><span class="n">onnx</span>
<span class="c1"># Step 3. Debug
</span><span class="n">python</span> <span class="n">preprocess</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="nb">input</span> <span class="n">original</span><span class="p">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">output</span> <span class="n">preprocess_2</span><span class="p">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">skip_symbolic_shape</span> <span class="bp">True</span>
<span class="n">python</span> <span class="n">debug</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">float_model</span> <span class="n">preprocess_2</span><span class="p">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">qdq_model</span> <span class="n">quantized_2</span><span class="p">.</span><span class="n">onnx</span>
</code></pre></div></div> <h1 id="result">Result</h1> <h2 id="shufflenetv2">shufflenetv2</h2> <h3 id="pytorch">Pytorch</h3> <h3 id="experiment">Experiment</h3> <ul> <li> <p>dataset :<br/> calibration : batch_size = 32, iteration = 32<br/> inference : batch_size = 128, iteration = 61</p> </li> <li> <p>hardware : cpu</p> </li> </ul> <table> <thead> <tr> <th style="text-align: left">method</th> <th style="text-align: left">model size [MB]</th> <th style="text-align: left">inference time [s]</th> <th style="text-align: left">loss</th> <th style="text-align: left">ocualr_nme [%]</th> <th style="text-align: left">pupil_nme [%]</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Original Model (partial fuse)</td> <td style="text-align: left">7.15</td> <td style="text-align: left">306.59</td> <td style="text-align: left">0.01121577</td> <td style="text-align: left">3.51807</td> <td style="text-align: left">4.87317</td> </tr> <tr> <td style="text-align: left">Original Model (all fuse)</td> <td style="text-align: left">7.01</td> <td style="text-align: left">227.92</td> <td style="text-align: left">0.01121577</td> <td style="text-align: left">3.51807</td> <td style="text-align: left">4.87317</td> </tr> <tr> <td style="text-align: left">Static PTQ calibrated with dummy input (partial fuse)</td> <td style="text-align: left">2.19</td> <td style="text-align: left">174.01</td> <td style="text-align: left">0.04872545</td> <td style="text-align: left">15.87943</td> <td style="text-align: left">21.99286</td> </tr> <tr> <td style="text-align: left">Static PTQ calibrated with our input dataset (partial fuse)</td> <td style="text-align: left">2.19</td> <td style="text-align: left">173.31</td> <td style="text-align: left">0.01834808</td> <td style="text-align: left">5.68243</td> <td style="text-align: left">7.87280</td> </tr> <tr> <td style="text-align: left">Static PTQ calibrated with our input dataset (all fuse)</td> <td style="text-align: left">2.01</td> <td style="text-align: left">161.74</td> <td style="text-align: left">0.01228440</td> <td style="text-align: left">3.83984</td> <td style="text-align: left">5.31811</td> </tr> </tbody> </table> <h3 id="result-1">Result</h3> <ol> <li>By Post-Training Static Quantization, model size of shufflenetv2 was reduced by about 3.5 times.</li> <li>By Post-Training Static Quantization, inference speed was improved by about 1.5 times.</li> <li> <p>To minimize the accuracy drop, it is better to use our dataset as input of calibration rather than dummy input.</p> <p>It’s because, during calibration, we can figure out the range of activations similarly to when the model was trained and inferred.</p> </li> <li>To minimize the accuracy drop, it is better to fuse all the layers of Conv-Bn or Conv-Bn-ReLU or Conv-ReLU because the number of layers to be quantized are reduced.</li> </ol> <h2 id="resnet18">resnet18</h2> <h3 id="pytorch-1">Pytorch</h3> <h3 id="exp-1--qat---the-effect-of-learning-rate">Exp 1. QAT - The effect of learning rate</h3> <p>The proper learning rate of fine-tuning is lr = 1e-8 which is 0.1 times the learning rate of pre-trained model 1e-7.</p> <p>I tested various values of learning rate to find the optimal value. For example, the left graph (lr = 1e-10) below shows underfitting and slow convergence due to small learning rate. However, the right graph (lr = 1e-8) below shows proper convergence.</p> <p>loss graph</p> <ul> <li>pink : test loss before quantization</li> <li>blue : training loss before quantization</li> <li>orange : test loss after quantization</li> </ul> <p>nme graph</p> <ul> <li>green : test nme before quantization</li> <li>red : training nme before quantization</li> <li>blue : test nme after quantization</li> </ul> <p>left : lr = 1e-10<br/> right : lr = 1e-8</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled9-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled9-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-02-Quantization/Untitled9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="exp-2-qat---the-effect-of-epoch-to-freeze-quantization-parameters-and-bn-stat">Exp 2. QAT - The effect of epoch to freeze quantization parameters and bn stat</h3> <p>There is no significant difference in the effect of epochs on which you will freeze the quantization parameters(observers) and BatchNorm stats.</p> <p>I tested various values of epoch to freeze the observers and bn stats, but there was no significant difference in the resulting converged value of loss or nme.</p> <p>loss graph</p> <ul> <li>pink : test loss before quantization</li> <li>blue : training loss before quantization</li> <li>orange : test loss after quantization</li> </ul> <p>nme graph</p> <ul> <li>green : test nme before quantization</li> <li>red : training nme before quantization</li> <li>blue : test nme after quantization</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled10-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled10-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-02-Quantization/Untitled10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="exp-3-qat---the-effect-of-observer-to-calibrate-the-quantization-parameters">Exp 3. QAT - The effect of observer to calibrate the quantization parameters</h3> <p>MovingAverageMinMaxObserver with U8S8 calibrated the quantization parameters(scale factor and zero-point integer) better than the default observer as shown in the graph below.</p> <p>I tested only three kinds of observers : default observer, histogram observer, and MovingAverageMinMaxObserver with U8S8. Among them, the last one performed the best, but there are also many other types of observers and other observers may perform better. The observers are listed in the link below.</p> <p><a href="https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/observer.py">https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/observer.py</a></p> <p>loss graph</p> <ul> <li>pink : test loss before quantization</li> <li>blue : training loss before quantization</li> <li>orange : test loss after quantization</li> </ul> <p>nme graph</p> <ul> <li>green : test nme before quantization</li> <li>red : training nme before quantization</li> <li>blue : test nme after quantization</li> </ul> <p>left : default observer<br/> right : MovingAverageMinMaxObserver with U8S8</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled11-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled11-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-02-Quantization/Untitled11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="experiment-1">Experiment</h3> <ul> <li> <p>dataset :<br/> calibration : batch_size = 32, iteration = 32<br/> inference : batch_size = 128, iteration = 61</p> </li> <li>hardware : cpu</li> <li>learning rate : 1e-8</li> </ul> <table> <thead> <tr> <th style="text-align: left">method</th> <th style="text-align: left">model size [MB]</th> <th style="text-align: left">inference time [s]</th> <th style="text-align: left">loss</th> <th style="text-align: left">ocualr_nme [%]</th> <th style="text-align: left">pupil_nme [%]</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Original Model</td> <td style="text-align: left">44.011</td> <td style="text-align: left">76.06</td> <td style="text-align: left">0.04312033</td> <td style="text-align: left">2.73897</td> <td style="text-align: left">3.79658</td> </tr> <tr> <td style="text-align: left">Static PTQ with default observer</td> <td style="text-align: left">11.121</td> <td style="text-align: left">22.30</td> <td style="text-align: left">0.07286325</td> <td style="text-align: left">4.54174</td> <td style="text-align: left">6.29569</td> </tr> <tr> <td style="text-align: left">QAT with default observer</td> <td style="text-align: left">11.121</td> <td style="text-align: left">22.04</td> <td style="text-align: left">0.07202724</td> <td style="text-align: left">4.48688</td> <td style="text-align: left">6.21954</td> </tr> <tr> <td style="text-align: left">QAT with MovingAverage MinMaxObserver (U8S8)</td> <td style="text-align: left">11.121</td> <td style="text-align: left">21.75</td> <td style="text-align: left">0.05595706</td> <td style="text-align: left">3.50271</td> <td style="text-align: left">4.85493</td> </tr> </tbody> </table> <h3 id="result-2">Result</h3> <ol> <li>By both Post-Training Static Quantization and Quantization-Aware Training, model size of resnet18 was reduced by about 4 times.</li> <li>By both Post-Training Static Quantization and Quantization-Aware Training, inference speed was improved by about 3.5 times.</li> <li>QAT performs a little bit better than Static PTQ due to the fine-tuning process.</li> <li>MovingAverageMinMaxObserver with U8S8 performed better than the default observer. However, there may exist another better observer since I only tested three kinds of observers.</li> </ol> <h3 id="generated-landmark">Generated landmark</h3> <p>You can see that the inference output of QAT model is better than that of Static PTQ model.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled12-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled12-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-02-Quantization/Untitled12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>When the input is a tight face, the inference output of QAT model seems nearly similar to that of original model.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled13-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled13-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-02-Quantization/Untitled13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="onnx">ONNX</h3> <h3 id="visualization-of-onnx-via-netronapp">Visualization of ONNX via netron.app</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled14-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled14-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-02-Quantization/Untitled14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled15-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled15-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-02-Quantization/Untitled15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled16-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled16-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled16-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-02-Quantization/Untitled16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Visualization of ONNX : Fused Model &amp; Quantized Model (QDQ format) </div> <h3 id="int8-quantization-of-onnx-runtime">INT8 Quantization of ONNX Runtime</h3> <p>INT8 Quantization follows the link below.</p> <p><a href="https://github.com/microsoft/onnxruntime-inference-examples/tree/main/quantization/image_classification/cpu">https://github.com/microsoft/onnxruntime-inference-examples/tree/main/quantization/image_classification/cpu</a></p> <h3 id="experiment-2">Experiment</h3> <ul> <li>dataset : inference : batch_size = 1, iteration = 7799</li> <li>hardware : cpu</li> </ul> <p>This table shows how I quantized each ONNX model adjusting combination of some parameters.</p> <table> <thead> <tr> <th style="text-align: left">IN8 Quantization</th> <th style="text-align: left">freezed layer</th> <th style="text-align: left">per channel</th> <th style="text-align: left">reduce range</th> <th style="text-align: left">CalibTensor Range Symmetric</th> <th style="text-align: left">Calib Moving Average</th> <th style="text-align: left">Smooth Quant</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Pre-processed Model</td> <td style="text-align: left"> </td> <td style="text-align: left"> </td> <td style="text-align: left"> </td> <td style="text-align: left"> </td> <td style="text-align: left"> </td> <td style="text-align: left"> </td> </tr> <tr> <td style="text-align: left">Quantized Model 1</td> <td style="text-align: left"> </td> <td style="text-align: left">T</td> <td style="text-align: left">F</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> </tr> <tr> <td style="text-align: left">Quantized Model 2</td> <td style="text-align: left"> </td> <td style="text-align: left">F</td> <td style="text-align: left">F</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> </tr> <tr> <td style="text-align: left">Quantized Model 3</td> <td style="text-align: left"> </td> <td style="text-align: left">T</td> <td style="text-align: left">F</td> <td style="text-align: left">F</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> </tr> <tr> <td style="text-align: left">Quantized Model 4</td> <td style="text-align: left"> </td> <td style="text-align: left">T</td> <td style="text-align: left">F</td> <td style="text-align: left">F</td> <td style="text-align: left">F</td> <td style="text-align: left">F</td> </tr> <tr> <td style="text-align: left">Quantized Model 5</td> <td style="text-align: left">fc layer</td> <td style="text-align: left">T</td> <td style="text-align: left">F</td> <td style="text-align: left">F</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> </tr> <tr> <td style="text-align: left">Quantized Model 6</td> <td style="text-align: left">ReLU &amp; Add</td> <td style="text-align: left">T</td> <td style="text-align: left">F</td> <td style="text-align: left">F</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> </tr> <tr> <td style="text-align: left">Quantized Model 7</td> <td style="text-align: left">layer 4 &amp; fc layer</td> <td style="text-align: left">T</td> <td style="text-align: left">F</td> <td style="text-align: left">F</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> </tr> <tr> <td style="text-align: left">Quantized Model 8</td> <td style="text-align: left">layer 4 &amp; fc layer</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> <td style="text-align: left">F</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> </tr> <tr> <td style="text-align: left">Quantized Model 9</td> <td style="text-align: left">layer 4 &amp; fc layer</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> <td style="text-align: left">F</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> </tr> <tr> <td style="text-align: left">Quantized Model 10</td> <td style="text-align: left">layer 4 &amp; fc layer</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> <td style="text-align: left">F</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> </tr> <tr> <td style="text-align: left">Quantized Model 11</td> <td style="text-align: left">layer 1 &amp; layer 4 &amp; fc layer</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> <td style="text-align: left">F</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> </tr> </tbody> </table> <table> <thead> <tr> <th style="text-align: left">INT8 Quantization</th> <th style="text-align: left">model size [MB]</th> <th style="text-align: left">inference time [s]</th> <th style="text-align: left">loss</th> <th style="text-align: left">ocualr_nme [%]</th> <th style="text-align: left">pupil_nme [%]</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Pre-processed Model</td> <td style="text-align: left">44.044</td> <td style="text-align: left">83.05</td> <td style="text-align: left">0.00867057</td> <td style="text-align: left">2.73897</td> <td style="text-align: left">3.79658</td> </tr> <tr> <td style="text-align: left">Quantized Model 1</td> <td style="text-align: left">11.177</td> <td style="text-align: left">90.04</td> <td style="text-align: left">2.04831275</td> <td style="text-align: left">594.30187</td> <td style="text-align: left">823.93989</td> </tr> <tr> <td style="text-align: left">Quantized Model 2</td> <td style="text-align: left">11.113</td> <td style="text-align: left">91.34</td> <td style="text-align: left">2.08157768</td> <td style="text-align: left">603.68516</td> <td style="text-align: left">836.92611</td> </tr> <tr> <td style="text-align: left">Quantized Model 3</td> <td style="text-align: left">11.177</td> <td style="text-align: left">93.21</td> <td style="text-align: left">0.19241423</td> <td style="text-align: left">58.71200</td> <td style="text-align: left">81.41132</td> </tr> <tr> <td style="text-align: left">Quantized Model 4</td> <td style="text-align: left">11.177</td> <td style="text-align: left">90.74</td> <td style="text-align: left">0.20964802</td> <td style="text-align: left">65.31481</td> <td style="text-align: left">90.57096</td> </tr> <tr> <td style="text-align: left">Quantized Model 5</td> <td style="text-align: left">11.377</td> <td style="text-align: left">94.70</td> <td style="text-align: left">0.19232825</td> <td style="text-align: left">58.67378</td> <td style="text-align: left">81.35814</td> </tr> <tr> <td style="text-align: left">Quantized Model 6</td> <td style="text-align: left">11.226</td> <td style="text-align: left">100.37</td> <td style="text-align: left">0.19220746</td> <td style="text-align: left">58.66837</td> <td style="text-align: left">81.35119</td> </tr> <tr> <td style="text-align: left">Quantized Model 7</td> <td style="text-align: left">35.923</td> <td style="text-align: left">90.62</td> <td style="text-align: left">0.12938738</td> <td style="text-align: left">39.69218</td> <td style="text-align: left">55.05709</td> </tr> <tr> <td style="text-align: left">Quantized Model 8</td> <td style="text-align: left">35.923</td> <td style="text-align: left">87.56</td> <td style="text-align: left">0.12887142</td> <td style="text-align: left">39.57574</td> <td style="text-align: left">54.89788</td> </tr> <tr> <td style="text-align: left">Quantized Model 9</td> <td style="text-align: left">35.923</td> <td style="text-align: left">61.08</td> <td style="text-align: left">0.12887123</td> <td style="text-align: left">39.57566</td> <td style="text-align: left">54.89777</td> </tr> <tr> <td style="text-align: left">Quantized Model 10</td> <td style="text-align: left">35.923</td> <td style="text-align: left">60.07</td> <td style="text-align: left">0.12887123</td> <td style="text-align: left">39.57566</td> <td style="text-align: left">54.89777</td> </tr> <tr> <td style="text-align: left">Quantized Model 11</td> <td style="text-align: left">36.384</td> <td style="text-align: left">65.18</td> <td style="text-align: left">0.11555405</td> <td style="text-align: left">35.54718</td> <td style="text-align: left">49.31533</td> </tr> </tbody> </table> <h3 id="result-3">Result</h3> <ol> <li>By ONNX Quantization, model size of resnet18 was reduced by about 4 times for Quantized Model 1~6. (However, there is significant accuracy drop.)</li> <li> <p>By ONNX Quantization, there is no notable improvement on inference time.</p> <p>The performance improvement depends on your model and hardware. The performance gain from quantization has two aspects: compute and memory. Old hardware has none or few of the instructions needed to perform efficient inference in int8. And quantization has overhead (from quantizing and dequantizing), so it is not rare to get worse performance on some devices.</p> <p>x86-64 with VNNI, GPU with Tensor Core int8 support, and ARM with dot-product instructions can get better performance in general. But, there may not exist a notable improvement on inference time.</p> </li> <li> <p>By ONNX Quantization, it is not rare to see significant accuracy drop. Then you can try U8U8.</p> <p>When to try U8U8 data type? :</p> <p>On x86-64 machines with AVX2 and AVX512 extensions, ONNX Runtime uses the VPMADDUBSW instruction for U8S8 for performance. However, this instruction might suffer from saturation issues: it can happen that the output does not fit into a 8-bit, 8-bit integer and has to be clamped (saturated) to fit. Generally, this is not a big issue for the final result. However, if you encounter a significant accuracy drop, it may be caused by saturation. In this case, you can try U8U8 with reduce_range.</p> </li> <li> <p>By ONNX Quantization, it is not rare to see significant accuracy drop. Then you can try reduce_range = True or per_channel = True.</p> <p>When to use reduce_range and per_channel quantization? :</p> <p>Reduce-range will quantize the weights to 7-bits. It is designed for the U8S8 format on AVX2 and AVX512 (non-VNNI) machines to mitigate saturation issues. This is not needed on machines supporting VNNI.</p> <p>Per-channel quantization can improve the accuracy for models whose weight ranges are large. You can try it if the accuracy drop is large. In addition, on AVX2 and AVX512 machines, you will generally need to enable reduce_range as well if per_channel is enabled.</p> </li> <li>By ONNX Quantization, it is not rare to see significant accuracy drop. To improve the problematic parts, you can compare the weights and activations tensors between the original computation graph and the quantized comptuation graph. By debugging, you can identify where they differ most and avoid quantizing these nodes.</li> <li> <p>I only tried ONNX Runtime quantization on CPU, but you can also try quantization on GPU and you can use many other Execution Providers.</p> <p>The ONNX Runtime material below suggests that you can try quantization on GPU if there is a significant accuracy drop.</p> <p><a href="https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html#quantization-on-gpu">https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html#quantization-on-gpu</a></p> <p>If you are interested in other Execution Providers, refer to the link below.</p> <p><a href="https://onnxruntime.ai/docs/execution-providers/">https://onnxruntime.ai/docs/execution-providers/</a></p> </li> <li>If the Post-Training Quantization method cannot meet accuracy goal, you can try using QAT (Quantization-Aware Training) to retrain the model. However, ONNX Runtime does not provide retraining at this time, so you should re-train your models with the original framework (in my case, Pytorch) and convert them back to ONNX.</li> </ol> <h3 id="generated-landmark-1">Generated landmark</h3> <p>You can see that accuracy drop of ONNX quantization is significant.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled17-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled17-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled17-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-02-Quantization/Untitled17.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="debugging">Debugging</h3> <p>Briefly, I will explain how to debug the weights of a model. To learn how to debug the activations of a model, refer to the link below.</p> <p><a href="https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/quantization/qdq_loss_debug.py">https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/quantization/qdq_loss_debug.py</a></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">create_weight_matching</span><span class="p">(</span><span class="sh">"</span><span class="s">path/float.onnx</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">path/qdq.onnx</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># This function dequantizes a quantized weight following
#	the linear dequantization equation x = s * (x_q - z)
# Then it returns dictA = {"onnx::Conv_193" : 
#					 									 {"float": w1, "dequantized" : w2},
#											 		 "onnx::Conv_194" : 
#														 {"float": w1, "dequantized" : w2}}
# Here, w1 means the fp32 weight of original model
# and w2 means the dequantized weight of quantized model
</span>
<span class="nf">compute_weight_error</span><span class="p">(</span><span class="n">dictA</span><span class="p">)</span>
<span class="c1"># This function computes SQNR = P_signal / P_noise 
#                             = 20log(|w1|/|w1-w2|)
# Then it returns dictB = {"onnx::Conv_193" : SQNR1,
#                          "onnx::Conv_194" : SQNR2}
# If the SQNR value is larger, then it means the error is smaller.
</span></code></pre></div></div> <p>By using the above functions, you can figure out which node has the significant quantization error and avoid quantizing those nodes.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled18-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled18-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled18-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-02-Quantization/Untitled18.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>SQNR of each node by debugging</p> <p>In my case, SQNR value for weight matrix of each node was in the range of [23, 37], which means that the value of \(\frac{\vert w1 \vert}{\vert w1-w2 \vert}\) is in the range of [14, 70].</p> <p>Also, SQNR value for bias vector of each node was in the range of [46, 71], which means that the value of \(\frac{\vert w1 \vert}{\vert w1-w2 \vert}\) is in the range of [199, 3548].</p> <h3 id="fp16-conversion-of-onnx-runtime">FP16 Conversion of ONNX Runtime</h3> <p>FP16 Conversion follows the link below.</p> <p><a href="https://onnxruntime.ai/docs/performance/model-optimizations/float16.html">https://onnxruntime.ai/docs/performance/model-optimizations/float16.html</a></p> <p>ONNX Runtime INT8 Quantization was not successful, so I also tried FP16 Conversion of an ONNX model.</p> <h3 id="experiment-3">Experiment</h3> <ul> <li>hardware : cpu</li> </ul> <table> <thead> <tr> <th style="text-align: left">FP16 Conversion</th> <th style="text-align: left">model size [MB]</th> <th style="text-align: left">inference time [s]</th> <th style="text-align: left">loss</th> <th style="text-align: left">ocualr_nme [%]</th> <th style="text-align: left">pupil_nme [%]</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Original Model</td> <td style="text-align: left">43.922</td> <td style="text-align: left">207.34</td> <td style="text-align: left">0.00867057</td> <td style="text-align: left">2.73897</td> <td style="text-align: left">3.79658</td> </tr> <tr> <td style="text-align: left">Converted Model</td> <td style="text-align: left">21.969</td> <td style="text-align: left">190.82</td> <td style="text-align: left">0.00867078</td> <td style="text-align: left">2.73904</td> <td style="text-align: left">3.79668</td> </tr> </tbody> </table> <h3 id="result-4">Result</h3> <ol> <li>By ONNX Runtime FP16 Conversion, the model size of resnet18 was reduced by about 2 times.</li> <li>By ONNX Runtime FP16 Conversion on CPU, there was no notable improvement on inference time since CPU cannot utilize FP16 speedup. However, the inference speed will be improved if you use GPUs.</li> <li>Accuracy drop almost does not occur after FP16 Conversion.</li> </ol> <h3 id="generated-landmark-2">Generated landmark</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled19-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled19-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled19-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-02-Quantization/Untitled19.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h1 id="conclusion">Conclusion</h1> <p>In conclusion, I provided a comprehensive overview of the diverse quantization techniques available in Pytorch and ONNX. We have mainly discussed three techniques : Post-Training Dynamic Quantization, Post-Training Static Quantization, and Quantization-Aware Training.</p> <p>Remember that quantization is required to efficiently deploy your model on resouce-constrained devices even if there is a trade-off of accuracy drop. Therefore, quantization has to be done targeting your specific hardware and there are actually various quantization details depending on the hardware. Starting with studying quantization in Pytorch and ONNX, I encourage you to dive deeper into quantization that fits your hardware.</p> <p>Moreover, as the field of deep learning continues to expand, quantization will persist as a critical factor for the widespread deployment of neural network models on various hardware platforms and real-world applications. Therefore, I encourage you to continuously have interest in performance improvement including quantization so that you can successfully bridge the gap between powerful AI models and resource-constrained real-world environments.</p> <h1 id="reference">Reference</h1> <h2 id="pytorch-2">Pytorch</h2> <h3 id="torchaoquantization">torch.ao.quantization</h3> <p>[1] Quantization :</p> <p><a href="https://github.com/Lornatang/PyTorch/blob/1a998bf1baded12869c466f8dcfb7b6130f57d02/docs/source/quantization.rst#L583">https://github.com/Lornatang/PyTorch/blob/1a998bf1baded12869c466f8dcfb7b6130f57d02/docs/source/quantization.rst#L583</a></p> <p>[2] Quantization :</p> <p><a href="https://pytorch.org/docs/stable/quantization.html">https://pytorch.org/docs/stable/quantization.html</a></p> <p>[3] Principle of Quantization :</p> <p><a href="https://pytorch.org/blog/quantization-in-practice/">https://pytorch.org/blog/quantization-in-practice/</a></p> <p>[4] PTQ, QAT :</p> <p><a href="https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html</a></p> <p>[5] Post-Training Static Quantization of resnet18 : <a href="https://github.com/Sanjana7395/static_quantization">https://github.com/Sanjana7395/static_quantization</a></p> <p>[6] Quantization-Aware Training of resnet18 :</p> <p><a href="https://gaussian37.github.io/dl-pytorch-quantization/">https://gaussian37.github.io/dl-pytorch-quantization/</a></p> <p>[7] Freeze observer, bn stat in QAT :</p> <p><a href="https://github.com/pytorch/vision/blob/main/references/classification/train_quantization.py">https://github.com/pytorch/vision/blob/main/references/classification/train_quantization.py</a></p> <p>[8] Observers :</p> <p><a href="https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/observer.py">https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/observer.py</a></p> <p>[9] fused modules :</p> <p><a href="https://pytorch.org/docs/stable/_modules/torch/ao/nn/intrinsic/modules/fused.html#ConvReLU2d">https://pytorch.org/docs/stable/_modules/torch/ao/nn/intrinsic/modules/fused.html#ConvReLU2d</a></p> <p>[10] Quantization - debug :</p> <p><a href="https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#torch-ao-ns-numeric-suite">https://pytorch.org/docs/stable/torch.ao.ns._numeric_suite.html#torch-ao-ns-numeric-suite</a></p> <p>[11] Quantized Transfer Learning : <a href="https://tutorials.pytorch.kr/intermediate/quantized_transfer_learning_tutorial.html#part-1-training-a-custom-classifier-based-on-a-quantized-feature-extractor">https://tutorials.pytorch.kr/intermediate/quantized_transfer_learning_tutorial.html#part-1-training-a-custom-classifier-based-on-a-quantized-feature-extractor</a></p> <h3 id="pytorch_quantization">pytorch_quantization</h3> <p>[12] Quantization :</p> <p><a href="https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization">https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization</a></p> <p>[13] PTQ, QAT :</p> <p><a href="https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/userguide.html">https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/userguide.html</a></p> <p>[14] quant_modules :</p> <p><a href="https://www.ccoderun.ca/programming/doxygen/tensorrt/namespacepytorch__quantization_1_1quant__modules.html">https://www.ccoderun.ca/programming/doxygen/tensorrt/namespacepytorch__quantization_1_1quant__modules.html</a></p> <p>[15] quant_conv :</p> <p><a href="https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/_modules/pytorch_quantization/nn/modules/quant_conv.html">https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/_modules/pytorch_quantization/nn/modules/quant_conv.html</a></p> <p>[16] Post-Training Static Quantization of resnet50 :</p> <p><a href="https://github.com/NVIDIA/TensorRT/blob/master/tools/pytorch-quantization/examples/torchvision/classification_flow.py">https://github.com/NVIDIA/TensorRT/blob/master/tools/pytorch-quantization/examples/torchvision/classification_flow.py</a></p> <h2 id="onnx-1">ONNX</h2> <p>[17] netron : <a href="https://netron.app/">https://netron.app/</a></p> <p>[18] ONNX :</p> <p><a href="https://pytorch.org/docs/stable/onnx.html">https://pytorch.org/docs/stable/onnx.html</a></p> <p><a href="https://gaussian37.github.io/dl-pytorch-deploy/#onnxruntime%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EB%AA%A8%EB%8D%B8-%EC%82%AC%EC%9A%A9-1">https://gaussian37.github.io/dl-pytorch-deploy/#onnxruntime을-이용한-모델-사용-1</a></p> <p>[19] inference in ONNX : <a href="https://seokhyun2.tistory.com/83">https://seokhyun2.tistory.com/83</a></p> <p>[20] ONNX Quantization :</p> <p><a href="https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html">https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html</a></p> <p>[21] ONNX Quantization on CPU :</p> <p><a href="https://github.com/microsoft/onnxruntime-inference-examples/tree/main/quantization/image_classification/cpu">https://github.com/microsoft/onnxruntime-inference-examples/tree/main/quantization/image_classification/cpu</a></p> <p>[22] ONNX Quantization on CPU - quantize :</p> <p><a href="https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/quantization/quantize.py">https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/quantization/quantize.py</a></p> <p>[23] ONNX Quantization on CPU - debug :</p> <p><a href="https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/quantization/qdq_loss_debug.py#L361">https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/quantization/qdq_loss_debug.py#L361</a></p> <p>[24] ONNX Quantization by ONNX Optimizer :</p> <p><a href="https://github.com/onnx/optimizer">https://github.com/onnx/optimizer</a></p> <p>[25] ONNX Quantization by neural-compressor :</p> <p><a href="https://github.com/intel/neural-compressor/blob/master/examples/onnxrt/image_recognition/onnx_model_zoo/resnet50/quantization/ptq_static/main.py">https://github.com/intel/neural-compressor/blob/master/examples/onnxrt/image_recognition/onnx_model_zoo/resnet50/quantization/ptq_static/main.py</a></p> <p>[26] TensorRT Execution Provider :</p> <p><a href="https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html">https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html</a></p> <p><a href="https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/observer.py">https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/observer.py</a></p> <p>[27] Huggingface Optimum : Export to ONNX, Quantization, Graph Optimization :</p> <p><a href="https://blog.naver.com/wooy0ng/223007164371">https://blog.naver.com/wooy0ng/223007164371</a></p> <p>[28] Export to ONNX (Brevitas) :</p> <p><a href="https://xilinx.github.io/brevitas/getting_started">https://xilinx.github.io/brevitas/getting_started</a></p> <p>[29] Export to ONNX (from Pytorch to ONNX) :</p> <p><a href="https://tutorials.pytorch.kr/advanced/super_resolution_with_onnxruntime.html">https://tutorials.pytorch.kr/advanced/super_resolution_with_onnxruntime.html</a></p> <p><a href="https://yunmorning.tistory.com/17">https://yunmorning.tistory.com/17</a></p> <p><a href="https://mmclassification.readthedocs.io/en/latest/tools/pytorch2onnx.html">https://mmclassification.readthedocs.io/en/latest/tools/pytorch2onnx.html</a></p> <p><a href="https://discuss.pytorch.org/t/onnx-export-of-quantized-model/76884/33">https://discuss.pytorch.org/t/onnx-export-of-quantized-model/76884/33</a></p> <p><a href="https://discuss.pytorch.org/t/onnx-export-of-quantized-model/76884/26?page=2">https://discuss.pytorch.org/t/onnx-export-of-quantized-model/76884/26?page=2</a></p> <p>[30] Conversion to TensorRT (from ONNX to TensorRT) :</p> <p><a href="https://mmclassification.readthedocs.io/en/latest/tools/onnx2tensorrt.html">https://mmclassification.readthedocs.io/en/latest/tools/onnx2tensorrt.html</a></p> <h2 id="other-references">Other references</h2> <p>[31] Quantization :</p> <p><a href="https://gaussian37.github.io/dl-concept-quantization/">https://gaussian37.github.io/dl-concept-quantization/</a></p> <p><a href="https://velog.io/@jooh95/%EB%94%A5%EB%9F%AC%EB%8B%9D-Quantization%EC%96%91%EC%9E%90%ED%99%94-%EC%A0%95%EB%A6%AC">https://velog.io/@jooh95/딥러닝-Quantization양자화-정리</a></p> <p><a href="https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Enhanced-low-precision-pipeline-to-accelerate-inference-with/post/1335626">https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Enhanced-low-precision-pipeline-to-accelerate-inference-with/post/1335626</a></p> <p>[32] timm resnet18 :</p> <p><a href="https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/resnet.py">https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/resnet.py</a></p> <p>[33] Depth-wise Separable Convolution : <a href="https://coding-yoon.tistory.com/122">https://coding-yoon.tistory.com/122</a></p> <p>[34] TensorRT :</p> <p><a href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#fit">https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#fit</a></p> <p>[35] Attention Round : <a href="https://arxiv.org/abs/2207.03088">https://arxiv.org/abs/2207.03088</a></p>]]></content><author><name></name></author><category term="quantization"/><category term="quantization"/><summary type="html"><![CDATA[Quantization in Pytorch and ONNX]]></summary></entry><entry><title type="html">Unsupervised Learning</title><link href="https://semyeong-yu.github.io/blog/2024/unsupervised-learning/" rel="alternate" type="text/html" title="Unsupervised Learning"/><published>2024-03-01T20:00:00+00:00</published><updated>2024-03-01T20:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/unsupervised-learning</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/unsupervised-learning/"><![CDATA[<p>뮌헨공대 (Technical University of Munich)에서 공부한<br/> [IN2375 Computer Vision - Detection, Segmentation and Tracking] 컴퓨터비전 노트 정리</p> <h3 id="unsupervised-learning">Unsupervised Learning</h3> <p>특징 : fast / costly (much computation)</p> <h3 id="self-supervised-learning">Self-supervised learning</h3> <blockquote> <p>Evaluation</p> </blockquote> <p>fine-tuning / linear probing / k-NN classification</p> <blockquote> <p>Pretext task</p> </blockquote> <p>사용자가 직접 정의한 새로운 different task (goal task와 관련 있어야 함)</p> <blockquote> <p>Contrastive Learning</p> </blockquote> <p>아이디어 :</p> <p>positive pair (image vs augmented image)와 negative pair (image vs other image) 만들어서 triplet loss 사용 또는 maximize contrastive score (아래 사진 참고)</p> <ul> <li>SimCLR (Simple framework for Contrastive Learning) :</li> </ul> <p>two augmented images에 대해 maximize similarity b.w. feature maps</p> <p>장점 : simple</p> <p>단점 : negative samples 수가 많을수록 gradient bias가 줄어드므로<br/> large batchsize가 필요한데 그러면 large computation 필요</p> <ul> <li>MoCo (Momentum Contrast) :</li> </ul> <ol> <li>memory queue를 통해 GPU memory footprint 작아서 large batchsize 가능</li> <li>dynamic update of momentum encoder로 성능 향상</li> </ol> <blockquote> <p>Non-Contrastive Learning</p> </blockquote> <p>other images (negative pair) 쓰지 않음</p> <ul> <li>DINO (self-distillation with no labels) :<br/> ViT 등에서 robust latent embedding 만드는 데 사용</li> </ul> <p>entering : teacher sharpening할 때 collapse되는 것을 방지하기 위해 moving average of center를 빼줌</p> <p>sharpening : teacher가 student보다 lower temperature로 sharpened</p> <ul> <li>DINOv2 :</li> </ul> <p>DINO보다 2배 faster, 3배 less memory by noisy student, adaptive resolution, data curation, …</p> <ul> <li>MAE (Masked Autoencoders) :</li> </ul> <p>high masking ratio 필요</p> <p>masked patches를 reconstruct한 뒤 이에 대해서만 loss 계산</p> <blockquote> <p>multi-view assumption</p> </blockquote> <p>view(crop) provides enough info. for downstream task</p> <blockquote> <p>PAC (pixel-adaptive conv.) layer</p> </blockquote> <p>conv.를 하기 전에 spatially varying kernel을 곱함</p> <h3 id="downstream-applications">Downstream Applications</h3> <blockquote> <p>semantic segmentation 에서 KNN correspondence</p> </blockquote> <p>two image에 대해 “segmentation correspondence”가 “feature(DINO) correspondence”를 모방하도록 학습</p> <blockquote> <p>motion-based VOS</p> </blockquote> <p>network G : image &amp; optical flow -&gt; object mask</p> <p>network I : masked optical flow &amp; image -&gt; reconstruct optical flow</p> <blockquote> <p>Contrastive Random Walk (object tracking)</p> </blockquote> <p>아래 사진 참고.</p> <p>cycle consistency loss (cross-entropy loss b.w. label_t=0 and label_t=T)</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-unsupervised-learning/img97-480.webp 480w,/assets/img/2024-03-01-unsupervised-learning/img97-800.webp 800w,/assets/img/2024-03-01-unsupervised-learning/img97-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-unsupervised-learning/img97.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-unsupervised-learning/img102-480.webp 480w,/assets/img/2024-03-01-unsupervised-learning/img102-800.webp 800w,/assets/img/2024-03-01-unsupervised-learning/img102-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-unsupervised-learning/img102.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-unsupervised-learning/img107-480.webp 480w,/assets/img/2024-03-01-unsupervised-learning/img107-800.webp 800w,/assets/img/2024-03-01-unsupervised-learning/img107-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-unsupervised-learning/img107.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-unsupervised-learning/img112-480.webp 480w,/assets/img/2024-03-01-unsupervised-learning/img112-800.webp 800w,/assets/img/2024-03-01-unsupervised-learning/img112-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-unsupervised-learning/img112.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container>]]></content><author><name></name></author><category term="cv-tasks"/><category term="vision"/><category term="unsupervised"/><summary type="html"><![CDATA[Unsupervised Learning]]></summary></entry><entry><title type="html">Semi-Supervised Learning</title><link href="https://semyeong-yu.github.io/blog/2024/semisupervised-learning/" rel="alternate" type="text/html" title="Semi-Supervised Learning"/><published>2024-03-01T19:00:00+00:00</published><updated>2024-03-01T19:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/semisupervised-learning</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/semisupervised-learning/"><![CDATA[<p>뮌헨공대 (Technical University of Munich)에서 공부한<br/> [IN2375 Computer Vision - Detection, Segmentation and Tracking] 컴퓨터비전 노트 정리</p> <h3 id="semi-supervised-learning">Semi-supervised Learning</h3> <blockquote> <p>아이디어</p> </blockquote> <p>use both labelled and unlabelled data</p> <blockquote> <p>assumption</p> </blockquote> <ul> <li>smoothness : if two inputs are close, their labels are same</li> <li>low density : decision boundary should pass through region with low density</li> <li>manifold : data come from multiple low-dim. manifolds if data points share same manifold, their labels are same</li> </ul> <blockquote> <p>unsupervised pre-processing</p> </blockquote> <p>feature extraction</p> <blockquote> <p>wrapper method 중 self-training</p> </blockquote> <p>OnAVOS는 slow이므로 first frame/new frame 대신<br/> offline으로 labelled/unlabelled data 사용</p> <p>initial prediction이 중요하므로 미리 train strong baseline on labelled set</p> <blockquote> <p>energy minimization (low-density assumption 적용)</p> </blockquote> <p>minimize \(-p(x_i)logp(x_i)\) = entropy of class distribution of each pixel \(x_i\)</p> <blockquote> <p>VAN (virtual adversarial network) (smoothness assumption 적용)</p> </blockquote> <p>labelled set : true posterior(gt) 와 adversarial 추가한 image의 prediction 비교</p> <p>unlabelled set : 기존 image의 prediction 과 adversarial 추가한 image의 prediction 비교</p> <blockquote> <p>Domain Alignment</p> </blockquote> <p>GAN의 원리 사용하여 unlabelled real data와 labelled synthetic data의 distribution을 비슷하게</p> <blockquote> <p>Consistency Regularization</p> </blockquote> <p>image에 transformation을 가하더라도 robust하게 consistent prediction을 하도록 consistency loss 추가</p> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-semisupervised-learning/img112-480.webp 480w,/assets/img/2024-03-01-semisupervised-learning/img112-800.webp 800w,/assets/img/2024-03-01-semisupervised-learning/img112-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-semisupervised-learning/img112.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-semisupervised-learning/img117-480.webp 480w,/assets/img/2024-03-01-semisupervised-learning/img117-800.webp 800w,/assets/img/2024-03-01-semisupervised-learning/img117-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-semisupervised-learning/img117.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-01-semisupervised-learning/img122-480.webp 480w,/assets/img/2024-03-01-semisupervised-learning/img122-800.webp 800w,/assets/img/2024-03-01-semisupervised-learning/img122-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-03-01-semisupervised-learning/img122.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container>]]></content><author><name></name></author><category term="cv-tasks"/><category term="vision"/><category term="semisupervised"/><summary type="html"><![CDATA[Semi-Supervised Learning]]></summary></entry></feed>