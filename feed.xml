<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://semyeong-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://semyeong-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-08-26T14:44:50+00:00</updated><id>https://semyeong-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">GaussianEditor</title><link href="https://semyeong-yu.github.io/blog/2024/GSeditor/" rel="alternate" type="text/html" title="GaussianEditor"/><published>2024-08-25T11:00:00+00:00</published><updated>2024-08-25T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/GSeditor</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/GSeditor/"><![CDATA[<h2 id="gaussianeditor-swift-and-controllable-3d-editing-with-gaussian-splatting">GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting</h2> <h4 id="yiwen-chen-zilong-chen-chi-zhang-feng-wang-xiaofeng-yang-yikai-wang-zhongang-cai-lei-yang-huaping-liu-guosheng-lin">Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, Guosheng Lin</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2311.14521">https://arxiv.org/abs/2311.14521</a><br/> project website :<br/> <a href="https://gaussianeditor.github.io/">https://gaussianeditor.github.io/</a><br/> code :<br/> <a href="https://github.com/buaacyw/GaussianEditor">https://github.com/buaacyw/GaussianEditor</a></p> </blockquote> <h2 id="paper-review-후기">Paper Review 후기</h2> <ul> <li>novelty : <ul> <li>SAM mask를 GS로 inverse rendering해서 target GS identify</li> <li>기존 GS에서 크게 벗어나지 않도록(stability) anchor loss</li> </ul> </li> <li>3DGS 나오고나서 3DGS 이용한 Editing에 대해 잽싸게 낸 논문이라<br/> 비교 대상도 없고<br/> Editing loss도 기존 기법을 그대로 써서 뭔가 novelty도 없는 느낌…?</li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li>NeRF-based 3D Editing : <ul> <li>Instruct-nerf2nerf: Editing 3D scenes with instructions</li> <li>Ed-nerf: Efficient text-guided editing of 3D scene using latent space nerf</li> <li>Clip-nerf: Text-and-Image driven manipulation of neural radiance fields</li> <li>Nerf-art: Text-driven neural radiance fields stylization</li> <li>Dreameditor: Text-driven 3D scene editing with neural fields</li> </ul> </li> <li>NeRF-based 3D Editing by MLP의 문제점 : <ul> <li><code class="language-plaintext highlighter-rouge">specific</code> scene parts를 직접 수정하는 데 제한</li> <li>inpainting 및 scene composition 과정이 <code class="language-plaintext highlighter-rouge">복잡</code></li> <li>strictly <code class="language-plaintext highlighter-rouge">masked area</code> 내에서만 editing 가능</li> </ul> </li> <li>3DGS-based 3D Editing의 문제점 : <ul> <li>Editing할 Gaussian을 <code class="language-plaintext highlighter-rouge">identify</code>(분류)해야 함</li> <li>SDS처럼 Diffusion model로 얻은 <code class="language-plaintext highlighter-rouge">random generative guidance</code>를 3DGS에 적용할 때<br/> randomness in loss로 인해<br/> view마다 non-consistent(random)한 image를 합성(Editing)하므로<br/> GS is directly affected by randomness,<br/> so 업데이트 불안정</li> <li><code class="language-plaintext highlighter-rouge">수많은</code> Gaussian points를 업데이트해야 함<br/> NeRF-based에서처럼 MLP NN buffering이 불가능하므로 불안정하여<br/> finely detailed result로 수렴하는 걸 방해</li> </ul> </li> </ul> <h2 id="method">Method</h2> <h3 id="gaussian-semantic-tracing">Gaussian Semantic Tracing</h3> <ul> <li> <p>전제 : 3DGS가 이미 잘 구성되어 있다고 가정하고, 특정 scene part를 제거 또는 추가하거나 inpainting하는 등 3D Editing 수행</p> </li> <li> <p>Gaussian Semantic Tracing :<br/> 훈련하는 동안 3D Editing할 target을 trace하기 위해 semantic label(mask) 생성</p> </li> </ul> <h4 id="parameters">Parameters</h4> <ul> <li> <p>\(x, s, q, \alpha , c\) (position, covariance(scale, quaternion), opacity, color) 뿐만 아니라<br/> \(m_{ij}\) (<code class="language-plaintext highlighter-rouge">semantic Gaussian mask</code> for i-th Gaussian and j-th semantic label) 추가</p> </li> <li> <p>densification할 때 clone/split된 points는 parent point의 semantic label를 그대로 물려받음</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-25-GSeditor/1-480.webp 480w,/assets/img/2024-08-25-GSeditor/1-800.webp 800w,/assets/img/2024-08-25-GSeditor/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-25-GSeditor/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 처음에 inaccurate segmentation mask에서 출발했더라도 Gaussian semantic tracing하는 동안 3DGS 업데이트하면서 semantic Gaussian mask도 알맞게 업데이트됨 </div> <h4 id="initial-labeling-process">Initial Labeling Process</h4> <ul> <li> <p>camera pose 하나 골라서 <code class="language-plaintext highlighter-rouge">SAM</code>(Segment Anything)으로 2D segmentation 수행한 뒤<br/> <code class="language-plaintext highlighter-rouge">inverse rendering</code>으로 2D mask를 3D Gaussian으로 unproject<br/> \(w_i^j = \Sigma o_i (p) \ast T_i^j (p) \ast M^j (p)\)<br/> where \(w_i^j\) : weight of i-th Gaussian for j-th semantic label<br/> where \(o, T, M, p\) : opacity, transmittance, mask, pixel</p> </li> <li> <p>average weight가 threshold를 넘는 경우에만 해당 i-th Gaussian이 j-th semantic class를 갖는다고 선별</p> </li> </ul> <h3 id="hierarchical-gaussian-splatting">Hierarchical Gaussian Splatting</h3> <ul> <li> <p>Hierarchical Gaussian Splatting :<br/> stabilized and fine results 만들기 위해 anchor loss 사용</p> </li> <li> <p>3D Editing 위해 densification할 때<br/> threshold를 manually 정하는 게 아니라,<br/> 3D position gradients가 top k% 안에 드는 3DGS들만 선택적으로 densify<br/> (k값이 점점 증가)</p> </li> <li><code class="language-plaintext highlighter-rouge">anchor loss</code> : <ul> <li>3D Editing 때문에 densification할 때마다 <code class="language-plaintext highlighter-rouge">기존</code>의 Gaussian param.을 anchor에 record</li> <li>3D Editing에 따라 변형되는 Gaussian param.가 각 anchor로부터 크게 벗어나지 않도록 함<br/> \(L_{anchor}^P = \Sigma_{i=0}^n \lambda_{i} (P_i - \hat P_i)^2\)<br/> where \(P : x, s, q, \alpha , c, m_{ij}\)<br/> where \(\lambda_{i}\) 값이 점점 증가 (새로 만들어지는 Gaussian param.의 영향이 크도록)</li> <li><code class="language-plaintext highlighter-rouge">stable</code> geometry formation under stochastic loss 보장</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Edit loss</code> : <ul> <li>3DGS model로 rendering한 image와 diffusion model 간의 차이<br/> \(L_{Edit} = D(\theta ; p, e)\)<br/> where \(D, \theta , p, e\) : Diffusion model, 3D model, camera pose, prompt</li> <li>2D diffusion model로 3D Editing하는 방법 :<br/> 1) <code class="language-plaintext highlighter-rouge">DreamFusion</code> <d-cite key="Dreamfusion">[1]</d-cite> 의 <code class="language-plaintext highlighter-rouge">SDS loss</code>처럼 3D model의 rendering과 other conditions를 2D diffusion model에 넣어준 뒤, noise 넣고 <code class="language-plaintext highlighter-rouge">denoising하는 과정에서 내놓은 score</code>가 3D model의 업데이트 방향을 guide<br/> 즉, 3D model로 만든 image가 2D diffusion에서의 그럴 듯한 image distribution에 부합하도록 함<br/> 2) <code class="language-plaintext highlighter-rouge">Instruct-nerf2nerf</code>처럼 3D model의 rendering과 prompts 이용해서 <code class="language-plaintext highlighter-rouge">2D Editing</code> 수행하는 데 초점을 두고, Edited 2D multi-view images를 training target으로 사용하여 3D model에게 guidance 줌</li> </ul> </li> <li>total loss :<br/> \(L = L_{Edit} + \Sigma_{P \in [x, s, q, \alpha , c]} \lambda_{P} L_{anchor}^P\)</li> </ul> <h3 id="3d-inpainting">3D Inpainting</h3> <ul> <li> <p>외부 모델들 사용해서 Efficient 3D Editing 구현</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Object Removal</code> (object 제거) :</p> <ul> <li>semantic label(mask) 가지는 3D Gaussian만 삭제하면 target object와 scene 사이의 interface에서 artifacts 생김</li> <li><code class="language-plaintext highlighter-rouge">precise mask</code>를 생성할 필요가 있음 <ul> <li>삭제한 3DGS와 가장 가까운 Gaussian을 KNN으로 identify</li> <li>이를 다양한 view-points로 project하여 mask를 <code class="language-plaintext highlighter-rouge">확장</code> (dilate)</li> <li>hole을 메꿔서 interface area를 정확하게 표현하도록 refined mask를 생성</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Diffusion model</code>을 이용해서 해당 area를 <code class="language-plaintext highlighter-rouge">2D inpainting</code> (object 삭제)</li> <li>inpainted image를 기반으로 <code class="language-plaintext highlighter-rouge">3DGS 업데이트</code></li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-25-GSeditor/4-480.webp 480w,/assets/img/2024-08-25-GSeditor/4-800.webp 800w,/assets/img/2024-08-25-GSeditor/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-25-GSeditor/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Object Incorporation by text</code> (object 추가 혹은 수정) : <ul> <li>editing area에 BB 만듦</li> <li><code class="language-plaintext highlighter-rouge">Stable Diffusion XL</code> model <d-cite key="SDXL">[2]</d-cite> 을 이용해서 해당 area에 <code class="language-plaintext highlighter-rouge">넣을 image</code>를 생성하고<br/> fg object is segmented</li> <li><code class="language-plaintext highlighter-rouge">Wonder3D</code> model <d-cite key="Wonder3D">[3]</d-cite> 을 이용해서 fg-segmented image를 3D textured <code class="language-plaintext highlighter-rouge">mesh</code>로 변환</li> <li>Hierarchical Gaussian Splatting을 이용해서 mesh를 새로운 <code class="language-plaintext highlighter-rouge">3DGS</code>로 변환</li> <li><code class="language-plaintext highlighter-rouge">DPT</code> <d-cite key="DPT">[4]</d-cite> 로 depth estimation해서 기존의 3DGS와 생성된 3DGS의 <code class="language-plaintext highlighter-rouge">depth를 align</code>해주고 기존의 3DGS와 생성된 3DGS를 <code class="language-plaintext highlighter-rouge">concatenate</code>(결합)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-25-GSeditor/5-480.webp 480w,/assets/img/2024-08-25-GSeditor/5-800.webp 800w,/assets/img/2024-08-25-GSeditor/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-25-GSeditor/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-25-GSeditor/3-480.webp 480w,/assets/img/2024-08-25-GSeditor/3-800.webp 800w,/assets/img/2024-08-25-GSeditor/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-25-GSeditor/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="experiments">Experiments</h2> <ul> <li>Implementation : <ul> <li>view-point (camera-pose) 개수 : 24-96개</li> <li>optimization : 3DGS가 이미 구성되었다는 전제 하에<br/> 3D Editing하는 데만 500-1000 steps, 5-10 min.<br/> (3 min. for Wonder3D mesh 생성 + 2 min. for 3DGS로 변환 및 refine)</li> </ul> </li> <li>Ablation Study : <ul> <li>w/o Semantic Tracing :<br/> target object만 Editing되는 게 아니라 image 전체 Editing</li> <li>w/o Hierarchical GS :<br/> uncontrolled densification 및 image blurring 초래</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-25-GSeditor/6-480.webp 480w,/assets/img/2024-08-25-GSeditor/6-800.webp 800w,/assets/img/2024-08-25-GSeditor/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-25-GSeditor/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-25-GSeditor/7-480.webp 480w,/assets/img/2024-08-25-GSeditor/7-800.webp 800w,/assets/img/2024-08-25-GSeditor/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-25-GSeditor/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="conclusion">Conclusion</h2> <ul> <li>2 strategies <ul> <li><code class="language-plaintext highlighter-rouge">Gaussian Semantic Tracing</code><br/> for precise Gaussian <code class="language-plaintext highlighter-rouge">identification</code> of editing areas</li> <li><code class="language-plaintext highlighter-rouge">Hierarchical GS</code><br/> for balance b.w. fluidity and <code class="language-plaintext highlighter-rouge">stability</code><br/> to achieve <code class="language-plaintext highlighter-rouge">detailed(fine) results</code> under stochastic guidance</li> </ul> </li> </ul> <h2 id="limitation">Limitation</h2> <ul> <li>supervision을 위해 <code class="language-plaintext highlighter-rouge">2D diffusion model에 의존</code>하여 3D editing을 수행하는데<br/> 현재 2D diffusion model은 특정 복잡한 prompts에 대해서는 effective guidance를 제공하는 데 어려움이 있어 3D editing에도 한계 있음</li> </ul> <h2 id="question">Question</h2> <ul> <li>Q1 : Edit loss에서 사용하는 SDS loss나 Instruct-nerf2nerf 기법은 이미 있는 내용이고,<br/> 본 논문에서 볼 건 아래의 두 가지 정도인데 (SAM mask를 GS로 inverse rendering해서 target GS identify하고<br/> 기존 GS에서 크게 벗어나지 않도록(stability) anchor loss)<br/> 별로 novelty가 없는 것 같다</li> <li> <p>A1 : ㅇㅈ</p> </li> <li>Q2 : 3D model Editing 분야 흐름?</li> <li>A2 : TBD</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="GS"/><category term="3d"/><category term="editing"/><summary type="html"><![CDATA[Swift and Controllable 3D Editing with Gaussian Splatting (CVPR 2024)]]></summary></entry><entry><title type="html">COLMAP-Free 3D Gaussian Splatting</title><link href="https://semyeong-yu.github.io/blog/2024/Colmapfree/" rel="alternate" type="text/html" title="COLMAP-Free 3D Gaussian Splatting"/><published>2024-08-24T11:00:00+00:00</published><updated>2024-08-24T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Colmapfree</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Colmapfree/"><![CDATA[<h2 id="colmap-free-3d-gaussian-splatting">COLMAP-Free 3D Gaussian Splatting</h2> <h4 id="yang-fu-sifei-liu-amey-kulkarni-jan-kautz-alexei-a-efros-xiaolong-wang">Yang Fu, Sifei Liu, Amey Kulkarni, Jan Kautz, Alexei A. Efros, Xiaolong Wang</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2312.07504">https://arxiv.org/abs/2312.07504</a><br/> project website :<br/> <a href="https://oasisyang.github.io/colmap-free-3dgs/">https://oasisyang.github.io/colmap-free-3dgs/</a><br/> pytorch code :<br/> <a href="https://github.com/NVlabs/CF-3DGS">https://github.com/NVlabs/CF-3DGS</a></p> </blockquote> <h2 id="introduction">Introduction</h2> <ul> <li>기존 novel-view-synthesis : <ul> <li>input images<br/> \(\rightarrow\) COLMAP library for SfM <code class="language-plaintext highlighter-rouge">pcd, camera pose</code> 계산<br/> \(\rightarrow\) NeRF or 3DGS</li> <li>단점 : 시간 많이 걸리고, feature 추출 오차에 대해 민감성이 있고, 반복적인 영역을 처리하는 데 어려움</li> </ul> </li> <li> <p>Motivation :<br/> <code class="language-plaintext highlighter-rouge">pose estimation</code>과 <code class="language-plaintext highlighter-rouge">novel-view-synthesis</code>를<br/> COLMAP과 3DGS로 나눠서 하지 말고<br/> <code class="language-plaintext highlighter-rouge">end-to-end로 동시에</code> 할 수는 없을까?</p> </li> <li>Related Work :<br/> 사전에 COLMAP library 사용하지 않기 위해<br/> <code class="language-plaintext highlighter-rouge">BARF, Nope-NeRF, L2G-NeRF</code> 등<br/> 여러 방법들이 제안되어 왔지만<br/> 여러 한계 있음 <ul> <li>perturbation이 적어야 함</li> <li><code class="language-plaintext highlighter-rouge">camera motion의 범위</code>가 너무 넓으면 안 됨<br/> (Nope-NeRF 등은 pose를 직접 optimize하는 게 아니라 ray casting process를 optimize하는 간접적인 방법이라서 camera 이동이 커지면 optimize 난이도가 복잡해짐)</li> <li><code class="language-plaintext highlighter-rouge">training time</code>이 너무 오래 걸림</li> <li>NeRF-based 기법들은 MLP-based implicit method이므로<br/> 3DGS처럼 explicit pcd를 요구하는 method에 적용하기 어렵</li> <li>regularization term이 많아져서 복잡하고 geometric prior를 요구하기도 함</li> </ul> </li> <li>COMALP-Free 3D GS : <ul> <li>3DGS가 <code class="language-plaintext highlighter-rouge">explicit</code> representation (pcd 등) 을 활용할 수 있기 때문에 새로운 접근이 가능해졌다 <ul> <li>temporal continuity data (video sequence)와<br/> explicit representation data (3DGS)를 이용해서<br/> pose estimation과 novel view synthesis를 동시에 수행</li> </ul> </li> <li>Local 3DGS : <ul> <li><code class="language-plaintext highlighter-rouge">initialization</code> 위해 Nope-Nerf 랑 비슷하게 monocular <code class="language-plaintext highlighter-rouge">depth-map</code> 사용</li> <li>목표 :<br/> 주어진 frame \(t-1\) 에서의 local 3D Gaussian 집합을 구성하고,<br/> frame \(t\) 에서의 local 3D Gaussian 집합으로 변환할 수 있는<br/> <code class="language-plaintext highlighter-rouge">relative camera pose (affine transformation)</code> 학습</li> </ul> </li> <li>Global 3DGS : <ul> <li>목표 :<br/> Local 3DGS에서 구한 relative camera pose를 기반으로<br/> Global 3DGS를 순차적으로 점진적으로 계속 업데이트해서<br/> entire scene <code class="language-plaintext highlighter-rouge">reconstruction</code> 결과가 깔끔하게 나타나도록 하자</li> </ul> </li> </ul> </li> <li>COLMAP vs 본 논문 : <ul> <li>COLMAP : 100장의 images를 <code class="language-plaintext highlighter-rouge">한 번에</code> 넣고 camera pose를 optimize</li> <li>본 논문 : video sequence를 <code class="language-plaintext highlighter-rouge">순차적으로</code> 실시간으로 받으며 점진적으로 optimize</li> </ul> </li> </ul> <h2 id="overview">Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-24-Colmapfree/1-480.webp 480w,/assets/img/2024-08-24-Colmapfree/1-800.webp 800w,/assets/img/2024-08-24-Colmapfree/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-24-Colmapfree/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="local-3dgs-for-relative-pose-estimation">Local 3DGS for Relative Pose Estimation</h2> <ul> <li>Initialization from a Single View : <ul> <li>initial frame을 monocular depth network (DPT)에 넣어 depth map \(D_1\) 생성</li> <li>3D mean :<br/> <code class="language-plaintext highlighter-rouge">initial frame</code> (2D 정보)과<br/> <code class="language-plaintext highlighter-rouge">initial depth map</code> \(D_1\) (3D 정보)와<br/> <code class="language-plaintext highlighter-rouge">intrinsic</code> param. 이용해서<br/> 3D pcd로 투영하고, 이를 initial 3DGS mean point로 사용</li> <li>opacity, SH-coeff., covariance(rotation, scale) :<br/> L1, D-SSIM photometric loss로 <code class="language-plaintext highlighter-rouge">optimal (initial) Local 3DGS</code>를 5초 정도만에 구함<br/> initial frame \(t = 1\) 에 대해<br/> \(G_t^{\ast} = \text{argmin}_{\alpha_{t}, c_t, r_t, s_t} L_{rgb}(R(G_t), I_t) = (1 - \lambda) L_1 + \lambda L_{D-SSIM}\)</li> </ul> </li> <li>Pose Estimation by 3D Gaussian Transformation : <ul> <li>Gaussian 집합 \(G_t\) 를 \(G_{t+1}\) 로 올바르게 변환할 수 있는 learnable SE-3 affine transformation \(T_t\) 를 찾아야 함</li> <li>전제 : video로 찍은 연속적인 frame이므로 \(T_t\) 의 값이 크지 않음</li> <li>photometric loss로 <code class="language-plaintext highlighter-rouge">optimal relative camera pose(affine transformation)</code>을 10초 안에 구함<br/> \(T_t^{\ast} = \text{argmin}_{T_t} L_{rbg} (R(T_t \odot G_t), I_{t+1})\)<br/> where \(G_t\) is <code class="language-plaintext highlighter-rouge">freezed</code> (self-rotation 등 방지)<br/> (geometric transformation(camera movement)에만 집중)</li> </ul> </li> </ul> <h2 id="global-3dgs-with-progressively-growing">Global 3DGS with Progressively Growing</h2> <ul> <li>Local 3DGS를 통해 optimal relative camera pose를 구했다 <ul> <li>한계 : frame \(F\) 와 frame \(F+t\) 간의 relative camera pose를 단순히 \(\prod_{k=F}^{F+t} T_k\) 처럼 곱으로 두면 오차가 점점 커져서<br/> entire <code class="language-plaintext highlighter-rouge">scene reconstruction 결과가 noisy</code></li> </ul> </li> <li>Global 3DGS : <ul> <li>frame이 들어올 때마다 relative camera pose \(T_t\) 와 frame \(t, t+1\) 이용해서 <code class="language-plaintext highlighter-rouge">optimal Global 3DGS</code> 업데이트 (progressively growing)</li> <li>어떻게 업데이트? :<br/> frame \(t+1\) 에는 frame \(t\) 에서 <code class="language-plaintext highlighter-rouge">보지 못한 일부 영역</code> 들이 있으므로<br/> 새로운 frame에 대한 <code class="language-plaintext highlighter-rouge">under-reconstruction densification</code>에 초점을 두어<br/> last frame까지 계속해서 점진적으로 scene reconstruction 수행<br/> (last frame까지 계속 under-reconstruction 상황(보지 못했던 영역)이 발생할 것이라는 전제)<br/> (새로운 테크닉은 아니고 3DGS에서의 adaptive density control과 동일)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-24-Colmapfree/2-480.webp 480w,/assets/img/2024-08-24-Colmapfree/2-800.webp 800w,/assets/img/2024-08-24-Colmapfree/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-24-Colmapfree/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Local 3DGS와 Global 3DGS는 iteratively optimized</li> </ul> <h2 id="experiment">Experiment</h2> <ul> <li>GS 말고 pose-free NeRF methods와 비교했을 때<br/> pose trajectory와 scene reconstruction 측면에서<br/> 본 논문이 훨씬 더 좋은 성능</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-24-Colmapfree/3-480.webp 480w,/assets/img/2024-08-24-Colmapfree/3-800.webp 800w,/assets/img/2024-08-24-Colmapfree/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-24-Colmapfree/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-24-Colmapfree/4-480.webp 480w,/assets/img/2024-08-24-Colmapfree/4-800.webp 800w,/assets/img/2024-08-24-Colmapfree/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-24-Colmapfree/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-24-Colmapfree/5-480.webp 480w,/assets/img/2024-08-24-Colmapfree/5-800.webp 800w,/assets/img/2024-08-24-Colmapfree/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-24-Colmapfree/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>COLMAP + 3DGS와 비교했을 때<br/> 본 논문과 동일한 성능</p> </li> <li>우리는 pose estimation을 할 때 photometric loss에만 의존했음 <ul> <li>photometric loss에만 의존해서 relative camera pose를 구했기 때문에 \(RPE_r, RPE_t\) 값이 Nope-NeRF보다 조금 높게 나타날 수 있음</li> <li>Nope-NeRF에서는 chamfer distance(point cloud 집합인 \(P_i\) 와 \(P_j\) 가 서로 가까워지도록 하는 point cloud loss) 추가하여 pose accuracy 높임</li> </ul> </li> <li>Nope-NeRF에서와 달리 본 논문에서 depth loss를 쓰면 pose accuracy는 비슷하고 novel view synthesis performance는 오히려 떨어지므로 depth loss는 안 씀</li> </ul> <h2 id="limitation">Limitation</h2> <ul> <li>camera pose와 3DGS를 동시에 순차적으로 optimize하므로<br/> video stream 혹은 ordered image 집합에만 적용 가능<br/> \(\rightarrow\) unordered image 집합에도 적용하는 future work 필요</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="COLMAP"/><category term="SfM"/><category term="GS"/><category term="depth"/><category term="pose"/><category term="rendering"/><category term="3d"/><summary type="html"><![CDATA[COLMAP-Free 3D Gaussian Splatting (CVPR 2024)]]></summary></entry><entry><title type="html">Mip-NeRF 360</title><link href="https://semyeong-yu.github.io/blog/2024/MipNeRF360/" rel="alternate" type="text/html" title="Mip-NeRF 360"/><published>2024-08-11T01:03:00+00:00</published><updated>2024-08-11T01:03:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/MipNeRF360</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/MipNeRF360/"><![CDATA[<h2 id="mip-nerf-360-unbounded-anti-aliased-neural-radiance-fields">Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields</h2> <h4 id="jonathan-t-barron-ben-mildenhall-dor-verbin-pratul-p-srinivasan-peter-hedman">Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, Peter Hedman</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2111.12077">https://arxiv.org/abs/2111.12077</a><br/> project website :<br/> <a href="https://jonbarron.info/mipnerf360/">https://jonbarron.info/mipnerf360/</a><br/> pytorch code :<br/> <a href="https://github.com/google-research/multinerf">https://github.com/google-research/multinerf</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>sampling 기법 개선하고, bounded scene으로 warp</li> <li>non-linear scene, ray parameterization :<br/> disparity에 비례하도록 sampling 개선하고<br/> bounded space로 mapping하여<br/> 임의의 방향과 깊이에 대한 unbounded scene 다룸</li> <li>efficient proposal-based online-distillation :<br/> higher capacity MLP을 조금만 evaluate해서<br/> 효율적으로 large scene 다룸</li> <li>interval-distortion-based regularizer :<br/> artifacts 줄이기 위해<br/> step-function을 delta-function에 가깝게 regularize</li> </ol> </blockquote> <h2 id="introduction">Introduction</h2> <ul> <li>임의의 <code class="language-plaintext highlighter-rouge">direction</code>(360 degrees)과 임의의 <code class="language-plaintext highlighter-rouge">depth</code>로 <code class="language-plaintext highlighter-rouge">unbounded</code> 되어있는 scene 문제 해결 <ul> <li>non-linear scene parameterization :<br/> sampling 개선하고 bounded space로 mapping하여<br/> 임의의 방향과 깊이에 대한 unbounded scene 다룰 수 있음</li> <li>online-distillation :<br/> higher capacity MLP을 조금만 evaluate해서 효율적으로 large scene 다룰 수 있음</li> <li>distortion-based regularizer :<br/> artifacts 줄이기 위한 regularization</li> </ul> </li> <li>NeRF model을 large unbounded scene에 적용하는 데 3가지 문제가 있다<br/> (자세한 내용은 스킵했는데 나중에 읽어보자) <ul> <li>Parametrization : Mip-NeRF는 3D coordinate가 bounded domain 안에 있는 경우만 처리 가능</li> <li>Efficiency : large-and-detailed scene은 large MLP를 필요로 해서 expensive</li> <li>Ambiguity : scene content가 임의의 distance에 있고 이는 only 적은 수의 ray로 관찰되기 때문에 inherent ambiguity 발생</li> </ul> </li> </ul> <h2 id="scene-and-ray-parameterization">Scene and Ray Parameterization</h2> <h3 id="ray-interval-parameterization">Ray Interval Parameterization</h3> <ul> <li> <p>Ray Interval Parameterization :<br/> samples의 경우 distance가 아니라 그의 역수인 <code class="language-plaintext highlighter-rouge">disparity에 비례</code>하여 분포하도록 하면<br/> 가까이 있는 content는 많이 sampling하고 멀리 있는 content는 덜 sampling함으로써<br/> <code class="language-plaintext highlighter-rouge">임의의 scale의 unbounded scene</code>을 잘 다룰 수 있음</p> </li> <li>NeRF : <ul> <li>NeRF에서는 distance에 비례하여 stratified uniform sampling 했음</li> <li>만약 NDC parameterization을 쓴다면<br/> NDC-space에서 distance에 비례하여 stratified uniform sampling 하면<br/> disparity (distance의 역수)에 비례하여 uniform sampling 한 것과 같은 효과를 가짐<br/> 그 이유는 <a href="https://semyeong-yu.github.io/blog/2024/NDC/">Normalized-Device-Coordinates</a> 의 Linear in Disparity 파트 참고</li> <li>그런데 NDC는 single direction으로만 unbounded된 scene (front-facing camera)에 대해서만 적합하고<br/> 모든 방향으로 unbounded된 scene에 대해서는 적합하지 않음</li> </ul> </li> <li>Mip-NeRF 360 : <ul> <li>처음부터 ray interval을 disparity (distance의 역수)에 비례하도록 <d-cite key="LLFF">[2]</d-cite> parameterize 한다</li> </ul> </li> </ul> <h3 id="ray-interval-parameterization-in-disparity">Ray Interval Parameterization in Disparity</h3> <ul> <li>distance along ray를 t-space 또는 s-space에서 나타내자 <ul> <li>t-space :<br/> Euclidean ray distance \(t \in [t_n, t_f]\)<br/> \(t = g^{-1}(s \cdot g(t_f) + (1-s) \cdot g(t_n))\)</li> <li>s-space :<br/> normalized ray distance \(s \in [0, 1]\)<br/> \(s = \frac{g(t)-g(t_n)}{g(t_f)-g(t_n)}\)</li> </ul> </li> <li>사용 예시 : <ul> <li>\(g(x) = \frac{1}{x}\) 로 설정할 경우<br/> <code class="language-plaintext highlighter-rouge">s-space에서 uniform sampling</code>하면<br/> <code class="language-plaintext highlighter-rouge">t-space에서 disparity에 비례</code>하여 distributed</li> <li>\(g(x) = log(x)\) 로 설정할 경우<br/> s-space에서 uniform sampling하면<br/> t-space에서는 logarithmic spacing <d-cite key="DONeRF">[3]</d-cite> 으로 distributed</li> </ul> </li> <li>기존 NeRF 모델에서는 t-distance를 따라 uniform sampling했지만<br/> 본 논문에서는 <code class="language-plaintext highlighter-rouge">s-distance</code>를 따라 uniform sampling하여 나타낸다</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/4-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/4-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="scene-parameterization">Scene Parameterization</h3> <ul> <li> <p>Scene Parameterization :<br/> unbounded scene을 radius-2 내부의 <code class="language-plaintext highlighter-rouge">bounded space</code>로 mapping하기 위해 <code class="language-plaintext highlighter-rouge">contract 함수</code>를 사용<br/> ray parameterization을 할 때 disparity에 비례하게 sampling 했으므로<br/> contract 함수도 consistently <code class="language-plaintext highlighter-rouge">disparity에 비례</code>하게 bounded space로 mapping<br/> \(\rightarrow\) scene origin에서 cast된 ray의 경우 contract 함수를 적용한 후에는 아래 그림의 주황색 영역에서 일정한 길이의 interval을 가진다</p> </li> <li>Define smooth coordinate-transformation function as \(f(x) : R^3 \rightarrow R^3\) <ul> <li>\(\mu, \Sigma\) 를 갖는 Gaussian에 non-linear \(f\) 를 적용하여 \(\mu_{c}, \Sigma_{c}\) 를 갖는 Gaussian으로 변형하려면<br/> \((\mu_{c}, \Sigma_{c}) = f(\mu, \Sigma) = (f(\mu), J_{f}(\mu) \Sigma J_{f}(\mu)^T)\)<br/> where \(f(x) \approx f(\mu) + J_{f}(\mu)(x-\mu)\) (linear approx.)</li> <li>이는 state transition model \(f = \text{contract}(x) = \begin{cases} x &amp; \text{if} \| x \| \leq 1 \\ (2 - \frac{1}{\| x \|})(\frac{x}{\| x \|}) &amp; \text{if} \| x \| \gt 1 \end{cases}\) 을 사용했을 때<br/> classic Extended Kalman filter <d-cite key="kalman">[1]</d-cite> 와 수학적으로 동일</li> <li>MipNeRF360에서는 contract 함수를<br/> <code class="language-plaintext highlighter-rouge">point가 아니라</code> Euclidean 3D-space에 있는 <code class="language-plaintext highlighter-rouge">Gaussian</code>에 적용!<br/> 또한<br/> <code class="language-plaintext highlighter-rouge">모든 방향</code> (360 degress)에 대해 적용!</li> </ul> </li> <li>IPE (integrated positional encoding) : <ul> <li>Mip-NeRF :<br/> \(\gamma (\mu, \Sigma) = \left[ \begin{bmatrix} sin(2^l \mu) \circledast exp(-\frac{1}{2} 4^l diag(\Sigma)) \\ cos(2^l \mu) \circledast exp(-\frac{1}{2} 4^l diag(\Sigma)) \end{bmatrix} \right]_{l=0}^{l=L-1}\)</li> <li>Mip-NeRF 360 :<br/> \(\gamma (\text{contract}(\mu, \Sigma))\)<br/> where<br/> \(f(x) = \text{contract}(x) = \begin{cases} x &amp; \text{if} \| x \| \leq 1 \\ (2 - \frac{1}{\| x \|})(\frac{x}{\| x \|}) &amp; \text{if} \| x \| \gt 1 \end{cases}\)<br/> and \(f(x) \approx f(\mu) + J_{f}(\mu)(x-\mu)\)<br/> and \(f(\mu, \Sigma) = (f(\mu), J_{f}(\mu) \Sigma J_{f}(\mu)^T)\)</li> <li>Mip-NeRF 360 procedure :<br/> casting cone<br/> \(\rightarrow\) uniform sampling in s-space<br/> \(\rightarrow\) contract 3D Gaussians in t-space into bounded sphere<br/> \(\rightarrow\) IPE \(\gamma\)<br/> \(\rightarrow\) MLP</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/1-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/1-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> contract 함수는 파란색 구(radius 1) 외부의 Gaussian(회색)을 주황색 영역(radius 1 ~ 2)의 Gaussian(빨간색)으로 mapping </div> <h2 id="coarse-to-fine-online-distillation">Coarse-to-Fine Online Distillation</h2> <ul> <li>기존 NeRF :<br/> coarse-MLP와 fine-MLP</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/2-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/2-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 위의 그림은 Mip-NeRF, 아래의 그림은 Mip-NeRF 360 </div> <ul> <li>Mip-NeRF 360 :<br/> proposal-MLP와 NeRF-MLP <ul> <li><code class="language-plaintext highlighter-rouge">small</code> proposal-MLP는 <code class="language-plaintext highlighter-rouge">many</code> samples로 <code class="language-plaintext highlighter-rouge">여러 번</code> evaluate하고,<br/> <code class="language-plaintext highlighter-rouge">large</code> NeRF-MLP는 <code class="language-plaintext highlighter-rouge">less</code> samples로 <code class="language-plaintext highlighter-rouge">딱 한 번</code> evaluate함으로써<br/> Mip-NeRF보다 조금만 더 costly하지만 훨씬 더 <code class="language-plaintext highlighter-rouge">higher capacity</code>를 가진 것과 같은 효과<br/> \(\rightarrow\) 효율적으로 <code class="language-plaintext highlighter-rouge">large unbounded scene</code>을 표현하기에 적절<br/> distill 효과가 좋아서 proposal-MLP의 경우 크기 줄이더라도 accuracy 감소하지 않음</li> <li>small proposal-MLP : <ul> <li>color 말고 volume density만 예측하여 weight \(\hat w\) 구함</li> </ul> </li> <li>large NeRF-MLP : <ul> <li>color, volume density 예측하여 weight \(w\) 구하고 rendering</li> </ul> </li> </ul> </li> <li>Loss :<br/> 아래 두 가지 loss로 각 MLP를 jointly train <ul> <li><code class="language-plaintext highlighter-rouge">reconstruction loss</code> : <ul> <li>large NeRF-MLP에서 rendering해서 구함<br/> 기존 NeRF 방식과 동일</li> <li><code class="language-plaintext highlighter-rouge">GT-image를 supervision</code>으로 하여 <code class="language-plaintext highlighter-rouge">NeRF-MLP만 업데이트</code></li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">proposal loss</code> : <ul> <li>두 MLP의 <code class="language-plaintext highlighter-rouge">weight histogram이 consistent</code>하도록 함<br/> 즉, proposal-MLP의 weight histogram이 NeRF-MLP의 weight histogram을 따라잡도록 함<br/> (Mip-NeRF 계열은 point가 아니라 interval 별로 weight를 구하므로 histogram을 만들 수 있음)</li> <li><code class="language-plaintext highlighter-rouge">NeRF-MLP의 weight를 supervision</code>으로 하여 <code class="language-plaintext highlighter-rouge">proposal-MLP만 업데이트</code><br/> (<code class="language-plaintext highlighter-rouge">online distillation</code> of NeRF-MLP’s knowledge into proposal-MLP)</li> <li>문제 :<br/> 하나의 histogram bin의 distribution에 대해 아무 것도 가정할 수 없음<br/> (하나의 bin의 distribution이 uniform일 수도 있고 특정 지점에 몰빵된 delta function일 수도 있음…)<br/> coarse \(\hat t\) 와 fine \(t\) (bins)가 매우 다를 수 있음</li> <li>가정 :<br/> 두 개의 histogram이 매우 달라보이더라도<br/> 둘 다 <code class="language-plaintext highlighter-rouge">어떤 하나의 동일한 (underlying continuous) true mass distribution으로부터 유래되었다고 설명할 수 있다면</code> 둘의 차이인 loss는 0 이다</li> <li>위의 가정에 따라<br/> NeRF-MLP (\(t\), \(w\))의 구간 \(T\) 와 겹치는 모든 proposal-MLP의 weight \(\hat w_{j}\) 를 더해서 아래와 같이 NeRF-MLP weight \(w\) 의 <code class="language-plaintext highlighter-rouge">upper bound</code>를 구하자<br/> \(\text{bound}(\hat t, \hat w, T) = \sum_{j: T \cap \hat T_{j} \neq \emptyset} \hat w_{j}\)<br/> (\(t\) 와 \(\hat t\) 가 정렬되어 있으므로 summed-area table로 효율적으로 계산 가능)</li> <li>만약 두 개의 histogram이 consistent하다면,<br/> NeRF-MLP (\(t\), \(w\))의 모든 구간 (\(T_i, w_i\))에 대해<br/> \(w_i \leq \text{bound}(\hat t, \hat w, T_i)\) 이어야 한다<br/> \(\rightarrow\)<br/> 아래와 같이 <code class="language-plaintext highlighter-rouge">proposal loss는 이를 위반하는 경우</code>에 해당한다<br/> \(L_{prop}(t, w, \hat t, \hat w) = \sum_{i}\frac{1}{w_i} \text{max}(0, w_i - \text{bound}(\hat t, \hat w, T_i))^2\)</li> <li>즉, <code class="language-plaintext highlighter-rouge">proposal-MLP가 NeRF-MLP의 upper-bound를 형성</code>하도록 한다는 것은<br/> proposal-MLP가 NeRF-MLP histogram의 개형을 <code class="language-plaintext highlighter-rouge">따라잡도록</code> 하는 효과!</li> <li>proposal loss가 asymmetirc loss인 이유 :<br/> proposal-MLP가 NeRF-MLP보다 coarse하기 때문에<br/> proposal-MLP weight가 NeRF-MLP weight의 upper bound를 형성하는 게 (overestimate) 당연하고,<br/> proposal-MLP weight가 NeRF-MLP weight를 underestimate (\(\text{bound}(\hat t, \hat w, T_i) &lt; w_i\)) 하는 경우에만 penalize</li> <li>proposal loss term에서 \(w_i\) 로 나누는 이유 :<br/> bound가 0일 때 \(\frac{dL_{prop}}{d\text{bound}} = \sum_{i} \frac{1}{w_i} \cdot 2 \cdot \text{max}(0, w_i - \text{bound}) \cdot (-1) = -2\sum_{i}1\) 와 같이<br/> \(w_i\) 크기와 상관없이 <code class="language-plaintext highlighter-rouge">gradient 값이 상수값</code>이 되어 균등하게 penalize하여 optimization에 도움됨</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/8-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/8-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 위의 histogram이 NeRF-MLP, 아래의 histogram이 proposal-MLP, 보통 proposal-MLP가 coarse하고 NeRF-MLP가 fine한데 여기선 반대로 그려져 있음 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/3-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/3-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> fine NeRF-MLP는 점점 scene content의 surface 쪽으로 weight가 집중되고, coarse proposal-MLP는 이를 따라잡으며 upper bound를 형성 </div> <h2 id="regularization-for-interval-based-models">Regularization for Interval-Based Models</h2> <ul> <li>Artifacts :<br/> NeRF 계열은 pose 문제 때문에 두 가지 주된 artifacts가 나타난다 <ul> <li><code class="language-plaintext highlighter-rouge">floater</code> :<br/> 특정 view를 너무 잘 설명하려던 나머지<br/> 실제로 물체가 존재하지 않는 small disconnected regions of dense volume에서 불필요하게 opacity를 예측하여<br/> 다른 view에서 보면 반투명한 blurry cloud처럼 보이는 부분</li> <li><code class="language-plaintext highlighter-rouge">background collapse</code> :<br/> 멀리 있는 surface가<br/> 반투명한 가까운 content로 잘못 모델링된 경우</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/5-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/5-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 반투명하게 떠다니는 게 floater, 좌하단에서 background surface가 가깝게 보이는 게 background collapse </div> <ul> <li>Artifacts 완화 : <ul> <li>기존 NeRF : <code class="language-plaintext highlighter-rouge">random noise</code><br/> <a href="https://semyeong-yu.github.io/blog/2024/NeRFcode/">NeRF-Code</a> 의 raw2outputs()에서 볼 수 있듯이<br/> raw-opacity에 random noise 더해서 \(\sigma_{i}\) 구함<br/> noise 덕분에 <code class="language-plaintext highlighter-rouge">불필요한 특정 지점에 overfit 되는 게 아니라 일관성 있게</code> 학습<br/> But, 부분적으로 artifacts 완화하고 reconstruction quality를 떨어뜨림</li> <li>Mip-NeRF 360 : <code class="language-plaintext highlighter-rouge">regularize</code><br/> ray-sampling은 이미 했고 weight를 구할 때<br/> <code class="language-plaintext highlighter-rouge">물체가 있을만한 정확한 지점에서 집중적으로 예측</code>하여<br/> <code class="language-plaintext highlighter-rouge">부정확한 지점에서의 불필요한 예측을 억제</code></li> </ul> </li> <li>Regularization for Interval-Based Models : <ul> <li><code class="language-plaintext highlighter-rouge">distortion loss</code> :<br/> step-function인 weight-histogram \(s, w\) 을 regularize하기 위해<br/> \(L_{dist}(s, w) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} w_s(u)w_s(v)|u-v|d_ud_v\) <ul> <li><code class="language-plaintext highlighter-rouge">NeRF-MLP 업데이트</code>할 때 artifacts 완화(<code class="language-plaintext highlighter-rouge">regularization</code>)하는 역할</li> </ul> </li> <li>위의 loss를 최소화하기 위해선<br/> \(w\) 를 매우 작은 \(|u-v|\) 에 몰빵하면 된다<br/> 즉, 위의 loss term만 있을 경우 histogram(step-function)이 <code class="language-plaintext highlighter-rouge">delta-function</code>에 가까워지면 된다</li> <li>t-distance 대신 <code class="language-plaintext highlighter-rouge">s-distance</code> 사용 :<br/> t-distance 사용하면 먼 거리에 있는 interval 길이가 길기 때문에 무조건 먼 거리의 interval 쪽으로 distortion 됨<br/> s-distance 기준으로 weight-histogram 만들어서 distortion loss 구하자!</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/6-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/6-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">distortion loss</code> : <ul> <li>weight \(w\) 는 step-function (각 interval 안에선 constant) 이므로<br/> 아래와 같이 계산하기 쉬운 꼴로 유도할 수 있음<br/> \(L_{dist}(s, w) = \sum_{i, j} w_i w_j |\frac{s_i + s_{i+1}}{2} - \frac{s_j + s_{j+1}}{2}| + \frac{1}{3} \sum_{i} w_i^2 (s_{i+1} - s_i)\)</li> <li>유도 과정 :<br/> 출처 : https://charlieppark.kr/ <ul> <li>\(L_{dist}(s, w) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} w_s(u)w_s(v)|u-v|d_ud_v\)<br/> where \(w_s(u) = w_i\) for \(u \in [s_i, s_{i+1})\)</li> <li>case 1. \(u, v\) are in the same interval : \(u, v \in [s_i, s_{i+1})\)<br/> \(\int_{s_i}^{s_{i+1}}\int_{s_i}^{s_{i+1}}w_i^2|u-v|d_ud_v\)<br/> \(= w_i^2 \frac{(s_{i+1}-s_i)^3}{3}\)</li> <li>case 2. \(u, v\) are in different intervals : \(u \in [s_i, s_{i+1}), v \in [s_j, s_{j+1})\) where \(i \neq j\)<br/> \(\int_{s_i}^{s_{i+1}}\int_{s_j}^{s_{j+1}}w_iw_j|u-v|d_ud_v\)<br/> \(\simeq \int_{s_i}^{s_{i+1}}\int_{s_j}^{s_{j+1}}w_iw_j|\frac{s_i + s_{i+1}}{2} - \frac{s_j + s_{j+1}}{2}|d_ud_v\)<br/> \(= w_iw_j|\frac{s_i + s_{i+1}}{2} - \frac{s_j + s_{j+1}}{2}|\cdot (s_{i+1}-s_i)(s_{j+1}-s_j)\)</li> <li> \[L_{dist}(s, w) = \sum_{i} w_i^2 \frac{(s_{i+1}-s_i)^3}{3} + \sum_{i, j} w_iw_j|\frac{s_i + s_{i+1}}{2} - \frac{s_j + s_{j+1}}{2}|\cdot (s_{i+1}-s_i)(s_{j+1}-s_j)\] </li> <li>\((s_{i+1} - s_i)^2\) 항과 \((s_{i+1}-s_i)(s_{j+1}-s_j)\) 항을 제거하여 학습의 안정성을 높임<br/> \(L_{dist}(s, w) = \frac{1}{3} \sum_{i} w_i^2 (s_{i+1} - s_i) + \sum_{i, j} w_i w_j |\frac{s_i + s_{i+1}}{2} - \frac{s_j + s_{j+1}}{2}|\)</li> <li>\(u, v\) 가 <code class="language-plaintext highlighter-rouge">same interval</code>에 있을 경우에는 \((s_{i+1}-s_i)\) 항으로 <code class="language-plaintext highlighter-rouge">각 구간의 (weighted) 너비</code>를 줄이고,<br/> \(u, v\) 가 <code class="language-plaintext highlighter-rouge">different interval</code>에 있을 경우에는 \(|\frac{s_i + s_{i+1}}{2} - \frac{s_j + s_{j+1}}{2}|\) 항으로 <code class="language-plaintext highlighter-rouge">두 구간 사이의 (weighted) 중심 거리</code>를 줄임<br/> 이 원리를 통해<br/> entire ray is unoccupied이 가능하다면 모든 weight가 0에 가까워지려 하고<br/> 불가능하다면 <code class="language-plaintext highlighter-rouge">weight를 few interval에 몰빵</code>하려 해서<br/> weight-histogram이 delta-function에 가까워질 수 있음</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/7-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/7-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="optimization">Optimization</h2> <ul> <li>Setting : <ul> <li>proposal-MLPs with 4 layers and 256 hidden_dim<br/> two proposal-MLP \((\hat s^0, \hat w^0)\) and \((\hat s^1, \hat w^1)\) each using 64 samples</li> <li>NeRF-MLP \((s, w)\) with 8 layers and 1024 hidden_dim<br/> one NeRF-MLP using 32 samples<br/> (NeRF와 Mip-NeRF에서는 MLP with 8 layers and 256 hidden_dim 사용했었음)</li> <li>total loss :<br/> \(L_{tot} = L_{recon}(C(t), C^{\ast}) + \lambda L_{dist}(s, w) + \sum_{k=0}^{1}L_{prop}(s, w, \hat s^k, \hat w^k)\)<br/> averaged over all rays in batch<br/> where author sets \(\lambda = 0.01\)</li> <li>\(L_{recon}\) and \(L_{dist}\) for NeRF-MLP <ul> <li>\(L_{recon}(x, x^{\ast}) = \sqrt{(x - x^{\ast})^2 + \epsilon^{2}}\) : Charbonnier loss<br/> slightly more stable than MSE</li> </ul> </li> <li>\(L_{prop}\) for proposal-MLP</li> <li>learning schedule :<br/> 250k iter. with batch size \(2^{14}\)<br/> Adam optimizer with \(\beta_{1} = 0.9, \beta_{2} = 0.999, \epsilon = 10^{-6}\)<br/> lr is annealed log-linearly from \(2 \times 10^{-3}\) to \(2 \times 10^{-5}\)<br/> warm-up phase of 512 iter.<br/> gradient clipping to norm of \(10^{-3}\)</li> </ul> </li> </ul> <h2 id="conclusion">Conclusion</h2> <ul> <li>Mip-NeRF extension for real-world unbounded scenes with unconstrained camera depth and orientations</li> <li>(Kalman-like) scene and ray parameterization</li> <li>efficient proposal-based coarse-to-fine distillation framework</li> <li>interval-distortion-based regularizer</li> <li>Mip-NeRF에 비해 57% reduction in MSE</li> </ul> <h2 id="question">Question</h2> <ul> <li>Q&amp;A reference : 3DGS online study</li> <li>Q1 : 아래의 문구가 이해되지 않습니다<br/> recall that the “bins” of those histograms \(t\) and \(\hat t\) need not be similar; indeed, if the proposal MLP successfully culls the set of distances where scene content exists, \(\hat t\) and \(t\) will be highly dissimilar</li> <li>A1 : 아래 사진의 (c)에서처럼 충분히 optimize되어 만약 coarse proposal-MLP가 이미 scene content가 있는 곳을 성공적으로 예측하고 있다면 이를 이용한 fine NeRF-MLP의 fine-samples는 그 곳에 더 촘촘히 존재할 것이므로 bin 간격이 달라져서 두 histogram이 크게 달라보인다<br/> 달라보이더라도 두 개의 histogram이 어떤 하나의 (true continuous underlying) mass distribution에서 유래되었다고 설명할 수 있으면 둘의 차이가 0이라고 가정하여 upper bound를 이용해서 proposal loss 만듬</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/3-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/3-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Q2 : 갑자기 든 생각인데 Mip-NeRF 360의 sampling 기법과 contract 함수가 background collapse의 원인이 될 수도 있지 않을까요?<br/> disparity에 비례하게 sampling하므로 먼 거리에 대해서는 덜 sampling한 채로 bounded space로 warp하는데,<br/> 먼 거리의 content에 대해 정보가 부족한 채로 warp하는 과정에서 왜곡이 일어날 수 있을 것 같다</li> <li>A2 : 그럴 수 있을 것 같습니다</li> </ul> <h2 id="code-review">Code Review</h2> <p>TBD</p> <h2 id="appendix">Appendix</h2> <h3 id="off-axis-positional-encoding">Off-Axis Positional Encoding</h3> <ul> <li> <p>Mip-NeRF의 IPE 식 :<br/> PE-basis를 identity matrix로 설정하였으므로<br/> \(P = \begin{pmatrix} \begin{matrix} 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2\end{matrix} &amp; \cdots &amp; \begin{matrix} 2^{L-1} &amp; 0 &amp; 0 \\ 0 &amp; 2^{L-1} &amp; 0 \\ 0 &amp; 0 &amp; 2^{L-1}\end{matrix} \end{pmatrix}^T\)<br/> \(diag(\Sigma_{r})\) 계산을 위해 \(diag(\Sigma)\) 만 알면 됨</p> </li> <li> <p>Mip-NeRF 360의 IPE 식 :<br/> IPE를 하기 전에 우선 Gaussian을 radius-2의 구 안으로 contract 해야 해서<br/> <code class="language-plaintext highlighter-rouge">어차피 full covariance matrix</code> \(\Sigma\) 가 필요하므로<br/> PE-basis를 아래와 같이 둔다<br/> (\(P\) 의 각 column이 twice-tessellated icosahedron(두 번 테셀레이션 된 정이십면체)의 unit-norm vertex이고, 음의 같은 방향으로 중복된 vertex는 제거)<br/> \(P = \begin{bmatrix} 0.8506508 &amp; 0 &amp; 0.5257311 \\ 0.809017 &amp; 0.5 &amp; 0.309017 \\ 0.5257311 &amp; 0.8506508 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0.809017 &amp; 0.5 &amp; -0.309017 \\ 0.8506508 &amp; 0 &amp; -0.5257311 \\ 0.309017 &amp; 0.809017 &amp; -0.5 \\ 0 &amp; 0.5257311 &amp; -0.8506508 \\ 0.5 &amp; 0.309017 &amp; -0.809017 \\ 0 &amp; 1 &amp; 0 \\ -0.5257311 &amp; 0.8506508 &amp; 0 \\ -0.309017 &amp; 0.809017 &amp; -0.5 \\ 0 &amp; 0.5257311 &amp; 0.8506508 \\ -0.309017 &amp; 0.809017 &amp; 0.5 \\ 0.309017 &amp; 0.809017 &amp; 0.5 \\ 0.5 &amp; 0.309017 &amp; 0.809017 \\ 0.5 &amp; -0.309017 &amp; 0.809017 \\ 0 &amp; 1 &amp; 0 \\ -0.5 &amp; 0.309017 &amp; 0.809017 \\ -0.809017 &amp; 0.5 &amp; 0.309017 \\ -0.809017 &amp; 0.5 &amp; -0.309017 \end{bmatrix}\)</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/9-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/9-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> column of PE-basis P </div> <ul> <li>Mip-NeRF 360의 IPE 식 : <ul> <li><code class="language-plaintext highlighter-rouge">off-axis feature</code>를 쓰면 <code class="language-plaintext highlighter-rouge">anisotropic Gaussian의 모양</code>까지 잘 encode할 수 있어서 rendering quality 향상</li> <li>IPE 식에서 \(diag(\Sigma_{r}) = diag(P \Sigma P^T)\) 를 계산할 때<br/> large \(P\) matrix에 대해 행렬곱을 하려면 너무 비싸므로<br/> \(diag(P \Sigma P^T)\) 대신 \(\text{sum}(P^T \circledast (\Sigma P^T), \text{dim}=0, \text{keepdim=False})\) 로 계산하면<br/> Mip-NeRF의 Axis-Aligned IPE보다 Mip-NeRF 360의 Off-Axis IPE가 살짝만 더 expensive</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/10-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/10-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 왼쪽은 Mip-NeRF의 Axis-Aligned IPE이고, 오른쪽은 Mip-NeRF 360의 Off-Axis IPE, 각 encoding basis로 Gaussian을 projection해서 marginal distribution을 구했을 때 Off-Axis Projection을 해야 Gaussian shape를 더 잘 구분할 수 있음 </div> <h3 id="annealing-weight">Annealing Weight</h3> <ul> <li> <p>proposal-weight PDF로 fine-sampling할 때<br/> proposal-weight \(\hat w\) 대신 \(\hat w_n \propto \hat w^{\frac{bn/N}{(b-1)n/N+1}}\)<br/> where \(n\) iter. out of \(N\) steps<br/> where 지수부분 \(\frac{bn/N}{(b-1)n/N+1}\) 은 0부터 1까지 빠르게 증가<br/> where bias hyper-param. \(b=10\)</p> </li> <li> <p>\(n=0\) 일 때는 \(\hat w_0 \propto 1\) 을 따라 sampling하고<br/> \(n=N\) 일 때는 \(\hat w_N \propto \hat w\) 을 따라 sampling하므로<br/> <code class="language-plaintext highlighter-rouge">훈련 초기</code> 단계에서 NeRF-MLP가 더 다양한 proposal-interval 범위를 <code class="language-plaintext highlighter-rouge">모험</code>(exploration)할 수 있도록 함</p> </li> </ul> <h3 id="dilation">Dilation</h3> <ul> <li> <p>proposal-weight PDF로 fine-sampling할 때<br/> aliasing-artifacts 줄이기 위해 먼저 proposal-histogram \((\hat t, \hat w)\) 을 dilate</p> </li> <li> <p>이유 :<br/> proposal-MLP는 오직 input ray로만 supervised되므로<br/> 특정 각도에 대해서만 예측 가능할 수 있다<br/> (<code class="language-plaintext highlighter-rouge">proposal-MLP는 rotationally aliased</code>)</p> </li> <li> <p>해결 : dilation<br/> proposal-MLP의 <code class="language-plaintext highlighter-rouge">interval을 넓힘</code>으로써 anti-aliasing<br/> <code class="language-plaintext highlighter-rouge">왜 interval 넓히면 anti-aliasing??? weight 변화가 급격하지 않아서???</code></p> <ul> <li>Step 1) proposal-MLP의 histogram \((\hat s, \hat w)\) 에서 weight를 구간 길이로 나눈 뒤 normalize해서 probability density \(\hat p\) 로 만듬<br/> \(\hat p_i = \frac{\hat w_i}{(\hat s_{i+1} - \hat s_{i})}\)</li> <li>Step 2) dilation factor (얼만큼 각 구간을 넓힐지) 계산<br/> fine intervals 개수가 많을수록 평균적인 구간 길이가 짧아지고 dilation factor가 작아짐<br/> \(\epsilon = \frac{a}{\prod_{k'=1}^{k-1} n_{k'}} + b\)<br/> where \(k\) 번째 coarse-to-fine resampling 단계에서 \(n_k\) 개의 fine intervals을 resample</li> <li>Step 3) 각 구간 \([\hat s_{i}, \hat s_{i+1}]\) 를 \([\hat s_{i} - \epsilon, \hat s_{i+1} + \epsilon]\) 로 확장</li> <li>Step 4) 확장한 각 구간마다 probability density 최대값을 구함<br/> \(\text{max}_{\hat s - \epsilon \leq s \lt \hat s + \epsilon} \hat p_{\hat s} (s)\)<br/> where \(\hat p_{\hat s} (s)\) is interpolation into the step function defined by \(\hat s, \hat p\) at \(s\)</li> <li>Step 5) 다시 구간 길이로 곱한 뒤 normalize해서 weight-histogram으로 만듬</li> </ul> </li> </ul> <h3 id="sampling">Sampling</h3> <ul> <li>Mip-NeRF : <ul> <li>sampling 방식 :<br/> coarse-histogram으로부터 \(n+1\) 개의 t-distance를 sampling한 뒤<br/> 각 fine-samples를 끝점으로 하여 \(n\) 개의 fine-intervals을 얻음</li> <li>단점 :<br/> 아래 그림과 같이 samples가 coarse-histogram 각 구간의 전범위를 전부 span하지 못하여<br/> 일부 구간이 sampling에서 제외되므로(충분히 다뤄지지 않으므로)<br/> coarse-histogram의 영향을 서서히 약화시킴</li> </ul> </li> <li>Mip-NeRF 360 : <ul> <li>sampling 방식 :<br/> coarse-histogram으로부터 \(n\) 개의 s-distance를 sampling한 뒤<br/> 각 구간의 mid-points \(n-1\) 개와 <code class="language-plaintext highlighter-rouge">coarse-sample의 양끝 점 2개</code>를 끝점으로 하여 \(n\) 개의 fine-intervals를 얻음</li> <li>효과 :<br/> samples가 coarse-histogram의 처음과 끝 구간도 전부 span하고<br/> 불규칙한 resampling도 감소하여<br/> rendering quality는 큰 변화 없지만 aliasing 줄이는 데 도움됨</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-11-MipNeRF360/11-480.webp 480w,/assets/img/2024-08-11-MipNeRF360/11-800.webp 800w,/assets/img/2024-08-11-MipNeRF360/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-11-MipNeRF360/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> sampling한 건 막대로 표시하였고, 이로부터 8개의 fine-intervals를 얻음 </div> <h3 id="background-colors">Background Colors</h3> <ul> <li>NeRF와 Mip-NeRF : <ul> <li>훈련시킬 때 background color를 <code class="language-plaintext highlighter-rouge">black or white</code>로 설정하는데<br/> scene recon.할 때 background가 <code class="language-plaintext highlighter-rouge">반투명</code>하게 reconstruct될 수도 있다</li> <li>문제 :<br/> 반투명한 background는<br/> view synthesis 자체에는 문제가 없지만<br/> ray-termination-distance가 부정확하여<br/> <code class="language-plaintext highlighter-rouge">부정확한 depth-map</code>을 생성할 수 있다</li> </ul> </li> <li>Mip-NeRF 360 : <ul> <li>Blender dataset :<br/> 이전과 똑같이 black or white로 background color 설정</li> <li>360 and LLFF dataset :<br/> 훈련시킬 때 background color를 \([0, 1]^3\) 사이의 <code class="language-plaintext highlighter-rouge">random color</code>로 설정하여<br/> scene recon.할 때 fully-opaque background이도록 함</li> </ul> </li> </ul> <h3 id="implementation-details">Implementation Details</h3> <ul> <li>\(\mu, \Sigma\) 를 갖는 Gaussian에 non-linear \(f\) 를 적용하여 \(\mu_{c}, \Sigma_{c}\) 를 갖는 Gaussian으로 변형하기 위해<br/> \((\mu_{c}, \Sigma_{c}) = (f(\mu), J_{f}(\mu) \Sigma J_{f}(\mu)^T)\)<br/> where \(f(x) \approx f(\mu) + J_{f}(\mu)(x-\mu)\) (linear approx.)</li> <li>이 때, \(J_{f}(\mu)\) matrix는 autodiff framework로 직접 계산할 수 있지만<br/> 사실 우리는 직접 explicitly matrix를 만들 필요가 없음</li> <li><code class="language-plaintext highlighter-rouge">less expensive 계산</code> 위해<br/> \(J_{f}(\mu)\)와 행렬곱하는 것과 똑같은 역할을 하는 함수를 이용<br/> e.g. \(J_{f}(\mu) \Sigma J_{f}(\mu)^T\) 계산 위해<br/> <code class="language-plaintext highlighter-rouge">Jax의 linearize</code> operator <d-cite key="Jax">[4]</d-cite>를 두 번 적용</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="rendering"/><category term="3d"/><category term="multiscale"/><category term="antialiasing"/><summary type="html"><![CDATA[Unbounded Anti-Aliased Neural Radiance Fields]]></summary></entry><entry><title type="html">Multi-Modal Study</title><link href="https://semyeong-yu.github.io/blog/2024/Multimodal/" rel="alternate" type="text/html" title="Multi-Modal Study"/><published>2024-08-05T15:00:00+00:00</published><updated>2024-08-05T15:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Multimodal</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Multimodal/"><![CDATA[<h2 id="multi-modal-study">Multi-Modal Study</h2> <h2 id="storyimager---a-unified-and-efficient-framework-for-coherent-story-visualization-and-completion">StoryImager - A Unified and Efficient Framework for Coherent Story Visualization and Completion</h2> <h4 id="ming-tao-bing-kun-bao-hao-tang-yaowei-wang-changsheng-xu">Ming Tao, Bing-Kun Bao, Hao Tang, Yaowei Wang, Changsheng Xu</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2404.05979">StoryImager</a></p> </blockquote> <ul> <li><code class="language-plaintext highlighter-rouge">Story Visualization</code> Task : <ul> <li>input : prince image / cat image / prompts</li> <li>output : story images</li> <li>video generation과는 다른 게, story visualization은 웹툰 같다고 생각하면 됨<br/> story visualization은 image 간의 consistency를 유지하긴 하지만, video generation처럼 frame끼리 연속성을 보장할 필요는 없음</li> </ul> </li> <li>StoryImager: <ul> <li>task : coherent story visualization and completion</li> <li>기존 모델은 visualization과 continuation을 위한 model을 별도로 필요했는데,<br/> 본 논문은 single model (<code class="language-plaintext highlighter-rouge">통합적인 framework</code>) 제시<br/> by <code class="language-plaintext highlighter-rouge">global consistency</code> 반영!</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/StoryImager/1-480.webp 480w,/assets/img/2024-08-05-Multimodal/StoryImager/1-800.webp 800w,/assets/img/2024-08-05-Multimodal/StoryImager/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/StoryImager/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Architecture :<br/> maintain <code class="language-plaintext highlighter-rouge">global consistency</code><br/> by context-feature-extractor<br/> and FS-CAM (frame-story cross-attention module)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/StoryImager/2-480.webp 480w,/assets/img/2024-08-05-Multimodal/StoryImager/2-800.webp 800w,/assets/img/2024-08-05-Multimodal/StoryImager/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/StoryImager/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p><code class="language-plaintext highlighter-rouge">random masking</code> :<br/> make a story board from story images<br/> \(\rightarrow\) VAE<br/> \(\rightarrow\) random masking to VAE latent space</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">context-feature-extractor</code> :</p> <ul> <li>word-embeddings \(\rightarrow\) transformer<br/> \(\rightarrow\) prior embeddings<br/> \(\rightarrow\) MLP<br/> \(\rightarrow\) frame-aware latent prior<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">story board images가 story prompts를 반영하도록</code> 하기 위해 masked VAE latent space와 concat해서 이를 FS-CAM에서 story board로 사용</li> <li>word-embeddings \(\rightarrow\) transformer<br/> \(\rightarrow\) context embeddings<br/> \(\rightarrow\) transformer<br/> \(\rightarrow\) global context<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">global 정보</code> 주기 위해 FS-CAM에서 text prompts로 사용</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/StoryImager/4-480.webp 480w,/assets/img/2024-08-05-Multimodal/StoryImager/4-800.webp 800w,/assets/img/2024-08-05-Multimodal/StoryImager/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/StoryImager/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>FS-CAM (<code class="language-plaintext highlighter-rouge">frame-story cross-attention module</code>) :<br/> 개별 story board - 개별 text prompts 를 locally cross-attention한 것과,<br/> 전체 story board - 전체 text prompts 를 globally cross-attention한 것을<br/> concat</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/StoryImager/3-480.webp 480w,/assets/img/2024-08-05-Multimodal/StoryImager/3-800.webp 800w,/assets/img/2024-08-05-Multimodal/StoryImager/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/StoryImager/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="intelligent-grimm---open-ended-visual-storytelling-via-latent-diffusion-models">Intelligent Grimm - Open-ended Visual Storytelling via Latent Diffusion Models</h2> <h4 id="chang-liu-haoning-wu-yujie-zhong-xiaoyun-zhang-yanfeng-wang-weidi-xie">Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, Yanfeng Wang, Weidi Xie</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2306.00973">Intelligent Grimm</a></p> </blockquote> <ul> <li>Intelligent Grimm : <ul> <li>NIPS 2023에서 novelty 부족으로 reject 당했다가 보완해서 CVPR 2024에 accept</li> <li>task : open-ended visual storytelling</li> <li>StoryGen : unseen characters에 대해서도 추가적인 character-specific-optimization 없이 story visualization 가능</li> <li>StorySalon : online-video, open-source e-book 등 소싱해서 만든 dataset</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">context encoding</code> : <ul> <li>SDM : pre-trained stable diffusion model<br/> CLIP : pre-trained CLIP encoder<br/> VAE : pre-trained VAE</li> <li>visual condition feature = [SDM(image1, CLIP(text1)), SDM(image2, CLIP(text2)), …, SDM(imagek-1, CLIP(textk-1))]<br/> k-th frame image 만들기 위해 cross-attention 하는 데 사용</li> </ul> </li> <li>visual-language contextual fusion :<br/> <code class="language-plaintext highlighter-rouge">cross-attention</code> 사용<br/> 아래 Fig. (b)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/IntelligentGrimm/1-480.webp 480w,/assets/img/2024-08-05-Multimodal/IntelligentGrimm/1-800.webp 800w,/assets/img/2024-08-05-Multimodal/IntelligentGrimm/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/IntelligentGrimm/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">conditional generation</code> : <ul> <li>prev. frame t-1 \(\rightarrow\) add noise<br/> \(\rightarrow\) denoising one step<br/> \(\rightarrow\) diffusion U-Net</li> <li>prev. text \(\rightarrow\) text encoder<br/> \(\rightarrow\) diffusion U-Net</li> <li>diffusion U-Net (self-attention, text-attention)<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">denoising one step에 해당되는 feature</code>를 추출</li> <li>extracted image-diffusion-denoising-feature<br/> &amp; random noise<br/> &amp; current text \(\rightarrow\) text encoder<br/> \(\rightarrow\) StoryGen model (self-attention, image-attention, text-attention)<br/> \(\rightarrow\) current frame t</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">multi-frame conditioning</code> :<br/> story의 경우 frame t에 영향을 주는 image들이 frame t-1, frame t-2, … 일 수 있음<br/> 이 때, 과거 frames에 모두 같은 random noise를 줄 경우 성능 좋지 않아서<br/> <code class="language-plaintext highlighter-rouge">현재에 가까운 과거 frame일수록 noise를 덜 주는 식으로 temporal order를 부여</code>하면 성능 좋음</li> </ul> <h2 id="generating-realistic-images-from-in-the-wild-sounds">Generating Realistic Images from In-the-wild Sounds</h2> <h4 id="taegyeong-lee-jeonghun-kang-hyeonyu-kim-taehwan-kim">Taegyeong Lee, Jeonghun Kang, Hyeonyu Kim, Taehwan Kim</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2309.02405">Image from in-the-wild Sounds</a></p> </blockquote> <ul> <li> <p>novelty :<br/> 이전까지는 wild sound와 image 간의 pair가 없어서 limited categories와 music의 sound로부터 image를 생성하는 연구만 진행되었음<br/> 본 논문은 sound와 image 간의 large paired dataset이 없더라도<br/> wild sound로부터 image를 생성하는 task를 최초로 제시</p> </li> <li> <p>method :</p> <ul> <li>stage (a) :<br/> <code class="language-plaintext highlighter-rouge">audio captioning</code>을 통해 sound를 text로 변환한 audio caption과<br/> sound의 dynamic 특성을 반영하기 위한 <code class="language-plaintext highlighter-rouge">audio attention</code>과<br/> 제대로 image visualization하기 위한 <code class="language-plaintext highlighter-rouge">sentence attention</code>을<br/> 함께 사용하여 positional encoding을 거친 뒤 vector w를 <code class="language-plaintext highlighter-rouge">initialize</code><br/> (이 때, Audio-Captioning-Transformer model의 decoder에서 나오는 확률값을 audio attention이라고 정의함)</li> <li>stage (b) :<br/> audio caption으로부터 만든 vector z와<br/> stage (a)의 vector w로부터<br/> new latent vector z’를 만들고,<br/> <code class="language-plaintext highlighter-rouge">stable-diffusion model</code>을 이용하여 이로부터 image를 생성한다<br/> 여기서 image와 vector z 간의 <code class="language-plaintext highlighter-rouge">CLIPscore similarity</code>를 이용해서 audio caption으로부터 만든 vector z를 optimize하고<br/> image와 audio 간의 <code class="language-plaintext highlighter-rouge">AudioCLIP similarity</code>를 이용해서 <code class="language-plaintext highlighter-rouge">audio를 직접 optimize</code>한다<br/> (image가 text에 맞게 생성되도록 image를 점점 변화시키면서 생성하는 Style-CLIP에서 영감을 받아 이를 diffusion model에 적용)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/ImageSound/1-480.webp 480w,/assets/img/2024-08-05-Multimodal/ImageSound/1-800.webp 800w,/assets/img/2024-08-05-Multimodal/ImageSound/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/ImageSound/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>local minimum에 빠지지 않기 위해 audio attention과 sentence attention을 이용한 stage (a)의 initialization이 매우 중요</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/ImageSound/2-480.webp 480w,/assets/img/2024-08-05-Multimodal/ImageSound/2-800.webp 800w,/assets/img/2024-08-05-Multimodal/ImageSound/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/ImageSound/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Q1 :<br/> image는 pixel 단위로 값이 있어서 feature map을 통해 vector로 만들 수 있고, text는 word 단위로 embedding을 통해 vector를 만들 수 있습니다. AudioCLIP을 통해 audio를 직접 optimize했다는데 audio는 무엇을 기준으로 vector로 만들어서 optimize 가능한 건가요?</p> </li> <li> <p>A1 :<br/> audio는 melspectrogram을 만든 뒤 ViT에서 image 다루듯이 똑같이 patch로 쪼개서 vector로 만든다<br/> AudioCLIP similarity의 경우 audio encoding과 image encoding과 text encoding 간의 contrastive learning을 통해 구할 수 있다</p> </li> </ul> <h2 id="vivit---a-video-vision-transformer">ViViT - A Video Vision Transformer</h2> <h4 id="anurag-arnab-mostafa-dehghani-georg-heigold-chen-sun-mario-lucic-cordelia-schmid">Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, Cordelia Schmid</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2103.15691">ViViT</a></p> </blockquote> <ul> <li> <p>video of (T, H, W, C)를 sampling하여<br/> token sequence of (n_t, n_h, n_w, C) 을 만들고<br/> positional embedding을 더한 뒤 (N, d)로 reshape해서 transformer의 input으로 넣어줌</p> </li> <li> <p>uniform frame sampling :<br/> ViT에서처럼 각 2D frame을 독립적으로 embedding 후 모든 token을 concat</p> </li> <li> <p>Tubelet sampling :<br/> temporal info.를 반영하기 위해 tokenization 단계에서 spatial, temporal info.를 fuse</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/ViViT/1-480.webp 480w,/assets/img/2024-08-05-Multimodal/ViViT/1-800.webp 800w,/assets/img/2024-08-05-Multimodal/ViViT/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/ViViT/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/ViViT/2-480.webp 480w,/assets/img/2024-08-05-Multimodal/ViViT/2-800.webp 800w,/assets/img/2024-08-05-Multimodal/ViViT/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/ViViT/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Model 1 :<br/> CNN과 달리 transformer layer는 token 수에 비례하게 quadratic complexity를 가지므로 input frame에 linearly 필요</p> </li> <li> <p>Model 2 (factorized encoder) :<br/> spatial과 temporal을 두 개의 transformer encoder로 구성하여 많은 연산량 필요</p> </li> <li> <p>Model 3 (factorized self-attention) :<br/> 여전히 두 개의 encoder로 특정 dim만 뽑아서 attention 연산</p> </li> <li> <p>Model 4 (factorized dot-product attention) :<br/> spatial head의 경우 spatial-dim.에 대해서만 attention 수행</p> </li> </ul> <h2 id="llama-vid---an-image-is-worth-2-tokens-in-large-language-models">LLaMA-VID - An Image is Worth 2 Tokens in Large Language Models</h2> <ul> <li> <p>task : 주로 Video-QA</p> </li> <li> <p>VLM :</p> <ul> <li>영화 같은 long video understanding</li> <li>token 수가 너무 많아서 문제</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/llamaVID/1-480.webp 480w,/assets/img/2024-08-05-Multimodal/llamaVID/1-800.webp 800w,/assets/img/2024-08-05-Multimodal/llamaVID/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/llamaVID/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>architecture <ul> <li>context attention :<br/> \(E_t = mean(softmax(Q_t X_t^T) X_t)\)</li> <li>context token : from video frame and user question</li> <li>content token : from video frame</li> </ul> </li> <li>contribution <ul> <li>각 video frame을 두 가지의 token으로 나타냄 <ul> <li>context token (one token)</li> <li>content token (one token으로 compressed될 수도 있고 아닐 수도 있음)</li> </ul> </li> <li>hour-long video understanding을 위한 instruction dataset 만듦</li> </ul> </li> </ul> <h2 id="peekaboo---interactive-video-generation-via-masked-diffusion">PEEKABOO - Interactive Video Generation via Masked-Diffusion</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/PEEKABOO/3-480.webp 480w,/assets/img/2024-08-05-Multimodal/PEEKABOO/3-800.webp 800w,/assets/img/2024-08-05-Multimodal/PEEKABOO/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/PEEKABOO/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Video Diffusion Model </div> <ul> <li>기존 video generation diffusion model : <ul> <li>성능 꽤 좋은데 아직 user가 control하기 어려움</li> <li>이전에 spatial control을 적용하려면 전체 network를 training시키거나 adapter로 training시키는 과정이 필요했다</li> <li>본 논문은 추가적인 training 없이 masked attention module을 사용하여<br/> diffusion의 3D UNet을 사용하는 다양한 model에 적용할 수 있는 방법을 제시</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/PEEKABOO/1-480.webp 480w,/assets/img/2024-08-05-Multimodal/PEEKABOO/1-800.webp 800w,/assets/img/2024-08-05-Multimodal/PEEKABOO/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/PEEKABOO/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>contribution : <ul> <li><code class="language-plaintext highlighter-rouge">UNet</code> 기반의 video generation model이라면 <code class="language-plaintext highlighter-rouge">spatio-temporal control</code> 가능<br/> (spatio-temporal control : video가 generated될 때 object size, location, and trajectory 등을 user가 control하는 것)</li> <li><code class="language-plaintext highlighter-rouge">training-free</code></li> <li>no additial latency at inference time</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/PEEKABOO/2-480.webp 480w,/assets/img/2024-08-05-Multimodal/PEEKABOO/2-800.webp 800w,/assets/img/2024-08-05-Multimodal/PEEKABOO/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/PEEKABOO/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Masked attention</code> : <ul> <li>fg에만 attention하도록 만들기 위해</li> <li>\(\text{MaskedAttention}(Q, K, V, M) = \text{softmax}(\frac{QK^T}{d}+M)V\)<br/> where \(M[i, j] = - \infty\) if bg(0)<br/> where \(M[i, j] = 0\) if fg(1)</li> </ul> </li> <li>binary mask : <ul> <li>image :<br/> input BB를 입력으로 받아서 BB object 있는 부분만 fg = 1이 되도록 binary mask \(M_{input}^f[i]\) 를 만들어서 latent size로 downsample<br/> where size of \(n_{frame} \times n_{latents}\)</li> <li>text :<br/> text embedding을 받아서 object 나타내는 단어만 fg = 1이 되도록 mask \(T[j]\)<br/> where size of \(n_{text}\)</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Masked cross attention</code> : <ul> <li>text와 image 간의 attention</li> <li>cross-attention mask :<br/> \(M_{CA}^f[i, j] = fg(M_{input}^f[i]) \ast fg(T[j]) + (1-fg(M_{input}^f[i])) \ast (1-fg(T[j]))\)<br/> where \(fg\) : pixel 또는 text token이 fg이면 1을 반환하고, bg이면 0을 반환<br/> where size of \(n_{latents} \times n_{text}\)</li> <li>cross-attention mask : <ul> <li>image와 text가 둘 다 fg(1)이거나 둘 다 bg(0)이면 1을 반환하고<br/> 둘 중 하나가 fg(1)이고 둘 중 하나가 bg(0)이면 0을 반환</li> <li>즉, <code class="language-plaintext highlighter-rouge">fg와 bg가 서로 attention하지 않도록</code>!<br/> <code class="language-plaintext highlighter-rouge">fg는 fg끼리, bg는 bg끼리 attention하도록</code>!</li> </ul> </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Masked spatial attention</code> : <ul> <li>image self-attention for spatial</li> <li>spatial-attention mask :<br/> \(M_{SA}^f[i, j] = fg(M_{input}^f[i]) \ast fg(M_{input}^f[j]) + (1-fg(M_{input}^f[i])) \ast (1-fg(M_{input}^f[j]))\)<br/> where size of \(n_{latents} \times n_{latents}\)</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Masked temporal attention</code> : <ul> <li>image self-attention for temporal</li> <li>temporal-attention mask :<br/> \(M_{TA}^i[f, k] = fg(M_{input}^f[i]) \ast fg(M_{input}^k[i]) + (1-fg(M_{input}^f[i])) \ast (1-fg(M_{input}^k[i]))\)<br/> where size of \(n_{frame} \times n_{frame}\)</li> </ul> </li> <li>Extension :<br/> image binary mask를 input BB 받아서 manually 만들지 않고<br/> text prompt 넣어주면 LLM이 대신 만들어줄 수 있음 (VideoDirectorGPT와 유사)<br/> \(\rightarrow\)<br/> 그럼 text prompt만 입력으로 넣어주면 user control이 가능한 video를 생성할 수 있음!</li> </ul> <h2 id="controlnet---adding-conditional-control-to-text-to-image-diffusion-models">ControlNet - Adding Conditional Control to Text-to-Image Diffusion Models</h2> <h2 id="instructpix2pix---learning-to-follow-image-editing-instructions">InstructPix2Pix - Learning to Follow Image Editing Instructions</h2>]]></content><author><name></name></author><category term="generative"/><category term="multi-modal"/><category term="generative"/><summary type="html"><![CDATA[Paper Review]]></summary></entry><entry><title type="html">NeRF-Code</title><link href="https://semyeong-yu.github.io/blog/2024/NeRFcode/" rel="alternate" type="text/html" title="NeRF-Code"/><published>2024-08-05T15:00:00+00:00</published><updated>2024-08-05T15:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/NeRFcode</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/NeRFcode/"><![CDATA[<h2 id="nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h2> <h4 id="ben-mildenhall-pratul-psrinivasan-matthew-tancik">Ben Mildenhall, Pratul P.Srinivasan, Matthew Tancik</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2003.08934">https://arxiv.org/abs/2003.08934</a><br/> project website :<br/> <a href="https://www.matthewtancik.com/nerf">https://www.matthewtancik.com/nerf</a><br/> pytorch code :<br/> <a href="https://github.com/yenchenlin/nerf-pytorch">https://github.com/yenchenlin/nerf-pytorch</a><br/> <a href="https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file">https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file</a><br/> tiny tensorflow code :<br/> <a href="https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb">https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb</a><br/> Overview image reference :<br/> <a href="https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#dataflow">https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#dataflow</a></p> </blockquote> <p>NeRF code는 빠른 실행을 위해 lower-level framework인 jax와 jit compile로 짜여진 버전도 있는데,<br/> 본 포스팅에서는 좀 더 익숙한 numpy, Pytorch framework로 코드 리뷰를 진행하였다</p> <h2 id="train-code-flow-overview">Train Code Flow Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/1-480.webp 480w,/assets/img/2024-08-05-NeRFcode/1-800.webp 800w,/assets/img/2024-08-05-NeRFcode/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="load-data">Load Data</h2> <ul> <li>load data : <ul> <li>load_llff.py</li> <li>load_blender.py</li> <li>load_LINEMOD.py</li> <li>load_deepvoxels.py</li> </ul> </li> </ul> <h3 id="load_llff_data">load_llff_data()</h3> <ul> <li>LLFF dataset : real dataset<br/> return images, poses, bds, render_poses, i_test <ul> <li>images : np (N, H, W, C)</li> <li>poses : np (N, 3, 5)<br/> camera poses<br/> poses[:, 0:3, 0:3] : 3-by-3 rotation matrix<br/> poses[:, 0:3, 3:4] : 3-by-1 translation matrix<br/> poses[:, 0:3, 4:5] : H, W, focal-length for intrinsic matrix</li> <li>bds : np (N, 2)<br/> scene bounds<br/> dim=1 : 2 = 1(near bound) + 1(far bound)</li> <li>render_poses : np (M, 3, 5)<br/> dim=0 : the number of generated poses for novel view synthesis<br/> generate new pose along sphere or spiral path</li> <li>i_test : int<br/> index of holdout-view (avg pose랑 가장 비슷한 pose를 갖는 view)<br/> training에서 제외하여 test할 때 사용</li> <li>near, far = 0., 1. if ndc is true else near, far = 0.9 * bds.min(), 1. * bds.max()</li> </ul> </li> </ul> <h3 id="load_blender_data">load_blender_data()</h3> <ul> <li>Blender dataset : synthetic dataset<br/> return images, poses, render_poses, hwf, i_split <ul> <li>images : np (N, H, W, C)<br/> blender dataset은 RGB-A channel을 가지고 있어 C = 4</li> <li>i_train, i_val, i_test = i_split</li> <li>near, far = 2., 6.<br/> (blender synthetic dataset은 통제된 환경에서 수집된 data이므로 ndc 사용하지 않고 frustum의 near, far plane 고정)</li> <li>투명한 배경을 흰 배경으로 만들려면<br/> RGB * opacity + (1 - opacity) 를 통해<br/> RGB 값을 opacity만큼 반영하고 opacity가 작을수록(투명할수록) 색상이 흰색(1.)에 가까워지도록 함<br/> images = images[…,:3]*images[…,-1:] + (1.-images[…,-1:])</li> <li>그냥 투명한 배경 그대로 쓰려면<br/> RGB-A channel에서 RGB channel만 가져와서 씀<br/> images = images[…,:3]</li> </ul> </li> </ul> <h3 id="load_linemod_data">load_LINEMOD_data()</h3> <ul> <li>LINEMOD dataset : real dataset<br/> return images, poses, render_poses, hwf, K, i_split, near, far</li> </ul> <h3 id="load_dv_data">load_dv_data()</h3> <ul> <li>Deepvoxels dataset : synthetic dataset<br/> return images, poses, render_poses, hwf, i_split <ul> <li>near, far = hemi_R - 1., hemi_R + 1.<br/> where hemi_R = np.mean(np.linalg.norm(poses[:,:3,-1], axis=-1))<br/> camera center들로 이루어진 반구의 평균 반지름</li> </ul> </li> </ul> <h2 id="create-nerf-model">Create NeRF Model</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/4-480.webp 480w,/assets/img/2024-08-05-NeRFcode/4-800.webp 800w,/assets/img/2024-08-05-NeRFcode/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>args.N_importance : fine-MLP에서 추가적으로 사용할 fine-sample 개수 <ul> <li>args.N_importance &gt; 0 : fine-MLP 사용함</li> <li>args.N_importance &lt;= 0 : fine-MLP 사용 안함</li> </ul> </li> <li>network_query_fn : 추후에 run_network() 사용하기 위한 함수 <ul> <li>input : position info., view-direction info., model</li> <li>output : model output</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/10-480.webp 480w,/assets/img/2024-08-05-NeRFcode/10-800.webp 800w,/assets/img/2024-08-05-NeRFcode/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>render_kwargs_train : dict for rendering <ul> <li>network_query_fn : 추후에 run_network() 사용하기 위한 함수</li> <li>perturb : 일반화 위해 stratified ray-sampling할 때 randomness 추가할지 여부<br/> (test할 때는 False)</li> <li>network_fine, network_fn : fine-MLP, coarse-MLP</li> <li>N_importance, N_samples : number of fine-sampling, coarse-sampling</li> <li>white_bkgd : rendering에서 alpha-channel 사용할 때 투명한 부분이 흰색으로 채워지도록 할지 여부</li> <li>raw_noise_std : regularize(artifacts 완화) 위해 raw2ouputs()에서 model output 중 opacity에 추가할 noise의 std값<br/> (test할 때는 0.)</li> <li>lindisp : <ul> <li>NDC를 사용하는 front-unbounded llff dataset의 경우 lindisp = False로 설정하여<br/> linearly sampling in depth, 즉 depth를 균등하게 sampling하여<br/> 먼 거리의 scene도 적절히 표현</li> <li>NDC를 사용하지 않는 나머지 dataset의 경우 lindisp = True로 설정하여<br/> linearly sampling in inverse-depth, 즉 가까운 depth를 더 많이 sampling하여<br/> 가까운 scene의 디테일을 잘 포착</li> </ul> </li> </ul> </li> </ul> <h3 id="positional-encoding">Positional Encoding</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/2-480.webp 480w,/assets/img/2024-08-05-NeRFcode/2-800.webp 800w,/assets/img/2024-08-05-NeRFcode/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>get_embedder() input :<br/> PE freq. 개수 \(L\) 과 PE 쓸지말지 여부</li> <li>get_embedder() output :<br/> PE-function과 PE 결과의 dim.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/3-480.webp 480w,/assets/img/2024-08-05-NeRFcode/3-800.webp 800w,/assets/img/2024-08-05-NeRFcode/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>self.embed_fns :<br/> 각 frequency(\(0 \sim 2^{L-1}\))와 각 period function(\(sin, cos\))에 대한<br/> list of lambda functions<br/> \([sin(2^0x), cos(2^0x), \ldots sin(2^{L-1}x), cos(2^{L-1}x)]\)</li> <li>Embedder.embed(x) :<br/> self.embed_fns의 각 PE-function을 input x에 적용하여 dim=-1에 대해 concat</li> </ul> <h3 id="nerf-model">NeRF model</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/5-480.webp 480w,/assets/img/2024-08-05-NeRFcode/5-800.webp 800w,/assets/img/2024-08-05-NeRFcode/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>input_ch : position info. dim. : 3</li> <li>input_ch_views : view-direction info. dim. : 3</li> <li>use_viewdirs : MLP input으로 view-direction info.를 사용할지 말지 여부<br/> (view-direction info.를 사용하면 RGB color 계산에 도움됨)</li> <li>output_ch : output(RGB, opacity) dim. : 4 use_viewdirs가 False일 때만 사용하는 값</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/6-480.webp 480w,/assets/img/2024-08-05-NeRFcode/6-800.webp 800w,/assets/img/2024-08-05-NeRFcode/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>input x를 position info.와 view-direction info.로 쪼갬</li> <li>self.use_viewdirs가 True일 때(view-direction info. 사용할 때) :<br/> position info.만 넣어서 opacity를 뽑은 뒤<br/> view-direction info.를 추가로 넣어서 RGB 뽑고<br/> dim=-1에 대해 concat</li> <li>self.use_viewdirs가 False일 때(view-direction info. 사용 안 할 때) :<br/> position info.만 넣어서 output_ch만큼 한 번에 뽑음</li> </ul> <h3 id="run_network">run_network</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/9-480.webp 480w,/assets/img/2024-08-05-NeRFcode/9-800.webp 800w,/assets/img/2024-08-05-NeRFcode/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/7-480.webp 480w,/assets/img/2024-08-05-NeRFcode/7-800.webp 800w,/assets/img/2024-08-05-NeRFcode/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>flatten position and flatten view-direction \(\rightarrow\) each positional encoding and concat \(\rightarrow\) batchify model and apply model \(\rightarrow\) reshape again output</li> </ul> <h3 id="batchify">batchify</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/8-480.webp 480w,/assets/img/2024-08-05-NeRFcode/8-800.webp 800w,/assets/img/2024-08-05-NeRFcode/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>input이 주어지면 chunk만큼씩 쪼개서 적용하는 model 반환</li> </ul> <h2 id="get-ray-with-batch">Get Ray with batch</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/11-480.webp 480w,/assets/img/2024-08-05-NeRFcode/11-800.webp 800w,/assets/img/2024-08-05-NeRFcode/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>rays : shape (N, 2, H, W, 3) <ul> <li>dim=1 : rays_o, rays_d</li> <li>dim=2, 3 : for H*W개의 pixels</li> <li>dim=4 : 3d</li> </ul> </li> <li>rays_rgb : shape (N, 3, H, W, 3) after concat with images <ul> <li>dim=1 : rays_o, rays_d, images</li> </ul> </li> <li>rays_rgb : shape (N, H, W, 3, 3) \(\rightarrow\) (N_train, H, W, 3, 3) \(\rightarrow\) (N_train * H * W, 3, 3) \(\rightarrow\) shuffle along dim=0 <ul> <li>dim=0 : the number of rays(pixels)</li> <li>dim=1 : rays_o, rays_d, images</li> <li>dim=2 : 3d for rays and rgb for images</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/14-480.webp 480w,/assets/img/2024-08-05-NeRFcode/14-800.webp 800w,/assets/img/2024-08-05-NeRFcode/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>batch : N_train * H * W 개의 ray를 batch size = N_rand-개씩 묶어서 전부 사용<br/> shape (N_train * H * W, 3, 3) \(\rightarrow\) shape (N_rand, 3, 3) \(\rightarrow\) (3, N_rand, 3)</li> <li>batch_rays : shape (2, N_rand, 3) <ul> <li>dim=0 : rays_o, rays_d</li> <li>dim=1 : the number of rays</li> </ul> </li> <li>target_s : shape (N_rand, 3) <ul> <li>dim=0 : the number of pixels</li> <li>dim=1 : target pixel RGB</li> </ul> </li> <li>shuffle rays_rgb by torch.randperm() for every epoch</li> </ul> <h3 id="get_rays_np">get_rays_np</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/12-480.webp 480w,/assets/img/2024-08-05-NeRFcode/12-800.webp 800w,/assets/img/2024-08-05-NeRFcode/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>parameter :<br/> K : intrinsic matrix of shape (3, 3)<br/> c2w : extrinsic matrix of shape (3, 4)</li> <li>line 1 :<br/> np.meshgrid([0, …, W-1], [0, …, H-1], indexing=’xy’) <ul> <li>indexing=’xy’ : 첫 번째 array를 row-방향으로 반복하고, 두 번째 array를 column-방향으로 반복</li> <li>i, j : both shape (H, W) : 2D-pixel-coordinate (x, y)</li> </ul> </li> <li>line 2 : <ul> <li>apply intrinsic matrix<br/> <a href="https://semyeong-yu.github.io/blog/2024/NeRF/">NeRF-Blog</a> 의 Ray from input image (pre-processing) 참고</li> <li>dirs : shape (H, W, 3) : 2D-normalized-coordinate</li> </ul> </li> <li>line 4 : <ul> <li>apply extrinsic matrix to calculate ray-direction</li> <li>dirs[…, np.newaxis, :] : shape (H, W, 1, 3) \(\rightarrow\) (H, W, 3, 3) by broad-casting</li> <li>c2w[:3, :3] : shape (3, 3) \(\rightarrow\) (H, W, 3, 3) by broad-casting</li> <li>ray_d : shape (H, W, 3)<br/> “elementwise-multiplication 후 sum”은 “matrix-multiplication”과 동일한 계산</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/13-480.webp 480w,/assets/img/2024-08-05-NeRFcode/13-800.webp 800w,/assets/img/2024-08-05-NeRFcode/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> * 오타 정정 : 2. matrix multiplication에서 [u, v, 1; u, v, 1; u, v, 1] 대신 [u; v; 1] </div> <ul> <li>line 6 : <ul> <li>apply extrinsic matrix to calculate ray-origin</li> <li>rays_o : shape (3,) \(\rightarrow\) (H, W, 3) by broad-casting</li> </ul> </li> </ul> <h2 id="get-ray-without-batch">Get Ray without batch</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/15-480.webp 480w,/assets/img/2024-08-05-NeRFcode/15-800.webp 800w,/assets/img/2024-08-05-NeRFcode/15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>차이점 :<br/> Get Ray with batch에서는 N_train * H * W 개의 ray를 batch size = N_rand-개씩 묶어서 전부 사용했다면<br/> Get Ray without batch에서는 N_train 중 training view 하나를 randomly 고른 뒤 H * W 개의 ray 중 N_rand-개를 randomly 골라서 사용</p> </li> <li>target : shape (N, H, W, C) \(\rightarrow\) (H, W, C)<br/> \(\rightarrow\) target_s : shape (N_rand, C)</li> <li>coords : H * W 개의 ray를 H-axis와 W-axis에서 인덱싱하기 위해 meshgrid of shape (H, W, 2) 생성 <ul> <li>초반부 iter. : 중심부 crop해서 meshgrid of shape (2 * dH, 2 * dW, 2) 생성</li> <li>후반부 iter. : meshgrid of shape (H, W, 2) 생성</li> <li>dim=2 : coords[:, :, 0]은 H-coord이고, coords[:, :, 1]은 W-coord</li> </ul> </li> <li>select_coords : shape (N_rand, 2)<br/> H * W 개의 ray 중 N_rand-개를 randomly 고름</li> <li>batch_rays : shape (2, N_rand, 3)</li> <li>target_s : shape (N_rand, 3)</li> </ul> <h2 id="render">Render</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/16-480.webp 480w,/assets/img/2024-08-05-NeRFcode/16-800.webp 800w,/assets/img/2024-08-05-NeRFcode/16-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/17-480.webp 480w,/assets/img/2024-08-05-NeRFcode/17-800.webp 800w,/assets/img/2024-08-05-NeRFcode/17-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/17.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>input : <ul> <li>chunk : 동시에 처리할 수 있는 최대 ray 수 (due to maximum memory usage)</li> <li>c2w_staticcam : view-direction의 영향을 확인하고자 할 때 사용<br/> 기존 c2w는 view-direction MLP input 만드는 데만 사용하고<br/> c2w_staticcam으로 rendering 위한 rays_o, rays_d 다시 계산</li> </ul> </li> <li>output : <ul> <li>rgb_map : shape (B, 3)<br/> predicted RGB values for B개의 rays</li> <li>disp_map : shape (B,)<br/> disparity map (inverse of depth)</li> <li>acc_map : shape (B,)<br/> sum of sample weights along each ray</li> <li>extras : 나머지 dict from render_rays()<br/> fine-MLP를 사용하는 경우에만 존재 <ul> <li>rgb0, disp0, acc0 : from coarse-MLP</li> <li>z_std : shape (B,)<br/> std of distances (\(t\) 값) of fine samples for each ray</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/19-480.webp 480w,/assets/img/2024-08-05-NeRFcode/19-800.webp 800w,/assets/img/2024-08-05-NeRFcode/19-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/19.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>rays : <ul> <li>if use_viewdirs = True : shape (N_rand, 8)<br/> dim=1 : 3(rays_o) + 3(rays_d) + 1(near) + 1(far)</li> <li>if use_viewdirs = False : shape (N_rand, 11)<br/> dim=1 : 3(rays_o) + 3(rays_d) + 1(near) + 1(far) + 3(viewdirs)</li> </ul> </li> <li>all_ret : dict <ul> <li>rgb_map : shape (N_rand, 3)</li> <li>disp_map : shape (N_rand,)</li> <li>acc_map : shape (N_rand,)</li> <li>raw : MLP raw output (raw2outputs() 안 한 것)</li> <li>rgb0, disp0, acc0 : from coarse-MLP</li> <li>z_std : shape (N_rand,)</li> </ul> </li> <li>render() output :<br/> rgb_map, disp_map, acc_map, (나머지 모아놓은)-dict</li> </ul> <h3 id="ndc_rays">ndc_rays</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/18-480.webp 480w,/assets/img/2024-08-05-NeRFcode/18-800.webp 800w,/assets/img/2024-08-05-NeRFcode/18-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/18.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>shift ray origin to near plane :<br/> NDC를 적용하기 전에 3D ray origin \(o\) 을 near plane 위 \(o_n\) 으로 옮긴다<br/> (world-coordinate에서 ray가 near plane에서 출발하도록)<br/> by \(o_n = o + t_nd\)<br/> where z-axis에서는 \(-n = o_z + t_nd_z\) 이므로 \(t_n = \frac{-(n+o_z)}{d_z}\)<br/> where n은 argument(near)</p> </li> <li> <p>project ray to NDC-space :<br/> ray \(r = o_n + td\) 를 NDC로 projection했을 때<br/> projected ray \(r^{\ast} = o^{\ast} + t^{\ast} d^{\ast}\) 에서<br/> \(o^{\ast} = \begin{bmatrix} -\frac{f_{cam}}{\frac{W}{2}}\frac{o_{n_x}}{o_{n_z}} \\ -\frac{f_{cam}}{\frac{H}{2}}\frac{o_{n_y}}{o_{n_z}} \\ 1 + \frac{2n}{o_{n_z}} \end{bmatrix}\) where n은 argument(near)<br/> and<br/> \(t^{\ast} = \frac{td_z}{o_{n_z} + td_z} = 1 - \frac{o_{n_z}}{o_{n_z} + td_z}\)<br/> and<br/> \(d^{\ast} = \begin{bmatrix} -\frac{f_{cam}}{\frac{W}{2}}(\frac{d_x}{d_z} - \frac{o_{n_x}}{o_{n_z}}) \\ -\frac{f_{cam}}{\frac{H}{2}}(\frac{d_y}{d_z} - \frac{o_{n_y}}{o_{n_z}}) \\ -2n\frac{1}{o_{n_z}} \end{bmatrix}\) where n은 argument(near)</p> </li> </ul> <h3 id="batchify_rays">batchify_rays</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/20-480.webp 480w,/assets/img/2024-08-05-NeRFcode/20-800.webp 800w,/assets/img/2024-08-05-NeRFcode/20-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/20.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Out-of-Memory를 방지하기 위해 N_rand-개의 rays를 더 작은 chunk (B개)로 쪼개서 rendering</p> </li> <li>ret : render_rays()의 output<br/> dict <ul> <li>rgb_map : shape (B, 3)<br/> predicted RGB values by alpha-compositing</li> <li>disp_map : shape (B,)<br/> disparity map (inverse of depth)</li> <li>acc_map : shape (B,)<br/> sum of sample weights along each ray</li> <li>raw : MLP raw output (raw2outputs() 안 한 것)</li> <li>rgb0, disp0, acc0 : from coarse-MLP</li> <li>z_std : shape (B,)<br/> std of distances (\(t\) 값) of fine-samples for each ray</li> </ul> </li> <li>all_ret : B-개씩 쪼개서 rendering한 걸 다시 N_rand-개로 합침<br/> dict <ul> <li>rgb_map : shape (N_rand, 3)</li> <li>disp_map : shape (N_rand,)</li> <li>acc_map : shape (N_rand,)</li> <li>raw : MLP raw output (raw2outputs() 안 한 것)</li> <li>rgb0, disp0, acc0 : from coarse-MLP</li> <li>z_std : shape (N_rand,)</li> </ul> </li> </ul> <h3 id="render_rays">render_rays</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/21-480.webp 480w,/assets/img/2024-08-05-NeRFcode/21-800.webp 800w,/assets/img/2024-08-05-NeRFcode/21-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/21.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>ray_batch of shape (B, 8) or (B, 11)로부터<br/> rays_o, rays_d, near, far, viewdirs 분리</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/22-480.webp 480w,/assets/img/2024-08-05-NeRFcode/22-800.webp 800w,/assets/img/2024-08-05-NeRFcode/22-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/22.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Stratified Sampling of distance \(t\) for coarse-MLP :<br/> z_vals : shape (B, N_samples) = (N_rays, N_samples)<br/> stratified sampled distance \(t\) <ul> <li>Let 균등한 간격을 나타내는 \(t_{vals} \in [0, 1]\) has shape (N_samples,)</li> <li>if lindisp = False:<br/> sample linearly in depth<br/> \(z_{vals} = near \cdot (1-t_{vals}) + far \cdot (t_{vals})\)</li> <li>if lindisp = True:<br/> sample linearly in inverse-depth<br/> \(z_{vals} = \frac{1}{\frac{1}{near} \cdot (1-t_{vals}) + \frac{1}{far} \cdot (t_{vals})}\)</li> <li>if perturb = True:<br/> add randomness</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/25-480.webp 480w,/assets/img/2024-08-05-NeRFcode/25-800.webp 800w,/assets/img/2024-08-05-NeRFcode/25-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/25.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> perturb=False이면 맨 윗줄을 coarse-samples로 쓰고, perturb=True이면 맨 아랫줄을 coarse-samples로 쓴다 </div> <ul> <li>pts, viewdirs : coarse-MLP input <ul> <li>pts : position info. \(r = o + td\) of shape (B, N_samples, 3)</li> <li>viewdirs : view-direction info. of shape (B, 3)</li> </ul> </li> <li>raw : coarse-MLP output<br/> shape (B, N_samples, 4) where 4 : for RGB, opacity</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/23-480.webp 480w,/assets/img/2024-08-05-NeRFcode/23-800.webp 800w,/assets/img/2024-08-05-NeRFcode/23-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/23.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Inverse-transform Sampling of distance \(t\) for fine-MLP : <ul> <li>coarse-samples :<br/> z_vals : shape (B, N_samples) = (N_rays, N_samples)</li> <li>fine-samples :<br/> coarse-MLP의 MLP output raw에 대해 raw2outputs()로 구한 weights 값을 Fine-Sampling에 사용<br/> z_samples : shape (B, N_importance)</li> <li>total sorted samples for fine-MLP :<br/> z_vals : shape (B, N_samples + N_importance)</li> </ul> </li> <li>pts, viewdirs : fine-MLP input <ul> <li>pts : position info. \(r = o + td\) of shape (B, N_samples + N_importance, 3)</li> <li>viewdirs : view-direction info. of shape (B, 3)</li> </ul> </li> <li>raw : fine-MLP output<br/> shape (B, N_samples + N_importance, 4) where 4 : RGB, opacity</li> </ul> <h3 id="sample_pdf">sample_pdf</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/29-480.webp 480w,/assets/img/2024-08-05-NeRFcode/29-800.webp 800w,/assets/img/2024-08-05-NeRFcode/29-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/29.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>input : <ul> <li>z_vals_mid : shape (B, N_samples - 1)<br/> stratified samples 사이의 중점</li> <li>weights[…, 1:-1] : shape (B, N_samples - 2)<br/> 시작점, 끝점 빼고 weight of each stratified sample</li> <li>det : stratified samples에 randomness 부여했다면 False</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/27-480.webp 480w,/assets/img/2024-08-05-NeRFcode/27-800.webp 800w,/assets/img/2024-08-05-NeRFcode/27-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/27.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/28-480.webp 480w,/assets/img/2024-08-05-NeRFcode/28-800.webp 800w,/assets/img/2024-08-05-NeRFcode/28-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/28.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>pdf : shape (B, N_samples - 2)<br/> \(\frac{w_i}{\sum_{j=1}^{num_{N_samples - 2}} w_j}\)</li> <li>cdf : shape (B, N_samples - 1)<br/> \(F_i = \sum_{j=1}^{i-1} f_j\)<br/> by torch.cumsum()<br/> 각 row는 0 ~ 1 에서 점점 증가하는 수로 이루어져 있음</li> <li>u : shape (B, N_importance) <ul> <li>det가 True (no randomness)일 경우 :<br/> \(\begin{bmatrix} 0 &amp; \frac{1}{N_{importance}-1} &amp; \cdots &amp; 1 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \end{bmatrix}\)</li> <li>det가 False (randomness)일 경우 :<br/> 0 ~ 1 사이의 random float로 이루어져 있음</li> </ul> </li> <li>inds : shape (B, N_importance)<br/> u를 cdf의 어디에 끼워넣을 수 있는지에 대한 index<br/> by torch.searchsorted()</li> <li>below : shape (B, N_importance)<br/> max(0, inds - 1)</li> <li>above : shape (B, N_importance)<br/> min(N_samples - 2, inds)</li> <li>inds_g : shape (B, N_importance, 2) and range [0, N_samples - 1)<br/> u가 위치할 수 있는 cdf의 두 경계의 index를 의미</li> <li>cdf_g : shape (B, N_importance, 2)<br/> torch.gather(cdf.expand(), 2, inds_g)<br/> inds_g에 따라 cdf의 값(확률값)을 추출해옴<br/> where cdf.expand() : shape (B, N_importance, N_samples - 1)<br/> where inds_g : shape (B, N_importance, 2) and range [0, N_samples - 1)</li> <li>bins_g : shape (B, N_importance, 2)<br/> torch.gather(bins.expand(), 2, inds_g)<br/> inds_g에 따라 bins의 값(coarse-samples 사이의 중점 \(t\) 값)을 추출해옴<br/> where bins.expand() : shape (B, N_importance, N_samples - 1)<br/> where inds_g : shape (B, N_importance, 2) and range [0, N_samples - 1)</li> <li>denom : shape (B, N_importance)<br/> u가 위치할 수 있는 구간의 cdf 값 차이</li> <li>t : shape (B, N_importance)<br/> u가 구간 내에서 차지하는 상대적인 위치</li> <li>samples : shape (B, N_importance)<br/> fine samples의 \(t\) 값<br/> bins_g[…, 0]과 bins_g[…, 1] 사이의 값</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/31-480.webp 480w,/assets/img/2024-08-05-NeRFcode/31-800.webp 800w,/assets/img/2024-08-05-NeRFcode/31-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/31.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> CDF 가로축의 empty circles는 coarse(stratified) samples 사이의 중점(mid-point)의 t값 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/24-480.webp 480w,/assets/img/2024-08-05-NeRFcode/24-800.webp 800w,/assets/img/2024-08-05-NeRFcode/24-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/24.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>render_rays() output : dict <ul> <li>rgb_map : shape (B, 3)<br/> predicted RGB values by alpha-compositing</li> <li>disp_map : shape (B,)<br/> disparity map (inverse of depth)</li> <li>acc_map : shape (B,)<br/> sum of sample weights along each ray</li> <li>raw : MLP raw output (raw2outputs() 안 한 것)</li> <li>rgb0, disp0, acc0 : from coarse-MLP</li> <li>z_std : shape (B,)<br/> std of distances (\(t\) 값) of fine-samples for each ray</li> </ul> </li> </ul> <h3 id="raw2outputs">raw2outputs</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/26-480.webp 480w,/assets/img/2024-08-05-NeRFcode/26-800.webp 800w,/assets/img/2024-08-05-NeRFcode/26-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/26.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>input : <ul> <li>raw : shape (B, num_samples, 4)</li> </ul> </li> <li> <p>dists : shape (B, num_samples)<br/> \(\delta_{i}\) : sample 간의 간격 in world-coordinate<br/> sample 간의 간격 in t-coordinate 에 \(\| d \|\) 곱해서 구함<br/> (dists[:, -1]은 마지막 sample부터 inf까지의 간격을 의미하는 매우 큰 수 1e10)</p> </li> <li> <p>rgb : shape (B, num_sample, 3)<br/> \(c_i\) : raw-RGB에 sigmoid 씌운 값<br/> sigmoid(raw[…, :3])<br/> so that \(c_i \in (0, 1)\)</p> </li> <li> <p>alpha : shape (B, num_samples)<br/> \(\alpha_{i} = 1 - \exp(- \sigma_{i} \delta_{i})\)<br/> where \(\sigma_{i}\) : raw-opacity에 noise 더하고 relu 씌운 값<br/> so that \(\sigma_{i} \in [0, \infty)\)</p> </li> <li> <p>weights : shape (B, num_samples)<br/> \(w_i = \alpha_{i} \times T_i\)<br/> where \(T_i = \prod_{j=1}^{i-1} (1-\alpha_{j}+1e-10)\) is obtained by torch.cumprod()</p> </li> <li>output : <ul> <li>rgb_map : shape (B, 3)<br/> predicted RGB values<br/> by volume rendering \(\hat{C}(r) = \sum_{i=1}^{num_{samples}} T_i \alpha_{i} c_i = \sum_{i=1}^{num_{samples}} w_i c_i\) <ul> <li>if white_bkgd (투명한 배경 대신 흰색) :<br/> \(\hat{C}(r) = \sum_{i=1}^{num_{samples}} w_i c_i + (1 - \sum_{i=1}^{num_{samples}} w_i)\)<br/> so that 투명해서 \(\sum_{i=1}^{num_{samples}} w_i\) 가 작을 때 RGB-color가 흰색(1.)에 가깝도록</li> </ul> </li> <li>disp_map : shape (B,)<br/> disparity map (inverse of depth)<br/> by \(\frac{1}{max(1e-10, \frac{\sum_{i=1}^{num_{samples}} w_i t_i}{\sum_{i=1}^{num_{samples}} w_i})}\)</li> <li>acc_map : shape (B,)<br/> sum of sample weights along each ray<br/> by \(\sum_{i=1}^{num_{samples}} w_i\)</li> <li>weights : shape (B, num_samples)<br/> weight of each sample<br/> \(w_i = \alpha_{i} \times T_i\)</li> <li>depth_map : shape (B,)<br/> depth map (estimated distance to object)<br/> by \(\sum_{i=1}^{num_{samples}} w_i t_i\)<br/> (weight가 높은 깊이 값 \(t_i\) 을 더 많이 반영하는 식으로 weighted sum)</li> </ul> </li> </ul> <h2 id="evaluation">Evaluation</h2> <h3 id="img2mse-for-loss-and-mse2psnr-for-psnr">img2mse for loss and mse2psnr for psnr</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/32-480.webp 480w,/assets/img/2024-08-05-NeRFcode/32-800.webp 800w,/assets/img/2024-08-05-NeRFcode/32-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/32.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/30-480.webp 480w,/assets/img/2024-08-05-NeRFcode/30-800.webp 800w,/assets/img/2024-08-05-NeRFcode/30-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/30.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>loss = coarse-MLP-loss + fine-MLP-loss<br/> where each is MSE loss b.w. predicted RGB and GT RGB of shape (N_rand, 3)</li> <li>PSNR : \(PSNR = -10 * log_{10}(loss)\)</li> <li>to8b : 0. ~ 1.에서 0 ~ 255 (8-bit)로 변환</li> </ul> <h3 id="test">test</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/34-480.webp 480w,/assets/img/2024-08-05-NeRFcode/34-800.webp 800w,/assets/img/2024-08-05-NeRFcode/34-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/34.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>args.i_video iter.마다<br/> novel view(render_poses)에 대해 rendering해서<br/> 여러 장의 rgb_map과 disp_map을 동영상으로 저장</li> <li>args.i_testset iter.마다<br/> test view에 대해 rendering해서<br/> 한 장의 rgb_map을 사진으로 저장</li> </ul> <h3 id="render_path">render_path</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/33-480.webp 480w,/assets/img/2024-08-05-NeRFcode/33-800.webp 800w,/assets/img/2024-08-05-NeRFcode/33-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/33.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>inference rendering (한 장씩)</li> <li>빠른 rendering을 위해 H, W, focal을 downsample</li> </ul> <h2 id="question">Question</h2> <ul> <li>Q1 : 왜 ndc_rays() 호출할 때 near bound n 값에 near = 1.으로 하드코딩해서 넣어주지?</li> <li>A1 : <code class="language-plaintext highlighter-rouge">????</code></li> <li>Q2 : 왜 blender dataset에서 render_poses 만들 때 phi=-30. 으로 하드코딩해서 넣어주지?</li> <li>A2 : <code class="language-plaintext highlighter-rouge">????</code></li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="rendering"/><category term="3d"/><summary type="html"><![CDATA[NeRF Code Review]]></summary></entry><entry><title type="html">MipNeRF</title><link href="https://semyeong-yu.github.io/blog/2024/MipNeRF/" rel="alternate" type="text/html" title="MipNeRF"/><published>2024-08-03T01:03:00+00:00</published><updated>2024-08-03T01:03:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/MipNeRF</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/MipNeRF/"><![CDATA[<h2 id="mip-nerf-a-multiscale-representation-for-anti-aliasing-neural-radiance-fields">Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</h2> <h4 id="jonathan-t-barron-ben-mildenhall-matthew-tancik-peter-hedman-ricardo-martin-brualla-pratul-p-srinivasan">Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2103.13415">https://arxiv.org/abs/2103.13415</a><br/> project website :<br/> <a href="https://jonbarron.info/mipnerf/">https://jonbarron.info/mipnerf/</a><br/> pytorch code :<br/> <a href="https://github.com/bebeal/mipnerf-pytorch">https://github.com/bebeal/mipnerf-pytorch</a><br/> <a href="https://github.com/google/mipnerf">https://github.com/google/mipnerf</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>scale 잘 반영하도록 sample(region)을 pre-filtering!!</li> <li>ray-tracing하여 point-encoding 대신<br/> cone-tracing하여 region-encoding 이므로<br/> frustum의 모양과 크기 정보를 encode할 수 있어서 scale 반영 가능</li> <li>IPE 단계에서<br/> <code class="language-plaintext highlighter-rouge">high variance (구간 길이가 일정하다면 distant view)</code>일 때<br/> <code class="language-plaintext highlighter-rouge">high freq.를 attenuate</code> (pre-filtering) 하여<br/> <code class="language-plaintext highlighter-rouge">임의의 continuous-space scale</code>을 가지는 scene에 대해 <code class="language-plaintext highlighter-rouge">anti-aliased</code> representation 학습 가능<br/> \(\rightarrow\) multi-resolution dataset에 대해 성능 대폭 향상<br/> \(\rightarrow\) scale-aware하므로 <code class="language-plaintext highlighter-rouge">single MLP</code> 하나만으로 충분하여 빠르고 가벼움</li> <li>camera center로부터 각 pixel로 3D cone을 쏜 다음,<br/> <code class="language-plaintext highlighter-rouge">frustum을 multi-variate Gaussian으로 근사</code>한 뒤,<br/> Gaussian 내 좌표를 positional encoding한 것 (PE-basis-lifted Gaussian)에 대해<br/> expected value \(E \left[ \gamma (x) \right]\) 계산<br/> 주의 : frustum이 Gaussian 분포를 따르는 게 아니라, frustum 내부의 mean, variance 값을 먼저 구한 뒤 해당 mean, variance 값을 갖는 Gaussian으로 frustum을 대신(근사)할 수 있다고 생각!</li> </ol> </blockquote> <h2 id="introduction">Introduction</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/2-480.webp 480w,/assets/img/2024-08-03-MipNeRF/2-800.webp 800w,/assets/img/2024-08-03-MipNeRF/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>기존 NeRF의 문제점 : <ul> <li>rendering 위해 sampling할 때 <code class="language-plaintext highlighter-rouge">single ray</code> per pixel 쏴서 composite 하므로<br/> dataset에 있는 물체의 크기(resolution)가 일정하지 않을 때<br/> multi-scales images에 대해 학습하더라도</li> <li><code class="language-plaintext highlighter-rouge">blurry</code> rendering in <code class="language-plaintext highlighter-rouge">close-up</code> views<br/> (because 가까이서 찍어서 zoom-out하면 물체 in <code class="language-plaintext highlighter-rouge">high resolution</code>)</li> <li><code class="language-plaintext highlighter-rouge">aliased</code>(계단) rendering in <code class="language-plaintext highlighter-rouge">distant</code> views<br/> (because 멀리서 찍어서 zoom-in하면 물체 in <code class="language-plaintext highlighter-rouge">low resolution</code>)</li> <li>그렇다고 multiple rays per pixel through its footprint로 brute-force super-sampling(offline rendering)하는 것은 정확하긴 하겠지만 too costly 비현실적</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Minmap</code> Approach :<br/> classic 컴퓨터 그래픽스 분야에서 rendering할 때 aliasing을 없애기 위한 <code class="language-plaintext highlighter-rouge">pre-filtering</code> 기법<br/> 본 논문인 Mip-NeRF가 여기서 영감을 얻음<br/> signal(e.g. image)을 diff. <code class="language-plaintext highlighter-rouge">downsampling scales</code>로 나타낸 뒤 pixel footprint를 근거로 ray에 사용하기 위한 <code class="language-plaintext highlighter-rouge">적절한 scale을 고른다</code><br/> render time에 할 복잡할 일을 pre-computation phase에 미리 하는 것일 뿐이긴 하지만, 주어진 texture마다 한 번만 minmap을 만들면 된다는 장점이 있다</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/4-480.webp 480w,/assets/img/2024-08-03-MipNeRF/4-800.webp 800w,/assets/img/2024-08-03-MipNeRF/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Mip-NeRF : <ul> <li>represent pre-filtered scene at <code class="language-plaintext highlighter-rouge">continuous space of scales</code></li> <li>ray 대신 <code class="language-plaintext highlighter-rouge">conical frustum</code> 사용해서 <code class="language-plaintext highlighter-rouge">anti-aliased</code> rendering with fine details</li> <li>multi-scale variant of dataset에 대해 평균 error rate 60% 감소</li> <li>NeRF가 hierarchical sampling을 위해 coarse and fine MLP를 분리했다면, Mip-NeRF는 <code class="language-plaintext highlighter-rouge">scale-aware</code>하므로 <code class="language-plaintext highlighter-rouge">single multi-scale MLP만으로 충분</code><br/> 따라서 NeRF보다 7% 빠르고, param. 수는 절반이고, sampling도 더 효율적</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/1-480.webp 480w,/assets/img/2024-08-03-MipNeRF/1-800.webp 800w,/assets/img/2024-08-03-MipNeRF/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> ray 대신 cone을 쏘고, point-encoding 대신 frustum region-encoding </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/3-480.webp 480w,/assets/img/2024-08-03-MipNeRF/3-800.webp 800w,/assets/img/2024-08-03-MipNeRF/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>차이 : 기존 NeRF는 <code class="language-plaintext highlighter-rouge">a single point</code>를 encode하고, Mip-NeRF는 <code class="language-plaintext highlighter-rouge">a region of space</code>를 encode</p> </li> <li> <p>기존 NeRF :<br/> camera center로부터 각 pixel로 ray를 하나 쏜 다음 point sampling한 뒤 positional encoding<br/> point-sampled feature는 ray가 보는 <code class="language-plaintext highlighter-rouge">volume의 모양과 크기를 무시</code>하는 것임<br/> 예를 들어 training할 때 camera1로부터 t 사이의 간격이 평균 10cm로 학습된 scene에 대해<br/> camera2로 inference를 할 때 t 사이의 간격이 평균 1cm로 sampling된다면<br/> 10개의 점은 같은 point-based feature를 갖게 되어 scale을 고려하지 못함<br/> 이러한 ambiguity가 기존 NeRF의 성능 하락의 요인</p> </li> <li> <p>Mip-NeRF :<br/> volume 정보를 반영하기 위해 camera center로부터 각 pixel로 3D cone을 쏜 다음, 3D point 및 그 주위의 Gaussian region을 encode하기 위해 IPE</p> </li> <li> <p>IPE (<code class="language-plaintext highlighter-rouge">integrated positional encoding</code>) :<br/> region을 encode하기 위한 방식<br/> <code class="language-plaintext highlighter-rouge">frustum을 multi-variate Gaussian으로 근사</code>한 뒤,<br/> Gaussian 내 좌표를 positional encoding한 것 (PE-basis-lifted Gaussian)에 대해 expected value \(E \left[ \gamma (x) \right]\) 계산</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/5-480.webp 480w,/assets/img/2024-08-03-MipNeRF/5-800.webp 800w,/assets/img/2024-08-03-MipNeRF/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 사진 출처 : https://xoft.tistory.com/16 </div> <h2 id="related-work">Related Work</h2> <h3 id="anti-aliasing-in-rendering">Anti-aliasing in Rendering</h3> <blockquote> <p><code class="language-plaintext highlighter-rouge">anti-aliasing</code>을 위한 고전적인 방법으로는 두 가지가 있다.</p> </blockquote> <ol> <li><code class="language-plaintext highlighter-rouge">supersampling</code> : <ul> <li>rendering할 때 <code class="language-plaintext highlighter-rouge">multiple rays per pixel</code>을 쏴서 Nyquist frequency에 가깝게 sampling rate를 높임 (super-sampling)</li> <li><code class="language-plaintext highlighter-rouge">expensive</code> as runtime increases linearly with the super-sampling rate, so used only in <code class="language-plaintext highlighter-rouge">offline</code> rendering</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">pre-filtering</code> : <ul> <li>target sampling rate에 맞춰서 Nyquist frequency를 줄이기 위해 scene에 <code class="language-plaintext highlighter-rouge">lowpass-filter</code>를 씌운 버전 사용</li> <li>scene을 미리 <code class="language-plaintext highlighter-rouge">downsampling multi-scales</code> (sparse voxel octree 또는 minmap)로 나타낸 뒤, <code class="language-plaintext highlighter-rouge">ray 대신 cone</code>을 추적하여 cone과 scene이 만나는 곳의 cone’s footprint에 대응되는 적절한 scale을 골라서 사용 (<code class="language-plaintext highlighter-rouge">target sampling rate에 맞는 적절한 scale</code>)</li> <li>scene에 filter 씌운 버전을 한 번만 미리 계산하면 되므로, better for <code class="language-plaintext highlighter-rouge">real-time</code> rendering</li> </ul> </li> </ol> <blockquote> <p>그런데 아래의 이유로 고전적인 multi-scale representation은 적용 불가능</p> <ul> <li>input scene의 <code class="language-plaintext highlighter-rouge">geometry를 미리 알 수 없으므로</code> pre-filtering 할 수가 없어서<br/> 대신 pre-filtering 방식 자체를 training할 때 학습해야 한다</li> <li>input scene의 <code class="language-plaintext highlighter-rouge">scale이 continuous</code>하므로 a fixed number of scales (discrete)과 상황이 다르다</li> </ul> </blockquote> <p>\(\rightarrow\) 결론 : 따라서 Mip-NeRF는 training하는 동안, <code class="language-plaintext highlighter-rouge">임의의 scale</code>에 대해 query 받을 수 있는, scene의 <code class="language-plaintext highlighter-rouge">pre-filtered representation</code>을 학습한다.</p> <h3 id="scene-representations-for-view-synthesis">Scene Representations for View Synthesis</h3> <ul> <li> <p>만약 images of scene are captured <code class="language-plaintext highlighter-rouge">densely</code> 라면, novel view synthesis 위해, intermediate representation of scene을 reconstruct하지 않고 <code class="language-plaintext highlighter-rouge">light field interpolation</code> 기법 사용</p> </li> <li>만약 images of scene are captured <code class="language-plaintext highlighter-rouge">sparsely</code> 라면, novel view synthesis 위해, <code class="language-plaintext highlighter-rouge">explicit representation</code> of the scene’s 3D geometry and appearance를 reconstruct <ul> <li><code class="language-plaintext highlighter-rouge">polygon-mesh-based</code> representation (<code class="language-plaintext highlighter-rouge">discrete, classic</code>) :<br/> with either diffuse or view-dependent textures<br/> can be stored efficiently, but mesh geometry optimization에 gradient descent를 이용하는 것은 불연속점 및 극소점 때문에 어렵</li> <li><code class="language-plaintext highlighter-rouge">volumetric</code> representation : <ul> <li><code class="language-plaintext highlighter-rouge">voxel-grid-based</code> representation (<code class="language-plaintext highlighter-rouge">discrete, classic</code>) : deep learning으로 학습 가능, but 고해상도의 scene에는 부적합</li> <li><code class="language-plaintext highlighter-rouge">coordinate-based</code> neural representation (<code class="language-plaintext highlighter-rouge">continuous, recent</code>) : 3D 좌표를 그 위치에서의 scene의 특징(e.g. volume density, radiance)으로 mapping하는 continuous function을 MLP로 예측 <ul> <li>implicit surface representation</li> <li><code class="language-plaintext highlighter-rouge">implicit volumetric NeRF representation</code></li> </ul> </li> </ul> </li> </ul> </li> <li>문제점 :<br/> 그동안 view synthesis를 할 때 sampling 및 aliasing에는 덜 주목했다<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">classic discrete</code> representation의 경우 앞에서 소개한 minmap, octree와 같은 고전적인 multi-scale pre-filtering 기법을 쓰면 aliasing 없이 rendering 가능하다<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">coordinate-based</code> representation의 경우 scale이 continuous하므로 고전적인 anti-aliasing 기법은 사용할 수 없다<br/> \(\rightarrow\) sparse voxel octree 기반의 multi-scale representation으로 implicit surface의 continuous neural representation을 구현한 논문 <d-cite key="continuouspriori">[1]</d-cite> 있긴 하지만, scene geometry의 priori를 알아야 한다는 제약이 있다<br/> \(\rightarrow\) Mip-NeRF는 training하는 동안, <code class="language-plaintext highlighter-rouge">임의의 scale</code>에 대해 query 받을 수 있는, scene의 <code class="language-plaintext highlighter-rouge">anti-aliased (pre-filtered)</code> representation을 학습한다.</li> </ul> <h2 id="method">Method</h2> <h3 id="cone-tracing-and-positional-encoding">Cone Tracing and Positional Encoding</h3> <ul> <li>Cone Tracing : <ul> <li>Let \(d\) is cone direction vector from \(o\) to image plane</li> <li>Let \(\hat r =\) radius at image plane \(o + d\) \(= \frac{2}{\sqrt{12}}\) of pixel-width<br/> so that image plane에서의 cone의 \(x, y\) 축 variance가 pixel’s footprint의 variance와 같아지도록<br/> \(\hat r\)은 ray의 radius 변화율, 즉 frustum의 넓이를 결정</li> <li>\(t \in [t_0, t_1]\) 일 때 conical frustum 내의 \(x\)는 아래 범위의 값을 가질 때 indicator function \(F(x, o, d, \hat r, t_0, t_1)=1\)이다<br/> \(F(x, o, d, \hat r, t_0, t_1) = 1 \left\{ (t_0 \lt \frac{d^T(x-o)}{\| d \|^2} \lt t_1) \land (\frac{d^T(x-o)}{\| d \| \| x-o \|} \gt \frac{1}{\sqrt{1+(\frac{\hat r}{\| d \|})^2}}) \right\}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/6-480.webp 480w,/assets/img/2024-08-03-MipNeRF/6-800.webp 800w,/assets/img/2024-08-03-MipNeRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Region Encoding :<br/> conical frustum 내에 있는 모든 좌표 \(x\)에 대해 직접<br/> expected value \(E \left[ \gamma (x) \right] = \frac{\int \gamma (x) F(x, o, d, \hat r, t_0, t_1) dx}{\int F(x, o, d, \hat r, t_0, t_1) dx}\) 계산하면<br/> region을 encode할 수 있는데<br/> 여기서 분자의 적분식은 closed-form solution이 없음<br/> \(\rightarrow\) 직접 계산하지 말고<br/> <code class="language-plaintext highlighter-rouge">conical-frustum을 multi-variate Gaussian으로 근사</code>한 뒤<br/> Gaussian 내에 있는 모든 좌표 \(x\)에 대해<br/> expected value \(E \left[ \gamma (x) \right]\) 계산</p> </li> <li>frustum을 multi-variate Gaussian으로 근사 : <ul> <li>conical-frustum 단면은 대칭적인 원이기 때문에<br/> \(o, d\) 뿐만 아니라 아래의 3가지 정보만 알면 Gaussian을 특정할 수 있다 <ul> <li><code class="language-plaintext highlighter-rouge">mean distance along ray from o</code> \(\mu_{t}\)</li> <li><code class="language-plaintext highlighter-rouge">variance along ray</code> \(\sigma_{t}^2\)</li> <li><code class="language-plaintext highlighter-rouge">variance perpendicular to ray</code> \(\sigma_{r}^2\)</li> </ul> </li> <li>Let mid-point \(t_{\mu} = \frac{t_0+t_1}{2}\)<br/> Let half-width \(t_{\sigma}=\frac{t_1-t_0}{2}\)</li> <li>아래 수식의 유도과정은 하위에 별도로 정리함<br/> \(\mu_{t} = t_{\mu} + \frac{2t_{\mu}t_{\sigma}^2}{3t_{\mu}^2+t_{\sigma}^2}\)<br/> \(\sigma_{t}^2 = \frac{t_{\sigma}^2}{3} - \frac{4t_{\sigma}^4(12t_{\mu}^2-t_{\sigma}^2)}{15(3t_{\mu}^2+t_{\sigma}^2)^2}\)<br/> \(\sigma_{r}^2 = \hat r^2 (\frac{t_{\mu}^2}{4} + \frac{5t_{\sigma}^2}{12} - \frac{4t_{\sigma}^4}{15(3t_{\mu}^2+t_{\sigma}^2)})\)</li> <li>위의 3가지 param.를 가지는 Gaussian은 <code class="language-plaintext highlighter-rouge">t-coordinate</code>에서 정의했는데<br/> 아래 수식에 의해 <code class="language-plaintext highlighter-rouge">world-coordinate</code>으로 변환할 수 있다<br/> \(\mu = o + \mu_{t}d\)<br/> \(\Sigma = \sigma_{t}^2(dd^T) + \sigma_{r}^2(I-\frac{dd^T}{\| d \|^2})\)<br/> where \(dd^T =\) \(d\) 의 outer product은 \(d\) 방향으로의 투영을 의미하는 rank-1 matrix<br/> where \(I-\frac{dd^T}{\| d \|^2}\) 는 \(\frac{d}{\| d\ \|}\) 와 수직인 subspace로의 투영을 의미하는 rank-2 matrix</li> </ul> </li> <li>Integrated Positional Encoding (IPE) : <ul> <li>목표 : 위에서 계산한 \(\mu, \Sigma\) 의 Gaussian 내에 있는 모든 좌표 \(x\)에 대해 expected value \(E \left[ \gamma (x) \right]\) 계산</li> <li>우선 <code class="language-plaintext highlighter-rouge">PE (positional-encoding) basis</code> P를 재정의<br/> \(P = \begin{pmatrix} \begin{matrix} 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2\end{matrix} &amp; \cdots &amp; \begin{matrix} 2^{L-1} &amp; 0 &amp; 0 \\ 0 &amp; 2^{L-1} &amp; 0 \\ 0 &amp; 0 &amp; 2^{L-1}\end{matrix} \end{pmatrix}^T\)<br/> \(\gamma (x) = \begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}\)</li> <li>\(E \left[ \gamma (x) \right]\) 는 expectation over \(\gamma (x) = \begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}\) 이므로<br/> \(x\) in Gaussian of \(\mu, \Sigma\) \(\rightarrow\) \(\gamma (x)\) in Gaussian of \(\mu_{r}, \Sigma_{r}\) 로 변환해야 한다<br/> 즉, <code class="language-plaintext highlighter-rouge">PE basis P로 lift</code>한 뒤의 mean과 covariance를 구해야 한다<br/> Since \(Cov[Ax, By] = A Cov[x, y] B^T\),<br/> \(\mu_{r} = P \mu\)<br/> \(\Sigma_{r} = P \Sigma P^T\)</li> <li>최종적으로 \(E \left[ \gamma (x) \right]\) , 즉 <code class="language-plaintext highlighter-rouge">expectation over lifted multi-variate Gaussian</code> of \(\mu_{r}, \Sigma_{r}\) 을 구하면 된다<br/> Since \(E_{k \sim N(\mu, \sigma^2)}[e^{itk}] = exp(i \mu t - \frac{1}{2} \sigma^2 t^2)\) and \(sin(k) = \frac{e^{ik}-e^{-ik}}{2i}\) and \(cos(k) = \frac{e^{ik}+e^{-ik}}{2}\),<br/> \(E_{k \sim N(\mu, \sigma^2)}[sin(k)] = sin(\mu)exp(-\frac{1}{2}\sigma^2)\) and \(E_{k \sim N(\mu, \sigma^2)}[cos(k)] = cos(\mu)exp(-\frac{1}{2}\sigma^2)\) for each axis-k<br/> (positional-encoding은 각 dim.을 independently encode하므로 marginal distribution of \(\gamma (x)\) 에 의존)<br/> \(\rightarrow\)<br/> \(\gamma (\mu, \Sigma) = E_{x \sim N(\mu, \Sigma)} [\gamma (x)] = E_{Px \sim N(\mu_{r}, \Sigma_{r})} [\begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}]\)<br/> \(= \begin{bmatrix} sin(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \\ cos(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \end{bmatrix}\)<br/> where \(\circledast\) is element-wise multiplication</li> <li>\(diag(\Sigma_{r})\) 만 필요하므로 \(\Sigma_{r}\) 전부 계산하지 말고 <code class="language-plaintext highlighter-rouge">efficiently diagonal만 계산</code><br/> PE-basis \(P\) 가 identity matrix이므로 \(diag(\Sigma)\) 만 필요<br/> \(diag(\Sigma_{r}) = diag(P \Sigma P^T) = \left[ diag(\Sigma), 4 diag(\Sigma), \ldots , 4^{L-1}diag(\Sigma) \right]^T\)<br/> where 3d-vector \(diag(\Sigma) = \sigma_{t}^2(d \circledast d) + \sigma_{r}^2(1-\frac{d \circledast d}{\| d \|^2})\)<br/> diagonal만 직접 계산하면, IPE feature는 PE feature랑 비슷하게 cost 소모</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/7-480.webp 480w,/assets/img/2024-08-03-MipNeRF/7-800.webp 800w,/assets/img/2024-08-03-MipNeRF/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>IPE vs PE : <ul> <li>PE :<br/> point를 encode<br/> 0~L까지의 <code class="language-plaintext highlighter-rouge">모든 frequencies에 대해 동일하게</code> encode<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">high-freq.</code> PE features are <code class="language-plaintext highlighter-rouge">aliased</code><br/> (PE period가 interval width보다 작은 경우 PE over interval oscillates repeatedly)</li> <li>IPE :<br/> interval region을 integrate하여 encode<br/> IPE feature를 만드는 데 사용된 interval \(t \in [t_0, t_1]\) width보다 period가 작은 <code class="language-plaintext highlighter-rouge">high freq.</code>의 경우 <code class="language-plaintext highlighter-rouge">attenuate</code>하여 <code class="language-plaintext highlighter-rouge">anti-aliasing</code><br/> by \(exp(-\frac{1}{2}(\omega \sigma)^2)\) term</li> <li>위와 같은 특성 덕분에 IPE는 interval 내 공간의 모양과 크기를 smoothly encode할 수 있는 anti-aliased PE 기법이다!</li> <li>high freq.는 IPE 단계 자체에서 attenuate되므로 <code class="language-plaintext highlighter-rouge">L을 hyper-param.로 두지 않고 extremely large fixed-value</code>로 두면 된다<br/> 본 논문에서는 IPE feature의 last dim.이 numerical epsilon보다 작아지는 값인 \(L=16\) 으로 둠</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/16-480.webp 480w,/assets/img/2024-08-03-MipNeRF/16-800.webp 800w,/assets/img/2024-08-03-MipNeRF/16-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> NeRF에서는 L이 너무 크면 overfitting, Mip-NeRF에서는 IPE 단계 자체에서 high freq.를 attenuate하므로 L 커도 상관 없음 </div> <ul> <li>IPE의 의미 :<br/> 이게 Mip-NeRF의 핵심!! <ul> <li>수식 :<br/> PE-basis P 는 다양한 frequency \(\omega\) 로 구성되어 있고<br/> 각 element는 \(E_{x \sim N(\mu, \Sigma)} [\gamma_{\omega} (x)] = sin(\omega \mu) exp(-\frac{1}{2}(\omega \sigma)^2)\)</li> <li>distant view :<br/> <code class="language-plaintext highlighter-rouge">distant views (low-resolution)</code>, 즉 멀리 있는 <code class="language-plaintext highlighter-rouge">wide frustum</code> (high variance \(\sigma\))의 경우에는<br/> <code class="language-plaintext highlighter-rouge">detail-info.</code> (high freq. \(\omega\))는 <code class="language-plaintext highlighter-rouge">training에 사용하지 않겠다</code><br/> \(\rightarrow\) more attenuation for high \(\sigma\) and high \(\omega\)</li> <li>close view :<br/> <code class="language-plaintext highlighter-rouge">close views (high-resolution)</code>, 즉 가까이 있는 <code class="language-plaintext highlighter-rouge">narrow frustum</code> (low variance \(\sigma\))의 경우에는<br/> <code class="language-plaintext highlighter-rouge">detail-info.</code> (high freq. \(\omega\))를 training할 때 좀 더 <code class="language-plaintext highlighter-rouge">허용</code></li> <li>위와 같이 scale을 반영할 수 있으므로 blurry 및 aliased rendering 문제 해결 가능!</li> </ul> </li> <li>수식 <code class="language-plaintext highlighter-rouge">Summary</code> : <ul> <li>frustum을 근사하는 multi-variate Gaussian의 mean, variance \(\mu, \sigma\) 를 구한다</li> <li>PE-basis P로 lift한 Gaussian의 mean, variance \(\mu_{r}, \Sigma_{r}\) 를 구한다<br/> \(P = \begin{pmatrix} \begin{matrix} 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2\end{matrix} &amp; \cdots &amp; \begin{matrix} 2^{L-1} &amp; 0 &amp; 0 \\ 0 &amp; 2^{L-1} &amp; 0 \\ 0 &amp; 0 &amp; 2^{L-1}\end{matrix} \end{pmatrix}^T\)<br/> \(\gamma (x) = \begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}\)<br/> \(\mu_{r} = P \mu\) and \(\Sigma_{r} = P \Sigma P^T\)</li> <li>\(E_{x \sim N(\mu_{r}, \Sigma_{r})} [\gamma (x)] = \begin{bmatrix} sin(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \\ cos(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \end{bmatrix}\)<br/> (efficiently \(\Sigma_{r}\) 의 diagonal만 직접 계산)<br/> \(diag(\Sigma_{r}) = diag(P \Sigma P^T) = \left[ diag(\Sigma), 4 diag(\Sigma), \ldots , 4^{L-1}diag(\Sigma) \right]^T\)</li> </ul> </li> </ul> <h3 id="conical-frustum-integral-derivation">Conical Frustum Integral Derivation</h3> <ul> <li> <p>우선 <code class="language-plaintext highlighter-rouge">Cartesian-coordinate</code>에서 <code class="language-plaintext highlighter-rouge">conical-coordinate</code>으로 변환<br/> \((x, y, z) = \varphi (r, t, \theta) = t \cdot (r cos \theta , r sin \theta , 1)\)<br/> where \(\theta \in [0, 2 \pi)\) and \(t \geq 0\) and \(\| r \| \leq \hat r\)<br/> Then,<br/> \(dx dy dz = | det(D \varphi) | dr dt d\theta\)<br/> \(= \begin{vmatrix} t cos\theta &amp; t sin\theta &amp; 0 \\ r cos\theta &amp; r sin\theta &amp; 1 \\ - rt sin\theta &amp; rt cos\theta &amp; 0 \end{vmatrix} dr dt d\theta\)<br/> \(= (rt^2cos^2\theta + rt^2sin\theta) dr dt d\theta\)<br/> \(= rt^2 dr dt d\theta\)</p> </li> <li> <p>conical frustum의 volume \(V = \int \int \int dx dy dz = \int_{0}^{2\pi} \int_{t_0}^{t_1} \int_{0}^{\hat r} r t^2 dr dt d\theta = \pi \hat r^2 \frac{t_1^3 - t_0^3}{3}\) 에 대해<br/> conical frustum에서 uniformly-sampling한 points의 <code class="language-plaintext highlighter-rouge">probability density function</code>은 \(\frac{rt^2}{V}\) 이다</p> </li> <li><code class="language-plaintext highlighter-rouge">t-axis</code> : <ul> <li> \[E \left[ t \right] = \int_{0}^{2\pi} \int_{t_0}^{t_1} \int_{0}^{\hat r} t \cdot \frac{rt^2}{V} dr dt d\theta = \frac{3(t_1^4 - t_0^4)}{4(t_1^3 - t_0^3)}\] </li> <li> \[E \left[ t^2 \right] = \int_{0}^{2\pi} \int_{t_0}^{t_1} \int_{0}^{\hat r} t^2 \cdot \frac{rt^2}{V} dr dt d\theta = \frac{3(t_1^5- t_0^5)}{5(t_1^3 - t_0^3)}\] </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">x-axis</code> (\(x = t r cos \theta\)) : <ul> <li> \[E \left[ t r cos\theta \right] = \int_{0}^{2\pi} \int_{t_0}^{t_1} \int_{0}^{\hat r} t r cos\theta \cdot \frac{rt^2}{V} dr dt d\theta = 0\] </li> <li> \[E \left[ (t r cos \theta)^2 \right] = \int_{0}^{2\pi} \int_{t_0}^{t_1} \int_{0}^{\hat r} t^2 r^2 cos^2 \theta \cdot \frac{rt^2}{V} dr dt d\theta = \frac{\hat r^2}{4} \frac{3(t_1^5 - t_0^5)}{5(t_1^3 - t_0^3)}\] </li> </ul> </li> <li> <p><code class="language-plaintext highlighter-rouge">y-axis</code> (\(y = t r sin \theta\)) :<br/> conical frustum은 x, y에 대해 symmetric하므로 위에서 구한 x-axis에서의 값과 동일</p> </li> <li>이제 conical frustum 내부에 있는 random point에 대한 mean, covariance 값을 구할 수 있다 <ul> <li><code class="language-plaintext highlighter-rouge">mean distance along ray from o</code> :<br/> \(\mu_{t} = E \left[ t \right] = \frac{3(t_1^4 - t_0^4)}{4(t_1^3 - t_0^3)}\)</li> <li><code class="language-plaintext highlighter-rouge">variance along ray</code> :<br/> \(\sigma_{t}^2 = E \left[ t^2 \right] - (E \left[ t \right])^2 = \frac{3(t_1^5- t_0^5)}{5(t_1^3 - t_0^3)} - \mu_{t}^2\)</li> <li><code class="language-plaintext highlighter-rouge">variance perpendicular to ray</code> :<br/> \(\sigma_{r}^2 = E \left[ x^2 \right] - (E \left[ x \right])^2 = \hat r^2 \frac{3(t_1^5 - t_0^5)}{20(t_1^3 - t_0^3)}\)</li> </ul> </li> <li>그런데 \(t_0, t_1\) 이 서로 가까우면 \(\frac{(t_1^5- t_0^5)}{(t_1^3 - t_0^3)}\) 과 같은 꼴은 numerically unstable as 0 or NaN instead of accurate values 이므로 training fail<br/> \(\rightarrow\)<br/> \(t_{\mu} = \frac{t_0+t_1}{2}\) and \(t_{\sigma}=\frac{t_1-t_0}{2}\) 로 re-parameterize하면<br/> <code class="language-plaintext highlighter-rouge">first-order term + correct(higher-order) term 꼴</code>로 정리 가능하고<br/> \(t_{\sigma}\) 가 작을 때에도 stable and accurate values 가짐 <ul> <li> \[\mu_{t} = t_{\mu} + \frac{2t_{\mu}t_{\sigma}^2}{3t_{\mu}^2+t_{\sigma}^2}\] </li> <li> \[\sigma_{t}^2 = \frac{t_{\sigma}^2}{3} - \frac{4t_{\sigma}^4(12t_{\mu}^2-t_{\sigma}^2)}{15(3t_{\mu}^2+t_{\sigma}^2)^2}\] </li> <li> \[\sigma_{r}^2 = \hat r^2 (\frac{t_{\mu}^2}{4} + \frac{5t_{\sigma}^2}{12} - \frac{4t_{\sigma}^4}{15(3t_{\mu}^2+t_{\sigma}^2)})\] </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">limitation</code> :<br/> frustum의 base 반지름과 top 반지름 차이가 클수록<br/> conical-frustum을 multi-variate Gaussian으로 approx.하는 건 inaccurate<br/> (예를 들어, camera FOV가 클 때 camera center와 매우 가까운 frustum)<br/> 대부분의 dataset에서는 흔하지 않은 case이긴 하지만,<br/> macro photography with fisheye lens와 같은 특별한 case에서 MipNeRF를 쓸 때는 frustum을 multi-variate Gaussian으로 approx.하는 게 문제가 될 수 있음</li> </ul> <h3 id="architecture">Architecture</h3> <ul> <li>아래 내용들을 제외하고는 NeRF의 Architecture와 동일 <ul> <li>ray-tracing 대신 cone-tracing</li> <li>PE 대신 IPE</li> <li>point-encoding 이므로 \(n\)개의 구간에 대해 \(n\)개의 point sampling<br/> \(\rightarrow\)<br/> interval(region)-encoding 이므로 \(n\)개의 구간을 위해 \(n+1\)개의 point sampling</li> <li>PE feature로는 scale을 반영할 수 없으므로 두 가지 MLP (coarse-MLP, fine-MLP) 이용해서 hierarchical sampling<br/> (coarse-MLP에서는 \(N_c=64\) points per ray, fine-MLP에서는 \(N_c+N_f=64+128\) points per ray)<br/> \(\rightarrow\)<br/> IPE feature 자체가 scale을 반영할 수 있으므로 MLP 하나를 반복해서 써서 hierarchical sampling<br/> (한 번은 \(N_c=128\) points per ray, 그 다음은 \(N_f=128\) points per ray)<br/> NeRF와 MipNeRF의 공정한 비교를 위해 같은 수(총 256개)의 point를 사용</li> <li>hierarchical sampling에서 piecewise-constant PDF of normalized \(w\) 에 따라 fine-sampling 하기 전에<br/> weight \(w_k\) 를 바로 사용하지 않고<br/> 2-tap MaxBlur filter 를 적용하여 weight의 wide and smooth upper bound 를 사용<br/> \(w_k^{\ast} = \frac{1}{2}(max(w_{k-1}, w_k) + max(w_k, w_{k+1})) + \alpha\)<br/> where 빈 공간에서도 일부 samples 추출되도록 보장하기 위해 \(\alpha=0.01\) 설정</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/8-480.webp 480w,/assets/img/2024-08-03-MipNeRF/8-800.webp 800w,/assets/img/2024-08-03-MipNeRF/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> single MLP 쓰니까 coarse loss와 fine loss 간의 balance 맞추기 위해 hyperparam. gamma = 0.1로 설정 </div> <ul> <li>MaxBlur filter : <ul> <li>MaxPool 대신 MaxBlurPool 쓰면 aliasing 감소 효과</li> <li>MipNeRF에서 weight에 MaxBlur filter 쓰는 이유 :<br/> scene content는 아무래도 연속적으로 존재하니까<br/> 인접한 samples 간의 weight \(w\) 가 갑작스럽게 변하거나 불연속적인 outlier 를 제외하여 smoothing 해주는 역할</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/10-480.webp 480w,/assets/img/2024-08-03-MipNeRF/10-800.webp 800w,/assets/img/2024-08-03-MipNeRF/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> MaxBlur on sample weight / plot reference : https://charlieppark.kr/ </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/9-480.webp 480w,/assets/img/2024-08-03-MipNeRF/9-800.webp 800w,/assets/img/2024-08-03-MipNeRF/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Setting :<br/> implementation on JaxNeRF<br/> 1 million iter., Adam optimizer, batch_size = 4096, lr from \(5 \cdot 10^{-4}\) to \(5 \cdot 10^{-6}\)</li> </ul> <h2 id="result">Result</h2> <ul> <li>multi-scale dataset에 대해 NeRF보다 error rate 60% 감소</li> <li>single-scale dataset에 대해 NeRF보다 error rate 17% 감소</li> <li>NeRF의 param.의 절반이고, NeRF보다 7% 빠름</li> <li>brute-force super-sampling한 버전보다 22배 빠른데 accuracy 비슷</li> </ul> <h2 id="question">Question</h2> <ul> <li>Q&amp;A reference : 3DGS online study</li> <li>Q1 : distant view (scene content in low-resolution)일 때 IPE의 \(exp(-\frac{1}{2}(\omega \sigma)^2)\) term에 의해 high freq.를 attenuate하여 anti-aliasing 가능한 건 이해했는데,<br/> close view (scene content in high-resolution)일 때 blurry rendering은 어떻게 해결??</li> <li>A1 : 위에서 “Method - Cone Tracing and Positional Encoding - IPE의 의미”에 설명해둠</li> <li>Q2 : image plane에서의 cone의 \(x, y\) 축 variance가 pixel’s footprint의 variance와 같아지도록<br/> \(\hat r =\) radius at image plane \(o + d\) \(= \frac{2}{\sqrt{12}}\) of pixel-width 로 설정한다는데 이 부분이 이해가 되지 않습니다</li> <li>A2 : uniform distribution을 가정했을 때 pixel의 square variance는 \(\frac{w^2}{12}\) 이고, cone at image plane의 circle variance는 \(\frac{\hat r^2}{4}\) 이므로 variance 값이 같으려면 \(\hat r = \frac{2}{\sqrt{12}} \times w\)</li> <li>Q3 : \(E \left[ \gamma (x) \right] = \frac{\int \gamma (x) F(x, o, d, \hat r, t_0, t_1) dx}{\int F(x, o, d, \hat r, t_0, t_1) dx}\) 에서 분자의 적분식을 closed-form으로 계산할 수 없어서 conical frustum을 multi-variate Gaussian으로 근사했다는데,<br/> conical frustum의 모양과 크기 범위에 대한 parameter가 주어진다면 frustum 내부의 점 \(x\) 에 sin 및 cos을 씌운 \(\gamma (x)\) 의 경우 \(x\) 에 대해 공간 적분할 수 있지 않나요?</li> <li>A3 : frustum 내에 있는 모든 좌표에 \(\gamma\) 를 씌워서 공간 적분하는 것 자체가 말도 안 되게 복잡한 식이라 closed-form solution이 없기 때문에 frustum의 mean과 variance를 구해서 Gaussian으로 근사해서 expected value 구합니다</li> <li>Q4 : 논문을 보면 frustum을 multi-variate Gaussian으로 근사하기 위해서는 먼저 \(F(x, o, d, \hat r, t_0, t_1)\) 의 mean, covariance를 계산해야 한다고 쓰여있던데<br/> appendix를 보면 indicator function인 F의 mean과 covariance가 아니라 conical frustum의 \(r, t, \theta\) 범위를 이용해서 공간 적분해서 \(t, x, y\) 축의 mean과 variance를 계산하지 않나요?</li> <li>A4 : 맞습니다. 논문에서 \(F(x, o, d, \hat r, t_0, t_1)\) 의 mean, covariance를 계산해야 한다고 언급되어 있는 것은 단순히 frustum 내부 범위에 속해있는 지점에 대해 적분을 통해 mean, variance를 구해야 한다는 뜻인 것 같습니다.</li> <li>Q5 : NeRF에서 rendering할 때는 EWA volume splatting과 같은 좌표계 변환을 고려하지 않아도 되나요?</li> <li>A5 : NeRF에서는 ray를 따라 MLP의 output을 alpha-compositing하여 직접 pixel 값을 얻어내므로 ray를 쓰기 위해 cam-to-world coordinate 변환만 필요하고, projection에 의한 non-linear 좌표계 변환과는 관련이 없다.<br/> 반면, Gaussian Splatting에서는 rendering할 때 3D Gaussian 자체를 직접 projection해서 쓰기 때문에 3D Gaussian covariance matrix on world-coordinate을 2D Gaussian covariance matrix on image-coordinate (ray-space)으로 projection해야 하므로 non-linear 좌표계 변환이 필요하다. 이를 위해 EWA volume splatting에 따라 non-linear transformation을 Taylor approx.하여 local affine transformation으로서 Jacobian을 사용한다</li> <li>Q6 : camera origin과 pixel 중심을 잇는 ray가 image plane에 수직이 아닌 pixel의 경우 \(\hat r\) 과 \(d\) 를 어떻게 정의하지?</li> <li>A6 : \(d\) 는 camera origin부터 pixel 중심까지의 거리 vector이고,<br/> cone 단면의 \(\hat r\)은 \(d\) 와 수직인 방향으로 \(\frac{2}{\sqrt{12}}\) of pixel-width 이므로<br/> cone 단면이 image plane 위에 있지 않은 꼴이 됨</li> </ul> <h2 id="appendix">Appendix</h2> <p>TBD</p> <h2 id="code-review">Code Review</h2> <h3 id="ipe-integrated-positional-encoding">IPE (integrated positional encoding)</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/11-480.webp 480w,/assets/img/2024-08-03-MipNeRF/11-800.webp 800w,/assets/img/2024-08-03-MipNeRF/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>self.scales : \([2^l, \ldots, 2^{L-1}]\) of shape (\(L-l\),)</li> <li>Let B : the number of cones<br/> Let S : the number of samples (regions) (Gaussians)</li> <li>case 1: Point-IPE<br/> position은 Gaussian으로 근사해서 IPE 씀 <ul> <li>x : \(\mu\) of shape (B, S, 3) and y : \(diag(\Sigma)\) of shape (B, S, 3)</li> <li>x_enc : \(\mu_{r}\) , 즉 PE-basis-lifted mean of shape (B, S, (\(L-l\)) * 6)<br/> where 6 = 3 (3d-vector) * 2 (sin and cos)<br/> \(\mu_{r} = P \mu = \begin{pmatrix} \begin{matrix} 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2\end{matrix} &amp; \cdots &amp; \begin{matrix} 2^{L-1} &amp; 0 &amp; 0 \\ 0 &amp; 2^{L-1} &amp; 0 \\ 0 &amp; 0 &amp; 2^{L-1}\end{matrix} \end{pmatrix}^T \begin{bmatrix} \mu_{x} \\ \mu_{y} \\ \mu_{z} \end{bmatrix}\)<br/> \(( \begin{bmatrix} \mu_{x} &amp; \mu_{y} &amp; \mu_{z} \\ \vdots &amp; \vdots &amp; \vdots \\ \mu_{x} &amp; \mu_{y} &amp; \mu_{z} \end{bmatrix} \circledast \begin{bmatrix} 2^l &amp; 2^l &amp; 2^l \\ \vdots &amp; \vdots &amp; \vdots \\ 2^{L-1} &amp; 2^{L-1} &amp; 2^{L-1} \end{bmatrix})\) .reshape(B, S, -1) \(= [\mu_{x}2^l, \mu_{y}2^l, \mu_{z}2^l, \ldots, \mu_{x}2^{L-1}, \mu_{y}2^{L-1}, \mu_{z}2^{L-1}]\) of shape ((\(L-l\)) * 3,)<br/> where \(\circledast\) is element-wise multiplication<br/> \(\cos{Px} = \sin{(Px + \frac{\pi}{2})}\)</li> <li>y_enc : \(diag(\Sigma_{r})\) , 즉 diagonal of PE-basis-lifted covariance of shape (B, S, (\(L-l\)) * 6)<br/> where 6 = 3 (3d-vector) * 2 (sin and cos)<br/> \(diag(\Sigma_{r}) = diag(P \Sigma P^T) = \left[ diag(\Sigma), 4 diag(\Sigma), \ldots , 4^{L-1}diag(\Sigma) \right]^T\)<br/> \(( \begin{bmatrix} \Sigma_{00} &amp; \Sigma_{11} &amp; \Sigma_{22} \\ \vdots &amp; \vdots &amp; \vdots \\ \Sigma_{00} &amp; \Sigma_{11} &amp; \Sigma_{22} \end{bmatrix} \circledast \begin{bmatrix} 4^l &amp; 4^l &amp; 4^l \\ \vdots &amp; \vdots &amp; \vdots \\ 4^{L-1} &amp; 4^{L-1} &amp; 4^{L-1} \end{bmatrix})\) .reshape(B, S, -1) \(= [\Sigma_{00}4^l, \Sigma_{11}4^l, \Sigma_{22}4^l, \ldots, \Sigma_{00}4^{L-1}, \Sigma_{11}4^{L-1}, \Sigma_{22}4^{L-1}]\)</li> <li>x_ret : \(\gamma (\mu, \Sigma)\) of shape (B, S, (\(L-l\)) * 6)<br/> \(\gamma (\mu, \Sigma) = E_{Px \sim N(\mu_{r}, \Sigma_{r})} [\begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}] = \begin{bmatrix} sin(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \\ cos(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \end{bmatrix}\)</li> <li>y_ret (covariance of \(\sin{z}\) where \(z \sim N(x_{enc}, y_{enc})\) ) 은 안 씀</li> </ul> </li> <li>case 2: View-Direction-PE<br/> view-direction은 IPE 말고 그냥 PE 씀 <ul> <li>x : view-direction of shape (B, S, 3) and y : None</li> <li>x_enc : \(Pd\) , 즉 PE-basis-lifted view-direction of shape (B, S, (\(L-l\)) * 6)</li> <li>x_ret : \(\gamma (d)\) of shape (B, S, (\(L-l\)) * 6)<br/> \(\gamma (d) = \begin{bmatrix} sin(Pd) \\ cos(Pd) \end{bmatrix}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/13-480.webp 480w,/assets/img/2024-08-03-MipNeRF/13-800.webp 800w,/assets/img/2024-08-03-MipNeRF/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/14-480.webp 480w,/assets/img/2024-08-03-MipNeRF/14-800.webp 800w,/assets/img/2024-08-03-MipNeRF/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> jax ver. </div> <h3 id="maxblur-filter">MaxBlur filter</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/12-480.webp 480w,/assets/img/2024-08-03-MipNeRF/12-800.webp 800w,/assets/img/2024-08-03-MipNeRF/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>\(w_k^{\ast} = \frac{1}{2}(max(w_{k-1}, w_k) + max(w_k, w_{k+1})) + \alpha\)<br/> where 빈 공간에서도 일부 samples 추출되도록 보장하기 위해 constant \(\alpha=0.01\) 설정</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/15-480.webp 480w,/assets/img/2024-08-03-MipNeRF/15-800.webp 800w,/assets/img/2024-08-03-MipNeRF/15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> jax ver. </div>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="rendering"/><category term="3d"/><category term="multiscale"/><category term="antialiasing"/><summary type="html"><![CDATA[A Multiscale Representation for Anti-Aliasing Neural Radiance Fields]]></summary></entry><entry><title type="html">Vim, Pycharm Debug Shortcut</title><link href="https://semyeong-yu.github.io/blog/2024/vim/" rel="alternate" type="text/html" title="Vim, Pycharm Debug Shortcut"/><published>2024-08-01T11:00:00+00:00</published><updated>2024-08-01T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/vim</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/vim/"><![CDATA[<h2 id="vim">Vim</h2> <p>vimtutor : 튜토리얼<br/> vim 파일명 : 노말모드 실행</p> <h3 id="mode">Mode</h3> <ul> <li> <p>입력모드 :<br/> i : 노말모드 &gt; 입력모드 (커서 앞)<br/> I : 노말모드 &gt; 입력모드 (문장 시작)<br/> a : 노말모드 &gt; 입력모드 (커서 뒤)<br/> A : 노말모드 &gt; 입력모드 (문장 끝)</p> </li> <li> <p>노말모드 :<br/> esc : 입력모드 &gt; 노말모드</p> </li> <li> <p>명령모드 :<br/> ‘:’ : 노말모드 &gt; 명령모드</p> </li> </ul> <h3 id="입력모드">입력모드</h3> <p>파일 수정</p> <h3 id="명령모드">명령모드</h3> <p>:q : 종료<br/> :q! : 강제 종료<br/> :w : 저장<br/> :w! : 강제 저장<br/> :wq : 저장 후 종료<br/> :wq! : 강제 저장 후 종료</p> <h3 id="노말모드">노말모드</h3> <h4 id="navigation">Navigation</h4> <ul> <li> <p>커서 :<br/> hjkl : 커서 이동</p> </li> <li> <p>line :<br/> 0 : line 앞<br/> $ : line 뒤<br/> 20G : 20번째 line 앞</p> </li> <li> <p>문단 :<br/> { : 문단 시작<br/> } : 문단 끝</p> </li> <li> <p>단어 :<br/> w : 다음 단어 앞<br/> 3w : 3번째 다음 단어 앞<br/> e : 다음 단어 뒤<br/> b : 이전 단어 앞<br/> 3b : 3번째 이전 단어 앞</p> </li> <li> <p>화면, 파일 :<br/> H : 화면 위<br/> M : 화면 중간<br/> L : 화면 끝<br/> gg : 파일 앞<br/> G : 파일 끝</p> </li> <li> <p>스크롤링 :<br/> Ctrl+u : 위로 스크롤링<br/> Ctrl+d : 아래로 스크롤링</p> </li> </ul> <h4 id="비주얼선택-잘라내기-복사-붙여넣기">비주얼(선택), 잘라내기, 복사, 붙여넣기</h4> <ul> <li> <p>비주얼(선택) 모드 :<br/> v : 비주얼(선택) 모드<br/> Ctrl+v : 블럭 단위 비주얼(선택) 모드<br/> v + hjkl : 드래그 선택<br/> v aw : 단어 1개 선택</p> </li> <li> <p>잘라내기 :<br/> x : 글자 잘라내기<br/> dd : line 잘라내기</p> </li> <li> <p>복사 :<br/> y : 복사<br/> yy : line 복사</p> </li> <li> <p>붙여넣기 :<br/> p : 붙여넣기<br/> “p 혹은 *p : 클립보드 붙여넣기</p> </li> </ul> <h4 id="반복-되감기-앞감기">반복, 되감기, 앞감기</h4> <ul> <li>. : 이전 명령 반복</li> <li>u : undo (되돌리기)</li> <li>Ctrl+r : redo</li> </ul> <h4 id="command--object-조합">Command + Object 조합</h4> <ul> <li> <p>예시 :<br/> d 3w : 다음 단어 3개 잘라내기<br/> d 2j : 아래 2줄 잘라내기<br/> c i[ : 대괄호 안에 있는 것을 변경</p> </li> <li> <p>Command :<br/> d : 잘라내기 (delete)<br/> y : 복사 (yank)<br/> c : 변경 (change)<br/> v : 선택 (visual)<br/> Ctrl+v : 블럭 단위 선택</p> </li> <li> <p>Object :<br/> 3w : 다음 단어 3개<br/> 3b : 이전 단어 3개<br/> aw : 단어 1개<br/> ap : 문단 1개<br/> as : line 1개<br/> i” : “ “ 안에 있는 것<br/> ip : 문단 안에 있는 것<br/> i{ : 중괄호 안에 있는 것<br/> i( : 소괄호 안에 있는 것<br/> a( : 소괄호 포함 모든 것<br/> a[ : 대괄포 포함 모든 것<br/> f( : 현재부터 소괄호(포함)까지<br/> t( : 현재부터 소괄호(미포함)까지<br/> /abc : 현재부터 abc(미포함)까지 (드래그 표시로 확인 가능)</p> </li> </ul> <h4 id="검색">검색</h4> <ul> <li>/<단어> : <단어> 검색 후 n 누르면 밑으로 계속 검색</단어></단어></li> <li>?<단어> : <단어> 검색 후 n 누르면 위로 계속 검색</단어></단어></li> <li>n : 계속 검색</li> </ul> <h2 id="pycharm-debug">Pycharm Debug</h2> <ul> <li> <p>실행 :<br/> Ctrl+F5 : 그냥 실행<br/> F9 : break point 설정<br/> F5 또는 우상단 벌레 버튼 : 디버깅 모드 실행 (첫 번째 break point 직전에서 멈춤)</p> </li> <li> <p>디버깅 모드 :<br/> F10 : 코드 한 줄 실행<br/> F11 : 함수 안으로 이동<br/> Shift+F11 : 함수 밖(호출 위치)로 이동<br/> F5 : 다음 breakpoint 직전에서 멈춤<br/> Shift+F5 또는 우상단 정지 버튼 : 디버깅 모드 해제</p> </li> </ul>]]></content><author><name></name></author><category term="cv-tasks"/><category term="vim"/><category term="pycharm"/><category term="debug"/><summary type="html"><![CDATA[vim, pycharm debug shortcut]]></summary></entry><entry><title type="html">Normalized Device Coordinates</title><link href="https://semyeong-yu.github.io/blog/2024/NDC/" rel="alternate" type="text/html" title="Normalized Device Coordinates"/><published>2024-07-30T15:00:00+00:00</published><updated>2024-07-30T15:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/NDC</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/NDC/"><![CDATA[<h2 id="ndc-normalized-device-coordinates">NDC: Normalized Device Coordinates</h2> <blockquote> <p>referenced blog :<br/> <a href="https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#background">https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#background</a></p> </blockquote> <h3 id="motivation">Motivation</h3> <p>NeRF에서<br/> MLP의 input은 3D world-coordinate이고,<br/> MLP의 output인 \(c, \sigma\) 를 accumulate해서 2D pixel-coordinate을 채운다<br/> 이 때, LLFF (Local Light Field Fusion) dataset 에 있는<br/> <code class="language-plaintext highlighter-rouge">unbounded (in single direction) 3D world-coordinate</code>의 scene 정보를<br/> <code class="language-plaintext highlighter-rouge">bounded 3D NDC space</code>로 project하면<br/> <code class="language-plaintext highlighter-rouge">MLP를 효율적으로 쓸 수 있다</code><br/> NDC space로의 projection 과정을 수식적으로 알아보고자 한다.</p> <h3 id="from-world-coordinate-to-ndc-to-pixel-coordinate">From world-coordinate To NDC To pixel-coordinate</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-30-NDC/1-480.webp 480w,/assets/img/2024-07-30-NDC/1-800.webp 800w,/assets/img/2024-07-30-NDC/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-30-NDC/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>camera transformation : <ul> <li><code class="language-plaintext highlighter-rouge">3D world-coordinate</code> (canonical-coordinate)<br/> \(\rightarrow\)<br/> <code class="language-plaintext highlighter-rouge">3D camera-coordinate</code></li> <li>extrinsic matrix \(\begin{bmatrix} R &amp; t \\ 0 &amp; 1 \end{bmatrix}\)</li> </ul> </li> <li>projection transformation : <ul> <li><code class="language-plaintext highlighter-rouge">3D camera-coordinate</code><br/> \(\rightarrow\)<br/> <code class="language-plaintext highlighter-rouge">3D NDC (normalized-device-coordinate)</code> (canonical view volume)</li> <li>normalized-device-coordinate (NDC) :<br/> <code class="language-plaintext highlighter-rouge">camera 원점이 중앙에 있는</code> \([-1, 1]^3\) cube (<code class="language-plaintext highlighter-rouge">정육면체</code>)</li> <li>frustum \(\rightarrow\) 직육면체 \(\rightarrow\) 정육면체<br/> consists of perspective projection and then orthographic projection<br/> z-axis 방향 바꾸기 포함</li> </ul> </li> <li>viewport transformation : <ul> <li><code class="language-plaintext highlighter-rouge">3D NDC</code><br/> \(\rightarrow\)<br/> <code class="language-plaintext highlighter-rouge">2D pixel-coordinate</code></li> <li>\([-1, 1]^3\) 의 NDC를 flatten하여 2 \(\times\) 2 square를 raster image로 mapping</li> <li>intrinsic matrix \(\begin{bmatrix} f_x &amp; s &amp; W/2 \\ 0 &amp; f_y &amp; H/2 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\) (초점거리 곱하고 원점 좌상단 이동)<br/> y-axis 방향 바꾸기 포함</li> </ul> </li> </ul> <h3 id="projection-transformation">Projection Transformation</h3> <blockquote> <p>Step 1. <code class="language-plaintext highlighter-rouge">Perspective Projection</code></p> </blockquote> <ul> <li>frustum을 bounded cuboid로 변환<br/> bound :<br/> \(x \in [l, r]\) where \(l \lt 0\), \(r \gt 0\)<br/> \(y \in [b, t]\) where \(b \lt 0\), \(t \gt 0\)<br/> \(z \in [f, n]\) where \(f \lt 0\), \(n \lt 0\)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-30-NDC/2-480.webp 480w,/assets/img/2024-07-30-NDC/2-800.webp 800w,/assets/img/2024-07-30-NDC/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-30-NDC/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-30-NDC/3-480.webp 480w,/assets/img/2024-07-30-NDC/3-800.webp 800w,/assets/img/2024-07-30-NDC/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-30-NDC/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 3D camera-coordinate </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-30-NDC/4-480.webp 480w,/assets/img/2024-07-30-NDC/4-800.webp 800w,/assets/img/2024-07-30-NDC/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-30-NDC/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>\(z = n\) plane은 그대로 냅두고, 직육면체 꼴이 되도록 그 뒤 plane 변환<br/> camera를 통과하는 any line은 z-axis에 평행한 line이 됨</p> </li> <li> <p>perspective projection matrix :<br/> \(P_{per} = \begin{bmatrix} n &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; n &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; n+f &amp; -nf \\ 0 &amp; 0 &amp; 1 &amp; 0 \end{bmatrix}\)<br/> \(P_{per} \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix} = \begin{bmatrix} nX \\ nY \\ (n+f)Z - nf \\ Z \end{bmatrix}\)<br/> <code class="language-plaintext highlighter-rouge">?????</code></p> </li> </ul> <blockquote> <p>Step 2. <code class="language-plaintext highlighter-rouge">Orthographic Projection</code></p> </blockquote> <ul> <li> <p>corner (l, b, n)이 원점이 되도록 shift한 뒤,<br/> \([0, r-l] \times [0, t-b] \times [f-n, 0]\) 의 직육면체를 \([0, 2] \times [0, 2] \times [-2, 0]\) 의 정육면체로 scale한 뒤,<br/> center (1, 1, -1)이 원점이 되도록 \([-1, 1]^3\) 으로 shift</p> </li> <li> <p>orthographic projection matrix :<br/> \(M_{orth} = \begin{pmatrix} I_{3 \times 3} &amp; \begin{matrix} -1 \\ -1 \\ 1 \end{matrix} \\ 0_{1 \times 3} &amp; 1 \end{pmatrix} \begin{pmatrix} \begin{matrix} \frac{2}{r-l} &amp; 0 &amp; 0 \\ 0 &amp; \frac{2}{t-b} &amp; 0 \\ 0 &amp; 0 &amp; \frac{2}{n-f} \end{matrix} &amp; 0_{3 \times 1} \\ 0_{1 \times 3} &amp; 1 \end{pmatrix} \begin{pmatrix} I_{3 \times 3} &amp; \begin{matrix} -l \\ -b \\ -n \end{matrix} \\ 0_{1 \times 3} &amp; 1 \end{pmatrix}\)<br/> \(= \begin{bmatrix} \frac{2}{r-l} &amp; 0 &amp; 0 &amp; -\frac{r+l}{r-l} \\ 0 &amp; \frac{2}{t-b} &amp; 0 &amp; -\frac{t+b}{t-b} \\ 0 &amp; 0 &amp; \frac{2}{n-f} &amp; -\frac{n+f}{n-f} \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix}\)</p> </li> </ul> <blockquote> <p>Step 3. <code class="language-plaintext highlighter-rouge">Projection Matrix</code></p> </blockquote> <p>Since perspective projection matrix is scalable,<br/> \(M_{proj} = M_{orth} (- P_{per})\)<br/> \(= \begin{bmatrix} \frac{2}{r-l} &amp; 0 &amp; 0 &amp; -\frac{r+l}{r-l} \\ 0 &amp; \frac{2}{t-b} &amp; 0 &amp; -\frac{t+b}{t-b} \\ 0 &amp; 0 &amp; \frac{2}{n-f} &amp; -\frac{n+f}{n-f} \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix} \begin{bmatrix} -n &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; -n &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; -n-f &amp; nf \\ 0 &amp; 0 &amp; -1 &amp; 0 \end{bmatrix}\)<br/> \(= \begin{bmatrix} -\frac{2n}{r-l} &amp; 0 &amp; \frac{r+l}{r-l} &amp; 0 \\ 0 &amp; -\frac{2n}{t-b} &amp; \frac{t+b}{t-b} &amp; 0 \\ 0 &amp; 0 &amp; -\frac{n+f}{n-f} &amp; \frac{2nf}{n-f} \\ 0 &amp; 0 &amp; -1 &amp; 0 \end{bmatrix}\)</p> <p>camera-coordinate에서 \(z \in [f, n]\) where \(f \lt 0\), \(n \lt 0\) 이었는데,<br/> NDC에서는 z-axis의 방향이 반대이므로<br/> \(f \lt 0\), \(n \lt 0\) 대신 \(f = -f \gt 0\), \(n = -n \gt 0\) 를 대입하면,<br/> \(M_{proj} = \begin{bmatrix} \frac{2n}{r-l} &amp; 0 &amp; \frac{r+l}{r-l} &amp; 0 \\ 0 &amp; \frac{2n}{t-b} &amp; \frac{t+b}{t-b} &amp; 0 \\ 0 &amp; 0 &amp; -\frac{n+f}{n-f} &amp; -\frac{2nf}{n-f} \\ 0 &amp; 0 &amp; -1 &amp; 0 \end{bmatrix}\)</p> <p>OpenGL과 같은 graphics frameworks에서는 보통<br/> \(M_{proj}X\) 를 \(M_{proj}X\) 의 fourth entry로 나눴을 때 \(M_{proj}X\) 의 third entry(Z 값)이 양수가 되도록 하기 때문에 (아래 Step 4의 NDC 참고)<br/> 조금 수정하면<br/> 최종적인 projection matrix는<br/> \(M_{proj} = \begin{bmatrix} \frac{2n}{r-l} &amp; 0 &amp; \frac{r+l}{r-l} &amp; 0 \\ 0 &amp; \frac{2n}{t-b} &amp; \frac{t+b}{t-b} &amp; 0 \\ 0 &amp; 0 &amp; -\frac{n+f}{f-n} &amp; -\frac{2nf}{f-n} \\ 0 &amp; 0 &amp; -1 &amp; 0 \end{bmatrix}\)</p> <p>camera frustum은 보통 symmetric하므로 \(l = -r\), \(b = -t\) 라 했을 때<br/> projection matrix는<br/> \(M_{proj} = \begin{bmatrix} \frac{n}{r} &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; \frac{n}{t} &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; -\frac{f+n}{f-n} &amp; -\frac{2nf}{f-n} \\ 0 &amp; 0 &amp; -1 &amp; 0 \end{bmatrix}\)</p> <blockquote> <p>Step 4. from <code class="language-plaintext highlighter-rouge">camera-coordinate</code> to <code class="language-plaintext highlighter-rouge">NDC</code></p> </blockquote> <ul> <li> <p>camera-coordinate :<br/> \(\boldsymbol X = \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}\)<br/> where \(Z \lt 0\)</p> </li> <li> <p>NDC :</p> <ul> <li>\(\begin{bmatrix} -\frac{n}{r}\frac{X}{Z} \\ -\frac{n}{t}\frac{Y}{Z} \\ \frac{f+n}{f-n} + \frac{2nf}{f-n}\frac{1}{Z} \\ 1 \end{bmatrix} = \boldsymbol x \sim M_{proj} \boldsymbol X = \begin{bmatrix} \frac{n}{r}X \\ \frac{n}{t}Y \\ -\frac{f+n}{f-n}Z -\frac{2nf}{f-n} \\ -Z \end{bmatrix}\)<br/> where \(Z \lt 0\) and \(n, f \gt 0\)</li> <li>검토해보면, \(Z = -n\) 은 \(x_Z = -1\) 로 mapping되고, \(Z = -f\) 는 \(x_Z = 1\) 로 잘 mapping되네~</li> <li>Let<br/> \(a_x = -\frac{n}{r}\)<br/> \(a_y = -\frac{n}{t}\)<br/> \(a_z = \frac{f+n}{f-n}\)<br/> \(b_z = \frac{2nf}{f-n}\)<br/> Then \(\boldsymbol x = \begin{bmatrix} a_x\frac{X}{Z} \\ a_y\frac{Y}{Z} \\ a_z + \frac{b_z}{Z} \\ 1 \end{bmatrix} = \begin{bmatrix} a_x\frac{X}{Z} \\ a_y\frac{Y}{Z} \\ a_z + \frac{b_z}{Z} \end{bmatrix}\)</li> </ul> </li> </ul> <h3 id="linear-in-disparity">Linear in Disparity</h3> <ul> <li>출처 : https://charlieppark.kr</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-30-NDC/5-480.webp 480w,/assets/img/2024-07-30-NDC/5-800.webp 800w,/assets/img/2024-07-30-NDC/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-30-NDC/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>LLFF (Local Light Field Fusion) : <ul> <li>Nyquist-rate에 따르면 카메라가 \(\frac{1}{2K_xf(\frac{1}{z_{min}}-\frac{1}{z_{max}})}\) 보다 촘촘히 있어야 한다<br/> \(\Delta_{u} \leq \frac{1}{2K_xf(\frac{1}{z_{min}}-\frac{1}{z_{max}})}\)<br/> where \(K_x = \text{min}(B_x, \frac{1}{2\Delta_{x})\)</li> <li>LLFF dataset에서 다루는 scene은 unbounded in single direction (front-facing) 이므로 \(z_{max} = \infty\) 이므로<br/> \(\Delta_{u} \leq \frac{1}{2K_xf\frac{1}{z_{min}}}\)</li> <li>즉, \(z_{min}\) 이 작을수록 (<code class="language-plaintext highlighter-rouge">물체가 가까이 있을수록</code>)<br/> 더 <code class="language-plaintext highlighter-rouge">촘촘한 view-sampling이 필요</code>하며<br/> <code class="language-plaintext highlighter-rouge">high-freq.</code> detail을 많이 가지고 있다는 의미이다</li> </ul> </li> <li>\(\begin{bmatrix} -\frac{n}{r}\frac{X}{Z} \\ -\frac{n}{t}\frac{Y}{Z} \\ \frac{f+n}{f-n} + \frac{2nf}{f-n}\frac{1}{Z} \\ 1 \end{bmatrix} = \boldsymbol x \sim M_{proj} \boldsymbol X = \begin{bmatrix} \frac{n}{r}X \\ \frac{n}{t}Y \\ -\frac{f+n}{f-n}Z -\frac{2nf}{f-n} \\ -Z \end{bmatrix}\)<br/> where \(Z \lt 0\) and \(n, f \gt 0\) 에서<br/> \(Z\) 축에 해당하는 \(\frac{f+n}{f-n} + \frac{2nf}{f-n}\frac{1}{Z}\) 만 보면<br/> <code class="language-plaintext highlighter-rouge">NDC space의 깊이</code> 값은 원래 camera coordinate의 깊이 값의 역수, 즉 <code class="language-plaintext highlighter-rouge">camera-coordinate의 disparity에 비례</code>한다는 것을 알 수 있다<br/> 즉, NDC space에서의 depth distance에 따라 stratified uniform sampling한다면<br/> 원래 camera coordinate의 disparity에 비례하여 sampling하는 효과를 가진다</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-30-NDC/6-480.webp 480w,/assets/img/2024-07-30-NDC/6-800.webp 800w,/assets/img/2024-07-30-NDC/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-30-NDC/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="projection-in-nerf-ray">Projection in NeRF ray</h3> <p>any 3D points on ray \(r = o + td\)를<br/> NDC space (camera 원점이 중앙에 있는 \([-1, 1]^3\) cube) 로 projection하면<br/> 3D points on projected ray \(r^{\ast} = o^{\ast} + t^{\ast} d^{\ast}\) 가 된다<br/> 위에서 유도한 Projection Matrix 를 사용하면<br/> \(\boldsymbol x = \begin{bmatrix} a_x\frac{o_x + td_x}{o_z + td_z} \\ a_y\frac{o_y + td_y}{o_z + td_z} \\ a_z + \frac{b_z}{o_z + td_z} \end{bmatrix} = \begin{bmatrix} o_x^{\ast} + t^{\ast} d_x^{\ast} \\ o_y^{\ast} + t^{\ast} d_y^{\ast} \\ o_z^{\ast} + t^{\ast} d_z^{\ast} \end{bmatrix}\)</p> <p>먼저 projected 원점 좌표를 구해보자<br/> \(t = t^{\ast} = 0\) 를 대입하면<br/> \(o^{\ast} = \begin{bmatrix} o_x^{\ast} \\ o_y^{\ast} \\ o_z^{\ast} \end{bmatrix} = \begin{bmatrix} a_x\frac{o_x}{o_z} \\ a_y\frac{o_y}{o_z} \\ a_z + \frac{b_z}{o_z} \end{bmatrix}\)</p> <p>다음으로 projected t와 d를 구해보자<br/> \(\begin{bmatrix} t^{\ast} d_x^{\ast} \\ t^{\ast} d_y^{\ast} \\ t^{\ast} d_z^{\ast} \end{bmatrix} = \begin{bmatrix} a_x\frac{o_x + td_x}{o_z + td_z} \\ a_y\frac{o_y + td_y}{o_z + td_z} \\ a_z + \frac{b_z}{o_z + td_z} \end{bmatrix} - \begin{bmatrix} o_x^{\ast} \\ o_y^{\ast} \\ o_z^{\ast} \end{bmatrix}\)<br/> \(= \begin{bmatrix} a_x\frac{o_x + td_x}{o_z + td_z} - a_x\frac{o_x}{o_z} \\ a_y\frac{o_y + td_y}{o_z + td_z} - a_y\frac{o_y}{o_z} \\ a_z + \frac{b_z}{o_z + td_z} - (a_z + \frac{b_z}{o_z}) \end{bmatrix}\)<br/> \(= \begin{bmatrix} a_x\frac{td_z}{o_z + td_z}(\frac{d_x}{d_z} - \frac{o_x}{o_z}) \\ a_y\frac{td_z}{o_z + td_z}(\frac{d_y}{d_z} - \frac{o_y}{o_z}) \\ -b_z\frac{td_z}{o_z + td_z}\frac{1}{o_z} \end{bmatrix}\)<br/> \(= \frac{td_z}{o_z + td_z} \begin{bmatrix} a_x(\frac{d_x}{d_z} - \frac{o_x}{o_z}) \\ a_y(\frac{d_y}{d_z} - \frac{o_y}{o_z}) \\ -b_z\frac{1}{o_z} \end{bmatrix}\)</p> <blockquote> <p>Result</p> </blockquote> <p>ray \(r = o + td\)를<br/> NDC space (camera 원점이 중앙에 있는 \([-1, 1]^3\) cube) 로 projection 했을 때<br/> projected ray \(r^{\ast} = o^{\ast} + t^{\ast} d^{\ast}\) 는 아래와 같이 구할 수 있다</p> <p>\(o^{\ast} = \begin{bmatrix} o_x^{\ast} \\ o_y^{\ast} \\ o_z^{\ast} \end{bmatrix} = \begin{bmatrix} a_x\frac{o_x}{o_z} \\ a_y\frac{o_y}{o_z} \\ a_z + \frac{b_z}{o_z} \end{bmatrix}\)<br/> and<br/> \(t^{\ast} = \frac{td_z}{o_z + td_z} = 1 - \frac{o_z}{o_z + td_z}\)<br/> and<br/> \(d^{\ast} = \begin{bmatrix} d_x^{\ast} \\ d_y^{\ast} \\ d_z^{\ast} \end{bmatrix} = \begin{bmatrix} a_x(\frac{d_x}{d_z} - \frac{o_x}{o_z}) \\ a_y(\frac{d_y}{d_z} - \frac{o_y}{o_z}) \\ -b_z\frac{1}{o_z} \end{bmatrix}\)</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">Ray Projection to NDC 장점</code></p> </blockquote> <ul> <li> <p>ray에서 \(t \in [0, \infty)\) 였다면 projected ray에서 \(t^{\ast} \in [0, 1)\)<br/> LLFF dataset에서<br/> camera에서 출발한 ray가 아무 object도 “hit”하지 않는다면 \(t = \infty\)일텐데,<br/> NDC (bounded cube)로 warp한다면 \(t^{\ast} \in [0, 1)\) 이므로<br/> MLP 효율적으로 쓸 수 있음</p> </li> <li> <p>single-direction이긴 하지만<br/> NDC space에서의 depth distance에 따라 stratified uniform sampling한다면<br/> 원래 camera coordinate의 disparity에 비례하여 sampling하는 효과를 가지므로<br/> 가까이 있는 content는 많이 sampling하고 멀리 있는 content는 덜 sampling함으로써<br/> 임의의 scale의 unbounded scene을 잘 다룰 수 있음</p> </li> </ul> <blockquote> <p>Projection transformation 한계</p> </blockquote> <p>LLFF dataset과 같이 <code class="language-plaintext highlighter-rouge">single</code> direction으로만 unbounded된 camera frustum, 즉 <code class="language-plaintext highlighter-rouge">front-facing scene</code>에 대해서만 적용 가능하고<br/> unbounded 360 scene에 대해서는 기본 NeRF가 잘 수행 못함<br/> \(\rightarrow\) MipNeRF360 등 NeRF 후속 연구에서 해결됨</p> <blockquote> <p>특정 case</p> </blockquote> <p>\(f_{cam}\)이 camera의 focal length이고,<br/> \(W, H\)가 image plane의 width, height in pix 일 때<br/> <code class="language-plaintext highlighter-rouge">image plane이 정확히 camera frustum의 near plane</code>에 있고<br/> <code class="language-plaintext highlighter-rouge">camera frustum의 far plane을 infinity로 확장</code>하도록<br/> camera를 설정하면,<br/> \(z = -n = -f_{cam} \lt 0\), \(r = \frac{W}{2}\), \(t = \frac{H}{2}\), \(z = -f \rightarrow -\infty\) 이므로</p> <p>\(a_x = -\frac{n}{r} = -\frac{f_{cam}}{\frac{W}{2}}\)<br/> \(a_y = -\frac{n}{t} = -\frac{f_{cam}}{\frac{H}{2}}\)<br/> \(\lim_{f \rightarrow \infty} a_z = \lim_{f \rightarrow \infty} \frac{f+n}{f-n} = 1\)<br/> \(\lim_{f \rightarrow \infty} b_z = \lim_{f \rightarrow \infty} \frac{2nf}{f-n} = 2n\)<br/> 이므로</p> <p>ray \(r = o + td\) 를 NDC로 projection했을 때<br/> projected ray \(r^{\ast} = o^{\ast} + t^{\ast} d^{\ast}\) 에서<br/> \(o^{\ast} = \begin{bmatrix} -\frac{f_{cam}}{\frac{W}{2}}\frac{o_x}{o_z} \\ -\frac{f_{cam}}{\frac{H}{2}}\frac{o_y}{o_z} \\ 1 + \frac{2n}{o_z} \end{bmatrix}\)<br/> and<br/> \(t^{\ast} = \frac{td_z}{o_z + td_z} = 1 - \frac{o_z}{o_z + td_z}\)<br/> and<br/> \(d^{\ast} = \begin{bmatrix} -\frac{f_{cam}}{\frac{W}{2}}(\frac{d_x}{d_z} - \frac{o_x}{o_z}) \\ -\frac{f_{cam}}{\frac{H}{2}}(\frac{d_y}{d_z} - \frac{o_y}{o_z}) \\ -2n\frac{1}{o_z} \end{bmatrix}\)</p> <h3 id="ndc-projection-in-nerf-pytorch-code">NDC projection in NeRF Pytorch code</h3> <p><a href="https://github.com/yenchenlin/nerf-pytorch">NeRF-Pytorch</a> 기준으로<br/> run_nerf_helpers.py의 ndc_rays()에 구현되어 있으며<br/> 자세한 설명은 <a href="https://semyeong-yu.github.io/blog/2024/NeRFcode/">NeRF-Code-Review</a>에 있음</p>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="NDC"/><category term="3d"/><summary type="html"><![CDATA[How NDC Works for Ray]]></summary></entry><entry><title type="html">State Space Model</title><link href="https://semyeong-yu.github.io/blog/2024/SSM/" rel="alternate" type="text/html" title="State Space Model"/><published>2024-07-18T15:00:00+00:00</published><updated>2024-07-18T15:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/SSM</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/SSM/"><![CDATA[<h2 id="state-space-model">State Space Model</h2> <blockquote> <p>참고 논문 :<br/> <a href="https://arxiv.org/abs/2406.07887">https://arxiv.org/abs/2406.07887</a><br/> 참고 강연 :<br/> by NVIDIA Wonmin Byeon</p> </blockquote> <h3 id="abstract">Abstract</h3> <ul> <li>Large Language Models (LLMs) are usually based on <code class="language-plaintext highlighter-rouge">Transformer</code> architectures. <ul> <li>Transformer-based models 장점 :<br/> highly <code class="language-plaintext highlighter-rouge">parallelizable</code><br/> can model <code class="language-plaintext highlighter-rouge">massive amounts of data</code></li> <li>Transformer-based models 단점 :<br/> significant <code class="language-plaintext highlighter-rouge">computational overhead</code> due to the <code class="language-plaintext highlighter-rouge">quadratic self-attention</code> calculations, especially on longer sequences<br/> large inference-time <code class="language-plaintext highlighter-rouge">memory requirements</code> from the <code class="language-plaintext highlighter-rouge">key-value cache</code></li> </ul> </li> <li>More recently, <code class="language-plaintext highlighter-rouge">State Space Models (SSM)</code> like Mamba have been shown to have fast parallelizable training and inference as an alternative of Transformer.<br/> In this talk, I present the strengths and weaknesses of <code class="language-plaintext highlighter-rouge">Mamba, Mamba-2, and Transformer models</code> at larger scales. I also introduce a <code class="language-plaintext highlighter-rouge">hybrid architecture consisting of Mamba-2, attention, and MLP layers</code>.<br/> While pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks that require <code class="language-plaintext highlighter-rouge">strong copying</code> or <code class="language-plaintext highlighter-rouge">in-context learning</code> abilities.<br/> In contrast, the hybrid model closely matches or exceeds the Transformer on all standard and long-context tasks and is predicted to be up to 8x faster when generating tokens at inference time.</li> </ul> <h3 id="is-attention-all-we-need">Is Attention All We Need?</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/4-480.webp 480w,/assets/img/2024-07-18-SSM/4-800.webp 800w,/assets/img/2024-07-18-SSM/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Transformer : <ul> <li>fast training due to parallelization</li> <li>slow inference for long sequence(context) <ul> <li>key-value cache can improve speed, but increase GPU memory</li> </ul> </li> </ul> </li> <li>RNN : <ul> <li>slow training due to no parallelization</li> <li>fast inference because scale linearly with sequence length</li> </ul> </li> <li>Mamba : <ul> <li>fast training</li> <li>fast inference because scale linearly with sequence length and can deal with unbounded context</li> </ul> </li> <li> <p>SSM or RNN :<br/> state = fixed-sized vector (compression)<br/> high efficiency, but low performance</p> </li> <li>Transformer :<br/> cache of entire history (no compression)<br/> high performance, but low efficiency</li> </ul> <h3 id="mamba-linear-time-sequence-modeling-with-selective-state-spaces">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</h3> <ul> <li>SSM</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/1-480.webp 480w,/assets/img/2024-07-18-SSM/1-800.webp 800w,/assets/img/2024-07-18-SSM/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Selective SSM :<br/> matrix B, C and step size are dependent on the input</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/2-480.webp 480w,/assets/img/2024-07-18-SSM/2-800.webp 800w,/assets/img/2024-07-18-SSM/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Parallel scan :<br/> The order does not matter through the associative property, so can calculate sequences in part and iteratively combine them</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/3-480.webp 480w,/assets/img/2024-07-18-SSM/3-800.webp 800w,/assets/img/2024-07-18-SSM/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Hardware-aware implementation :<br/> minimize copying between RAMs</li> </ul> <h3 id="mamba-2">Mamba-2</h3> <ul> <li>Mamba에서 Main Bottleneck이 Parallel scan 부분이었는데,<br/> Mamba-2는 divide input into chunks 등 architecture 개선으로 이를 해결하고자 했음</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/13-480.webp 480w,/assets/img/2024-07-18-SSM/13-800.webp 800w,/assets/img/2024-07-18-SSM/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="limitations-of-mamba">Limitations of Mamba</h3> <ul> <li>Poor at MMLU and Phonebook task<br/> 아래를 요구하는 task에 대해서는 Mamba가 잘 못함 <ul> <li>in-context learning</li> <li>info. routing between tokens</li> <li>copying from the context (bad on long-context tasks)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/5-480.webp 480w,/assets/img/2024-07-18-SSM/5-800.webp 800w,/assets/img/2024-07-18-SSM/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/6-480.webp 480w,/assets/img/2024-07-18-SSM/6-800.webp 800w,/assets/img/2024-07-18-SSM/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="hybrid-architecture-of-mamba-and-transformer">Hybrid Architecture of Mamba and Transformer</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/7-480.webp 480w,/assets/img/2024-07-18-SSM/7-800.webp 800w,/assets/img/2024-07-18-SSM/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Our Hybrid Mamba-Transformer Model <ul> <li>Minimize the number of Attention Layers and Maximize the number of MLPs</li> <li>Does not necessarily need Rotary Position Embedding (RoPE)</li> <li>evenly spread attention and MLP layers</li> <li>Place Mamba layer at the beginning, so has no position embedding</li> <li>Group-Query Attention (GQA) makes more efficient</li> <li>Global Attention makes better performance</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/11-480.webp 480w,/assets/img/2024-07-18-SSM/11-800.webp 800w,/assets/img/2024-07-18-SSM/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Mamba-2 Hybrid<br/> Inference Speed is fast<br/> Now, states in Mamba can understand longer history!</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/12-480.webp 480w,/assets/img/2024-07-18-SSM/12-800.webp 800w,/assets/img/2024-07-18-SSM/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Attention Layer is bottleneck at Hybrid model,<br/> so Context Length가 길어질수록 Speedup 증가율은 줄어듬</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/8-480.webp 480w,/assets/img/2024-07-18-SSM/8-800.webp 800w,/assets/img/2024-07-18-SSM/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/9-480.webp 480w,/assets/img/2024-07-18-SSM/9-800.webp 800w,/assets/img/2024-07-18-SSM/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="summary">Summary</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/10-480.webp 480w,/assets/img/2024-07-18-SSM/10-800.webp 800w,/assets/img/2024-07-18-SSM/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 왼쪽부터 4K, 16K, 32K-based models </div> <p>Mamba-2 Hybrid는 Transformer와 달리 Quadratic calculation까지 필요 없고 inference 빠름<br/> but, Attention Layer가 Bottleneck이듯이 해결해야 할 사항들이 남아 있어 앞으로도 발전 가능성 있음</p>]]></content><author><name></name></author><category term="cv-tasks"/><category term="SSM"/><category term="Mamba"/><summary type="html"><![CDATA[SSM]]></summary></entry><entry><title type="html">3D Gaussian Splatting</title><link href="https://semyeong-yu.github.io/blog/2024/GS/" rel="alternate" type="text/html" title="3D Gaussian Splatting"/><published>2024-07-11T10:00:00+00:00</published><updated>2024-07-11T10:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/GS</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/GS/"><![CDATA[<h2 id="3d-gaussian-splatting-for-real-time-radiance-field-rendering">3D Gaussian Splatting for Real-Time Radiance Field Rendering</h2> <h4 id="bernhard-kerbl-georgios-kopanas-thomas-leimkühler-george-drettakis">Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, George Drettakis</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2308.04079">https://arxiv.org/abs/2308.04079</a><br/> project website :<br/> <a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/</a><br/> code :<br/> <a href="https://github.com/graphdeco-inria/gaussian-splatting">https://github.com/graphdeco-inria/gaussian-splatting</a><br/> referenced blog :<br/> <a href="https://xoft.tistory.com/51">https://xoft.tistory.com/51</a></p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/2-480.webp 480w,/assets/img/2024-07-11-GS/2-800.webp 800w,/assets/img/2024-07-11-GS/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="abstract">Abstract</h2> <ul> <li>novel 3D Gaussian scene representation with real-time differentiable renderer<br/> <code class="language-plaintext highlighter-rouge">수많은 3D Gaussian이 모여 scene을 구성</code>하고 있다!</li> <li>Very Fast rendering (\(\geq\) 100 FPS) :<br/> real-time as \(\geq\) 30 FPS<br/> rasterization이 optimization의 main bottleneck인데, 3DGS는 fast rasterization 가짐</li> <li>Higher Quality than SOTA Mip-NeRF360(2022)</li> <li>Faster Training than SOTA InstantNGP(2022)</li> </ul> <h2 id="introduction">Introduction</h2> <h3 id="why-3d-gaussian">Why 3D Gaussian?</h3> <p>3D scene representation 방법</p> <ol> <li><code class="language-plaintext highlighter-rouge">Mesh or Point</code> <ul> <li>explicit</li> <li>good for fast GPU/CUDA-based rasterization(3D \(\rightarrow\) 2D)</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">NeRF</code> method <ul> <li>implicit (MLP로 geometry 및 appearance를 표현)</li> <li>ray marching</li> <li>continuous coordinate-based representation</li> <li>interpolate values stored in voxels, hash grids, or points</li> <li>But,,, continuous ray로부터 discrete points를 뽑아 내는 <code class="language-plaintext highlighter-rouge">stochastic sampling</code> for rendering 때문에 <code class="language-plaintext highlighter-rouge">연산량이 많고 noise</code> 생김</li> <li>MLP는 dot product 및 더하기(kernel regression)의 특성상 <code class="language-plaintext highlighter-rouge">orthogonality</code>를 흐리기 때문에 high-freq. output을 잘 표현할 수 없어서 따로 미리 positional encoding을 수행</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">3D Gaussian</code> method <ul> <li>explicit (3D Gaussian으로 geometry를, SH coeff.로 appearance를 표현)</li> <li>differentiable volumetric representation</li> <li>efficient rasterization(projection and \(\alpha\)-blending)</li> <li>3D Gaussian(ellipsoid)이나 SH coeff.라는 explicit 표현 자체가 <code class="language-plaintext highlighter-rouge">orthogonality</code>를 잘 살리기 때문에 high-freq. output 잘 표현 가능</li> </ul> </li> </ol> <h3 id="rendering-nerf-vs-3dgs">Rendering (NeRF vs 3DGS)</h3> <ul> <li>NeRF : <ul> <li>ray per pixel 쏴서 coarse(stratified) and fine(PDF) sampling하고,</li> <li>MLP로 sampled points의 color 및 volume density를 구하고,</li> <li>이 값들을 volume rendering 식으로 summation</li> </ul> </li> <li>3DGS : <ul> <li>image를 tile(14 \(\times\) 14 pixel)들로 나누고,</li> <li>tile마다 Gaussian을 Depth에 따라 정렬한 뒤</li> <li>앞에서부터 뒤로 \(\alpha\)-blending</li> </ul> </li> </ul> <h2 id="related-work">Related Work</h2> <p>생략 (추후에 다시 볼 수도)</p> <h2 id="overview">Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/1-480.webp 480w,/assets/img/2024-07-11-GS/1-800.webp 800w,/assets/img/2024-07-11-GS/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For unbounded and complete scenes,<br/> For 1080p high resolution and real-time(\(\geq\) 30 fps) rendering,</p> <ol> <li><code class="language-plaintext highlighter-rouge">input</code> : <ul> <li>Most point-based methods require <code class="language-plaintext highlighter-rouge">MVS</code>(Multi-View Stereo) data,<br/> but 3DGS only needs <code class="language-plaintext highlighter-rouge">SfM points</code> for initialization</li> <li>COLMAP 등 SfM(Structure-from-Motion) camera calibration으로 얻은 <code class="language-plaintext highlighter-rouge">sparse point cloud</code>에서 시작해서<br/> scene을 3D Gaussians로 나타냄으로써<br/> <code class="language-plaintext highlighter-rouge">empty space에서의 불필요한 계산을 하지 않도록</code> continuous volumetric radiance fields 정보를 저장</li> <li>NeRF-synthetic dataset의 경우 bg가 없어서 3DGS random initialization으로도 좋은 퀄리티 달성</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">optimization</code> interleaved with <code class="language-plaintext highlighter-rouge">adaptive density control</code> : <ul> <li>optimize 4 parameters :<br/> 3D position(mean), anisotropic covariance, opacity, and spherical harmonic coeff.(color)<br/> <code class="language-plaintext highlighter-rouge">highly anisotropic volumetric splats</code>는 <code class="language-plaintext highlighter-rouge">fine structures</code>를 compact하게 나타낼 수 있음!!<br/> <code class="language-plaintext highlighter-rouge">spherical harmonics</code>를 통해 <code class="language-plaintext highlighter-rouge">directional appearance(color)</code>를 잘 나타낼 수 있음!!<d-cite key="Plenoxels">[1]</d-cite>, <d-cite key="InstantNGP">[2]</d-cite></li> <li>adaptive density control :<br/> gradient 기반으로 Gaussian 형태를 변화시키기 위해, add and occasionally remove 3D Gaussians during optimization</li> </ul> </li> <li>differentiable visibility-aware <code class="language-plaintext highlighter-rouge">real-time rendering</code> :<br/> perform \(\alpha\)-blending of <code class="language-plaintext highlighter-rouge">anisotropic splats</code> respecting visibility order<br/> by fast <code class="language-plaintext highlighter-rouge">GPU sorting</code> algorithm and <code class="language-plaintext highlighter-rouge">tile-based rasterization</code>(projection and \(\alpha\)-blending)<br/> 한편, accumulated \(\alpha\) values를 tracking함으로써 <code class="language-plaintext highlighter-rouge">Gaussians 수에 제약 없이</code> 빠른 backward pass도 가능</li> </ol> <hr/> <h3 id="pseudo-code">Pseudo-Code</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/3-480.webp 480w,/assets/img/2024-07-11-GS/3-800.webp 800w,/assets/img/2024-07-11-GS/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>빨간 박스 : initialization<br/> 파란 박스 : optimization<br/> 초록 박스 : 특정 iter.마다 Gaussian을 clone, split, remove</p> <h2 id="differentiable-3d-gaussian-splatting">Differentiable 3D Gaussian Splatting</h2> <h3 id="3d-gaussian">3D Gaussian</h3> <ul> <li> <p><code class="language-plaintext highlighter-rouge">differentiable</code> volumetric representation의 특성을 가지고 있으면서도 빠른 rendering을 위해 <code class="language-plaintext highlighter-rouge">unstructured and explicit</code>한 게 무엇이 있을까?<br/> \(\rightarrow\) 3D Gaussian !!</p> </li> <li> <p>a point를 a small planar circle with a normal이라고 가정하는 이전 Point-based rendering 논문들 <d-cite key="Point1">[3]</d-cite> <d-cite key="Point2">[4]</d-cite> 과 달리<br/> <code class="language-plaintext highlighter-rouge">SfM points는 sparse해서 normals(법선)를 estimate하기 어려울</code> 뿐만 아니라, estimate 한다 해도 very noisy normals를 optimize하는 것은 매우 어렵<br/> \(\rightarrow\) normals 필요 없는 3D Gaussians !!<br/> k-dim. Gaussian : \(G(\boldsymbol x) = (2\pi)^{-\frac{k}{2}}det(\Sigma)^{-\frac{1}{2}}e^{-\frac{1}{2}(\boldsymbol x - \boldsymbol \mu)^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu)}\)</p> </li> </ul> <h2 id="parameters-to-train">Parameters to train</h2> <ol> <li><code class="language-plaintext highlighter-rouge">scale vector</code> \(s\) and <code class="language-plaintext highlighter-rouge">quaternion</code> \(q\) for <code class="language-plaintext highlighter-rouge">covariance matrix</code></li> <li><code class="language-plaintext highlighter-rouge">spherical harmonics</code>(SH) coeff. for <code class="language-plaintext highlighter-rouge">color</code></li> <li><code class="language-plaintext highlighter-rouge">opacity</code> \(\alpha\)</li> <li><code class="language-plaintext highlighter-rouge">3D position</code> for <code class="language-plaintext highlighter-rouge">mean</code></li> </ol> <h3 id="parameter-1-covariance-matrix">Parameter 1. Covariance matrix</h3> <blockquote> <p>scale vector(scale) and quaternion(rotation) for covariance matrix</p> </blockquote> <ul> <li>covariance matrix는 positive semi-definite \(x^T M x \geq 0\) for all \(x \in R^n\)이어야만 physical meaning을 가지는데,<br/> \(\Sigma\) 를 직접 바로 optimize하면 invalid covariance matrix가 될 수 있음<br/> 그렇다면!!</li> </ul> <p>\(\Sigma\) 가 <code class="language-plaintext highlighter-rouge">symmetric</code> and <code class="language-plaintext highlighter-rouge">positive semi-definite</code>이도록 \(\Sigma = R S S^T R^T\) 로 정의해서<br/> \(\Sigma\) 대신 <code class="language-plaintext highlighter-rouge">x,y,z-axis scale</code>을 나타내는 <code class="language-plaintext highlighter-rouge">3D vector</code> \(s\) 와 <code class="language-plaintext highlighter-rouge">rotation</code>을 나타내는 <code class="language-plaintext highlighter-rouge">4D quaternion</code> \(q\) 를 optimize 하자!!<br/> quaternion에 대한 설명은 <a href="https://semyeong-yu.github.io/blog/2024/Quaternion">Quaternion</a> 블로그 참고!!</p> <ul> <li><code class="language-plaintext highlighter-rouge">scale</code> 3D vector \(s\) <code class="language-plaintext highlighter-rouge">초기값</code> :<br/> <a href="https://github.com/graphdeco-inria/gaussian-splatting/blob/b2ada78a779ba0455dfdc2b718bdf1726b05a1b6/scene/gaussian_model.py#L134C1-L134C1">GaussianModel().create_from_pcd()</a><br/> SfM sparse point cloud의 각 점에 대해 가장 가까운 점 3개까지의 거리의 평균을 각 axis(\(x, y, z\))별로 구한 것을 3 \(\times\) 1 \(s\)라 할 때<br/> normalize 효과를 위해 log, sqrt 씌운 뒤<br/> 3 \(\times\) 1 \(log(\sqrt{s})\) 의 값을 3번 복사하여 3 \(\times\) 3 scale matrix \(S\)를 초기화 <pre><code class="language-Python">dist2 = torch.clamp_min(distCUDA2(torch.from_numpy(np.asarray(pcd.points)).float().cuda()), 0.0000001)
scales = torch.log(torch.sqrt(dist2))[...,None].repeat(1, 3)
</code></pre> </li> <li> <p><code class="language-plaintext highlighter-rouge">scale</code> 3D vector \(s\) <code class="language-plaintext highlighter-rouge">activation function</code> :<br/> smooth gradient 얻기 위해 exponential activation function을 씌움</p> </li> <li><code class="language-plaintext highlighter-rouge">quaternion</code> \(q\) <code class="language-plaintext highlighter-rouge">초기값</code> :<br/> 각 점에 대해 \(\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}\) 으로 quaternion을 초기화하고<br/> 이를 이용하여 rotation matrix \(R\) 초기화 <pre><code class="language-Python">rots = torch.zeros((fused_point_cloud.shape[0], 4), device="cuda")
rots[:, 0] = 1
</code></pre> </li> <li><code class="language-plaintext highlighter-rouge">anisotropic covariance</code>는 다양한 모양의 geometry를 나타내기 위해 optimize하기에 적합!</li> </ul> <blockquote> <p>EWA volume splatting (2001) :<br/> world-to-camera 는 linear transformation 이지만,<br/> <code class="language-plaintext highlighter-rouge">camera-to-image (projection)</code> 는 <code class="language-plaintext highlighter-rouge">non-linear transformation</code> 이다!!</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/4-480.webp 480w,/assets/img/2024-07-11-GS/4-800.webp 800w,/assets/img/2024-07-11-GS/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 위 그림 : camera coordinate / 아래 그림 : image coordinate (ray space) </div> <ul> <li><code class="language-plaintext highlighter-rouge">world</code> coordinate (3D) : <ul> <li> \[\boldsymbol u = \begin{bmatrix} u_0 \\ u_1 \\ u_2 \end{bmatrix}\] </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">camera</code> coordinate (3D) : <ul> <li>\(\boldsymbol t = \begin{bmatrix} t_0 \\ t_1 \\ t_2 \end{bmatrix}\)<br/> \(= W \boldsymbol u + d\)<br/> where \(W\) : <code class="language-plaintext highlighter-rouge">viewing transformation</code> affine matrix from world coordinate to camera coordinate</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">image</code> coordinate (2D) : <ul> <li>\(\boldsymbol x = \begin{bmatrix} x_0 \\ x_1 \\ x_2 \end{bmatrix}\)<br/> \(= \phi(\boldsymbol t) = \begin{bmatrix} \frac{t_0}{t_2} \\ \frac{t_1}{t_2} \\ \| (t_0, t_1, t_2)^T \| \end{bmatrix}\)</li> <li>function \(\phi\) 는 non-linear하므로 Affine transformation이 불가능하다.</li> <li><code class="language-plaintext highlighter-rouge">Local Affine (Linear) transform으로 Approx.</code>하기 위해 \(\boldsymbol t = \boldsymbol t_{k}\) 에서의 <code class="language-plaintext highlighter-rouge">Taylor Approx.</code>를 이용하면,<br/> \(\phi_{k}(\boldsymbol t) = \phi(\boldsymbol t_{k}) + \boldsymbol J_{k} \cdot (\boldsymbol t - \boldsymbol t_{k})\)<br/> where<br/> \(\boldsymbol J_{k} = \frac{d\phi}{d \boldsymbol t}(\boldsymbol t_{k}) = \begin{bmatrix} \frac{d\phi}{d \boldsymbol t_{0}}(\boldsymbol t_{k}) &amp; \frac{d\phi}{d \boldsymbol t_{1}}(\boldsymbol t_{k}) &amp; \frac{d\phi}{d \boldsymbol t_{2}}(\boldsymbol t_{k}) \end{bmatrix} = \begin{bmatrix} \frac{1}{t_{k, 2}} &amp; 0 &amp; -\frac{t_{k, 0}}{t_{k, 2}^2} \\ 0 &amp; \frac{1}{t_{k, 2}} &amp; -\frac{t_{k, 1}}{t_{k, 2}^2} \\ \frac{t_{k, 0}}{l} &amp; \frac{t_{k, 1}}{l} &amp; \frac{t_{k, 2}}{l} \end{bmatrix}\)<br/> and ray distance \(l = \| (t_{k, 0}, t_{k, 1}, t_{k, 2})^T \|\)<br/> Here, \(J\) : <code class="language-plaintext highlighter-rouge">Jacobian</code>(각 axis로 편미분한 matrix) of the <code class="language-plaintext highlighter-rouge">affine approx.</code> of the <code class="language-plaintext highlighter-rouge">projective transformation</code> from camera coordinate to image coordinate</li> <li>즉, camera coordinate에서 임의의 좌표 \(\boldsymbol t_{k}\) 주변에 존재하는 입력 좌표 \(\boldsymbol t\)에 대해서는 image coordinate으로의 affine(linear) transformation이 충족된다.</li> <li>Gaussian Splatting 논문의 경우 <code class="language-plaintext highlighter-rouge">Gaussian의 중심점</code>을 \(\boldsymbol t_{k}\) 로 두면 그 주변의 \(\boldsymbol t\)에 대해서는 Jacobian을 이용한 affine(linear) transformation 가능!</li> </ul> </li> </ul> <blockquote> <p><code class="language-plaintext highlighter-rouge">Projection</code> of 3D Gaussian <code class="language-plaintext highlighter-rouge">covariance</code> to 2D</p> </blockquote> <ul> <li> <p><code class="language-plaintext highlighter-rouge">world coordinate</code> :<br/> \(\Sigma\) : 3 \(\times\) 3 covariance matrix of 3D Gaussian</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">image coordiante</code> (z=1) :<br/> \(\Sigma^{\ast} = J W \Sigma W^T J^T\) : covariance matrix of 2D splat</p> <ul> <li>Step 1. world-to-camera (<code class="language-plaintext highlighter-rouge">affine</code>) :<br/> \(\boldsymbol u \rightarrow W \boldsymbol u + d\)</li> <li>Step 2. camera-to-image (<code class="language-plaintext highlighter-rouge">local affine approx.</code>) :<br/> Projection<br/> \(W \boldsymbol u + d \rightarrow \phi_{k}(W \boldsymbol u + d) = x_k + \boldsymbol J_{k} W \boldsymbol u + \boldsymbol J_{k} (d - \boldsymbol t_{k})\)<br/> 상수 부분을 제외하면 \(\boldsymbol x = \boldsymbol J_{k} W \boldsymbol u\)</li> <li>Step 3. covariance 특성 :<br/> \(Cov[Ax] = E[(Ax - E[Ax])(Ax - E[Ax])^T]\)<br/> \(= E[A(x - E[x])(x - E[x])^TA^T] = A Cov[x] A^T\)</li> <li>Step 4. <code class="language-plaintext highlighter-rouge">world-to-image covariance</code> :<br/> \(\boldsymbol u \rightarrow \boldsymbol J_{k} W \boldsymbol u\) 이므로<br/> \(\Sigma \rightarrow \boldsymbol J \boldsymbol W \Sigma \boldsymbol W^T \boldsymbol J^T\)</li> <li>Step 5. <code class="language-plaintext highlighter-rouge">covariance dimension reduction</code> :<br/> 추가로, \(\boldsymbol J \boldsymbol W \Sigma \boldsymbol W^T \boldsymbol J^T\) 로 계산한 \(\Sigma^{\ast}\) 는 3-by-3 matrix 인데,<br/> 3D Gaussian을 한쪽 축으로 적분하면 2D Gaussian과 동일한 값을 가지게 되므로<br/> 3-by-3 covariance matrix의 3번째 행과 열의 값을 버린<br/> 2-by-2 matrix를 projected 2D covariance matrix 로 사용!</li> </ul> </li> </ul> <blockquote> <p>param. gradient 직접 유도 (Appendix A.)</p> </blockquote> <p>training할 때 automatic differentiation으로 인한 <code class="language-plaintext highlighter-rouge">overhead를 방지</code>하기 위해 <code class="language-plaintext highlighter-rouge">param. gradient를 직접 유도</code>함!</p> <ol> <li> <p>By chain rule, \(\frac{d\Sigma^{\ast}}{ds} = \frac{d\Sigma^{\ast}}{d\Sigma}\frac{d\Sigma}{ds}\) and \(\frac{d\Sigma^{\ast}}{dq} = \frac{d\Sigma^{\ast}}{d\Sigma}\frac{d\Sigma}{dq}\)</p> </li> <li> <p>By covariance dimension reduction, \(\Sigma^{\ast}\) 는 \(U \Sigma U^T\) 의 좌상단 2-by-2 matrix<br/> where \(U = JW\)<br/> So, 편미분 값은 \(\frac{d\Sigma^{\ast}}{d\Sigma_{ij}} = \begin{bmatrix} U_{1, i} U_{1, j} &amp; U_{1, i} U_{2, j} \\ U_{1, j} U_{2, i} &amp; U_{2, i} U_{2, j} \end{bmatrix}\)</p> </li> <li> <p>For symmetric and positive semi-definite property of covariance matrix, we set \(\Sigma = MM^T\)<br/> where \(M = RS\)<br/> So, \(\frac{d\Sigma}{ds} = \frac{d\Sigma}{dM} \frac{dM}{ds}\) and \(\frac{d\Sigma}{dq} = \frac{d\Sigma}{dM} \frac{dM}{dq}\)<br/> where \(\frac{d\Sigma}{dM} = 2M^T\)</p> </li> <li> <p>\(M = RS\)<br/> where \(S = \begin{bmatrix} s_x &amp; s_x &amp; s_x \\ s_y &amp; s_y &amp; s_y \\ s_z &amp; s_z &amp; s_z \end{bmatrix}\)<br/> So, \(\frac{dM_{i, j}}{ds_k} = \begin{cases} R_{i, k} &amp; \text{if j=k} \\ 0 &amp; O.W. \end{cases}\)</p> </li> <li> <p>\(M = RS\) and \(R(q) = \begin{bmatrix} 1 - 2 \cdot (q_j^2 + q_k^2) &amp; 2 \cdot (q_iq_j - q_rq_k) &amp; 2 \cdot (q_iq_k + q_rq_j) \\ 2 \cdot (q_iq_j + q_rq_k) &amp; 1 - 2 \cdot (q_i^2 + q_k^2) &amp; 2 \cdot (q_jq_k - q_rq_i) \\ 2 \cdot (q_iq_k - q_rq_j) &amp; 2 \cdot (q_jq_k + q_rq_i) &amp; 1 - 2 \cdot (q_i^2 + q_j^2) \end{bmatrix}\)<br/> where \(q = \begin{bmatrix} q_r \\ q_i \\ q_j \\ q_k \end{bmatrix}\)<br/> So, \(\frac{dM}{dq_r} = 2 \begin{bmatrix} 0 &amp; -s_y q_k &amp; s_z q_j \\ s_x q_k &amp; 0 &amp; -s_z q_i \\ -s_x q_j &amp; s_y q_i &amp; 0 \end{bmatrix}\)<br/> and \(\frac{dM}{dq_i} = 2 \begin{bmatrix} 0 &amp; s_y q_j &amp; s_z q_k \\ s_x q_j &amp; -2 s_y q_i &amp; -s_z q_r \\ s_x q_k &amp; s_y q_r &amp; -2 s_z q_i \end{bmatrix}\)<br/> and \(\frac{dM}{dq_j} = 2 \begin{bmatrix} -2 s_x q_j &amp; s_y q_i &amp; s_z q_r \\ s_x q_i &amp; 0 &amp; s_z q_k \\ -s_x q_r &amp; s_y q_k &amp; -2 s_z q_j \end{bmatrix}\)<br/> and \(\frac{dM}{dq_k} = 2 \begin{bmatrix} -2 s_x q_k &amp; -s_y q_r &amp; s_z q_i \\ s_x q_r &amp; -2 s_y q_k &amp; s_z q_j \\ s_x q_i &amp; s_y q_j &amp; 0 \end{bmatrix}\)</p> </li> <li> <p>gradient for quaternion normalization is straightforward</p> </li> </ol> <h3 id="parameter-2-spherical-harmonicssh-coeff">Parameter 2. Spherical Harmonics(SH) coeff.</h3> <ul> <li><code class="language-plaintext highlighter-rouge">Spherical Harmonics</code> (SH) :<br/> spherical coordinate 에서 <code class="language-plaintext highlighter-rouge">각도</code> (\(\theta, \phi\))를 입력받아 <code class="language-plaintext highlighter-rouge">구의 표면 위치에서의 값</code>을 출력하는 함수<br/> spherical coordinate 에서 라플라스 방정식을 풀면 아래 수식과 같음</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/5-480.webp 480w,/assets/img/2024-07-11-GS/5-800.webp 800w,/assets/img/2024-07-11-GS/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/6-480.webp 480w,/assets/img/2024-07-11-GS/6-800.webp 800w,/assets/img/2024-07-11-GS/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/7-480.webp 480w,/assets/img/2024-07-11-GS/7-800.webp 800w,/assets/img/2024-07-11-GS/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> l이 같은 함수들은 same band l에 있다고 말함 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/8-480.webp 480w,/assets/img/2024-07-11-GS/8-800.webp 800w,/assets/img/2024-07-11-GS/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 가로축 : theta, 세로축 : phi, 채도 : SH magnitude, 색상 : SH phase </div> <ul> <li> <p>SH coeff. <code class="language-plaintext highlighter-rouge">초기값</code> :<br/> 0-band SH (\(\theta, \phi\) 와 관계없는 view-independent color) 의 경우 SfM으로 얻은 point cloud의 RGB color값과 RGB2SH 이용하여 초기화<br/> 다른 band의 경우 0으로 초기화</p> </li> <li>SH 의 역할 : <ul> <li>SH에서 band 수를 제한해서 쓴다는 것은 높은 band (high freq. 또는 detail info.)는 자른다는 의미이므로 <code class="language-plaintext highlighter-rouge">smoothing</code> 역할</li> <li>적은 비용(coeff. 몇 개만 사용)으로 SH function을 <code class="language-plaintext highlighter-rouge">approx.</code></li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">SH coeff.</code>로 <code class="language-plaintext highlighter-rouge">color</code> 나타내는 법 :<br/> Fourier Series 에서처럼,<br/> SH coeff. \(k_{l}^{m}\) 의 optimal 값을 구해서<br/> \(k_{l}^{m}\) 와 \(Y_l^m(\theta, \phi)\) 의 weighted sum!<br/> \(C = \Sigma_{l=0}^{l_{max}} \Sigma_{m=-l}^{l} k_l^m Y_l^m(\theta, \phi)\)<br/> 즉, <code class="language-plaintext highlighter-rouge">trainable parameter</code> : SH coeff.인 \(k_{l}^{m}\)<br/> (<code class="language-plaintext highlighter-rouge">light source</code>마다 SH coeff. \(k_{l}^{m}\) 다르므로 find optimal value)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/9-480.webp 480w,/assets/img/2024-07-11-GS/9-800.webp 800w,/assets/img/2024-07-11-GS/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="parameter-3-opacity">Parameter 3. opacity</h3> <ul> <li> <p>opacity \(\alpha\) <code class="language-plaintext highlighter-rouge">초기값</code> :<br/> 임의의 실수값으로 초기화<br/> inverse_sigmoid(0.1 * torch.ones(…))</p> </li> <li> <p>opacity \(\alpha\) <code class="language-plaintext highlighter-rouge">range</code> :<br/> \(\alpha \in [0, 1)\) 이므로<br/> 마지막에 sigmoid activation function을 씌워서 smooth gradient를 얻음</p> </li> </ul> <h3 id="parameter-4-3d-positionmean">Parameter 4. 3D position(mean)</h3> <h2 id="fast-differentiable-rasterizer-for-gaussians">Fast Differentiable Rasterizer for Gaussians</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/10-480.webp 480w,/assets/img/2024-07-11-GS/10-800.webp 800w,/assets/img/2024-07-11-GS/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Tile Rasterizer</p> </blockquote> <ul> <li> <p>기능 : 3D Gaussians로 구성된 3D model을 특정 camera pose에 대해 2D rendering</p> </li> <li><code class="language-plaintext highlighter-rouge">input</code> : <ul> <li>image의 rendering할 width, height</li> <li>3D Gaussian의 xyz-mean, covariance in world-coordinate</li> <li>3D Gaussian의 color, opacity</li> <li>current camera pose</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Frustum Culling</code> :<br/> 주어진 camera pose에서 view frustum을 그려서<br/> view frustum과 교차하는 확률이 99% confidence interval 범위 밖에 있는 3D Gaussians는 제거(culling)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/16-480.webp 480w,/assets/img/2024-07-11-GS/16-800.webp 800w,/assets/img/2024-07-11-GS/16-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Guard Band</code> :<br/> 아래의 경우 projected 2D covariance 계산이 불안정하기 때문에 개별적으로 제거 <ul> <li><code class="language-plaintext highlighter-rouge">view frustum의 near plane에 가까이 있는</code> Gaussian의 경우,<br/> EWA Volume Splatting에서 언급된 cam-to-img projection <code class="language-plaintext highlighter-rouge">nonlinearity</code>가 심하기 때문에<br/> projection matrix를 Jacobian으로 approx.한 값에 더 큰 artifact가 생김</li> <li>view frustum 밖에 멀리 떨어진 경우 <code class="language-plaintext highlighter-rouge">?????</code><br/> 코드에서는 이 경우는 빼버렸음 (주석 처리)<br/> (diff-gaussian-rasterization/cuda_rasterizer/auxiliary.h)</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Create Tiles</code> :<br/> <code class="language-plaintext highlighter-rouge">CUDA 병렬 처리</code>를 위해<br/> \(w \times h\)의 image를 \(16 \times 16\) pixel의 tiles로 쪼갬</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/11-480.webp 480w,/assets/img/2024-07-11-GS/11-800.webp 800w,/assets/img/2024-07-11-GS/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p><code class="language-plaintext highlighter-rouge">Parallelism</code> :<br/> <code class="language-plaintext highlighter-rouge">tile마다</code> 개별 <code class="language-plaintext highlighter-rouge">CUDA thread</code> block으로 실행하여<br/> forward/backward processing, data loading/sharing을 병렬처리<br/> (여러 threads가 Gaussian points를 shared memory에 collaboratively load)<br/> (VRAM과 DRAM 사이의 이동은 overhead 발생하기 때문에 <code class="language-plaintext highlighter-rouge">VRAM</code>에서 모두 처리해버릴 수 있도록 <code class="language-plaintext highlighter-rouge">CUDA Functions</code>(.cu)를 직접 짬!)</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Duplicate with Keys</code> :</p> <ul> <li><code class="language-plaintext highlighter-rouge">view-space-depth</code>와 <code class="language-plaintext highlighter-rouge">tile-ID</code>를 이용하여 tile마다 각 Gaussian의 key를 생성<br/> tile-ID 쪽이 MSB<br/> view-space-depth 쪽이 LSB<br/> 각 Gaussian의 value는 Gaussian’s index</li> <li><code class="language-plaintext highlighter-rouge">CUDA 병렬처리</code> 덕분에 2D Gaussian 하나가 3개의 tiles에 걸쳐 있다면, 3개의 2D Gaussians로 복제(<code class="language-plaintext highlighter-rouge">instance화</code>)되는 것처럼 작동</li> <li>tile1-depth1, tile1-depth2, tile1-depth3, tile2-depth1, tile2-depth2, … 순으로 정렬됨</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/12-480.webp 480w,/assets/img/2024-07-11-GS/12-800.webp 800w,/assets/img/2024-07-11-GS/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Sort by Keys</code> : <ul> <li>tile마다 Depth 기준으로 <code class="language-plaintext highlighter-rouge">Radix Sort</code></li> <li><code class="language-plaintext highlighter-rouge">처음에 한 번</code> sort 하고 나면 끝!! 추가로 per-pixel sorting 할 필요 없음</li> <li>tile마다 개별 thread로 parallel하게 실행하므로 single radix sort 만으로 all splats are ordered</li> <li>pixel-wise sorting이 아니라 Gaussians sort라서 \(\alpha\)-blending approx.이긴 한데, <code class="language-plaintext highlighter-rouge">splats가 각 pixel size 정도로 작기 때문에</code> 해당 approx. 오차는 무시 가능!</li> <li>쨌든 이 덕분에 visible artifacts 없이 training, rendering performance 베리베리 굳</li> </ul> </li> </ul> <pre><code class="language-Python">from collections import deque
# 양방향에서 삽입/삭제 가능한 queue형 자료구조

# 1의 자릿수 기준으로 정렬한 뒤
# 10의 자릿수 기준으로 정렬한 뒤
# ...
def radixSort():
    nums = list(map(int, input().split(' ')))
    buckets = [deque() for _ in range(10)] # 각 자릿수(0~9)에 대응되는 10개의 empty deque()
    
    max_val = max(nums)
    queue = deque(nums) # 정렬할 숫자들
    digit = 1 # 정렬 기준이 되는 자릿수
    
    while (max_val &gt;= digit): # 가장 큰 수의 자릿수일 때까지만 실행
        while queue:
            num = queue.popleft() # 정렬할 숫자
            buckets[(num // digit) % 10].append(num) # 각 자릿수(0~9)에 따라 buckets에 num을 넣는다.
        
        # 해당 정렬 기준 자릿수에서 buckets에 다 넣었으면, buckets에 담겨있는 순서대로 꺼내와서 정렬한다.
        for bucket in buckets:
            while bucket:
                queue.append(bucket.popleft())

        digit *= 10 # 정렬 기준이 되는 자릿수 증가시키기
    
    print(list(queue))
</code></pre> <ul> <li><code class="language-plaintext highlighter-rouge">Identify Tile Ranges</code> : <ul> <li>tile별 Gaussian list를 효율적으로 관리하기 위해<br/> tile마다 Gaussian list 범위 식별</li> <li>이 또한 <code class="language-plaintext highlighter-rouge">parallel</code>하게 이루어짐<br/> 64-bit key(Gaussian)마다 개별 thread를 launch하여 상위 32-bit(tile-ID)를 two neighbors와 비교</li> </ul> </li> <li> <p><code class="language-plaintext highlighter-rouge">Get Tile Ranges</code> :<br/> i-th tile에 대한 Gaussian list 범위 읽어옴</p> </li> <li>\(\alpha\)-Blending in Order (<code class="language-plaintext highlighter-rouge">forward process</code>) : <ul> <li>tile별 CUDA 병렬처리에 의해 <code class="language-plaintext highlighter-rouge">각 pixel에 대해</code><br/> <code class="language-plaintext highlighter-rouge">color</code> 및 <code class="language-plaintext highlighter-rouge">opacity</code> \(\alpha\) 값을 Gaussian list의 <code class="language-plaintext highlighter-rouge">앞에서 뒤로</code> accumulate<br/> \(c = \alpha_{1}c_{1} + (1 - \alpha_{1})(\alpha_{2}c_{2} + (1 - \alpha_{2})(\cdots)) = \alpha_{1}c_{1} + (1 - \alpha_{1})\alpha_{2}c_{2} + (1 - \alpha_{1})(1 - \alpha_{2})(\cdots) = \cdots = \sum_{i=1}^{N}(\alpha_{i}c_{i}\prod_{j=1}^{i-1}(1-\alpha_{j}))\) where \(\alpha_{0} = 0\)</li> <li>i-th tile에 있는 pixels 중 a pixel’s accumulated opacity 값이 target saturation threshold를 넘어서면, 해당 i-th thread STOP (유일한 STOP 조건)</li> <li><code class="language-plaintext highlighter-rouge">Gaussian의 개수를 제한하지 않음</code>으로써 scene-specific hyper-param. tuning 없이 arbitrary depth complexity를 가지는 scene을 커버 가능<br/> (GPU Radix Sort 덕분에 parallelism(병렬) 및 amortized(분할상환) 가능하여 Gaussian 개수 늘릴 수 있었음)</li> <li><code class="language-plaintext highlighter-rouge">기존 기법들은 pixel마다 정렬이 필요</code>해서 inefficient했지만<br/> 본 논문은 tile별 CUDA 병렬처리 덕분에 efficient<br/> (e.g. NeRF : ray per pixel 쏴서 t-distance를 pixel별로 정렬해야 함)</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Backward process</code> : <ul> <li>Gaussian의 <code class="language-plaintext highlighter-rouge">opacity 비율에 따라</code> <code class="language-plaintext highlighter-rouge">뒤에서 앞으로</code> gradient update</li> <li>\(t\) -th tile 내 <code class="language-plaintext highlighter-rouge">각 pixel에 대해</code><br/> \(t\) -th tile의 Gaussian list 내 Gaussians와 (expensive) overlap testing하여 해당 pixel과 겹치는 Gaussians를 업데이트</li> <li>overhead 방지 위해 직접 backward gradient update 식을 구해서 이용</li> <li>backward process를 위해 <d-cite key="Point1">[3]</d-cite>처럼 <code class="language-plaintext highlighter-rouge">pixel마다</code> global memory에 blended points list를 저장할 수도 있지만<br/> dynamic memory management overhead가 생기기 때문에<br/> forward process에서 <code class="language-plaintext highlighter-rouge">tile마다</code> 구했던 range 및 sorted Gaussian list를 <code class="language-plaintext highlighter-rouge">재사용</code></li> <li>\(\alpha\)-blending으로 합쳤던 각 Gaussian으로 gradient back-propagation을 해주려면<br/> blending 각 step에서의 accumulated opacity 값이 필요한데,<br/> 이를 별도의 list에 저장해두고 훑는 게 아니라,<br/> each point stores the final accumulated opacity in the forward process<br/> and<br/> divide this by each point’s opacity in our back-to-front traversal<br/> to obtain the required coefficients for gradient computation<br/> <code class="language-plaintext highlighter-rouge">?????</code></li> <li>numerical stability 위해 <ul> <li>0으로 나눠지는 경우를 방지하기 위해 \(\alpha\) 값이 \(\epsilon = \frac{1}{255}\)보다 작다면 blending update 안 함</li> <li>opacity \(\alpha\) 를 upper bound 0.99로 clamp</li> <li>rasterization할 때 front-to-back blending 값 \(c\) 가 0.9999를 초과하기 전에 STOP</li> </ul> </li> </ul> </li> <li> <p>Primitives :<br/> 본 논문의 Gaussians는 <code class="language-plaintext highlighter-rouge">Euclidean space</code>에 <code class="language-plaintext highlighter-rouge">primitives</code>를 남김 <code class="language-plaintext highlighter-rouge">?????</code><br/> \(\rightarrow\) <d-cite key="Plenoxels">[1]</d-cite>, <d-cite key="MipNeRF360">[10]</d-cite>과 달리 distant or large Gaussians 처리를 위해 space compaction, warping, or projection 할 필요가 없음</p> </li> <li>Efficient Rasterization : <ul> <li>Pulsar 논문<d-cite key="Pulsar">[5]</d-cite> 에서처럼<br/> an entire image에 대해 가장 작은 원소(<code class="language-plaintext highlighter-rouge">primitives</code>)를 미리 정렬(<code class="language-plaintext highlighter-rouge">pre-sort</code>)하여 <code class="language-plaintext highlighter-rouge">primitives = Gaussians ?????</code><br/> pixel-wise sorting 비용을 절감</li> <li>differentiable</li> <li>arbitrary number of Gaussians에 대해 backpropagation 가능<br/> with low additional memory : O(1) per pixel</li> <li>2D projection 가능</li> </ul> </li> </ul> <h2 id="optimization-with-adaptive-density-control-of-3d-gaussians">Optimization with Adaptive Density Control of 3D Gaussians</h2> <h3 id="optimization">Optimization</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/13-480.webp 480w,/assets/img/2024-07-11-GS/13-800.webp 800w,/assets/img/2024-07-11-GS/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Loss :<br/> predicted image와 GT image를 비교하는<br/> <code class="language-plaintext highlighter-rouge">L1 loss</code> 및 <code class="language-plaintext highlighter-rouge">D-SSIM loss</code><br/> D-SSIM : Directional Structural Similarity Index Measure</p> </li> <li> <p>3D Gaussian의 xyz-mean에 대해서만 <d-cite key="Plenoxels">[1]</d-cite>에서처럼 <code class="language-plaintext highlighter-rouge">standard exponential decay scheduling</code> 사용</p> </li> <li>Adam optimizer로 네 가지 param. 업데이트 <ul> <li>3D xyz-mean</li> <li>3D covariance</li> <li>color</li> <li>opacity</li> </ul> </li> <li>optimization 세부 사항 : <ul> <li>연산을 <code class="language-plaintext highlighter-rouge">low resol.부터 warm-up</code> :<br/> 목적 : model이 효율적으로 coarse info.부터 학습하도록 하여 <code class="language-plaintext highlighter-rouge">stability</code> 향상<br/> 초기에 4배 작은 image로 optimization 진행하고 250, 500 iter.에서 2배씩 upsampling</li> <li>Spherical Harmonics <code class="language-plaintext highlighter-rouge">low band부터 warm-up</code> :<br/> 목적 : 처음부터 high band로 detail까지 학습하려고 하면<br/> scene의 corner를 촬영하거나 inside-out 방식(카메라가 촬영 대상의 내부에 위치하여 바깥쪽을 촬영) 때문에<br/> <code class="language-plaintext highlighter-rouge">놓친 angular 영역이 있을 경우 SH의 0-band coeff. (base or diffuse color)가 부적절</code>하게 만들어질 수 있어서<br/> 처음에는 0-band coeff.를 optimize하고 매 1000 iter.마다 band 수 늘려서 4-band coeff.까지 optimization</li> </ul> </li> </ul> <h3 id="adaptive-density-control-of-gaussians">Adaptive Density Control of Gaussians</h3> <p>optimization of 4 param.의 경우 매 iter.마다 update하지만,<br/> Adaptive Density Control of Gaussians의 경우 <code class="language-plaintext highlighter-rouge">100 iter.마다</code> update</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/14-480.webp 480w,/assets/img/2024-07-11-GS/14-800.webp 800w,/assets/img/2024-07-11-GS/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Remove</code> :<br/> \(\alpha\) 값이 threshold보다 작거나<br/> world-space에서 크기가 매우 크거나<br/> view-space에서 footprint가 매우 큰 경우<br/> 3D Gaussians 제거</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/15-480.webp 480w,/assets/img/2024-07-11-GS/15-800.webp 800w,/assets/img/2024-07-11-GS/15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Gaussians가 scene을 제대로 표현 못 하는 중<br/> \(\rightarrow\) scene을 제대로 표현하기 위해선 Gaussian position을 크게 옮겨야 함<br/> \(\rightarrow\) view-space positional gradient \(\Delta_{p} L\)가 큼<br/> \(\rightarrow\) under/over-reconstruction 상황이므로 clone/split을 통해 정확한 위치에 Gaussian이 분포하도록 하자</p> </li> <li><code class="language-plaintext highlighter-rouge">Split</code> :<br/> <code class="language-plaintext highlighter-rouge">over-reconstruction</code>의 경우 3D Gaussians split <ul> <li>split : 1개의 Gaussian을 <code class="language-plaintext highlighter-rouge">2개로 분리</code>하고 각 scale을 줄인 후 <code class="language-plaintext highlighter-rouge">기존 3D Gaussian의 PDF</code>에 따라 sampling하여 배치<br/> Gaussians의 수는 증가하지만, total volume은 유지</li> <li>조건 1. <code class="language-plaintext highlighter-rouge">view-space positional gradient</code> \(\Delta_{p} L\)의 avg. magnitude \(\geq\) threshold \(\tau_{pos}\)</li> <li>조건 2. <code class="language-plaintext highlighter-rouge">covariance</code>가 큼</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Clone</code> :<br/> <code class="language-plaintext highlighter-rouge">under-reconstruction</code>의 경우 3D Gaussians clone <ul> <li>clone : <code class="language-plaintext highlighter-rouge">같은 크기로 copy</code> 후 <code class="language-plaintext highlighter-rouge">positional gradient 방향</code>에 배치<br/> total volume 및 Gaussians의 수 모두 증가</li> <li>조건 1. <code class="language-plaintext highlighter-rouge">view-space positional gradient</code> \(\Delta_{p} L\)의 avg. magnitude \(\geq\) threshold \(\tau_{pos}\)</li> <li>조건 2. <code class="language-plaintext highlighter-rouge">covariance</code>가 작음</li> </ul> </li> <li>3000 iter.마다 \(\alpha\) <code class="language-plaintext highlighter-rouge">알파 값을 주기적으로 0으로 초기화</code> 하면 전체 Gaussian 조절에 큰 도움이 됨! <ul> <li>효과 1. volumetric 기법의 특성상 <code class="language-plaintext highlighter-rouge">camera와 가까운 영역</code>에서 많은 <code class="language-plaintext highlighter-rouge">floater</code>들이 생겨서 Gaussian density가 증가하는데, 이를 제거해주는 역할<br/> floater 해결 관련 논문 : <d-cite key="floater1">[6]</d-cite> <d-cite key="floater2">[7]</d-cite> <d-cite key="floater3">[8]</d-cite></li> <li>효과 2. <code class="language-plaintext highlighter-rouge">큰 Gaussian들이 중첩</code>되어 있는 case를 제거해주는 역할</li> </ul> </li> </ul> <h2 id="results">Results</h2> <h3 id="implementation">Implementation</h3> <ul> <li> <p>custom CUDA kernel :<br/> tile-based rasterization을 위해<br/> custom CUDA kernel를 추가하여 사용 like <d-cite key="Point1">[3]</d-cite>, <d-cite key="Plenoxels">[1]</d-cite>, <d-cite key="superfast">[9]</d-cite></p> </li> <li> <p>Radix Sort :<br/> fast Radix Sort를 위해 NVIDIA CUB sorting routines <d-cite key="radixsort">[11]</d-cite> 사용</p> </li> <li> <p>interactive image viewer :<br/> open-source SIBR <a href="https://gitlab.inria.fr/sibr/sibr_core">SIBR</a> 이용해서<br/> interactive image-rendering viewer 만듬 (frame rate 측정에 사용)</p> </li> </ul> <h3 id="evaluation">Evaluation</h3> <ul> <li>Dataset :<br/> bounded indoor scenes와 unbounded outdoor scenes 전부 커버 <ul> <li>synthetic Blender dataset (Nerf) :<br/> have exhaustive set of bounded views with exact camera param.<br/> \(\rightarrow\) SOTA result even with 100K uniformly random initialization</li> <li>Mip-Nerf360 dataset</li> <li>Tanks&amp;Temples dataset</li> <li>Hedman et al. dataset</li> </ul> </li> <li>Metrics : <ul> <li>PSNR</li> <li>L-PIPS</li> <li>SSIM (D-SSIM)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/28-480.webp 480w,/assets/img/2024-07-11-GS/28-800.webp 800w,/assets/img/2024-07-11-GS/28-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/28.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Comparison : <ul> <li><code class="language-plaintext highlighter-rouge">Quality</code> : NeRF 계열 중 SOTA인 <code class="language-plaintext highlighter-rouge">Mip-Nerf360</code> <d-cite key="MipNeRF360">[10]</d-cite>과 비교 <ul> <li>끝까지 훈련시켰을 때 비슷한 quality 보이고,</li> <li>training speed는 35-45 min. versus 48 hours</li> </ul> </li> <li>Traning/Rendering <code class="language-plaintext highlighter-rouge">Speed</code> : NeRF 계열 중 SOTA인 <code class="language-plaintext highlighter-rouge">InstantNGP</code> <d-cite key="InstantNGP">[2]</d-cite>, <code class="language-plaintext highlighter-rouge">Plenoxels</code> <d-cite key="Plenoxels">[1]</d-cite> 과 비교 <ul> <li>speed SOTA인 <d-cite key="InstantNGP">[2]</d-cite> , <d-cite key="Plenoxels">[1]</d-cite> 과 비슷한 quality 가질 때까지 training 5-10 min.밖에 안 걸리고,</li> <li>훈련 더 하면 <d-cite key="InstantNGP">[2]</d-cite>, <d-cite key="Plenoxels">[1]</d-cite>보다 더 좋은 quality 가짐</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/19-480.webp 480w,/assets/img/2024-07-11-GS/19-800.webp 800w,/assets/img/2024-07-11-GS/19-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/19.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/17-480.webp 480w,/assets/img/2024-07-11-GS/17-800.webp 800w,/assets/img/2024-07-11-GS/17-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/17.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 7K iter.으로도 꽤 좋은 결과 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/18-480.webp 480w,/assets/img/2024-07-11-GS/18-800.webp 800w,/assets/img/2024-07-11-GS/18-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/18.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Comparison : <ul> <li>Compactness :<br/> anisotropic 3D Gaussians는<br/> scene representation 뿐만 아니라<br/> complex shape with a lower number of param.을 모델링하는 데도 쓰일 수 있음 <ul> <li>space carving으로 얻은 <d-cite key="Point3">[12]</d-cite> 의 initial point cloud에서 시작했을 때 <d-cite key="Point3">[12]</d-cite> 의 PSNR 값은 2-4 min.만에 넘겨버림</li> <li>또한, <d-cite key="Point3">[12]</d-cite> 의 point cloud의 4분의 1만큼만 써도 작은 model size로도 <d-cite key="Point3">[12]</d-cite> 의 PSNR 넘겨버림</li> </ul> </li> </ul> </li> </ul> <blockquote> <p>Space Carving :</p> <ul> <li>설명 : 여러 camera에 대해 voxel-space에서 object 있는 부분만 남기고 깎아내는 기법</li> <li>이유 : 3D reconstruction을 할 때 color 정보만으로 segmentation 가능할 정도로 background는 simple할수록 좋기 때문</li> <li>한계 : 빛, 그림자 같은 정보는 사용하지 않기 때문에 fg/bg 판단만 가능하다. 따라서 lidar처럼 camera에 depth-detection 메커니즘이 없을 경우 물체 내부의 구멍 같은 건 reconstruct 불가능</li> </ul> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/20-480.webp 480w,/assets/img/2024-07-11-GS/20-800.webp 800w,/assets/img/2024-07-11-GS/20-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/20.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="ablation-study">Ablation Study</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/21-480.webp 480w,/assets/img/2024-07-11-GS/21-800.webp 800w,/assets/img/2024-07-11-GS/21-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/21.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> PSNR score for Ablation Study </div> <ul> <li><code class="language-plaintext highlighter-rouge">Intialization (SfM)</code> : <ul> <li>uniformly sample a cube (random initialization w/o SfM points) :<br/> 주로 <code class="language-plaintext highlighter-rouge">background</code> 퀄리티 저하<br/> training view가 충분하지 않은 영역에서는 optimization으로 제거할 수 없는 <code class="language-plaintext highlighter-rouge">floater</code> 많이 발생<br/> \(\rightarrow\) synthetic NeRF dataset의 경우 bg가 없고 have exhaustive set of bounded views with exact input camera param. 이므로 random initialization으로도 성능 굳</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/22-480.webp 480w,/assets/img/2024-07-11-GS/22-800.webp 800w,/assets/img/2024-07-11-GS/22-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/22.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Densification (clone, split)</code> : <ul> <li>Split : <code class="language-plaintext highlighter-rouge">background</code> reconstruction에 중요한 역할</li> <li>Clone : <code class="language-plaintext highlighter-rouge">thin</code> structure reconstruction에 중요한 역할</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/23-480.webp 480w,/assets/img/2024-07-11-GS/23-800.webp 800w,/assets/img/2024-07-11-GS/23-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/23.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Unlimited depth complexity of splats with gradients</code> : <ul> <li>Limited-BW :<br/> 각 tile의 Gaussian list에서 앞에서부터 N개까지만 gradient 전파할 경우<br/> Pulsar <d-cite key="Pulsar">[5]</d-cite>에서의 값의 2배인 N=10으로 했는데도 unstable optimization 초래</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/24-480.webp 480w,/assets/img/2024-07-11-GS/24-800.webp 800w,/assets/img/2024-07-11-GS/24-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/24.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> left: N=10 / right: N=inf </div> <ul> <li><code class="language-plaintext highlighter-rouge">Anisotropic Covariance</code> : <ul> <li>isotropic convariance :<br/> single scala value (radius of 3D Gaussian)를 optimize할 경우<br/> 같은 Gaussian 개수를 쓰더라도 <code class="language-plaintext highlighter-rouge">align with surfaces</code> 잘 하지 못해서 <code class="language-plaintext highlighter-rouge">fine</code> structure 잘 나타내지 못함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/25-480.webp 480w,/assets/img/2024-07-11-GS/25-800.webp 800w,/assets/img/2024-07-11-GS/25-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/25.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Spherical Harmonics</code> : <ul> <li>color 나타낼 때 <code class="language-plaintext highlighter-rouge">view-dependent</code> effect 담당</li> </ul> </li> </ul> <h2 id="discussion">Discussion</h2> <h3 id="limitations--future-work">Limitations &amp; Future Work</h3> <ul> <li><code class="language-plaintext highlighter-rouge">training view가 부족한 영역</code>에서는 여전히 <code class="language-plaintext highlighter-rouge">floater</code>, <code class="language-plaintext highlighter-rouge">elongated(길쭉한) artifacts</code>, <code class="language-plaintext highlighter-rouge">splotchy(얼룩진) Gaussians</code> 등 artifacts 발생 (Mip-NeRF360 등 prev. methods도 마찬가지)<br/> \(\rightarrow\) regularization으로 alleviate 가능</li> <li><code class="language-plaintext highlighter-rouge">view-dependent appearance</code>가 나타나는 영역에서는 large Gaussian 만들 때 <code class="language-plaintext highlighter-rouge">guard band</code> 등의 이유로 <code class="language-plaintext highlighter-rouge">popping</code> artifacts 발생<br/> \(\rightarrow\) better culling과 regularization으로 alleviate 가능</li> <li>Gaussians <code class="language-plaintext highlighter-rouge">depth-order</code> 갑자기 바뀔 수 있음<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">anti-aliasing</code>으로 해결 가능</li> <li>urban dataset처럼 very <code class="language-plaintext highlighter-rouge">large scene</code>에 대해서는 <code class="language-plaintext highlighter-rouge">position learning-rate</code>를 줄이는 게 도움됨</li> <li>prev. point-based methods에 비해서는 compact하긴 하지만, NeRF-based methods에 비해서는 memory consumption이 훨씬 큼<br/> e.g. large scene을 학습할 때 최대 GPU memory consumption은 20GB를 넘김<br/> \(\rightarrow\) InstantNGP에서처럼 optimization 과정을 low-level implementation 하면 괜찮<br/> e.g. scene을 rendering할 때도 model 저장하는 데 몇백MB, rasterizer 저장하는 데 30-500MB 필요<br/> \(\rightarrow\) memory consumption을 줄이기 위한 추후 개선 필요 (point-clouds compression technique <d-cite key="pointcompress">[13]</d-cite>을 적용해볼 수 있을 듯)</li> <li>3D Gaussians를 mesh reconstruction에 사용할 수 있는지 연구가 진행된다면 본 논문이 정확히 volumetric 과 surface representation 사이 어디에 위치해있는지를 이해할 수 있음</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/26-480.webp 480w,/assets/img/2024-07-11-GS/26-800.webp 800w,/assets/img/2024-07-11-GS/26-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/26.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> left(Mip-NeRF360): floaters and grainy(오돌토돌한, 거친) appearance / right(3DGS): low-detail bg from coarse Gaussians </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/27-480.webp 480w,/assets/img/2024-07-11-GS/27-800.webp 800w,/assets/img/2024-07-11-GS/27-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/27.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> training에서 많이 보지 못한 view의 경우 left(Mip-NeRF360), right(3DGS) 모두 artifacts 발생 </div> <h3 id="conclusion">Conclusion</h3> <ul> <li><code class="language-plaintext highlighter-rouge">3D Gaussian</code> :<br/> volumetric rendering의 특성을 살림과 동시에 fast splat-based rasterization 가능<br/> continuous representation이어야만 fast, high-quality radiance field training 가능하다는 기존 통념을 반전시킴</li> <li><code class="language-plaintext highlighter-rouge">CUDA</code> Implementation :<br/> training time의 80%는 Pytorch code (for 가독성)<br/> rasterization만 optimized CUDA kernels (for real-time)<br/> \(\rightarrow\) InstantNGP <d-cite key="InstantNGP">[2]</d-cite>처럼 optimization 나머지 부분도 전부 CUDA로 옮기면 훨씬 speedup 가능</li> <li><code class="language-plaintext highlighter-rouge">real-time rasterization by GPU</code> :<br/> rasterization이 main bottleneck인데<br/> GPU 힘으로 real-time rasterization pipeline 구현한 게<br/> 기존 volumetric ray-marching NeRF-based 기법보다 faster training, rendering 가능했던 비결</li> <li>Higher Quality than SOTA Mip-NeRF360(2022)</li> <li>Faster Training than SOTA InstantNGP(2022)</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="gaussian"/><category term="splatting"/><category term="rendering"/><category term="3d"/><category term="view"/><category term="synthesis"/><summary type="html"><![CDATA[3D GS for Real-Time Radiance Field Rendering]]></summary></entry></feed>