<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://semyeong-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://semyeong-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-08-06T06:47:20+00:00</updated><id>https://semyeong-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Multi-Modal Study</title><link href="https://semyeong-yu.github.io/blog/2024/Multimodal/" rel="alternate" type="text/html" title="Multi-Modal Study"/><published>2024-08-05T15:00:00+00:00</published><updated>2024-08-05T15:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Multimodal</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Multimodal/"><![CDATA[<h2 id="multi-modal-study">Multi-Modal Study</h2> <h3 id="storyimager---a-unified-and-efficient-framework-for-coherent-story-visualization-and-completion">StoryImager - A Unified and Efficient Framework for Coherent Story Visualization and Completion</h3> <h4 id="ming-tao-bing-kun-bao-hao-tang-yaowei-wang-changsheng-xu">Ming Tao, Bing-Kun Bao, Hao Tang, Yaowei Wang, Changsheng Xu</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2404.05979">StoryImager</a></p> </blockquote> <ul> <li><code class="language-plaintext highlighter-rouge">Story Visualization</code> Task : <ul> <li>input : prince image / cat image / prompts</li> <li>output : story images</li> <li>video generation과는 다른 게, story visualization은 웹툰 같다고 생각하면 됨<br/> story visualization은 image 간의 consistency를 유지하긴 하지만, video generation처럼 frame끼리 연속성을 보장할 필요는 없음</li> </ul> </li> <li>StoryImager: <ul> <li>task : coherent story visualization and completion</li> <li>기존 모델은 visualization과 continuation을 위한 model을 별도로 필요했는데,<br/> 본 논문은 single model (<code class="language-plaintext highlighter-rouge">통합적인 framework</code>) 제시<br/> by <code class="language-plaintext highlighter-rouge">global consistency</code> 반영!</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/StoryImager/1-480.webp 480w,/assets/img/2024-08-05-Multimodal/StoryImager/1-800.webp 800w,/assets/img/2024-08-05-Multimodal/StoryImager/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/StoryImager/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Architecture :<br/> maintain <code class="language-plaintext highlighter-rouge">global consistency</code><br/> by context-feature-extractor<br/> and FS-CAM (frame-story cross-attention module)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/StoryImager/2-480.webp 480w,/assets/img/2024-08-05-Multimodal/StoryImager/2-800.webp 800w,/assets/img/2024-08-05-Multimodal/StoryImager/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/StoryImager/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p><code class="language-plaintext highlighter-rouge">random masking</code> :<br/> make a story board from story images<br/> \(\rightarrow\) VAE<br/> \(\rightarrow\) random masking to VAE latent space</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">context-feature-extractor</code> :</p> <ul> <li>word-embeddings \(\rightarrow\) transformer<br/> \(\rightarrow\) prior embeddings<br/> \(\rightarrow\) MLP<br/> \(\rightarrow\) frame-aware latent prior<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">story board images가 story prompts를 반영하도록</code> 하기 위해 masked VAE latent space와 concat해서 이를 FS-CAM에서 story board로 사용</li> <li>word-embeddings \(\rightarrow\) transformer<br/> \(\rightarrow\) context embeddings<br/> \(\rightarrow\) transformer<br/> \(\rightarrow\) global context<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">global 정보</code> 주기 위해 FS-CAM에서 text prompts로 사용</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/StoryImager/4-480.webp 480w,/assets/img/2024-08-05-Multimodal/StoryImager/4-800.webp 800w,/assets/img/2024-08-05-Multimodal/StoryImager/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/StoryImager/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>FS-CAM (<code class="language-plaintext highlighter-rouge">frame-story cross-attention module</code>) :<br/> 개별 story board - 개별 text prompts 를 locally cross-attention한 것과,<br/> 전체 story board - 전체 text prompts 를 globally cross-attention한 것을<br/> concat</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/StoryImager/3-480.webp 480w,/assets/img/2024-08-05-Multimodal/StoryImager/3-800.webp 800w,/assets/img/2024-08-05-Multimodal/StoryImager/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/StoryImager/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="intelligent-grimm---open-ended-visual-storytelling-via-latent-diffusion-models">Intelligent Grimm - Open-ended Visual Storytelling via Latent Diffusion Models</h3> <h4 id="chang-liu-haoning-wu-yujie-zhong-xiaoyun-zhang-yanfeng-wang-weidi-xie">Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, Yanfeng Wang, Weidi Xie</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2306.00973">Intelligent Grimm</a></p> </blockquote> <ul> <li>Intelligent Grimm : <ul> <li>NIPS 2023에서 novelty 부족으로 reject 당했다가 보완해서 CVPR 2024에 accept</li> <li>task : open-ended visual storytelling</li> <li>StoryGen : unseen characters에 대해서도 추가적인 character-specific-optimization 없이 story visualization 가능</li> <li>StorySalon : online-video, open-source e-book 등 소싱해서 만든 dataset</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">context encoding</code> : <ul> <li>SDM : pre-trained stable diffusion model<br/> CLIP : pre-trained CLIP encoder<br/> VAE : pre-trained VAE</li> <li>visual condition feature = [SDM(image1, CLIP(text1)), SDM(image2, CLIP(text2)), …, SDM(imagek-1, CLIP(textk-1))]<br/> k-th frame image 만들기 위해 cross-attention 하는 데 사용</li> </ul> </li> <li>visual-language contextual fusion :<br/> <code class="language-plaintext highlighter-rouge">cross-attention</code> 사용<br/> 아래 Fig. (b)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/IntelligentGrimm/1-480.webp 480w,/assets/img/2024-08-05-Multimodal/IntelligentGrimm/1-800.webp 800w,/assets/img/2024-08-05-Multimodal/IntelligentGrimm/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/IntelligentGrimm/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">conditional generation</code> : <ul> <li>prev. frame t-1 \(\rightarrow\) add noise<br/> \(\rightarrow\) denoising one step<br/> \(\rightarrow\) diffusion U-Net</li> <li>prev. text \(\rightarrow\) text encoder<br/> \(\rightarrow\) diffusion U-Net</li> <li>diffusion U-Net (self-attention, text-attention)<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">denoising one step에 해당되는 feature</code>를 추출</li> <li>extracted image-diffusion-denoising-feature<br/> &amp; random noise<br/> &amp; current text \(\rightarrow\) text encoder<br/> \(\rightarrow\) StoryGen model (self-attention, image-attention, text-attention)<br/> \(\rightarrow\) current frame t</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">multi-frame conditioning</code> :<br/> story의 경우 frame t에 영향을 주는 image들이 frame t-1, frame t-2, … 일 수 있음<br/> 이 때, 과거 frames에 모두 같은 random noise를 줄 경우 성능 좋지 않아서<br/> <code class="language-plaintext highlighter-rouge">현재에 가까운 과거 frame일수록 noise를 덜 주는 식으로 temporal order를 부여</code>하면 성능 좋음</li> </ul> <h3 id="generating-realistic-images-from-in-the-wild-sounds">Generating Realistic Images from In-the-wild Sounds</h3> <h4 id="taegyeong-lee-jeonghun-kang-hyeonyu-kim-taehwan-kim">Taegyeong Lee, Jeonghun Kang, Hyeonyu Kim, Taehwan Kim</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2309.02405">Image from in-the-wild Sounds</a></p> </blockquote> <ul> <li> <p>novelty :<br/> 이전까지는 wild sound와 image 간의 pair가 없어서 limited categories와 music의 sound로부터 image를 생성하는 연구만 진행되었음<br/> 본 논문은 sound와 image 간의 large paired dataset이 없더라도<br/> wild sound로부터 image를 생성하는 task를 최초로 제시</p> </li> <li> <p>method :</p> <ul> <li>stage (a) :<br/> <code class="language-plaintext highlighter-rouge">audio captioning</code>을 통해 sound를 text로 변환한 audio caption과<br/> sound의 dynamic 특성을 반영하기 위한 <code class="language-plaintext highlighter-rouge">audio attention</code>과<br/> 제대로 image visualization하기 위한 <code class="language-plaintext highlighter-rouge">sentence attention</code>을<br/> 함께 사용하여 positional encoding을 거친 뒤 vector w를 <code class="language-plaintext highlighter-rouge">initialize</code><br/> (이 때, Audio-Captioning-Transformer model의 decoder에서 나오는 확률값을 audio attention이라고 정의함)</li> <li>stage (b) :<br/> audio caption으로부터 만든 vector z와<br/> stage (a)의 vector w로부터<br/> new latent vector z’를 만들고,<br/> <code class="language-plaintext highlighter-rouge">stable-diffusion model</code>을 이용하여 이로부터 image를 생성한다<br/> 여기서 image와 vector z 간의 <code class="language-plaintext highlighter-rouge">CLIPscore similarity</code>를 이용해서 audio caption으로부터 만든 vector z를 optimize하고<br/> image와 audio 간의 <code class="language-plaintext highlighter-rouge">AudioCLIP similarity</code>를 이용해서 <code class="language-plaintext highlighter-rouge">audio를 직접 optimize</code>한다<br/> (image가 text에 맞게 생성되도록 image를 점점 변화시키면서 생성하는 Style-CLIP에서 영감을 받아 이를 diffusion model에 적용)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/ImageSound/1-480.webp 480w,/assets/img/2024-08-05-Multimodal/ImageSound/1-800.webp 800w,/assets/img/2024-08-05-Multimodal/ImageSound/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/ImageSound/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>local minimum에 빠지지 않기 위해 audio attention과 sentence attention을 이용한 stage (a)의 initialization이 매우 중요</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/ImageSound/2-480.webp 480w,/assets/img/2024-08-05-Multimodal/ImageSound/2-800.webp 800w,/assets/img/2024-08-05-Multimodal/ImageSound/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/ImageSound/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Q1 :<br/> image는 pixel 단위로 값이 있어서 feature map을 통해 vector로 만들 수 있고, text는 word 단위로 embedding을 통해 vector를 만들 수 있습니다. AudioCLIP을 통해 audio를 직접 optimize했다는데 audio는 무엇을 기준으로 vector로 만들어서 optimize 가능한 건가요?</p> </li> <li> <p>A1 :<br/> audio는 melspectrogram을 만든 뒤 ViT에서 image 다루듯이 똑같이 patch로 쪼개서 vector로 만든다<br/> AudioCLIP similarity의 경우 audio encoding과 image encoding과 text encoding 간의 contrastive learning을 통해 구할 수 있다</p> </li> </ul> <h3 id="vivit---a-video-vision-transformer">ViViT - A Video Vision Transformer</h3> <h4 id="anurag-arnab-mostafa-dehghani-georg-heigold-chen-sun-mario-lucic-cordelia-schmid">Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, Cordelia Schmid</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2103.15691">ViViT</a></p> </blockquote> <ul> <li> <p>video of (T, H, W, C)를 sampling하여<br/> token sequence of (n_t, n_h, n_w, C) 을 만들고<br/> positional embedding을 더한 뒤 (N, d)로 reshape해서 transformer의 input으로 넣어줌</p> </li> <li> <p>uniform frame sampling :<br/> ViT에서처럼 각 2D frame을 독립적으로 embedding 후 모든 token을 concat</p> </li> <li> <p>Tubelet sampling :<br/> temporal info.를 반영하기 위해 tokenization 단계에서 spatial, temporal info.를 fuse</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/ViViT/1-480.webp 480w,/assets/img/2024-08-05-Multimodal/ViViT/1-800.webp 800w,/assets/img/2024-08-05-Multimodal/ViViT/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/ViViT/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/ViViT/2-480.webp 480w,/assets/img/2024-08-05-Multimodal/ViViT/2-800.webp 800w,/assets/img/2024-08-05-Multimodal/ViViT/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-Multimodal/ViViT/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Model 1 :<br/> CNN과 달리 transformer layer는 token 수에 비례하게 quadratic complexity를 가지므로 input frame에 linearly 필요</p> </li> <li> <p>Model 2 (factorized encoder) :<br/> spatial과 temporal을 두 개의 transformer encoder로 구성하여 많은 연산량 필요</p> </li> <li> <p>Model 3 (factorized self-attention) :<br/> 여전히 두 개의 encoder로 특정 dim만 뽑아서 attention 연산</p> </li> <li> <p>Model 4 (factorized dot-product attention) :<br/> spatial head의 경우 spatial-dim.에 대해서만 attention 수행</p> </li> </ul>]]></content><author><name></name></author><category term="generative"/><category term="multi-modal"/><category term="generative"/><summary type="html"><![CDATA[Paper Review]]></summary></entry><entry><title type="html">NeRF-Code</title><link href="https://semyeong-yu.github.io/blog/2024/NeRFcode/" rel="alternate" type="text/html" title="NeRF-Code"/><published>2024-08-05T15:00:00+00:00</published><updated>2024-08-05T15:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/NeRFcode</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/NeRFcode/"><![CDATA[<h2 id="nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h2> <h4 id="ben-mildenhall-pratul-psrinivasan-matthew-tancik">Ben Mildenhall, Pratul P.Srinivasan, Matthew Tancik</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2003.08934">https://arxiv.org/abs/2003.08934</a><br/> project website :<br/> <a href="https://www.matthewtancik.com/nerf">https://www.matthewtancik.com/nerf</a><br/> pytorch code :<br/> <a href="https://github.com/yenchenlin/nerf-pytorch">https://github.com/yenchenlin/nerf-pytorch</a><br/> <a href="https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file">https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file</a><br/> tiny tensorflow code :<br/> <a href="https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb">https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb</a><br/> Overview image reference :<br/> <a href="https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#dataflow">https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#dataflow</a></p> </blockquote> <h3 id="train-code-flow-overview">Train Code Flow Overview</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/1-480.webp 480w,/assets/img/2024-08-05-NeRFcode/1-800.webp 800w,/assets/img/2024-08-05-NeRFcode/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="load-data">Load Data</h3> <ul> <li>load data : <ul> <li>load_llff.py</li> <li>load_blender.py</li> <li>load_LINEMOD.py</li> <li>load_deepvoxels.py</li> </ul> </li> </ul> <h4 id="load_llff_data">load_llff_data()</h4> <ul> <li>LLFF dataset : real dataset<br/> return images, poses, bds, render_poses, i_test <ul> <li>images : np (N, H, W, C)</li> <li>poses : np (N, 3, 5)<br/> camera poses<br/> poses[:, 0:3, 0:3] : 3-by-3 rotation matrix<br/> poses[:, 0:3, 3:4] : 3-by-1 translation matrix<br/> poses[:, 0:3, 4:5] : H, W, focal-length for intrinsic matrix</li> <li>bds : np (N, 2)<br/> scene bounds<br/> dim=1 : 2 = 1(near bound) + 1(far bound)</li> <li>render_poses : np (M, 3, 5)<br/> dim=0 : the number of generated poses for novel view synthesis<br/> generate new pose along sphere or spiral path</li> <li>i_test : int<br/> index of holdout-view (avg pose랑 가장 비슷한 pose를 갖는 view)<br/> training에서 제외하여 test할 때 사용</li> <li>near, far = 0., 1. if ndc is true else near, far = 0.9 * bds.min(), 1. * bds.max()</li> </ul> </li> </ul> <h4 id="load_blender_data">load_blender_data()</h4> <ul> <li>Blender dataset : synthetic dataset<br/> return images, poses, render_poses, hwf, i_split <ul> <li>images : np (N, H, W, C)<br/> blender dataset은 RGB-A channel을 가지고 있어 C = 4</li> <li>i_train, i_val, i_test = i_split</li> <li>near, far = 2., 6.<br/> (blender synthetic dataset은 통제된 환경에서 수집된 data이므로 ndc 사용하지 않고 frustum의 near, far plane 고정)</li> <li>투명한 배경을 흰 배경으로 만들려면<br/> RGB * opacity + (1 - opacity) 를 통해<br/> RGB 값을 opacity만큼 반영하고 opacity가 작을수록(투명할수록) 색상이 흰색(1.)에 가까워지도록 함<br/> images = images[…,:3]*images[…,-1:] + (1.-images[…,-1:])</li> <li>그냥 투명한 배경 그대로 쓰려면<br/> RGB-A channel에서 RGB channel만 가져와서 씀<br/> images = images[…,:3]</li> </ul> </li> </ul> <h4 id="load_linemod_data">load_LINEMOD_data()</h4> <ul> <li>LINEMOD dataset : real dataset<br/> return images, poses, render_poses, hwf, K, i_split, near, far</li> </ul> <h4 id="load_dv_data">load_dv_data()</h4> <ul> <li>Deepvoxels dataset : synthetic dataset<br/> return images, poses, render_poses, hwf, i_split <ul> <li>near, far = hemi_R - 1., hemi_R + 1.<br/> where hemi_R = np.mean(np.linalg.norm(poses[:,:3,-1], axis=-1))<br/> camera center들로 이루어진 반구의 평균 반지름</li> </ul> </li> </ul> <h3 id="create-nerf-model">Create NeRF Model</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/4-480.webp 480w,/assets/img/2024-08-05-NeRFcode/4-800.webp 800w,/assets/img/2024-08-05-NeRFcode/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>args.N_importance : 추가적으로 fine-MLP에서 사용할 sample 개수 <ul> <li>args.N_importance &gt; 0 : fine-MLP 사용함</li> <li>args.N_importance &lt;= 0 : fine-MLP 사용 안함</li> </ul> </li> <li>network_query_fn : 추후에 run_network() 사용하기 위한 함수 <ul> <li>input : position info., view-direction info., model</li> <li>output : model output</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/10-480.webp 480w,/assets/img/2024-08-05-NeRFcode/10-800.webp 800w,/assets/img/2024-08-05-NeRFcode/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>render_kwargs_train : dict for rendering <ul> <li>network_query_fn : 추후에 run_network() 사용하기 위한 함수</li> <li>perturb : 일반화 위해 ray-sampling할 때 작은 난수를 추가할지 여부</li> <li>network_fine, network_fn : fine-MLP, coarse-MLP</li> <li>N_importance, N_samples : number of sampels for fine-MLP, coarse-MLP</li> <li>white_bkgd : rendering에서 alpha-channel 사용할 때 투명한 부분이 흰색으로 채워지도록 할지 여부</li> <li>raw_noise_std : 일반화 위해 model output 중 opacity에 추가할 noise의 std값</li> <li>lindisp : <ul> <li>NDC를 사용하는 llff dataset의 경우 lindisp = False로 설정하여<br/> 먼 거리의 scene도 적절히 표현하기 위해 depth를 균등하게 sampling</li> <li>NDC를 사용하지 않는 나머지 dataset의 경우 lindisp = True로 설정하여<br/> 가까운 scene의 디테일을 잘 포착하기 위해 가까운 depth를 더 많이 sampling</li> </ul> </li> </ul> </li> </ul> <h4 id="positional-encoding">Positional Encoding</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/2-480.webp 480w,/assets/img/2024-08-05-NeRFcode/2-800.webp 800w,/assets/img/2024-08-05-NeRFcode/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>get_embedder() input :<br/> PE freq. 개수 \(L\) 과 PE 쓸지말지 여부</li> <li>get_embedder() output :<br/> PE-function과 PE 결과의 dim.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/3-480.webp 480w,/assets/img/2024-08-05-NeRFcode/3-800.webp 800w,/assets/img/2024-08-05-NeRFcode/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>self.embed_fns :<br/> 각 frequency(\(0 \sim 2^{L-1}\))와 각 period function(\(sin, cos\))에 대한<br/> list of lambda functions<br/> \([sin(2^0x), cos(2^0x), \ldots sin(2^{L-1}x), cos(2^{L-1}x)]\)</li> <li>Embedder.embed(x) :<br/> self.embed_fns의 각 PE-function을 input x에 적용하여 dim=-1에 대해 concat</li> </ul> <h4 id="nerf">NeRF</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/5-480.webp 480w,/assets/img/2024-08-05-NeRFcode/5-800.webp 800w,/assets/img/2024-08-05-NeRFcode/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>input_ch : position info. dim.</li> <li>input_ch_views : view-direction info. dim.</li> <li>use_viewdirs : view-direction info.를 사용할지 말지 여부<br/> (view-direction info.를 사용하면 RGB color 계산에 도움됨)</li> <li>output_ch :<br/> use_viewdirs가 False일 때만 사용하는 값<br/> args.N_importance &gt; 0일 때(fine-MLP 사용할 때)는 5 (RGB, opacity, <code class="language-plaintext highlighter-rouge">?????</code>)<br/> args.N_importance &lt;= 0일 때(fine-MLP 사용 안 할 때)는 4 (RGB, opacity)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/6-480.webp 480w,/assets/img/2024-08-05-NeRFcode/6-800.webp 800w,/assets/img/2024-08-05-NeRFcode/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>input x를 position info.와 view-direction info.로 쪼갬</li> <li>self.use_viewdirs가 True일 때(view-direction info. 사용할 때) :<br/> position info.만 넣어서 opacity를 뽑은 뒤<br/> view-direction info.를 추가로 넣어서 RGB 뽑고<br/> dim=-1에 대해 concat</li> <li>self.use_viewdirs가 False일 때(view-direction info. 사용 안 할 때) :<br/> position info.만 넣어서 output_ch만큼 한 번에 뽑음</li> </ul> <h4 id="run_network">run_network</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/9-480.webp 480w,/assets/img/2024-08-05-NeRFcode/9-800.webp 800w,/assets/img/2024-08-05-NeRFcode/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/7-480.webp 480w,/assets/img/2024-08-05-NeRFcode/7-800.webp 800w,/assets/img/2024-08-05-NeRFcode/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>flatten position and flatten view-direction \(\rightarrow\) positional encoding \(\rightarrow\) batchify model and apply model \(\rightarrow\) reshape again output</li> </ul> <h4 id="batchify">batchify</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-NeRFcode/8-480.webp 480w,/assets/img/2024-08-05-NeRFcode/8-800.webp 800w,/assets/img/2024-08-05-NeRFcode/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-05-NeRFcode/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>input이 주어지면 chunk만큼씩 쪼개서 적용하는 model 반환</li> </ul> <p>###</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_rays_np</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">c2w</span><span class="p">):</span>
    <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">W</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">W</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">H</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">H</span><span class="p">))</span> <span class="c1"># pytorch's meshgrid has indexing='ij', so both i and j have shape (W, H)
</span>    <span class="n">i</span> <span class="o">=</span> <span class="n">i</span><span class="p">.</span><span class="nf">t</span><span class="p">()</span> <span class="c1"># width grid : shape (H, W)
</span>    <span class="n">j</span> <span class="o">=</span> <span class="n">j</span><span class="p">.</span><span class="nf">t</span><span class="p">()</span> <span class="c1"># height grid : shape (H, W)
</span>
    <span class="c1"># Apply intrinsic matrix
</span>    <span class="n">dirs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([(</span><span class="n">i</span><span class="o">-</span><span class="n">K</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span><span class="o">/</span><span class="n">K</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="p">(</span><span class="n">j</span><span class="o">-</span><span class="n">K</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span> <span class="o">/</span> <span class="n">K</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">i</span><span class="p">)],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># dirs : shape (H, W, 3) : H*W개의 3D rays
</span>    
    <span class="c1"># Apply extrinsic matrix
</span>    <span class="c1"># Rotate ray directions from camera frame to the world frame by applying dot product
</span>    <span class="n">rays_d</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dirs</span><span class="p">[...,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">c2w</span><span class="p">[:</span><span class="mi">3</span><span class="p">,</span> <span class="p">:</span><span class="mi">3</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># same with "rays_d = [c2w.dot(dir) for dir in dirs]"
</span>    <span class="c1"># dirs[..., np.newaxis, :] : shape (H, W, 1, 3) -&gt; (H, W, 3, 3) by broadcasting 
</span>    <span class="c1"># c2w[:3, :3] : shape (3, 3) -&gt; (H, W, 3, 3) by broadcasting
</span>    <span class="c1"># rays_d : shape (H, W, 3)
</span>    
    <span class="c1"># Translate camera frame's origin to the world frame. It is the origin of all rays.
</span>    <span class="n">rays_o</span> <span class="o">=</span> <span class="n">c2w</span><span class="p">[:</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">expand</span><span class="p">(</span><span class="n">rays_d</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="c1"># rays_o : shape (3, H*W) -&gt; (H, W, 3)
</span>    <span class="k">return</span> <span class="n">rays_o</span><span class="p">,</span> <span class="n">rays_d</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="rendering"/><category term="3d"/><summary type="html"><![CDATA[NeRF Code Review]]></summary></entry><entry><title type="html">MipNeRF</title><link href="https://semyeong-yu.github.io/blog/2024/MipNeRF/" rel="alternate" type="text/html" title="MipNeRF"/><published>2024-08-03T01:03:00+00:00</published><updated>2024-08-03T01:03:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/MipNeRF</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/MipNeRF/"><![CDATA[<h2 id="mip-nerf-a-multiscale-representation-for-anti-aliasing-neural-radiance-fields">Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</h2> <h4 id="jonathan-t-barron-ben-mildenhall-matthew-tancik-peter-hedman-ricardo-martin-brualla-pratul-p-srinivasan">Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2103.13415">https://arxiv.org/abs/2103.13415</a><br/> project website :<br/> <a href="https://jonbarron.info/mipnerf/">https://jonbarron.info/mipnerf/</a><br/> pytorch code :<br/> <a href="https://github.com/bebeal/mipnerf-pytorch">https://github.com/bebeal/mipnerf-pytorch</a><br/> <a href="https://github.com/google/mipnerf">https://github.com/google/mipnerf</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>ray-tracing하여 point-encoding 대신 cone-tracing하여 region-encoding 이므로 frustum의 모양과 크기 정보를 encode할 수 있어서 scale 반영 가능</li> <li>IPE 단계에서 <code class="language-plaintext highlighter-rouge">high variance (distant view)</code>일 때 <code class="language-plaintext highlighter-rouge">high freq.를 attenuate</code> (pre-filtering) 하여<br/> <code class="language-plaintext highlighter-rouge">임의의 continuous-space scale</code>을 가지는 scene에 대해 <code class="language-plaintext highlighter-rouge">anti-aliased</code> representation 학습 가능<br/> \(\rightarrow\) multi-resolution dataset에 대해 성능 대폭 향상<br/> \(\rightarrow\) scale-aware하므로 <code class="language-plaintext highlighter-rouge">single MLP</code> 하나만으로 충분하여 빠르고 가벼움</li> <li>camera center로부터 각 pixel로 3D cone을 쏜 다음,<br/> <code class="language-plaintext highlighter-rouge">frustum을 multi-variate Gaussian으로 근사</code>한 뒤,<br/> Gaussian 내 좌표를 positional encoding한 것 (PE-basis-lifted Gaussian)에 대해 expected value \(E \left[ \gamma (x) \right]\) 계산<br/> 주의 : frustum이 Gaussian 분포를 따르는 게 아니라, frustum 내부의 mean, variance 값을 먼저 구한 뒤 해당 mean, variance 값을 갖는 Gaussian으로 frustum을 대신(근사)할 수 있다고 생각!</li> </ol> </blockquote> <h2 id="introduction">Introduction</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/2-480.webp 480w,/assets/img/2024-08-03-MipNeRF/2-800.webp 800w,/assets/img/2024-08-03-MipNeRF/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>기존 NeRF의 문제점 : <ul> <li>rendering 위해 sampling할 때 <code class="language-plaintext highlighter-rouge">single ray</code> per pixel 쏴서 composite 하므로<br/> dataset에 있는 물체의 크기(resolution)가 일정하지 않을 때<br/> multi-scales images에 대해 학습하더라도</li> <li><code class="language-plaintext highlighter-rouge">blurry</code> rendering in <code class="language-plaintext highlighter-rouge">close-up</code> views<br/> (because 가까이서 찍어서 zoom-out하면 물체 in <code class="language-plaintext highlighter-rouge">high resolution</code>)</li> <li><code class="language-plaintext highlighter-rouge">aliased</code>(계단) rendering in <code class="language-plaintext highlighter-rouge">distant</code> views<br/> (because 멀리서 찍어서 zoom-in하면 물체 in <code class="language-plaintext highlighter-rouge">low resolution</code>)</li> <li>그렇다고 multiple rays per pixel through its footprint로 brute-force super-sampling(offline rendering)하는 것은 정확하긴 하겠지만 too costly 비현실적</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Minmap</code> Approach :<br/> classic 컴퓨터 그래픽스 분야에서 rendering할 때 aliasing을 없애기 위한 <code class="language-plaintext highlighter-rouge">pre-filtering</code> 기법<br/> 본 논문인 Mip-NeRF가 여기서 영감을 얻음<br/> signal(e.g. image)을 diff. <code class="language-plaintext highlighter-rouge">downsampling scales</code>로 나타낸 뒤 pixel footprint를 근거로 ray에 사용하기 위한 <code class="language-plaintext highlighter-rouge">적절한 scale을 고른다</code><br/> render time에 할 복잡할 일을 pre-computation phase에 미리 하는 것일 뿐이긴 하지만, 주어진 texture마다 한 번만 minmap을 만들면 된다는 장점이 있다</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/4-480.webp 480w,/assets/img/2024-08-03-MipNeRF/4-800.webp 800w,/assets/img/2024-08-03-MipNeRF/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Mip-NeRF : <ul> <li>represent pre-filtered scene at <code class="language-plaintext highlighter-rouge">continuous space of scales</code></li> <li>ray 대신 <code class="language-plaintext highlighter-rouge">conical frustum</code> 사용해서 <code class="language-plaintext highlighter-rouge">anti-aliased</code> rendering with fine details</li> <li>multi-scale variant of dataset에 대해 평균 error rate 60% 감소</li> <li>NeRF가 hierarchical sampling을 위해 coarse and fine MLP를 분리했다면, Mip-NeRF는 <code class="language-plaintext highlighter-rouge">scale-aware</code>하므로 <code class="language-plaintext highlighter-rouge">single multi-scale MLP만으로 충분</code><br/> 따라서 NeRF보다 7% 빠르고, param. 수는 절반이고, sampling도 더 효율적</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/1-480.webp 480w,/assets/img/2024-08-03-MipNeRF/1-800.webp 800w,/assets/img/2024-08-03-MipNeRF/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> ray 대신 cone을 쏘고, point-encoding 대신 frustum region-encoding </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/3-480.webp 480w,/assets/img/2024-08-03-MipNeRF/3-800.webp 800w,/assets/img/2024-08-03-MipNeRF/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>차이 : 기존 NeRF는 <code class="language-plaintext highlighter-rouge">a single point</code>를 encode하고, Mip-NeRF는 <code class="language-plaintext highlighter-rouge">a region of space</code>를 encode</p> </li> <li> <p>기존 NeRF :<br/> camera center로부터 각 pixel로 ray를 하나 쏜 다음 point sampling한 뒤 positional encoding<br/> point-sampled feature는 ray가 보는 <code class="language-plaintext highlighter-rouge">volume의 모양과 크기를 무시</code>하는 것임<br/> 예를 들어 training할 때 camera1로부터 t 사이의 간격이 평균 10cm로 학습된 scene에 대해<br/> camera2로 inference를 할 때 t 사이의 간격이 평균 1cm로 sampling된다면<br/> 10개의 점은 같은 point-based feature를 갖게 되어 scale을 고려하지 못함<br/> 이러한 ambiguity가 기존 NeRF의 성능 하락의 요인</p> </li> <li> <p>Mip-NeRF :<br/> volume 정보를 반영하기 위해 camera center로부터 각 pixel로 3D cone을 쏜 다음, 3D point 및 그 주위의 Gaussian region을 encode하기 위해 IPE</p> </li> <li> <p>IPE (<code class="language-plaintext highlighter-rouge">integrated positional encoding</code>) :<br/> region을 encode하기 위한 방식<br/> <code class="language-plaintext highlighter-rouge">frustum을 multi-variate Gaussian으로 근사</code>한 뒤,<br/> Gaussian 내 좌표를 positional encoding한 것 (PE-basis-lifted Gaussian)에 대해 expected value \(E \left[ \gamma (x) \right]\) 계산</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/5-480.webp 480w,/assets/img/2024-08-03-MipNeRF/5-800.webp 800w,/assets/img/2024-08-03-MipNeRF/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="related-work">Related Work</h2> <h3 id="anti-aliasing-in-rendering">Anti-aliasing in Rendering</h3> <blockquote> <p><code class="language-plaintext highlighter-rouge">anti-aliasing</code>을 위한 고전적인 방법으로는 두 가지가 있다.</p> </blockquote> <ol> <li><code class="language-plaintext highlighter-rouge">supersampling</code> : <ul> <li>rendering할 때 <code class="language-plaintext highlighter-rouge">multiple rays per pixel</code>을 쏴서 Nyquist frequency에 가깝게 sampling rate를 높임 (super-sampling)</li> <li><code class="language-plaintext highlighter-rouge">expensive</code> as runtime increases linearly with the super-sampling rate, so used only in <code class="language-plaintext highlighter-rouge">offline</code> rendering</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">pre-filtering</code> : <ul> <li>target sampling rate에 맞춰서 Nyquist frequency를 줄이기 위해 scene에 <code class="language-plaintext highlighter-rouge">lowpass-filter</code>를 씌운 버전 사용</li> <li>scene을 미리 <code class="language-plaintext highlighter-rouge">downsampling multi-scales</code> (sparse voxel octree 또는 minmap)로 나타낸 뒤, <code class="language-plaintext highlighter-rouge">ray 대신 cone</code>을 추적하여 cone과 scene이 만나는 곳의 cone’s footprint에 대응되는 적절한 scale을 골라서 사용 (<code class="language-plaintext highlighter-rouge">target sampling rate에 맞는 적절한 scale</code>)</li> <li>scene에 filter 씌운 버전을 한 번만 미리 계산하면 되므로, better for <code class="language-plaintext highlighter-rouge">real-time</code> rendering</li> </ul> </li> </ol> <blockquote> <p>그런데 아래의 이유로 고전적인 multi-scale representation은 적용 불가능</p> <ul> <li>input scene의 <code class="language-plaintext highlighter-rouge">geometry를 미리 알 수 없으므로</code> pre-filtering 할 수가 없어서<br/> 대신 pre-filtering 방식 자체를 training할 때 학습해야 한다</li> <li>input scene의 <code class="language-plaintext highlighter-rouge">scale이 continuous</code>하므로 a fixed number of scales (discrete)과 상황이 다르다</li> </ul> </blockquote> <p>\(\rightarrow\) 결론 : 따라서 Mip-NeRF는 training하는 동안, <code class="language-plaintext highlighter-rouge">임의의 scale</code>에 대해 query 받을 수 있는, scene의 <code class="language-plaintext highlighter-rouge">pre-filtered representation</code>을 학습한다.</p> <h3 id="scene-representations-for-view-synthesis">Scene Representations for View Synthesis</h3> <ul> <li> <p>만약 images of scene are captured <code class="language-plaintext highlighter-rouge">densely</code> 라면, novel view synthesis 위해, intermediate representation of scene을 reconstruct하지 않고 <code class="language-plaintext highlighter-rouge">light field interpolation</code> 기법 사용</p> </li> <li>만약 images of scene are captured <code class="language-plaintext highlighter-rouge">sparsely</code> 라면, novel view synthesis 위해, <code class="language-plaintext highlighter-rouge">explicit representation</code> of the scene’s 3D geometry and appearance를 reconstruct <ul> <li><code class="language-plaintext highlighter-rouge">polygon-mesh-based</code> representation (<code class="language-plaintext highlighter-rouge">discrete, classic</code>) :<br/> with either diffuse or view-dependent textures<br/> can be stored efficiently, but mesh geometry optimization에 gradient descent를 이용하는 것은 불연속점 및 극소점 때문에 어렵</li> <li><code class="language-plaintext highlighter-rouge">volumetric</code> representation : <ul> <li><code class="language-plaintext highlighter-rouge">voxel-grid-based</code> representation (<code class="language-plaintext highlighter-rouge">discrete, classic</code>) : deep learning으로 학습 가능, but 고해상도의 scene에는 부적합</li> <li><code class="language-plaintext highlighter-rouge">coordinate-based</code> neural representation (<code class="language-plaintext highlighter-rouge">continuous, recent</code>) : 3D 좌표를 그 위치에서의 scene의 특징(e.g. volume density, radiance)으로 mapping하는 continuous function을 MLP로 예측 <ul> <li>implicit surface representation</li> <li><code class="language-plaintext highlighter-rouge">implicit volumetric NeRF representation</code></li> </ul> </li> </ul> </li> </ul> </li> <li>문제점 :<br/> 그동안 view synthesis를 할 때 sampling 및 aliasing에는 덜 주목했다<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">classic discrete</code> representation의 경우 앞에서 소개한 minmap, octree와 같은 고전적인 multi-scale pre-filtering 기법을 쓰면 aliasing 없이 rendering 가능하다<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">coordinate-based</code> representation의 경우 scale이 continuous하므로 고전적인 anti-aliasing 기법은 사용할 수 없다<br/> \(\rightarrow\) sparse voxel octree 기반의 multi-scale representation으로 implicit surface의 continuous neural representation을 구현한 논문 <d-cite key="continuouspriori">[1]</d-cite> 있긴 하지만, scene geometry의 priori를 알아야 한다는 제약이 있다<br/> \(\rightarrow\) Mip-NeRF는 training하는 동안, <code class="language-plaintext highlighter-rouge">임의의 scale</code>에 대해 query 받을 수 있는, scene의 <code class="language-plaintext highlighter-rouge">anti-aliased (pre-filtered)</code> representation을 학습한다.</li> </ul> <h2 id="method">Method</h2> <h3 id="cone-tracing-and-positional-encoding">Cone Tracing and Positional Encoding</h3> <ul> <li>Cone Tracing : <ul> <li>Let \(d\) is cone direction vector from \(o\) to image plane</li> <li>Let \(\hat r =\) radius at image plane \(o + d\) \(= \frac{2}{\sqrt{12}}\) of pixel-width<br/> so that image plane에서의 cone의 \(x, y\) 축 variance가 pixel’s footprint의 variance와 같아지도록<br/> \(\hat r\)은 ray의 radius 변화율, 즉 frustum의 넓이를 결정</li> <li>\(t \in [t_0, t_1]\) 일 때 conical frustum 내의 \(x\)는 아래 범위의 값을 가질 때 indicator function \(F(x, o, d, \hat r, t_0, t_1)=1\)이다<br/> \(F(x, o, d, \hat r, t_0, t_1) = 1 \left\{ (t_0 \lt \frac{d^T(x-o)}{\| d \|^2} \lt t_1) \land (\frac{d^T(x-o)}{\| d \| \| x-o \|} \gt \frac{1}{\sqrt{1+(\frac{\hat r}{\| d \|})^2}}) \right\}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/6-480.webp 480w,/assets/img/2024-08-03-MipNeRF/6-800.webp 800w,/assets/img/2024-08-03-MipNeRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Region Encoding :<br/> conical frustum 내에 있는 모든 좌표 \(x\)에 대해 직접<br/> expected value \(E \left[ \gamma (x) \right] = \frac{\int \gamma (x) F(x, o, d, \hat r, t_0, t_1) dx}{\int F(x, o, d, \hat r, t_0, t_1) dx}\) 계산하면<br/> region을 encode할 수 있는데<br/> 여기서 분자의 적분식은 closed-form solution이 없음<br/> \(\rightarrow\) 직접 계산하지 말고<br/> <code class="language-plaintext highlighter-rouge">conical-frustum을 multi-variate Gaussian으로 근사</code>한 뒤<br/> Gaussian 내에 있는 모든 좌표 \(x\)에 대해<br/> expected value \(E \left[ \gamma (x) \right]\) 계산</p> </li> <li>frustum을 multi-variate Gaussian으로 근사 : <ul> <li>conical-frustum은 대칭적인 원이기 때문에<br/> \(o, d\) 뿐만 아니라 아래의 3가지 정보만 알면 Gaussian을 특정할 수 있다 <ul> <li><code class="language-plaintext highlighter-rouge">mean distance along ray from o</code> \(\mu_{t}\)</li> <li><code class="language-plaintext highlighter-rouge">variance along ray</code> \(\sigma_{t}^2\)</li> <li><code class="language-plaintext highlighter-rouge">variance perpendicular to ray</code> \(\sigma_{r}^2\)</li> </ul> </li> <li>Let mid-point \(t_{\mu} = \frac{t_0+t_1}{2}\)<br/> Let half-width \(t_{\sigma}=\frac{t_1-t_0}{2}\)</li> <li>아래 수식의 유도과정은 하위에 별도로 정리함<br/> \(\mu_{t} = t_{\mu} + \frac{2t_{\mu}t_{\sigma}^2}{3t_{\mu}^2+t_{\sigma}^2}\)<br/> \(\sigma_{t}^2 = \frac{t_{\sigma}^2}{3} - \frac{4t_{\sigma}^4(12t_{\mu}^2-t_{\sigma}^2)}{15(3t_{\mu}^2+t_{\sigma}^2)^2}\)<br/> \(\sigma_{r}^2 = \hat r^2 (\frac{t_{\mu}^2}{4} + \frac{5t_{\sigma}^2}{12} - \frac{4t_{\sigma}^4}{15(3t_{\mu}^2+t_{\sigma}^2)})\)</li> <li>위의 3가지 param.를 가지는 Gaussian은 <code class="language-plaintext highlighter-rouge">t-coordinate</code>에서 정의했는데<br/> 아래 수식에 의해 <code class="language-plaintext highlighter-rouge">world-coordinate</code>으로 변환할 수 있다<br/> \(\mu = o + \mu_{t}d\)<br/> \(\Sigma = \sigma_{t}^2(dd^T) + \sigma_{r}^2(I-\frac{dd^T}{\| d \|^2})\)<br/> where \(dd^T =\) \(d\) 의 outer product은 \(d\) 방향으로의 투영을 의미하는 rank-1 matrix<br/> where \(I-\frac{dd^T}{\| d \|^2}\) 는 \(\frac{d}{\| d\ \|}\) 와 수직인 subspace로의 투영을 의미하는 rank-2 matrix</li> </ul> </li> <li>Integrated Positional Encoding (IPE) : <ul> <li>목표 : 위에서 계산한 \(\mu, \Sigma\) 의 Gaussian 내에 있는 모든 좌표 \(x\)에 대해 expected value \(E \left[ \gamma (x) \right]\) 계산</li> <li>우선 <code class="language-plaintext highlighter-rouge">PE (positional-encoding) basis</code> P를 재정의<br/> \(P = \begin{pmatrix} \begin{matrix} 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2\end{matrix} &amp; \cdots &amp; \begin{matrix} 2^{L-1} &amp; 0 &amp; 0 \\ 0 &amp; 2^{L-1} &amp; 0 \\ 0 &amp; 0 &amp; 2^{L-1}\end{matrix} \end{pmatrix}^T\)<br/> \(\gamma (x) = \begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}\)</li> <li>\(E \left[ \gamma (x) \right]\) 는 expectation over \(\gamma (x) = \begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}\) 이므로<br/> \(x\) in Gaussian of \(\mu, \Sigma\) \(\rightarrow\) \(\gamma (x)\) in Gaussian of \(\mu_{r}, \Sigma_{r}\) 로 변환해야 한다<br/> 즉, <code class="language-plaintext highlighter-rouge">PE basis P로 lift</code>한 뒤의 mean과 covariance를 구해야 한다<br/> Since \(Cov[Ax, By] = A Cov[x, y] B^T\),<br/> \(\mu_{r} = P \mu\)<br/> \(\Sigma_{r} = P \Sigma P^T\)</li> <li>최종적으로 \(E \left[ \gamma (x) \right]\) , 즉 <code class="language-plaintext highlighter-rouge">expectation over lifted multi-variate Gaussian</code> of \(\mu_{r}, \Sigma_{r}\) 을 구하면 된다<br/> Since \(E_{k \sim N(\mu, \sigma^2)}[e^{itk}] = exp(i \mu t - \frac{1}{2} \sigma^2 t^2)\) and \(sin(k) = \frac{e^{ik}-e^{-ik}}{2i}\) and \(cos(k) = \frac{e^{ik}+e^{-ik}}{2}\),<br/> \(E_{k \sim N(\mu, \sigma^2)}[sin(k)] = sin(\mu)exp(-\frac{1}{2}\sigma^2)\) and \(E_{k \sim N(\mu, \sigma^2)}[cos(k)] = cos(\mu)exp(-\frac{1}{2}\sigma^2)\) for each axis-k<br/> (positional-encoding은 각 dim.을 independently encode하므로 marginal distribution of \(\gamma (x)\) 에 의존)<br/> \(\rightarrow\)<br/> \(\gamma (\mu, \Sigma) = E_{x \sim N(\mu, \Sigma)} [\gamma (x)] = E_{Px \sim N(\mu_{r}, \Sigma_{r})} [\begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}]\)<br/> \(= \begin{bmatrix} sin(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \\ cos(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \end{bmatrix}\)<br/> where \(\circledast\) is element-wise multiplication</li> <li>\(diag(\Sigma_{r})\) 만 필요하므로 \(\Sigma_{r}\) 전부 계산하지 말고 <code class="language-plaintext highlighter-rouge">efficiently diagonal만 계산</code><br/> \(diag(\Sigma_{r}) = diag(P \Sigma P^T) = \left[ diag(\Sigma), 4 diag(\Sigma), \ldots , 4^{L-1}diag(\Sigma) \right]^T\)<br/> where 3d-vector \(diag(\Sigma) = \sigma_{t}^2(d \circledast d) + \sigma_{r}^2(1-\frac{d \circledast d}{\| d \|^2})\)<br/> diagonal만 직접 계산하면, IPE feature는 PE feature랑 비슷하게 cost 소모</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/7-480.webp 480w,/assets/img/2024-08-03-MipNeRF/7-800.webp 800w,/assets/img/2024-08-03-MipNeRF/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>IPE vs PE : <ul> <li>PE :<br/> point를 encode<br/> 0~L까지의 <code class="language-plaintext highlighter-rouge">모든 frequencies에 대해 동일하게</code> encode<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">high-freq.</code> PE features are <code class="language-plaintext highlighter-rouge">aliased</code><br/> (PE period가 interval width보다 작은 경우 PE over interval oscillates repeatedly)</li> <li>IPE :<br/> interval region을 integrate하여 encode<br/> IPE feature를 만드는 데 사용된 interval \(t \in [t_0, t_1]\) width보다 period가 작은 <code class="language-plaintext highlighter-rouge">high freq.</code>의 경우 <code class="language-plaintext highlighter-rouge">attenuate</code>하여 <code class="language-plaintext highlighter-rouge">anti-aliasing</code><br/> by \(exp(-\frac{1}{2}(\omega \sigma)^2)\) term</li> <li>위와 같은 특성 덕분에 IPE는 interval 내 공간의 모양과 크기를 smoothly encode할 수 있는 anti-aliased PE 기법이다!</li> <li>high freq.는 IPE 단계 자체에서 attenuate되므로 <code class="language-plaintext highlighter-rouge">L을 hyper-param.로 두지 않고 extremely large fixed-value</code>로 두면 된다</li> </ul> </li> <li>IPE의 의미 :<br/> 이게 Mip-NeRF의 핵심!! <ul> <li>수식 :<br/> PE-basis P 는 다양한 frequency \(\omega\) 로 구성되어 있고<br/> 각 element는 \(E_{x \sim N(\mu, \Sigma)} [\gamma_{\omega} (x)] = sin(\omega \mu) exp(-\frac{1}{2}(\omega \sigma)^2)\)</li> <li>distant view :<br/> <code class="language-plaintext highlighter-rouge">distant views (low-resolution)</code>, 즉 멀리 있는 <code class="language-plaintext highlighter-rouge">wide frustum</code> (high variance \(\sigma\))의 경우에는<br/> <code class="language-plaintext highlighter-rouge">detail-info.</code> (high freq. \(\omega\))는 <code class="language-plaintext highlighter-rouge">training에 사용하지 않겠다</code><br/> \(\rightarrow\) more attenuation for high \(\sigma\) and high \(\omega\)</li> <li>close view :<br/> <code class="language-plaintext highlighter-rouge">close views (high-resolution)</code>, 즉 가까이 있는 <code class="language-plaintext highlighter-rouge">narrow frustum</code> (low variance \(\sigma\))의 경우에는<br/> <code class="language-plaintext highlighter-rouge">detail-info.</code> (high freq. \(\omega\))를 training할 때 좀 더 <code class="language-plaintext highlighter-rouge">허용</code></li> <li>위와 같이 scale을 반영할 수 있으므로 blurry 및 aliased rendering 문제 해결 가능!</li> </ul> </li> <li>수식 <code class="language-plaintext highlighter-rouge">Summary</code> : <ul> <li>frustum을 근사하는 multi-variate Gaussian의 mean, variance \(\mu, \sigma\) 를 구한다</li> <li>PE-basis P로 lift한 Gaussian의 mean, variance \(\mu_{r}, \Sigma_{r}\) 를 구한다<br/> \(P = \begin{pmatrix} \begin{matrix} 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2\end{matrix} &amp; \cdots &amp; \begin{matrix} 2^{L-1} &amp; 0 &amp; 0 \\ 0 &amp; 2^{L-1} &amp; 0 \\ 0 &amp; 0 &amp; 2^{L-1}\end{matrix} \end{pmatrix}^T\)<br/> \(\gamma (x) = \begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}\)<br/> \(\mu_{r} = P \mu\) and \(\Sigma_{r} = P \Sigma P^T\)</li> <li>\(E_{x \sim N(\mu_{r}, \Sigma_{r})} [\gamma (x)] = \begin{bmatrix} sin(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \\ cos(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \end{bmatrix}\)<br/> (efficiently \(\Sigma_{r}\) 의 diagonal만 직접 계산)</li> </ul> </li> </ul> <h3 id="conical-frustum-integral-derivation">Conical Frustum Integral Derivation</h3> <ul> <li> <p>우선 <code class="language-plaintext highlighter-rouge">Cartesian-coordinate</code>에서 <code class="language-plaintext highlighter-rouge">conical-coordinate</code>으로 변환<br/> \((x, y, z) = \varphi (r, t, \theta) = t \cdot (r cos \theta , r sin \theta , 1)\)<br/> where \(\theta \in [0, 2 \pi)\) and \(t \geq 0\) and \(\| r \| \leq \hat r\)<br/> Then,<br/> \(dx dy dz = | det(D \varphi) | dr dt d\theta\)<br/> \(= \begin{vmatrix} t cos\theta &amp; t sin\theta &amp; 0 \\ r cos\theta &amp; r sin\theta &amp; 1 \\ - rt sin\theta &amp; rt cos\theta &amp; 0 \end{vmatrix} dr dt d\theta\)<br/> \(= (rt^2cos^2\theta + rt^2sin\theta) dr dt d\theta\)<br/> \(= rt^2 dr dt d\theta\)</p> </li> <li> <p>conical frustum의 volume \(V = \int \int \int dx dy dz = \int_{0}^{2\pi} \int_{t_0}^{t_1} \int_{0}^{\hat r} r t^2 dr dt d\theta = \pi \hat r^2 \frac{t_1^3 - t_0^3}{3}\) 에 대해<br/> conical frustum에서 uniformly-sampling한 points의 <code class="language-plaintext highlighter-rouge">probability density function</code>은 \(\frac{rt^2}{V}\) 이다</p> </li> <li><code class="language-plaintext highlighter-rouge">t-axis</code> : <ul> <li> \[E \left[ t \right] = \int_{0}^{2\pi} \int_{t_0}^{t_1} \int_{0}^{\hat r} t \cdot \frac{rt^2}{V} dr dt d\theta = \frac{3(t_1^4 - t_0^4)}{4(t_1^3 - t_0^3)}\] </li> <li> \[E \left[ t^2 \right] = \int_{0}^{2\pi} \int_{t_0}^{t_1} \int_{0}^{\hat r} t^2 \cdot \frac{rt^2}{V} dr dt d\theta = \frac{3(t_1^5- t_0^5)}{5(t_1^3 - t_0^3)}\] </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">x-axis</code> (\(x = t r cos \theta\)) : <ul> <li> \[E \left[ t r cos\theta \right] = \int_{0}^{2\pi} \int_{t_0}^{t_1} \int_{0}^{\hat r} t r cos\theta \cdot \frac{rt^2}{V} dr dt d\theta = 0\] </li> <li> \[E \left[ (t r cos \theta)^2 \right] = \int_{0}^{2\pi} \int_{t_0}^{t_1} \int_{0}^{\hat r} t^2 r^2 cos^2 \theta \cdot \frac{rt^2}{V} dr dt d\theta = \frac{\hat r^2}{4} \frac{3(t_1^5 - t_0^5)}{5(t_1^3 - t_0^3)}\] </li> </ul> </li> <li> <p><code class="language-plaintext highlighter-rouge">y-axis</code> (\(y = t r sin \theta\)) :<br/> conical frustum은 x, y에 대해 symmetric하므로 위에서 구한 x-axis에서의 값과 동일</p> </li> <li>이제 conical frustum 내부에 있는 random point에 대한 mean, covariance 값을 구할 수 있다 <ul> <li><code class="language-plaintext highlighter-rouge">mean distance along ray from o</code> :<br/> \(\mu_{t} = E \left[ t \right] = \frac{3(t_1^4 - t_0^4)}{4(t_1^3 - t_0^3)}\)</li> <li><code class="language-plaintext highlighter-rouge">variance along ray</code> :<br/> \(\sigma_{t}^2 = E \left[ t^2 \right] - (E \left[ t \right])^2 = \frac{3(t_1^5- t_0^5)}{5(t_1^3 - t_0^3)} - \mu_{t}^2\)</li> <li><code class="language-plaintext highlighter-rouge">variance perpendicular to ray</code> :<br/> \(\sigma_{r}^2 = E \left[ x^2 \right] - (E \left[ x \right])^2 = \hat r^2 \frac{3(t_1^5 - t_0^5)}{20(t_1^3 - t_0^3)}\)</li> </ul> </li> <li>그런데 \(t_0, t_1\) 이 서로 가까우면 \(\frac{(t_1^5- t_0^5)}{(t_1^3 - t_0^3)}\) 과 같은 꼴은 numerically unstable as 0 or NaN instead of accurate values 이므로 training fail<br/> \(\rightarrow\)<br/> \(t_{\mu} = \frac{t_0+t_1}{2}\) and \(t_{\sigma}=\frac{t_1-t_0}{2}\) 로 re-parameterize하면<br/> <code class="language-plaintext highlighter-rouge">first-order term + correct(higher-order) term 꼴</code>로 정리 가능하고<br/> \(t_{\sigma}\) 가 작을 때에도 stable and accurate values 가짐 <ul> <li> \[\mu_{t} = t_{\mu} + \frac{2t_{\mu}t_{\sigma}^2}{3t_{\mu}^2+t_{\sigma}^2}\] </li> <li> \[\sigma_{t}^2 = \frac{t_{\sigma}^2}{3} - \frac{4t_{\sigma}^4(12t_{\mu}^2-t_{\sigma}^2)}{15(3t_{\mu}^2+t_{\sigma}^2)^2}\] </li> <li> \[\sigma_{r}^2 = \hat r^2 (\frac{t_{\mu}^2}{4} + \frac{5t_{\sigma}^2}{12} - \frac{4t_{\sigma}^4}{15(3t_{\mu}^2+t_{\sigma}^2)})\] </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">limitation</code> :<br/> frustum의 base 반지름과 top 반지름 차이가 클수록<br/> conical-frustum을 multi-variate Gaussian으로 approx.하는 건 inaccurate<br/> (예를 들어, camera FOV가 클 때 camera center와 매우 가까운 frustum)<br/> 대부분의 dataset에서는 흔하지 않은 case이긴 하지만,<br/> macro photography with fisheye lens와 같은 특별한 case에서 MipNeRF를 쓸 때는 frustum을 multi-variate Gaussian으로 approx.하는 게 문제가 될 수 있음</li> </ul> <h3 id="architecture">Architecture</h3> <ul> <li>아래 내용들을 제외하고는 NeRF의 Architecture와 동일 <ul> <li>ray-tracing 대신 cone-tracing</li> <li>PE 대신 IPE</li> <li>point-encoding 이므로 \(n\)개의 구간에 대해 \(n\)개의 point sampling<br/> \(\rightarrow\)<br/> interval(region)-encoding 이므로 \(n\)개의 구간을 위해 \(n+1\)개의 point sampling</li> <li>PE feature로는 scale을 반영할 수 없으므로 두 가지 MLP (coarse-MLP, fine-MLP) 이용해서 hierarchical sampling<br/> (coarse-MLP에서는 \(N_c=64\) points per ray, fine-MLP에서는 \(N_c+N_f=64+128\) points per ray)<br/> \(\rightarrow\)<br/> IPE feature 자체가 scale을 반영할 수 있으므로 MLP 하나를 반복해서 써서 hierarchical sampling<br/> (한 번은 \(N_c=128\) points per ray, 그 다음은 \(N_f=128\) points per ray)<br/> NeRF와 MipNeRF의 공정한 비교를 위해 같은 수(총 256개)의 point를 사용</li> <li>hierarchical sampling에서 piecewise-constant PDF of normalized \(w\) 에 따라 fine-sampling 하기 전에<br/> weight \(w_k\) 를 바로 사용하지 않고<br/> 2-tap MaxBlur filter 를 적용한 weight의 wide and smooth upper bound 를 사용<br/> \(w_k^{\ast} = \frac{1}{2}(max(w_{k-1}, w_k) + max(w_k, w_{k+1})) + \alpha\)<br/> where 빈 공간에서도 일부 samples 추출되도록 보장하기 위해 \(\alpha=0.01\) 설정</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/8-480.webp 480w,/assets/img/2024-08-03-MipNeRF/8-800.webp 800w,/assets/img/2024-08-03-MipNeRF/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> single MLP 쓰니까 coarse loss와 fine loss 간의 balance 맞추기 위해 hyperparam. gamma = 0.1로 설정 </div> <ul> <li>MaxBlur filter : <ul> <li>MaxPool 대신 MaxBlurPool 쓰면 aliasing 감소 효과</li> <li>MipNeRF에서 weight에 MaxBlur filter 쓰는 이유 :<br/> scene content는 아무래도 연속적으로 존재하니까<br/> 인접한 samples 간의 weight \(w\) 가 갑작스럽게 변하거나 불연속적인 outlier 를 제외하여 smoothing 해주는 역할</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/10-480.webp 480w,/assets/img/2024-08-03-MipNeRF/10-800.webp 800w,/assets/img/2024-08-03-MipNeRF/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> plot reference : charlieppark.kr </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/9-480.webp 480w,/assets/img/2024-08-03-MipNeRF/9-800.webp 800w,/assets/img/2024-08-03-MipNeRF/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-08-03-MipNeRF/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Setting :<br/> implementation on JaxNeRF<br/> 1 million iter., Adam optimizer, batch_size = 4096, lr from \(5 \cdot 10^{-4}\) to \(5 \cdot 10^{-6}\)</li> </ul> <h2 id="result">Result</h2> <ul> <li>multi-scale dataset에 대해 NeRF보다 error rate 60% 감소</li> <li>single-scale dataset에 대해 NeRF보다 error rate 17% 감소</li> <li>NeRF의 param.의 절반이고, NeRF보다 7% 빠름</li> <li>brute-force super-sampling한 버전보다 22배 빠른데 accuracy 비슷</li> </ul> <h2 id="question">Question</h2> <ul> <li>Q1 : distant view (scene content in low-resolution)일 때 IPE의 \(exp(-\frac{1}{2}(\omega \sigma)^2)\) term에 의해 high freq.를 attenuate하여 anti-aliasing 가능한 건 이해했는데,<br/> close view (scene content in high-resolution)일 때 blurry rendering은 어떻게 해결??</li> <li>A1 : 위에서 “Method - Cone Tracing and Positional Encoding - IPE의 의미”에 설명해둠</li> <li>Q2 : image plane에서의 cone의 \(x, y\) 축 variance가 pixel’s footprint의 variance와 같아지도록<br/> \(\hat r =\) radius at image plane \(o + d\) \(= \frac{2}{\sqrt{12}}\) of pixel-width 로 설정한다는데 이 부분이 이해가 되지 않습니다</li> <li>A2 : uniform distribution을 가정했을 때 pixel의 square variance는 \(\frac{w^2}{12}\) 이고, cone at image plane의 circle variance는 \(\frac{\hat r^2}{4}\) 이므로 variance 값이 같으려면 \(\hat r = \frac{2}{\sqrt{12}} \times w\)</li> <li>Q3 : \(E \left[ \gamma (x) \right] = \frac{\int \gamma (x) F(x, o, d, \hat r, t_0, t_1) dx}{\int F(x, o, d, \hat r, t_0, t_1) dx}\) 에서 분자의 적분식을 closed-form으로 계산할 수 없어서 conical frustum을 multi-variate Gaussian으로 근사했다는데,<br/> conical frustum의 모양과 크기 범위에 대한 parameter가 주어진다면 frustum 내부의 점 \(x\) 에 sin 및 cos을 씌운 \(\gamma (x)\) 의 경우 \(x\) 에 대해 공간 적분할 수 있지 않나요?</li> <li>A3 : frustum 내에 있는 모든 좌표에 \(\gamma\) 를 씌워서 공간 적분하는 것 자체가 말도 안 되게 복잡한 식이라 closed-form solution이 없기 때문에 frustum의 mean과 variance를 구해서 Gaussian으로 근사해서 expected value 구합니다</li> <li>Q4 : 논문을 보면 frustum을 multi-variate Gaussian으로 근사하기 위해서는 먼저 \(F(x, o, d, \hat r, t_0, t_1)\) 의 mean, covariance를 계산해야 한다고 쓰여있던데<br/> appendix를 보면 indicator function인 F의 mean과 covariance가 아니라 conical frustum의 \(r, t, \theta\) 범위를 이용해서 공간 적분해서 \(t, x, y\) 축의 mean과 variance를 계산하지 않나요?</li> <li>A4 : 맞습니다. 논문에서 \(F(x, o, d, \hat r, t_0, t_1)\) 의 mean, covariance를 계산해야 한다고 언급되어 있는 것은 단순히 frustum 내부 범위에 속해있는 지점에 대해 적분을 통해 mean, variance를 구해야 한다는 뜻인 것 같습니다.</li> <li>Q5 : NeRF에서 rendering할 때는 EWA volume splatting과 같은 좌표계 변환을 고려하지 않아도 되나요?</li> <li>A5 : NeRF에서는 ray를 따라 MLP의 output을 alpha-compositing하여 직접 pixel 값을 얻어내므로 ray를 쓰기 위해 cam-to-world coordinate 변환만 필요하고, projection에 의한 non-linear 좌표계 변환과는 관련이 없다.<br/> 반면, Gaussian Splatting에서는 rendering할 때 3D Gaussian 자체를 직접 projection해서 쓰기 때문에 3D Gaussian covariance matrix on world-coordinate을 2D Gaussian covariance matrix on image-coordinate (ray-space)으로 projection해야 하므로 non-linear 좌표계 변환이 필요하다. 이를 위해 EWA volume splatting에 따라 non-linear transformation을 Taylor approx.하여 local affine transformation으로서 Jacobian을 사용한다</li> <li>Q6 : camera origin과 pixel 중심을 잇는 ray가 image plane에 수직이 아닌 pixel의 경우 \(\hat r\) 과 \(d\) 를 어떻게 정의하지?</li> <li>A6 : \(d\) 는 camera origin부터 pixel 중심까지의 거리 vector이고,<br/> cone 단면의 \(\hat r\)은 \(d\) 와 수직인 방향으로 \(\frac{2}{\sqrt{12}}\) of pixel-width 이므로<br/> cone 단면이 image plane 위에 있지 않은 꼴이 됨</li> </ul> <h2 id="appendix">Appendix</h2> <p>B. The L Hyperparameter in PE and IPE 읽을 차례</p>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="nerf"/><category term="rendering"/><category term="3d"/><category term="multiscale"/><category term="antialiasing"/><summary type="html"><![CDATA[A Multiscale Representation for Anti-Aliasing Neural Radiance Fields]]></summary></entry><entry><title type="html">Vim, Pycharm Debug Shortcut</title><link href="https://semyeong-yu.github.io/blog/2024/vim/" rel="alternate" type="text/html" title="Vim, Pycharm Debug Shortcut"/><published>2024-08-01T11:00:00+00:00</published><updated>2024-08-01T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/vim</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/vim/"><![CDATA[<h2 id="vim">Vim</h2> <p>vimtutor : 튜토리얼<br/> vim 파일명 : 노말모드 실행</p> <h3 id="mode">Mode</h3> <ul> <li> <p>입력모드 :<br/> i : 노말모드 &gt; 입력모드 (커서 앞)<br/> I : 노말모드 &gt; 입력모드 (문장 시작)<br/> a : 노말모드 &gt; 입력모드 (커서 뒤)<br/> A : 노말모드 &gt; 입력모드 (문장 끝)</p> </li> <li> <p>노말모드 :<br/> esc : 입력모드 &gt; 노말모드</p> </li> <li> <p>명령모드 :<br/> ‘:’ : 노말모드 &gt; 명령모드</p> </li> </ul> <h3 id="입력모드">입력모드</h3> <p>파일 수정</p> <h3 id="명령모드">명령모드</h3> <p>:q : 종료<br/> :q! : 강제 종료<br/> :w : 저장<br/> :w! : 강제 저장<br/> :wq : 저장 후 종료<br/> :wq! : 강제 저장 후 종료</p> <h3 id="노말모드">노말모드</h3> <h4 id="navigation">Navigation</h4> <ul> <li> <p>커서 :<br/> hjkl : 커서 이동</p> </li> <li> <p>line :<br/> 0 : line 앞<br/> $ : line 뒤<br/> 20G : 20번째 line 앞</p> </li> <li> <p>문단 :<br/> { : 문단 시작<br/> } : 문단 끝</p> </li> <li> <p>단어 :<br/> w : 다음 단어 앞<br/> 3w : 3번째 다음 단어 앞<br/> e : 다음 단어 뒤<br/> b : 이전 단어 앞<br/> 3b : 3번째 이전 단어 앞</p> </li> <li> <p>화면, 파일 :<br/> H : 화면 위<br/> M : 화면 중간<br/> L : 화면 끝<br/> gg : 파일 앞<br/> G : 파일 끝</p> </li> <li> <p>스크롤링 :<br/> Ctrl+u : 위로 스크롤링<br/> Ctrl+d : 아래로 스크롤링</p> </li> </ul> <h4 id="비주얼선택-잘라내기-복사-붙여넣기">비주얼(선택), 잘라내기, 복사, 붙여넣기</h4> <ul> <li> <p>비주얼(선택) 모드 :<br/> v : 비주얼(선택) 모드<br/> Ctrl+v : 블럭 단위 비주얼(선택) 모드<br/> v + hjkl : 드래그 선택<br/> v aw : 단어 1개 선택</p> </li> <li> <p>잘라내기 :<br/> x : 글자 잘라내기<br/> dd : line 잘라내기</p> </li> <li> <p>복사 :<br/> y : 복사<br/> yy : line 복사</p> </li> <li> <p>붙여넣기 :<br/> p : 붙여넣기<br/> “p 혹은 *p : 클립보드 붙여넣기</p> </li> </ul> <h4 id="반복-되감기-앞감기">반복, 되감기, 앞감기</h4> <ul> <li>. : 이전 명령 반복</li> <li>u : undo (되돌리기)</li> <li>Ctrl+r : redo</li> </ul> <h4 id="command--object-조합">Command + Object 조합</h4> <ul> <li> <p>예시 :<br/> d 3w : 다음 단어 3개 잘라내기<br/> d 2j : 아래 2줄 잘라내기<br/> c i[ : 대괄호 안에 있는 것을 변경</p> </li> <li> <p>Command :<br/> d : 잘라내기 (delete)<br/> y : 복사 (yank)<br/> c : 변경 (change)<br/> v : 선택 (visual)<br/> Ctrl+v : 블럭 단위 선택</p> </li> <li> <p>Object :<br/> 3w : 다음 단어 3개<br/> 3b : 이전 단어 3개<br/> aw : 단어 1개<br/> ap : 문단 1개<br/> as : line 1개<br/> i” : “ “ 안에 있는 것<br/> ip : 문단 안에 있는 것<br/> i{ : 중괄호 안에 있는 것<br/> i( : 소괄호 안에 있는 것<br/> a( : 소괄호 포함 모든 것<br/> a[ : 대괄포 포함 모든 것<br/> f( : 현재부터 소괄호(포함)까지<br/> t( : 현재부터 소괄호(미포함)까지<br/> /abc : 현재부터 abc(미포함)까지 (드래그 표시로 확인 가능)</p> </li> </ul> <h4 id="검색">검색</h4> <ul> <li>/<단어> : <단어> 검색 후 n 누르면 밑으로 계속 검색</단어></단어></li> <li>?<단어> : <단어> 검색 후 n 누르면 위로 계속 검색</단어></단어></li> <li>n : 계속 검색</li> </ul> <h2 id="pycharm-debug">Pycharm Debug</h2> <ul> <li> <p>실행 :<br/> Ctrl+F5 : 그냥 실행<br/> F9 : break point 설정<br/> F5 또는 우상단 벌레 버튼 : 디버깅 모드 실행 (첫 번째 break point 직전에서 멈춤)</p> </li> <li> <p>디버깅 모드 :<br/> F10 : 코드 한 줄 실행<br/> F11 : 함수 안으로 이동<br/> Shift+F11 : 함수 밖(호출 위치)로 이동<br/> F5 : 다음 breakpoint 직전에서 멈춤<br/> Shift+F5 또는 우상단 정지 버튼 : 디버깅 모드 해제</p> </li> </ul>]]></content><author><name></name></author><category term="cv-tasks"/><category term="vim"/><category term="pycharm"/><category term="debug"/><summary type="html"><![CDATA[vim, pycharm debug shortcut]]></summary></entry><entry><title type="html">Structure-from-Motion Revisited (COLMAP)</title><link href="https://semyeong-yu.github.io/blog/2024/Colmap/" rel="alternate" type="text/html" title="Structure-from-Motion Revisited (COLMAP)"/><published>2024-07-31T11:00:00+00:00</published><updated>2024-07-31T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Colmap</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Colmap/"><![CDATA[<h2 id="structure-from-motion-revisited">Structure-from-Motion Revisited</h2> <h4 id="johannes-l-schonberger-jan-michael-frahm">Johannes L. Schonberger, Jan-Michael Frahm</h4> <blockquote> <p>paper :<br/> <a href="chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://openaccess.thecvf.com/content_cvpr_2016/papers/Schonberger_Structure-From-Motion_Revisited_CVPR_2016_paper.pdf">chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://openaccess.thecvf.com/content_cvpr_2016/papers/Schonberger_Structure-From-Motion_Revisited_CVPR_2016_paper.pdf</a><br/> referenced blog :<br/> <a href="https://xoft.tistory.com/88">https://xoft.tistory.com/88</a></p> </blockquote> <h3 id="colmap">COLMAP</h3> <ul> <li>COLMAP :<br/> SfM (Structure from Motion)과 MVS (Multi-View Stereo)를 수행하는 library</li> <li><code class="language-plaintext highlighter-rouge">SfM</code> : <ul> <li>input : images</li> <li>output : camera parameter(intrinsic, extrinsic), 3D point cloud</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">MVS</code> : <ul> <li>input : SfM의 output</li> <li>output : 3D model reconstruction</li> </ul> </li> </ul> <h3 id="sfm-history">SfM History</h3> <ul> <li> <p>SfM 시초 :<br/> “3d model acquisition from extended image sequences”, 1996.<br/> “Structure from motion without correspondence”, CVPR, 2000.<br/> “Automatic camera recovery for closed or open image sequences”, ECCV, 1998.<br/> “Relative 3d reconstruction using multiple uncalibrated images”, IJR, 1995.<br/> “Visual modeling with a hand-held camera”, IJCV, 2004</p> </li> <li> <p>internet images로 3D reconstruction 수행 :<br/> “Multi-view matching for unordered image sets, or How do I organize my holiday snaps?”, ECCV, 2002.<br/> “Photo tourism: exploring photo collections in 3d”, ACM TOG, 2006.<br/> “Detailed real-time urban 3d reconstruction from video”, IJCV, 2008.</p> </li> <li>input images 수 늘리는 연구 : <ul> <li>수천장 처리 : “Building rome in a day”, ICCV, 2009.</li> <li>수백만장 처리 : “Building Rome on a Cloudless Day”, ECCV, 2010.<br/> “Towards linear-time incremental structure from motion”, 3DV, 2013.<br/> “From Single Image Query to Detailed 3D Reconstruction”, CVPR, 2015.<br/> “From Dusk Till Dawn: Modeling in the Dark”, CVPR, 2016.</li> <li>수억장 처리 : “Reconstructing the World* in Six Days *(As Captured by the Yahoo 100 Million Image Dataset)”, CVPR, 2015.</li> </ul> </li> <li>SfM 연구 발전 방향 :<br/> incremental, hierarchical, global 으로 총 3가지 발전 방향이 있었고,<br/> 그 중 images를 sequentially 처리하는 incremental SfM이 가장 인기 있었지만<br/> robustness, accuracy, completeness, scalability 관점에서 general SfM을 만들기 어려웠음<br/> \(\rightarrow\) 근데 이를 본 논문 (COLMAP)에서 해결!!</li> </ul> <h3 id="incremental-sfm">Incremental SfM</h3> <ul> <li>images</li> <li>correspondence search <ul> <li><code class="language-plaintext highlighter-rouge">feature extraction</code> :<br/> image마다 geometric-radiometric-invariant feature 추출<br/> e.g. SIFT <a href="https://velog.io/@everyman123/SIFT-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0">link</a> (scale-rotation-invariant feature extraction)<br/> e.g. SURF, HOG, BRIEF, ORB 등 <a href="https://ggommappooh.tistory.com/entry/%EC%BB%B4%ED%93%A8%ED%84%B0-%EB%B9%84%EC%A0%84-Feature-Descriptors">link</a></li> <li><code class="language-plaintext highlighter-rouge">matching</code> :<br/> extracted feature를 바탕으로 2 images에서 같은 scene part 찾기<br/> image마다 feature를 비교하므로 time complexity \(O(N_I^2 N_F^2)\)<br/> e.g. “Building Rome in a day”, ICCV, 2009.<br/> e.g. “Building Rome on a Cloudless Day”, ECCV, 2010.<br/> e.g. “Vocmatch: Efficient multiview correspondence for structure from motion”, ECCV, 2014.<br/> e.g. “Reconstructing the World in Six Days (As Captured by the Yahoo 100 Million Image Dataset)”, CVPR, 2015.<br/> e.g. “MatchMiner: Efficient Spanning Structure Mining in Large Image Collections”, ECCV, 2012.<br/> e.g. “PAIGE: PAirwise Image Geometry Encoding for Improved Efficiency in Structure-from-Motion”, CVPR, 2015.<br/> e.g. “Towards linear-time incremental structure from motion”, 3DV, 2013.<br/> e..g CNN-based SuperGlue(2019)<br/> e.g. ViT-based LoFTR(2021)</li> <li><code class="language-plaintext highlighter-rouge">geometric verification</code> : <ul> <li>matching 결과를 보장하기 위한 검증 과정 (걸러냄)</li> <li>GRIC과 같은 기법을 통해 어떤 geometry model로 검증할 지 결정<br/> geometry model 예시 : Fundametal Matrix, Trifocal Tensor, Projective Matrix, Calibration Matrix, Rigid Transformation, Affine Transformation</li> <li>Epipolar Geometry로 검증하는 예시 : Fundamental Matrix로 relative camera pose를 추정한 뒤 camera1의 feature points를 camera2로 projection 했을 때 matching points로 잘 mapping 되는지 검증<br/> Fundamental Matrix 계산하기 위해 eight-point, five-point, RANSAC, LMedS, QDEGSAC 등의 기법 사용 가능</li> <li>output : scene graph (verified image pair 및 correpondence map)</li> </ul> </li> </ul> </li> <li>incremental reconstruction <ul> <li><code class="language-plaintext highlighter-rouge">initialization</code> :<br/> 최초로 등록한 2개의 images 선택 (매우 중요한 단계임)<br/> 여러 camera로부터 많이 overlap되는 scene을 가진 images로 선택하면, Bundle Adjustment 단계에서 overlap part가 반복적으로 최적화되면서 reconstruction 성능이 높아지지만 연산 시간도 늘어남</li> <li><code class="language-plaintext highlighter-rouge">image registration</code> : <ul> <li>camera pose를 world-coordinate에 등록한다</li> <li>initial 2 images의 경우 : fundamental matrix를 알고 있으므로 intrinsic param.와 extrinsic param. (camera pose)을 추정할 수 있다</li> <li>이후 images의 경우 : 이미 등록된 image들과의 feature correspondence와 PnP 알고리즘을 사용하여 intrinsic param.와 extrinsic param.를 추정할 수 있다</li> <li>PnP 알고리즘 : 3D points 위치와 projected 2D points 위치를 기반으로 camera pose를 추정</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">triangulation</code> :<br/> ddd</li> <li><code class="language-plaintext highlighter-rouge">bundle adjustment</code> :<br/> ddd</li> <li><code class="language-plaintext highlighter-rouge">outlier filtering</code> :<br/> RANSAC 및 minimal pose solver 등의 기법을 사용하여 outlier를 걸러냄<br/> RANSAC : dataset의 randomly selected points에 대해 Fundamental Matrix 등 geometry model을 추정한 뒤 해당 model이 dataset에 얼마나 잘 부합하는지를 반복적으로 검증</li> </ul> </li> <li>3D reconstruction</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="SfM"/><category term="pose"/><category term="3d"/><summary type="html"><![CDATA[SfM library]]></summary></entry><entry><title type="html">Normalized Device Coordinates</title><link href="https://semyeong-yu.github.io/blog/2024/NDC/" rel="alternate" type="text/html" title="Normalized Device Coordinates"/><published>2024-07-30T15:00:00+00:00</published><updated>2024-07-30T15:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/NDC</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/NDC/"><![CDATA[<h2 id="ndc-normalized-device-coordinates">NDC: Normalized Device Coordinates</h2> <blockquote> <p>referenced blog :<br/> <a href="https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#background">https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#background</a></p> </blockquote> <h3 id="motivation">Motivation</h3> <p>NeRF에서<br/> MLP의 input은 3D world-coordinate이고,<br/> MLP의 output인 \(c, \sigma\) 를 accumulate해서 2D pixel-coordinate을 채운다<br/> 이 때, LLFF (Local Light Field Fusion) dataset 에 있는<br/> <code class="language-plaintext highlighter-rouge">unbounded (in single direction) 3D world-coordinate</code>의 scene 정보를<br/> <code class="language-plaintext highlighter-rouge">bounded 3D NDC space</code>로 project하면<br/> <code class="language-plaintext highlighter-rouge">MLP를 효율적으로 쓸 수 있다</code><br/> NDC space로의 projection 과정을 수식적으로 알아보고자 한다.</p> <h3 id="from-world-coordinate-to-ndc-to-pixel-coordinate">From world-coordinate To NDC To pixel-coordinate</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-30-NDC/1-480.webp 480w,/assets/img/2024-07-30-NDC/1-800.webp 800w,/assets/img/2024-07-30-NDC/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-30-NDC/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>camera transformation : <ul> <li><code class="language-plaintext highlighter-rouge">3D world-coordinate</code> (canonical-coordinate)<br/> \(\rightarrow\)<br/> <code class="language-plaintext highlighter-rouge">3D camera-coordinate</code></li> <li>extrinsic matrix \(\begin{bmatrix} R &amp; t \\ 0 &amp; 1 \end{bmatrix}\)</li> </ul> </li> <li>projection transformation : <ul> <li><code class="language-plaintext highlighter-rouge">3D camera-coordinate</code><br/> \(\rightarrow\)<br/> <code class="language-plaintext highlighter-rouge">3D NDC (normalized-device-coordinate)</code> (canonical view volume)</li> <li>normalized-device-coordinate (NDC) :<br/> <code class="language-plaintext highlighter-rouge">camera 원점이 중앙에 있는</code> \([-1, 1]^3\) cube (<code class="language-plaintext highlighter-rouge">정육면체</code>)</li> <li>frustum \(\rightarrow\) 직육면체 \(\rightarrow\) 정육면체<br/> consists of perspective projection and then orthographic projection<br/> z-axis 방향 바꾸기 포함</li> </ul> </li> <li>viewport transformation : <ul> <li><code class="language-plaintext highlighter-rouge">3D NDC</code><br/> \(\rightarrow\)<br/> <code class="language-plaintext highlighter-rouge">2D pixel-coordinate</code></li> <li>\([-1, 1]^3\) 의 NDC를 flatten하여 2 \(\times\) 2 square를 raster image로 mapping</li> <li>intrinsic matrix \(\begin{bmatrix} f_x &amp; s &amp; W/2 \\ 0 &amp; f_y &amp; H/2 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\) (초점거리 곱하고 원점 좌상단 이동)<br/> y-axis 방향 바꾸기 포함</li> </ul> </li> </ul> <h3 id="projection-transformation">Projection Transformation</h3> <blockquote> <p>Step 1. <code class="language-plaintext highlighter-rouge">Perspective Projection</code></p> </blockquote> <ul> <li>frustum을 bounded cuboid로 변환<br/> bound :<br/> \(x \in [l, r]\) where \(l \lt 0\), \(r \gt 0\)<br/> \(y \in [b, t]\) where \(b \lt 0\), \(t \gt 0\)<br/> \(z \in [f, n]\) where \(f \lt 0\), \(n \lt 0\)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-30-NDC/2-480.webp 480w,/assets/img/2024-07-30-NDC/2-800.webp 800w,/assets/img/2024-07-30-NDC/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-30-NDC/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-30-NDC/3-480.webp 480w,/assets/img/2024-07-30-NDC/3-800.webp 800w,/assets/img/2024-07-30-NDC/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-30-NDC/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 3D camera-coordinate </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-30-NDC/4-480.webp 480w,/assets/img/2024-07-30-NDC/4-800.webp 800w,/assets/img/2024-07-30-NDC/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-30-NDC/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>\(z = n\) plane은 그대로 냅두고, 직육면체 꼴이 되도록 그 뒤 plane 변환<br/> camera를 통과하는 any line은 z-axis에 평행한 line이 됨</p> </li> <li> <p>perspective projection matrix :<br/> \(P_{per} = \begin{bmatrix} n &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; n &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; n+f &amp; -nf \\ 0 &amp; 0 &amp; 1 &amp; 0 \end{bmatrix}\)<br/> \(P_{per} \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix} = \begin{bmatrix} nX \\ nY \\ (n+f)Z - nf \\ Z \end{bmatrix}\)<br/> <code class="language-plaintext highlighter-rouge">?????</code></p> </li> </ul> <blockquote> <p>Step 2. <code class="language-plaintext highlighter-rouge">Orthographic Projection</code></p> </blockquote> <ul> <li> <p>corner (l, b, n)이 원점이 되도록 shift한 뒤,<br/> \([0, r-l] \times [0, t-b] \times [f-n, 0]\) 의 직육면체를 \([0, 2] \times [0, 2] \times [-2, 0]\) 의 정육면체로 scale한 뒤,<br/> center (1, 1, -1)이 원점이 되도록 \([-1, 1]^3\) 으로 shift</p> </li> <li> <p>orthographic projection matrix :<br/> \(M_{orth} = \begin{pmatrix} I_{3 \times 3} &amp; \begin{matrix} -1 \\ -1 \\ 1 \end{matrix} \\ 0_{1 \times 3} &amp; 1 \end{pmatrix} \begin{pmatrix} \begin{matrix} \frac{2}{r-l} &amp; 0 &amp; 0 \\ 0 &amp; \frac{2}{t-b} &amp; 0 \\ 0 &amp; 0 &amp; \frac{2}{n-f} \end{matrix} &amp; 0_{3 \times 1} \\ 0_{1 \times 3} &amp; 1 \end{pmatrix} \begin{pmatrix} I_{3 \times 3} &amp; \begin{matrix} -l \\ -b \\ -n \end{matrix} \\ 0_{1 \times 3} &amp; 1 \end{pmatrix}\)<br/> \(= \begin{bmatrix} \frac{2}{r-l} &amp; 0 &amp; 0 &amp; -\frac{r+l}{r-l} \\ 0 &amp; \frac{2}{t-b} &amp; 0 &amp; -\frac{t+b}{t-b} \\ 0 &amp; 0 &amp; \frac{2}{n-f} &amp; -\frac{n+f}{n-f} \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix}\)</p> </li> </ul> <blockquote> <p>Step 3. <code class="language-plaintext highlighter-rouge">Projection Matrix</code></p> </blockquote> <p>Since perspective projection matrix is scalable,<br/> \(M_{proj} = M_{orth} (- P_{per})\)<br/> \(= \begin{bmatrix} \frac{2}{r-l} &amp; 0 &amp; 0 &amp; -\frac{r+l}{r-l} \\ 0 &amp; \frac{2}{t-b} &amp; 0 &amp; -\frac{t+b}{t-b} \\ 0 &amp; 0 &amp; \frac{2}{n-f} &amp; -\frac{n+f}{n-f} \\ 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix} \begin{bmatrix} -n &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; -n &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; -n-f &amp; nf \\ 0 &amp; 0 &amp; -1 &amp; 0 \end{bmatrix}\)<br/> \(= \begin{bmatrix} -\frac{2n}{r-l} &amp; 0 &amp; \frac{r+l}{r-l} &amp; 0 \\ 0 &amp; -\frac{2n}{t-b} &amp; \frac{t+b}{t-b} &amp; 0 \\ 0 &amp; 0 &amp; -\frac{n+f}{n-f} &amp; \frac{2nf}{n-f} \\ 0 &amp; 0 &amp; -1 &amp; 0 \end{bmatrix}\)</p> <p>camera-coordinate에서 \(z \in [f, n]\) where \(f \lt 0\), \(n \lt 0\) 이었는데,<br/> NDC에서는 z-axis의 방향이 반대이므로<br/> \(f \lt 0\), \(n \lt 0\) 대신 \(f = -f \gt 0\), \(n = -n \gt 0\) 를 대입하면,<br/> \(M_{proj} = \begin{bmatrix} \frac{2n}{r-l} &amp; 0 &amp; \frac{r+l}{r-l} &amp; 0 \\ 0 &amp; \frac{2n}{t-b} &amp; \frac{t+b}{t-b} &amp; 0 \\ 0 &amp; 0 &amp; -\frac{n+f}{n-f} &amp; -\frac{2nf}{n-f} \\ 0 &amp; 0 &amp; -1 &amp; 0 \end{bmatrix}\)</p> <p>OpenGL과 같은 graphics frameworks에서는 보통<br/> \(M_{proj}X\) 를 \(M_{proj}X\) 의 fourth entry로 나눴을 때 \(M_{proj}X\) 의 Z 값이 양수가 되도록 하기 때문에 (아래 Step 4의 NDC 참고)<br/> 조금 수정하면<br/> 최종적인 projection matrix는<br/> \(M_{proj} = \begin{bmatrix} \frac{2n}{r-l} &amp; 0 &amp; \frac{r+l}{r-l} &amp; 0 \\ 0 &amp; \frac{2n}{t-b} &amp; \frac{t+b}{t-b} &amp; 0 \\ 0 &amp; 0 &amp; -\frac{n+f}{f-n} &amp; -\frac{2nf}{f-n} \\ 0 &amp; 0 &amp; -1 &amp; 0 \end{bmatrix}\)</p> <p>camera frustum은 보통 symmetric하므로 \(l = -r\), \(b = -t\) 라 했을 때<br/> projection matrix는<br/> \(M_{proj} = \begin{bmatrix} \frac{n}{r} &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; \frac{n}{t} &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; -\frac{f+n}{f-n} &amp; -\frac{2nf}{f-n} \\ 0 &amp; 0 &amp; -1 &amp; 0 \end{bmatrix}\)</p> <blockquote> <p>Step 4. from <code class="language-plaintext highlighter-rouge">camera-coordinate</code> to <code class="language-plaintext highlighter-rouge">NDC</code></p> </blockquote> <ul> <li> <p>camera-coordinate :<br/> \(\boldsymbol X = \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}\)</p> </li> <li> <p>NDC :<br/> \(\begin{bmatrix} -\frac{n}{r}\frac{X}{Z} \\ -\frac{n}{t}\frac{Y}{Z} \\ \frac{f+n}{f-n} + \frac{2nf}{f-n}\frac{1}{Z} \\ 1 \end{bmatrix} = \boldsymbol x \sim M_{proj} \boldsymbol X = \begin{bmatrix} \frac{n}{r}X \\ \frac{n}{t}Y \\ -\frac{f+n}{f-n}Z -\frac{2nf}{f-n} \\ -Z \end{bmatrix}\)<br/> Let \(a_x = -\frac{n}{r}\)<br/> \(a_y = -\frac{n}{t}\)<br/> \(a_z = \frac{f+n}{f-n}\)<br/> \(b_z = \frac{2nf}{f-n}\)<br/> Then \(\boldsymbol x = \begin{bmatrix} a_x\frac{X}{Z} \\ a_y\frac{Y}{Z} \\ a_z + \frac{b_z}{Z} \\ 1 \end{bmatrix} = \begin{bmatrix} a_x\frac{X}{Z} \\ a_y\frac{Y}{Z} \\ a_z + \frac{b_z}{Z} \end{bmatrix}\)</p> </li> </ul> <h3 id="projection-in-nerf-ray">Projection in NeRF ray</h3> <p>any 3D points on ray \(r = o + td\)를<br/> NDC space (camera 원점이 중앙에 있는 \([-1, 1]^3\) cube) 로 projection하면<br/> 3D points on projected ray \(r^{\ast} = o^{\ast} + t^{\ast} d^{\ast}\) 가 된다<br/> 위에서 유도한 Projection Matrix 를 사용하면<br/> \(\boldsymbol x = \begin{bmatrix} a_x\frac{o_x + td_x}{o_z + td_z} \\ a_y\frac{o_y + td_y}{o_z + td_z} \\ a_z + \frac{b_z}{o_z + td_z} \end{bmatrix} = \begin{bmatrix} o_x^{\ast} + t^{\ast} d_x^{\ast} \\ o_y^{\ast} + t^{\ast} d_y^{\ast} \\ o_z^{\ast} + t^{\ast} d_z^{\ast} \end{bmatrix}\)</p> <p>먼저 projected 원점 좌표를 구해보자<br/> \(t = t^{\ast} = 0\) 를 대입하면<br/> \(o^{\ast} = \begin{bmatrix} o_x^{\ast} \\ o_y^{\ast} \\ o_z^{\ast} \end{bmatrix} = \begin{bmatrix} a_x\frac{o_x}{o_z} \\ a_y\frac{o_y}{o_z} \\ a_z + \frac{b_z}{o_z} \end{bmatrix}\)</p> <p>다음으로 projected t와 d를 구해보자<br/> \(\begin{bmatrix} t^{\ast} d_x^{\ast} \\ t^{\ast} d_y^{\ast} \\ t^{\ast} d_z^{\ast} \end{bmatrix} = \begin{bmatrix} a_x\frac{o_x + td_x}{o_z + td_z} \\ a_y\frac{o_y + td_y}{o_z + td_z} \\ a_z + \frac{b_z}{o_z + td_z} \end{bmatrix} - \begin{bmatrix} o_x^{\ast} \\ o_y^{\ast} \\ o_z^{\ast} \end{bmatrix}\)<br/> \(= \begin{bmatrix} a_x\frac{o_x + td_x}{o_z + td_z} - a_x\frac{o_x}{o_z} \\ a_y\frac{o_y + td_y}{o_z + td_z} - a_y\frac{o_y}{o_z} \\ a_z + \frac{b_z}{o_z + td_z} - (a_z + \frac{b_z}{o_z}) \end{bmatrix}\)<br/> \(= \begin{bmatrix} a_x\frac{td_z}{o_z + td_z}(\frac{d_x}{d_z} - \frac{o_x}{o_z}) \\ a_y\frac{td_z}{o_z + td_z}(\frac{d_y}{d_z} - \frac{o_y}{o_z}) \\ -b_z\frac{td_z}{o_z + td_z}\frac{1}{o_z} \end{bmatrix}\)<br/> \(= \frac{td_z}{o_z + td_z} \begin{bmatrix} a_x(\frac{d_x}{d_z} - \frac{o_x}{o_z}) \\ a_y(\frac{d_y}{d_z} - \frac{o_y}{o_z}) \\ -b_z\frac{1}{o_z} \end{bmatrix}\)</p> <blockquote> <p>Result</p> </blockquote> <p>ray \(r = o + td\)를<br/> NDC space (camera 원점이 중앙에 있는 \([-1, 1]^3\) cube) 로 projection 했을 때<br/> projected ray \(r^{\ast} = o^{\ast} + t^{\ast} d^{\ast}\) 는 아래와 같이 구할 수 있다</p> <p>\(o^{\ast} = \begin{bmatrix} o_x^{\ast} \\ o_y^{\ast} \\ o_z^{\ast} \end{bmatrix} = \begin{bmatrix} a_x\frac{o_x}{o_z} \\ a_y\frac{o_y}{o_z} \\ a_z + \frac{b_z}{o_z} \end{bmatrix}\)<br/> and<br/> \(t^{\ast} = \frac{td_z}{o_z + td_z} = 1 - \frac{o_z}{o_z + td_z}\)<br/> and<br/> \(d^{\ast} = \begin{bmatrix} d_x^{\ast} \\ d_y^{\ast} \\ d_z^{\ast} \end{bmatrix} = \begin{bmatrix} a_x(\frac{d_x}{d_z} - \frac{o_x}{o_z}) \\ a_y(\frac{d_y}{d_z} - \frac{o_y}{o_z}) \\ -b_z\frac{1}{o_z} \end{bmatrix}\)</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">Ray Projection to NDC 장점</code></p> </blockquote> <p>ray에서 \(t \in [0, \infty)\) 였다면 projected ray에서 \(t^{\ast} \in [0, 1)\)</p> <p>LLFF dataset에서<br/> camera에서 출발한 ray가 아무 object도 “hit”하지 않는다면 \(t = \infty\)일텐데,<br/> NDC (bounded cube)로 warp한다면 \(t^{\ast} \in [0, 1)\) 이므로<br/> MLP 효율적으로 쓸 수 있음</p> <blockquote> <p>Projection transformation 한계</p> </blockquote> <p>LLFF dataset과 같이 <code class="language-plaintext highlighter-rouge">single</code> direction으로만 unbounded된 camera frustum, 즉 <code class="language-plaintext highlighter-rouge">front-facing scene</code>에 대해서만 적용 가능하고<br/> unbounded 360 scene에 대해서는 기본 NeRF가 잘 수행 못함<br/> \(\rightarrow\) MipNeRF360 등 NeRF 후속 연구에서 해결됨</p> <blockquote> <p>특정 case</p> </blockquote> <p>\(f_{cam}\)이 camera의 focal length이고,<br/> \(W, H\)가 image plane의 width, height in pix 일 때<br/> <code class="language-plaintext highlighter-rouge">image plane이 정확히 camera frustum의 near plane</code>에 있고<br/> <code class="language-plaintext highlighter-rouge">camera frustum의 far plane을 infinity로 확장</code>하도록<br/> camera를 설정하면,<br/> \(z = -n = -f_{cam} \lt 0\), \(r = \frac{W}{2}\), \(t = \frac{H}{2}\), \(z = -f \rightarrow -\infty\) 이므로</p> <p>\(a_x = -\frac{n}{r} = -\frac{f_{cam}}{\frac{W}{2}}\)<br/> \(a_y = -\frac{n}{t} = -\frac{f_{cam}}{\frac{H}{2}}\)<br/> \(\lim_{f \rightarrow \infty} a_z = \lim_{f \rightarrow \infty} \frac{f+n}{f-n} = 1\)<br/> \(\lim_{f \rightarrow \infty} b_z = \lim_{f \rightarrow \infty} \frac{2nf}{f-n} = 2n\)<br/> 이므로</p> <p>ray \(r = o + td\) 를 NDC로 projection했을 때<br/> projected ray \(r^{\ast} = o^{\ast} + t^{\ast} d^{\ast}\) 에서<br/> \(o^{\ast} = \begin{bmatrix} -\frac{f_{cam}}{\frac{W}{2}}\frac{o_x}{o_z} \\ -\frac{f_{cam}}{\frac{H}{2}}\frac{o_y}{o_z} \\ 1 + \frac{2n}{o_z} \end{bmatrix}\)<br/> and<br/> \(t^{\ast} = \frac{td_z}{o_z + td_z} = 1 - \frac{o_z}{o_z + td_z}\)<br/> and<br/> \(d^{\ast} = \begin{bmatrix} -\frac{f_{cam}}{\frac{W}{2}}(\frac{d_x}{d_z} - \frac{o_x}{o_z}) \\ -\frac{f_{cam}}{\frac{H}{2}}(\frac{d_y}{d_z} - \frac{o_y}{o_z}) \\ -2n\frac{1}{o_z} \end{bmatrix}\)</p> <h3 id="ndc-projection-in-nerf-code">NDC projection in NeRF code</h3> <p><a href="https://github.com/yenchenlin/nerf-pytorch">NeRF-Pytorch</a> 기준으로<br/> run_nerf_helpers.py의 ndc_rays()에서 구현</p> <pre><code class="language-Python">def ndc_rays(H, W, focal, near, rays_o, rays_d):
    # shift ray origins to near plane
    t = -(near + rays_o[...,2]) / rays_d[...,2]
    rays_o = rays_o + t[...,None] * rays_d
    
    # projection
    o0 = -1. / (W / (2. * focal)) * rays_o[...,0] / rays_o[...,2]
    o1 = -1. / (H / (2. * focal)) * rays_o[...,1] / rays_o[...,2]
    o2 =  1. +  2. * near / rays_o[...,2]

    d0 = -1. / (W / (2. * focal)) * (rays_d[...,0]/rays_d[...,2] - rays_o[...,0]/rays_o[...,2])
    d1 = -1. / (H / (2. * focal)) * (rays_d[...,1]/rays_d[...,2] - rays_o[...,1]/rays_o[...,2])
    d2 = -2. * near / rays_o[...,2]
    
    rays_o = torch.stack([o0,o1,o2], -1)
    rays_d = torch.stack([d0,d1,d2], -1)
    
    return rays_o, rays_d
</code></pre>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="NDC"/><category term="3d"/><summary type="html"><![CDATA[How NDC Works for Ray]]></summary></entry><entry><title type="html">State Space Model</title><link href="https://semyeong-yu.github.io/blog/2024/SSM/" rel="alternate" type="text/html" title="State Space Model"/><published>2024-07-18T15:00:00+00:00</published><updated>2024-07-18T15:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/SSM</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/SSM/"><![CDATA[<h2 id="state-space-model">State Space Model</h2> <blockquote> <p>참고 논문 :<br/> <a href="https://arxiv.org/abs/2406.07887">https://arxiv.org/abs/2406.07887</a><br/> 참고 강연 :<br/> by NVIDIA Wonmin Byeon</p> </blockquote> <h3 id="abstract">Abstract</h3> <ul> <li>Large Language Models (LLMs) are usually based on <code class="language-plaintext highlighter-rouge">Transformer</code> architectures. <ul> <li>Transformer-based models 장점 :<br/> highly <code class="language-plaintext highlighter-rouge">parallelizable</code><br/> can model <code class="language-plaintext highlighter-rouge">massive amounts of data</code></li> <li>Transformer-based models 단점 :<br/> significant <code class="language-plaintext highlighter-rouge">computational overhead</code> due to the <code class="language-plaintext highlighter-rouge">quadratic self-attention</code> calculations, especially on longer sequences<br/> large inference-time <code class="language-plaintext highlighter-rouge">memory requirements</code> from the <code class="language-plaintext highlighter-rouge">key-value cache</code></li> </ul> </li> <li>More recently, <code class="language-plaintext highlighter-rouge">State Space Models (SSM)</code> like Mamba have been shown to have fast parallelizable training and inference as an alternative of Transformer.<br/> In this talk, I present the strengths and weaknesses of <code class="language-plaintext highlighter-rouge">Mamba, Mamba-2, and Transformer models</code> at larger scales. I also introduce a <code class="language-plaintext highlighter-rouge">hybrid architecture consisting of Mamba-2, attention, and MLP layers</code>.<br/> While pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks that require <code class="language-plaintext highlighter-rouge">strong copying</code> or <code class="language-plaintext highlighter-rouge">in-context learning</code> abilities.<br/> In contrast, the hybrid model closely matches or exceeds the Transformer on all standard and long-context tasks and is predicted to be up to 8x faster when generating tokens at inference time.</li> </ul> <h3 id="is-attention-all-we-need">Is Attention All We Need?</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/4-480.webp 480w,/assets/img/2024-07-18-SSM/4-800.webp 800w,/assets/img/2024-07-18-SSM/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Transformer : <ul> <li>fast training due to parallelization</li> <li>slow inference for long sequence(context) <ul> <li>key-value cache can improve speed, but increase GPU memory</li> </ul> </li> </ul> </li> <li>RNN : <ul> <li>slow training due to no parallelization</li> <li>fast inference because scale linearly with sequence length</li> </ul> </li> <li>Mamba : <ul> <li>fast training</li> <li>fast inference because scale linearly with sequence length and can deal with unbounded context</li> </ul> </li> <li> <p>SSM or RNN :<br/> state = fixed-sized vector (compression)<br/> high efficiency, but low performance</p> </li> <li>Transformer :<br/> cache of entire history (no compression)<br/> high performance, but low efficiency</li> </ul> <h3 id="mamba-linear-time-sequence-modeling-with-selective-state-spaces">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</h3> <ul> <li>SSM</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/1-480.webp 480w,/assets/img/2024-07-18-SSM/1-800.webp 800w,/assets/img/2024-07-18-SSM/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Selective SSM :<br/> matrix B, C and step size are dependent on the input</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/2-480.webp 480w,/assets/img/2024-07-18-SSM/2-800.webp 800w,/assets/img/2024-07-18-SSM/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Parallel scan :<br/> The order does not matter through the associative property, so can calculate sequences in part and iteratively combine them</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/3-480.webp 480w,/assets/img/2024-07-18-SSM/3-800.webp 800w,/assets/img/2024-07-18-SSM/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Hardware-aware implementation :<br/> minimize copying between RAMs</li> </ul> <h3 id="mamba-2">Mamba-2</h3> <ul> <li>Mamba에서 Main Bottleneck이 Parallel scan 부분이었는데,<br/> Mamba-2는 divide input into chunks 등 architecture 개선으로 이를 해결하고자 했음</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/13-480.webp 480w,/assets/img/2024-07-18-SSM/13-800.webp 800w,/assets/img/2024-07-18-SSM/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="limitations-of-mamba">Limitations of Mamba</h3> <ul> <li>Poor at MMLU and Phonebook task<br/> 아래를 요구하는 task에 대해서는 Mamba가 잘 못함 <ul> <li>in-context learning</li> <li>info. routing between tokens</li> <li>copying from the context (bad on long-context tasks)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/5-480.webp 480w,/assets/img/2024-07-18-SSM/5-800.webp 800w,/assets/img/2024-07-18-SSM/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/6-480.webp 480w,/assets/img/2024-07-18-SSM/6-800.webp 800w,/assets/img/2024-07-18-SSM/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="hybrid-architecture-of-mamba-and-transformer">Hybrid Architecture of Mamba and Transformer</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/7-480.webp 480w,/assets/img/2024-07-18-SSM/7-800.webp 800w,/assets/img/2024-07-18-SSM/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Our Hybrid Mamba-Transformer Model <ul> <li>Minimize the number of Attention Layers and Maximize the number of MLPs</li> <li>Does not necessarily need Rotary Position Embedding (RoPE)</li> <li>evenly spread attention and MLP layers</li> <li>Place Mamba layer at the beginning, so has no position embedding</li> <li>Group-Query Attention (GQA) makes more efficient</li> <li>Global Attention makes better performance</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/11-480.webp 480w,/assets/img/2024-07-18-SSM/11-800.webp 800w,/assets/img/2024-07-18-SSM/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Mamba-2 Hybrid<br/> Inference Speed is fast<br/> Now, states in Mamba can understand longer history!</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/12-480.webp 480w,/assets/img/2024-07-18-SSM/12-800.webp 800w,/assets/img/2024-07-18-SSM/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Attention Layer is bottleneck at Hybrid model,<br/> so Context Length가 길어질수록 Speedup 증가율은 줄어듬</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/8-480.webp 480w,/assets/img/2024-07-18-SSM/8-800.webp 800w,/assets/img/2024-07-18-SSM/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/9-480.webp 480w,/assets/img/2024-07-18-SSM/9-800.webp 800w,/assets/img/2024-07-18-SSM/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="summary">Summary</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-18-SSM/10-480.webp 480w,/assets/img/2024-07-18-SSM/10-800.webp 800w,/assets/img/2024-07-18-SSM/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-18-SSM/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 왼쪽부터 4K, 16K, 32K-based models </div> <p>Mamba-2 Hybrid는 Transformer와 달리 Quadratic calculation까지 필요 없고 inference 빠름<br/> but, Attention Layer가 Bottleneck이듯이 해결해야 할 사항들이 남아 있어 앞으로도 발전 가능성 있음</p>]]></content><author><name></name></author><category term="cv-tasks"/><category term="SSM"/><category term="Mamba"/><summary type="html"><![CDATA[SSM]]></summary></entry><entry><title type="html">3D Gaussian Splatting</title><link href="https://semyeong-yu.github.io/blog/2024/GS/" rel="alternate" type="text/html" title="3D Gaussian Splatting"/><published>2024-07-11T10:00:00+00:00</published><updated>2024-07-11T10:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/GS</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/GS/"><![CDATA[<h2 id="3d-gaussian-splatting-for-real-time-radiance-field-rendering">3D Gaussian Splatting for Real-Time Radiance Field Rendering</h2> <h4 id="bernhard-kerbl-georgios-kopanas-thomas-leimkühler-george-drettakis">Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, George Drettakis</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2308.04079">https://arxiv.org/abs/2308.04079</a><br/> project website :<br/> <a href="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/">https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/</a><br/> code :<br/> <a href="https://github.com/graphdeco-inria/gaussian-splatting">https://github.com/graphdeco-inria/gaussian-splatting</a><br/> referenced blog :<br/> <a href="https://xoft.tistory.com/51">https://xoft.tistory.com/51</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>DD</li> </ol> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/2-480.webp 480w,/assets/img/2024-07-11-GS/2-800.webp 800w,/assets/img/2024-07-11-GS/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="abstract">Abstract</h2> <ul> <li>novel 3D Gaussian scene representation with real-time differentiable renderer<br/> <code class="language-plaintext highlighter-rouge">수많은 3D Gaussian이 모여 scene을 구성</code>하고 있다!</li> <li>Very Fast rendering (\(\geq\) 100 FPS) :<br/> real-time as \(\geq\) 30 FPS<br/> rasterization이 optimization의 main bottleneck인데, 3DGS는 fast rasterization 가짐</li> <li>Higher Quality than SOTA Mip-NeRF360(2022)</li> <li>Faster Training than SOTA InstantNGP(2022)</li> </ul> <h2 id="introduction">Introduction</h2> <h3 id="why-3d-gaussian">Why 3D Gaussian?</h3> <p>3D scene representation 방법</p> <ol> <li><code class="language-plaintext highlighter-rouge">Mesh or Point</code> <ul> <li>explicit</li> <li>good for fast GPU/CUDA-based rasterization(3D \(\rightarrow\) 2D)</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">NeRF</code> method <ul> <li>implicit (MLP로 geometry 및 appearance를 표현)</li> <li>ray marching</li> <li>continuous coordinate-based representation</li> <li>interpolate values stored in voxels, hash grids, or points</li> <li>But,,, continuous ray로부터 discrete points를 뽑아 내는 <code class="language-plaintext highlighter-rouge">stochastic sampling</code> for rendering 때문에 <code class="language-plaintext highlighter-rouge">연산량이 많고 noise</code> 생김</li> <li>MLP는 dot product 및 더하기(kernel regression)의 특성상 <code class="language-plaintext highlighter-rouge">orthogonality</code>를 흐리기 때문에 high-freq. output을 잘 표현할 수 없어서 따로 미리 positional encoding을 수행</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">3D Gaussian</code> method <ul> <li>explicit (3D Gaussian으로 geometry를, SH coeff.로 appearance를 표현)</li> <li>differentiable volumetric representation</li> <li>efficient rasterization(projection and \(\alpha\)-blending)</li> <li>3D Gaussian(ellipsoid)이나 SH coeff.라는 explicit 표현 자체가 <code class="language-plaintext highlighter-rouge">orthogonality</code>를 잘 살리기 때문에 high-freq. output 잘 표현 가능</li> </ul> </li> </ol> <h3 id="rendering-nerf-vs-3dgs">Rendering (NeRF vs 3DGS)</h3> <ul> <li>NeRF : <ul> <li>ray per pixel 쏴서 coarse(stratified) and fine(PDF) sampling하고,</li> <li>MLP로 sampled points의 color 및 volume density를 구하고,</li> <li>이 값들을 volume rendering 식으로 summation</li> </ul> </li> <li>3DGS : <ul> <li>image를 tile(14 \(\times\) 14 pixel)들로 나누고,</li> <li>tile마다 Gaussian을 Depth에 따라 정렬한 뒤</li> <li>앞에서부터 뒤로 \(\alpha\)-blending</li> </ul> </li> </ul> <h2 id="related-work">Related Work</h2> <p>생략 (추후에 다시 볼 수도)</p> <h2 id="overview">Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/1-480.webp 480w,/assets/img/2024-07-11-GS/1-800.webp 800w,/assets/img/2024-07-11-GS/1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For unbounded and complete scenes,<br/> For 1080p high resolution and real-time(\(\geq\) 30 fps) rendering,</p> <ol> <li><code class="language-plaintext highlighter-rouge">input</code> : <ul> <li>Most point-based methods require <code class="language-plaintext highlighter-rouge">MVS</code>(Multi-View Stereo) data,<br/> but 3DGS only needs <code class="language-plaintext highlighter-rouge">SfM points</code> for initialization</li> <li>COLMAP 등 SfM(Structure-from-Motion) camera calibration으로 얻은 <code class="language-plaintext highlighter-rouge">sparse point cloud</code>에서 시작해서<br/> scene을 3D Gaussians로 나타냄으로써<br/> <code class="language-plaintext highlighter-rouge">empty space에서의 불필요한 계산을 하지 않도록</code> continuous volumetric radiance fields 정보를 저장</li> <li>NeRF-synthetic dataset의 경우 bg가 없어서 3DGS random initialization으로도 좋은 퀄리티 달성</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">optimization</code> interleaved with <code class="language-plaintext highlighter-rouge">adaptive density control</code> : <ul> <li>optimize 4 parameters :<br/> 3D position(mean), anisotropic covariance, opacity, and spherical harmonic coeff.(color)<br/> <code class="language-plaintext highlighter-rouge">highly anisotropic volumetric splats</code>는 <code class="language-plaintext highlighter-rouge">fine structures</code>를 compact하게 나타낼 수 있음!!<br/> <code class="language-plaintext highlighter-rouge">spherical harmonics</code>를 통해 <code class="language-plaintext highlighter-rouge">directional appearance(color)</code>를 잘 나타낼 수 있음!!<d-cite key="Plenoxels">[1]</d-cite>, <d-cite key="InstantNGP">[2]</d-cite></li> <li>adaptive density control :<br/> gradient 기반으로 Gaussian 형태를 변화시키기 위해, add and occasionally remove 3D Gaussians during optimization</li> </ul> </li> <li>differentiable visibility-aware <code class="language-plaintext highlighter-rouge">real-time rendering</code> :<br/> perform \(\alpha\)-blending of <code class="language-plaintext highlighter-rouge">anisotropic splats</code> respecting visibility order<br/> by fast <code class="language-plaintext highlighter-rouge">GPU sorting</code> algorithm and <code class="language-plaintext highlighter-rouge">tile-based rasterization</code>(projection and \(\alpha\)-blending)<br/> 한편, accumulated \(\alpha\) values를 tracking함으로써 <code class="language-plaintext highlighter-rouge">Gaussians 수에 제약 없이</code> 빠른 backward pass도 가능</li> </ol> <hr/> <h3 id="pseudo-code">Pseudo-Code</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/3-480.webp 480w,/assets/img/2024-07-11-GS/3-800.webp 800w,/assets/img/2024-07-11-GS/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>빨간 박스 : initialization<br/> 파란 박스 : optimization<br/> 초록 박스 : 특정 iter.마다 Gaussian을 clone, split, remove</p> <h2 id="differentiable-3d-gaussian-splatting">Differentiable 3D Gaussian Splatting</h2> <h3 id="3d-gaussian">3D Gaussian</h3> <ul> <li> <p><code class="language-plaintext highlighter-rouge">differentiable</code> volumetric representation의 특성을 가지고 있으면서도 빠른 rendering을 위해 <code class="language-plaintext highlighter-rouge">unstructured and explicit</code>한 게 무엇이 있을까?<br/> \(\rightarrow\) 3D Gaussian !!</p> </li> <li> <p>a point를 a small planar circle with a normal이라고 가정하는 이전 Point-based rendering 논문들 <d-cite key="Point1">[3]</d-cite> <d-cite key="Point2">[4]</d-cite> 과 달리<br/> <code class="language-plaintext highlighter-rouge">SfM points는 sparse해서 normals(법선)를 estimate하기 어려울</code> 뿐만 아니라, estimate 한다 해도 very noisy normals를 optimize하는 것은 매우 어렵<br/> \(\rightarrow\) normals 필요 없는 3D Gaussians !!<br/> k-dim. Gaussian : \(G(\boldsymbol x) = (2\pi)^{-\frac{k}{2}}det(\Sigma)^{-\frac{1}{2}}e^{-\frac{1}{2}(\boldsymbol x - \boldsymbol \mu)^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu)}\)</p> </li> </ul> <h2 id="parameters-to-train">Parameters to train</h2> <ol> <li><code class="language-plaintext highlighter-rouge">scale vector</code> \(s\) and <code class="language-plaintext highlighter-rouge">quaternion</code> \(q\) for <code class="language-plaintext highlighter-rouge">covariance matrix</code></li> <li><code class="language-plaintext highlighter-rouge">spherical harmonics</code>(SH) coeff. for <code class="language-plaintext highlighter-rouge">color</code></li> <li><code class="language-plaintext highlighter-rouge">opacity</code> \(\alpha\)</li> <li><code class="language-plaintext highlighter-rouge">3D position</code> for <code class="language-plaintext highlighter-rouge">mean</code></li> </ol> <h3 id="parameter-1-covariance-matrix">Parameter 1. Covariance matrix</h3> <blockquote> <p>covariance matrix and scale vector(scale) and quaternion(rotation)</p> </blockquote> <ul> <li>covariance matrix는 positive semi-definite \(x^T M x \geq 0\) for all \(x \in R^n\)이어야만 physical meaning을 가지는데,<br/> \(\Sigma\) 를 직접 바로 optimize하면 invalid covariance matrix가 될 수 있음<br/> 그렇다면!!</li> </ul> <p>\(\Sigma\) 가 <code class="language-plaintext highlighter-rouge">symmetric</code> and <code class="language-plaintext highlighter-rouge">positive semi-definite</code>이도록 \(\Sigma = R S S^T R^T\) 로 정의해서<br/> \(\Sigma\) 대신 <code class="language-plaintext highlighter-rouge">x,y,z-axis scale</code>을 나타내는 <code class="language-plaintext highlighter-rouge">3D vector</code> \(s\) 와 <code class="language-plaintext highlighter-rouge">rotation</code>을 나타내는 <code class="language-plaintext highlighter-rouge">4D quaternion</code> \(q\) 를 optimize 하자!!<br/> quaternion에 대한 설명은 <a href="https://semyeong-yu.github.io/blog/2024/Quaternion">Quaternion</a> 블로그 참고!!</p> <ul> <li><code class="language-plaintext highlighter-rouge">scale</code> 3D vector \(s\) <code class="language-plaintext highlighter-rouge">초기값</code> :<br/> <a href="https://github.com/graphdeco-inria/gaussian-splatting/blob/b2ada78a779ba0455dfdc2b718bdf1726b05a1b6/scene/gaussian_model.py#L134C1-L134C1">GaussianModel().create_from_pcd()</a><br/> SfM sparse point cloud의 각 점에 대해 가장 가까운 점 3개까지의 거리의 평균을 각 axis(\(x, y, z\))별로 구한 것을 3 \(\times\) 1 \(s\)라 할 때<br/> normalize 효과를 위해 log, sqrt 씌운 뒤<br/> 3 \(\times\) 1 \(log(\sqrt{s})\) 의 값을 3번 복사하여 3 \(\times\) 3 scale matrix \(S\)를 초기화 <pre><code class="language-Python">dist2 = torch.clamp_min(distCUDA2(torch.from_numpy(np.asarray(pcd.points)).float().cuda()), 0.0000001)
scales = torch.log(torch.sqrt(dist2))[...,None].repeat(1, 3)
</code></pre> </li> <li> <p><code class="language-plaintext highlighter-rouge">scale</code> 3D vector \(s\) <code class="language-plaintext highlighter-rouge">activation function</code> :<br/> smooth gradient 얻기 위해 exponential activation function을 씌움</p> </li> <li><code class="language-plaintext highlighter-rouge">quaternion</code> \(q\) <code class="language-plaintext highlighter-rouge">초기값</code> :<br/> 각 점에 대해 \(\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}\) 으로 quaternion을 초기화하고<br/> 이를 이용하여 rotation matrix \(R\) 초기화 <pre><code class="language-Python">rots = torch.zeros((fused_point_cloud.shape[0], 4), device="cuda")
rots[:, 0] = 1
</code></pre> </li> <li><code class="language-plaintext highlighter-rouge">anisotropic covariance</code>는 다양한 모양의 geometry를 나타내기 위해 optimize하기에 적합!</li> </ul> <blockquote> <p>param. gradient 직접 유도</p> </blockquote> <p>training할 때 automatic differentiation으로 인한 <code class="language-plaintext highlighter-rouge">overhead를 방지</code>하기 위해 <code class="language-plaintext highlighter-rouge">param. gradient를 직접 유도</code>함!</p> <p>Appendix A. <code class="language-plaintext highlighter-rouge">?????</code></p> <blockquote> <p>EWA volume splatting (2001) :<br/> world-to-camera 는 linear transformation 이지만,<br/> <code class="language-plaintext highlighter-rouge">camera-to-image (projection)</code> 는 <code class="language-plaintext highlighter-rouge">non-linear transformation</code> 이다!!</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/4-480.webp 480w,/assets/img/2024-07-11-GS/4-800.webp 800w,/assets/img/2024-07-11-GS/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 위 그림 : camera coordinate / 아래 그림 : image coordinate (ray space) </div> <ul> <li><code class="language-plaintext highlighter-rouge">world</code> coordinate (3D) : <ul> <li> \[\boldsymbol u = \begin{bmatrix} u_0 \\ u_1 \\ u_2 \end{bmatrix}\] </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">camera</code> coordinate (3D) : <ul> <li>\(\boldsymbol t = \begin{bmatrix} t_0 \\ t_1 \\ t_2 \end{bmatrix}\)<br/> \(= W \boldsymbol u + d\)<br/> where \(W\) : <code class="language-plaintext highlighter-rouge">viewing transformation</code> affine matrix from world coordinate to camera coordinate</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">image</code> coordinate (2D) : <ul> <li>\(\boldsymbol x = \begin{bmatrix} x_0 \\ x_1 \\ x_2 \end{bmatrix}\)<br/> \(= \phi(\boldsymbol t) = \begin{bmatrix} \frac{t_0}{t_2} \\ \frac{t_1}{t_2} \\ \| (t_0, t_1, t_2)^T \| \end{bmatrix}\)</li> <li>function \(\phi\) 는 non-linear하므로 Affine transformation이 불가능하다.</li> <li><code class="language-plaintext highlighter-rouge">Local Affine (Linear) transform으로 Approx.</code>하기 위해 \(\boldsymbol t = \boldsymbol t_{k}\) 에서의 <code class="language-plaintext highlighter-rouge">Taylor Approx.</code>를 이용하면,<br/> \(\phi_{k}(\boldsymbol t) = \phi(\boldsymbol t_{k}) + \boldsymbol J_{k} \cdot (\boldsymbol t - \boldsymbol t_{k})\)<br/> where<br/> \(\boldsymbol J_{k} = \frac{d\phi}{d \boldsymbol t}(\boldsymbol t_{k}) = \begin{bmatrix} \frac{d\phi}{d \boldsymbol t_{0}}(\boldsymbol t_{k}) &amp; \frac{d\phi}{d \boldsymbol t_{1}}(\boldsymbol t_{k}) &amp; \frac{d\phi}{d \boldsymbol t_{2}}(\boldsymbol t_{k}) \end{bmatrix} = \begin{bmatrix} \frac{1}{t_{k, 2}} &amp; 0 &amp; -\frac{t_{k, 0}}{t_{k, 2}^2} \\ 0 &amp; \frac{1}{t_{k, 2}} &amp; -\frac{t_{k, 1}}{t_{k, 2}^2} \\ \frac{t_{k, 0}}{l} &amp; \frac{t_{k, 1}}{l} &amp; \frac{t_{k, 2}}{l} \end{bmatrix}\)<br/> and ray distance \(l = \| (t_{k, 0}, t_{k, 1}, t_{k, 2})^T \|\)<br/> Here, \(J\) : <code class="language-plaintext highlighter-rouge">Jacobian</code>(각 axis로 편미분한 matrix) of the <code class="language-plaintext highlighter-rouge">affine approx.</code> of the <code class="language-plaintext highlighter-rouge">projective transformation</code> from camera coordinate to image coordinate</li> <li>즉, camera coordinate에서 임의의 좌표 \(\boldsymbol t_{k}\) 주변에 존재하는 입력 좌표 \(\boldsymbol t\)에 대해서는 image coordinate으로의 affine(linear) transformation이 충족된다.</li> <li>Gaussian Splatting 논문의 경우 <code class="language-plaintext highlighter-rouge">Gaussian의 중심점</code>을 \(\boldsymbol t_{k}\) 로 두면 그 주변의 \(\boldsymbol t\)에 대해서는 Jacobian을 이용한 affine(linear) transformation 가능!</li> </ul> </li> </ul> <blockquote> <p><code class="language-plaintext highlighter-rouge">Projection</code> of 3D Gaussian <code class="language-plaintext highlighter-rouge">covariance</code> to 2D</p> </blockquote> <ul> <li> <p><code class="language-plaintext highlighter-rouge">world coordinate</code> :<br/> \(\Sigma\) : 3 \(\times\) 3 covariance matrix of 3D Gaussian</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">image coordiante</code> (z=1) :<br/> \(\Sigma^{\ast} = J W \Sigma W^T J^T\) : covariance matrix of 2D splat</p> <ul> <li>Step 1. world-to-camera (<code class="language-plaintext highlighter-rouge">affine</code>) :<br/> \(\boldsymbol u \rightarrow W \boldsymbol u + d\)</li> <li>Step 2. camera-to-image (<code class="language-plaintext highlighter-rouge">local affine approx.</code>) :<br/> Projection<br/> \(W \boldsymbol u + d \rightarrow \phi_{k}(W \boldsymbol u + d) = x_k + \boldsymbol J_{k} W \boldsymbol u + \boldsymbol J_{k} (d - \boldsymbol t_{k})\)<br/> 상수 부분을 제외하면 \(\boldsymbol x = \boldsymbol J_{k} W \boldsymbol u\)</li> <li>Step 3. covariance 특성 :<br/> \(Cov[Ax] = E[(Ax - E[Ax])(Ax - E[Ax])^T]\)<br/> \(= E[A(x - E[x])(x - E[x])^TA^T] = A Cov[x] A^T\)</li> <li>Step 4. <code class="language-plaintext highlighter-rouge">world-to-image covariance</code> :<br/> \(\boldsymbol u \rightarrow \boldsymbol J_{k} W \boldsymbol u\) 이므로<br/> \(\Sigma \rightarrow \boldsymbol J \boldsymbol W \Sigma \boldsymbol W^T \boldsymbol J^T\)</li> <li>Step 5. <code class="language-plaintext highlighter-rouge">covariance dimension reduction</code> :<br/> 추가로, \(\boldsymbol J \boldsymbol W \Sigma \boldsymbol W^T \boldsymbol J^T\) 로 계산한 \(\Sigma^{\ast}\) 는 3-by-3 matrix 인데,<br/> 3D Gaussian을 한쪽 축으로 적분하면 2D Gaussian과 동일한 값을 가지게 되므로<br/> 3-by-3 covariance matrix의 3번째 행과 열의 값을 버린<br/> 2-by-2 matrix를 projected 2D covariance matrix 로 사용하면 됨!</li> </ul> </li> </ul> <h3 id="parameter-2-spherical-harmonicssh-coeff">Parameter 2. Spherical Harmonics(SH) coeff.</h3> <ul> <li><code class="language-plaintext highlighter-rouge">Spherical Harmonics</code> (SH) :<br/> spherical coordinate 에서 <code class="language-plaintext highlighter-rouge">각도</code> (\(\theta, \phi\))를 입력받아 <code class="language-plaintext highlighter-rouge">구의 표면 위치에서의 값</code>을 출력하는 함수<br/> spherical coordinate 에서 라플라스 방정식을 풀면 아래 수식과 같음</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/5-480.webp 480w,/assets/img/2024-07-11-GS/5-800.webp 800w,/assets/img/2024-07-11-GS/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/6-480.webp 480w,/assets/img/2024-07-11-GS/6-800.webp 800w,/assets/img/2024-07-11-GS/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/7-480.webp 480w,/assets/img/2024-07-11-GS/7-800.webp 800w,/assets/img/2024-07-11-GS/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> l이 같은 함수들은 same band l에 있다고 말함 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/8-480.webp 480w,/assets/img/2024-07-11-GS/8-800.webp 800w,/assets/img/2024-07-11-GS/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 가로축 : theta, 세로축 : phi, 채도 : SH magnitude, 색상 : SH phase </div> <ul> <li> <p>SH coeff. <code class="language-plaintext highlighter-rouge">초기값</code> :<br/> 0-band SH (\(\theta, \phi\) 와 관계없는 view-independent color) 의 경우 SfM으로 얻은 point cloud의 RGB color값과 RGB2SH 이용하여 초기화<br/> 다른 band의 경우 0으로 초기화</p> </li> <li>SH 의 역할 : <ul> <li>SH에서 band 수를 제한해서 쓴다는 것은 높은 band (high freq. 또는 detail info.)는 자른다는 의미이므로 <code class="language-plaintext highlighter-rouge">smoothing</code> 역할</li> <li>적은 비용(coeff. 몇 개만 사용)으로 SH function을 <code class="language-plaintext highlighter-rouge">approx.</code></li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">SH coeff.</code>로 <code class="language-plaintext highlighter-rouge">color</code> 나타내는 법 :<br/> Fourier Series 에서처럼,<br/> SH coeff. \(k_{l}^{m}\) 의 optimal 값을 구해서<br/> \(k_{l}^{m}\) 와 \(Y_l^m(\theta, \phi)\) 의 weighted sum!<br/> \(C = \Sigma_{l=0}^{l_{max}} \Sigma_{m=-l}^{l} k_l^m Y_l^m(\theta, \phi)\)<br/> 즉, <code class="language-plaintext highlighter-rouge">trainable parameter</code> : SH coeff.인 \(k_{l}^{m}\)<br/> (<code class="language-plaintext highlighter-rouge">light source</code>마다 SH coeff. \(k_{l}^{m}\) 다르므로 find optimal value)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/9-480.webp 480w,/assets/img/2024-07-11-GS/9-800.webp 800w,/assets/img/2024-07-11-GS/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="parameter-3-opacity">Parameter 3. opacity</h3> <ul> <li> <p>opacity \(\alpha\) <code class="language-plaintext highlighter-rouge">초기값</code> :<br/> 임의의 실수값으로 초기화<br/> inverse_sigmoid(0.1 * torch.ones(…))</p> </li> <li> <p>opacity \(\alpha\) <code class="language-plaintext highlighter-rouge">range</code> :<br/> \(\alpha \in [0, 1)\) 이므로<br/> 마지막에 sigmoid activation function을 씌워서 smooth gradient를 얻음</p> </li> </ul> <h3 id="parameter-4-3d-positionmean">Parameter 4. 3D position(mean)</h3> <h2 id="fast-differentiable-rasterizer-for-gaussians">Fast Differentiable Rasterizer for Gaussians</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/10-480.webp 480w,/assets/img/2024-07-11-GS/10-800.webp 800w,/assets/img/2024-07-11-GS/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Tile Rasterizer</p> </blockquote> <ul> <li> <p>기능 : 3D Gaussians로 구성된 3D model을 특정 camera pose에 대해 2D rendering</p> </li> <li><code class="language-plaintext highlighter-rouge">input</code> : <ul> <li>image의 rendering할 width, height</li> <li>3D Gaussian의 xyz-mean, covariance in world-coordinate</li> <li>3D Gaussian의 color, opacity</li> <li>current camera pose</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Frustum Culling</code> :<br/> 주어진 camera pose에서 view frustum을 그려서<br/> view frustum과 교차하는 확률이 99% confidence interval 범위 밖에 있는 3D Gaussians는 제거(culling)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/16-480.webp 480w,/assets/img/2024-07-11-GS/16-800.webp 800w,/assets/img/2024-07-11-GS/16-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Guard Band</code> :<br/> 아래의 경우 projected 2D covariance 계산이 불안정하기 때문에 개별적으로 제거 <ul> <li><code class="language-plaintext highlighter-rouge">view frustum의 near plane에 가까이 있는</code> Gaussian의 경우,<br/> EWA Volume Splatting에서 언급된 cam-to-img projection <code class="language-plaintext highlighter-rouge">nonlinearity</code>가 심하기 때문에<br/> projection matrix를 Jacobian으로 approx.한 값에 더 큰 artifact가 생김</li> <li>view frustum 밖에 멀리 떨어진 경우 <code class="language-plaintext highlighter-rouge">?????</code><br/> 코드에서는 이 경우는 빼버렸음 (주석 처리)<br/> (diff-gaussian-rasterization/cuda_rasterizer/auxiliary.h)</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Create Tiles</code> :<br/> <code class="language-plaintext highlighter-rouge">CUDA 병렬 처리</code>를 위해<br/> \(w \times h\)의 image를 \(16 \times 16\) pixel의 tiles로 쪼갬</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/11-480.webp 480w,/assets/img/2024-07-11-GS/11-800.webp 800w,/assets/img/2024-07-11-GS/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p><code class="language-plaintext highlighter-rouge">Parallelism</code> :<br/> <code class="language-plaintext highlighter-rouge">tile마다</code> 개별 <code class="language-plaintext highlighter-rouge">CUDA thread</code> block으로 실행하여<br/> forward/backward processing, data loading/sharing을 병렬처리<br/> (여러 threads가 Gaussian points를 shared memory에 collaboratively load)<br/> (VRAM과 DRAM 사이의 이동은 overhead 발생하기 때문에 <code class="language-plaintext highlighter-rouge">VRAM</code>에서 모두 처리해버릴 수 있도록 <code class="language-plaintext highlighter-rouge">CUDA Functions</code>(.cu)를 직접 짬!)</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Duplicate with Keys</code> :</p> <ul> <li><code class="language-plaintext highlighter-rouge">view-space-depth</code>와 <code class="language-plaintext highlighter-rouge">tile-ID</code>를 이용하여 tile마다 각 Gaussian의 key를 생성<br/> tile-ID 쪽이 MSB<br/> view-space-depth 쪽이 LSB<br/> 각 Gaussian의 value는 Gaussian’s index</li> <li><code class="language-plaintext highlighter-rouge">CUDA 병렬처리</code> 덕분에 2D Gaussian 하나가 3개의 tiles에 걸쳐 있다면, 3개의 2D Gaussians로 복제(<code class="language-plaintext highlighter-rouge">instance화</code>)되는 것처럼 작동</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/12-480.webp 480w,/assets/img/2024-07-11-GS/12-800.webp 800w,/assets/img/2024-07-11-GS/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Sort by Keys</code> : <ul> <li>tile마다 Depth 기준으로 <code class="language-plaintext highlighter-rouge">Radix Sort</code></li> <li>여기서 한 번 sort 하고 나면 끝!! 추가로 per-pixel sorting 할 필요 없음</li> <li>pixel-wise sorting이 아니라 Gaussians sort라서 \(\alpha\)-blending approx.이긴 한데, <code class="language-plaintext highlighter-rouge">splats가 각 pixel size 정도가 되기 때문에</code> 해당 approx. 오차는 무시 가능!</li> <li>쨌든 이 덕분에 visible artifacts 없이 training, rendering performance 베리베리 굳</li> </ul> </li> </ul> <pre><code class="language-Python">from collections import deque
# 양방향에서 삽입/삭제 가능한 queue형 자료구조

# 1의 자릿수 기준으로 정렬한 뒤
# 10의 자릿수 기준으로 정렬한 뒤
# ...
def radixSort():
    nums = list(map(int, input().split(' ')))
    buckets = [deque() for _ in range(10)] # 각 자릿수(0~9)에 대응되는 10개의 empty deque()
    
    max_val = max(nums)
    queue = deque(nums) # 정렬할 숫자들
    digit = 1 # 정렬 기준이 되는 자릿수
    
    while (max_val &gt;= digit): # 가장 큰 수의 자릿수일 때까지만 실행
        while queue:
            num = queue.popleft() # 정렬할 숫자
            buckets[(num // digit) % 10].append(num) # 각 자릿수(0~9)에 따라 buckets에 num을 넣는다.
        
        # 해당 정렬 기준 자릿수에서 buckets에 다 넣었으면, buckets에 담겨있는 순서대로 꺼내와서 정렬한다.
        for bucket in buckets:
            while bucket:
                queue.append(bucket.popleft())

        digit *= 10 # 정렬 기준이 되는 자릿수 증가시키기
    
    print(list(queue))
</code></pre> <ul> <li> <p><code class="language-plaintext highlighter-rouge">Identify Tile Ranges</code> :<br/> tile별 Gaussian list를 효율적으로 관리하기 위해<br/> tile마다 Gaussian list 범위 식별</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Get Tile Ranges</code> :<br/> 모든 tile에 대해 Gaussian list 범위 읽어옴</p> </li> <li>\(\alpha\)-Blending in Order (<code class="language-plaintext highlighter-rouge">forward process</code>) : <ul> <li>tile별 CUDA 병렬처리에 의해 각 pixel에 대해 <code class="language-plaintext highlighter-rouge">color</code> 및 <code class="language-plaintext highlighter-rouge">opacity</code> \(\alpha\) 값을 Gaussian list의 <code class="language-plaintext highlighter-rouge">앞에서 뒤로</code> accumulate</li> <li>i-th tile에 있는 pixels 중 a pixel’s accumulated opacity 값이 target saturation threshold를 넘어서면, 해당 i-th thread STOP (유일한 STOP 조건)</li> <li><code class="language-plaintext highlighter-rouge">Gaussian의 개수를 제한하지 않음</code>으로써 scene-specific hyper-param. tuning 없이 arbitrary depth complexity를 가지는 scene을 커버 가능<br/> (GPU Radix Sort 덕분에 parallelism(병렬) 및 amortized(분할상환) 가능하여 Gaussian 개수 늘릴 수 있었음)</li> <li><code class="language-plaintext highlighter-rouge">기존 기법들은 pixel마다 정렬이 필요</code>해서 inefficient했지만<br/> 본 논문은 tile별 CUDA 병렬처리 덕분에 efficient<br/> (e.g. NeRF : ray per pixel 쏴서 accumulate per pixel for volume rendering <code class="language-plaintext highlighter-rouge">???</code>)</li> </ul> <p>\(c = \alpha_{1}c_{1} + (1 - \alpha_{1})(\alpha_{2}c_{2} + (1 - \alpha_{2})(\cdots)) = \alpha_{1}c_{1} + (1 - \alpha_{1})\alpha_{2}c_{2} + (1 - \alpha_{1})(1 - \alpha_{2})(\cdots) = \cdots = \sum_{i=1}^{N}(\alpha_{i}c_{i}\prod_{j=1}^{i-1}(1-\alpha_{j}))\) where \(\alpha_{0} = 0\)</p> </li> <li><code class="language-plaintext highlighter-rouge">Backward process</code> : <ul> <li>각 tile의 Gaussian list에 대해 Gaussian의 <code class="language-plaintext highlighter-rouge">opacity 비율에 따라</code> <code class="language-plaintext highlighter-rouge">뒤에서 앞으로</code> gradient update</li> <li>직접 backward gradient update 식을 구해서 이용</li> <li>backward process를 위해 <d-cite key="Point1">[3]</d-cite>처럼 <code class="language-plaintext highlighter-rouge">pixel마다</code> global memory에 blended points list를 저장할 수도 있지만<br/> dynamic memory management overhead가 생기기 때문에<br/> forward process에서 <code class="language-plaintext highlighter-rouge">tile마다</code> 구했던 range 및 sorted Gaussian list를 <code class="language-plaintext highlighter-rouge">재사용</code></li> <li>\(\alpha\)-blending으로 합쳤던 각 Gaussian으로 gradient backpropagation을 해주려면<br/> \(\alpha\)-blending 각 step에서의 accumulated opacity 값이 필요한데,<br/> 이를 따로 list에 저장해두고 훑는 게 아니라,<br/> \(\alpha\)-blending을 할 때 그때그때 각 Gaussian point에 지금까지의 accumulated opacity 값인 \(\alpha_{l}\)을 저장해두고,<br/> \(\alpha\)-blending final step에서의 total accumulated opacity 값 \(\alpha_{N}\)만 backward process에 넘겨 주면<br/> backward process할 때 \(\alpha_{N}\)를 \(\alpha_{l}\)로 나눈 값을 gradient 계산에 사용 <code class="language-plaintext highlighter-rouge">?????</code></li> <li>0으로 나눠지는 경우를 방지하기 위해 \(\alpha\) 값이 \(\frac{1}{255}\)보다 작다면 blending update 안 함</li> <li>rasterization 중에 blending 값이 0.9999를 초과하기 전에 STOP<br/> <code class="language-plaintext highlighter-rouge">?????</code></li> </ul> </li> <li> <p>Primitives :<br/> 본 논문의 Gaussians는 <code class="language-plaintext highlighter-rouge">Euclidean space</code>에 <code class="language-plaintext highlighter-rouge">primitives</code>를 남김 <code class="language-plaintext highlighter-rouge">?????</code><br/> \(\rightarrow\) <d-cite key="Plenoxels">[1]</d-cite>, <d-cite key="MipNeRF360">[10]</d-cite>과 달리 distant or large Gaussians 처리를 위해 space compaction, warping, or projection 할 필요가 없음 <code class="language-plaintext highlighter-rouge">?????</code></p> </li> <li>Efficient Rasterization : <ul> <li>Pulsar 논문<d-cite key="Pulsar">[5]</d-cite> 에서처럼<br/> an entire image에 대해 가장 작은 원소(<code class="language-plaintext highlighter-rouge">primitives</code>)를 미리 정렬(<code class="language-plaintext highlighter-rouge">pre-sort</code>)하여 <code class="language-plaintext highlighter-rouge">primitives = Gaussians ?????</code><br/> pixel-wise sorting 비용을 절감</li> <li>differentiable</li> <li>arbitrary number of Gaussians에 대해 backpropagation 가능<br/> with low additional memory : O(1) per pixel</li> <li>2D projection 가능</li> </ul> </li> </ul> <h2 id="optimization-with-adaptive-density-control-of-3d-gaussians">Optimization with Adaptive Density Control of 3D Gaussians</h2> <h3 id="optimization">Optimization</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/13-480.webp 480w,/assets/img/2024-07-11-GS/13-800.webp 800w,/assets/img/2024-07-11-GS/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Loss :<br/> predicted image와 GT image를 비교하는<br/> <code class="language-plaintext highlighter-rouge">L1 loss</code> 및 <code class="language-plaintext highlighter-rouge">D-SSIM loss</code><br/> D-SSIM : Directional Structural Similarity Index Measure</p> </li> <li> <p>3D Gaussian의 xyz-mean에 대해서만 <d-cite key="Plenoxels">[1]</d-cite>에서처럼 <code class="language-plaintext highlighter-rouge">standard exponential decay scheduling</code> 사용</p> </li> <li>Adam optimizer로 네 가지 param. 업데이트 <ul> <li>3D xyz-mean</li> <li>3D covariance</li> <li>color</li> <li>opacity</li> </ul> </li> <li>optimization 세부 사항 : <ul> <li>연산을 <code class="language-plaintext highlighter-rouge">low resol.부터 warm-up</code> :<br/> 목적 : model이 효율적으로 coarse info.부터 학습하도록 하여 <code class="language-plaintext highlighter-rouge">stability</code> 향상<br/> 초기에 4배 작은 image로 optimization 진행하고 250, 500 iter.에서 2배씩 upsampling</li> <li>Spherical Harmonics <code class="language-plaintext highlighter-rouge">low band부터 warm-up</code> :<br/> 목적 : 처음부터 high band로 detail까지 학습하려고 하면<br/> scene의 corner를 촬영하거나 inside-out 방식(카메라가 촬영 대상의 내부에 위치하여 바깥쪽을 촬영) 때문에<br/> <code class="language-plaintext highlighter-rouge">놓친 angular 영역이 있을 경우 SH의 0-band coeff. (base or diffuse color)가 부적절</code>하게 만들어질 수 있어서<br/> 처음에는 0-band coeff.를 optimize하고 매 1000 iter.마다 band 수 늘려서 4-band coeff.까지 optimization</li> </ul> </li> </ul> <h3 id="adaptive-density-control-of-gaussians">Adaptive Density Control of Gaussians</h3> <p>optimization of 4 param.의 경우 매 iter.마다 update하지만,<br/> Adaptive Density Control of Gaussians의 경우 <code class="language-plaintext highlighter-rouge">100 iter.마다</code> update</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/14-480.webp 480w,/assets/img/2024-07-11-GS/14-800.webp 800w,/assets/img/2024-07-11-GS/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Remove</code> :<br/> \(\alpha\) 값이 threshold보다 작거나<br/> world-space에서 크기가 매우 크거나<br/> view-space에서 footprint가 매우 큰 경우<br/> 3D Gaussians 제거</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/15-480.webp 480w,/assets/img/2024-07-11-GS/15-800.webp 800w,/assets/img/2024-07-11-GS/15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Gaussians가 scene을 제대로 표현 못 하는 중<br/> \(\rightarrow\) scene을 제대로 표현하기 위해선 Gaussian position을 크게 옮겨야 함<br/> \(\rightarrow\) view-space positional gradient \(\Delta_{p} L\)가 큼<br/> \(\rightarrow\) under/over-reconstruction 상황이므로 clone/split을 통해 정확한 위치에 Gaussian이 분포하도록 하자</p> </li> <li><code class="language-plaintext highlighter-rouge">Split</code> :<br/> <code class="language-plaintext highlighter-rouge">over-reconstruction</code>의 경우 3D Gaussians split <ul> <li>split : 1개의 Gaussian을 <code class="language-plaintext highlighter-rouge">2개로 분리</code>하고 각 scale을 줄인 후 <code class="language-plaintext highlighter-rouge">기존 3D Gaussian의 PDF</code>에 따라 sampling하여 배치<br/> Gaussians의 수는 증가하지만, total volume은 유지</li> <li>조건 1. <code class="language-plaintext highlighter-rouge">view-space positional gradient</code> \(\Delta_{p} L\)의 avg. magnitude \(\geq\) threshold \(\tau_{pos}\)</li> <li>조건 2. <code class="language-plaintext highlighter-rouge">covariance</code>가 큼</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Clone</code> :<br/> <code class="language-plaintext highlighter-rouge">under-reconstruction</code>의 경우 3D Gaussians clone <ul> <li>clone : <code class="language-plaintext highlighter-rouge">같은 크기로 copy</code> 후 <code class="language-plaintext highlighter-rouge">positional gradient 방향</code>에 배치<br/> total volume 및 Gaussians의 수 모두 증가</li> <li>조건 1. <code class="language-plaintext highlighter-rouge">view-space positional gradient</code> \(\Delta_{p} L\)의 avg. magnitude \(\geq\) threshold \(\tau_{pos}\)</li> <li>조건 2. <code class="language-plaintext highlighter-rouge">covariance</code>가 작음</li> </ul> </li> <li>3000 iter.마다 \(\alpha\) <code class="language-plaintext highlighter-rouge">알파 값을 주기적으로 0으로 초기화</code> 하면 전체 Gaussian 조절에 큰 도움이 됨! <ul> <li>효과 1. volumetric 기법의 특성상 <code class="language-plaintext highlighter-rouge">camera와 가까운 영역</code>에서 많은 <code class="language-plaintext highlighter-rouge">floater</code>들이 생겨서 Gaussian density가 증가하는데, 이를 제거해주는 역할<br/> floater 해결 관련 논문 : <d-cite key="floater1">[6]</d-cite> <d-cite key="floater2">[7]</d-cite> <d-cite key="floater3">[8]</d-cite></li> <li>효과 2. <code class="language-plaintext highlighter-rouge">큰 Gaussian들이 중첩</code>되어 있는 case를 제거해주는 역할</li> </ul> </li> </ul> <h2 id="results">Results</h2> <h3 id="implementation">Implementation</h3> <ul> <li> <p>custom CUDA kernel :<br/> tile-based rasterization을 위해<br/> custom CUDA kernel를 추가하여 사용 like <d-cite key="Point1">[3]</d-cite>, <d-cite key="Plenoxels">[1]</d-cite>, <d-cite key="superfast">[9]</d-cite></p> </li> <li> <p>Radix Sort :<br/> fast Radix Sort를 위해 NVIDIA CUB sorting routines <d-cite key="radixsort">[11]</d-cite> 사용</p> </li> <li> <p>interactive image viewer :<br/> open-source SIBR <a href="https://gitlab.inria.fr/sibr/sibr_core">SIBR</a> 이용해서<br/> interactive image-rendering viewer 만듬 (frame rate 측정에 사용)</p> </li> </ul> <h3 id="evaluation">Evaluation</h3> <ul> <li>Dataset :<br/> bounded indoor scenes와 unbounded outdoor scenes 전부 커버 <ul> <li>synthetic Blender dataset (Nerf) :<br/> have exhaustive set of bounded views with exact camera param.<br/> \(\rightarrow\) SOTA result even with 100K uniformly random initialization</li> <li>Mip-Nerf360 dataset</li> <li>Tanks&amp;Temples dataset</li> <li>Hedman et al. dataset</li> </ul> </li> <li>Metrics : <ul> <li>PSNR</li> <li>L-PIPS</li> <li>SSIM (D-SSIM)</li> </ul> </li> <li>Comparison : <ul> <li><code class="language-plaintext highlighter-rouge">Quality</code> : NeRF 계열 중 SOTA인 <code class="language-plaintext highlighter-rouge">Mip-Nerf360</code> <d-cite key="MipNeRF360">[10]</d-cite>과 비교 <ul> <li>끝까지 훈련시켰을 때 비슷한 quality 보이고,</li> <li>training speed는 35-45 min. versus 48 hours</li> </ul> </li> <li>Traning/Rendering <code class="language-plaintext highlighter-rouge">Speed</code> : NeRF 계열 중 SOTA인 <code class="language-plaintext highlighter-rouge">InstantNGP</code> <d-cite key="InstantNGP">[2]</d-cite>, <code class="language-plaintext highlighter-rouge">Plenoxels</code> <d-cite key="Plenoxels">[1]</d-cite> 과 비교 <ul> <li>speed SOTA인 <d-cite key="InstantNGP">[2]</d-cite> , <d-cite key="Plenoxels">[1]</d-cite> 과 비슷한 quality 가질 때까지 training 5-10 min.밖에 안 걸리고,</li> <li>훈련 더 하면 <d-cite key="InstantNGP">[2]</d-cite>, <d-cite key="Plenoxels">[1]</d-cite>보다 더 좋은 quality 가짐</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/19-480.webp 480w,/assets/img/2024-07-11-GS/19-800.webp 800w,/assets/img/2024-07-11-GS/19-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/19.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/17-480.webp 480w,/assets/img/2024-07-11-GS/17-800.webp 800w,/assets/img/2024-07-11-GS/17-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/17.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 7K iter.으로도 꽤 좋은 결과 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/18-480.webp 480w,/assets/img/2024-07-11-GS/18-800.webp 800w,/assets/img/2024-07-11-GS/18-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/18.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Comparison : <ul> <li>Compactness :<br/> anisotropic 3D Gaussians는<br/> scene representation 뿐만 아니라<br/> complex shape with a lower number of param.을 모델링하는 데도 쓰일 수 있음 <ul> <li>space carving으로 얻은 <d-cite key="Point3">[12]</d-cite> 의 initial point cloud에서 시작했을 때 <d-cite key="Point3">[12]</d-cite> 의 PSNR 값은 2-4 min.만에 넘겨버림</li> <li>또한, <d-cite key="Point3">[12]</d-cite> 의 point cloud의 4분의 1만큼만 써도 작은 model size로도 <d-cite key="Point3">[12]</d-cite> 의 PSNR 넘겨버림</li> </ul> </li> </ul> </li> </ul> <blockquote> <p>Space Carving :</p> <ul> <li>설명 : 여러 camera에 대해 voxel-space에서 object 있는 부분만 남기고 깎아내는 기법</li> <li>이유 : 3D reconstruction을 할 때 color 정보만으로 segmentation 가능할 정도로 background는 simple할수록 좋기 때문</li> <li>한계 : 빛, 그림자 같은 정보는 사용하지 않기 때문에 fg/bg 판단만 가능하다. 따라서 lidar처럼 camera에 depth-detection 메커니즘이 없을 경우 물체 내부의 구멍 같은 건 reconstruct 불가능</li> </ul> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/20-480.webp 480w,/assets/img/2024-07-11-GS/20-800.webp 800w,/assets/img/2024-07-11-GS/20-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/20.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="ablation-study">Ablation Study</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/21-480.webp 480w,/assets/img/2024-07-11-GS/21-800.webp 800w,/assets/img/2024-07-11-GS/21-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/21.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> PSNR score for Ablation Study </div> <ul> <li><code class="language-plaintext highlighter-rouge">Intialization (SfM)</code> : <ul> <li>uniformly sample a cube (random initialization w/o SfM points) :<br/> 주로 <code class="language-plaintext highlighter-rouge">background</code> 퀄리티 저하<br/> training view가 충분하지 않은 영역에서는 optimization으로 제거할 수 없는 <code class="language-plaintext highlighter-rouge">floater</code> 많이 발생<br/> \(\rightarrow\) synthetic NeRF dataset의 경우 bg가 없고 have exhaustive set of bounded views with exact input camera param. 이므로 random initializatino으로도 성능 굳</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/22-480.webp 480w,/assets/img/2024-07-11-GS/22-800.webp 800w,/assets/img/2024-07-11-GS/22-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/22.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Densification (clone, split)</code> : <ul> <li>Split : <code class="language-plaintext highlighter-rouge">background</code> reconstruction에 중요한 역할</li> <li>Clone : <code class="language-plaintext highlighter-rouge">thin</code> structure reconstruction에 중요한 역할</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/23-480.webp 480w,/assets/img/2024-07-11-GS/23-800.webp 800w,/assets/img/2024-07-11-GS/23-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/23.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Unlimited depth complexity of splats with gradients</code> : <ul> <li>Limited-BW :<br/> 각 tile의 Gaussian list에서 앞에서부터 N개까지만 gradient 전파할 경우<br/> Pulsar <d-cite key="Pulsar">[5]</d-cite>에서의 값의 2배인 N=10으로 했는데도 unstable optimization 초래</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/24-480.webp 480w,/assets/img/2024-07-11-GS/24-800.webp 800w,/assets/img/2024-07-11-GS/24-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/24.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> left: N=10 / right: N=inf </div> <ul> <li><code class="language-plaintext highlighter-rouge">Anisotropic Covariance</code> : <ul> <li>isotropic convariance :<br/> single scala value (radius of 3D Gaussian)를 optimize할 경우<br/> 같은 Gaussian 개수를 쓰더라도 <code class="language-plaintext highlighter-rouge">align with surfaces</code> 잘 하지 못해서 <code class="language-plaintext highlighter-rouge">fine</code> structure 잘 나타내지 못함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/25-480.webp 480w,/assets/img/2024-07-11-GS/25-800.webp 800w,/assets/img/2024-07-11-GS/25-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/25.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Spherical Harmonics</code> : <ul> <li>color 나타낼 때 <code class="language-plaintext highlighter-rouge">view-dependent</code> effect 담당</li> </ul> </li> </ul> <h2 id="discussion">Discussion</h2> <h3 id="limitations--future-work">Limitations &amp; Future Work</h3> <ul> <li><code class="language-plaintext highlighter-rouge">training view가 부족한 영역</code>에서는 여전히 <code class="language-plaintext highlighter-rouge">floater</code>, <code class="language-plaintext highlighter-rouge">elongated(길쭉한) artifacts</code>, <code class="language-plaintext highlighter-rouge">splotchy(얼룩진) Gaussians</code> 등 artifacts 발생 (Mip-NeRF360 등 prev. methods도 마찬가지)<br/> \(\rightarrow\) regularization으로 alleviate 가능</li> <li><code class="language-plaintext highlighter-rouge">view-dependent appearance</code>가 나타나는 영역에서는 large Gaussian 만들 때 <code class="language-plaintext highlighter-rouge">guard band</code> 등의 이유로 <code class="language-plaintext highlighter-rouge">popping</code> artifacts 발생<br/> \(\rightarrow\) better culling과 regularization으로 alleviate 가능</li> <li>Gaussians <code class="language-plaintext highlighter-rouge">depth-order</code> 갑자기 바뀔 수 있음<br/> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">anti-aliasing</code>으로 해결 가능</li> <li>urban dataset처럼 very <code class="language-plaintext highlighter-rouge">large scene</code>에 대해서는 <code class="language-plaintext highlighter-rouge">position learning-rate</code>를 줄이는 게 도움됨</li> <li>prev. point-based methods에 비해서는 compact하긴 하지만, NeRF-based methods에 비해서는 memory consumption이 훨씬 큼<br/> e.g. large scene을 학습할 때 최대 GPU memory consumption은 20GB를 넘김<br/> \(\rightarrow\) InstantNGP에서처럼 optimization 과정을 low-level implementation 하면 괜찮<br/> e.g. scene을 rendering할 때도 model 저장하는 데 몇백MB, rasterizer 저장하는 데 30-500MB 필요<br/> \(\rightarrow\) memory consumption을 줄이기 위한 추후 개선 필요 (point-clouds compression technique <d-cite key="pointcompress">[13]</d-cite>을 적용해볼 수 있을 듯)</li> <li>3D Gaussians를 mesh reconstruction에 사용할 수 있는지 연구가 진행된다면 본 논문이 정확히 volumetric 과 surface representation 사이 어디에 위치해있는지를 이해할 수 있음</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/26-480.webp 480w,/assets/img/2024-07-11-GS/26-800.webp 800w,/assets/img/2024-07-11-GS/26-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/26.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> left(Mip-NeRF360): floaters and grainy(오돌토돌한, 거친) appearance / right(3DGS): low-detail bg from coarse Gaussians </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-11-GS/27-480.webp 480w,/assets/img/2024-07-11-GS/27-800.webp 800w,/assets/img/2024-07-11-GS/27-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-11-GS/27.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> training에서 많이 보지 못한 view의 경우 left(Mip-NeRF360), right(3DGS) 모두 artifacts 발생 </div> <h3 id="conclusion">Conclusion</h3> <ul> <li><code class="language-plaintext highlighter-rouge">3D Gaussian</code> :<br/> volumetric rendering의 특성을 살림과 동시에 fast splat-based rasterization 가능<br/> continuous representation이어야만 fast, high-quality radiance field training 가능하다는 기존 통념을 반전시킴</li> <li><code class="language-plaintext highlighter-rouge">CUDA</code> Implementation :<br/> training time의 80%는 Pytorch code (for 가독성)<br/> rasterization만 optimized CUDA kernels (for real-time)<br/> \(\rightarrow\) InstantNGP <d-cite key="InstantNGP">[2]</d-cite>처럼 optimization 나머지 부분도 전부 CUDA로 옮기면 훨씬 speedup 가능</li> <li><code class="language-plaintext highlighter-rouge">real-time rasterization by GPU</code> :<br/> rasterization이 main bottleneck인데<br/> GPU 힘으로 real-time rasterization pipeline 구현한 게<br/> 기존 volumetric ray-marching NeRF-based 기법보다 faster training, rendering 가능했던 비결</li> <li>Higher Quality than SOTA Mip-NeRF360(2022)</li> <li>Faster Training than SOTA InstantNGP(2022)</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="gaussian"/><category term="splatting"/><category term="rendering"/><category term="3d"/><category term="view"/><category term="synthesis"/><summary type="html"><![CDATA[3D GS for Real-Time Radiance Field Rendering]]></summary></entry><entry><title type="html">Pytorch Tensor</title><link href="https://semyeong-yu.github.io/blog/2024/TorchTensor/" rel="alternate" type="text/html" title="Pytorch Tensor"/><published>2024-07-09T15:00:00+00:00</published><updated>2024-07-09T15:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/TorchTensor</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/TorchTensor/"><![CDATA[<h2 id="pytorch-tensor-summary">Pytorch Tensor Summary</h2> <h3 id="python-문법">Python 문법</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/2-480.webp 480w,/assets/img/2024-07-09-TorchTensor/2-800.webp 800w,/assets/img/2024-07-09-TorchTensor/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="create-and-access-tensor">Create and Access Tensor</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/3-480.webp 480w,/assets/img/2024-07-09-TorchTensor/3-800.webp 800w,/assets/img/2024-07-09-TorchTensor/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/4-480.webp 480w,/assets/img/2024-07-09-TorchTensor/4-800.webp 800w,/assets/img/2024-07-09-TorchTensor/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="tensor-slice-indexing">Tensor slice indexing</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/5-480.webp 480w,/assets/img/2024-07-09-TorchTensor/5-800.webp 800w,/assets/img/2024-07-09-TorchTensor/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="tensor-listtensor--boolean-indexing">Tensor list(tensor) &amp; boolean indexing</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/6-480.webp 480w,/assets/img/2024-07-09-TorchTensor/6-800.webp 800w,/assets/img/2024-07-09-TorchTensor/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/7-480.webp 480w,/assets/img/2024-07-09-TorchTensor/7-800.webp 800w,/assets/img/2024-07-09-TorchTensor/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="gather-scatter_add-indexing">gather, scatter_add indexing</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/8-480.webp 480w,/assets/img/2024-07-09-TorchTensor/8-800.webp 800w,/assets/img/2024-07-09-TorchTensor/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="where-condition">where condition</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/9-480.webp 480w,/assets/img/2024-07-09-TorchTensor/9-800.webp 800w,/assets/img/2024-07-09-TorchTensor/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="timing">Timing</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/10-480.webp 480w,/assets/img/2024-07-09-TorchTensor/10-800.webp 800w,/assets/img/2024-07-09-TorchTensor/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="reshape-permute">Reshape, Permute</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/11-480.webp 480w,/assets/img/2024-07-09-TorchTensor/11-800.webp 800w,/assets/img/2024-07-09-TorchTensor/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="element-wise-reduction-operation-concatenation">element-wise, reduction operation, concatenation</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/12-480.webp 480w,/assets/img/2024-07-09-TorchTensor/12-800.webp 800w,/assets/img/2024-07-09-TorchTensor/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="matrix-operation">Matrix operation</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/13-480.webp 480w,/assets/img/2024-07-09-TorchTensor/13-800.webp 800w,/assets/img/2024-07-09-TorchTensor/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/14-480.webp 480w,/assets/img/2024-07-09-TorchTensor/14-800.webp 800w,/assets/img/2024-07-09-TorchTensor/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="broadcasting">Broadcasting</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/15-480.webp 480w,/assets/img/2024-07-09-TorchTensor/15-800.webp 800w,/assets/img/2024-07-09-TorchTensor/15-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="in-place-gpu">In-place, GPU</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-09-TorchTensor/16-480.webp 480w,/assets/img/2024-07-09-TorchTensor/16-800.webp 800w,/assets/img/2024-07-09-TorchTensor/16-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-09-TorchTensor/16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="cv-tasks"/><category term="pytorch"/><summary type="html"><![CDATA[import torch]]></summary></entry><entry><title type="html">Pytorch Basic Code (DDP)</title><link href="https://semyeong-yu.github.io/blog/2024/PytorchBasic/" rel="alternate" type="text/html" title="Pytorch Basic Code (DDP)"/><published>2024-07-08T11:00:00+00:00</published><updated>2024-07-08T11:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/PytorchBasic</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/PytorchBasic/"><![CDATA[<h2 id="pytorch-basic-code-distributeddataparallel-ver">Pytorch Basic Code (DistributedDataParallel ver.)</h2> <h3 id="deal-with-json-image-csv">Deal with json, image, csv</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/3-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/3-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">import os
import json
import pandas as pd
import csv
from PIL import Image
import cv2

# read json
if os.path.exists(json_path):
    with open(json_path, "r") as f:
        data = json.load(f)
    f.close()

# read image 방법 1.
img = Image.open(image_path).convert('RGB') # PIL image object in range [0, 255]
img.show()

# read image 방법 2.
img = cv2.imread(image_path) # np.ndarray of shape (H, W, C) in range [0, 255] in BGR mode
cv2.imshow('Image', img)
cv2.waitKey(0)
cv2.destroyAllWindows()

# read csv 방법 1. general case
data = pd.read_csv(csv_path, sep="|", index_col=0, skiprows=[1], na_values=['?', 'nan']).values # 0-th column (1-th row는 제외) ('?'와 'nan'은 결측값으로 인식)

# read csv 방법 2. special case : csv가 row별로 dictionary 형태일 때
if os.path.exists(csv_path):
    with open(csv_path, "r") as f:
        reader = csv.DictReader(f, delimiter=",")
        data = [{key : value for key, value in row.items()} for row in reader] # row별로 읽음

# write to csv
dataset = [] # list of dictionaries
dataset.append({"id":id, "w":w, "h":h, "class":i})
pd.DataFrame(dataset).to_csv(output_path, index=False) # output_path : ".../dataset.csv"
</code></pre> <h3 id="convert-to-tensor">Convert to Tensor</h3> <p>data.py의 CustomDataset(torch.utils.data.Dataset)에서 image는 <code class="language-plaintext highlighter-rouge">shape (C, H, W) tensor</code>여야 하기 때문에<br/> PIL.Image.open() 또는 cv2.imread()로 얻은<br/> PIL image object 또는 np.ndarray를 적절한 shape 및 range의 tensor로 변환해주어야 한다</p> <ul> <li>PIL image object \(\rightarrow\) Tensor <ul> <li>torchvision.transforms.ToTensor()</li> <li>np.array(), torch.tensor()</li> <li>getdata(), torch.tensor()</li> </ul> </li> <li>np.ndarray \(\rightarrow\) Tensor <ul> <li>torch.tensor()</li> </ul> </li> </ul> <pre><code class="language-Python">PIL_img = Image.open(image_path).convert('RGB') # PIL image object of size (W, H) in range [0, 255]

# 방법 1. torchvision.transforms.ToTensor()
transform = torchvision.transforms.Compose([
    torchvision.transforms.Resize((256, 256)),
    torchvision.transforms.ToTensor() # convert to tensor in range [0., 1.]
])
img = transform(PIL_img) # tensor of shape (C, H, W) in range [0., 1.]

# 방법 2. np.array(), torch.tensor()
img = np.array(PIL_img) # np.ndarray of shape (H, W, C) in range [0., 255.]
img = torch.tensor(img.transpose((2, 0, 1)).astype(float)).mul_(1.0) / 255.0 # tensor of shape (C, H, W) in range [0., 1.]

# 방법 3. getdata(), torch.tensor()
img_data = PIL_img.getdata()
img = torch.tensor(img_data, dtype=torch.float32) # tensor of shape (H*W*C,) in range [0, 255]
img = img.view(PIL_img.size[1], PIL_img.size[0], 3).permute(2, 0, 1) / 255.0 # tensor of shape (C, H, W) in range [0., 1.]
</code></pre> <pre><code class="language-Python">img = cv2.imread(image_path) # np.ndarray of shape (H, W, C) in range [0., 255.]

img = torch.tensor(img.transpose((2, 0, 1)).astype(float)).mul_(1.0) / 255.0 # tensor of shape (C, H, W) in range [0., 1.]
</code></pre> <h3 id="create-dataset">Create Dataset</h3> <ul> <li><code class="language-plaintext highlighter-rouge">data augmentation</code> : <ul> <li>Resize</li> <li>ToTensor</li> <li>RandomHorizontalFlip</li> <li>RandomVerticalFlip</li> <li>Normalize</li> <li>RandomRotation</li> <li>RandomAffine <ul> <li>shear</li> <li>scale (zoom-in/out)</li> </ul> </li> <li>RandomResizedCrop</li> <li>ColorJitter</li> <li>GaussianBlur</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/4-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/4-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/4-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">import os
import torch
from torch.utils.data import Dataset
import cv2
import numpy as np
import glob
import random

# Create Dataset
class CustomDataset(Dataset):
    def __init__(self, args, mode):
        # lazy-loading :
        # load할 data가 너무 크다면 __init__()에서는 load할 파일명만 저장해놓고 __getitem__()에서 필요할 때마다 load
        self.args = args
        self.mode = mode
        
        if mode == 'train':
            self.data_path = os.path.join(args.data_path, 'train_blur')
        elif mode == 'val':
            self.data_path = os.path.join(args.data_path, 'val_blur')
        elif mode == 'test':
            self.data_path = os.path.join(args.data_path, 'test_blur')
        
        # a list of data/train_blur/*.png
        self.blur_path_list = sorted(glob.glob(os.path.join(self.data_path, '*.png')))
        
        # a list of data/train_sharp/*.png
        self.sharp_path_list = [os.path.normpath(path.replace('blur', 'sharp') for path in self.blur_path_list)]

    def __getitem__(self, idx):
        # should return float tensor!!
        blur_path = self.blur_path_list[idx]
        # np.ndarray of shape (H, W, C) in range [0, 255]
        blur_img = cv2.imread(blur_path) 

        if self.mode == 'train':
            sharp_path = self.sharp_path_list[idx]
            sharp_img = cv2.imread(sharp_path)
            
            # np.ndarray of shape (pat, pat, C) where pat is patch_size
            blur_img, sharp_img = self.augment(self.get_random_patch(blur_img, sharp_img)) 
            
            # tensor of shape (C, pat, pat) in range [0, 1]
            return self.np2tensor(blur_img), self.np2tensor(sharp_img) 
        
        elif self.mode == 'val':
            sharp_path = self.sharp_path_list[idx]
            sharp_img = cv2.imread(sharp_path)
            return self.np2tensor(blur_img), self.np2tensor(sharp_img)
        
        elif self.mode == 'test':
            return self.np2tensor(blur_img), blur_path

    def np2tensor(self, x):
        # input : shape (H, W, C) / range [0, 255]
        # output : shape (C, H, W) / range [0, 1]
        ts = (2, 0, 1)
        x = torch.tensor(x.transpose(ts).astype(float)).mul_(1.0) # _ : in-place
        x = x / 255.0 # normalize
        return x

    def get_random_patch(self, blur_img, sharp_img):
        H, W, C = blur_img.shape # shape (H, W, C)

        pat = self.args.patch_size # pat : patch size
        iw = random.randrange(0, W - pat + 1) # iw : range [0, W - pat]
        ih = random.randrange(0, H - pat + 1) # ih : range [0, H - pat]

        blur_img = blur_img[ih:ih + pat, iw:iw + pat, :] # shape (pat, pat, C)
        sharp_img = sharp_img[ih:ih + pat, iw:iw + pat, :]

        return blur_img, sharp_img # shape (pat, pat, C)

    def augment(self, blur_img, sharp_img):
        # random horizontal flip
        if random.random() &lt; 0.5:
            blur_img = blur_img[:, ::-1, :] # Width-axis를 flip
            sharp_img = sharp_img[:, ::-1, :]
            '''
            flow-mask pair의 경우 C-dim.이 3 = 2(optical flow x, y) + 1(occlusion mask) 이므로
            shape (T, H, W, 3)의 flow-mask pair를 horizontal flip을 하려면
            flow = flow[:, :, ::-1, :]
            flow[:, :, :, 0] *= -1
            '''
            
        # random vertical flip
        if random.random() &lt; 0.5:
            blur_img = blur_img[::-1, :, :] # Height-axis를 flip
            sharp_img = sharp_img[::-1, :, :]
            '''
            flow = flow[:, ::-1, :, :]
            flow[:, :, :, 1] *= -1
            '''

        return blur_img, sharp_img

    def __len__(self):
        return len(self.path_list)
</code></pre> <h3 id="dataloader">DataLoader</h3> <ul> <li>DataParallel(DP) vs DistributedDataParallel(DDP) :<br/> <a href="https://tkayyoo.tistory.com/27#tktag2">Pytorch DP and DDP</a> <ul> <li>DataParallel(DP) :<br/> single-process<br/> multi-thread<br/> single-machine</li> <li>DistributedDataParallel(DDP) :<br/> multi-process<br/> single-machine과 multi-machine 모두 가능</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">rank</code> : <ul> <li>전체 distributed system에서 process 순서</li> <li>4-CPU system이 2개 있을 경우<br/> rank = machine 번호(0~1) * machine 당 process 개수(4) + process 번호(0~3)</li> <li>rank = 0인 process에 대해서만 wandb로 train log 출력</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">world_size</code> : <ul> <li>전체 distributed system에서 총 process 개수</li> <li>4-GPU system이 2개 있을 경우<br/> world_size = machine 개수(2) * machine 당 process 개수(4)</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">torch.distributed.init_process_group()</code> : <ul> <li>분산 학습 환경 초기화 : 분산 학습하는 각 process 간의 통신을 설정</li> <li>backend : ‘gloo’ for CPU, ‘nccl’ for GPU, ‘mpi’ for 고성능</li> <li>init_method : 각 process가 서로 탐색하는 방법(url)<br/> 예시 : ‘env://’, f’tcp://127.0.0.1:11203’</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">torch.utils.data.distributed.DistributedSampler()</code> : <ul> <li>world_size(총 process 개수)만큼 dataset을 분할하여 모든 process가 동일한 양의 dataset을 갖도록 함</li> <li>DistributedSampler는 각 epoch마다 dataset을 무작위로 분할</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">torch.utils.data.DataLoader()</code> : <ul> <li>shuffle=False :<br/> 보통 training일 때는 일반화를 위해 shuffle=True로 두지만<br/> 분산 학습을 할 때는 같은 epoch 내에서<br/> 각 process가 서로 다른 dataset을 처리하기 위해 (중복 방지)<br/> shuffle=False로 설정</li> <li>num_workers : cpu data load할 때 multi-processing core 개수</li> <li>pin_memory=True :<br/> data load한 장치(CPU)에서 GPU로 data를 옮길 때<br/> host memory가 아닌 CPU의 page-locked memory로 할당하고<br/> GPU는 이를 참조하여 복사하므로 전송 시간을 단축<br/> pin_memory=True와 non_blocking=True는 함께 사용</li> <li>drop_last=True : 나눠떨어지지 않는 마지막 batch를 버림</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">_collate_fn(input)</code> : <ul> <li>DataLoader()에서 1개의 batch로 묶을 때 사용하는 custom 전처리 함수</li> <li>input : <ul> <li>1개의 batch에 해당하는 입력</li> <li>Dataset(torch.utils.data.Dataset)의 <strong>getitem</strong>(self, idx)이 return img, target 형태일 때<br/> [(img1, target1), (img2, target2), …]의 형태</li> </ul> </li> <li>output : <ul> <li>for iter, (x, y) in enumerate(dataloader): 의 x, y</li> </ul> </li> <li>예 : 길이가 다른 input들을 batch로 묶기 위해 padding, tokenization<br/> img의 경우 CustomDataset()에서 augmentation으로 shape (C, H, W)로 통일해줬다면 (N, C, H, W)로 묶을 수 있지만,<br/> object detection task에서 target의 경우 n_box가 image마다 다르므로 N batch로 묶기 위해 padding해주어야 함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/5-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/5-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/5-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
import torch.distributed as dist
from datetime import timedelta

def _collate_fn(samples):
    # ...

# main_worker : 각 process가 실행하는 함수
def main_worker(process_id, args):

    rank = args.machine_id * args.num_processes + process_id
    
    world_size = args.num_machines * args.num_processes
    
    dist.init_process_group(backend='nccl', init_method='env://', world_size=world_size, rank=rank, timeout=timedelta(300))
    
    ###################################################################
    
    # for epoch in range ... 밖에서
    train_dataset = CustomDataset(args, 'train')
    '''
    train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transforms.Compose(
        [
        transforms.ToTensor(), 
        transforms.Normalize((0.1302,), (0.3069,))
        ]))  
    '''

    # machine 당 process 수로 나눔
    batch_size = int(args.batch_size / args.num_processes) 
    num_workers = int(args.num_workers / args.num_processes)

    # for epoch in range ... 안에서
    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)

    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=_collate_fn, pin_memory=True, drop_last=True, sampler=train_sampler)
</code></pre> <h3 id="train">Train</h3> <ul> <li><code class="language-plaintext highlighter-rouge">torch.multiprocessing.spawn()</code> : <ul> <li>main_worker : 각 process가 실행하는 함수</li> <li>nprocs : machine 당 process 개수인 4로 설정<br/> main_worker()의 첫 번째 argument는 process_id인 0~3이 됨</li> <li>args : main_worker()에 추가로 전달할 tuple 형태의 argument</li> <li>join</li> <li>daemon</li> <li>start_method</li> </ul> </li> </ul> <ol> <li><code class="language-plaintext highlighter-rouge">model</code> initialize, and set cuda, and parallelize <ul> <li>nn.parallel.DistributedDataParallel :<br/> 각 model 복사본은 각자의 optimizer를 이용해 gradient를 구하고<br/> rank=0의 process와 통신하여 gradient의 평균을 구해서 backpropagation 진행<br/> GIL(global interpreter lock)의 제약을 해결</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">wandb</code> init <ul> <li>wandb.init() : wandb 초기화<br/> vars(args)는 args 객체의 <strong>dict</strong> 속성을 반환<br/> {‘transforms’ : ‘BaseTransform’, ‘crop_size’ : 224}과 같이 반환</li> <li>wandb.watch() : wandb 기록<br/> 모든 param.의 gradient를 기록<br/> arg.log_interval-번째 batch마다 log 기록</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">optimizer, scheduler</code> initialize</li> <li>load <code class="language-plaintext highlighter-rouge">checkpoint</code></li> <li><code class="language-plaintext highlighter-rouge">train</code> with <code class="language-plaintext highlighter-rouge">barrier</code> <ul> <li>torch.distributed.barrier() :<br/> 분산 학습 환경에서<br/> 모든 process가 이 장벽에 도달할 때까지 대기하여<br/> 모든 process가 synchronize된 상태에서 훈련이 진행되도록 함</li> <li>torch.cuda.empty_cache() :<br/> 더 이상 사용하지 않는 tensor들을 GPU cached memory에서 해제<br/> 장점 : GPU memory 확보<br/> 단점 : 너무 자주 호출하면 메모리 할당/해제에 따른 성능 저하 발생</li> <li>train :<br/> args.accumulation_steps만큼 loss를 누적한 뒤 backward<br/> args.accumulation_steps마다 gradient 및 measurement 초기화, rank=0 logging</li> <li>validation :<br/> with torch.no_grad(): 로 gradient 누적 안 함!</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">wandb</code> and <code class="language-plaintext highlighter-rouge">distributed</code> finish</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/6-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/6-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/6-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">from importlib import import_module
import torch
import torch.nn as nn
import torch.multiprocessing as mp
from utils import *
from tqdm import tqdm
import wandb

def main():
    args = arg_parse()
    fix_seed(args.random_seed)
    
    # rank=0인 process를 실행하는 system의 IP 주소
    # rank=0인 system이 모든 backend 통신을 설정!
    os.environ['MASTER_ADDR'] = '127.0.0.1'
    # 해당 system에서 사용 가능한 PORT           
    os.environ['MASTER_PORT'] = '8892'

    mp.spawn(main_worker, nprocs=args.num_processes, args=(args,))
    # DDP가 아니라면, main.py에 def main_worker()의 내용을 넣고, train.py에 class Runner 만들자
    '''
    class Runner:
        def __init__(self, args, model):
            self.args = args
            self.model = model
            pass
        def train(self, dataloader, epoch):
            pass
        def validate(self, dataloader, epoch):
            pass
        def test(self, dataloader):
            pass
    '''

def main_worker(process_id, args):
    global best_acc
    best_acc = 0.0

    # 1. model initialize, and set cuda, and parallelize
    model = MyFMANet()

    torch.cuda.set_device(process_id)
    model.cuda(process_id)
    criterion = nn.NLLLoss().cuda(process_id) # criterion = nn.CrossEntropyLoss(reduction='mean').cuda(process_id)

    model = nn.parallel.DistributedDataParallel(model, device_ids=[process_id])

    # 2. wandb init
    if rank == 0:
        wandb.init(project=args.prj_name, name=f"{args.exp_name}", entity="semyeongyu", config=vars(args))
        wandb.watch(model, log='all', log_freq=args.log_interval)

    # 3. optimizer, scheduler initialize
    optimizer = getattr(import_module("torch.optim"), args.optimizer)(model.parameters(), lr=args.lr, betas=(args.b1, args.b2), weight_decay=args.weight_decay)
    scheduler = getattr(import_module("torch.optim.lr_scheduler"), args.lr_scheduler)(optimizer, T_max=args.period, eta_min=0, last_epoch=-1, verbose=True)
    # T_max : 주기 1번 도는 데 걸리는 최대 iter. 수 / eta_min : lr의 최솟값 / last_epoch : 학습 시작할 때의 epoch

    # 4. load checkpoint
    if args.resume_from:
        start_epoch, model, optimizer, scheduler = load_checkpoint(args.checkpoint_path, model, optimizer, scheduler, rank)
    else:
        start_epoch = 0

    # 5. train with barrier
    dist.barrier()

    for epoch in range(start_epoch, args.n_epochs):
        train_sampler.set_epoch(epoch) # train_sampler가 epoch끼리 동일하게 data 분할하는 것을 방지하기 위해

        optimizer.zero_grad() # epoch마다 gradient 초기화

        train_loss = train(train_loader, model, criterion, optimizer, scheduler, epoch, args)

        dist.barrier()

        if rank == 0:
            val_acc, val_loss = validate(val_loader, model, criterion, epoch, args)
            
            # best acc일 때마다 save checkpoint
            if (best_top1 &lt; val_acc):
                best_top1 = val_acc # best_top1은 global var.
                save_checkpoint(
                    {
                        'epoch': epoch,
                        'model': model.state_dict(),
                        'best_top1': best_top1,
                        'optimizer': optimizer.state_dict(),
                        'scheduler': scheduler.state_dict()
                    }, os.path.join(args.checkpoint_dir, args.exp_name), f"{epoch}_{round(best_top1, 4)}.pt"
                )
        
        torch.cuda.empty_cache() 
    
    # 6. wandb and distributed finish
    if rank == 0:
        wandb.run.finish()

    dist.destroy_process_group()
</code></pre> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/7-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/7-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">def train(train_loader, model, criterion, optimizer, scheduler, epoch, args):
    model.train()
    train_acc, train_loss = AverageMeter(), AverageMeter() 
    # measurement of acc and loss

    pbar = tqdm(enumerate(train_loader), total=len(train_loader))
    for step, (x, y_gt) in pbar:
        x, y_gt = x.cuda(non_blocking=True), y_gt.cuda(non_blocking=True) 
        # cuda device에 올려야 함
        # pin_memory=True와 non_blocking=True는 함께 사용

        # forward
        y_pred = model(x)

        # loss divided by accumulation_steps
        loss = criterion(y_pred, y_gt) / args.accumulation_steps

        # gradient 누적
        loss.backward()
        
        # measurement
        train_acc.update(
            topk_accuracy(y_pred.clone().detach(), y_gt).item(), x.size(0))
        train_loss.update(loss.item() * args.accumulation_steps, x.size(0))

        # args.accumulation_steps만큼 loss를 누적한 뒤 평균값으로 backward
        if (step+1) % args.accumulation_steps == 0:
            # gradient clipping
            nn.utils.clip_grad_norm_(model.parameters(), max_norm=args.max_norm)

            # backward
            optimizer.step()
            scheduler.step()

            # gradient 초기화
            optimizer.zero_grad()

            # logging
            dist.barrier()
            if rank == 0:
                # wandb log
                wandb.log(
                    {
                        "Training Loss": round(train_loss.avg, 4),
                        "Training Accuracy": round(train_acc.avg, 4),
                        "Learning Rate": optimizer.param_groups[0]['lr']
                    }
                )

                # tqdm log
                description = f'Epoch: {epoch+1}/{args.n_epochs} || Step: {(step+1)//args.accumulation_steps}/{len(train_loader)//args.accumulation_steps} || Training Loss: {round(train_loss.avg, 4)}'
                pbar.set_description(description)

                # measurement 초기화
                train_loss.init()
                train_acc.init()
    
    return train_loss.avg
</code></pre> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/8-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/8-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">def validate(val_loader, model, criterion, epoch, args):
    model.eval()
    val_acc, val_loss = AverageMeter(), AverageMeter() # measurement of acc and loss

    pbar = tqdm(enumerate(val_loader), total=len(val_loader))
    with torch.no_grad(): # validation은 gradient 누적 안 함!!
        for step, (x, y_gt) in pbar:
            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)
            
            # forward
            y_pred = model(x)
            loss = criterion(y_pred, y_gt)
            
            # measurement
            val_acc.update(topk_accuracy(y_pred.clone().detach(), y_gt).item(), x.size(0)) 
            val_loss.update(loss.item(), x.size(0))

            # tqdm log
            description = f'Epoch: {epoch+1}/{args.n_epochs} || Step: {step+1}/{len(val_loader)} || Validation Loss: {round(loss.item(), 4)} || Validation Accuracy: {round(val_acc.avg, 4)}'
            pbar.set_description(description)

    # wandb log
    wandb.log(
        {
            'Validation Loss': round(val_loss.avg, 4),
            'Validation Accuracy': round(val_acc.avg, 4)
        }
    )

    return val_acc.avg, val_loss.avg
</code></pre> <h3 id="utils">Utils</h3> <ul> <li><code class="language-plaintext highlighter-rouge">argument parser</code></li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/9-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/9-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/9-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">import argparse

def arg_parse():
    parser = argparse.ArgumentParser()

    parser.add_argument("--transforms", type=str, default="BaseTransform")
    parser.add_argument("--crop_size", type=int, default=224)

    args = parser.parse_args()

    return args
</code></pre> <ul> <li><code class="language-plaintext highlighter-rouge">seed</code></li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/10-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/10-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">import random
import torch
import numpy as np

def fix_seed(random_seed):
    torch.manual_seed(random_seed)
    torch.cuda.manual_seed(random_seed)
    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(random_seed)
    random.seed(random_seed)
</code></pre> <ul> <li><code class="language-plaintext highlighter-rouge">checkpoint</code> : dictionary of elements below <ul> <li>epoch</li> <li>model.state_dict()</li> <li>best_acc</li> <li>optimizer.state_dict()</li> <li>scheduler.state_dict()</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/11-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/11-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/11-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">def save_checkpoint(checkpoint, saved_dir, file_name):
    os.makedirs(saved_dir, exist_ok=True)
    output_path = os.path.join(saved_dir, file_name)
    torch.save(checkpoint, output_path) 
    # checkpoint : dictionary

def load_checkpoint(checkpoint_path, model, optimizer, scheduler, rank=-1):
    # checkpoint_path : ".../240325.pt"
    if rank != -1: # 분산학습 yes
        map_location = {"cuda:%d" % 0 : "cuda:%d" % rank}
        checkpoint = torch.load(checkpoint_path, map_location=map_location)
    else: # 분산학습 no
        checkpoint = torch.load(checkpoint_path)

    start_epoch = checkpoint['epoch']

    model.load_state_dict(checkpoint['model'])
    
    if optimizer is not None:
        optimizer.load_state_dict(checkpoint['optimizer'])
    
    if scheduler is not None:
        scheduler.load_state_dict(checkpoint['scheduler'])

    return start_epoch, model, optimizer, scheduler
</code></pre> <ul> <li><code class="language-plaintext highlighter-rouge">augmentation</code></li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/12-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/12-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">import albumentations as A
from albumentations.pytorch.transforms import ToTensorV2

class BaseTransform(object):
    def __init__(self, crop_size = 224):
        self.transform = A.Compose(
            [   
                A.RandomResizedCrop(crop_size, crop_size),
                A.HorizontalFlip(),
                A.Normalize(),
                ToTensorV2() 
# albumentations에서는 normalize 이후에 ToTensorV2를 사용해줘야 함 (여기서 어차피 shape (C,H,W)로 변경)
            ]
        )

    def __call__(self, img):
# BaseTransform()은 nn.Module을 상속한 게 아니므로 forward를 구현해도 __call__과 연결되어 있지 않음
# 따라서 __call__()을 직접 구현해줘야 함
        return self.transform(image=img)
</code></pre> <ul> <li><code class="language-plaintext highlighter-rouge">measurement</code></li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/13-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/13-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/13-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">class AverageMeter(object):
    def __init__(self):
        self.init()
    
    def init(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val*n
        self.count += n
        self.avg = self.sum / self.count

def topk_accuracy(pred, gt, k=1):
    # pred : shape (N, class)
    # gt : shape (N,)
    _, pred_topk = pred.topk(k, dim=1)
    n_correct = torch.sum(pred_topk.squeeze() == gt)

    return n_correct / len(gt)
</code></pre> <h3 id="multi-attention">Multi-Attention</h3> <ul> <li>FMA-Net (2024) <d-cite key="FMANet">[1]</d-cite>의 Multi-Attention 구현<br/> 출처 : <a href="https://github.com/KAIST-VICLab/FMA-Net">FMA-Net Code</a></li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/2-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/2-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Multi-Attention :</p> <ul> <li><code class="language-plaintext highlighter-rouge">CO(center-oriented)</code> attention :<br/> better align \(\tilde F_{w}^{i}\) to \(F_{c}^{0}\) (center feature map of initial temporally-anchored feature)</li> <li><code class="language-plaintext highlighter-rouge">DA(degradation-aware)</code> attention :<br/> \(\tilde F_{w}^{i}\) becomes globally adaptive to spatio-temporally variant degradation by using degradation kernels \(K^{D}\)</li> </ul> </blockquote> <ul> <li> <p>CO attention :<br/> \(Q=W_{q} F_{c}^{0}\)<br/> \(K=W_{k} \tilde F_{w}^{i}\)<br/> \(V=W_{v} \tilde F_{w}^{i}\)<br/> \(COAttn(Q, K, V) = softmax(\frac{QK^{T}}{\sqrt{d}})V\)<br/> 실험 결과, \(\tilde F_{w}^{i}\)가 자기 자신(self-attention)이 아니라 \(F_{c}^{0}\)과의 relation에 집중할 때 better performance</p> </li> <li> <p>DA attention :<br/> CO attention과 비슷하지만,<br/> Query 만들 때 \(F_{c}^{0}\) 대신 \(k^{D, i}\) 사용<br/> \(\tilde F_{w}^{i}\) becomes globally adaptive to spatio-temporally-variant degradation<br/> \(k^{D, i} \in R^{H \times W \times C}\) : degradation features adjusted by conv. with \(K^{D}\) (motion-aware spatio-temporally-variant degradation kernels) 에 대해<br/> \(Q=W_{q} k^{D, i}\)<br/> DA attention은 \(Net^{D}\) 말고 \(Net^{R}\) 에서만 사용</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">nn.Conv2d()</code> :</p> <ul> <li> \[H_{out} = \lfloor 1 + \frac{H_{in} + 2 \times pad - dilation \times (K-1) - 1}{stride} \rfloor\] </li> <li>argument : <ul> <li>groups :<br/> shape (\(C_{in}\), \(C_{out}\), K, K) 대신 \(C_{in}\), \(C_{out}\) 을 groups-개로 쪼개서<br/> shape (\(\frac{C_{in}}{groups}\), \(\frac{C_{out}}{groups}\), K, K)를 groups-번 실행하여 concat</li> </ul> </li> <li>variable : <ul> <li>weight : shape (\(C_{out}\), \(\frac{C_{in}}{groups}\), K, K)</li> <li>bias : shape (\(C_{out}\),)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/14-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/14-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/14-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-07-08-PytorchBasic/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <pre><code class="language-Python">import torch
import torch.nn as nn

class Attention(nn.Module):
    # Restormer (CVPR 2022) transposed-attention block
    # original source code: https://github.com/swz30/Restormer
    def __init__(self, dim, n_head, bias):
        super(Attention, self).__init__()
        self.n_head = n_head # multi-head for channel dim.
        self.temperature = nn.Parameter(torch.ones(n_head, 1, 1)) 
        # multi-head 별로 scale factor를 parameterize

        # W_q
        self.q = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)
        self.q_dwconv = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, groups=dim, bias=bias)

        # W_kv
        self.kv_conv = nn.Conv2d(dim, dim*2, kernel_size=1, bias=bias)
        self.kv_dwconv = nn.Conv2d(dim*2, dim*2, kernel_size=3, stride=1, padding=1, groups=dim*2, bias=bias)

        # W_o
        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)

    def forward(self, x, f):
        # first input x : shape (N, C, H, W) -&gt; makes key and value
        # second input f : shape (N, C, H, W) -&gt; makes query
        N, C, H, W = x.shape

        # Apply W_q and W_kv
        q = self.q_dwconv(self.q(f)) # query q : shape (N, C, H, W)
        kv = self.kv_dwconv(self.kv_conv(x)) # kv : shape (N, 2*C, H, W)
        k, v = kv.chunk(2, dim=1) # key k and value v : shape (N, C, H, W)

        # Multi-Head Attention
        q = einops.rearrange(q, 'b (head c) h w -&gt; b head c (h w)', head=self.n_head)
        # query q : shape (N, C, H, W) -&gt; shape (N, M, C/M, H * W)
        k = einops.rearrange(k, 'b (head c) h w -&gt; b head c (h w)', head=self.n_head)
        # key k : shape (N, C, H, W) -&gt; shape (N, M, C/M, H * W)
        v = einops.rearrange(v, 'b (head c) h w -&gt; b head c (h w)', head=self.n_head)
        # value v : shape (N, C, H, W) -&gt; shape (N, M, C/M, H * W)

        # matrix mul.을 할 spatial dim.을 normalize
        q = torch.nn.functional.normalize(q, dim=-1)
        k = torch.nn.functional.normalize(k, dim=-1)

        '''
        - q @ k.transpose(-2, -1) = similarity :
          shape (N, M, C/M, C/M)
        - self.temperature = scale factor for each head :
          shape (M, 1, 1) -&gt; shape (N, M, C/M, C/M) 
        '''
        attn = (q @ k.transpose(-2, -1)) * self.temperature 
        attn = attn.softmax(dim=-1) # convert to probability distribution

        out = (attn @ v) # shape (N, M, C/M, H*W)
        
        # Multi-Head Attention - concatenation
        out = einops.rearrange(out, 'b head c (h w) -&gt; b (head c) h w', head=self.n_head, h=H, w=W) 
        # shape (N, C, H, W)

        # Apply W_o
        out = self.project_out(out) # shape (N, C, H, W)

        return out

class LayerNorm(nn.Module):
    def __init__(self, normalized_shape):
        super(LayerNorm, self).__init__()
        
        # learnable param.
        self.weight = nn.Parameter(torch.ones(normalized_shape)) # shape (C,)
        self.bias = nn.Parameter(torch.zeros(normalized_shape)) # shape (C,)
    
    def forward(self, x):
        # x : shape (N, C, H, W)
        # LayerNorm : dim. C에 대해 normalize
        mu = x.mean(1, keepdim=True)
        sigma = x.var(1, keepdim=True, unbiased=False)
        return (x - mu) / torch.sqrt(sigma + 1e-5) * self.weight + self.bias

class MultiAttentionBlock(nn.Module):
    def __init__(self, dim, n_head, ffn_expansion_factor, bias, is_DA):
        super(MultiAttentionBlock, self).__init__()
        self.norm1 = LayerNorm(dim)
        # center-oriented attention
        self.co_attn = Attention(dim, n_head, bias) 
        self.norm2 = LayerNorm(dim)
        self.ffn1 = FeedForward(dim, bias)

        if is_DA:
            self.norm3 = LayerNorm(dim)
            # degradation-aware attention
            self.da_attn = Attention(dim, n_head, bias) 
            self.norm4 = LayerNorm(dim)
            self.ffn2 = FeedForward(dim, bias)

    def forward(self, Fw, F0_c, Kd):
        Fw = Fw + self.co_attn(self.norm1(Fw), F0_c)
        Fw = Fw + self.ffn1(self.norm2(Fw))

        if Kd is not None:
            Fw = Fw + self.da_attn(self.norm3(Fw), Kd)
            Fw = Fw + self.ffn2(self.norm4(Fw))

        return Fw
</code></pre>]]></content><author><name></name></author><category term="cv-tasks"/><category term="pytorch"/><summary type="html"><![CDATA[Dataset, DataLoader, Train, Attention, ...]]></summary></entry></feed>