<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://semyeong-yu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://semyeong-yu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-06T08:26:58+00:00</updated><id>https://semyeong-yu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">NoPoSplat</title><link href="https://semyeong-yu.github.io/blog/2025/NoPoSplat/" rel="alternate" type="text/html" title="NoPoSplat"/><published>2025-02-03T10:00:00+00:00</published><updated>2025-02-03T10:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/NoPoSplat</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/NoPoSplat/"><![CDATA[<h2 id="no-pose-no-problem---surprisingly-simple-3d-gaussian-splats-from-sparse-unposed-images">No Pose, No Problem - Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images</h2> <h4 id="botao-ye-sifei-liu-haofei-xu-xueting-li-marc-pollefeys-ming-hsuan-yang-songyou-peng">Botao Ye, Sifei Liu, Haofei Xu, Xueting Li, Marc Pollefeys, Ming-Hsuan Yang, Songyou Peng</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2410.24207">https://arxiv.org/abs/2410.24207</a><br/> project website :<br/> <a href="https://noposplat.github.io/">https://noposplat.github.io/</a><br/> code :<br/> <a href="https://github.com/cvg/NoPoSplat">https://github.com/cvg/NoPoSplat</a></p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/4.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/4.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="contribution">Contribution</h2> <p><code class="language-plaintext highlighter-rouge">pose-free generalizable sparse-view 3D recon. model in canonical Gaussian space!</code></p> <ul> <li>model : <ul> <li><code class="language-plaintext highlighter-rouge">unposed</code> (no extrinsic) <code class="language-plaintext highlighter-rouge">sparse-view</code> images로부터 3DGS를 통해 3D scene recon.하는 feed-forward network 제시</li> <li><code class="language-plaintext highlighter-rouge">photometric loss만으로</code> train 가능<br/> (<code class="language-plaintext highlighter-rouge">GT depth 사용 X</code>, explicit matching loss 사용 X)</li> <li>본 논문은 intrinsic의 영향을 받는 image appearance에만 의존하여 recon.을 수행하므로<br/> <code class="language-plaintext highlighter-rouge">scale ambiguity</code> 문제 해결을 위해 <code class="language-plaintext highlighter-rouge">intrinsic embedding method</code> 사용<br/> (intrinsic은 input으로 사용)</li> <li>covariance, opacity, color를 예측하는 Gaussian Param. Head에서 fine texture detail 주기 위해 <code class="language-plaintext highlighter-rouge">RGB shortcut</code> 사용</li> </ul> </li> <li>downstream tasks : <ul> <li>recon.된 3DGS를 이용하여 novel-view-synthesis 및 pose-estimation task 수행 가능 <ul> <li>특히 limited input image overlap (sparse) 상황에서는 pose-required methods보다 더 좋은 성능</li> <li>정확히 pose-estimation 수행하는 two-stage coarse-to-fine pipeline 제시</li> </ul> </li> <li>generalize well to out-of-distribution data</li> </ul> </li> <li>Gaussian Space : <ul> <li><code class="language-plaintext highlighter-rouge">first input view의 local camera coordinate</code>을 <code class="language-plaintext highlighter-rouge">canonical space</code>로 고정하고 모든 input view의 3DGS들을 해당 space에서 directly 예측</li> <li>기존에는 transform-then-fuse pipeline이었는데,<br/> 본 논문은 global coordinate으로의 <code class="language-plaintext highlighter-rouge">explicit transform 없이</code> canonical space 내에서의 different views의 fusion 자체를 직접 network로 학습</li> <li>local coordinate에서 global coordinate으로 3DGS를 explicitly transform할 필요가 없으므로<br/> explicitly transform하면서 생기는 per-frame Gaussians의 misalignment를 방지할 수 있고, extrinsic pose 없이도 (pose-free) 3D recon. 가능</li> </ul> </li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li>SfM : <ul> <li>bundle adjustment 등 최적화 과정을 거치는데,<br/> off-the-shelf pose estimation method 사용하는 것 자체가 많은 연산을 필요로 하고 runtime 늘림</li> <li>3D recon.에 only two frames만 input으로 사용하더라도<br/> SfM을 통해 해당 two frames의 camera pose를 구하려면 many poses from dense videos 필요 (impractical)</li> <li>textureless area (원형 호수 등) 또는 image가 sparse한 영역에서는 잘 못 함</li> </ul> </li> <li>Pose-Free Method : <ul> <li>pose-estimation과 3D recon.을 single pipeline으로 통합하자! : <d-cite key="DBARF">[1]</d-cite>, <d-cite key="Flowcam">[2]</d-cite>, <d-cite key="Unifying">[3]</d-cite> <ul> <li>pose-estimation과 scene-recon.을 번갈아가며 수행하는 sequential process 에서 error가 쌓이기 때문에<br/> SOTA novel-view-synthesis methods보다 성능 bad</li> </ul> </li> <li>DUSt3R, MASt3R 계열</li> </ul> </li> <li>DUSt3R, MASt3R : <ul> <li>공통점 1)<br/> pose-free method</li> <li>공통점 2)<br/> directly predict in canonical space</li> <li>차이점 1)<br/> DUSt3R, MASt3R는 transformer output이 3D pointmap (point cloud)인데,<br/> NoPoSplat은 mean, covariance, opacity, color를 가진 3DGS (rasterization) 사용</li> <li>차이점 2)<br/> NoPoSplat은 DUSt3R, MASt3R 계열과 달리 <code class="language-plaintext highlighter-rouge">GT depth 필요 없고 photometric loss만으로</code> 훈련 가능</li> </ul> </li> <li>pixelSplat, MVSplat : <ul> <li>차이점 1) (아래 그림 참고)<br/> pixelSplat, MVSplat은 먼저 intrinsic을 이용해 2D-to-3D로 unproject(lift)하여 each local-coordinate에서 3DGS를 예측한 뒤 extrinsic을 이용해 world-coordinate으로 transform한 뒤 fuse했는데,<br/> NoPoSplat은 canonical space 내에서의 different views의 fusion 자체를 directly network로 학습하기 때문에 <code class="language-plaintext highlighter-rouge">global coordinate으로 transform할 필요가 없으므로</code> 이에 따른 <code class="language-plaintext highlighter-rouge">misalignment를 방지</code>할 수 있고 <code class="language-plaintext highlighter-rouge">camera pose (extrinsic)도 필요 없음</code></li> <li>차이점 2)<br/> pixelSplat에선 epipolar constraint, MVSplat에선 cost volume이라는 geometry prior를 사용하였는데<br/> image view overlap이 적을 때는 geometry prior가 정확하지 않음.<br/> NoPoSplat은 (image overlap이 클 때 유리한) <code class="language-plaintext highlighter-rouge">geometry prior들을 사용하지 않음</code></li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/2.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/2.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="method">Method</h2> <h3 id="architecture">Architecture</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/3.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/3.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>I/O :<br/> \(f_{\theta} : \left\{ (I^{v}, k^{v}) \right\}_{v=1}^{V} \mapsto \left\{ \bigcup (\mu_{j}^{v}, \alpha_{j}^{v}, r_{j}^{v}, s_{j}^{v}, c_{j}^{v}) \right\}_{j=1, \ldots, H \times W}^{v=1, \ldots, V}\) <ul> <li>input : <ul> <li>sparse unposed multi-view images \(I\) (image 개수 \(V\))</li> <li>camera intrinsics \(k\) (available from modern devices <d-cite key="intrinsic">[4]</d-cite>)</li> </ul> </li> <li>output : <ul> <li>mean \(\mu \in R^{3}\), opacity \(\alpha \in R\), rotation \(r \in R^{4}\), scale \(s \in R^{3}\), SH \(c \in R^{k}\) (\(k\) degrees of freedom)</li> </ul> </li> </ul> </li> <li>Pipeline : <ul> <li><code class="language-plaintext highlighter-rouge">Encoder, Decoder</code> : <ul> <li>특히 input views끼리 content overlap이 적은 상황 (sparse) 에서는<br/> epipolar constraint나 cost volume 같은 geometry prior가 없더라도<br/> simple ViT 구조만으로도 좋은 성능 달성 가능</li> <li>RGB images를 image tokens로 patchify, flatten한 뒤<br/> intrinsic token과 concatenate한 뒤<br/> Encoder and Decoder에 feed-forward</li> </ul> </li> <li>Gaussian Parameter Prediction Head :<br/> DPT 구조 <ul> <li><code class="language-plaintext highlighter-rouge">Gaussian Center Head</code> :<br/> Decoder feature 사용</li> <li><code class="language-plaintext highlighter-rouge">Gaussian Param Head</code> :<br/> RGB image와 Decoder feature 사용 <ul> <li><code class="language-plaintext highlighter-rouge">RGB shortcut</code> :<br/> 3D recon.에서 fine texture detail을 잡는 것이 중요하기 때문에 사용</li> <li>Decoder feature :<br/> high-level semantic info.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <h3 id="gaussian-space">Gaussian Space</h3> <ul> <li>baseline: <code class="language-plaintext highlighter-rouge">Local-to-Global Gaussian Space</code> <ul> <li>pixelSplat, MVSplat 등</li> <li>how :<br/> 먼저 each pixel의 depth를 network로 예측한 뒤<br/> predicted depth와 intrinsic을 이용해 2D-to-3D로 unproject(lift)하여 each local-coordinate에서 3DGS 예측한 뒤<br/> extrinsic을 이용해 world-coordinate으로 transform한 뒤<br/> 모든 transformed 3DGS들을 fuse</li> <li>issue : <ul> <li>local-coordinate에서 world-coordinate으로 transform할 때 <code class="language-plaintext highlighter-rouge">accurate camera pose</code> (extrinsic) 필요한데, 이는 input view가 sparse한 real-world 상황에서 얻기 어렵</li> <li>특히 input view가 sparse할 때 또는 out-of-distribution data로 일반화할 때는<br/> <code class="language-plaintext highlighter-rouge">each transformed 3DGS들을 조화롭게 combine</code>하는 게 어렵</li> </ul> </li> </ul> </li> <li>NoPoSplat: <code class="language-plaintext highlighter-rouge">Canonical Gaussian Space</code> <ul> <li>how :<br/> first input view를 global referecne coordinate으로 고정한 뒤 (\([R | t] = [\boldsymbol I | \boldsymbol 0]\))<br/> 해당 coordinate 내에서 each input view \(v\) 마다 set \(\left\{ \mu_{j}^{v \rightarrow 1}, r_{j}^{v \rightarrow 1}, c_{j}^{v \rightarrow 1}, \alpha_{j}, s_{j} \right\}\) 을 예측<br/> where view \(1\) : canonical Gaussian space</li> <li>benefit : <ul> <li>global coordinate으로 explicitly transform할 필요가 없으므로 camera pose (extrinsic) 필요 없음</li> <li>explicitly transform-then-fuse하는 게 아니라 fuse 자체를 network로 학습하는 것이기 때문에<br/> 조화로운 global representation 가능</li> </ul> </li> </ul> </li> </ul> <h3 id="camera-intrinsic-embedding">Camera Intrinsic Embedding</h3> <ul> <li>Camera Intrinsic Embedding : <ul> <li>issue :<br/> only appearance에만 의존하여 3D recon.을 수행함<br/> <code class="language-plaintext highlighter-rouge">scale ambiguity</code> (scale misalignment) 문제 해결 필요!<br/> 필요한 geometric info.를 제공하기 위해!<br/> intrinsic \(k = [f_{x}, f_{y}, c_{x}, c_{y}]\)</li> <li>solve : <ul> <li>Trial 1) Global Intrinsic Embedding by Addition :<br/> intrinsic \(k\) 을 linear layer에 통과시킨 뒤 RGB image token에 add</li> <li>Trial 2) Global Intrinsic Embedding by Concat :<br/> intrinsic \(k\) 을 linear layer에 통과시킨 뒤 RGB image token에 concat</li> <li>Trial 3) Pixel-wise (Dense) Intrinsic Embedding :<br/> each pixel \(p_{j}\)에 대해 ray direction \(K^{-1} p_{j}\) 구한 뒤<br/> SH 이용해서 high-dim. feature로 변환한 뒤<br/> RGB image와 concat</li> </ul> </li> </ul> </li> </ul> <h3 id="training-and-inference">Training and Inference</h3> <ul> <li> <p>Loss :<br/> only photometric loss<br/> (linear comb. of MSE and LPIPS)</p> </li> <li>Relative Pose Estimation :<br/> canonical space에 3DGS들이 있다는 전제 하에<br/> <code class="language-plaintext highlighter-rouge">two-stage coarse-to-fine pipeline</code> <ul> <li>Coarse Stage :<br/> Gaussian center에 <code class="language-plaintext highlighter-rouge">PnP algorithm with RANSAC</code> (efficient as done in ms) 적용하여<br/> <code class="language-plaintext highlighter-rouge">initial rough pose estimate</code> 구하기</li> <li>Fine Stage :<br/> <code class="language-plaintext highlighter-rouge">3DGS param.을 freeze</code>한 채<br/> training에 사용했던 <code class="language-plaintext highlighter-rouge">photometric loss</code>를 이용해<br/> target view와 align되도록 rough <code class="language-plaintext highlighter-rouge">target camera pose를 optimize</code>(refine) <ul> <li>automatic diff.에서의 overhead를 줄이기 위해<br/> camera Jacobian을 계산 <d-cite key="GSslam">[5]</d-cite></li> </ul> </li> </ul> </li> <li>Evaluation-Time Pose Alignment : <ul> <li>unposed input images의 경우<br/> scene은 다른데 rendered two images는 같을 수 있으므로<br/> just two input views로 3D scene recon. 수행하는 건 사실 ambiguous</li> <li>GT camera pose를 이용하는 other baseline들 <d-cite key="pose1">[6]</d-cite>, <a href="https://semyeong-yu.github.io/blog/2024/pixelSplat/">7</a>과 비교하기 위해 (evaluation purpose)<br/> pose-free methods <d-cite key="nopose1">[8]</d-cite>, <d-cite key="nopose2">[9]</d-cite>의 경우 target view에 대한 camera pose를 optimize한 뒤 비교에 사용</li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <h3 id="implementation">Implementation</h3> <ul> <li>Experiment : <ul> <li>Dataset : <ul> <li>training :<br/> RE10K (RealEstate10k) : indoor real estate<br/> DL3DV : outdoor (camera motion pattern 더 다양)</li> <li>zero-shot generalization :<br/> ACID : nature scene by drone<br/> DTU<br/> ScanNet<br/> ScanNet++<br/> in-the-whild mobile phone capture<br/> SORA-generated images</li> </ul> </li> <li>camera overlap :<br/> SOTA dense feature matching method <d-cite key="ROMA">[10]</d-cite> 로<br/> input images’ camera overlap 정도를 측정하여<br/> small (0.05%-0.3%), medium (0.3%-0.55%), large (0.55%-0.8%)로 나눔</li> <li>Baseline : <ul> <li>pose-required novel-view-synthesis :<br/> pixelNeRF, AttnRend, pixelSplat, MVSplat</li> <li>pose-free novel-view-synthesis and relative pose estimation :<br/> DUSt3R, MASt3R, Splatt3R, CoPoNeRF, RoMa</li> </ul> </li> <li>Implementation :<br/> encoder, decoder, Gaussian center head는 MASt3R의 weights로 initialize하고<br/> (사실 scratch부터 training해도 성능 비슷하긴 함)<br/> Gaussian param head는 randomly initialize</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/5.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/5.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/5.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/5.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="result">Result</h3> <ul> <li>Novel View Synthesis : <ul> <li>SOTA pose-free (DUSt3R, MASt3R, Splatt3R) : <ul> <li>DUSt3R 계열은 <code class="language-plaintext highlighter-rouge">per-pixel depth loss</code>에 의존하기 때문에 each views를 <code class="language-plaintext highlighter-rouge">fuse하는 게 어렵</code><br/> 그래서 대부분 상황에서 NoPoSplat이 훨씬 더 좋음</li> </ul> </li> <li>SOTA pose-required (pixelSplat, MVSplat) : <ul> <li>pixelSplat, MVSplat은 <code class="language-plaintext highlighter-rouge">input view overlap이 작을 때 부정확한 geometry prior</code> (epipolar constraint, cost volume)을 사용하기 때문에<br/> image view overlap이 작은 상황에서는 NoPoSplat이 더 좋음</li> <li>pixelSplat, MVSplat은 <code class="language-plaintext highlighter-rouge">transform-then-fuse strategy</code>를 사용하는데 <code class="language-plaintext highlighter-rouge">misalignment</code>로 부정확할 수 있기 때문에<br/> canonical space에서 directly 예측하는 NoPoSplat이 더 좋을 수 있음</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/6.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/6.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/6.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/6.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/7.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/7.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/7.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/7.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Relative Pose Estimation :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/8.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/8.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/8.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/8.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Geometry Reconstruction : <ul> <li>SOTA pose-required (pixelSplat, MVSplat) : <ul> <li>pixelSplat, MVSplat은 explicitly transform-then-fuse하는 과정에서 두 input images의 경계 영역에서 misalignment (아래 그림에서 파란색 화살표로 표기) 가 있고,<br/> input views’ overlap이 적을 때는 geometry prior가 부정확해서 distortion (아래 그림에서 분홍색 화살표로 표기) 있는데,<br/> NoPoSplat은 canonical space에서 directly 예측하므로 해결</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/9.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/9.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/9.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/9.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Cross-Dataset Generalization :<br/> NoPoSplat은 geometry prior를 사용하지 않으므로 다양한 scene type에 adapt 가능<br/> 심지어 ScanNet++로의 zero-shot generalization에 대해 RE10K로 훈련시킨 NoPoSplat과 ScanNet++로 훈련시킨 pose-required Splatt3R을 비교했을 때 NoPoSplat이 더 좋음!</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/10.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/10.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/10.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/10.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Model Efficiency :<br/> NoPoSplat은 0.015초만에 (66 FPS) 3DGS 예측 가능<br/> (additional geometry prior 안 쓰니까 speed 빠름!)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/11.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/11.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/11.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/11.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> inference on RTX 4090 GPU </div> <ul> <li>In-the-Wild Unposed Images :<br/> 3D Generation task에 적용 가능!<br/> 먼저 text/image to multi-image/video model 이용해서 sparse scene-level multi-view images 얻은 뒤<br/> Ours (NoPoSplat) 이용해서 3D model 얻음</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/13.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/13.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/13.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/13.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="ablation-study">Ablation Study</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/12.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/12.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/12.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-02-03-NoPoSplat/12.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Ablation Study : <ul> <li><code class="language-plaintext highlighter-rouge">Output Canonical Gaussian Space</code> :<br/> transform-then-fuse pipeline of pose-required methods has <code class="language-plaintext highlighter-rouge">ghosting artifacts</code></li> <li><code class="language-plaintext highlighter-rouge">Camera Intrinsic Embedding</code> :<br/> no intrinsic leads to <code class="language-plaintext highlighter-rouge">blurry</code> results due to <code class="language-plaintext highlighter-rouge">scale ambiguity</code><br/> 실험적으로 intrinsic token concat. 방식이 best</li> <li><code class="language-plaintext highlighter-rouge">RGB Shortcut</code> :<br/> no RGB Shortcut leads to <code class="language-plaintext highlighter-rouge">blurry</code> results in texture-rich areas<br/> (위 그림의 quilt in row 1 and chair in row 3)</li> <li><code class="language-plaintext highlighter-rouge">3 Input Views</code> instead of 2 :<br/> baselines과의 공평한 비교를 위해 NoPoSplat은 two input-views setting을 사용했는데<br/> three input-views를 사용할 경우 성능이 훨씬 좋아졌음!</li> </ul> </li> </ul> <h2 id="conclusion">Conclusion</h2> <ul> <li> <p>Future Work :<br/> NoPoSplat은 static scene에만 적용했는데, dynamic scene에 NoPoSplat의 pipeline을 확장 적용!</p> </li> <li> <p>Limitation :</p> <ul> <li><code class="language-plaintext highlighter-rouge">camera intrinsic은 known</code>이라는 걸 가정!</li> <li>feed-forward model은 <code class="language-plaintext highlighter-rouge">non-generative</code>하므로 <code class="language-plaintext highlighter-rouge">unseen region</code>에는 대응 못 함</li> <li><code class="language-plaintext highlighter-rouge">static scene</code>에 적용</li> </ul> </li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br/> 사실 NoPoSplat은 camera pose 이용한 global coordinate으로의 explicit transform이나 geometry prior (epopiolar constraint, cost volume 등)나 GT depth 없이<br/> 오로지 implicit network의 학습에 의존하여 scene recon. 능력을 학습하겠다는 건데<br/> photometric loss만으로도 잘 학습이 되나? two input images 경계면의 smoothness 등 추가 regularization loss 추가해주는 게 낫지 않음?</p> </li> <li> <p>A1 :<br/> TBD</p> </li> <li> <p>Q2 :<br/> photometric loss에만 의존하기 때문에 ViT semantic info. 말고도 more info. 주기 위해 intrinsic과 RGB shortcut을 사용하는데<br/> 둘 말고 또 추가하면 좋은 거 있을까?</p> </li> <li> <p>A2 :<br/> TBD</p> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="dynamic"/><category term="GS"/><category term="SfMfree"/><summary type="html"><![CDATA[Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images (ICLR 2025)]]></summary></entry><entry><title type="html">MonST3R</title><link href="https://semyeong-yu.github.io/blog/2025/MonST3R/" rel="alternate" type="text/html" title="MonST3R"/><published>2025-01-22T10:00:00+00:00</published><updated>2025-01-22T10:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/MonST3R</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/MonST3R/"><![CDATA[<h2 id="monst3r---a-simple-approach-for-estimating-geometry-in-the-presence-of-motion">MonST3R - A Simple Approach for Estimating Geometry in the Presence of Motion</h2> <h4 id="junyi-zhang-charles-herrmann-junhwa-hur-varun-jampani-trevor-darrell-forrester-cole-deqing-sun-ming-hsuan-yang">Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, Ming-Hsuan Yang</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2410.03825">https://arxiv.org/abs/2410.03825</a><br/> project website :<br/> <a href="https://monst3r-project.github.io/">https://monst3r-project.github.io/</a></p> </blockquote> <h2 id="contribution">Contribution</h2> <p><code class="language-plaintext highlighter-rouge">static scene에 사용됐던 DUSt3R를 dynamic scene에 확장한 버전!</code></p> <ul> <li>geometry-first approach that <code class="language-plaintext highlighter-rouge">directly</code> estimates <code class="language-plaintext highlighter-rouge">per-timestep geometry (pointmap)</code> of <code class="language-plaintext highlighter-rouge">dynamic</code> scene <ul> <li>이전까지의 논문들은 <d-cite key="GaussianMarbles">[1]</d-cite>, <d-cite key="TrackRecon">[2]</d-cite>, <d-cite key="Kumar">[3]</d-cite>, <d-cite key="Barsan">[4]</d-cite>, <d-cite key="Mustafa">[5]</d-cite>, <d-cite key="Lei">[6]</d-cite>, <d-cite key="Chu">[7]</d-cite>, <d-cite key="Wangb">[8]</d-cite>, <d-cite key="Wanga">[9]</d-cite>, <d-cite key="Liu">[10]</d-cite> 처럼<br/> depth, optical flow, trajectory estimation을 사용하는 subtasks로 쪼갠 뒤<br/> global optimization 또는 multi-stage pipeline 등으로 합치는<br/> complex system을 쓰는데,<br/> 이는 보통 느리고, 다루기 힘들고, prone-to-error at each step</li> <li>이전까지의 논문들은 motion과 geometry를 함께 사용하여 dynamic scene을 다뤘는데,<br/> motion, depth label, camera pose 정보가 있는 GT dynamic video data는 거의 없다<br/> (그래서 다른 model(prior)를 쓰는데, 이는 부정확성이 쌓일 수 있음)</li> <li>대신 본 논문은<br/> limited data로 only DUSt3R의 decoder and head만 fine-tuning하여 (<code class="language-plaintext highlighter-rouge">small-scale fine-tuning</code>)<br/> <code class="language-plaintext highlighter-rouge">explicit motion representation 없이</code><br/> only <code class="language-plaintext highlighter-rouge">geometry</code> (pointmap)를 <code class="language-plaintext highlighter-rouge">directly</code> 예측하는 pipeline 제시!</li> <li>each timestep마다 DUSt3R 방식으로 pointmap (geometry) 예측한 뒤<br/> 같은 camera coordinate frame (global pointmap)에 대해 <code class="language-plaintext highlighter-rouge">3D align</code></li> <li>downstream tasks :<br/> 예측한 pointmap (geometry) 를 바탕으로<br/> feed-forward 4D reconstruction 뿐만 아니라<br/> video depth estimation, camera pose estimation, video segmentation 등<br/> 여러 downstream video-specific tasks에 적용</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Final Loss</code> :<br/> \(\hat X = \text{argmin}_{X, P_{W}, \sigma} L_{align} (X, \sigma, P_{W}) + w_{smooth} L_{smooth} (X) + w_{flow} L_{flow} (X)\) <ul> <li>세 가지 loss :<br/> 3D alignment loss \(L_{align}\), camera trajectory smoothness loss \(L_{smooth}\), flow projection loss \(L_{flow}\)</li> <li>learnable param. :<br/> <code class="language-plaintext highlighter-rouge">global pointmap</code> \(X\), global pointmap으로의 <code class="language-plaintext highlighter-rouge">3D alignment transformation</code> \(P_{W}\), <code class="language-plaintext highlighter-rouge">scale factor</code> \(\sigma\) 를 업데이트하는데,<br/> 얘네는 본질적으로 re-parameterization에 의해 <code class="language-plaintext highlighter-rouge">depthmap</code> \(\hat D\), <code class="language-plaintext highlighter-rouge">extrinsic</code> \(\hat P\), <code class="language-plaintext highlighter-rouge">intrinsic</code> \(\hat K\) 로 구성되어 있음<br/> 즉, MonST3R는 결국 jointly optimize video depthmap and camera pose (extrinsic, intrinsic)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/2.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/2.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="related-works">Related Works</h2> <ul> <li>DUSt3R :<br/> DUSt3R를 바로 dynamic scene에 적용할 경우 두 가지 한계 발생 <ul> <li>문제 1) (static scene인 것처럼) fg object에 align하여 bg가 misaligned<br/> DUSt3R는 static scene으로만 학습됐기 때문에<br/> dynamic scene의 pointmaps를 알맞게 align하지 못하여<br/> moving fg object가 가만히 있는 것처럼 align되고<br/> static bg element는 misaligned</li> <li>문제 2) fg object의 geometry(depth)를 잘 예측하지 못하여 fb object를 bg에 둠</li> <li>해결)<br/> domain mismatch이므로 다시 train!<br/> 본 논문은 limited data를 최대한 사용하여 small-scale fine-tuning하는 training strategy 제시</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/3.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/3.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>motion mask :<br/> DUSt3R는 static scene으로 훈련되었기 때문에 dynamic scene에 적용하기 위해<br/> GT motion mask를 사용할 수도 있다 <ul> <li>inference할 때<br/> image의 dynamic region은 black pixels로 대체하고<br/> corresponding tokens는 mask tokens로 대체하여<br/> dynamic objects를 masking out 할 수도 있는데,<br/> black pixels와 mask tokens는 out-of-distribution w.r.t training 이므로<br/> pose estimation 결과가 안 좋아짐</li> <li>본 논문은 그렇게 무작정 dynamic region을 mask out 하지 않고 이 문제 해결!</li> </ul> </li> </ul> <h2 id="architecture">Architecture</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/1.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/1.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/1.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/1.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="method">Method</h2> <h3 id="main-idea">Main Idea</h3> <p>DUSt3R의 아이디어를 그대로 가져오고,<br/> DUSt3R의 각 output pointmap \(X^{t} \in R^{W \times H \times 3}\) 이 time 정보 \(t\) 를 가지고 있음</p> <h3 id="training-dataset">Training Dataset</h3> <p>real-world dynamic scene은 보통 GT camera pose를 가지고 있지 않으므로<br/> SfM 등 sensor measurement 또는 post-processing을 통해 추정하는데<br/> 이는 부정확할 수 있고 costly하므로<br/> 본 논문은 GT camera pose, depth 정보를 알 수 있는 synthetic datasets를<br/> dynamic fine-tuning을 위한 training dataset으로 사용</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/4.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/4.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Training Dataset for Dynamic Fine-Tuning :<br/> PointOdyssey는 dynamic objects 많아서 많이 사용하고<br/> TartanAir는 static scene이라서 적게 사용하고<br/> Waymo는 specialized domain이라서 적게 사용 <ul> <li>3 synthetic datasets : <ul> <li>PointOdyssey (Zheng et al.)</li> <li>TartanAir (Wang et al.)</li> <li>Spring (Mehl et al.)</li> </ul> </li> <li>1 real-world dataset : <ul> <li>Waymo (Sun et al.) with LiDAR</li> </ul> </li> </ul> </li> </ul> <h3 id="training-strategy">Training Strategy</h3> <p>dataset이 small-scale이므로<br/> data efficiency를 극대화시키기 위해<br/> 다양한 training techniques 사용</p> <ul> <li>Training Strategies : <ul> <li>전략 1)<br/> encoder는 freeze한 뒤<br/> network의 decoder와 prediction head만 fine-tune<br/> (encoder(CroCo)의 geometric knowledge는 유지)</li> <li>전략 2)<br/> each video마다 temporal stride 1~9 만큼 떨어진 two frames를 sampling하여 input pair로 사용하는데,<br/> stride가 클수록 sampling prob.도 linearly 큼<br/> \(\rightarrow\)<br/> 서로 더 멀리 떨어진 frame pair, 즉 large motion에 more weights 부여</li> <li>전략 3)<br/> Field-of-View augmentation (center crop with various image scales) 사용하여<br/> 다양한 camera intrinsics에도 일반화 가능하도록!<br/> (training videos에는 해당 variation이 흔하지 않음)</li> </ul> </li> </ul> <h3 id="dynamic-global-point-clouds-and-camera-pose">Dynamic Global Point Clouds and Camera Pose</h3> <p>frame 수가 많기 때문에<br/> pairwise pointmap 들로부터 직접 하나의 dynamic global point cloud를 추출하는 건 어렵.<br/> 지금부터 pairwise model을 이용해서<br/> <code class="language-plaintext highlighter-rouge">dynamic global pcd</code> \(\hat X\) 와 <code class="language-plaintext highlighter-rouge">camera pose</code> \(\hat K, \hat P = [\hat R | \hat T]\) 를 <code class="language-plaintext highlighter-rouge">동시에</code> optimize하는 방법을 소개하겠다</p> <ul> <li>Video Graph : <ul> <li>DUSt3R의 경우<br/> global alignment를 위해 모든 frame pair에 대해 connectivity graph를 만드는데,<br/> dynamic scene video에 대해 이렇게 graph 만드려면 too expensive</li> <li>계산량 줄이기 위해<br/> 전체 frames에 대해 graph를 만드는 게 아니라<br/> video의 <code class="language-plaintext highlighter-rouge">sliding temporal window</code> 내에 있는 frames에 대해 <code class="language-plaintext highlighter-rouge">국소적인 graph</code> 만듦</li> <li>sliding temporal window 내에 있는 모든 each frame pair<br/> \((t, t') \in W^{t} = {(a, b) | a, b \in [t, \ldots, t + w], a \neq b}\) 에 대해<br/> (\(w\) : temporal window size)<br/> MonST3R로 pairwise pointmap을 구하고,<br/> off-the-shelf method로 optical flow 구함</li> <li>runtime 줄이기 위해 strided sampling 적용</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/5.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/5.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/5.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/5.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Dynamic Global Point Cloud and Pose Optimization : <ul> <li>goal :<br/> 모든 pairwise pointmaps를 <code class="language-plaintext highlighter-rouge">같은 global coordinate frame</code>에 모아서 <code class="language-plaintext highlighter-rouge">world-coordinate pointmap</code> \(X^{t} \in R^{H \times W \times 3}\) 만들기</li> <li>re-parameterization : <ul> <li>notation :<br/> \(P^{t} = [R^{t} | T^{t}]\) : extrinsic camera pose<br/> \(K^{t}\) : intrinsic<br/> \(D^{t}\) : per-frame depthmap</li> <li>global pointmap \(X^{t}\) :<br/> <code class="language-plaintext highlighter-rouge">depthmap, intrinsic, extrinsic</code> 을 이용하여 parameterize <code class="language-plaintext highlighter-rouge">global pointmap</code><br/> \(X_{i,j}^{t} = P^{t^{-1}} h (K^{t^{-1}} [i D_{i,j}^{t} ; j D_{i,j}^{t} ; D_{i,j}^{t}])\) <ul> <li>intrinsic \(K^{t^{-1}}\) :<br/> depthmap 정보를 2D pixel-coordinate \((i, j)\) 에서 3D camera-coordinate으로 변환한 뒤</li> <li>homogeneous mapping \(h(\cdot)\) :<br/> homogeneous-coordinate으로 변환한 뒤<br/> (\(R^{t}, T^{t}\) 를 하나의 행렬로 표현 가능하도록 하여 just 행렬 곱셈을 통해 변환 가능)</li> <li>extrinsic \(P^{t^{-1}}\) :<br/> world-coordinate으로 변환</li> </ul> </li> </ul> </li> <li>loss :<br/> dynamic scene video이기 때문에 DUSt3R의 3D alignment loss 뿐만 아니라 두 가지 video-specific loss 추가 <ul> <li>Loss 1) DUSt3R의 <code class="language-plaintext highlighter-rouge">3D alignment</code> loss : <ul> <li>goal :<br/> <code class="language-plaintext highlighter-rouge">각 pairwise pointmap</code> \(X^{t; t \leftarrow t'}\), \(X^{t'; t \leftarrow t'}\) 을<br/> <code class="language-plaintext highlighter-rouge">world-coordinate</code>의 <code class="language-plaintext highlighter-rouge">global pointmap</code> \(X^{t}\) 에 <code class="language-plaintext highlighter-rouge">align</code>시키는<br/> single rigid transformation \(P^{t;e}\)<br/> (\(X^{t; t \leftarrow t'}\) 와 \(X^{t'; t \leftarrow t'}\) 는 둘 다 <code class="language-plaintext highlighter-rouge">이미 같은 camera-coordinate</code> (\(t\) 의 frame) 에 align되어 있으므로<br/> \(X^{t; t \leftarrow t'}\) 을 global pointmap에 align시키는 \(P\) 와<br/> \(X^{t'; t \leftarrow t'}\) 을 global pointmap에 align시키는 \(P\) 는 같음)</li> <li>how :<br/> \(L_{align}(X, \sigma, P_{W}) = \sum_{W^{i} \in W} \sum_{e \in W} \sum_{t \in e} \| C^{t; e} \cdot (X^{t} - \sigma^{e} P^{t;e} X^{t;e}) \|_{1}\) <ul> <li>notation :<br/> \(W^{i} \in W\) : each sliding temporal window<br/> \(e = (t, t') \in W^{i}\) : each frame pair within the window<br/> \(t \in e\) : each frame<br/> \(\sigma^{e}\) : frame 크기 차이를 보정하는 per-(frame pair) scale factor<br/> \(P_{W}\) : sliding temporal window 내의 여러 frame pair에 대한 3D alignment transformation 집합</li> </ul> </li> </ul> </li> <li>Loss 2) <code class="language-plaintext highlighter-rouge">camera trajectory smoothness</code> loss : <ul> <li>goal :<br/> nearby timestep에 대해 \(R, T\) 가 크게 변하지 않도록 하여<br/> <code class="language-plaintext highlighter-rouge">시간에 따라 camera motion (extrinsic) 이 smooth</code>하도록</li> <li>how :<br/> \(L_{smooth}(X) = \sum_{t=0}^{N} (\| R^{t^{T}} R^{t+1} - I \|_{f} + \| R^{t^{T}} (T^{t+1} - T^{t}) \|_{2})\)</li> </ul> </li> <li>Loss 3) <code class="language-plaintext highlighter-rouge">flow projection</code> loss : <ul> <li>goal :<br/> confident <code class="language-plaintext highlighter-rouge">static region</code>에 대해<br/> global pointmaps \(X^{t}\) 와 camera poses \(K^{t}, R^{t}, T^{T}\), 즉 <code class="language-plaintext highlighter-rouge">camera motion만으로 계산한 optical flow</code>가<br/> <code class="language-plaintext highlighter-rouge">off-the-shelf method가 내놓은 optical flow</code>와 consistent하도록</li> <li>how :<br/> \(L_{flow}(X) = \sum_{W^{i} \in W} \sum_{t \rightarrow t' \in W^{i}} \| S^{global; t \rightarrow t'} \cdot (F_{cam}^{global; t \rightarrow t'} - F_{est}^{t \rightarrow t'}) \|_{1}\) <ul> <li>\(S^{global; t \rightarrow t'}\) : static region에 대해 loss term 걸어줌<br/> (static mask 구하는 방법 : 아래의 Confident Static Regions 섹션에서 설명!)<br/> (\(X^{t}\) 가 learnable 하므로 학습 중에 계속 updated)</li> <li>\(F_{cam}^{global; t \rightarrow t'}\) : global pointmap \(X^{t}\) 에 camera motion (intrinsic, extrinsic)을 적용하여 계산한 optical flow field</li> <li>\(F_{est}^{t \rightarrow t'}\) : off-the-shelf method가 내놓은 optical flow field</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Final Loss</code> :<br/> \(\hat X = \text{argmin}_{X, P_{W}, \sigma} L_{align} (X, \sigma, P_{W}) + w_{smooth} L_{smooth} (X) + w_{flow} L_{flow} (X)\) <ul> <li>learnable param. :<br/> <code class="language-plaintext highlighter-rouge">global pointmap</code> \(X\), global pointmap으로의 <code class="language-plaintext highlighter-rouge">3D alignment transformation</code> \(P_{W}\), <code class="language-plaintext highlighter-rouge">scale factor</code> \(\sigma\) 를 업데이트하는데,<br/> 얘네는 본질적으로 re-parameterization에 의해 <code class="language-plaintext highlighter-rouge">depthmap</code> \(\hat D\), <code class="language-plaintext highlighter-rouge">extrinsic</code> \(\hat P\), <code class="language-plaintext highlighter-rouge">intrinsic</code> \(\hat K\) 로 구성되어 있음<br/> 즉, MonST3R는 결국 jointly optimize video depthmap and camera pose (extrinsic, intrinsic)</li> <li>\(w_{smooth} = 0.01, w_{flow} = 0.01\)<br/> (\(L_{flow} \lt 20\) 일 때, 즉 camera poses를 roughly align한 뒤에 \(L_{flow}\) 활성화함)<br/> (\(L_{flow} \gt 50\) 일 때, 즉 초기에 motion mask is updated)</li> </ul> </li> </ul> <h2 id="downstream-applications">Downstream Applications</h2> <h3 id="intrinsics-and-relative-pose-estimation">Intrinsics and Relative Pose Estimation</h3> <ul> <li> <p>Intrinsic Pose Estimation :<br/> time \(t\) 에서의 pointmap \(X^{t}\) 을 이용해서<br/> 2D image와 3D pointmap이 align되도록 하는<br/> focal length \(f^{t}\) 를 추정함으로써<br/> camera intrinsic \(K^{t}\) 추정</p> </li> <li> <p>Relative Pose Estimation :<br/> DUSt3R와 달리 dynamic objects는<br/> epipolar matrix 또는 Procrustes alignment를 위한 가정(<code class="language-plaintext highlighter-rouge">???</code>)들에 위배<br/> \(\rightarrow\)<br/> 대신 주어진 3D point와 corresponding 2D point를 바탕으로 추정하는 PnP algorithm(<code class="language-plaintext highlighter-rouge">???</code>)과<br/> random sampling 방식의 RANSAC algorithm(<code class="language-plaintext highlighter-rouge">???</code>) 사용<br/> (dynamic scene이어도 대부분의 pixels는 static할 것이므로<br/> randomly-sampled points는 static elements에 더 가중치를 두기 때문에<br/> relative pose는 inliers(static)로 robustly estimate 가능)</p> </li> </ul> <h3 id="confident-static-regions">Confident Static Regions</h3> <ul> <li>Static Mask :<br/> 단순하게 <code class="language-plaintext highlighter-rouge">두 optical flow field가 일치하는 (차이가 적은) 영역</code>을 <code class="language-plaintext highlighter-rouge">Static Region</code>으로 간주!<br/> static mask \(S^{t \rightarrow t'} = [\alpha \gt \| F_{cam}^{t \rightarrow t'} - F_{est}^{t \rightarrow t'} \|_{1}]\)<br/> (이 confident static mask를 나중에 global pose optimization에도 사용할 거임!) <ul> <li>\(F_{cam}^{t \rightarrow t'}\) :<br/> <code class="language-plaintext highlighter-rouge">camera motion</code>만으로 optical flow 추정 <ul> <li>Step 1)<br/> frame pair \(I^{t}\), \(I^{t'}\) 로부터<br/> pointmaps \(X^{t; t \leftarrow t'}\), \(X^{t'; t \leftarrow t'}\) 와 pointmaps \(X^{t; t' \leftarrow t}\), \(X^{t'; t' \leftarrow t}\) 추정</li> <li>Step 2)<br/> 위에서 언급한 방법으로 Intrinsics \(K^{t}, K^{t'}\) 와 Relative Pose \(R^{t \rightarrow t'}, T^{t \rightarrow t'}\) 추정</li> <li>Step 3)<br/> only camera motion \(t \rightarrow t'\) 이용해서<br/> optical flow field \(F_{cam}^{t \rightarrow t'}\) 추정<br/> \(F_{cam}^{t \rightarrow t'} = \pi (D^{t; t \leftarrow t'} K^{t'} R^{t \rightarrow t'} K^{t^{-1}} \hat x + K^{t'} T^{t \rightarrow t'}) - x\) <ul> <li>notation :<br/> \(x\) : pixel-coordinate<br/> \(\hat x\) : homogeneous coordinate<br/> \(\pi(\cdot)\) : projection operation \((x,y,z) \rightarrow (\frac{x}{z}, \frac{y}{z})\)<br/> \(D^{t; t \leftarrow t'}\) : estimated depth from pointmap \(X^{t; t \leftarrow t'}\)</li> <li>Step 3-1)<br/> intrinsic 이용하여 frame \(t\) 에 대해 2D에서 3D로 backproject</li> <li>Step 3-2)<br/> camera motion (relative camera pose) \(t \rightarrow t'\) 적용<br/> (\(R^{t \rightarrow t'} X + T^{t \rightarrow t'}\))</li> <li>Step 3-3)<br/> intrinsic 이용하여 frame \(t'\) 에 대해 다시 3D에서 2D image coordinate으로 project</li> </ul> </li> </ul> </li> <li>\(F_{est}^{t \rightarrow t'}\) :<br/> <code class="language-plaintext highlighter-rouge">off-the-shelf method</code> <d-cite key="SEARAFT">[11]</d-cite> 이용해서 optical flow 추정</li> </ul> </li> </ul> <h3 id="video-depth">Video Depth</h3> <p>optimal global pointmap \(\hat X\) 자체가 re-parameterization에 의해<br/> per-frame depthmap \(\hat D\) 로 이루어져 있고,<br/> just \(\hat D\) 자체가 video depth</p> <h2 id="experiment">Experiment</h2> <h3 id="results">Results</h3> <ul> <li>Single-Frame and Video <code class="language-plaintext highlighter-rouge">Depth Estimation</code> : <ul> <li>baseline : <ul> <li>video depth method :<br/> NVDS <d-cite key="NVDS">[12]</d-cite><br/> ChronoDepth <d-cite key="Chrono">[13]</d-cite><br/> DepthCrafter <d-cite key="DepthCrafter">[14]</d-cite></li> <li>single-frame depth method :<br/> Depth-Anything-V2 <d-cite key="DepthAnything">[15]</d-cite><br/> Marigold <d-cite key="Marigold">[16]</d-cite><br/> DUSt3R <a href="https://semyeong-yu.github.io/blog/2024/DUSt3R/">blog</a></li> <li>joint video depth and pose estimation method :<br/> CasualSAM <d-cite key="CasualSAM">[17]</d-cite><br/> Robust-CVD <d-cite key="RobustCVD">[18]</d-cite></li> </ul> </li> <li>benchmark dataset : <ul> <li>video depth :<br/> KITTI<br/> Sintel<br/> Bonn</li> <li>monocular single-frame depth :<br/> NYU-v2</li> </ul> </li> <li>metric : <d-cite key="DepthCrafter">[14]</d-cite>, <d-cite key="DepthAnything">[15]</d-cite> 에서처럼<br/> Abs Rel : absolute relative error<br/> \(\sigma \lt 1.25\) : percentage of inlier points <code class="language-plaintext highlighter-rouge">???</code><br/> (All methods output scale- and/or shift- invariant depth estimates. For video depth evaluation, we align a single scale and/or shift factor per each sequence, whereas the single-frame evaluation adopts per-frame median scaling, following DUSt3R) <code class="language-plaintext highlighter-rouge">???</code></li> <li>results : <ul> <li>video depth :<br/> MonST3R는 specialized video depth estimation techniques와 유사한 성능</li> <li>single-frame depth :<br/> DUSt3R 구조를 dynamic scene’s video에 대해 fine-tuning했는데도<br/> single-frame depth estimation에 대해 여전히 DUSt3R와 유사한 성능</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/6.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/6.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/6.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/6.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">Camera Pose Estimation</code> :<br/> obtain camera trajectories <ul> <li>baseline : <ul> <li>joint video depth and pose estimation method (경쟁자들) :<br/> CasualSAM <d-cite key="CasualSAM">[17]</d-cite><br/> Robust-CVD <d-cite key="RobustCVD">[18]</d-cite></li> <li>learning-based visual odometry method :<br/> DROID-SLAM (GT intrinsic 필요)<br/> Particle-SfM (Ours보다 5배 느림)<br/> DPVO (GT intrinsic 필요)<br/> LEAP-VO (GT intrinsic 필요)</li> <li>DUSt3R with GT motion mask :<br/> 단순히 dynamic region의 pixel과 token을 mask out<br/> (Related Works 섹션의 motion mask에서 설명함)</li> </ul> </li> <li>benchmark dataset :<br/> Sintel<br/> TUM-dynamics<br/> ScanNet</li> <li>metric : Particle-SfM, LEAP-VO에서처럼<br/> Sim(3) Umeyama alignment 적용한 뒤 <code class="language-plaintext highlighter-rouge">???</code><br/> Absolute Translation Error (ATE)<br/> Relative Translation Error (RPE trans)<br/> Relative Rotation Error (RPE rot)</li> <li>results :<br/> joint depth and pose estimation methods 중에 제일 성능 좋고<br/> GT intrinsic 필요 없는데도 pose-specific methods와 유사한 성능</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/7.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/7.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/7.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/7.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/8.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/8.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/8.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/8.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="ablation-study">Ablation Study</h3> <ul> <li>Ablation Study : <ul> <li>training dataset :<br/> datasets 섞어 쓰면 camera pose estimation에 도움!</li> <li>fine-tuning strategy :<br/> only decoder and head만 fine-tuning하는 게 다른 training strategies보다 나음!</li> <li>loss :<br/> 본 논문에서 언급한 세 가지 loss (\(L_{align}, L_{smooth}, L_{flow}\)) 는<br/> video depth accuracy를 크게 해치지 않으면서 pose estimation accuracy를 높임!</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-22-MonST3R/9.PNG-480.webp 480w,/assets/img/2025-01-22-MonST3R/9.PNG-800.webp 800w,/assets/img/2025-01-22-MonST3R/9.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-22-MonST3R/9.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="limitation">Limitation</h2> <ul> <li>Limitation : <ul> <li>이론적으로는 dynamic camera intrinsics를 estimate할 수 있지만,<br/> 사실상 이는 <code class="language-plaintext highlighter-rouge">careful hyperparameter tuning</code>과 <code class="language-plaintext highlighter-rouge">manual constraints</code>를 필요로 함</li> <li><code class="language-plaintext highlighter-rouge">out-of-distribution inputs</code>에 struggle<br/> e.g. 건물 내부 또는 도심 야외 등으로 훈련한 경우 넓은 공터 같은 새로운 scene에 대해서는 제대로 작동 안함 <ul> <li>해결법 :<br/> training set을 확장하면 MonST3R가 in-the-wild videos에 대해서도 더 robust해질 듯</li> </ul> </li> </ul> </li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br/> 왜 static region에 대해서만 optical flow가 consistent하도록 하는 flow projection loss 적용함?</p> </li> <li> <p>A1 :<br/> ddd</p> </li> <li> <p>Q2 :<br/> Gaussian Marbles에서는 frame끼리 divide-and-conquer로 merge하면서 frame 전후 관계를 trajectory로 연결하여 temporal info.를 이용함.<br/> MonST3R는 각 timestep의 pointmap을 global pointmap으로 align하는데 camera trajectory smoothness loss 말고 3D alignment loss에서 frame 전후 관계, 즉 temporal info.를 이용함?</p> </li> <li> <p>A2 :<br/> TBD</p> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="dynamic"/><category term="GS"/><category term="geometry"/><category term="SfMfree"/><summary type="html"><![CDATA[A Simple Approach for Estimating Geometry in the Presence of Motion (ICLR 2025)]]></summary></entry><entry><title type="html">Dynamic Gaussian Marbles</title><link href="https://semyeong-yu.github.io/blog/2025/GaussianMarbles/" rel="alternate" type="text/html" title="Dynamic Gaussian Marbles"/><published>2025-01-16T12:00:00+00:00</published><updated>2025-01-16T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/GaussianMarbles</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/GaussianMarbles/"><![CDATA[<h2 id="dynamic-gaussian-marbles-for-novel-view-synthesis-of-casual-monocular-videos">Dynamic Gaussian Marbles for Novel View Synthesis of Casual Monocular Videos</h2> <h4 id="colton-stearns-adam-harley-mikaela-uy-florian-dubost-federico-tombari-gordon-wetzstein-leonidas-guibas">Colton Stearns, Adam Harley, Mikaela Uy, Florian Dubost, Federico Tombari, Gordon Wetzstein, Leonidas Guibas</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2406.18717">https://arxiv.org/abs/2406.18717</a><br/> project website :<br/> <a href="https://geometry.stanford.edu/projects/dynamic-gaussian-marbles.github.io/">https://geometry.stanford.edu/projects/dynamic-gaussian-marbles.github.io/</a></p> </blockquote> <h2 id="contribution">Contribution</h2> <ul> <li>Dynamic 4D Gaussian Splatting : <ul> <li>이전까지는 multiple simultaneous viewpoints of a scene 세팅 (dense multi-camera setup)에서의 recon. 논문들이 많았음<br/> \(\rightarrow\)<br/> 평소의 casual <code class="language-plaintext highlighter-rouge">monocular video</code> (challenging)로 4D recon.을 수행해보자!</li> <li>input에 multi-view info.가 없는 underconstrained monocular video더라도<br/> prior (<code class="language-plaintext highlighter-rouge">careful optimization strategy</code> 및 <code class="language-plaintext highlighter-rouge">off-the-shelf depth and motion estimation</code> 및 <code class="language-plaintext highlighter-rouge">geometry-based regularization</code>) 이용해서<br/> 적절한 constraint를 복원할 수 있다!</li> </ul> </li> <li>Dynamic Gaussian Marbles :<br/> monocular setting의 어려움을 해결하기 위해 GS에서 세 가지 사항을 변경<br/> 이를 통해 Gaussian trajectories를 학습할 수 있음 <ul> <li>isotropic Gaussian Marbles :<br/> <code class="language-plaintext highlighter-rouge">isotropic</code> Gaussian을 사용함으로써<br/> Gaussian의 <code class="language-plaintext highlighter-rouge">degrees of freedom을 줄이고</code><br/> <code class="language-plaintext highlighter-rouge">local shape보다는 motion과 apperance</code> 표현하는 데 더 집중하도록 제한</li> <li>hierarchical divide-and-conquer learning strategy :<br/> time 길이가 어느 정도 짧아야 잘 포착할 수 있으므로 <br/> long video를 short <code class="language-plaintext highlighter-rouge">subsequences로 나누고 optimize by iteratively merging the subsequences</code><br/> (long-sequence tracking 대신 인접한 subsequences를 붙이는 task로!) <ul> <li>procedure :<br/> 아래의 과정을 반복하며 locality와 global coherence를 모두 챙김! <ul> <li><code class="language-plaintext highlighter-rouge">motion estimation</code> :<br/> \(G^{b}\) 의 frame을 \(G^{a}\) 의 trajectory에 하나씩 더해 가며 motion estimation을 수행하므로 <d-cite key="Dynamic3DGS">[1]</d-cite> 처럼 <code class="language-plaintext highlighter-rouge">locality</code>와 smoothness로부터 benefit</li> <li><code class="language-plaintext highlighter-rouge">merge</code></li> <li><code class="language-plaintext highlighter-rouge">global adjustment</code> : <d-cite key="4DGS">[2]</d-cite> 처럼 <code class="language-plaintext highlighter-rouge">global coherence</code>라는 benefit</li> </ul> </li> </ul> </li> <li>prior :<br/> monocular video로도 recon. 잘 수행하기 위해 prior 이용 <ul> <li><code class="language-plaintext highlighter-rouge">image(2D)-space prior</code> : SAM (Rendering loss-segmentation), CoTracker (Tracking loss), DepthAnything (Rendering loss-depthmap)</li> <li><code class="language-plaintext highlighter-rouge">geometry(3D)-space prior</code> : regularization of Gaussian trajectories with rigidity (Isometry loss) and Chamfer priors (3D Alignment loss)</li> </ul> </li> </ul> </li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li> <p>Gaussian Splatting :<br/> TBD</p> </li> <li> <p>Dynamic Gaussian Splatting :<br/> TBD</p> </li> <li> <p>Other Dynamic Nerual Scene Representations :<br/> TBD</p> </li> </ul> <h2 id="method">Method</h2> <h3 id="overview">Overview</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/1.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/1.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/1.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/1.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/3.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/3.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="dynamic-gaussian-marbles">Dynamic Gaussian Marbles</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/2.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/2.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> simpler Gaussian Marble의 경우에만 generalize well to novel view </div> <ul> <li>Gaussian marble : <ul> <li><code class="language-plaintext highlighter-rouge">isotropic</code> :<br/> \(R = I\) and \(S = s \in R^{1}\) <ul> <li>anisotropic Gaussian은 expensive할 뿐만 아니라<br/> underconstrained monocular cam. setting에서는 오히려 degrees of freedom 많으면 poor하다는 걸 실험적으로 발견</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">semantic instance</code> :<br/> assign each Gaussian marble to semantic instance \(y \in N\) by SAM-driven TrackAnything</li> <li><code class="language-plaintext highlighter-rouge">dynamic trajectory</code> :<br/> trajectory \(\Delta X \in R^{T \times 3}\) : a sequence of translations which maps marble’s position change at each timestep</li> </ul> </li> </ul> <h3 id="divide-and-conquer-motion-estimation">Divide-and-Conquer Motion Estimation</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/3.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/3.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Training Procedure : <ul> <li>Step 1) <code class="language-plaintext highlighter-rouge">initialization for each frame</code><br/> initialize Gaussian marbles \([G_{11}, G_{22}, \ldots, G_{TT}]\) for each frame<br/> (initial marbles \(G_{ii}\) have trajectory length 1) <ul> <li>Step 1-1) obtain prior (<code class="language-plaintext highlighter-rouge">depthmap</code> and <code class="language-plaintext highlighter-rouge">segmentation</code>)<br/> obtain monocular (LiDAR) depthmap and segmentation from SAM-driven TrackAnything <d-cite key="TrackAnything">[3]</d-cite></li> <li>Step 1-2) <code class="language-plaintext highlighter-rouge">unproject</code> from 2D to 3D<br/> unproject the depthmap into point cloud<br/> perform outlier removal and downsampling</li> <li>Step 1-3) initialize Gaussian marbles and trajectory <ul> <li>Gaussian marbles : <ul> <li>mean \(\mu \in R^{3}\) : Step 1-2)에서 얻은 pcd</li> <li>color \(c \in R^{3}\) : pixel color (pixel-aligned Gaussians)</li> <li><code class="language-plaintext highlighter-rouge">instance class</code> \(y \in R^{1}\) : Step 1-1)에서 얻은 segmentation</li> <li>scale \(s \in R^{1}\) and opacity \(\alpha \in R^{1}\) : 3DGS 논문에서 했던대로 초기화</li> </ul> </li> <li>trajectory : <ul> <li><code class="language-plaintext highlighter-rouge">trajectory</code> : \(\Delta X = [\boldsymbol 0] \in R^{T \times 3}\)</li> </ul> </li> </ul> </li> </ul> </li> <li>Step 2) bottom-up divide-and-conquer merge<br/> merge short-trajectories into longer trajectories<br/> e.g. \(G = [G_{12}, G_{34}, G_{56}, G_{78}] \rightarrow G = [G_{14}, G{58}]\) <ul> <li>Step 2-1) <code class="language-plaintext highlighter-rouge">motion estimation</code> <ul> <li>Step 2-1-1) make a pair b.w. adjacent marbles <ul> <li>adjacent Gaussian marble set끼리 a pair로 묶음<br/> e.g. \([(G_{12}^{a}, G_{34}^{b}), (G_{56}^{a}, G_{78}^{b})]\)</li> <li>\(G^{a}\) 는 merge할 prev. frames’ Gaussians이고,<br/> \(G^{b}\) 는 merge할 next frames’ Gaussians</li> </ul> </li> <li>Step 2-1-2) \(G^{a}\) 의 trajectory 확장 <ul> <li>goal :<br/> \(G_{12}^{a}\) 의 trajectory인 \(\Delta X = [\Delta X_{1}, \Delta X_{2}]\) 는 이미 학습되어 merge된 motion이고,<br/> \(G_{12}^{a}\) 의 trajectory와 \(G_{34}^{b}\) 의 frame \(3\) 을 잇는 motion \(\Delta X_{3}\) 을 학습해야 함!</li> <li>constant-velocity assumption에 따라 trajectory를 \(\Delta X = [\Delta X_{1}, \Delta X_{2}, \Delta X_{3}^{init}]\) 로 확장</li> </ul> </li> <li>Step 2-1-3) trajectory optimization <ul> <li>\(G_{12}^{a}\) 를 frame \(3\) 에 render한 뒤<br/> \(\Delta X_{3}\) 이 frame \(3\) 으로의 motion을 잘 반영하도록 \(\Delta X_{3}\) 을 업데이트<br/> (\(\eta\) 번 반복 by 아래에서 설명할 Loss)</li> </ul> </li> <li>Step 2-1-4) repeat <ul> <li>Step 2-1-2), Step 2-1-3)을 반복<br/> until \(G^{a}\) 의 trajectory가 \(G_{34}^{b}\) 내 모든 frames를 커버할 때까지</li> <li>e.g. \(G^{a}\) 의 trajectory를 \(\Delta X = [\Delta X_{1}, \Delta X_{2}, \Delta X_{3}, \Delta X_{4}^{init}]\) 로 확장한 뒤<br/> \(G^{a}\) 를 frame \(4\) 에 render한 뒤<br/> \(\Delta X_{4}\) 가 frame \(4\) 으로의 motion을 잘 반영하도록 \(\Delta X_{4}\) 을 업데이트</li> </ul> </li> </ul> </li> <li>Step 2-2) <code class="language-plaintext highlighter-rouge">merge</code> <ul> <li>motion estimation을 거치고 나면 \(G_{ij}^{a}\) 와 \(G_{ij}^{b}\) 가 같은 frame subsequence \([i, j]\) 를 recon.할 것이므로<br/> merge by just union \(G_{ij} = G_{ij}^{a} \cup G_{ij}^{b}\)<br/> (set size 2배 됨)</li> <li>computational load 줄이기 위해<br/> opacity 또는 scale이 너무 작은 Gaussians는 drop하고,<br/> random downsampling 수행하여<br/> set size를 constant하게 유지</li> </ul> </li> <li>Step 2-3) <code class="language-plaintext highlighter-rouge">global adjustment</code> <ul> <li>merge로 합치고 나서도 still optimized라는 보장이 없기 때문에<br/> newly merged Gaussians를 모두 jointly optimize</li> <li>merged set가 \(G_{ij}\) 라고 했을 때<br/> \([i, j]\) 내 a frame을 randomly sampling하고<br/> \(G_{ij}\) 의 모든 Gaussians를 해당 frame에 render한 뒤<br/> Gaussian \(c, s, \alpha, \Delta X\) 을 업데이트<br/> (\(\beta\) 번 반복)</li> <li>그럼 merged Gaussians \(G_{ij}\) 가 global Gaussians로 인정받을 수 있음!</li> </ul> </li> </ul> </li> </ul> </li> <li>Inference Procedure : <ul> <li>learned Gaussian trajectories 이용해서<br/> render (roll out) into specific timestep \(t\)</li> </ul> </li> </ul> <h3 id="loss">Loss</h3> <p>Motion Estimation 단계에서 아래의 Loss들 사용!</p> <ul> <li><code class="language-plaintext highlighter-rouge">Tracking</code> Loss :<br/> \(L_{track} = \sum_{p \in P} \sum_{g \in N(p_{i})} \alpha_{i}^{'} \| D_{i} \| \mu_{i}^{'} - p_{i} \| - D_{j} \| \mu_{j}^{'} - p_{j} \| \|\)<br/> where \(\mu_{i}^{'}\) and \(D_{i}\) : mean and depth of projected 2D Gaussian<br/> where \(P\) : tracked points by CoTracker <d-cite key="CoTracker">[4]</d-cite><br/> where \(N(p_{i})\) : tracked point \(p_{i}\) 와의 the nearest 3D Gaussians<br/> where \(\alpha_{i}^{'}\) : Gaussian’s opacity <ul> <li>goal :<br/> <code class="language-plaintext highlighter-rouge">2D point track</code>인 CoTracker <d-cite key="CoTracker">[4]</d-cite> (2D prior)를 사용하여<br/> <code class="language-plaintext highlighter-rouge">Gassian marble trajectories</code>를 regularize</li> <li>Step 1)<br/> \(G^{b}\) 의 frame \(j\) 로의 \(\Delta X_{j}\) 를 optimize하고자 할 때,<br/> CoTracker <d-cite key="CoTracker">[4]</d-cite> 를 이용하여 frames \([j - w , j + w]\) (\(w = 12\))에서의 point tracks \(P\) 를 estimate<br/> (from 2D frame to 2D frame)<br/> (일종의 GT로 사용)</li> <li>Step 2)<br/> a source frame \(i \in [j - w , j + w]\) 을 randomly sampling</li> <li>Step 3)<br/> Gaussian marble trajectory \(\Delta X\) 로부터 frame \(i\) 와 frame \(j\) 에서의 3DGS position을 sampling하고<br/> 3DGS를 2D Gaussian in image plane으로 project시켜 2D mean, depth, covariance 구함</li> <li>Step 4)<br/> Step 1)의 tracked point \(p_{i \rightarrow j}\) 와 가장 가까운 \(K = 32\) 개의 Step 3)의 2D Gaussians를 구한 뒤<br/> <code class="language-plaintext highlighter-rouge">2D tracked point와 2D Gaussian 사이의 거리</code>가 frame \(i\), \(j\) 에서 거의 <code class="language-plaintext highlighter-rouge">일정하게 유지되도록</code> loss term 걸어줌<br/> (for <code class="language-plaintext highlighter-rouge">temporal consistency</code>)</li> </ul> </li> <li>Rendering Loss : <ul> <li><code class="language-plaintext highlighter-rouge">image</code> rendering하여<br/> GT image와의 L1 loss 및 LPIPS loss 구함</li> <li><code class="language-plaintext highlighter-rouge">disparity map</code> rendering하여<br/> initial disparity estimation과의 L1 loss 구함</li> <li><code class="language-plaintext highlighter-rouge">segmentation map</code> rendering하여<br/> SAM(off-the-shelf instance segmentation)과의 L1 loss 구함</li> </ul> </li> <li>Geometry Loss : <ul> <li><code class="language-plaintext highlighter-rouge">Local Isometry</code> Loss :<br/> \(L_{iso-local} = \sum_{g^{a} \in G} \sum_{g^{b} \in N(g^{a})} | \| \mu_{i}^{a} - \mu_{i}^{b} \| - \| \mu_{j}^{a} - \mu_{j}^{b} \| |\) <ul> <li>goal :<br/> prev. works <d-cite key="Dynamic3DGS">[2]</d-cite>, <d-cite key="DynamicPointFields">[5]</d-cite> 에서처럼<br/> Gaussian marbles가 <code class="language-plaintext highlighter-rouge">locally rigid motion</code>을 따르도록 regularize</li> <li>Step 1)<br/> \(G^{b}\) 의 frame \(j\) 로의 \(\Delta X_{j}\) 를 optimize하고자 할 때,<br/> 우선 a source timestep \(i \in [j - 1 , j + 1]\) 을 randomly sampling</li> <li>Step 2)<br/> 3DGS \(g^{a} \in G\) 및 이와 가까운 3DGS들 \(g^{b} \in N(g^{a})\) 에 대해<br/> <code class="language-plaintext highlighter-rouge">nearest 3D Gaussians끼리의 거리</code>가 frame \(i\), \(j\) 에서 거의 <code class="language-plaintext highlighter-rouge">일정하게 유지되도록</code> loss term 걸어줌</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Instance Isometry</code> Loss :<br/> \(L_{iso-instance} = \sum_{g^{a} \in G} \sum_{g^{b} \in Y(g^{a})} | \| \mu_{i}^{a} - \mu_{i}^{b} \| - \| \mu_{j}^{a} - \mu_{j}^{b} \| |\) <ul> <li>goal :<br/> 각 semantic instance가 일관적으로 움직이도록 regularize</li> <li>Step 1)<br/> \(G^{b}\) 의 frame \(j\) 로의 \(\Delta X_{j}\) 를 optimize하고자 할 때,<br/> 우선 a source timestep \(i \in [j - 1 , j + 1]\) 을 randomly sampling</li> <li>Step 2)<br/> 3DGS \(g^{a} \in G\) 및 이와 semantic label이 같은 3DGS들 \(g^{b} \in Y(g^{a})\) 에 대해<br/> <code class="language-plaintext highlighter-rouge">semantic label이 같은 3D Gaussians끼리의 거리</code>가 frame \(i\), \(j\) 에서 거의 <code class="language-plaintext highlighter-rouge">일정하게 유지되도록</code> loss term 걸어줌</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">3D Alignment</code> Loss :<br/> \(L_{chamfer} = \sum_{g^{1} \in G^{1}} \text{min}_{g^{2} \in G^{2}} \| \mu^{1} - \mu^{2} \| + \sum_{g^{2} \in G^{2}} \text{min}_{g^{1} \in G^{1}} \| \mu^{1} - \mu^{2} \|\) <ul> <li>goal :<br/> merge하고나서 <code class="language-plaintext highlighter-rouge">Global Adjustment</code>할 때 a frame에 전부 rendering해서 optimize하므로 <code class="language-plaintext highlighter-rouge">projected 2D image plane 상에서는 align</code> 되어 있음<br/> 그런데 merge하고나서 <code class="language-plaintext highlighter-rouge">3D space 상에서도 align</code>할 필요 있음<br/> (3DGS를 align한다는 게 무슨 의미지? 모든 3DGS가 함께 scene recon.에 기여하도록 서로 가깝게 만든다는 건가 <code class="language-plaintext highlighter-rouge">???</code>)<br/> (만약에 3D alignment 하지 않으면 3D and novel-view 상에서 <code class="language-plaintext highlighter-rouge">cloudy artifacts</code> 생김)<br/> (off-the-shelf depth estimation이 time에 따라 inconsistent할 경우 이와 같은 상황 발생)</li> <li>Step 1)<br/> 두 pcd 집합을 서로 가깝게 만드는 Chamfer loss를 적용할 건데,<br/> merge할 Gaussian set \(G^{a}\) 와 \(G^{b}\) 는 scene의 명확히 서로 다른 부분을 관측하고 있으므로<br/> 둘 사이에 Chamfer loss를 바로 적용하면 안 됨</li> <li>Step 2)<br/> set \(G_{12}^{a}\) 와 \(G_{34}^{b}\) 를 single frame의 subsets \([G_{1}^{a}, G_{2}^{a}, G_{3}^{b}, G_{4}^{b}]\) 로 나눔<br/> where \(G_{1}^{a}\) contains Gaussians initialized from frame \(1\)</li> <li>Step 3)<br/> 해당 subsets list를 random shuffle한 뒤<br/> 맨 앞의 25%는 set \(G^{1}\) 으로 묶고, 다음 25%는 set \(G^{2}\) 로 묶음<br/> (\(G^{1}\) 과 \(G^{2}\) 가 <code class="language-plaintext highlighter-rouge">scene의 어떤 부분을 보고 있는지 명확히 정해지지 않도록 randomness 부여</code>)<br/> (만약 이렇게 randomness 부여하지 않는다면 observed scene content difference에 overfitting될 수 있음 <code class="language-plaintext highlighter-rouge">???</code>)</li> <li>Step 4)<br/> \(G^{1}\) 과 \(G^{2}\) 에 대해 2-way Chamfer distance 계산</li> </ul> </li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <h3 id="dataset">Dataset</h3> <ul> <li>training :<br/> 아래의 두 가지 datasets는 multi-view info.를 포함하고 있으므로<br/> monocular setting을 모방하기 위해<br/> training and evaluation protocol을 수정 <ul> <li>NVIDIA Dynamic Scenes Dataset : <ul> <li>구성 :<br/> 7 videos<br/> 12 calibrated cameras</li> <li>setting :<br/> prev. benchmarked evaluations는 각 timestep마다 different training camera를 사용하는데,<br/> (monocular teleporting camera 방식 <d-cite key="monocular">[6]</d-cite>)<br/> 이는 realistic monocular video setting이 아니므로<br/> 본 논문에서는 single camera 4를 training에 사용하고 single camera 3, 5, 6을 evaluation에 사용</li> </ul> </li> <li>DyCheck iPhone Dataset : <ul> <li>구성 :<br/> 7 videos</li> <li>setting :<br/> single camera로 구성되어 있는 monocular video setting이긴 하지만<br/> multi-view info.를 포함하도록 3D trajectory가 scene 전체를 돌기 때문에<br/> camera의 calculated motion은 일상 video와 다르다 <ul> <li>방법 1) official benchmark 그대로 사용</li> <li>방법 2) camera pose 제거<br/> We remove camera poses, offloading the camera motion into the learned 4D scene representation’s dynamics. We find this setting interesting because it simulates additional dynamic content, where previously “static” regions of the scene now have rigid dynamics equal to the inverse camera motion, which must be solved by the scene representation itself. <code class="language-plaintext highlighter-rouge">???</code></li> </ul> </li> </ul> </li> </ul> </li> <li>test : <ul> <li>Total-Recon Dataset :<br/> 2 time-synchronized and calibrated videos with LiDAR</li> <li>Davis Dataset</li> <li>YouTube-VOS Dataset</li> <li>real-world videos</li> </ul> </li> </ul> <h3 id="implementation">Implementation</h3> <ul> <li>Implementation : <ul> <li>NVIDIA Dynamic Scenes Dataset : <ul> <li>120,000 Gaussians per frame and upsample to 240,000 Gaussians during the last stage of global adjustment</li> <li>\(\eta = 128\) on motion estimation and \(\beta = 48\) on global adjustment</li> <li>stop divide-and-conquer after learning subsequences of length 32 on both fg and bg</li> </ul> </li> <li>DyCheck iPhone Dataset : <ul> <li>220,000 Gaussians per frame if camear pose exists else 180,000 Gaussians</li> <li>\(\eta = 80\) on motion estimation and \(\beta = 32\) on global adjustment</li> <li>stop divide-and-conquer after learning subsequences of length 8 on fg and length 32(512) on bg<br/> (when there exists camera pose, need to learn more a dynamic bg)</li> </ul> </li> <li>Total-Recon Dataset : <ul> <li>120,000 Gaussians per frame</li> <li>$\eta = 80\(on motion estimation and\)\beta = 32$$ on global adjustment</li> <li>stop divide-and-conquer after learning subsequences of length 8 on fg and length 32 on bg</li> </ul> </li> </ul> </li> </ul> <h3 id="results">Results</h3> <ul> <li>Dynamic Novel View Synthesis : <ul> <li>TBD</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/4.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/4.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/8.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/8.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/8.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/8.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/6.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/6.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/6.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/6.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Tracking and Editing : <ul> <li>TBD</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/5.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/5.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/5.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/5.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="ablation-study">Ablation Study</h3> <ul> <li>motion estimation : <ul> <li>frame 간의 motion 정보를 학습</li> <li>locality and smoothness 보장</li> </ul> </li> <li>global adjustment : <ul> <li>merge하고나서 global Gaussian이 specific frame을 잘 render하도록</li> <li>global coherence 보장</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-16-GaussianMarbles/7.PNG-480.webp 480w,/assets/img/2025-01-16-GaussianMarbles/7.PNG-800.webp 800w,/assets/img/2025-01-16-GaussianMarbles/7.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-16-GaussianMarbles/7.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="conclusion">Conclusion</h2> <ul> <li>Limitation :<br/> extremely challenging open-world dynamic and monocular novel-view-synthesis는 잘 못 함 <ul> <li><code class="language-plaintext highlighter-rouge">2D image prior에 의존</code>하기 때문에<br/> SAM (segmentation), CoTracker (tracking), DepthAnything (depth estimation) 가 부정확할 경우<br/> 결과 안 좋음</li> <li><code class="language-plaintext highlighter-rouge">3D geometric prior에도 의존</code>하는데,<br/> <code class="language-plaintext highlighter-rouge">rapid and non-rigid motion</code>을 포함한 scene의 경우<br/> 잘 대응 못 함</li> </ul> </li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br/> Gaussian의 motion을 학습하는 것이라면 없던 object가 등장하거나 원래 있던 object가 frame 밖으로 벗어나는 경우에도 잘 대응할 수 있는지?<br/> CoTracker 등 여러 prior들도 위의 상황에 잘 대응하는지?</p> </li> <li> <p>A1 :<br/> TBD</p> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="Gaussian"/><category term="Marble"/><category term="degree"/><category term="freedom"/><category term="dynamic"/><category term="view"/><category term="synthesis"/><summary type="html"><![CDATA[Dynamic Gaussian Marbles for Novel View Synthesis of Casual Monocular Videos (SIGGRAPH 2024)]]></summary></entry><entry><title type="html">Feed Forward Bullet Time Reconstruction</title><link href="https://semyeong-yu.github.io/blog/2025/BTimer/" rel="alternate" type="text/html" title="Feed Forward Bullet Time Reconstruction"/><published>2025-01-10T12:00:00+00:00</published><updated>2025-01-10T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2025/BTimer</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2025/BTimer/"><![CDATA[<h2 id="feed-forward-bullet-time-reconstruction-of-dynamic-scenes-from-monocular-videos">Feed Forward Bullet Time Reconstruction of Dynamic Scenes from Monocular Videos</h2> <h4 id="hanxue-liang-jiawei-ren-ashkan-mirzaei-antonio-torralba-ziwei-liu-igor-gilitschenski-sanja-fidler-cengiz-oztireli-huan-ling-zan-gojcic-jiahui-huang">Hanxue Liang, Jiawei Ren, Ashkan Mirzaei, Antonio Torralba, Ziwei Liu, Igor Gilitschenski, Sanja Fidler, Cengiz Oztireli, Huan Ling, Zan Gojcic, Jiahui Huang</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2412.03526">https://arxiv.org/abs/2412.03526</a><br/> project website :<br/> <a href="https://research.nvidia.com/labs/toronto-ai/bullet-timer/">https://research.nvidia.com/labs/toronto-ai/bullet-timer/</a></p> </blockquote> <h2 id="contribution">Contribution</h2> <ul> <li>model : <ul> <li><code class="language-plaintext highlighter-rouge">motion-aware</code> <code class="language-plaintext highlighter-rouge">feed-forward</code> model for real-time recon. and novel-view-synthesis of <code class="language-plaintext highlighter-rouge">dynamic</code> scenes</li> <li>obtain <code class="language-plaintext highlighter-rouge">scalability</code> and <code class="language-plaintext highlighter-rouge">generalization</code> by using both static and dynamic scene datasets<br/> (static and dynamic recon.에 모두 사용 가능)</li> <li>Procedure : <ul> <li>Step 1) pre-train on large static scene dataset</li> <li>Step 2) video duration or FPS에 구애받지 않고 scale effectively across datasets</li> <li>Step 3) output multi-view volumetric video representation</li> </ul> </li> <li>recon. a bullet-time scene within 150ms with SOTA performance on a single GPU<br/> from 12 context frames of \(256 \times 256\) resolution</li> <li>bullet (target) timestamp \(t_{b}\) 를 원하는 each timestamp in video로 설정하면<br/> full video를 recon.할 수 있으므로<br/> 각 frame을 <code class="language-plaintext highlighter-rouge">parallel</code>하게 recon. 가능</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/2m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/2m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/2m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/2m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><code class="language-plaintext highlighter-rouge">BulletTimer</code> (Novelty 1.) :<br/> main model <ul> <li>recon. at <code class="language-plaintext highlighter-rouge">arbitrary</code> target (bullet) timestamp and <code class="language-plaintext highlighter-rouge">arbitrary</code> novel-view<br/> by adding <code class="language-plaintext highlighter-rouge">bullet-time embedding</code> to all the context (input) frames<br/> and <code class="language-plaintext highlighter-rouge">aggregating</code> pred. from all the context frames</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">NTE Module</code> (Novelty 2.) :<br/> <code class="language-plaintext highlighter-rouge">pre-processing</code> (FPI) <ul> <li><code class="language-plaintext highlighter-rouge">fast motion</code>에 대응하기 위해<br/> model에 feed하기 전에<br/> <code class="language-plaintext highlighter-rouge">intermediate (interpolated) frames를 predict</code></li> <li>inference할 때<br/> arbitrary target (bullet) timestamp에 대해<br/> recon.할 수 있도록 도움</li> </ul> </li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li> <p>Dynamic scene recon. from monocular video :<br/> still challenging<br/> due to inherently ill-posed (해가 무수히 많음) nature of dynamic recon. from limited observations</p> </li> <li> <p>Static scene recon. :</p> <ul> <li>optimization-based (per-scene) :<br/> NeRF, HyperNeRF</li> <li>learning-based (feed-forward) :<br/> MonoNeRF, GS-LRM</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/5m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/5m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/5m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/5m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Dynamic scene recon. :<br/> dynamic scene은 complex motion 때문에 ambiguity 존재<br/> 이를 해소하는 데 도움될 data prior 필요 <ul> <li>optimization-based (per-scene) : <ul> <li>use contraint (data prior) like depth and optical flow <d-cite key="33">[1]</d-cite>, <d-cite key="36">[2]</d-cite>, <d-cite key="37">[3]</d-cite>, <d-cite key="68">[4]</d-cite><br/> \(\rightarrow\)<br/> given data와 위의 data prior 간의 싱크를 맞추는 게 challenging <d-cite key="34">[5]</d-cite>, <d-cite key="63">[6]</d-cite></li> <li>per-scene approach는 time-consuming and thus scale 어렵</li> </ul> </li> <li>learning-based (<code class="language-plaintext highlighter-rouge">feed-forward</code>) : <ul> <li>directly predict recon. in feed-forward manner<br/> so, can <code class="language-plaintext highlighter-rouge">learn strong inherent prior directly from data</code> <d-cite key="7">[7]</d-cite>, <d-cite key="10">[8]</d-cite>, <d-cite key="12">[9]</d-cite>, <d-cite key="25">[10]</d-cite>, <d-cite key="53">[11]</d-cite></li> <li>근데 dynamic scene 모델링하기 복잡하고 4D supervision data 부족해서 적용하는데 한계 있었음</li> <li>지금 시점 기준 L4GM <d-cite key="53">[11]</d-cite> 이 유일한 feed-forward dynamic recon. model인데,<br/> synthetic object-centric dataset으로 훈련돼서<br/> fixed camera view-point와 multi-view supervision을 필요로 한다는 한계와<br/> real-world scene에 generalize하기 어렵다는 한계가 있었음</li> </ul> </li> </ul> </li> <li>Feed-Forward Dynamic scene recon. : <ul> <li>본 논문은<br/> <code class="language-plaintext highlighter-rouge">pixel-aligned 3DGS</code> <d-cite key="79">[12]</d-cite> 를 기반으로<br/> novel BulletTimer and NTE module 제안</li> </ul> </li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li>Dynamic 3D Representation : <ul> <li>TBD</li> </ul> </li> <li>Novel-View-Synthesis : <ul> <li>TBD</li> </ul> </li> <li>Feed-Forward Reconstruction : <ul> <li>TBD</li> </ul> </li> </ul> <h2 id="overview">Overview</h2> <ul> <li> <p>notation :<br/> context frames \(I_{c} \subset I\)<br/> camera poses \(P_{c} \subset P\)<br/> context timestamps \(T_{c} \subset T\)<br/> bullet timestamp \(t_{b} \in [\text{min}(T_{c}), \text{max}(T_{c})]\)<br/> recon. at timestamp \(t \notin T\) by NTE module</p> </li> <li> <p>Architecture :</p> <ul> <li>Training :<br/> BTimer와 NTE Module을 별도로 각각 train<br/> (not end-to-end)</li> <li>Inference :<br/> NTE Module로 pre-process한 뒤<br/> BTimer 사용</li> </ul> </li> </ul> <h2 id="method">Method</h2> <h3 id="btimer-bullet-timer">BTimer (Bullet Timer)</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/3m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/3m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/3m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/3m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Model Design : <ul> <li>encode (input) :<br/> \(i\)-th frame \(I_{i} \in I_{c}\) 을 \(8 \times 8\) 짜리 patches로 나눈 뒤<br/> \(j\)-th patch에 대해<br/> per-patch input token \(f_{ij} |_{j=1}^{HW / 64} = f_{ij}^{rgb} + f_{ij}^{pose} + f_{i}^{time}\) 만든 뒤<br/> concatenate input tokens from all context frames<br/> and feed into Transformer <ul> <li><code class="language-plaintext highlighter-rouge">image</code> encoder :<br/> GS-LRM <d-cite key="79">[12]</d-cite> 에서 영감을 받아,<br/> <code class="language-plaintext highlighter-rouge">ViT</code> model을 backbone으로 사용</li> <li><code class="language-plaintext highlighter-rouge">camera pose</code> encoder :<br/> <code class="language-plaintext highlighter-rouge">camera Plucker embedding</code> <d-cite key="70">[13]</d-cite></li> <li><code class="language-plaintext highlighter-rouge">time</code> encoder :<br/> PE (Positional Encoding) 및 linear layer를 거쳐<br/> \(t_{i}\) 와 \(t_{b}\) 를 각각 \(f_{i}^{ctx}\) 와 \(f_{i}^{bullet}\) 으로 encode한 뒤<br/> \(f_{i}^{time} = f_{i}^{ctx} + f_{i}^{bullet}\) <ul> <li><code class="language-plaintext highlighter-rouge">context (input)</code> timestamp \(t_{i}\) from context (input) frame \(I_{i}\)</li> <li><code class="language-plaintext highlighter-rouge">bullet (target)</code> timestamp \(t_{b}\) that is <code class="language-plaintext highlighter-rouge">shared</code> across context (input) frames</li> </ul> </li> </ul> </li> <li>decode (output) :<br/> transformer의 per-patch output token \(f_{ij}^{out}\) 을<br/> <code class="language-plaintext highlighter-rouge">per-patch 3DGS param. at bullet timestamp</code> \(G_{ij} \in R^{8 \times 8 \times 12}\) 로 regression <ul> <li>each Gaussian has 12 param. as color \(c \in R^{3}\), scale \(s \in R^{3}\), rotation unit quaternion \(q \in R^{4}\), opacity \(\sigma \in R\), and ray distance \(\tau \in R\)</li> <li>3D position is obtained by pixel-aligned unprojection \(\mu = o + \tau d\)<br/> (\(o\) and \(d\) are obtained from camera pose \(P_{i}\))</li> </ul> </li> </ul> </li> <li> <p>Loss :<br/> \(L_{RGB} = L_{MSE} + \lambda L_{LPIPS}\) with \(\lambda = 0.5\)</p> </li> <li>Timestamp :<br/> context (input) frames와 bullet (target supervision) frame 을 잘 고르는 게 중요 <ul> <li><code class="language-plaintext highlighter-rouge">In-context Supervision</code> : <ul> <li>bullet timestamp is randomly selected from context frames<br/> \(t_{b} \in T_{c}\)</li> <li>model이 context timestamp에 대해 정확히 recon. 가능하도록</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Interpolation Supervision</code> : <ul> <li>bullet timestamp lies between two adjacent context frames<br/> \(t_{b} \notin T_{c}\)</li> <li>model이 dynamic parts를 interpolate할 수 있도록</li> <li>pixel-aligned 3DGS의 inductive bias 때문에<br/> motion이 복잡하고 빠를 때 intermediate timestamp에 대해 예측 잘 못 함<br/> \(\rightarrow\)<br/> 먼저 NTE module의 도움을 받아 pre-process한 뒤<br/> BTimer 사용</li> <li>local minimum 방지 및 view 간 consistency 상승</li> </ul> </li> </ul> </li> <li>Inference : <ul> <li>bullet (target) timestamp \(t_{b}\) 를 원하는 each timestamp in video로 설정하면<br/> full video를 recon.할 수 있으므로<br/> 각 frame을 <code class="language-plaintext highlighter-rouge">parallel</code>하게 recon. 가능</li> <li><code class="language-plaintext highlighter-rouge">???</code><br/> For a video longer than the number of training context views \(| I_{c} |\),<br/> at timestamp \(t\), apart from including this exact timestamp and setting \(t_{b} = t\),<br/> we uniformly distribute the remaining \(| I_{c} | − 1\) required context frames across the whole duration of the video<br/> to form the input batch with \(| I_{c} |\) frames</li> </ul> </li> </ul> <h3 id="nte-module-novel-time-enhancer">NTE Module (Novel Time Enhancer)</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/4m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/4m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/4m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/4m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>NTE Module Design :<br/> decoder-only LVSM <d-cite key="27">[13]</d-cite> 에서 영감을 받아,<br/> BTimer model과 구조 똑같지만,<br/> I/O가 다름 <ul> <li>input : <ul> <li>context frame : <ul> <li>context (input) timestamp embedding<br/> (BTimer model과 달리 bullet timestamp embedding은 안 넣음)</li> <li>camera pose Plucker embedding</li> <li>context (input) frame</li> </ul> </li> <li>intermediate frame : <ul> <li>bullet (target) timestamp embedding</li> <li>target camera pose Plucker embedding</li> </ul> </li> </ul> </li> <li>output : <ul> <li>Transformer의 per-patch output tokens 중 target token을<br/> unpatchify and project directly to RGB values by linear layer<br/> \(\rightarrow\)<br/> RGB frame for any bullet (target) timestamp<br/> (this RGB frame은 NTE module network의 direct output이고, 3DGS로 rendering한 게 아님!!)<br/> \(\rightarrow\)<br/> NTE Module의 output은<br/> BTimer에서 bullet timestamp의 image로 쓰임</li> </ul> </li> <li>Implementation : <ul> <li>LVSM <d-cite key="27">[13]</d-cite> 에서처럼<br/> 안정적인 훈련을 위해 <code class="language-plaintext highlighter-rouge">QK-norm</code> 사용<br/> (Q와 K의 내적 과정에서 값이 너무 크거나 작으면 gradient explode or vanish 발생할 수 있으므로<br/> Q와 K를 normalize)</li> <li>target token에 attention할 수 있도록<br/> <code class="language-plaintext highlighter-rouge">masked attention</code> 사용</li> <li> <d-cite key="50">[14]</d-cite> <p>에서처럼<br/> 빠른 inference를 위해 <code class="language-plaintext highlighter-rouge">KV-Cache</code> 사용<br/> (training할 때는 전체 input sequence에 대해 K, Q, V를 계산하지만,<br/> inference할 때는 prev. token에서 계산한 K, V를 cache에 저장한 채 매번 Q만 새로 계산함으로써 input sequence 전체에 대해 K, V를 매번 계산할 필요 없어 계산 비용 감소)</p> </li> <li>NTE Module has negligible overhead on runtime</li> </ul> </li> </ul> </li> <li> <p>Loss :<br/> \(L_{RGB} = L_{MSE} + \lambda L_{LPIPS}\) with \(\lambda = 0.5\)</p> </li> <li>BTimer and NTE Module : <ul> <li>NTE Module로 직접 RGB image 예측하여<br/> novel-view-synthesis 할 수 있긴 한데, 그럼 성능 안 좋음<br/> (Ablation Study에 있음)</li> <li>feed-forward transformer (NTE Module)로 FPI pre-process한 뒤<br/> <code class="language-plaintext highlighter-rouge">feed-forward transformer</code> (BTimer)로 data info. 포착하여<br/> <code class="language-plaintext highlighter-rouge">3DGS param. 예측</code>한 뒤<br/> 3DGS rasterization으로 novel-view-synthesis</li> </ul> </li> </ul> <h3 id="curriculum-training-at-scale">Curriculum Training at Scale</h3> <ul> <li>Generalizability : <ul> <li>data 다양성이 많을수록 model generalizability가 높아짐<br/> static dataset은 많이 존재하고<br/> dynamic dataset은 적게 존재하지만 motion awareness 및 temporal consistency 확보 가능</li> <li>본 논문의 model인 BTimer는<br/> generalizable to both static and dynamic scenes <ul> <li>static scene : equalize all \(t_{b}\)</li> <li>dynamic scene : recon. at arbitrary bullet \(t_{b}\)</li> <li>different domain에서는 different model 필요로 하는<br/> GS-LRM <d-cite key="79">[12]</d-cite> or MVSplat <d-cite key="10">[8]</d-cite> 과는 다름</li> </ul> </li> </ul> </li> <li>Curriculum Training : <ul> <li>Stage 1) <code class="language-plaintext highlighter-rouge">Low-res to High-res Static Pretraining</code> <ul> <li>static dataset으로 pre-train <ul> <li>both synthetic and real-world</li> <li>390K training samples</li> <li>normalize datasets to be bounded in \(10^{3}\) cube</li> <li>종류 : <ul> <li>Objaverse</li> <li>RE10K</li> <li>MVImgNet</li> <li>DL3DV</li> </ul> </li> </ul> </li> <li>no time embedding<br/> (static scene이니까)</li> <li>data distribution이 복잡하기 때문에<br/> coarse 세팅 (low-resol.(\(128 \times 128\)) and few-view(\(| I_{c} | = 4\)))에서 시작해서<br/> 점점 fine 세팅 (high-resol.(\(256 \times 256 \rightarrow 512 \times 512\)))으로 train</li> </ul> </li> <li>Stage 2) <code class="language-plaintext highlighter-rouge">Dynamic Scene Co-training</code> <ul> <li>dynamic dataset으로 fine-tuning <ul> <li>종류 : <ul> <li>Kubric</li> <li>PointOdyssey</li> <li>DynamicReplica</li> <li>Spring</li> </ul> </li> </ul> </li> <li>4D dynamic dataset이 부족하기 때문에<br/> 안정적인 훈련을 위해<br/> static dataset을 함께 사용하여 co-training</li> <li>Internet video로부터 camera pose를 매기는 customized pipeline 구축하여<br/> real-world data에 대한 robustness 향상 <ul> <li>먼저 PANDA-70M dataset에서 random select한 video를 20s 길이의 clips로 자르기</li> <li>SAM으로 video의 dynamic objects를 mask out</li> <li>DROID-SLAM으로 video camera pose를 estimate</li> <li>reprojection error 측정하여 low-quality의 video 및 pose는 필터링</li> <li>최종적으로 obtain 40K clips with high-quality camera trajectories</li> </ul> </li> </ul> </li> <li>Stage 3) <code class="language-plaintext highlighter-rouge">Long-context Window Fine-tuning</code> <ul> <li>NTE Module 말고 BTimer model에만 적용</li> <li>context (input) image 수를<br/> \(| I_{c} | = 4\) 에서 \(| I_{c} | = 12\) 로 늘려서<br/> long video recon.하는 데 도움</li> </ul> </li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <h3 id="implementation">Implementation</h3> <ul> <li>Implementation : <ul> <li>Backbone Transformer :<br/> FlashAttention-3 <d-cite key="13">[15]</d-cite> and FlexAttention <d-cite key="24">[16]</d-cite></li> <li>3DGS Rasterization :<br/> gsplat library <d-cite key="74">[17]</d-cite></li> <li>Training Schedule : <ul> <li>BTimer :<br/> totally 4 days on 64 A100 GPUs <ul> <li>Stage 1)<br/> \(128^{2}\) resol. 90K iter. init lr \(4 \times 10^{-4}\)<br/> \(\rightarrow\)<br/> \(256^{2}\) resol. 90K iter. init lr \(2 \times 10^{-4}\)<br/> \(\rightarrow\)<br/> \(512^{2}\) resol. 50K iter. init lr \(1 \times 10^{-4}\)</li> <li>Stage 2)<br/> 10K iter.</li> <li>Stage 3)<br/> 5K iter.</li> </ul> </li> <li>NTE Module : <ul> <li>Stage 1)<br/> \(128^{2}\) resol. 140K iter. init lr \(4 \times 10^{-4}\)<br/> \(\rightarrow\)<br/> \(256^{2}\) resol. 60K iter. init lr \(2 \times 10^{-4}\)<br/> \(\rightarrow\)<br/> \(512^{2}\) resol. 30K iter. init lr \(1 \times 10^{-4}\)</li> <li>Stage 2)<br/> 20K iter.</li> </ul> </li> </ul> </li> <li>Inference time : <ul> <li>BTimer : <ul> <li>20 ms for 4-view \(256^{2}\) recon.</li> <li>150 ms for 12-view \(256^{2}\) recon.</li> <li>4.2 s for 12-view \(512 \times 896\) recon.</li> </ul> </li> <li>NTE : <ul> <li>0.44 s for 4-view \(512 \times 896\) recon. w/o KV cache</li> </ul> </li> </ul> </li> </ul> </li> </ul> <h3 id="results">Results</h3> <ul> <li>Dynamic Novel-View-Synthesis (Quantitative) : <ul> <li>DyCheck Benchmark <d-cite key="22">[18]</d-cite> : <ul> <li>dataset :<br/> DyCheck iPhone dataset (7 dynamic scenes by 3 synchronized cameras)</li> <li>baseline :<br/> TiNeuVox, NSFF, T-NeRF, Nerfies, HyperNeRF, PGDVS, direct depth warp <ul> <li>BTimer는 per-scene optimization method에 competitive performance 달성</li> <li>BTimer는 consistent depth estimate 없이도 PGDVS보다 성능 좋음</li> </ul> </li> </ul> </li> <li>NVIDIA Dynamic Scene Benchmark <d-cite key="75">[19]</d-cite> : <ul> <li>dataset :<br/> NVIDIA Dynamic Scene dataset (9 dynamic scenes by 12 forward-facing synchronized cameras)</li> <li>baseline :<br/> HyperNeRF, DynNeRF, NSFF, RoDynRF, MonoNeRF, 4D-GS, Casual-FVS <ul> <li>feed-forward 방식이므로 optimization time 필요 없음</li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/6m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/6m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/6m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/6m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Dynamic Novel-View-Synthesis (Qualitative) : <ul> <li>test on real-world scene 위해<br/> DAVIS dataset의 monocular videos 이용하고,<br/> customized pipeline으로 camera pose estimate해서 사용</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/8m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/8m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/8m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/8m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/7m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/7m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/7m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/7m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Static Novel-View-Synthesis : <ul> <li>RealEstate10K Benchmark : <ul> <li>baseline : pixelSplat, MVSplat, GPNR, GS-LRM</li> </ul> </li> <li>Tanks &amp; Temples Benchmark :<br/> from InstantSplat Benchmark <ul> <li>baseline : GS-LRM (SOTA)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/10m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/10m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/10m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/10m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> single dataset(Ours-Static)보다 mixed-dataset(Ours-Full) 사용하는 게 generalization 및 성능 훨씬 좋음 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/9m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/9m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/9m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/9m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="ablation-study">Ablation Study</h3> <ul> <li>Ablation 1) Context Frames : <ul> <li>train할 때 context frames 더 많이 쓰면 3DGS prediction이 progressively 많아지므로 more complete scene recon. 가능</li> <li>inference할 때 서로 멀리 떨어진 context frames를 arbitrarily 골라서 커버하는 view 범위 넓힘</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/12m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/12m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/12m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/12m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Ablation 2) Curriculum Training : <ul> <li>Stage 1)<br/> single dataset 말고 multiple dataset 써야<br/> geometry와 sharp detail 잡는 데 도움</li> <li>Stage 2)<br/> static scene을 섞어서 co-train해야<br/> geometry 및 rich detail 잡는 데 도움</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/11m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/11m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/11m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/11m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Ablation 3) Interpolation Supervision : <ul> <li>temporal and multi-view consistency 챙기는 데 도움</li> <li>bullet timestamp가 input frames에 없을 때를 훈련하지 않으면<br/> white-edge artifacts 생김<br/> (interpolation loss를 cheat하려고 camera에 너무 가까운 3DGS를 만들기 때문)</li> </ul> </li> <li>Ablation 4) NTE Module : <ul> <li>motion이 빠르고 복잡할 때 도움<br/> (ghosting artifacts 해소)</li> <li>BTimer 없이<br/> 3D info. 쓰지 않는 NTE Module만으로 novel-view-synthesis 수행하면<br/> input camera trajectory와 먼 novel-view에 대해서는 잘 recon. 못 함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/13m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/13m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/13m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2025-01-10-BTimer/13m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="conclusion">Conclusion</h2> <ul> <li>Limitation : <ul> <li><code class="language-plaintext highlighter-rouge">geometry</code> : <ul> <li>SOTA depth prediction model <d-cite key="71">[20]</d-cite> 만큼 정확하게<br/> geometry (depth map)을 recover하지는 않음</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">memory issue</code> :<br/> transformer를 사용하다보니 memory 많이 소요 <ul> <li>need 3 days on 64 A100 GPU (40GB VRAM)</li> <li>up to \(512 \times 904\) spatial resol.</li> <li>up to 12 context frames</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">pose</code> : <ul> <li>need camera pose param.</li> <li>future work :<br/> DUSt3R, NoPoSplat처럼 pose-free일 순 없을까?</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">non-generative</code> : <ul> <li>본 논문은 feed-forward Transformer 모델이고<br/> generative model이 아니기 때문에<br/> cannot generate unseen region<br/> (unseen view를 예측하는 view extrapolation 불가능)</li> <li>future work :<br/> generative prior 사용하여 view extrapolation 수행</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">novelty</code> : <ul> <li>사실 arbitrary bullet timestamp를 input token에 추가한 뒤<br/> 모든 input frames를 transformer에 때려넣고 원하는 bullet timestamp에서의 frame을 뽑아내는 video interpolation 방식으로 보이고,<br/> 다만 차이점은 각 frame image를 transformer output으로 구하는 게 아니라 각 frame의 3DGS param.를 transformer output으로 구하는 것이고..<br/> 모델 자체의 novelty보다는 implementation을 잘 해서 결과 좋게 낸 것 같다..</li> <li>(static, dynamic) data를 많이 쓰고 stage 별 training을 통해 높은 performance를 달성할 수 있었고<br/> feed-forward 방식을 통해 빠른 속도를 달성</li> </ul> </li> </ul> </li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br/> NTE Module이 마지막에 linear layer로 RGB value를 예측함으로써<br/> pixel-space에서 RGB image at bullet timestamp 를 interpolate하고<br/> 이를 BTimer에 사용하는데,<br/> latent-space에서 interpolation 다룬 뒤 BTimer에 넘기면 더 성능 좋아질 수 있지 않을까?</p> </li> <li> <p>A1 :<br/> TBD</p> </li> <li> <p>Q2 :<br/> NTE Module이 예측한 pixel-space RGB image가 BTimer의 input으로 들어가는데,<br/> NTE Module output이 부정확하면 drift 연쇄적으로 BTimer의 결과에도 악영향 미칠 거 같아.<br/> refinement, uncertainty(confidence) 등으로 NTE Module output의 부정확성을 감소시켜 성능 높일 수 있을까?</p> </li> <li> <p>A2 :<br/> TBD</p> </li> <li> <p>Q3 :<br/> limitation 중에 unseen view는 recon.하지 못한다는 게 있는데 (view extrapolation 불가능)<br/> 본 논문이 generalizability를 가진다는 말은<br/> static and dynamic unseen dataset (scene)에 대응할 수 있어서인거지?</p> </li> <li> <p>A3 :<br/> TB</p> </li> <li> <p>Q4 :<br/> BTimer와 NTE Module을 각각 별도로 train하므로 not end-to-end인데<br/> end-to-end training할 수는 없을까?</p> </li> <li> <p>A4 :<br/> TBD</p> </li> <li> <p>Q5 :<br/> pixelSplat에서는 a pair of images를 transformer의 input으로 넣어 둘의 관계를 파악하여 static scene recon.하는데<br/> dynamic scene recon.에서는 motion 정보를 캡처해야 하기 때문에<br/> transformer의 input으로 두 장이 아니라 여러 장의 image를 넣어주어야 하는거야?</p> </li> <li> <p>A5 :<br/> TBD</p> </li> <li> <p>Q6 :<br/> interpolation supervision으로 context (input) frame이 아닌 그 사이의 frame에 대해 rendering할 때 GT는 무엇으로 두나요?</p> </li> <li> <p>A6 :<br/> context (input) frame의 image와 camera pose를 직접 interpolate하여 사용 <code class="language-plaintext highlighter-rouge">???</code></p> </li> <li> <p>Q7 :<br/> 다른 논문들을 보면 4DGS처럼 canonical time에 대한 시간에 따른 Gaussian 변화량을 MLP로 학습하거나,<br/> 또는 Dynamic Gaussian Marbles처럼 prev. frame의 GS가 next frame의 GS에 미치는 영향을 학습하기 위해 global adjustment해서 gaussian trajectory를 학습함으로써<br/> GS끼리 정보를 주고받습니다.<br/> 본 논문에서는 모든 input frames를 BTimer에 parallel하게 넣어준 뒤 bullet (target) timestamp마다 3DGS param.를 따로 뽑아내는데<br/> 그럼 3DGS끼리는 정보를 공유하지 않는 건가요?</p> </li> <li> <p>A7 :<br/> 네, 일단 BTimer 이 논문에서는 모든 input frames를 BTimer에 parallel하게 때려넣은 뒤 self-attention에 의존해서 t를 포함한 frames 간의 관계를 학습하는 것 같습니다.</p> </li> <li> <p>Q8 :<br/> 어차피 3DGS끼리 정보를 공유하지 않는 거면 굳이 3DGS를 사용한 이유가 있나요?</p> </li> <li> <p>A8 :<br/> novel-view-synthesis task에서 novel camera pose에 대한 image를 뽑아내려면 3D info.를 이용해야 recon.이 잘 될 것이기 때문에 3DGS를 이용합니다.<br/> 논문에서 언급되어 있듯이 NTE Module만을 이용해서 from 2D to 2D로 novel-view-synthesis task를 수행하면 quality가 좋지 않았다고 합니다.</p> </li> <li> <p>Q9 :<br/> camera pose의 영향을 많이 받을 것 같아요. 만약에 input frame 3에서 보였던 물체가 frame 밖을 벗어나거나 occlusion 때문에 input frame 4에서 안 보이게 되었을 때에도 잘 recon.하려면 prev. frame의 3D info. 정보를 결합해서 반영하는 식이어야 할 것 같은데, 각 bullet timestamp의 3D info.끼리 어떻게 relate되는지에 대한 내용이 없으니까 이와 같은 상황에 잘 대응할 수 있는지 궁금합니다.</p> </li> <li> <p>A9 :<br/> TBD</p> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="general"/><category term="dynamic"/><category term="GS"/><category term="view"/><category term="synthesis"/><summary type="html"><![CDATA[Feed Forward Bullet Time Reconstruction of Dynamic Scenes from Monocular Videos (CVPR 2025)]]></summary></entry><entry><title type="html">CUDA Programming</title><link href="https://semyeong-yu.github.io/blog/2024/CUDA/" rel="alternate" type="text/html" title="CUDA Programming"/><published>2024-12-29T12:00:00+00:00</published><updated>2024-12-29T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/CUDA</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/CUDA/"><![CDATA[<blockquote> <p>Lecture :<br/> 24F EE514 Parallel Computing<br/> by KAIST Minsoo Rhu <a href="https://sites.google.com/view/kaist-via">VIA Research Group</a></p> </blockquote> <h2 id="spmd-programming">SPMD Programming</h2> <ul> <li>GPGPU Programming : <ul> <li>serial part : in CPU C code (host)</li> <li>parallel part : in GPU SPMD kernel code (device)</li> </ul> </li> <li>SPMD (Single Program Multiple Data) : <ul> <li>grid (kernel) \(\supset\) block (SM) \(\supset\) warp \(\supset\) thread <ul> <li>gridDim : grid 내 block 개수</li> <li>blockIdx : block index</li> <li>blockDim : block 내 thread 개수</li> <li>threadIdx : thread index</li> </ul> </li> <li>보통<br/> 1 warp = 32 threads<br/> 1 block = 256 threads</li> <li>block 내 threads끼리 shared memory를 공유</li> </ul> </li> <li>memory address space : <ul> <li>1 address에 1 Byte를 저장하므로<br/> memory address가 32-bit일 때<br/> \(2^{32}\) Byte 저장 가능</li> <li>linear memory address space를 implement하는 것은 복잡</li> </ul> </li> <li>Shared Memory Model <ul> <li>shared var. in shared address space에 저장함으로써 threads끼리 communicate</li> <li>atomicity : threads끼리 겹치지 않도록 mutual exclusion <ul> <li>semaphore</li> <li>mutex : \(\text{LOCK(mylock); //critical section UNLOCK(mylock);}\)</li> <li>atomic : \(\text{atomic{//critical section}}\) 또는 \(\text{atomicAdd(x, 10);}\)</li> </ul> </li> <li>efficient implementation을 위해 hardware support 필요<br/> processor 수가 많으면 costly할 수 있음</li> <li>e.g. OpenMP</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-CUDA/2m.PNG-480.webp 480w,/assets/img/2024-12-29-CUDA/2m.PNG-800.webp 800w,/assets/img/2024-12-29-CUDA/2m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-CUDA/2m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Message Passing Model <ul> <li>thread는 각자 private address space를 가지고 있고<br/> threads끼리 message 주고받음으로써 communicate</li> <li>system-wide load/store를 위한 hardware implementation 필요 없음</li> <li>e.g. Open MPI</li> </ul> </li> </ul> <h2 id="cuda-programming">CUDA Programming</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-CUDA/3m.PNG-480.webp 480w,/assets/img/2024-12-29-CUDA/3m.PNG-800.webp 800w,/assets/img/2024-12-29-CUDA/3m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-CUDA/3m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>CUDA APIs : <ul> <li>\(\text{cudaMalloc()}\) : device(GPU) global memory에 allocate</li> <li>\(\text{cudaFree()}\) : device(GPU) global memory free</li> <li>\(\text{cudaMemcpy()}\) : data transfer between host(CPU) and device(GPU)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-CUDA/4m.PNG-480.webp 480w,/assets/img/2024-12-29-CUDA/4m.PNG-800.webp 800w,/assets/img/2024-12-29-CUDA/4m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-CUDA/4m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>CUDA Function : <ul> <li>\(\text{__global__}\) :<br/> kernel function 정의 (host에서 call해서 device에서 execute) 정의<br/> return void</li> </ul> </li> </ul> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// define kernel func.</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">vecAddKernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">A</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">B</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">C</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span>
<span class="p">{</span>
  <span class="c1">// global rank</span>
  <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>

<span class="c1">// n : global rank</span>
<span class="n">dim3</span> <span class="nf">DimGrid</span><span class="p">((</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mi">256</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="c1">// 256 threads per block</span>
<span class="c1">// DimBlock.x = 256</span>
<span class="n">dim3</span> <span class="nf">DimBlock</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>

<span class="c1">// call kernel func.</span>
<span class="c1">// kernel func.&lt;&lt;#block, #thread&gt;&gt;(param.)</span>
<span class="n">vecAddKernel</span><span class="o">&lt;&lt;</span><span class="n">DimGrid</span><span class="p">,</span> <span class="n">DimBlock</span><span class="o">&gt;&gt;</span><span class="p">(</span><span class="n">d_A</span><span class="p">,</span> <span class="n">d_B</span><span class="p">,</span> <span class="n">d_C</span><span class="p">,</span> <span class="n">n</span><span class="p">);</span>
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-CUDA/5m.PNG-480.webp 480w,/assets/img/2024-12-29-CUDA/5m.PNG-800.webp 800w,/assets/img/2024-12-29-CUDA/5m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-CUDA/5m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>limitation : bottlenecked by global memory bandwidth<br/> \(\rightarrow\)<br/> solution : scratchpad memory (shared memory) <ul> <li>cache : transparent to programmer (it just works)</li> <li>scratchpad : programmer has to manually manage data movement</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-CUDA/6m.PNG-480.webp 480w,/assets/img/2024-12-29-CUDA/6m.PNG-800.webp 800w,/assets/img/2024-12-29-CUDA/6m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-CUDA/6m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>CUDA Variable : <ul> <li>\(\text{int LocalVar;}\) :<br/> <code class="language-plaintext highlighter-rouge">register</code>에 저장하여 thread 혼자서 사용</li> <li>\(\text{(__device__) __shared__ int SharedVar;}\) :<br/> <code class="language-plaintext highlighter-rouge">shared memory</code>에 저장하여 block 내 threads끼리 공유</li> <li>\(\text{__device__ int GlobalVar;}\) :<br/> <code class="language-plaintext highlighter-rouge">global memory</code>에 저장하여 grid 내 모든 threads가 공유</li> <li>\(\text{(__device__) __constant__ int SharedVar;}\) :<br/> <code class="language-plaintext highlighter-rouge">constant memory</code>에 저장하여 grid 내 모든 threads가 공유</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-29-CUDA/7m.PNG-480.webp 480w,/assets/img/2024-12-29-CUDA/7m.PNG-800.webp 800w,/assets/img/2024-12-29-CUDA/7m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-29-CUDA/7m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="tiled-matrix-multiplication">Tiled Matrix Multiplication</h2> <ul> <li>Matrix Multiplication without shared memory :<br/> each thread has to access global memory,<br/> so performance is bottlenecked by global memory bandwidth</li> </ul> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">MatrixMulKernel</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">M</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">P</span><span class="p">,</span> <span class="kt">int</span> <span class="n">Width</span><span class="p">){</span>
  <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>

  <span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

  <span class="k">if</span> <span class="p">((</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">Width</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">col</span> <span class="o">&lt;</span> <span class="n">Width</span><span class="p">)){</span>
    <span class="kt">float</span> <span class="n">value</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="c1">// each thread computes one element of output matrix</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">Width</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">){</span>
      <span class="n">value</span> <span class="o">+=</span> <span class="n">M</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">Width</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">N</span><span class="p">[</span><span class="n">k</span> <span class="o">*</span> <span class="n">Width</span> <span class="o">+</span> <span class="n">col</span><span class="p">];</span>
    <span class="p">}</span>
    
    <span class="n">P</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">Width</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span><span class="p">;</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <ul> <li>Tiling Algorithm : <ul> <li>17p TBD</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="others"/><category term="CUDA"/><category term="GPU"/><category term="kernel"/><category term="parallel"/><summary type="html"><![CDATA[.cu coding]]></summary></entry><entry><title type="html">Quark</title><link href="https://semyeong-yu.github.io/blog/2024/Quark/" rel="alternate" type="text/html" title="Quark"/><published>2024-12-23T12:00:00+00:00</published><updated>2024-12-23T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Quark</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Quark/"><![CDATA[<h2 id="quark---real-time-high-resolution-and-general-neural-view-synthesis">Quark - Real-time, High-resolution, and General Neural View Synthesis</h2> <h4 id="john-flynn-michael-broxton-lukas-murmann-lucy-chai-matthew-duvall-clément-godard-kathryn-heal-srinivas-kaza-stephen-lombardi-xuan-luo-supreeth-achar-kira-prabhu-tiancheng-sun-lynn-tsai-ryan-overbeck">John Flynn, Michael Broxton, Lukas Murmann, Lucy Chai, Matthew DuVall, Clément Godard, Kathryn Heal, Srinivas Kaza, Stephen Lombardi, Xuan Luo, Supreeth Achar, Kira Prabhu, Tiancheng Sun, Lynn Tsai, Ryan Overbeck</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2411.16680">https://arxiv.org/abs/2411.16680</a><br/> project website :<br/> <a href="https://quark-3d.github.io/">https://quark-3d.github.io/</a><br/> reference :<br/> Presentation of https://charlieppark.kr from 3D-Nerd Community</p> </blockquote> <h2 id="contribution">Contribution</h2> <ul> <li>Architecture : <ul> <li>3D space에서 ray를 쏘거나(NeRF) Gaussian list를 구해서(3DGS) alpha-compositing하는 게 아니라<br/> <code class="language-plaintext highlighter-rouge">layered RGB image(or depth map)</code>를 구해서 alpha-compositing</li> <li><code class="language-plaintext highlighter-rouge">target view가 어떤 input view에 얼만큼 attention해야 하는지</code>를 <code class="language-plaintext highlighter-rouge">iteratively refine</code>하여<br/> Blend Weights를 구해서 input view들을 interpolate하는 방식</li> <li>refinement로 Blend Weights 구해서<br/> input images를 blend하여 layered RGB images를 구하므로<br/> input view가 멀리멀리 sparse하게 떨어져 있어야<br/> recon.할 때 모든 영역 커버 가능</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Generalizable</code> : <ul> <li>pre-trained model 가져온 뒤<br/> pre-trained model이 학습하지 못했던 <code class="language-plaintext highlighter-rouge">unseen scene</code>에 대해<br/> <code class="language-plaintext highlighter-rouge">fine-tuning 없이</code> <code class="language-plaintext highlighter-rouge">refinement</code>로<br/> layered depth map 쫘르륵 얻어내서 novel view recon. 가능!</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Real-time</code> <code class="language-plaintext highlighter-rouge">Reconstruction</code> and Rendering : <ul> <li>3DGS에서는 real-time rendering이었는데<br/> 본 논문은 recon. 자체도 real-time<br/> (inference하는 데 총 33ms at 1080p with single A100 GPU)</li> </ul> </li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li>Generalizable : <ul> <li>IBRNet :<br/> rendering 시간은 오래 걸리지만 generalizable</li> <li>ENeRF :<br/> cost volume, depth-guided sampling, volume rendering 사용</li> <li>GPNR :<br/> 2-view VFT, Epipolar Transformer 사용</li> <li>CO3D - NeRFormer :<br/> 반복 between attention on feature-dim. and attention on ray-direction-dim.</li> </ul> </li> <li>Quark의 직계 조상 paper : <ul> <li>DeepView <d-cite key="DeepView">[1]</d-cite> : <ul> <li>MPI (여러 depth에 대해 image를 중첩한 multi-plane image)</li> <li>한계 : input view와 target view 간의 camera 이동이 크면 안 됨</li> </ul> </li> <li>Immersive light field video with a layered mesh representation <d-cite key="Immersive">[2]</d-cite> : <ul> <li>MSI (여러 depth에 대해 곡면 image를 중첩한 multi-spherical image) (= layered mesh)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/2m.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/2m.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/2m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-23-Quark/2m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="overview">Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/3m.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/3m.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/3m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-23-Quark/3m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>I/O : <ul> <li>input : sparse multi-view images (\(\in R^{M \times H \times W \times 3}\))<br/> (sensitive to view selection)<br/> (pose 정보 필요)</li> <li>output : novel view image</li> <li>Quark는 pretrained model (pretrained with 8 input views of scenes(Spaces, RFF, Nex-Shiny, and SWORD)) 가져와서<br/> unseen scene에 대한 refinement로 novel target view synthesis 가능 (generalizable) <ul> <li>Spaces : Quark의 직계 조상 격인 DeepView에서 사용한 dataset</li> <li>RFF : NeRF에서 사용한 Real Forward Facing dataset</li> <li>Nex-Shiny : NeX에서 사용한 shiny object이 포함된 dataset</li> <li>SWORD : real-world scene dataset</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/1m.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/1m.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/1m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-23-Quark/1m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Architecture :<br/> U-Net style <ul> <li>Encoder :<br/> Obtain feature pyramid \(I_{\downarrow 8}, I_{\downarrow 4}, I_{\downarrow 2}, I_{\downarrow 0}\) from input image</li> <li>Iterative Updates : <ul> <li>pre-trained model을 가져와서 학습하는데,<br/> layered depth map을 업데이트하는 방법은<br/> gradient descent 이용한 <code class="language-plaintext highlighter-rouge">fine-tuning이 아니라</code><br/> input view feature 이용한 <code class="language-plaintext highlighter-rouge">refinement</code>임!!</li> <li>U-Net skip-connection과 비슷하지만 <code class="language-plaintext highlighter-rouge">Update &amp; Fuse 단계가 novel</code><br/> (아래에서 별도로 설명)</li> </ul> </li> <li>Upsample &amp; Activate : <ul> <li>image resolution으로 upsample한 뒤<br/> Layered Depth Map at target view 구함 <ul> <li>Depth \(d \in R^{L \times H \times W \times 1}\)<br/> (이 때, depth map은 linear in disparity (가까운 high-freq. 영역에서 더 촘촘히))</li> <li>Opacity \(\sigma \in R^{L \times H \times W \times 1}\)</li> <li>Blend Weights \(\beta \in R^{L \times H \times W \times M}\)<br/> by attention softmax weight</li> </ul> </li> </ul> </li> <li>Rendering : <ul> <li>input images \(\in R^{M \times H \times W \times 3}\) 를 Layered Depth Map (target view)로 back-project한 뒤<br/> Blend Weights \(\beta\) 로 input images를 blend해서 per-layer RGB 얻음</li> <li>Opacity \(\sigma\) 로 per-layer RGB를 alpha-compositing해서 final RGB image at target view 얻고,<br/> Opacity \(\sigma\) 로 Depth \(d\) 를 alpha-compositing해서 Depth Map 얻음</li> <li>training할 때는 stadard differentiable rendering 사용하지만<br/> inference할 때는 1080p resol. at 1.3 ms per frame 위해 CUDA-optimized renderer 사용</li> </ul> </li> </ul> </li> </ul> <h2 id="method">Method</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/4m.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/4m.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/4m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-23-Quark/4m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Update &amp; Fuse : <ul> <li>Step 1) Render to Input Views <ul> <li>from <code class="language-plaintext highlighter-rouge">layer space (target view)</code> to <code class="language-plaintext highlighter-rouge">image space (input view)</code><br/> (feature pyramid \(I_{\downarrow k}\) 와 합치기 위해!)</li> <li>feature volume \(V^{(n)}\)<br/> \(\rightarrow\) obtain appearance \(a\), density \(\sigma\), depth map \(d\)<br/> (depth map \(d = \delta + \text{tanh}\) 는 depth anchor \(\delta\) 근처의 depth)<br/> \(\rightarrow\) project from target-view into input-view by \(P_{\theta}\)<br/> \(\rightarrow\) obtain rendered feature \(\tilde I\) by alpha-compositing \(O\) at input-view<br/> (\(\tilde I\) : intermediate LDM(layered depth map))</li> </ul> </li> <li>Step 2) Update Block <ul> <li><code class="language-plaintext highlighter-rouge">rendered feature</code> \(\tilde I\) 를<br/> <code class="language-plaintext highlighter-rouge">feature pyramid</code> \(I_{\downarrow k}\), <code class="language-plaintext highlighter-rouge">input view-direction</code> \(\gamma\) 등 input image에 대한 정보와 섞음 <ul> <li>input view-direction 넣어줄 때 Ray Encoding \(\gamma\) 수행 : <ul> <li>obtain difference vector (아래 그림 참고)<br/> (input view가 target view에서 멀리 떨어져 있을수록 값이 큼)<br/> \(\rightarrow\) tanh and Sinusoidal PE</li> <li>tanh 사용하므로<br/> difference vector가 0 근처일 때<br/> 즉, input view가 target view 근처일 때 gradient 많이 반영</li> <li>input view’s ray가 frustum 밖으로 벗어나더라도<br/> near, far plane과의 교점을 구할 수 있으므로<br/> Ray Encoding 가능</li> <li>view-direction 넣어줘야<br/> view-dependent color 만들 수 있고<br/> reflection, non-lambertian surface 잘 구현 가능</li> </ul> </li> </ul> </li> </ul> </li> <li>Step 3) Back-project <ul> <li>from <code class="language-plaintext highlighter-rouge">image space (input view)</code> to <code class="language-plaintext highlighter-rouge">layer space (target view)</code><br/> (feature volume \(V^{(n)}\) 과 합치기 위해!)</li> <li>back-project from input-view into target-view by \(P_{\theta}^{T} (I, d)\)<br/> \(\rightarrow\) obtain residual feature \(\Delta\)</li> </ul> </li> <li>Step 4) One-to-Many Attention <ul> <li><code class="language-plaintext highlighter-rouge">feature volume</code> \(V^{(n)}\) 을 <code class="language-plaintext highlighter-rouge">query</code>로,<br/> Step 1~3)에서 얻은 <code class="language-plaintext highlighter-rouge">residual feature</code> \(\Delta\) 를 <code class="language-plaintext highlighter-rouge">key, value</code>로 하여<br/> One-to-Many attention 수행하여<br/> updated feature volume \(V^{(n+1)}\) 얻음<br/> Then, target view가 input view의 feature들을 aggregate하여 이용할 수 있게 됨!!<br/> 즉, target view가 어떤 input view에 얼만큼 attention해야 하는지! <ul> <li><code class="language-plaintext highlighter-rouge">query</code> : <code class="language-plaintext highlighter-rouge">target view</code> 정보 at target view space</li> <li><code class="language-plaintext highlighter-rouge">key, value</code> : <code class="language-plaintext highlighter-rouge">input view</code> 정보 at target view space</li> <li><code class="language-plaintext highlighter-rouge">One-to-Many attention</code> : <ul> <li>cross-attention과 비슷하지만<br/> <code class="language-plaintext highlighter-rouge">redundant matrix multiplication 없애서</code><br/> complexity 줄여서<br/> real-time reconstruction에 기여!</li> <li>\(\text{MultiHead}(Q, K, V) = \text{concat}(\text{head}_{1}, \cdots, \text{head}_{h}) W^{O}\)<br/> where \(\text{head}_{i} = \text{Attention}(QW_{i}^{Q}, KW_{i}^{K}, VW_{i}^{V})\) 식을 써보면<br/> \(W_{i}^{Q} (W_{i}^{K})^{T}\) 항에서 \(W^{Q}\) 와 \(W^{K}\) 가 redundant 하고<br/> \(\text{concat}(\cdots W_{i}^{V}) W^{O}\) 항에서 \(W^{V}\) 와 \(W^{O}\) 가 redundant 하므로<br/> \(\text{head}_{i} = \text{Attention}(QW_{i}^{Q}, K, V)\) 로 바꿔서<br/> \(W^{Q}\) 와 \(W^{O}\) 만 사용</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/5m.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/5m.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/5m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-23-Quark/5m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> difference vector for input view-direction (Ray Encoding) </div> <h2 id="result">Result</h2> <ul> <li>Training : <ul> <li>Dataset : Spaces, RFF, Nex-Shiny, SWORD</li> <li>Loss : \(\text{10 * L1} + \text{LPIPS}\)</li> <li>Input : 8 views (randomly sampled from 16 views nearest to object)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/6m.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/6m.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/6m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-23-Quark/6m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Inference time : recon.까지 포함해서 총 33ms at 1080p single A100 GPU</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/7m.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/7m.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/7m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-23-Quark/7m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Generalizable method와의 비교 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/8m.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/8m.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/8m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-23-Quark/8m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/9m.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/9m.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/9m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-23-Quark/9m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Non-Generalizable method와의 비교 </div> <h2 id="discussion">Discussion</h2> <ul> <li>Limitation : <ul> <li>view selection :<br/> training할 때 sparse(8개) input views를 사용하는데, <code class="language-plaintext highlighter-rouge">view selection</code>에 매우 민감 (중요함) (heuristic)</li> <li>Blend Weights :<br/> target view RGB image를 rendering하기 전에 input RGB images를 blend하는데, <ul> <li><code class="language-plaintext highlighter-rouge">view dependency</code>를 잘 캡처 못한다<br/> \(\rightarrow\) Ray Encoding으로 해소하긴 함</li> <li>input images의 <code class="language-plaintext highlighter-rouge">focal length</code>가 각기 다르면 잘 recon.하지 못한다</li> </ul> </li> <li>sparse input :<br/> real-time rendering 뿐만 아니라 real-time recon. 위해<br/> 적은 수(8 ~ 16)의 <code class="language-plaintext highlighter-rouge">sparse inputs</code> 사용</li> <li>light, shadow 고려 X</li> <li>conv. network를 일반화에 사용했을 때 생기는 깨지는 artifacts 발생 (홈페이지 영상 참고)</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="general"/><category term="view"/><category term="synthesis"/><summary type="html"><![CDATA[Real-time, High-resolution, and General Neural View Synthesis (SIGGRAPH 2024)]]></summary></entry><entry><title type="html">PhysGaussian</title><link href="https://semyeong-yu.github.io/blog/2024/PhysGaussian/" rel="alternate" type="text/html" title="PhysGaussian"/><published>2024-12-20T12:00:00+00:00</published><updated>2024-12-20T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/PhysGaussian</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/PhysGaussian/"><![CDATA[<h2 id="physgaussian---physics-integrated-3d-gaussians-for-generative-dynamics">PhysGaussian - Physics-Integrated 3D Gaussians for Generative Dynamics</h2> <h4 id="tianyi-xie-zeshun-zong-yuxing-qiu-xuan-li-yutao-feng-yin-yang-chenfanfu-jiang">Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, Chenfanfu Jiang</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2311.12198">https://arxiv.org/abs/2311.12198</a><br/> project website :<br/> <a href="https://xpandora.github.io/PhysGaussian/">https://xpandora.github.io/PhysGaussian/</a><br/> code :<br/> <a href="https://github.com/XPandora/PhysGaussian">https://github.com/XPandora/PhysGaussian</a><br/> reference :<br/> <a href="https://xoft.tistory.com/101">https://xoft.tistory.com/101</a></p> </blockquote> <h2 id="contribution">Contribution</h2> <ul> <li><code class="language-plaintext highlighter-rouge">Physics Simulation</code>을 <code class="language-plaintext highlighter-rouge">3DGS</code>에 결합 : <ul> <li>3DGS에 부피, 질량, 속도 (Physics) property를 부여하여<br/> 3DGS의 covariance 및 rotation matrix가 시간에 따라 물리 법칙에 따라 변화</li> <li>MPM simulation 장점과 3DGS rendering 장점을 결합하여<br/> unified simulation-rendering pipeline 제시</li> </ul> </li> </ul> <p>결과 영상이 재밌음!</p> <h2 id="overview">Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-20-PhysGaussian/1m.PNG-480.webp 480w,/assets/img/2024-12-20-PhysGaussian/1m.PNG-800.webp 800w,/assets/img/2024-12-20-PhysGaussian/1m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-20-PhysGaussian/1m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Step 1) <code class="language-plaintext highlighter-rouge">3DGS optimization</code><br/> Anisotropic Loss를 추가하여 3DGS를 둥글둥글하게 만듦</li> <li>Step 2) <code class="language-plaintext highlighter-rouge">3DGS Internel Filling</code><br/> Object 내부 공간을 3DGS로 채워서 continuum(연속체)로 만듦</li> <li>Step 3) <code class="language-plaintext highlighter-rouge">Physics Integration</code> <ul> <li>Dynamics :<br/> 3DGS에 부피, 질량 부여하여<br/> 시간에 따라 Continuum Mechanics(연속체 역학)이라는 물리 법칙 따르도록 함</li> <li>Kinematics :<br/> Gaussian Evolution, SH Transform을 통해<br/> 시간에 따른 물리적인 변화를 3DGS로 모델링</li> </ul> </li> </ul> <h2 id="backgrounds-on-continuum-mechanics">Backgrounds on Continuum Mechanics</h2> <ul> <li>Conservation of Mass (질량 보존 법칙) :<br/> 시간 \(t\) 가 바뀌어도 infinitesimal region 내 질량은 항상 일정하게 유지된다!!<br/> \(\int_{B_{\epsilon}^{t}} \rho (x, t) = \int_{B_{\epsilon}^{0}} \rho (\phi^{-1}(x, t), 0)\) <ul> <li>\(B_{\epsilon}^{t}\) : infinitesimal region at \(t\)</li> <li>\(\rho(x, t)\) : density field at \(x, t\)</li> <li>\(x = \phi(x_{0}, t)\) : deformation map from \(x_{0}, 0\) to \(x, t\)</li> </ul> </li> <li>Conservation of Momentum (운동량 보존 법칙) :<br/> 시간 \(t\) 가 바뀌어도 물질의 운동량은 변하지 않는다!!<br/> 운동량 변화량 : \(\rho(x, t) \overset{\cdot}{v}(x, t) = \nabla \cdot \sigma(x, t) + f^{ext}\) <ul> <li>\(\overset{\cdot}{v}(x, t)\) : 가속도 field at \(x, t\)</li> <li>\(\sigma = \frac{1}{det(F)}\frac{\partial \psi}{\partial F}F^{E}(F^{E})^{T}\) : Cauchy stress tensor (물체 내부에서 발생하는 응력)<br/> where \(\psi(F)\) : hyperelastic energy density<br/> where deformation field gradient \(F = F^{E} F^{P}\) <ul> <li>\(F^{E}\) : elastic part (탄성)<br/> 물체에 stress를 가해서 조직에 구조적인 변형이 발생한 후,<br/> stress를 제거했을 때 원래 상태로 되돌아가는 성질</li> <li>\(F^{P}\) : plastic part (소성)<br/> 물체에 stress를 가해서 조직에 구조적인 변형이 발생한 후,<br/> stress가 탄성 범위를 넘어가서<br/> stress를 제거하더라도 원래 상태로 되돌아오지 않는 성질</li> </ul> </li> <li>\(f^{ext}\) : external force per unit volume</li> </ul> </li> </ul> <h2 id="mpm-material-point-method">MPM (Material Point Method)</h2> <ul> <li>핵심 :<br/> <code class="language-plaintext highlighter-rouge">particle과 grid 간에 운동량이 상호작용</code>하여<br/> 이 과정에서 질량과 운동량이 보존되어<br/> <code class="language-plaintext highlighter-rouge">simulation</code> 했을 때 현실과 비슷 <ul> <li>Lagrangian Particle Domain :<br/> particle의 위치, 질량, 운동량, 응력, 부피, 외력 등을 모델링하고<br/> particle 별로 추적하여 update</li> <li>Eulerian Grid Domain :<br/> 공간을 3D grid로 나누어서 grid를 통해 particle 이동<br/> cell의 크기가 작아질수록 정확도는 올라가지만 연산속도는 느려짐</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-20-PhysGaussian/2m.PNG-480.webp 480w,/assets/img/2024-12-20-PhysGaussian/2m.PNG-800.webp 800w,/assets/img/2024-12-20-PhysGaussian/2m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-20-PhysGaussian/2m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Procedure : <ul> <li>Particle to Node :<br/> each particle의 물리량을 grid 상의 adjacent 8 nodes로 분배</li> <li>Nodal Solution :<br/> 집계된 힘을 이용해서 each node의 \(a = \frac{F}{m}\), \(v\) 를 update</li> <li>Node to Particle :<br/> each node의 \(a, v\) 를 particle로 전파 by weighted sum</li> <li>Update Particles :<br/> each particle의 \(a, v\) 이용해서 새로운 particle 위치 갱신</li> </ul> </li> </ul> <p>실험 예시 : <a href="https://vimeo.com/267058393">SIGGRAPH2018</a></p> <h2 id="physics-integrated-3dgs">Physics-Integrated 3DGS</h2> <p>그렇다면 어떻게 물리 법칙을 3DGS에 적용할까??</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-20-PhysGaussian/7m.PNG-480.webp 480w,/assets/img/2024-12-20-PhysGaussian/7m.PNG-800.webp 800w,/assets/img/2024-12-20-PhysGaussian/7m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-20-PhysGaussian/7m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>방법 1) deformation gradient \(F_{p}\) 로 approx. <ul> <li>local affine transformation of deformation map \(\phi\) :<br/> \(\tilde \phi (X, t) = x_{p} + F_{p} (X - X_{p})\) <ul> <li>\(X\) : arbitrary point</li> <li>\(X_{p}\) : particle \(p\) 의 initial point</li> <li>\(x_{p}\) : particle \(p\) 의 current point</li> <li>\(F_{p}\) : 점이 어떻게 이동하는지에 대한 deformation gradient matrix<br/> (물리 법칙 적용)</li> </ul> </li> <li>3DGS position, covariance matrix 변화 :<br/> By approx. deformation map,<br/> \(x_{p}(t) = \tilde \phi (X_{p}, t)\)<br/> \(\Sigma_{p}(t) = F_{p}(t) \Sigma_{p} F_{p}(t)^{T}\)</li> <li>Gaussian 수식 변화 :<br/> \(G_{p}(x, t) = e^{-\frac{1}{2}(x-x_{p})^{T}(F_{p}(t) \Sigma_{p} F_{p}(t)^{T})^{-1}(x-x_{p})}\)</li> <li>grid 부피를 particle 수로 나누어서 각 particle 부피 \(V_{p}^{0}\) 를 초기화하고<br/> 이로써 각 particle(Gaussian)은 질량 \(m_{p} = \phi_{p} V_{p}\) 를 가지게 되고<br/> MPM Simulation을 바탕으로 Gaussian이 물리 법칙을 따름</li> <li>아래의 이유로 Physics와 3DGS의 결합은 자연스러움 <ul> <li>Gaussian itself가<br/> Continuum의 discretized form으로 간주되므로<br/> 직접 simulation 가능</li> <li>물리 법칙에 의해 변형된 Deformed Gaussian은<br/> 3DGS rasterization에 의해<br/> 직접 rendering 가능</li> <li>따라서 WS2(What you see is What you simulate) 달성</li> </ul> </li> </ul> </li> <li>방법 2) <code class="language-plaintext highlighter-rouge">incremental update</code> <ul> <li>deformation gradient \(F_{p}\) 에 의존하지 않고<br/> Langrangian framework(MPM simulation)에 더 잘 맞는<br/> Gaussian Kinematic(운동학) 방법 제시</li> <li>computational fluid dynamics (전산 유체 역학)에 따라 <ul> <li>covariance matrix :<br/> covariance matrix는 discretize되어<br/> \(\Sigma_{p}(t) = F_{p}(t) \Sigma_{p} F_{p}(t)^{T}\)<br/> 대신<br/> \(\Sigma_{p}^{n+1} = \Sigma_{i}^{n} + \Delta t \overset{\cdot}{\Sigma_{p}^{n}} = \Sigma_{i}^{n} + \Delta t (\nabla v_{p} \Sigma_{p}^{n} + \Sigma_{p}^{n} \nabla v_{p}^{T})\)</li> <li>rotation matrix :<br/> 마찬가지로 \(R_{p}^{0} = I\) 에서 출발해서 비슷하게 update 가능</li> <li>즉, <code class="language-plaintext highlighter-rouge">covariance matrix와 rotation matrix가 물리 법칙을 따르면서 incrementally update되도록 설계</code>!!</li> </ul> </li> <li>위의 수식을 통해 deformation gradient \(F_{p}\) 를 직접 구하지 않더라도<br/> Gaussian covariance를 \(t^{n}\) 에서 \(t^{n+1}\) 으로 incremental update 가능</li> </ul> </li> </ul> <h2 id="orientation-of-sh">Orientation of SH</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-20-PhysGaussian/3m.PNG-480.webp 480w,/assets/img/2024-12-20-PhysGaussian/3m.PNG-800.webp 800w,/assets/img/2024-12-20-PhysGaussian/3m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-20-PhysGaussian/3m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>SH는 view direction에 따른 color를 모델링하는, hard-coding되어 있는 함수이다<br/> 따라서 시간 \(t\) 에 따라 particle(Gaussian)이 rotate하면 색깔이 전혀 달라지므로<br/> view direction에 particle(Gaussian)의 역회전을 적용 <ul> <li>particle(Gaussian)의 회전 정보 :<br/> surface orientation을 사용한 <a href="https://arxiv.org/abs/2201.08845">Point-NeRF</a> 와 달리<br/> 방법 1)의 경우 polar decomposition을 통해 deformation gradient \(F_{p} = R_{p}S_{p}\) 에서 \(R_{p}\) 추출해서 사용<br/> 방법 2)의 경우 polar decomposition을 통해 \((I + \Delta t v_{p}) R_{p}^{n}\) 에서 \(R_{p}^{n+1}\) 추출해서 사용</li> </ul> </li> </ul> <h2 id="internal-filling">Internal Filling</h2> <ul> <li>recon. Gaussians는 surface 근처에 분포하는 경향이 있으므로<br/> object의 내부 구조는 비어 있는 채로 surface에 가려져 있음<br/> \(\rightarrow\)<br/> object의 deformation이 클 경우 내부가 노출될 수도 있고<br/> 질량을 가지는 물리 법칙에 따르는 volumetric object으로 만들기 위해<br/> 비어 있는 내부 영역도 particles(Gaussians)로 채워야 함</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-20-PhysGaussian/4m.PNG-480.webp 480w,/assets/img/2024-12-20-PhysGaussian/4m.PNG-800.webp 800w,/assets/img/2024-12-20-PhysGaussian/4m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-20-PhysGaussian/4m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Internal Filling : <ul> <li>Step 1)<br/> discretize<br/> from continuous <code class="language-plaintext highlighter-rouge">3D opacity field</code> \(d(x) = \sum_{p} \sigma_{p} e^{-\frac{1}{2}(x-x_{p})^{T}\Sigma_{p}^{-1}(x-x_{p})}\)<br/> into discrete 3D grid</li> <li>Step 2)<br/> low opacity(\(\sigma_{i} \lt \sigma_{th}\))를 가지는 grid에서<br/> high opacity(\(\sigma_{j} \gt \sigma_{th}\))를 가지는 grid로<br/> ray가 통과할 때<br/> 이를 intersection이라고 하자</li> <li>Step 3)<br/> 아래 두 가지 조건을 만족할 때 object 내부에 있다고 간주하고 3DGS 생성 <ul> <li>Condition 1) :<br/> 3D grid 상에서 6 axes 방향으로 ray casting한 뒤<br/> object 내부에 있는 grid의 경우 항상 surface와 intersect할 것이므로<br/> intersection 개수가 6개인지 체크하여 candidate grids 선택</li> <li>Condition 2) :<br/> candidate grids를 refine하기 위해<br/> additional ray를 casting하여 intersection 개수 체크</li> </ul> </li> <li>Step 4)<br/> object 내부에 채워 넣은 gaussian들도 3D 상에서 visualize할 필요가 있을 수 있음<br/> internal-filled particle(Gaussian)의 경우<br/> opacity \(\sigma_{p}\) 와 color \(C_{p}\) 는 closest Gaussian의 것을 물려받고<br/> covariance matrix는 \(\text{diag}(r_{p}^{2}, r_{p}^{2}, r_{p}^{2})\) 으로 initialize<br/> where \(r_{p}\) : particle radius from its volume \(V_{p}^{0} = \frac{4 \pi r_{p}^{3}}{3}\)<br/> (본 논문의 저자는 시도하지 않았지만 internal filling을 위해 generative model을 사용하면 more realistic results 가능할 듯)</li> </ul> </li> </ul> <h2 id="anisotropy-regularizer">Anisotropy Regularizer</h2> <ul> <li>3DGS가 너무 얇을 경우<br/> large deformation일 때 Gaussian이 object surface의 바깥쪽으로 튀어나와<br/> <code class="language-plaintext highlighter-rouge">plush artifacts</code> 발생 가능</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-20-PhysGaussian/9m.PNG-480.webp 480w,/assets/img/2024-12-20-PhysGaussian/9m.PNG-800.webp 800w,/assets/img/2024-12-20-PhysGaussian/9m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-20-PhysGaussian/9m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> chair에 Twist deformation 가했을 때 생기는 plush artifacts </div> <ul> <li>\(L_{aniso} = \frac{1}{| P |} \sum_{p \in P} \text{max}(\frac{\text{max}(S_{p})}{\text{min}(S_{p})}, r) - r\)<br/> where \(S_{p}\) : scale matrix of 3DGS <ul> <li>\(\frac{\text{max}(S_{p})}{\text{min}(S_{p})} \leq r\)<br/> 즉, 장축과 단축의 길이 비가 threshold \(r\) 을 넘지 않도록<br/> 3DGS를 <code class="language-plaintext highlighter-rouge">둥글둥글하게</code> 만듦</li> </ul> </li> </ul> <h2 id="experiments">Experiments</h2> <ul> <li> <p>Dataset :<br/> InstantNGP, NerfStudio, DroneDeployNeRF, \(\cdots\)</p> </li> <li> <p>Resource :<br/> 24-core 3.50GHz Intel i9-10920X machine with Nvidia RTX 3090 GPU</p> </li> <li>MPM Simulation : <ul> <li>MPM :<br/> <a href="https://zeshunzong.github.io/reduced-order-mpm/">SIGGRAPH2023</a></li> <li>simulation region :<br/> simulation region을 manually 선택하여 \(2 \times 2 \times 2\) cube로 normalize한 뒤 3D dense crid로 discretize</li> <li>particle :<br/> controlled movement(흔들리는 여우 얼굴 등)를 보일 specific particles만 선택적으로 velocities 수정하고<br/> 나머지 particles는 물리 법칙을 따르는 natural motion</li> </ul> </li> <li>Qualitative Results :<br/> <a href="https://xpandora.github.io/PhysGaussian/">Video</a> 를 보면<br/> Simulation 할 때 <ul> <li>Fox의 경우<br/> 물체의 원래 형태로 되돌아가는 Elasticity (탄성) 성질을 적용</li> <li>Plane의 경우<br/> 물체의 원래 형태로 되돌아가지 않는 Metal (금속) 성질을 적용</li> <li>Ruins의 경우<br/> Sand 효과 (granular-level frictional effect based on Druker-Prager plastic model)를 적용</li> <li>Toast의 경우<br/> MPM Simulation에 따라 큰 deformation이 발생하면 입자가 여러 그룹으로 분리되는 Fracture</li> <li>Jam의 경우<br/> Paste 효과 (non-Newtonian fluid based on Herschel Bulkley plastic model)를 적용</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-20-PhysGaussian/5m.PNG-480.webp 480w,/assets/img/2024-12-20-PhysGaussian/5m.PNG-800.webp 800w,/assets/img/2024-12-20-PhysGaussian/5m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-20-PhysGaussian/5m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Quantitative Results : <ul> <li>deformation에 대한 GT를 만들기 위해<br/> BlenderNeRF로 scene 합성한 뒤 lattice deformation tool로 Bend 및 Twist</li> <li>3가지 model과 비교 <ul> <li><a href="https://arxiv.org/abs/2205.04978">NeRF-Editing</a> : <ul> <li><a href="https://arxiv.org/abs/2106.10689">NeuS</a> 로 추출한 surface mesh를 이용해서 NeRF 를 변형하는데,<br/> surface recon.에 초점이 맞춰진 연구여서 volumetric simulation과 결합했을 때<br/> rendering 퀄리티가 낮았음</li> <li>deformation이 extracted surface mesh와 dilated cage mesh의 정밀도에 의존하는데<br/> mesh가 지나치게 크면 경계가 공백이 될 수 있음</li> </ul> </li> <li><a href="https://arxiv.org/abs/2309.13101">Deforming-NeRF</a> : <ul> <li>고해상도 deformation cage mesh를 사용해서 변형하여 향상된 결과 보이지만<br/> interpolation 과정에서 local detail을 filtering하면서 성능 낮아짐</li> </ul> </li> <li><a href="https://arxiv.org/abs/2303.05512">PAC-NeRF</a> : <ul> <li>단순한 object, texture를 표현하도록 디자인되어<br/> particle representation을 통해 flexible하지만 rendering 퀄리티는 여전히 높지 않음</li> </ul> </li> </ul> </li> <li>Ours :<br/> zero-order info.(deformation map)와 first-order info.(deformation gradient)를 모두 활용하였으므로<br/> deformation 후에도 높은 성능 보임</li> <li>Ablation Study : <ul> <li>Fixed Covariance :<br/> 3DGS에 translation만 적용하여<br/> covariance는 그대로 사용</li> <li>Rigid Covariance :<br/> 3DGS에 rigid transformation 적용하여<br/> covariance를 수정하여 물리 법칙을 따르도록</li> <li>Fixed Harmonics :<br/> SH에서 view direction을 rotate하지 않음</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-20-PhysGaussian/6m.PNG-480.webp 480w,/assets/img/2024-12-20-PhysGaussian/6m.PNG-800.webp 800w,/assets/img/2024-12-20-PhysGaussian/6m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-20-PhysGaussian/6m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 논문에서 언급한 기법들을 적용하지 않을 경우 Gaussian이 surface를 제대로 덮지 않아 artifacts 발생 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-20-PhysGaussian/8m.PNG-480.webp 480w,/assets/img/2024-12-20-PhysGaussian/8m.PNG-800.webp 800w,/assets/img/2024-12-20-PhysGaussian/8m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-20-PhysGaussian/8m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> E는 elasticity(탄성도), v는 poission ratio(volume 보존 정도) </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-20-PhysGaussian/10m.PNG-480.webp 480w,/assets/img/2024-12-20-PhysGaussian/10m.PNG-800.webp 800w,/assets/img/2024-12-20-PhysGaussian/10m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-12-20-PhysGaussian/10m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 당겼을 때 Physics-based Ours는 물리 법칙에 따라 volume을 잘 보존하지만, Geometry-based NeRF-Editing은 volume 보존하지 않음 </div> <h2 id="conclusion">Conclusion</h2> <ul> <li>Limitation : <ul> <li>그림자 고려 안 함</li> <li>material param.를 manually 정해주어야 함<br/> (GS segmentation과 differentiable MPM simulator를 결합하여 video로부터 param. 자동 assign 가능하긴 함)</li> </ul> </li> <li>Future Work : <ul> <li>more versatile materials like liquid 다루기</li> <li>more intuitive user control 포함하기</li> <li>LLM 기술 적용하기</li> <li>geometry-aware 3DGS recon. 결합하여 generative dynamics (생성 동역학) 향상시키기</li> </ul> </li> <li>마무리하며..<br/> 3DGS와 전혀 다른 분야를 통합하는 논문들이 종종 나오는데<br/> 이 논문도 결과가 재미있게 나온 논문이었다!!</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="Physics"/><category term="Simulation"/><category term="MPM"/><category term="3DGS"/><summary type="html"><![CDATA[Physics-Integrated 3D Gaussians for Generative Dynamics (CVPR 2024)]]></summary></entry><entry><title type="html">MASt3R</title><link href="https://semyeong-yu.github.io/blog/2024/MASt3R/" rel="alternate" type="text/html" title="MASt3R"/><published>2024-11-21T12:00:00+00:00</published><updated>2024-11-21T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/MASt3R</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/MASt3R/"><![CDATA[<h2 id="grounding-image-matching-in-3d-with-mast3r">Grounding Image Matching in 3D with MASt3R</h2> <h4 id="vincent-leroy-yohann-cabon-jérôme-revaud">Vincent Leroy, Yohann Cabon, Jérôme Revaud</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2406.09756">https://arxiv.org/abs/2406.09756</a><br/> project website :<br/> <a href="https://europe.naverlabs.com/blog/mast3r-matching-and-stereo-3d-reconstruction/">https://europe.naverlabs.com/blog/mast3r-matching-and-stereo-3d-reconstruction/</a><br/> code :<br/> <a href="https://github.com/naver/mast3r">https://github.com/naver/mast3r</a><br/> reference :<br/> <a href="https://xoft.tistory.com/100">https://xoft.tistory.com/100</a></p> </blockquote> <h3 id="contribution">Contribution</h3> <ul> <li>DUSt3R : <ul> <li>많은 연산을 필요로 하는 <code class="language-plaintext highlighter-rouge">SfM 생략</code> (pose-free)</li> <li>transformer 기반으로<br/> <code class="language-plaintext highlighter-rouge">2D(img pixel)-to-3D(point map)</code> mapping 예측하여<br/> <code class="language-plaintext highlighter-rouge">regression-based</code> 3D recon. 수행</li> <li>predicted pointmap 기반으로<br/> intrinsic/extrinsic camera param. 추정 가능</li> </ul> </li> <li>MASt3R : <ul> <li>DUSt3R 후속 논문으로,<br/> DUSt3R을 활용하여 <code class="language-plaintext highlighter-rouge">Image Matching에 특화</code>시킴 <ul> <li><code class="language-plaintext highlighter-rouge">Image Matching</code> 문제를 <code class="language-plaintext highlighter-rouge">3D 상에서</code> 풂</li> <li>quality 향상 및 속도 개선 및 많은 images 수 커버 가능</li> </ul> </li> </ul> </li> </ul> <h3 id="image-matching">Image Matching</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/1m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/1m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/1m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/1m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Image Matching 문제를 3D 상에서 풀기 때문에<br/> 2개 image의 공통 영역이 매우 적더라도 Image Matching 잘 수행 가능!</p> </li> <li> <p>Image Matching 기법의 문제 및 해결 :</p> <ul> <li>문제 1)<br/> 전통적인 Image Matching은 keypoint을 추출하여 local-invariant descriptor로 변형한 뒤<br/> feature space에서 거리를 비교하여 Matching을 수행했음<br/> 조명 변화와 시점 변화에도 정확했고 적은 keypoint 수로도 [ms] 단위로 Matching 가능했음<br/> 하지만 geometry context는 고려하지 못했고<br/> 반복 패턴이나 low-texture 영역에서는 잘 수행하지 못했음</li> <li>해결 1)<br/> local descriptor 대신 global descriptor를 이용하는 SuperGlue(2020) 기법</li> <li>문제 2)<br/> SuperGlue(2020)의 경우 keypoint descriptor가 충분히 encode되지 않으면 matching 도중에 global text를 활용할 수 없었음</li> <li>해결 2)<br/> keypoint 대신 image 전체를 한 번에 matching하는 dense holistic matching 기법<br/> thanks to global attention<br/> e.g. LoFTR(2021) : 반복 패턴 및 low-texture 영역에 robust하고 dense correspondence 만들 수 있음</li> <li>문제 3)<br/> LoFTR(2021)은 Map-free localization benchmark의 VCRE 평가에서 낮은 성능<br/> 현실적으로 Image Matching task는 같은 3D point에 대응되는 pixel을 찾는 문제인데 지금까지 전통적인 matching 기법들은 전부 2D 상에서 이루어졌기 때문</li> <li>해결 3)<br/> 2D pixel - 3D point correspondence 다루는 DUSt3R 활용</li> <li>문제 4)<br/> DUSt3R은 3D recon.을 목적으로 만들어졌기 때문에<br/> 시점 변화에는 강인하지만 Image Matching에서는 비교적 부정확</li> <li>해결 4)<br/> MASt3R(본 논문)에서는 DUSt3R을 Image Matching에 특화하는 방법에 대해 다룸!</li> </ul> </li> </ul> <h3 id="dust3r-framework">DUSt3R Framework</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/2m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/2m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/2m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/2m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>DUSt3R과 달라진 점 : <ul> <li>DUSt3R에서는 3D recon.이 목적이었기 때문에<br/> view-point가 달라지더라도 같은 크기의 물체를 recon.해야 함<br/> 즉, scale-invariant하도록 만들기 위해<br/> 각 view-point에서 averaged depth 값으로 나누어 normalize해주었음<br/> \(\rightarrow\)<br/> MASt3R에서는 서로 다른 scale의 images인 상태에서 image matching task를 수행해야 하므로<br/> (scale을 고려해야 하므로)<br/> regression loss에서 <code class="language-plaintext highlighter-rouge">scale(depth) normalization 파트를 없앰</code></li> </ul> </li> <li>Loss : <ul> <li><code class="language-plaintext highlighter-rouge">regression loss</code> :<br/> \(L_{regr} (v, i) = \| \frac{1}{z} X_{i}^{v, 1} - \frac{1}{\bar z} \bar X_{i}^{v, 1} \|\) <ul> <li>\(i\) : each point, \(v\) : each view</li> <li>\(z = \bar z\) : averaged depth of GT point</li> </ul> </li> <li>confidence loss :<br/> \(L_{conf} = \sum_{v \in \{ 1, 2 \}} \sum_{i \in D^{v}} C_{i}^{v, 1} L_{regr}(v, i) - \alpha \text{log} C_{i}^{v, 1}\) <ul> <li>\(C_{i}^{v, 1}\) : confidence score<br/> 물체인 부분에서는 3D point를 비교적 정확히 예측할 수 있으므로 confidence가 높고,<br/> 하늘 또는 반투명인 부분에서는 3D point를 정확하게 예측할 수 없으므로 confidence가 낮게 나옴</li> <li>\(C_{i}^{v, 1} L_{regr}(v, i)\) :<br/> confidence가 큰 <code class="language-plaintext highlighter-rouge">(확실한) point</code>에서는 GT와의 <code class="language-plaintext highlighter-rouge">regression loss</code> \(L_{regr}\) 가 더 <code class="language-plaintext highlighter-rouge">작도록</code></li> <li>\(- \alpha \text{log} C_{i}^{v, 1}\) : regularization term<br/> <code class="language-plaintext highlighter-rouge">confidence</code> \(C_{i}^{v, 1}\) 값이 <code class="language-plaintext highlighter-rouge">너무 작아지지 않도록</code></li> </ul> </li> <li>matching loss (<code class="language-plaintext highlighter-rouge">cross-entropy classification loss</code>) :<br/> \(L_{match} = - \sum_{(i, j) \in \hat M} (\text{log} \frac{s_{\tau} (i, j)}{\sum_{k \in P^{1}} s_{\tau} (k, j)} + \text{log} \frac{s_{\tau} (i, j)}{\sum_{k \in P^{2}} s_{\tau} (i, k)})\)<br/> 다음 section에서 언급할 예정</li> <li>total loss :<br/> \(L_{tot} = L_{conf} + \beta L_{match}\)</li> </ul> </li> </ul> <h3 id="matching-prediction-head">Matching Prediction Head</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/2m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/2m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/2m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/2m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>기존 DUSt3R의 Head output :<br/> extreme한 view-point 변화에도 robust <ul> <li>per-pixel <code class="language-plaintext highlighter-rouge">Pointmap</code> \(X_{i}^{v, 1} \in R^{H \times W \times 3}\)</li> <li>per-pixel <code class="language-plaintext highlighter-rouge">Confidence</code> score \(C_{i}^{v, 1} \in R^{H \times W}\)</li> </ul> </li> <li>MASt3R의 new Head output : <ul> <li>per-pixel <code class="language-plaintext highlighter-rouge">Local Feature</code> \(D_{i}^{v} \in R^{H \times W \times d}\) (\(d = 24\))</li> </ul> </li> <li>Fast NN :<br/> Fast Reciprocal Matching by Nearest Neighborhood<br/> (다음 section에서 설명 예정) <ul> <li>predicted <code class="language-plaintext highlighter-rouge">Pointmap</code> 이용하여 <code class="language-plaintext highlighter-rouge">Geometrical matching</code> 수행 <ul> <li>2개의 image에 대한 3D pointmap을 겹쳤을 때 align되도록<br/> <code class="language-plaintext highlighter-rouge">3D 공간 상에서 pixel correspondence</code>를 찾음</li> <li>\(X^{2, k}\) 중에 3D point \(X_{i}^{1, k}\) 와 가장 가까운 3D point가 \(X_{j}^{2, k}\) 이고,<br/> 동시에 \(X^{1, k}\) 중에 3D point \(X_{j}^{2, k}\) 와 가장 가까운 3D point가 \(X_{i}^{1, k}\) 일 때<br/> 두 pixel \(i, j\) 사이에 correspondence 있다고 함</li> </ul> </li> <li>predicted <code class="language-plaintext highlighter-rouge">Local Feature</code> 이용하여 <code class="language-plaintext highlighter-rouge">Feature-based matching</code> 수행</li> <li>얘네들 어떻게 합치는지 <code class="language-plaintext highlighter-rouge">???</code></li> </ul> </li> <li>Loss : <ul> <li>matching loss :<br/> constrastive learning에 사용되는 infoNCE Loss를 변형하여<br/> \(L_{match} = - \sum_{(i, j) \in \hat M} (\text{log} \frac{s_{\tau} (i, j)}{\sum_{k \in P^{1}} s_{\tau} (k, j)} + \text{log} \frac{s_{\tau} (i, j)}{\sum_{k \in P^{2}} s_{\tau} (i, k)})\)<br/> where \(s_{\tau} (i, j) = \text{exp}(- \tau D_{i}^{1 T} D_{j}^{2})\)<br/> 근데 \(s_{\tau} (i, j)\) 는 similarity score이니까 \(\text{exp}\) 안에 - 가 없어야 할 것 같음! 오타인가? 아니면 내가 잘못 생각하고 있나?<br/> <code class="language-plaintext highlighter-rouge">?????</code> <ul> <li>cross-entropy classification loss 꼴<br/> (regression loss 꼴 아님)<br/> \(\rightarrow\)<br/> 정확히 correct pixel pair (<code class="language-plaintext highlighter-rouge">positive sample</code>) \((i, j)\) 에 대해서는 \(s_{\tau} (i, j)\) 이 높아지고<br/> nearby pixel (<code class="language-plaintext highlighter-rouge">negative sample</code>) \((i+1, j)\) 에 대해서는 \(s_{\tau} (i+1, j)\) 이 낮아지도록 설계하여<br/> <code class="language-plaintext highlighter-rouge">nearby pixel로 regression하는 게 아니라</code> <code class="language-plaintext highlighter-rouge">정확한 correct pixel pair를 분류</code>하도록 하므로<br/> high-precision image matching 가능</li> <li>positive sample :<br/> pixel correspondence가 있는 pixel pair (2개 image에서 모두 나타나고 3D point가 일치하는 pixel pair)<br/> 1번 image의 \(i\)-th pixel이 2번 image의 \(j\)-th pixel과 correspondence 있다면<br/> \((i, j) \in \hat M = \{ (i, j) | \hat X_{i}^{1, 1} = \hat X_{j}^{2, 1} \}\)<br/> where \(X^{v, 1}\) : 1번 view-point를 중심좌표계로 두고 \(v\) 번 view에서 보이는 3D point 좌표</li> <li>negative sample :<br/> positive sample들을 모은 뒤<br/> \(\hat M\) 에서 대응되지 않는 pixel pair</li> <li>\(P^{1} = \{ i | (i, j) \in \hat M \}\) and \(P^{2} = \{ j | (i, j) \in \hat M \}\)<br/> 따라서 log 안의 분자는 positive sample의 score에 해당하고<br/> log 안의 분모는 negative sample의 score에 해당</li> </ul> </li> </ul> </li> </ul> <h3 id="fast-reciprocal-matching">Fast Reciprocal Matching</h3> <p>그렇다면 위에서 positive sample \(M\) 은 어떻게 찾을까?</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/4m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/4m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/4m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/4m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Fast Reciprocal Matching :<br/> feature space \(D\) 에서 reciprocal matching 수행 <ul> <li>기존 matching :<br/> Nearest Neighbor 기법 사용하여<br/> \(D^{1}\) 중에 \(D_{j}^{2}\) 와 가장 유사한 pixel이 \(D_{i}^{1}\) 이고,<br/> 동시에 \(D^{2}\) 중에 \(D_{i}^{1}\) 와 가장 유사한 pixel이 \(D_{j}^{2}\) 일 때<br/> 해당 \((i, j)\) pair에는 pixel correspondence가 있다고 함 <ul> <li>complexity \(O(W^{2} H^{2})\)</li> </ul> </li> <li>Fast Reciprocal matching :<br/> <code class="language-plaintext highlighter-rouge">연산 줄이기 위해 image의 부분 pixel들로 matching</code> 진행 <ul> <li>Step 1)<br/> image 1 에서 \(k\) 개의 pixel을 uniform sampling하여 \(U^{0}\) 로 표기</li> <li>Step 2)<br/> 기존 matching 방법대로 Nearest Neighbor 기법 사용하여<br/> mapping from \(U^{0}\) to \(V^{1}\) 진행<br/> (\(V^{t+1}\) : image 2에서 \(U^{t}\) 와 가장 유사한 pixel들의 집합)</li> <li>Step 3)<br/> 기존 matching 방법대로 Nearest Neighbor 기법 사용하여<br/> 다시 mapping from \(V^{1}\) to \(U^{1}\) 진행<br/> (\(U^{t+1}\) : image 1에서 \(V^{t+1}\) 와 가장 유사한 pixel들의 집합)</li> <li>Step 4)<br/> 만약 \(U^{t}\) 와 \(U^{t+1}\) 이 같다면 reciprocal pair로 저장<br/> \(M_{k}^{t} = \{ (i, j) | i \in U^{t+1}, j \in V^{t+1} \}\)</li> <li>Step 5)<br/> 또 다른 \(k\) 개의 pixel을 uniform sampling하여 Step 1) ~ Step 4)를 반복<br/> 이 때, 이전 loop에서 matching된 \(U^{t}\) 는 빼서 계산하므로 (\(U^{t+1} = U^{t+1} \setminus U^{t}\))<br/> 점점 un-converged point \(| U^{t} |\) 가 줄어들어 수렴</li> <li>Step 6)<br/> 최종적으로 reciprocal pair 집합 (positive sample) 만들 수 있음<br/> \(M_{k} = \cup_{t} M_{k}^{t}\)</li> <li>complexity \(O(kWH)\)<br/> (모든 pixel 조합을 비교하지 않음)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/5m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/5m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/5m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/5m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/6m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/6m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/6m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/6m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="coarse-to-fine-matching">Coarse-to-Fine Matching</h3> <ul> <li>Coarse-to-Fine Matching :<br/> <code class="language-plaintext highlighter-rouge">연산 줄이기 위해</code><br/> <code class="language-plaintext highlighter-rouge">저해상도 image pair에서 Fast Reciprocal Matching 수행하여 집중해야 할 영역(window pair)을 찾고,</code><br/> <code class="language-plaintext highlighter-rouge">고해상도 window pair에서 Fine Matching 수행하여 fine pixel correpondence 얻음</code><br/> low-resolution algorithm으로 high-resolution images를 match하기 위한 기법 <ul> <li>Step 1)<br/> \(k\) 배 subsampling하여 two downscaled images에서 Fast Reciprocal Matching 수행<br/> coarse pixel pair 집합을 \(M_{k}^{0}\) 으로 표기</li> <li>Step 2)<br/> 원본 고해상도 image 위에 grid of overlapping window \(\in R^{w \times 4}\) 를 만든 뒤<br/> (each window crop measures 512 pixels in its largest dimension <code class="language-plaintext highlighter-rouge">?????</code>)<br/> (인접한 windows overlap by 50%)<br/> coarse pixel pair \(M_{k}^{0}\) 를 가장 많이 포함하는 window pair \((w_{1}, w_{2}) \in W^{1} \times W^{2}\) 찾음</li> <li>Step 3)<br/> coarse pixel pair \(M_{k}^{0}\) 의 90%가 커버될 때까지<br/> greedy fashion으로 window pair를 추가</li> <li>Step 4)<br/> 최종적으로 each window pair를 두 이미지로 보고,<br/> each window pair에 대해 각각 matching 수행하여 fine pixel pair 집합 구함<br/> Then they are finally mapped back to the original image coordinates and concatenated,<br/> thus providing dense full-resolution matching</li> </ul> </li> </ul> <h3 id="experiments">Experiments</h3> <ul> <li>Map-free Localization</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/7m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/7m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/7m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/7m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/9m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/9m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/9m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/9m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/8m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/8m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/8m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/8m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Multi-view Relative Pose Estimation</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/10m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/10m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/10m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/10m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Visual Localization</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/11m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/11m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/11m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/11m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Multi-view 3D Reconstruction</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/12m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/12m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/12m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/12m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-21-MASt3R/13m.PNG-480.webp 480w,/assets/img/2024-11-21-MASt3R/13m.PNG-800.webp 800w,/assets/img/2024-11-21-MASt3R/13m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-21-MASt3R/13m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="future-work">Future Work</h3> <ul> <li>pose 없이 2D-to-3D 수행하는 DUSt3R, MASt3R에<br/> 3DGS를 적용한 NoPoSplat <a href="https://noposplat.github.io/">Link</a></li> </ul> <h3 id="question">Question</h3> <ul> <li> <p>Q1 :<br/> DUSt3R와 달리 MASt3R에서는 averaged depth 값으로 나누지 않는 이유를 다시 설명해주실 수 있을까요?</p> </li> <li>A1 : <ul> <li>DUSt3R에서는 3D recon.이 목적이었기 때문에<br/> view-point가 달라지더라도 같은 크기의 물체를 recon.해야 함<br/> 즉, scale-invariant하도록 만들기 위해<br/> 각 view-point에서 averaged depth 값으로 나누어 normalize해주었음</li> <li>MASt3R에서는 서로 다른 scale의 images인 상태 그 자체에서 image matching task를 수행해야 하므로<br/> (scale을 고려해야 하므로)<br/> regression loss에서 scale(depth) normalization 파트를 없앰</li> </ul> </li> <li> <p>Q2 :<br/> 3D 상에서 matching을 수행하므로 image 1에서 잘 보이는 부분이 image 2에서 잘 보이지 않더라도 잘 matching된다고 하셨는데,<br/> 왜 3D 상에서 matching을 수행한다고 하는지 이해가 되지 않습니다.</p> </li> <li>A2 :<br/> Fast Reciprocal Matching 기법으로 두 가지 matching을 수행하는데, <ul> <li>predicted 3D pointmap을 이용한 geometrical matching :<br/> 2개의 image에 대한 3D pointmap을 겹쳤을 때 align되도록<br/> 3D 공간 상에서 pixel correspondence를 찾음 <ul> <li>\(X^{2, k}\) 중에 3D point \(X_{i}^{1, k}\) 와 가장 가까운 3D point가 \(X_{j}^{2, k}\) 이고,<br/> 동시에 \(X^{1, k}\) 중에 3D point \(X_{j}^{2, k}\) 와 가장 가까운 3D point가 \(X_{i}^{1, k}\) 일 때<br/> 두 pixel \(i, j\) 사이에 correspondence 있다고 함</li> </ul> </li> <li>predicted local feature를 이용한 feature-based matching</li> </ul> </li> <li> <p>Q3 :<br/> geometrical matching 결과랑 feature-based matching 결과를 어떻게 합치나요?</p> </li> <li> <p>A3 :<br/> 그 부분은 아직 살펴보지 못해서 코드를 한 번 봐야 알 수 있을 것 같습니다. 알아본 뒤 블로그 포스팅에 업데이트해놓도록 하겠습니다.<br/> TBD <code class="language-plaintext highlighter-rouge">?????</code></p> </li> <li> <p>Q4 :<br/> \(U^{t} = U^{t+1}\) 이면 reciprocal pair로 저장하는데<br/> 만약 실패한 pixels가 많으면 결국 complexity가 \(O(kWH)\) 로 낮아지지 않을 것 같은데<br/> complexity \(O(kWH)\) 가 어떻게 달성되나요?</p> </li> <li> <p>A4 :<br/> TBD <code class="language-plaintext highlighter-rouge">?????</code></p> </li> <li> <p>Q5 :<br/> Fast Reciprocal Matching에서 \(U^{t} \rightarrow V^{t+1}\) 와 \(V^{t+1} \rightarrow U^{t+1}\) 을 어떻게 정의하나요?<br/> \(U^{t} \rightarrow V^{t+1}\) 는 Nearest Neighbor mapping이고 \(V^{t+1} \rightarrow U^{t+1}\) 는 transformer로 학습된 mapping 으로 진행하는 식인가요?</p> </li> <li>A5 :<br/> 아니요, \(U^{t} \rightarrow V^{t+1}\) 와 \(V^{t+1} \rightarrow U^{t+1}\) 둘 다 Nearest Neighbor mapping입니다.<br/> \(D^{1}\) 중에 \(D_{j}^{2}\) 와 가장 유사한 pixel이 \(D_{i}^{1}\) 이고,<br/> 동시에 \(D^{2}\) 중에 \(D_{i}^{1}\) 와 가장 유사한 pixel이 \(D_{j}^{2}\) 일 때<br/> 해당 \((i, j)\) pair에는 pixel correspondence가 있다고 합니다.<br/> 즉, \(i\) 랑 가장 가까운 게 \(j\) 이더라도, \(j\) 랑 가장 가까운 게 \(i\) 가 아닐 수도 있다는 의미입니다.<br/> 따라서 \(i \in U^{t}\) 와 가장 가까운 게 \(j \in V^{t+1}\) 이고, \(j \in V^{t+1}\) 와 가장 가까운 게 \(i \in U^{t+1}\) 일 때 reciprocal pair로 저장(correspondence 존재)합니다.</li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="point"/><category term="regression"/><category term="pose"/><category term="free"/><summary type="html"><![CDATA[Grounding Image Matching in 3D with MASt3R]]></summary></entry><entry><title type="html">DUSt3R</title><link href="https://semyeong-yu.github.io/blog/2024/DUSt3R/" rel="alternate" type="text/html" title="DUSt3R"/><published>2024-11-19T12:00:00+00:00</published><updated>2024-11-19T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/DUSt3R</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/DUSt3R/"><![CDATA[<h2 id="dust3r---geometric-3d-vision-made-easy-cvpr-2024">DUSt3R - Geometric 3D Vision Made Easy (CVPR 2024)</h2> <h4 id="shuzhe-wang-vincent-leroy-yohann-cabon-boris-chidlovskii-jerome-revaud">Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, Jerome Revaud</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2312.14132">https://arxiv.org/abs/2312.14132</a><br/> project website :<br/> <a href="https://europe.naverlabs.com/research/publications/dust3r-geometric-3d-vision-made-easy/">https://europe.naverlabs.com/research/publications/dust3r-geometric-3d-vision-made-easy/</a><br/> code :<br/> <a href="https://github.com/naver/dust3r">https://github.com/naver/dust3r</a><br/> reference :<br/> <a href="https://xoft.tistory.com/83">https://xoft.tistory.com/83</a></p> </blockquote> <h3 id="contribution">Contribution</h3> <ul> <li> <p>MVS(Multi-View Stereo) 분야에서는 일반적으로 camera param.를 알아야 해서<br/> bundle adjustment 등 최적화 과정을 거치는 SfM(Structure from Motion)을 사용해서 camera param. estimaton을 하지만<br/> 이는 많은 연산 필요</p> </li> <li> <p>DUSt3R :</p> <ul> <li>많은 연산을 필요로 하는 <code class="language-plaintext highlighter-rouge">SfM 생략</code> (pose-free)</li> <li>transformer 기반으로<br/> <code class="language-plaintext highlighter-rouge">2D(img pixel)-to-3D(point map)</code> mapping 예측하여<br/> <code class="language-plaintext highlighter-rouge">regression-based</code> 3D recon. 수행</li> <li>2-view transformer 이용하여 self-supervised regression</li> <li>1번 view를 기준으로 2번 view의 3D points를 <code class="language-plaintext highlighter-rouge">상대적으로 align</code>하므로<br/> (3D point의 절대적인 위치를 추정하는 게 아니므로)<br/> intrinsic/extrinsic <code class="language-plaintext highlighter-rouge">camera param. 몰라도</code> ok</li> <li>predicted pointmap 기반으로<br/> intrinsic/extrinsic camera param. 추정 가능</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/1m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/1m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/1m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/1m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="algorithm">Algorithm</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/2m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/2m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/2m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/2m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Algorithm :<br/> shared-weight 및 cross-attention으로 relevant relation을 학습함으로써<br/> (view A의 pcd의 빈 부분을 view B의 pcd 도움으로 채우고, vice versa)<br/> 3D geometry를 학습할 수 있음! <ul> <li>Step 1) input<br/> image 2장</li> <li>Step 2) ViT encoder<br/> 두 images의 feature 비교하기 위해<br/> <code class="language-plaintext highlighter-rouge">Siamese</code> (shared weight) 구조 사용</li> <li>Step 3) Transformer decoder<br/> 두 features의 관계를 학습하여<br/> aligned pointmap 만들기 위해<br/> <code class="language-plaintext highlighter-rouge">self-attention and cross-attention</code> 수행</li> <li>Step 4) Head output<br/> per-pixel <code class="language-plaintext highlighter-rouge">Pointmap</code> \(X_{i}^{v, 1} \in R^{W \times H \times 3}\)<br/> and<br/> per-pixel <code class="language-plaintext highlighter-rouge">Confidence</code> score \(C_{i}^{v, 1} \in R^{W \times H}\)<br/> (이 때, 두 Pointmap 모두 <code class="language-plaintext highlighter-rouge">첫 번째 view(frame)의 coordinate에 aligned</code>)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/3m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/3m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/3m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/3m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>1번 camera : base view, 2번 camera : reference view<br/> \(G_{i}^{1}\) : 1번 view feature의 Transformer Decoder에서 \(i\)-th Block<br/> \(G_{i}^{2}\) : 2번 view feature의 Transformer Decoder에서 \(i\)-th Block</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/4m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/4m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/4m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/4m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>Pointmap :<br/> \(X^{1, 1}\) : 1번 view-point를 중심좌표계로 두고 1번 view에서 보이는 3D point 좌표<br/> \(X^{2, 1}\) : 1번 view-point를 중심좌표계로 두고 2번 view에서 보이는 3D point 좌표</p> </li> <li>Confidence score :<br/> \(C_{i}^{v, 1}\) : 1번 view 시점을 기준으로 \(v\) 번 view에서 보이는 \(i\)-th 3D point의 confidence score <ul> <li>물체인 부분에서는 3D point를 비교적 정확히 예측할 수 있으므로 confidence가 높고,<br/> 하늘 또는 반투명인 부분에서는 3D point를 정확하게 예측할 수 없으므로 confidence가 낮게 나옴</li> <li>\(C_{i}^{v, 1} = 1 + \text{exp}(\tilde C_{i}^{v, 1}) \gt 1\) 로 설정하여<br/> 하나의 view에만 존재해서 추정하기 어려운 3D point의 경우에는 extrapolate할 수 있도록 <code class="language-plaintext highlighter-rouge">???</code></li> </ul> </li> <li>1번 view를 기준으로 2번 view의 3D points를 <code class="language-plaintext highlighter-rouge">상대적으로 align</code>하므로<br/> 3D point의 <code class="language-plaintext highlighter-rouge">절대적인 위치를 추정하는 게 아니므로</code><br/> intrinsic/extrinsic <code class="language-plaintext highlighter-rouge">camera param. 몰라도</code> ok</li> </ul> <h3 id="loss">Loss</h3> <ul> <li>regression loss :<br/> \(L_{regr} (v, i) = \| \frac{1}{z} X_{i}^{v, 1} - \frac{1}{\bar z} \bar X_{i}^{v, 1} \|\) <ul> <li>\(i\) : each point, \(v\) : each view</li> <li>\(z = \text{norm}(X^{1, 1}, X^{2, 1})\) : averaged depth of prediction point</li> <li>\(\bar z = \text{norm}(\bar X^{1, 1}, \bar X^{2, 1})\) : averaged depth of GT point</li> <li> <table> <tbody> <tr> <td>$$\text{norm}(X^{1, 1}, X^{2, 1}) = \frac{1}{</td> <td>D^{1}</td> <td>+</td> <td>D^{2}</td> <td>} \sum_{v \in { 1, 2 }} \sum_{i \in D^{v}} | X_{i}^{v, 1} |$$ : 모든 depth 값에 대한 평균</td> </tr> </tbody> </table> </li> </ul> </li> <li>final loss :<br/> \(L_{conf} = \sum_{v \in \{ 1, 2 \}} \sum_{i \in D^{v}} C_{i}^{v, 1} L_{regr}(v, i) - \alpha \text{log} C_{i}^{v, 1}\) <ul> <li>\(C_{i}^{v, 1} L_{regr}(v, i)\) :<br/> confidence가 큰 <code class="language-plaintext highlighter-rouge">(확실한) point</code>에서는 GT와의 <code class="language-plaintext highlighter-rouge">regression loss</code> \(L_{regr}\) 가 더 <code class="language-plaintext highlighter-rouge">작도록</code></li> <li>\(- \alpha \text{log} C_{i}^{v, 1}\) : regularization term<br/> <code class="language-plaintext highlighter-rouge">confidence</code> \(C_{i}^{v, 1}\) 값이 <code class="language-plaintext highlighter-rouge">너무 작아지지 않도록</code></li> </ul> </li> </ul> <h3 id="experiments">Experiments</h3> <ul> <li>Model<br/> CroCo pre-trained model 사용하여 weight initialization <ul> <li>encoder : ViT-Large</li> <li>decoder : ViT-Base</li> <li>head : DPT (ViT를 Depth Estimation에 적용한 연구)</li> </ul> </li> </ul> <h3 id="downstream---stereo-pixel-matching">Downstream - stereo pixel matching</h3> <ul> <li>2개의 image에 대한 3D pointmap을 겹쳤을 때 align되도록<br/> <code class="language-plaintext highlighter-rouge">3D 공간 상에서 pixel correspondence</code>를 찾음 <ul> <li>\(X^{2, k}\) 중에 3D point \(X_{i}^{1, k}\) 와 가장 가까운 3D point가 \(X_{j}^{2, k}\) 이고,<br/> 동시에 \(X^{1, k}\) 중에 3D point \(X_{j}^{2, k}\) 와 가장 가까운 3D point가 \(X_{i}^{1, k}\) 일 때<br/> 두 pixel \(i, j\) 사이에 correspondence 있다고 함</li> <li>모든 pixel에 대해 correspondence가 생기지는 않음</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/5m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/5m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/5m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/5m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="downstream---camera-intrinsic-estimation">Downstream - camera intrinsic estimation</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/6m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/6m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/6m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/6m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>camera intrinsic :<br/> camera intrinsic을 추정한다는 것은<br/> 보통 sclaing matrix, 즉 focal length를 추정한다는 얘기임 <ul> <li>2D translation : principal point의 위치<br/> (보통 이미지의 정가운데)</li> <li>2D shear : 카메라가 기울어진 정도<br/> (보통 카메라는 기울어져 있지 않으므로 shear matrix는 고려 X)</li> <li>2D scaling : focal length</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/7m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/7m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/7m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/7m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>위의 수식 설명 : <ul> <li>focal length는 3D point를 2D image plane으로 projection시킬 때 사용됨<br/> Weiszfeld algorithm을 이용해서 2D 상에서 위의 반복 최적화 문제를 풀면<br/> 해당 optimal <code class="language-plaintext highlighter-rouge">focal length</code>를 가질 때 <code class="language-plaintext highlighter-rouge">2D image와 3D point가 align</code>됨</li> <li>camera-coordinate에서 최적화 수행<br/> where pixel-coordinate : 2D \((i, j) \in ([0, W], [0, H])\) (좌상단이 원점)<br/> where camera-coordinate : 2D $$i^{‘}, j^{‘} \in ([-\frac{W}{2}, \frac{W}{2}], [-\frac{H}{2}, \frac{H}{2}]) (정중앙이 원점)<br/> where world-coordinate : 3D</li> </ul> </li> </ul> <h3 id="downstream---camera-extrinsic-estimation">Downstream - camera extrinsic estimation</h3> <ul> <li>Relative Pose Estimation : <ul> <li>방법 1)<br/> 위에서 언급한 2D pixel matching과 intrinsic estimation을 수행한 뒤<br/> Eight-Point algorithm 등 이용해서<br/> epipolar(essential) matrix와 relative pose를 추정</li> <li>방법 2)<br/> 서로 다른 view 시점에서 보이는 3D pointmap이 동일해지도록<br/> SVD-based procrustes alignment algorithm 이용해 3D 상에서 반복 최적화 문제를 풀어서<br/> optimal rotation matrix \(R\), translation vector \(t\), scale factor \(\sigma\) 추정 <ul> <li>procrustes alignment algorithm은 noise 및 outlier에 민감하므로<br/> 주어진 3D point와 corresponding 2D point를 바탕으로 camera pose를 추정하는 PnP algorithm과<br/> random sampling 방식의 RANSAC (Random Sample Consensus) algorithm 이용해서 위 수식의 해를 찾음<br/> <code class="language-plaintext highlighter-rouge">?????</code></li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/8m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/8m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/8m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/8m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Absolute Pose Estimation (visual localization) : <ul> <li>방법 1)<br/> 위에서 언급한 2D pixel matching과 instrinsic estimation을 수행한 뒤<br/> PnP RANSAC algorithm 이용해서 optimal rotation matrix 및 translation vector 추정</li> <li>방법 2)<br/> GT pointmap을 이용<br/> 즉, 위에서 언급한 Relative Pose Estimation을 수행할 때<br/> 해당 GT로 scale을 맞춰서 진행</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/9m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/9m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/9m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/9m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Experiment on Absolute Pose Estimation : <ul> <li>test dataset :<br/> 7Scenes, Cambridge Landmark<br/> (training에 사용되지 않은 dataset)</li> <li>각 값은 translation error (cm) / rotation error (degree)</li> <li>방식 :<br/> query image가 주어지면<br/> 가장 관련 있는 image를 test dataset에서 찾아<br/> 2개 image 간의 pixel을 matching하여 Absolute Camera Pose 계산<br/> (근데 query image와 test dataset image 간의 GT camera pose가 존재하나 <code class="language-plaintext highlighter-rouge">???</code>)</li> <li>비교 :<br/> FM(feature matching 기법), E2E(end-to-end learning 기법)과 비교했을 때<br/> SOTA 성능은 아니지만<br/> DUSt3R이 visual localization을 목적으로 학습되지 않았는데도 오차가 작다는 것을 확인할 수 있음</li> </ul> </li> </ul> <h3 id="downstream---global-alignment">Downstream - Global Alignment</h3> <ul> <li>Global Alignment :<br/> 3장 이상의 images로부터 예측한 Pointmap을 3D space에서 align하는 방법 <ul> <li>여러 장의 images를 다루기 위해<br/> <code class="language-plaintext highlighter-rouge">Graph</code> 만듦 (각 image가 vertex이고, 같은 visual contents를 공유하고 있으면 edge)</li> <li>DUSt3R 이용해서<br/> 모든 edge pair에 대해 Pointmap \(X_{i}^{v, 1} \in R^{W \times H \times 3}\) 과 Confidence score \(C_{i}^{v, 1} \in R^{W \times H}\) 계산</li> <li>여러 장의 images로 3D 상에서 반복 최적화 문제 풀어서<br/> 여러 장의 images로부터 얻은 Pointmap들이 3D 상에서 align되도록 함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/10m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/10m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/10m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/10m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>위의 수식 설명 :<br/> 3D 상에서 위의 반복 최적화 문제를 풀어서<br/> optimal \(\xi_{i}^{v}, \sigma_{e}, P_{e}\) 얻으면<br/> \(N\) 개의 images로 얻은 \(N\) 개의 <code class="language-plaintext highlighter-rouge">3D Pointmap을 align</code>하여<br/> Global pointmap을 얻을 수 있음<br/> 코드 : <a href="https://github.com/naver/dust3r/blob/01b2f1d1e6c6c386f95a007406defb5b8a5d2042/dust3r/cloud_opt/optimizer.py">Code</a> <ul> <li>\(C_{i}^{v, e}\) : confidence score from DUSt3R prediction<br/> (image \(e\) 의 view 시점을 기준으로, image \(v\) view에서 보이는 \(i\)-th pixel에 대응되는 값)</li> <li>\(X_{i}^{v, e}\) : pointmap from DUSt3R prediction</li> <li>\(\xi_{i}^{v}\) : global pointmap in world-coordinate</li> <li>\(\sigma_{e}\) : edge로 연결되어 있는 2개 images 간의 scale factor<br/> (0이 되는 것을 방지하기 위해 \(\prod_{e} \sigma_{e} = 1\) 로 설계)</li> <li>\(P_{e}\) : edge로 연결되어 있는 2개 images 간의 relative pose</li> </ul> </li> <li>위의 방법은<br/> <code class="language-plaintext highlighter-rouge">전통적인 SfM bundle adjustment 방법과 달리</code><br/> <code class="language-plaintext highlighter-rouge">빠르고 단순하게 regression(gradient descent)-based</code>로 반복 최적화 문제 풂 <ul> <li>bundle adjustment :<br/> 2D reprojection error 최소화</li> <li>본 논문 :<br/> 2D reprojection 뿐만 아니라 3D projection error을 같이 최소화</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/13m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/13m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/13m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/13m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="downstream---depth-estimation">Downstream - Depth Estimation</h3> <ul> <li>Monocular Depth Estimation</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/11m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/11m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/11m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/11m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Multi-view Depth Estimation</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/12m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/12m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/12m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/12m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="downstream---dense-3d-reconstruction">Downstream - Dense 3D reconstruction</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-11-19-DUSt3R/14m.PNG-480.webp 480w,/assets/img/2024-11-19-DUSt3R/14m.PNG-800.webp 800w,/assets/img/2024-11-19-DUSt3R/14m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-11-19-DUSt3R/14m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="limitation">Limitation</h3> <ul> <li>장점 : <ul> <li>camera pose 정보 또는 SfM 없어도 self-supervised 방식으로 여러 downstream task 수행 가능</li> <li>원형 호수 파라노마처럼 돌면서 찍을 경우 view에 따라 큰 차이가 없어서 COLMAP은 잘 못 하는데 DUSt3R는 그래도 괜찮</li> <li>RGB-based라서 LiDAR가 못 잡는 투명한 물체도 잘 잡음</li> </ul> </li> <li>한계 : <ul> <li>각 downstream task에서 SOTA 급은 아님.<br/> 왜냐하면 regression 방식으로 정확하게 3D recon.하려면 depth가 엄청 정밀해야 하는데 그렇지 않고,<br/> regression 방식이라 COLMAP 방식보다는 오차가 있음.<br/> 그래도 pose 없이 높은 성능 이뤘다는 데에 의미가 있음</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="point"/><category term="regression"/><category term="pose"/><category term="free"/><summary type="html"><![CDATA[Geometric 3D Vision Made Easy (CVPR 2024)]]></summary></entry><entry><title type="html">Deblurring 3D Gaussian Splatting</title><link href="https://semyeong-yu.github.io/blog/2024/Deblurring3DGS/" rel="alternate" type="text/html" title="Deblurring 3D Gaussian Splatting"/><published>2024-10-30T12:00:00+00:00</published><updated>2024-10-30T12:00:00+00:00</updated><id>https://semyeong-yu.github.io/blog/2024/Deblurring3DGS</id><content type="html" xml:base="https://semyeong-yu.github.io/blog/2024/Deblurring3DGS/"><![CDATA[<h2 id="deblurring-3d-gaussian-splatting-eccv-2024">Deblurring 3D Gaussian Splatting (ECCV 2024)</h2> <h4 id="byeonghyeon-lee-howoong-lee-xiangyu-sun-usman-ali-eunbyung-park">Byeonghyeon Lee, Howoong Lee, Xiangyu Sun, Usman Ali, Eunbyung Park</h4> <blockquote> <p>paper :<br/> <a href="https://arxiv.org/abs/2401.00834">https://arxiv.org/abs/2401.00834</a><br/> project website :<br/> <a href="https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/">https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/</a><br/> code :<br/> <a href="https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting">https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting</a></p> </blockquote> <blockquote> <p>핵심 :</p> <ol> <li>defocus blur 구현 :<br/> MLP로 covariance(rotation, scaling)의 변화량을 모델링해서<br/> covariance를 키워서<br/> defocus-blurred image 얻음</li> <li>camera motion blur 구현 :<br/> MLP로 position 및 covariance의 변화량을 모델링해서<br/> M개의 3DGS sets를 만든 뒤<br/> 이걸로 만든 M개의 sharp imgs를 average해서<br/> camera-motion-blurred image 얻음</li> <li>위의 MLP를 training에서만 사용하므로<br/> still real-time rendering at inference</li> <li>sparse point clouds 보상하기 위해 points 추가<br/> 또한 먼 거리에 있는 3DGS는 덜 prune out</li> </ol> </blockquote> <h3 id="introduction">Introduction</h3> <ul> <li>3DGS : <ul> <li>novel-view로 inference할 때<br/> NeRF는 새로운 각도를 MLP에 넣어야만 color, opacity 얻을 수 있지만<br/> 3DGS는 spherical harmonics, explicit 기법이라 새로운 각도에 대해서도 바로 color, opacity 얻을 수 있어서<br/> volume rendering이 빠름</li> <li>differentiable splatting-based rasterization with parallelism</li> </ul> </li> <li>본 논문 : <ul> <li>핵심 : <ul> <li>각 3DGS의 <code class="language-plaintext highlighter-rouge">covariance</code>를 수정하여 <code class="language-plaintext highlighter-rouge">blur(adjacent pixels의 혼합)를 모델링하는 작은 MLP</code> 사용</li> <li>training 시에는 MLP output 곱해서 blurry image를 생성하고<br/> inference 시에는 MLP 사용하지 않아서 real-time으로 sharp image 생성</li> </ul> </li> <li>문제 : <ul> <li>3DGS는 initial point cloud에 많이 의존하는데<br/> given images가 <code class="language-plaintext highlighter-rouge">blurry</code>하면 SfM은 유효한 feature를 식별하지 못해서 <code class="language-plaintext highlighter-rouge">매우 적은 수의 point</code> cloud를 추출함</li> <li>심지어 depth가 크면 SfM은 맨 끝에 있는 점을 거의 추출하지 않음</li> </ul> </li> <li>해결 : <ul> <li>sparse point cloud를 방지하고자<br/> <code class="language-plaintext highlighter-rouge">N-nearest-neighbor interpolation으로 points 추가</code></li> <li>먼 거리의 평면에 많은 Gaussian을 유지하기 위해<br/> <code class="language-plaintext highlighter-rouge">위치에 따라 Gaussian pruning</code></li> </ul> </li> <li>contribution :<br/> SOTA qualtiy인데 훨씬 빠른 rendering speed (\(\gt 200\) FPS)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/1m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/1m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/1m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/1m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Overall Architecture </div> <h3 id="related-works">Related Works</h3> <ul> <li>Image Deblurring : <ul> <li>\(g(x) = \sum_{s \in S_{h}} h(x, s) f(x) + n(x)\)<br/> where \(g(x)\) : blurry image and \(f(x)\) : latent sharp image<br/> where \(h(x, s)\) : blur kernel or PSF (Point Spread Function)<br/> where \(n(x)\) : additive white Gaussian noise (occurs in nature images)</li> <li>지금까지 2D image deblurring은 많이 연구되어 왔는데<br/> 3D scene deblurring은 3D view consistency 부족 때문에 연구하기 어려웠음</li> </ul> </li> <li>Fast NeRF : <ul> <li>방법 1)<br/> use additional data-structure to reduce the size and number of MLP layers<br/> but, fail to reach real-time view synthesis <ul> <li>grid-based :<br/> Hexplane, TensoRF, K-planes, Mip-grid, Masked wavelet representation, Direct voxel grid optimization, F2-nerf</li> <li>hash-based :<br/> InstantNGP, Zip-nerf</li> </ul> </li> <li>방법 2)<br/> trained param.을 faster representation으로 bake해서 real-time rendering <ul> <li>Baking neural radiance fields, Merf, Bakedsdf</li> </ul> </li> </ul> </li> <li>Deblurring NeRF :<br/> 자세한 건 <a href="https://semyeong-yu.github.io/blog/2024/DeblurNeRF/">Link</a> 참조 <ul> <li>DoF-NeRF <d-cite key="DofNeRF">[1]</d-cite> : <ul> <li>단점 :<br/> train하기 위해 all-in-focus image와 blurry image 모두 필요<br/> (all-in-focus image : 화면 전체가 초점이 맞춰져 있는 image)</li> </ul> </li> <li>Deblur-NeRF <d-cite key="DeblurNeRF">[2]</d-cite> : <ul> <li>장점 :<br/> train할 때 all-in-focus image 필요 없음</li> <li>핵심 :<br/> additional small MLP 사용해서<br/> per-pixel blur kernel 예측</li> </ul> </li> <li>DP-NeRF <d-cite key="DpNeRF">[3]</d-cite> and PDRF <d-cite key="PDRF">[4]</d-cite> : <ul> <li>Deblur-NeRF 발전시킴</li> </ul> </li> <li>Hybrid <d-cite key="Hybrid">[5]</d-cite> and Sharp-NeRF <d-cite key="SharpNeRF">[6]</d-cite> and BAD-NeRF <d-cite key="BADNeRF">[7]</d-cite> : <ul> <li>camera motion blur와 defocus blur 중 하나만 다룸</li> </ul> </li> </ul> </li> <li>Deblurring NeRF 요약 : <ul> <li>deblur task 잘 수행하지만<br/> NeRF 자체가 rendering time이 오래 걸림<br/> \(\rightarrow\)<br/> real-time differentiable rasterizer 이용하는<br/> 3DGS로 deblur task 수행하자!</li> </ul> </li> </ul> <h3 id="background">Background</h3> <ul> <li> <p>3DGS <a href="https://semyeong-yu.github.io/blog/2024/GS/">Link</a> 참고</p> </li> <li> <p>Blur :</p> <ul> <li>Defocus Blur :<br/> 렌즈의 <code class="language-plaintext highlighter-rouge">초점이 맞지 않아서</code> 흐려진 경우<br/> e.g. 인물 사진에서 인물만 초점이 맞고 배경은 흐릿한 경우</li> <li>Camera Motion Blur :<br/> 셔터가 열려 있는 동안 카메라가 움직이거나 피사체가 <code class="language-plaintext highlighter-rouge">움직여서</code> 흐려진 경우<br/> e.g. 달리는 자동차를 촬영한 경우</li> </ul> </li> </ul> <h3 id="defocus-blur">Defocus Blur</h3> <ul> <li>Motivation : <ul> <li>Defocus Blur는 일반적으로<br/> 실제 image와 PSF(point spread func.)(2D Gaussian function) 간의<br/> convolution으로 모델링<br/> 즉, a pixel이 주위 pixels에 영향을 미칠 경우 blur</li> <li>여기서 영감을 받아<br/> <code class="language-plaintext highlighter-rouge">covariance(크기)가 큰 3DGS는 Blur</code>를 유발하고<br/> <code class="language-plaintext highlighter-rouge">covariance(크기)가 작은 3DGS는 Sharp</code> image에 기여한다고 가정<br/> (covariance(dispersion)가 클수록 Gaussian이 더 많은 pixels에 걸쳐 있으니까<br/> 더 많은 이웃한 pixels 간의 interference 표현 가능)</li> <li>그렇다면 covariance \(\Sigma = R S S^{T} R^{T}\) 를 변경하여 Blur를 모델링해야겠다!</li> </ul> </li> <li>Defocus Blur를 모델링하는 MLP :<br/> \((\delta r_{j}, \delta s_{j}) = F_{\theta}(\gamma(x_{j}), r_{j}, s_{j}, \gamma(v))\)<br/> where input : \(j\)-th Gaussian’s position, rotation, scale, view-direction<br/> where output : \(j\)-th Gaussian’s rotation change, scale change<br/> (\(\gamma\) : positional encoding) <ul> <li>transformed 3DGS : <ul> <li>rotation quaternion : \(\hat r_{j} = r_{j} \cdot \text{min}(1.0, \lambda_{s} \delta r_{j} + (1 - \lambda_{s}))\)</li> <li>scaling : \(\hat s_{j} = s_{j} \cdot \text{min}(1.0, \lambda_{s} \delta s_{j} + (1 - \lambda_{s}))\) <ul> <li>\(\cdot\) : element-wise multiplication</li> <li>\(\lambda_{s}\) 로 scale하고 \((1 - \lambda_{s})\) 로 shift : for optimization stability <code class="language-plaintext highlighter-rouge">???</code></li> <li>rotation 및 scaling 변화량의 <code class="language-plaintext highlighter-rouge">최솟값을 1로 clip</code> :<br/> \(\hat s_{j} \geq s_{j}\) 이므로 transformed 3DGS는 <code class="language-plaintext highlighter-rouge">더 큰 covariance</code>를 가져서<br/> <code class="language-plaintext highlighter-rouge">Defocus Blur</code>의 근본 원인인 주변 정보의 interference을 모델링할 수 있게 됨</li> </ul> </li> </ul> </li> <li>inference :<br/> scaling factor로 covariance 변화시키는 게 blur kernel의 역할을 하므로<br/> <code class="language-plaintext highlighter-rouge">training</code> 시에는 <code class="language-plaintext highlighter-rouge">transformed 3DGS</code>가 <code class="language-plaintext highlighter-rouge">blurry</code> image를 생성하지만<br/> <code class="language-plaintext highlighter-rouge">inference</code> 시에는 MLP를 사용하지 않은 <code class="language-plaintext highlighter-rouge">기존 3DGS</code>가 <code class="language-plaintext highlighter-rouge">sharp</code> image를 생성<br/> \(\rightarrow\)<br/> training할 때는 MLP forwarding과 간단한 element-wise multiplication만 추가 비용이고,<br/> inference할 때는 MLP를 사용하지 않아 Vanilla-3DGS와 모든 단계가 동일하므로<br/> <code class="language-plaintext highlighter-rouge">추가 비용 없이 real-time rendering</code> 가능</li> </ul> </li> </ul> <h3 id="selective-blurring">Selective Blurring</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/2m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/2m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/2m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/2m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>초점에 의한 Defocus Blur는 <code class="language-plaintext highlighter-rouge">영역마다 흐린 수준이 다름</code><br/> 본 논문에서는 <code class="language-plaintext highlighter-rouge">각 3DGS마다</code> 다르게 \(\delta_{r}, \delta_{s}\) 를 추정하므로<br/> Gaussian의 covariance를 선택적으로 확대시킬 수 있어서<br/> 영역에 따라 다르게 blurring 할 수 있으므로<br/> <code class="language-plaintext highlighter-rouge">pixel 단위의 blurring</code>을 보다 유연하게 모델링 가능 <ul> <li>defocus blur가 심한 영역에 있는 3DGS는 \(\delta_{s}\) 가 더 크도록</li> <li>당연히 단일 유형의 Gaussian Blur kernel을 써서 평균 blurring을 모델링하는 것보다<br/> 본 논문에서처럼 3DGS마다 다른 Blur kernel을 적용하여 pixel 단위 blurring을 모델링하는 게 더 좋음!</li> </ul> </li> </ul> <h3 id="camera-motion-blur">Camera motion Blur</h3> <ul> <li> <p>셔터가 열려 있는 exposure time 동안<br/> camera movement가 있으면<br/> light intensities from multipe sources가 inter-mixed되어<br/> Camera motion Blur 발생</p> </li> <li> <p>Camera motion Blur를 모델링하는 MLP :<br/> \({(\delta x_{j}^{(i)}, \delta r_{j}^{(i)}, \delta s_{j}^{(i)})}_{i=1}^{M} = F_{\theta}(\gamma(x_{j}), r_{j}, s_{j}, \gamma(v))\)</p> <ul> <li>transformed 3DGS : <ul> <li>3D position : \(\hat x_{j}^{(i)} = x_{j} + \lambda_{p} \delta x_{j}^{(i)}\) (shift)</li> <li>rotation quaternion : \(\hat r_{j}^{(i)} = r_{j} \cdot \delta r_{j}^{(i)}\) (element-wise multiplication)</li> <li>scaling : \(\hat s_{j}^{(i)} = s_{j} \cdot \delta s_{j}^{(i)}\) (element-wise multiplication) <ul> <li>Camera motion Blur의 경우<br/> Defocus Blur와 달리 covariance를 무조건 키워야 되는 게 아니므로<br/> min-clip by 1.0 없음</li> </ul> </li> </ul> </li> <li>Camera motion Blur :<br/> \(I_{b} = \frac{1}{M} \sum_{i=1}^{M} I_{i}\) <ul> <li>셔터가 열려 있는 동안 카메라가 움직이는 각 discrete moment는<br/> 각 3DGS set에 대응됨</li> <li>\(j\)-th Gaussian 의 <code class="language-plaintext highlighter-rouge">camera movement</code>를 나타내기 위해<br/> <code class="language-plaintext highlighter-rouge">M개의 auxiliary 3DGS sets</code> 만들어서<br/> <code class="language-plaintext highlighter-rouge">M개의 clean images</code> rendering해서<br/> <code class="language-plaintext highlighter-rouge">average</code>해서 camera-motion-blurred image 얻음</li> </ul> </li> <li>inference :<br/> 마찬가지로 <code class="language-plaintext highlighter-rouge">inference</code> 시에는 MLP를 사용하지 않은 <code class="language-plaintext highlighter-rouge">기존 3DGS</code>가 clean image를 생성<br/> \(\rightarrow\)<br/> inference할 때는 MLP로 \(M\)-개의 3DGS sets 만들지 않고<br/> Vanilla-3DGS와 모든 단계가 동일하므로<br/> <code class="language-plaintext highlighter-rouge">추가 비용 없이 real-time rendering</code> 가능</li> </ul> </li> </ul> <h3 id="compensation-for-sparse-point-cloud">Compensation for Sparse Point Cloud</h3> <ul> <li> <p>문제 1)<br/> 3DGS는 initial point cloud에 많이 의존하는데<br/> given input multi-view images가 <code class="language-plaintext highlighter-rouge">blurry</code>하면<br/> SfM은 유효한 feature를 식별하지 못해서<br/> 매우 적은 수의 <code class="language-plaintext highlighter-rouge">sparse point cloud</code>를 추출함</p> </li> <li> <p>해결 :</p> <ul> <li>sparse point cloud를 방지하고자<br/> \(N_{st}\) iter. 후에 \(N_{p}\)-개의 points를 uniform \(U(\alpha, \beta)\) 에서 sampling하여 추가<br/> where \(\alpha\) : 기존 point cloud 위치의 최솟값<br/> where \(\beta\) : 기존 point cloud 위치의 최댓값</li> <li>새로운 point의 <code class="language-plaintext highlighter-rouge">색상은 KNN(K-Nearest-Neighbor) interpolation</code>으로 할당</li> <li>새로운 points를 uniform 분포에서 sampling해서 <code class="language-plaintext highlighter-rouge">빈 공간에 불필요한 points</code>가 생길 수 있으므로<br/> nearest neighbor까지의 거리가 threshold \(t_{d}\) 를 초과하는 points는 <code class="language-plaintext highlighter-rouge">폐기</code></li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/3m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/3m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/3m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/3m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/12m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/12m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/12m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/12m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 가운데는 without adding points, 오른쪽은 with adding extra points </div> <ul> <li> <p>문제 2)<br/> 심지어 depth of field가 크면<br/> SfM은 맨 끝에 있는 점을 거의 추출하지 않음</p> </li> <li> <p>해결 :<br/> Deblur-NeRF dataset은 forward-facing scene으로만 구성되어 있으므로<br/> dataset에 기록된 <code class="language-plaintext highlighter-rouge">z-axis 값</code>은 <code class="language-plaintext highlighter-rouge">relative depth</code> from any viewpoint라고 볼 수 있음</p> <ul> <li>방법 1) 먼 거리에 있는 3DGS 수 늘리기<br/> 먼 거리의 평면에 있는 3DGS에 대해 denisfy<br/> \(\rightarrow\)<br/> 과도한 densification은 Blur 모델링을 방해하고 추가 계산 비용 필요</li> <li>방법 2) <code class="language-plaintext highlighter-rouge">먼 거리에 있는 3DGS는 덜 prune out</code><br/> pruning threshold를 깊이에 따라 다르게 scaling<br/> as \(t_{p}, 0.9 t_{p}, \cdots , \frac{1}{w_{p}} t_{p}\)<br/> (먼 거리의 3DGS일수록 낮은 threshold) <br/> \(\rightarrow\)<br/> real-time rendering을 고려했을 때<br/> 유연한 pruning으로도 먼 거리의 3DGS sparsity를 보상하기에 충분하다는 걸 경험적으로 발견</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/4m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/4m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/4m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/4m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="experiment">Experiment</h3> <ul> <li>Setting : <ul> <li>dataset : Deblur-NeRF dataset <ul> <li>have both synthetic and real images</li> <li>has camera motion blur or defocus blur</li> </ul> </li> <li>GPU : NVIDIA RTX 4090 GPU (24GB)</li> <li>optimzier : Adam</li> <li>iter. : \(20,000\)</li> <li>Blur를 모델링하는 small MLP : <ul> <li>lr : \(1e^{-3}\)</li> <li>hidden layer : 4 <ul> <li>3 layers : shared</li> <li>1 layer : head for each \(\delta\)</li> </ul> </li> <li>hidden unit : 64</li> <li>activation : ReLU</li> <li>initialization : Xavier</li> <li>scaling factor for \(\delta\) : \(\lambda_{s}, \lambda_{p} = 1 e^{-2}\)</li> </ul> </li> <li>sparse point cloud를 보상하기 위해 <ul> <li>\(N_{st} = 2,500\) iter. 후에 \(N_{p}\) 개의 point 추가<br/> \(N_{p}\) 는 기존 point cloud 규모에 비례하며 최대 200,000개</li> <li>색상은 \(K = 4\) 의 KNN interpolation으로 할당</li> <li>nearest neighbor까지의 거리가 \(t_{d} = 2\) 을 초과하는 point는 폐기</li> </ul> </li> <li>먼 거리에 있는 3DGS는 덜 pruning하기 위해<br/> pruning threshold를 깊이에 따라 다르게 scaling <ul> <li>pruning threshold \(t_{p} = 5 e^{-3}\) and densification threshold \(2 e^{-4}\)<br/> for real defocus blur dataset</li> <li>pruning threshold \(t_{p} = 1 e^{-2}\) and densification threshold \(5 e^{-4}\)<br/> for real camera motion blur dataset</li> <li>pruning threshold multiplier \(w_{p} = 3\)</li> </ul> </li> <li>camera motion blur를 구현하기 위해<br/> \(M = 5\) 개의 3DGS sets 만들어서<br/> \(M = 5\) 개의 clean images를 average</li> </ul> </li> </ul> <h3 id="results">Results</h3> <ul> <li>Results : <ul> <li><code class="language-plaintext highlighter-rouge">SOTA deblurring NeRF</code>만큼 <code class="language-plaintext highlighter-rouge">PSNR</code> 높음</li> <li><code class="language-plaintext highlighter-rouge">3DGS</code>만큼 <code class="language-plaintext highlighter-rouge">FPS</code> 높음</li> <li>비교 대상으로 쓰인 논문들 : <ul> <li>Deblur-NeRF, Sharp-NeRF, DP-NeRF, PDRF</li> <li>original 3DGS</li> <li>Restormer로 input training images 먼저 deblur한 뒤 original 3DGS</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/5m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/5m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/5m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/5m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/6m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/6m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/6m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/6m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> real-world Defocus Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/7m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/7m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/7m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/7m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> real-world Defocus Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/8m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/8m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/8m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/8m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> synthesized Defocus Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/9m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/9m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/9m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/9m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> synthesized Defocus Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/13m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/13m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/13m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/13m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> real-world Camera motion Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/14m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/14m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/14m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/14m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> real-world Camera motion Blur Dataset </div> <h3 id="ablation-study">Ablation Study</h3> <ul> <li>Ablation Study : <ul> <li>Extra points allocation</li> <li>Depth-based pruning</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/10m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/10m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/10m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/10m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Extra points allocation </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/11m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/11m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/11m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/2024-10-30-Deblurring3DGS/11m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Depth-based pruning </div> <h3 id="limitation-and-future-work">Limitation and Future Work</h3> <ul> <li>Limitation : <ul> <li>volumetric rendering 기반의 NeRF-based deblurring 기법들을<br/> rasterization 기반의 3DGS에 적용하기 어렵<br/> \(\rightarrow\)<br/> MLP로 <code class="language-plaintext highlighter-rouge">world-space</code>에서의 rays 또는 kernels를 변형하는 대신<br/> MLP로 <code class="language-plaintext highlighter-rouge">rasterized image space</code>에서의 kernels를 변형하면<br/> Deblurring 3DGS 구현 가능<br/> \(\rightarrow\)<br/> 하지만 kernel interpolation 방향으로 가면<br/> pixel interpolation은 추가 비용이 발생하며<br/> 3DGS의 geometry를 implicitly 변형하는 것일 뿐이므로<br/> 해당 방법은 3DGS로 blur를 모델링하는 최적의 방법이 아닐 것이다<br/> 이를 개선하기 위한 future works 필요</li> </ul> </li> </ul> <h3 id="code-review">Code Review</h3> <ul> <li>blur kernel 함수 :<br/> Defocus Blur 및 Camera motion Blur <ul> <li>정의 : <a href="https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/scene/blur_kernel.py#L74">https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/scene/blur_kernel.py#L74</a></li> <li>호출 : <a href="https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/gaussian_renderer/__init__.py#L101">https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/gaussian_renderer/<strong>init</strong>.py#L101</a></li> </ul> </li> <li>sparse point cloud 보상하기 위해 add points : <ul> <li><a href="https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/scene/gaussian_model.py#L444">https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/scene/gaussian_model.py#L444</a></li> </ul> </li> </ul> <h3 id="question">Question</h3> <ul> <li> <p>Q1 :<br/> small MLP는 어떤 architecture로 구성되어 있나요?<br/> Dust3R 기반의 논문들을 보면<br/> transformer 등 pre-trained complex model 가져와서 쓰는데<br/> feed-forward 방식으로 학습하므로<br/> 빠르면서도 성능이 좋습니다. 이를 적용할 수 있지 않을까요?</p> </li> <li> <p>A1 :<br/> 일단 본 논문에서는 fast training 유지하기 위해 shallow MLP가 simply fc layers로 구성되어 있습니다<br/> 말씀해주신대로 simple shallow MLP 대신 더 좋은 network 쓰면 성능이 더 좋아질 것 같다고 생각하는데,<br/> deblurring task를 다룬 본 논문 이후의 논문들을 아직 읽어보지 않아서<br/> 혹시 읽어보고 좋은 아이디어 있다면 공유하도록 하겠습니다.</p> </li> <li> <p>Q2 :<br/> 본 논문이 deblurring task를 위해 pre-trained 3DGS를 가져와서 fine-tuning하는 것인가요?</p> </li> <li> <p>A2 :<br/> 아닙니다. 기존 3DGS에 blur를 모델링하는 MLP만 추가해서 scratch부터 training하고,<br/> 이로써 input image가 더러워도(blurry하더라도) clean image를 rendering할 수 있게 됩니다.<br/> 그리고 기존 3DGS와 같이 per-scene 방식으로 학습하는 것으로 알고 있습니다.</p> </li> <li> <p>Q3 :<br/> blurry input image를 dataset에서 미리 빼버리면 deblurring을 해야 하는 상황이 없어지잖아요<br/> 이처럼 제가 생각하기에는 굳이 deblurring을 해야 하나 라는 생각이 듭니다.</p> </li> <li> <p>A3 :<br/> 일단 deblurring이라는 게 super-resolution처럼 하나의 task로 생각할 수 있습니다<br/> input image가 blurry 할 수 있는데 말씀해주신대로 이를 dataset에서 미리 뺀다는 것 자체가 manual effort를 필요로 합니다 (이를 model이 대신 해준다면 좋겠죠)<br/> 그리고 만약 주어진 모델이 deblurring을 수행할 수 있다면 다른 모델의 앞단에 쓰여서 blur를 제거하는 pre-processing 용도로도 쓰일 수 있습니다.<br/> 이로써 input images가 현실에서 있을 법한 더러운(blurry) 이미지더라도 상관 없이 input으로 쓸 수 있습니다.<br/> 2D image 또는 video를 deblurring하는 논문들은 이미 많이 있는데<br/> 3D scene deblurring의 경우에는 3D view consistency 때문에 어려움이 있었습니다.<br/> 그러다가 3DGS 등장 이후로 처음 3DGS deblurring을 시도한 논문이 본 논문이라고 보시면 될 것 같습니다.</p> </li> <li> <p>Q4 :<br/> 그렇다면 deblurring task라는 게 uncertainty를 해결하는 것이라고 볼 수 있을까요? 아니면 이것과는 별개의 task로 봐야 할까요?</p> </li> <li> <p>A4 :<br/> (3D recon. 및 novel view synthesis에서 uncertainty라는 용어가 자주 등장하는데, 관련 논문들을 아직 많이 읽어보지 않아서 확실하게 답변드리지 못하겠습니다.)</p> </li> <li> <p>Q5 :<br/> dataset에 있는 image들이 blurry하지 않고 clean(sharp) 하더라도<br/> camera explore 하면서 novel view에 대해 rendering을 하다보면 rendered image에 blur가 생길 수 있을 것 같은데<br/> deblurring이라는 게 이러한 blur도 제거해주나요?</p> </li> <li> <p>A5 :<br/> 일단 본 논문에서 deblur를 하는 원리는 covariance를 조정하는 MLP로 blur를 모델링하여<br/> 해당 MLP(blur 담당)를 사용하지 않는 inference에서는 deblurred image가 rendering되는 것입니다<br/> 하지만 input이 blurry해서 생긴 blur가 아니라 rendering하다보니 생기는 artifacts로서의 blur의 경우라면<br/> 해당 MLP가 artifacts로서의 blur도 잘 모델링해줄지는 모르겠습니다. 더 찾아봐야 할 것 같습니다.</p> </li> <li> <p>Q6 :<br/> 혹시 본 논문을 읽으면서 생각해보셨던 limitation이 있을까요? 논문에 적혀있는 것 말고 개인적인 생각이 있으신지 궁금합니다.<br/> 저는 뭔가 본 논문의 알고리즘이 artificial하다는 생각이 들었습니다.</p> </li> <li> <p>A6 :<br/> (개인적으로 생각해본 limitation 답변 못 드림 ㅠㅠ 앞으로는 논문 읽을 때 novelty 말고도 limitation이 무엇일지 생각하는 습관 길러보자!)<br/> 기존 deblur nerf에서는 deblur kernel을 이용해서 여러 ray를 쏴서 2D 상에서 pixel들을 interpolate해서 blur를 모델링하는데<br/> deblurring 3DGS에서는 3D 상에서 Gaussian covariance를 키우는 방식으로 interpolate를 비슷하게 구현했다는 논리(가정)이고<br/> 결과적으로 성능이 좋게 나왔으니 본인들 주장(가정)이 맞았다 인 것 같아서 말씀해주신대로 artificial한 느낌이 들긴 하네요</p> </li> </ul>]]></content><author><name></name></author><category term="3d-view-synthesis"/><category term="3DGS"/><category term="deblur"/><summary type="html"><![CDATA[ECCV 2024]]></summary></entry></feed>