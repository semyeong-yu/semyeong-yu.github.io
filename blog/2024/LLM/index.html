<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Solar LLM with Langchain | Semyeong Yu </title> <meta name="author" content="Semyeong Yu"> <meta name="description" content="Upstage Solar LLM as Personalized LLM"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://semyeong-yu.github.io/blog/2024/LLM/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Solar LLM with Langchain",
            "description": "Upstage Solar LLM as Personalized LLM",
            "published": "July 03, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Semyeong</span> Yu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Solar LLM with Langchain</h1> <p>Upstage Solar LLM as Personalized LLM</p> </d-title> <d-article> <h2 id="solar-llm-by-upstage">Solar LLM by Upstage</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-03-LLM/1-480.webp 480w,/assets/img/2024-07-03-LLM/1-800.webp 800w,/assets/img/2024-07-03-LLM/1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-07-03-LLM/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="solar-llm-as-personalized-llm">Solar LLM as Personalized LLM</h3> <ul> <li> <p>Referenced Github :<br> <a href="https://github.com/UpstageAI/cookbook/tree/main" rel="external nofollow noopener" target="_blank">UpstageAI-cookbook</a><br> <a href="https://github.com/semyeong-yu/LLM-cookbook" rel="external nofollow noopener" target="_blank">UpstageAI-cookbook</a><br> <a href="https://github.com/iamtaewan/solarllm-oracle-cookbook" rel="external nofollow noopener" target="_blank">OracleDB-cookbook</a></p> </li> <li> <p>Solar mini LLM :<br> small-size<br> best LLM for fine-tuning<br> can be used as personalized LLM</p> </li> <li> <p>Future of AI Ecosystem Hierarchy :<br> Domain-specific and self-fine-tuned LLMs<br> Solar LLM O/S<br> O/S<br> AI chips</p> </li> <li> <p>Langchain :<br> LLM과 application의 통합을 간소화하는 SDK</p> </li> <li> <p>핵심 기능 : 앞으로 아래에서 배울 예정!!</p> <ul> <li>LLM 사용 (query, context)</li> <li>Groundedness Check (팩트체크)</li> <li>Layout Analyzer (PDF 또는 img에서 정보 추출)</li> <li>Embedding and DB vector store (embedding vector를 DB에 저장)</li> <li>Define Custom Tools (img 생성, 뉴스 검색, 스케쥴 관리 등)</li> </ul> </li> </ul> <h3 id="chat">Chat</h3> <pre><code class="language-Python">from langchain_upstage import ChatUpstage

llm = ChatUpstage()
llm.invoke("What's the best season to get to Korean?") # invoke llm

llm = ChatUpstage(model="solar-1-mini-chat-ja")
llm.invoke("ソーラーLLM、こんにちは。ソーラーLLM、こんにちは。")
</code></pre> <h3 id="few-shot-learning---chain">Few-shot Learning - Chain</h3> <pre><code class="language-Python"># 1. use Chat Prompt Template
from langchain_core.prompts import ChatPromptTemplate

# 2. 농담조로 말하도록 Few-shot Learning
chat_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant."),
        ("human", "What is the capital of France?"),
        ("ai", "I know of it. It's Paris!!"),
        ("human", "What about Korea?"),
    ]
)

# 3. define and invoke chain
from langchain_core.output_parsers import StrOutputParser

chain = chat_prompt | llm | StrOutputParser()
chain.invoke({})
</code></pre> <pre><code class="language-Python"># 1. use Prompt Template
from langchain_core.prompts import PromptTemplate

# 2. 한 번에 답을 내도록 (Standard Prompting) Few-shot Learning
# 한 번에 답을 내려다보니 llm이 답 틀리게 내놓음
prompt_template = PromptTemplate.from_template(
    """
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?

A: The answer is 11.

Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?

A: the answer is
"""
)

# 3. define and invoke chain
chain = prompt_template | llm | StrOutputParser()
chain.invoke({})
</code></pre> <pre><code class="language-Python"># 1. use Prompt Template
from langchain_core.prompts import PromptTemplate

# 2. 설명하면서 답을 내도록 (Chain-of-Thought Prompting) Few-shot Learning
# 설명하면서 답을 내니 llm이 알맞게 답을 내놓음
prompt_template = PromptTemplate.from_template(
    """
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?

A: Roger started with 5 balls. 2 cans of 3 tennis balls
each is 6 tennis balls. 5 + 6 = 11. The answer is 11.

Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?"""
)

# 3. define and invoke chain
chain = prompt_template | llm | StrOutputParser()
chain.invoke({})
</code></pre> <h3 id="zero-shot-learning---chain">Zero-shot Learning - Chain</h3> <pre><code class="language-Python">from langchain_core.prompts import PromptTemplate

# Zero-shot, 즉 예시를 주지 않고
# "Let's think step by step"이라는 마법의 한 문장만 써줬는데도
# 답 잘 내놓음
prompt_template = PromptTemplate.from_template(
    """
Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?

A: The answer is 11.

Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?

A: Let's think step by step.
"""
)
chain = prompt_template | llm | StrOutputParser()
chain.invoke({})
</code></pre> <h3 id="divide-and-conquer">Divide-and-Conquer</h3> <ul> <li>Please provide three questions from the following text</li> </ul> <p>보다는</p> <ul> <li>Please extract three keywords from the following text 한 다음<br> Please provide one question from the following text regarding “Depth up-scaling (DUS)”</li> </ul> <h3 id="prompt-반복">Prompt 반복</h3> <p>python f-string과 비슷한 원리</p> <pre><code class="language-Python">from langchain_core.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template(
    "Tell me a {adjective} joke about {content}."
)
# prompt_template.format(adjective="funny", content="chickens")

chain = prompt_template | llm | StrOutputParser()
chain.invoke({"adjective": "funny", "content": "chickens"})
</code></pre> <h3 id="keep-message-history-in-langchain-prompts">Keep Message History in LangChain Prompts</h3> <p>MessagesPlaceholder</p> <pre><code class="language-Python">from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# General Chat form with Message History
rag_with_history_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant."),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{input}"),
    ]
)

from langchain_core.messages import AIMessage, HumanMessage

# Message History argument
history = [
    HumanMessage("What is the capital of France?"),
    AIMessage("It's Paris!!"),
]

chain = rag_with_history_prompt | llm | StrOutputParser()
chain_result = chain.invoke({"history": history, "input": "What about Korea?"})
print(chain_result)
</code></pre> <h3 id="groundedness-check-with-langchain">Groundedness Check with LangChain</h3> <p>Groundedness Check :<br> 답(answer)이 주어진 문맥(context)과 일맥상통하는지 (구라가 아닌지) <code class="language-plaintext highlighter-rouge">팩트 체크</code>!</p> <pre><code class="language-Python">from langchain_upstage import UpstageGroundednessCheck

groundedness_check = UpstageGroundednessCheck()

answer = chain.invoke(
    {
        "question": "What is DUS?",
        "Context": context,
    }
)
print("Potential answer: ", answer)

gc_result = groundedness_check.invoke({"context": context, "answer": answer})
print("GC check result: ", gc_result)
if gc_result.lower().startswith("grounded"):
    print("Groundedness check passed")
else:
    print("Groundedness check failed")
</code></pre> <h3 id="pdf-loader-context로-사용">PDF Loader (Context로 사용)</h3> <p>PDF에 있는 내용을 읽어와서 Context로 사용!</p> <pre><code class="language-Python">from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("pdfs/solar_sample.pdf")
docs = loader.load()  # or layzer.lazy_load()
print(docs[0].page_content[:1000])
</code></pre> <h3 id="layout-analyzer-context로-사용">Layout Analyzer (Context로 사용)</h3> <p>LLM이 받아들이기 좋은 형태로 문서를 읽기 위해<br> Extract layouts, tables, and figures from any document to .html file<br> Maximize RAG performance (RAG는 이후에 설명 예정)</p> <pre><code class="language-Python">! pip3 install -qU  markdownify  langchain-upstage  requests
import os
from langchain_upstage import UpstageLayoutAnalysisLoader
from IPython.display import display, HTML

os.environ["UPSTAGE_API_KEY"] = "UPSTAGE_API_KEY"

loader = UpstageLayoutAnalysisLoader("invoice.png", split="page", use_ocr=True)
# For improved memory efficiency, consider using the lazy_load method to load documents page by page.
pages = loader.load()  # or loader.lazy_load()
for page in pages:
    print(page)

loader = UpstageLayoutAnalysisLoader("pdfs/solar_sample.pdf", output_type="html")
docs = layzer.load() # or loader.lazy_load()
display(HTML(docs[0].page_content[:5000]))
</code></pre> <h3 id="rag-retrieval-augmented-generation">RAG: Retrieval Augmented Generation</h3> <ul> <li>RAG (Retrieval Augmented Generation) : <ul> <li> <code class="language-plaintext highlighter-rouge">pdf, html 등 주어진 파일에서 query와 관련 있는 부분만 검색해서 context로서 사용</code>!</li> <li>Large language models (LLMs) have a limited context size</li> <li>Not all context is relevant to a given question</li> <li> <code class="language-plaintext highlighter-rouge">Relevant context is retrieved(검색) from external data sources</code> and added to the prompt</li> <li>LLM generates a response based on this augmented context prompt</li> <li>RAG is particularly useful for Question Answering on custom datasets</li> <li>Query \(\rightarrow\) Retrieve (Search) \(\rightarrow\) Augmented Prompt \(\rightarrow\) LLM \(\rightarrow\) Answer</li> </ul> </li> <li>Chunking, Splitting : <ul> <li>Fixed-size chunking : split text into equal-sized chunks based on character or token count</li> <li>Semantic chunking : split text based on semantic boundaries like sentences, paragraphs, or sections</li> <li>Hierarchical chunking : create chunks at multiple levels of granularity (The ideal chunk size depends on the embedding model, retrieval use-case, and downstream task)</li> </ul> </li> </ul> <pre><code class="language-Python">from langchain_upstage import UpstageLayoutAnalysisLoader
from langchain_community.retrievers import BM25Retriever
from langchain_text_splitters import (
    Language,
    RecursiveCharacterTextSplitter,
)

layzer = UpstageLayoutAnalysisLoader(
    "pdfs/kim-tse-2008.pdf", use_ocr=True, output_type="html"
)
docs = layzer.load()

text_splitter = RecursiveCharacterTextSplitter.from_language(
    chunk_size=1000, chunk_overlap=100, language=Language.HTML
)
splits = text_splitter.split_documents(docs)

retriever = BM25Retriever.from_documents(splits)

query = "What is bug classficiation?"
context_docs = retriever.invoke("bug") # keyword search
chain.invoke({"question": query, "Context": context_docs})
</code></pre> <h3 id="keyword-search-대신-semantic-search-with-embedding-space">Keyword Search 대신 Semantic Search with Embedding space</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-03-LLM/2-480.webp 480w,/assets/img/2024-07-03-LLM/2-800.webp 800w,/assets/img/2024-07-03-LLM/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-07-03-LLM/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Solar-Embedding-1-Large (v1.0)<br> Convert unstructured text data into embedding vectors</p> <pre><code class="language-Python">from langchain_upstage import UpstageEmbeddings
from langchain_chroma import Chroma
from langchain_upstage import UpstageEmbeddings
from langchain.docstore.document import Document

embeddings_model = UpstageEmbeddings(
  api_key="UPSTAGE_API_KEY", 
  model="solar-embedding-1-large"
)
embeddings = embeddings_model.embed_documents(
    [
        "What is the best season to visit Korea?",
    ]
)
query_result = embeddings.embed_query("What does Sam do?") # vector

sample_text_list = [
    "Korea is a beautiful country to visit in the spring.",
    "The best time to visit Korea is in the fall.",
    "Best way to find bug is using unit test.",
    "Python is a great programming language for beginners.",
    "Sung Kim is a great teacher.",
    "맛있는 좋은 과일을 많이 먹어 볼까?"
]

sample_docs = [Document(page_content=text) for text in sample_text_list]

vectorstore = Chroma.from_documents(
    documents=sample_docs,
    embedding=UpstageEmbeddings(model="solar-embedding-1-large"),
)

retriever = vectorstore.as_retriever()
result_docs = retriever.invoke("When to visit Korea?")
print(result_docs[0].page_content[:100])
</code></pre> <h3 id="rag-summary">RAG Summary</h3> <ol> <li>load doc</li> <li>chunking, splits</li> <li>embedding, indexing (vector store)</li> <li>retrieval, augmenting context (find Top-k most similar doc chunks in vector store with the query embedding)</li> <li>invoke chain based on the augmented context by retrieval</li> </ol> <pre><code class="language-Python">from langchain_upstage import UpstageLayoutAnalysisLoader
from langchain_text_splitters import (
    Language,
    RecursiveCharacterTextSplitter,
)
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_upstage import ChatUpstage

# 1. load doc
layzer = UpstageLayoutAnalysisLoader("pdfs/kim-tse-2008.pdf", output_type="html")
docs = layzer.load()

# 2. chunking &amp; splits
text_splitter = RecursiveCharacterTextSplitter.from_language(
    chunk_size=1000, chunk_overlap=100, language=Language.HTML
)
splits = text_splitter.split_documents(docs)

# 3. Embedding &amp; indexing
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=UpstageEmbeddings(model="solar-embedding-1-large"),
)

# 4. retrieve
retriever = vectorstore.as_retriever()
result_docs = retriever.invoke("What is Bug Classification?")
print(len(result_docs))
print(result_docs[0].page_content[:100])

# 5. invoke chain based on the augmented context by retrieval 
llm = ChatUpstage()

prompt_template = PromptTemplate.from_template(
    """
    Please provide most correct answer from the following context. 
    If the answer is not present in the context, please write "The information is not present in the context."
    ---
    Question: {question}
    ---
    Context: {Context}
    """
)

chain = prompt_template | llm | StrOutputParser()
chain.invoke({"question": "What is bug classficiation?", "Context": result_docs})
</code></pre> <h3 id="oracle-db를-persistent-memory로-쓰기">Oracle DB를 persistent memory로 쓰기</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-03-LLM/3-480.webp 480w,/assets/img/2024-07-03-LLM/3-800.webp 800w,/assets/img/2024-07-03-LLM/3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-07-03-LLM/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>매번 100만 개의 pdf를 load해서 index할 수는 없는 일!</p> <ol> <li>load doc</li> <li>chunking, splits</li> <li>embedding, indexing (<code class="language-plaintext highlighter-rouge">vector store with Oracle DB</code>)</li> <li>retrieval, augmenting context</li> <li>split한 text가 이미 Oracle DB vector store에 있는지 체크</li> <li>없다면 embedding, indexing again</li> <li>invoke chain based on the augmented context by retrieval</li> </ol> <pre><code class="language-Python">! pip3 install -qU  markdownify  langchain-upstage rank_bm25

import oracledb
from langchain_upstage import UpstageLayoutAnalysisLoader

# 0. connect to Oracle DB
username=os.environ["DB_USER"]
password=os.environ["DB_PASSWORD"]
dsn=os.environ["DSN"]

con = oracledb.connect(user=username, password=password, dsn=dsn)

try: 
    conn23c = oracledb.connect(user=username, password=password, dsn=dsn)
    print("Connection successful!", conn23c.version)
except Exception as e:
    print("Connection failed!")

# 1. load doc
layzer = UpstageLayoutAnalysisLoader("pdfs/kim-tse-2008.pdf", output_type="html")
docs = layzer.load()

# 2. chunking &amp; splits
text_splitter = RecursiveCharacterTextSplitter.from_language(
    chunk_size=1000, chunk_overlap=100, language=Language.HTML
)
splits = text_splitter.split_documents(docs)

# check if text is in the vector store
def is_in_vectorstore(vectorstore, text):
    search_results = vectorstore.get(ids=[text])
    if search_results and search_results["ids"]:
        return True
    else:
        return False

# 3. Embedding &amp; indexing 방법 1.
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=UpstageEmbeddings(model="solar-embedding-1-large"),
)

# 3. Embedding &amp; indexing 방법 2.

knowledge_base = OracleVS.from_documents(docs, upstage_embeddings, client=conn23c, table_name="text_embeddings2", distance_strategy=DistanceStrategy.DOT_PRODUCT)

# result_chunks = knowledge_base.similarity_search(user_question)

vectorstore = OracleVS(client=conn23c, embedding_function=upstage_embeddings, table_name="text_embeddings2", distance_strategy=DistanceStrategy.DOT_PRODUCT) # create vector store

oraclevs.create_index(
    client=conn23c,
    vector_store=vectorstore,
    params={
        "idx_name": "ivf_idx1",
        "idx_type": "IVF",
    },
) # index 추가

# 4. retrieve
retriever = vectorstore.as_retriever()

unique_splits = [
    split for split in splits if not is_in_vectorstore(vectorstore, split.page_content)
]
print(len(unique_splits))

# 5. split한 text가 이미 Oracle DB vector store에 있는지 체크
# 6. 없다면 embedding, indexing again
if len(unique_splits) &gt; 0:
    vectorstore = Chroma.from_documents(
        ids=[split.page_content for split in unique_splits],
        persist_directory="./chroma_db",
        documents=unique_splits,
        embedding=UpstageEmbeddings(model="solar-embedding-1-large"),
)

# 7. invoke chain based on the augmented context by retrieval 
llm = ChatUpstage()

prompt_template = PromptTemplate.from_template(
    """
    Please provide most correct answer from the following context. 
    If the answer is not present in the context, please write "The information is not present in the context."
    ---
    Question: {question}
    ---
    Context: {Context}
    ---
    Output: please, response in Korean
    """
)

chain = ({"context": retriever, "question": RunnablePassthrough()} | prompt_template | llm | StrOutputParser())
response = chain.invoke("What is bug classficiation?")
</code></pre> <h3 id="smart-rag">Smart RAG</h3> <p>local vector store에 검색했을 때</p> <ul> <li>내가 아는 건 <code class="language-plaintext highlighter-rouge">local RAG</code> 로 처리</li> <li>내가 모르는 건 <code class="language-plaintext highlighter-rouge">external search</code> 로 처리</li> </ul> <pre><code class="language-Python"># 주어진 context만으로 주어진 question에 답변할 수 있는지 판단
# RAG or Search?
def is_in(question, context):
    is_in_conetxt = """As a helpful assistant, 
please use your best judgment to determine if the answer to the question is within the given context. 
If the answer is present in the context, please respond with "yes". 
If not, please respond with "no". 
Only provide "yes" or "no" and avoid including any additional information. 
Please do your best. Here is the question and the context:
---
CONTEXT: {context}
---
QUESTION: {question}
---
OUTPUT (yes or no):"""

    is_in_prompt = PromptTemplate.from_template(is_in_conetxt)
    chain = is_in_prompt | ChatUpstage() | StrOutputParser()

    response = chain.invoke({"context": context, "question": question})
    print(response)
    return response.lower().startswith("yes")
</code></pre> <pre><code class="language-Python"># Smart RAG, Self-Improving RAG
import os
from tavily import TavilyClient

def smart_rag(question, context):
    if not is_in(question, context):
        print("Searching in tavily")
        tavily = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])
        context = tavily.search(query=question)

    chain = prompt_template | llm | StrOutputParser()
    return chain.invoke({"context": context, "question": question})
</code></pre> <pre><code class="language-Python">smart_rag("What is DUS?", solar_summary)
# 질문에 대한 답변이 solar_summary에 있는 내용이므로 RAG  
# yes  
# 'The answer to the question "What is DUS?" is:\n\nDepth Up-Scaling (DUS)'
</code></pre> <pre><code class="language-Python">smart_rag("How to get to Seoul from SF?", solar_summary)
# solar_summary에 없는 내용이므로 Search  
# no  
# Searching in tavily
# 'The answer to "How to get to Seoul from SF?" is:\n\n1. Fly from San Francisco (SFO) to Seoul (ICN) with airlines such as ANA, Japan Airlines, Asiana Airlines, Korean Air, and United Airlines.\n2. Take a train from Incheon Int\'l Airport T1 to Seoul Station.\n3. Take the BART from Civic Center / UN Plaza to Milpitas and then fly from San Jose (SJC) to Incheon (ICN).\n\nPlease note that the cheapest flights from San Francisco to Seoul start at $453 with AIR PREMIA.'
</code></pre> <h3 id="smart-rag-with-tools">Smart RAG with Tools</h3> <ol> <li>Define <code class="language-plaintext highlighter-rouge">Custom Tools</code> </li> <li>Create a list of tools</li> <li>Bind the tools to LLM</li> </ol> <p>특정 task (산수 계산 혹은 뉴스기사 검색 등) 맞춤형으로<br> custom tools를 정의함으로써<br> LLM 답변의 질을 높일 수 있음!</p> <pre><code class="language-Python">! pip3 install -qU  markdownify  langchain-upstage rank_bm25

from langchain_core.tools import tool
import requests
import os
from tavily import TavilyClient

# external API to search
tavily = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])

# 1. Define Custom Tools
@tool
def add(a: int, b: int) -&gt; int:
    """Adds a and b."""
    return a + b

@tool
def solar_paper_search(query: str) -&gt; str:
    """Query for research paper about solarllm, dus, llm and general AI.
    If the query is about DUS, Upstage, AI related topics, use this.
    """
    return solar_summary

@tool
def internet_search(query: str) -&gt; str:
    """This is for query for internet search engine like Google.
    Query for general topics.
    """
    return tavily.search(query=query)

@tool
def get_news(topic: str) -&gt; str:
    """Get latest news about a topic.
    If users are more like recent news, use this.
    """
    # https://newsapi.org/v2/everything?q=tesla&amp;from=2024-04-01&amp;sortBy=publishedAt&amp;apiKey=API_KEY
    # change this to request news from a real API
    news_url = f"https://newsapi.org/v2/everything?q={topic}&amp;apiKey={os.environ['NEWS_API_KEY']}"
    respnse = requests.get(news_url)
    return respnse.json()

# 2. Create a list of tools 
tools = [add, solar_paper_search, internet_search, get_news]

# 3. Bind the tools to LLM
llm_with_tools = llm.bind_tools(tools)

llm_with_tools.invoke("What is Solar LLM?").tool_calls
# 출력 : [{'name': 'solar_paper_search', 'args': {'query': 'Solar LLM'}, 'id': 'cb1687d2-7c6a-45dc-8287-19376c335cd4'}]
llm_with_tools.invoke("What's best place in Seoul?").tool_calls
# 출력 : [{'name': 'internet_search', 'args': {'query': 'best place in Seoul'}, 'id': '1f86d563-de15-460a-abc0-0e644e284518'}]
</code></pre> <pre><code class="language-Python"># Smart RAG, Self-Improving RAG
import os
from tavily import TavilyClient

def call_tool_func(tool_call):
    tool_name = tool_call["name"].lower()
    if tool_name not in globals():
        print("Tool not found", tool_name)
        return None
    selected_tool = globals()[tool_name]
    return selected_tool.invoke(tool_call["args"])

def tool_rag(question):
    for _ in range(3): # try 3 times
        tool_calls = llm_with_tools.invoke(question).tool_calls
        if tool_calls:
            break
        else:
            print("try again")

    if not tool_calls:
        return "I'm sorry, I don't have an answer for that."
    
    print(tool_calls)
    context = ""
    for tool_call in tool_calls:
        context += str(call_tool_func(tool_call))

    chain = prompt_template | llm | StrOutputParser()
    return chain.invoke({"context": context, "question": question})

tool_rag("What is Solar llm?")
# 출력 : [{'name': 'solar_paper_search', 'args': {'query': 'What is Solar llm?'}, 'id': 'cb291b01-a1aa-4839-84a8-a473f4eb0920'}] 'Solar llm is a large language model (LLM) with 10.7 billion parameters.'
tool_rag("What is news about Tesla?")
# 출력 : [{'name': 'get_news', 'args': {'topic': 'Tesla'}, 'id': 'aade5002-b9e2-4a23-92d7-fd66f12cfeb6'}] "The news about Tesla is that the company has issued a voluntary recall for nearly 4,000 Cybertrucks due to a fault with the accelerator pedal that could get trapped, pushing the car to full speed."
</code></pre> <h3 id="fine-tuning-with-predibase">Fine-tuning with Predibase</h3> <p>CFT (Continued Fine-Tuning) : feedback database에 기반하여 계속 fine-tuning</p> <pre><code class="language-Python">adapter = pb.adapters.create(
  config=FinetuningConfig(
    base_model = "solar-1-mini-chat-240612",
    epochs = 1, # default: 3
    rank = 1, # default: 16
  ),
  dataset = pb_dataset, # also accepts the dataset name as str
  repo = repo,
  description = "initial model with defaults"
)
</code></pre> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="disqus_thread" style="max-width: 800px; margin: 0 auto;"> <script type="text/javascript">var disqus_shortname="semyeong-yu",disqus_identifier="/blog/2024/LLM",disqus_title="Solar LLM with Langchain";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Semyeong Yu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5YMLX9VHFX"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5YMLX9VHFX");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>