<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Pytorch Basic Code (DDP) | Semyeong Yu </title> <meta name="author" content="Semyeong Yu"> <meta name="description" content="Dataset, DataLoader, Train, Attention, ..."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://semyeong-yu.github.io/blog/2024/PytorchBasic/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Pytorch Basic Code (DDP)",
            "description": "Dataset, DataLoader, Train, Attention, ...",
            "published": "July 08, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Semyeong</span> Yu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Pytorch Basic Code (DDP)</h1> <p>Dataset, DataLoader, Train, Attention, ...</p> </d-title> <d-article> <h2 id="pytorch-basic-code-distributeddataparallel-ver">Pytorch Basic Code (DistributedDataParallel ver.)</h2> <h3 id="deal-with-json-image-csv">Deal with json, image, csv</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/3-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/3-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-07-08-PytorchBasic/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <pre><code class="language-Python">import os
import json
import pandas as pd
import csv
from PIL import Image
import cv2

# read json
if os.path.exists(json_path):
    with open(json_path, "r") as f:
        data = json.load(f)
    f.close()

# read image 방법 1.
img = Image.open(image_path).convert('RGB') # PIL image object in range [0, 255]
img.show()

# read image 방법 2.
img = cv2.imread(image_path) # np.ndarray of shape (H, W, C) in range [0, 255] in BGR mode
cv2.imshow('Image', img)
cv2.waitKey(0)
cv2.destroyAllWindows()

# read csv 방법 1. general case
data = pd.read_csv(csv_path, sep="|", index_col=0, skiprows=[1], na_values=['?', 'nan']).values # 0-th column (1-th row는 제외) ('?'와 'nan'은 결측값으로 인식)

# read csv 방법 2. special case : csv가 row별로 dictionary 형태일 때
if os.path.exists(csv_path):
    with open(csv_path, "r") as f:
        reader = csv.DictReader(f, delimiter=",")
        data = [{key : value for key, value in row.items()} for row in reader] # row별로 읽음

# write to csv
dataset = [] # list of dictionaries
dataset.append({"id":id, "w":w, "h":h, "class":i})
pd.DataFrame(dataset).to_csv(output_path, index=False) # output_path : ".../dataset.csv"
</code></pre> <h3 id="convert-to-tensor">Convert to Tensor</h3> <p>data.py의 CustomDataset(torch.utils.data.Dataset)에서 image는 <code class="language-plaintext highlighter-rouge">shape (C, H, W) tensor</code>여야 하기 때문에<br> PIL.Image.open() 또는 cv2.imread()로 얻은<br> PIL image object 또는 np.ndarray를 적절한 shape 및 range의 tensor로 변환해주어야 한다</p> <ul> <li>PIL image object \(\rightarrow\) Tensor <ul> <li>torchvision.transforms.ToTensor()</li> <li>np.array(), torch.tensor()</li> <li>getdata(), torch.tensor()</li> </ul> </li> <li>np.ndarray \(\rightarrow\) Tensor <ul> <li>torch.tensor()</li> </ul> </li> </ul> <pre><code class="language-Python">PIL_img = Image.open(image_path).convert('RGB') # PIL image object of size (W, H) in range [0, 255]

# 방법 1. torchvision.transforms.ToTensor()
transform = torchvision.transforms.Compose([
    torchvision.transforms.Resize((256, 256)),
    torchvision.transforms.ToTensor() # convert to tensor in range [0., 1.]
])
img = transform(PIL_img) # tensor of shape (C, H, W) in range [0., 1.]

# 방법 2. np.array(), torch.tensor()
img = np.array(PIL_img) # np.ndarray of shape (H, W, C) in range [0., 255.]
img = torch.tensor(img.transpose((2, 0, 1)).astype(float)).mul_(1.0) / 255.0 # tensor of shape (C, H, W) in range [0., 1.]

# 방법 3. getdata(), torch.tensor()
img_data = PIL_img.getdata()
img = torch.tensor(img_data, dtype=torch.float32) # tensor of shape (H*W*C,) in range [0, 255]
img = img.view(PIL_img.size[1], PIL_img.size[0], 3).permute(2, 0, 1) / 255.0 # tensor of shape (C, H, W) in range [0., 1.]
</code></pre> <pre><code class="language-Python">img = cv2.imread(image_path) # np.ndarray of shape (H, W, C) in range [0., 255.]

img = torch.tensor(img.transpose((2, 0, 1)).astype(float)).mul_(1.0) / 255.0 # tensor of shape (C, H, W) in range [0., 1.]
</code></pre> <h3 id="create-dataset">Create Dataset</h3> <ul> <li> <code class="language-plaintext highlighter-rouge">data augmentation</code> : <ul> <li>Resize</li> <li>ToTensor</li> <li>RandomHorizontalFlip</li> <li>RandomVerticalFlip</li> <li>Normalize</li> <li>RandomRotation</li> <li>RandomAffine <ul> <li>shear</li> <li>scale (zoom-in/out)</li> </ul> </li> <li>RandomResizedCrop</li> <li>ColorJitter</li> <li>GaussianBlur</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/4-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/4-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-07-08-PytorchBasic/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <pre><code class="language-Python">import os
import torch
from torch.utils.data import Dataset
import cv2
import numpy as np
import glob
import random

# Create Dataset
class CustomDataset(Dataset):
    def __init__(self, args, mode):
        # load할 data가 너무 크다면 __init__()에서는 load할 파일명만 저장해놓고 __getitem__()에서 필요할 때마다 load
        self.args = args
        self.mode = mode
        
        if mode == 'train':
            self.data_path = os.path.join(args.data_path, 'train_blur')
        elif mode == 'val':
            self.data_path = os.path.join(args.data_path, 'val_blur')
        elif mode == 'test':
            self.data_path = os.path.join(args.data_path, 'test_blur')
        
        # a list of data/train_blur/*.png
        self.blur_path_list = sorted(glob.glob(os.path.join(self.data_path, '*.png')))
        
        # a list of data/train_sharp/*.png
        self.sharp_path_list = [os.path.normpath(path.replace('blur', 'sharp') for path in self.blur_path_list)]

    def __getitem__(self, idx):
        # should return float tensor!!
        blur_path = self.blur_path_list[idx]
        # np.ndarray of shape (H, W, C) in range [0, 255]
        blur_img = cv2.imread(blur_path) 

        if self.mode == 'train':
            sharp_path = self.sharp_path_list[idx]
            sharp_img = cv2.imread(sharp_path)
            
            # np.ndarray of shape (pat, pat, C) where pat is patch_size
            blur_img, sharp_img = self.augment(self.get_random_patch(blur_img, sharp_img)) 
            
            # tensor of shape (C, pat, pat) in range [0, 1]
            return self.np2tensor(blur_img), self.np2tensor(sharp_img) 
        
        elif self.mode == 'val':
            sharp_path = self.sharp_path_list[idx]
            sharp_img = cv2.imread(sharp_path)
            return self.np2tensor(blur_img), self.np2tensor(sharp_img)
        
        elif self.mode == 'test':
            return self.np2tensor(blur_img), blur_path

    def np2tensor(self, x):
        # input : shape (H, W, C) / range [0, 255]
        # output : shape (C, H, W) / range [0, 1]
        ts = (2, 0, 1)
        x = torch.tensor(x.transpose(ts).astype(float)).mul_(1.0) # _ : in-place
        x = x / 255.0 # normalize
        return x

    def get_random_patch(self, blur_img, sharp_img):
        H, W, C = blur_img.shape # shape (H, W, C)

        pat = self.args.patch_size # pat : patch size
        iw = random.randrange(0, W - pat + 1) # iw : range [0, W - pat]
        ih = random.randrange(0, H - pat + 1) # ih : range [0, H - pat]

        blur_img = blur_img[ih:ih + pat, iw:iw + pat, :] # shape (pat, pat, C)
        sharp_img = sharp_img[ih:ih + pat, iw:iw + pat, :]

        return blur_img, sharp_img # shape (pat, pat, C)

    def augment(self, blur_img, sharp_img):
        # random horizontal flip
        if random.random() &lt; 0.5:
            blur_img = blur_img[:, ::-1, :] # Width-axis를 flip
            sharp_img = sharp_img[:, ::-1, :]
            '''
            flow-mask pair의 경우 C-dim.이 3 = 2(optical flow x, y) + 1(occlusion mask) 이므로
            shape (T, H, W, 3)의 flow-mask pair를 horizontal flip을 하려면
            flow = flow[:, :, ::-1, :]
            flow[:, :, :, 0] *= -1
            '''
            
        # random vertical flip
        if random.random() &lt; 0.5:
            blur_img = blur_img[::-1, :, :] # Height-axis를 flip
            sharp_img = sharp_img[::-1, :, :]
            '''
            flow = flow[:, ::-1, :, :]
            flow[:, :, :, 1] *= -1
            '''

        return blur_img, sharp_img

    def __len__(self):
        return len(self.path_list)
</code></pre> <h3 id="dataloader">DataLoader</h3> <ul> <li>DataParallel(DP) vs DistributedDataParallel(DDP) :<br> <a href="https://tkayyoo.tistory.com/27#tktag2" rel="external nofollow noopener" target="_blank">Pytorch DP and DDP</a> <ul> <li>DataParallel(DP) :<br> single-process<br> multi-thread<br> single-machine</li> <li>DistributedDataParallel(DDP) :<br> multi-process<br> single-machine과 multi-machine 모두 가능</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">rank</code> : <ul> <li>전체 distributed system에서 process 순서</li> <li>4-CPU system이 2개 있을 경우<br> rank = machine 번호(0~1) * machine 당 process 개수(4) + process 번호(0~3)</li> <li>rank = 0인 process에 대해서만 wandb로 train log 출력</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">world_size</code> : <ul> <li>전체 distributed system에서 총 process 개수</li> <li>4-GPU system이 2개 있을 경우<br> world_size = machine 개수(2) * machine 당 process 개수(4)</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">torch.distributed.init_process_group()</code> : <ul> <li>분산 학습 환경 초기화 : 분산 학습하는 각 process 간의 통신을 설정</li> <li>backend : ‘gloo’ for CPU, ‘nccl’ for GPU, ‘mpi’ for 고성능</li> <li>init_method : 각 process가 서로 탐색하는 방법(url)<br> 예시 : ‘env://’, f’tcp://127.0.0.1:11203’</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">torch.utils.data.distributed.DistributedSampler()</code> : <ul> <li>world_size(총 process 개수)만큼 dataset을 분할하여 모든 process가 동일한 양의 dataset을 갖도록 함</li> <li>DistributedSampler는 각 epoch마다 dataset을 무작위로 분할</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">torch.utils.data.DataLoader()</code> : <ul> <li>shuffle=False :<br> 보통 training일 때는 일반화를 위해 shuffle=True로 두지만<br> 분산 학습을 할 때는 같은 epoch 내에서<br> 각 process가 서로 다른 dataset을 처리하기 위해 (중복 방지)<br> shuffle=False로 설정</li> <li>num_workers : cpu data load할 때 multi-processing core 개수</li> <li>pin_memory=True :<br> data load한 장치(CPU)에서 GPU로 data를 옮길 때<br> host memory가 아닌 CPU의 page-locked memory로 할당하고<br> GPU는 이를 참조하여 복사하므로 전송 시간을 단축<br> pin_memory=True와 non_blocking=True는 함께 사용</li> <li>drop_last=True : 나눠떨어지지 않는 마지막 batch를 버림</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">_collate_fn(input)</code> : <ul> <li>DataLoader()에서 1개의 batch로 묶을 때 사용하는 custom 전처리 함수</li> <li>input : <ul> <li>1개의 batch에 해당하는 입력</li> <li>Dataset(torch.utils.data.Dataset)의 <strong>getitem</strong>(self, idx)이 return img, target 형태일 때<br> [(img1, target1), (img2, target2), …]의 형태</li> </ul> </li> <li>output : <ul> <li>for iter, (x, y) in enumerate(dataloader): 의 x, y</li> </ul> </li> <li>예 : 길이가 다른 input들을 batch로 묶기 위해 padding, tokenization<br> img의 경우 CustomDataset()에서 augmentation으로 shape (C, H, W)로 통일해줬다면 (N, C, H, W)로 묶을 수 있지만,<br> object detection task에서 target의 경우 n_box가 image마다 다르므로 N batch로 묶기 위해 padding해주어야 함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/5-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/5-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-07-08-PytorchBasic/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <pre><code class="language-Python">from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
import torch.distributed as dist
from datetime import timedelta

def _collate_fn(samples):
    # ...

# main_worker : 각 process가 실행하는 함수
def main_worker(process_id, args):

    rank = args.machine_id * args.num_processes + process_id
    
    world_size = args.num_machines * args.num_processes
    
    dist.init_process_group(backend='nccl', init_method='env://', world_size=world_size, rank=rank, timeout=timedelta(300))
    
    ###################################################################
    
    # for epoch in range ... 밖에서
    train_dataset = CustomDataset(args, 'train')
    '''
    train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transforms.Compose(
        [
        transforms.ToTensor(), 
        transforms.Normalize((0.1302,), (0.3069,))
        ]))  
    '''

    # machine 당 process 수로 나눔
    batch_size = int(args.batch_size / args.num_processes) 
    num_workers = int(args.num_workers / args.num_processes)

    # for epoch in range ... 안에서
    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)

    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=_collate_fn, pin_memory=True, drop_last=True, sampler=train_sampler)
</code></pre> <h3 id="train">Train</h3> <ul> <li> <code class="language-plaintext highlighter-rouge">torch.multiprocessing.spawn()</code> : <ul> <li>main_worker : 각 process가 실행하는 함수</li> <li>nprocs : machine 당 process 개수인 4로 설정<br> main_worker()의 첫 번째 argument는 process_id인 0~3이 됨</li> <li>args : main_worker()에 추가로 전달할 tuple 형태의 argument</li> <li>join</li> <li>daemon</li> <li>start_method</li> </ul> </li> </ul> <ol> <li> <code class="language-plaintext highlighter-rouge">model</code> initialize, and set cuda, and parallelize <ul> <li>nn.parallel.DistributedDataParallel :<br> 각 model 복사본은 각자의 optimizer를 이용해 gradient를 구하고<br> rank=0의 process와 통신하여 gradient의 평균을 구해서 backpropagation 진행<br> GIL(global interpreter lock)의 제약을 해결</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">wandb</code> init <ul> <li>wandb.init() : wandb 초기화<br> vars(args)는 args 객체의 <strong>dict</strong> 속성을 반환<br> {‘transforms’ : ‘BaseTransform’, ‘crop_size’ : 224}과 같이 반환</li> <li>wandb.watch() : wandb 기록<br> 모든 param.의 gradient를 기록<br> arg.log_interval-번째 batch마다 log 기록</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">optimizer, scheduler</code> initialize</li> <li>load <code class="language-plaintext highlighter-rouge">checkpoint</code> </li> <li> <code class="language-plaintext highlighter-rouge">train</code> with <code class="language-plaintext highlighter-rouge">barrier</code> <ul> <li>torch.distributed.barrier() :<br> 분산 학습 환경에서<br> 모든 process가 이 장벽에 도달할 때까지 대기하여<br> 모든 process가 synchronize된 상태에서 훈련이 진행되도록 함</li> <li>torch.cuda.empty_cache() :<br> 더 이상 사용하지 않는 tensor들을 GPU cached memory에서 해제<br> 장점 : GPU memory 확보<br> 단점 : 너무 자주 호출하면 메모리 할당/해제에 따른 성능 저하 발생</li> <li>train :<br> args.accumulation_steps만큼 loss를 누적한 뒤 backward<br> args.accumulation_steps마다 gradient 및 measurement 초기화, rank=0 logging</li> <li>validation :<br> with torch.no_grad(): 로 gradient 누적 안 함!</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">wandb</code> and <code class="language-plaintext highlighter-rouge">distributed</code> finish</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/6-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/6-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-07-08-PytorchBasic/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <pre><code class="language-Python">from importlib import import_module
import torch
import torch.nn as nn
import torch.multiprocessing as mp
from utils import *
from tqdm import tqdm
import wandb

def main():
    args = arg_parse()
    fix_seed(args.random_seed)
    
    # rank=0인 process를 실행하는 system의 IP 주소
    # rank=0인 system이 모든 backend 통신을 설정!
    os.environ['MASTER_ADDR'] = '127.0.0.1'
    # 해당 system에서 사용 가능한 PORT           
    os.environ['MASTER_PORT'] = '8892'

    mp.spawn(main_worker, nprocs=args.num_processes, args=(args,))
    # DDP가 아니라면, main.py에 def main_worker()의 내용을 넣고, train.py에 class Runner 만들자
    '''
    class Runner:
        def __init__(self, args, model):
            self.args = args
            self.model = model
            pass
        def train(self, dataloader, epoch):
            pass
        def validate(self, dataloader, epoch):
            pass
        def test(self, dataloader):
            pass
    '''

def main_worker(process_id, args):
    global best_acc
    best_acc = 0.0

    # 1. model initialize, and set cuda, and parallelize
    model = MyFMANet()

    torch.cuda.set_device(process_id)
    model.cuda(process_id)
    criterion = nn.NLLLoss().cuda(process_id) # criterion = nn.CrossEntropyLoss(reduction='mean').cuda(process_id)

    model = nn.parallel.DistributedDataParallel(model, device_ids=[process_id])

    # 2. wandb init
    if rank == 0:
        wandb.init(project=args.prj_name, name=f"{args.exp_name}", entity="semyeongyu", config=vars(args))
        wandb.watch(model, log='all', log_freq=args.log_interval)

    # 3. optimizer, scheduler initialize
    optimizer = getattr(import_module("torch.optim"), args.optimizer)(model.parameters(), lr=args.lr, betas=(args.b1, args.b2), weight_decay=args.weight_decay)
    scheduler = getattr(import_module("torch.optim.lr_scheduler"), args.lr_scheduler)(optimizer, T_max=args.period, eta_min=0, last_epoch=-1, verbose=True)
    # T_max : 주기 1번 도는 데 걸리는 최대 iter. 수 / eta_min : lr의 최솟값 / last_epoch : 학습 시작할 때의 epoch

    # 4. load checkpoint
    if args.resume_from:
        start_epoch, model, optimizer, scheduler = load_checkpoint(args.checkpoint_path, model, optimizer, scheduler, rank)
    else:
        start_epoch = 0

    # 5. train with barrier
    dist.barrier()

    for epoch in range(start_epoch, args.n_epochs):
        train_sampler.set_epoch(epoch) # train_sampler가 epoch끼리 동일하게 data 분할하는 것을 방지하기 위해

        optimizer.zero_grad() # epoch마다 gradient 초기화

        train_loss = train(train_loader, model, criterion, optimizer, scheduler, epoch, args)

        dist.barrier()

        if rank == 0:
            val_acc, val_loss = validate(val_loader, model, criterion, epoch, args)
            
            # best acc일 때마다 save checkpoint
            if (best_top1 &lt; val_acc):
                best_top1 = val_acc # best_top1은 global var.
                save_checkpoint(
                    {
                        'epoch': epoch,
                        'model': model.state_dict(),
                        'best_top1': best_top1,
                        'optimizer': optimizer.state_dict(),
                        'scheduler': scheduler.state_dict()
                    }, os.path.join(args.checkpoint_dir, args.exp_name), f"{epoch}_{round(best_top1, 4)}.pt"
                )
        
        torch.cuda.empty_cache() 
    
    # 6. wandb and distributed finish
    if rank == 0:
        wandb.run.finish()

    dist.destroy_process_group()
</code></pre> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/7-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/7-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/7-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-07-08-PytorchBasic/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <pre><code class="language-Python">def train(train_loader, model, criterion, optimizer, scheduler, epoch, args):
    model.train()
    train_acc, train_loss = AverageMeter(), AverageMeter() 
    # measurement of acc and loss

    pbar = tqdm(enumerate(train_loader), total=len(train_loader))
    for step, (x, y_gt) in pbar:
        x, y_gt = x.cuda(non_blocking=True), y_gt.cuda(non_blocking=True) 
        # cuda device에 올려야 함
        # pin_memory=True와 non_blocking=True는 함께 사용

        # forward
        y_pred = model(x)

        # loss divided by accumulation_steps
        loss = criterion(y_pred, y_gt) / args.accumulation_steps

        # gradient 누적
        loss.backward()
        
        # measurement
        train_acc.update(
            topk_accuracy(y_pred.clone().detach(), y_gt).item(), x.size(0))
        train_loss.update(loss.item() * args.accumulation_steps, x.size(0))

        # args.accumulation_steps만큼 loss를 누적한 뒤 평균값으로 backward
        if (step+1) % args.accumulation_steps == 0:
            # gradient clipping
            nn.utils.clip_grad_norm_(model.parameters(), max_norm=args.max_norm)

            # backward
            optimizer.step()
            scheduler.step()

            # gradient 초기화
            optimizer.zero_grad()

            # logging
            dist.barrier()
            if rank == 0:
                # wandb log
                wandb.log(
                    {
                        "Training Loss": round(train_loss.avg, 4),
                        "Training Accuracy": round(train_acc.avg, 4),
                        "Learning Rate": optimizer.param_groups[0]['lr']
                    }
                )

                # tqdm log
                description = f'Epoch: {epoch+1}/{args.n_epochs} || Step: {(step+1)//args.accumulation_steps}/{len(train_loader)//args.accumulation_steps} || Training Loss: {round(train_loss.avg, 4)}'
                pbar.set_description(description)

                # measurement 초기화
                train_loss.init()
                train_acc.init()
    
    return train_loss.avg
</code></pre> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/8-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/8-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/8-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-07-08-PytorchBasic/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <pre><code class="language-Python">def validate(val_loader, model, criterion, epoch, args):
    model.eval()
    val_acc, val_loss = AverageMeter(), AverageMeter() # measurement of acc and loss

    pbar = tqdm(enumerate(val_loader), total=len(val_loader))
    with torch.no_grad(): # validation은 gradient 누적 안 함!!
        for step, (x, y_gt) in pbar:
            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)
            
            # forward
            y_pred = model(x)
            loss = criterion(y_pred, y_gt)
            
            # measurement
            val_acc.update(topk_accuracy(y_pred.clone().detach(), y_gt).item(), x.size(0)) 
            val_loss.update(loss.item(), x.size(0))

            # tqdm log
            description = f'Epoch: {epoch+1}/{args.n_epochs} || Step: {step+1}/{len(val_loader)} || Validation Loss: {round(loss.item(), 4)} || Validation Accuracy: {round(val_acc.avg, 4)}'
            pbar.set_description(description)

    # wandb log
    wandb.log(
        {
            'Validation Loss': round(val_loss.avg, 4),
            'Validation Accuracy': round(val_acc.avg, 4)
        }
    )

    return val_acc.avg, val_loss.avg
</code></pre> <h3 id="utils">Utils</h3> <ul> <li><code class="language-plaintext highlighter-rouge">argument parser</code></li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/9-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/9-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/9-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-07-08-PytorchBasic/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <pre><code class="language-Python">import argparse

def arg_parse():
    parser = argparse.ArgumentParser()

    parser.add_argument("--transforms", type=str, default="BaseTransform")
    parser.add_argument("--crop_size", type=int, default=224)

    args = parser.parse_args()

    return args
</code></pre> <ul> <li><code class="language-plaintext highlighter-rouge">seed</code></li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/10-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/10-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/10-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-07-08-PytorchBasic/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <pre><code class="language-Python">import random
import torch
import numpy as np

def fix_seed(random_seed):
    torch.manual_seed(random_seed)
    torch.cuda.manual_seed(random_seed)
    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(random_seed)
    random.seed(random_seed)
</code></pre> <ul> <li> <code class="language-plaintext highlighter-rouge">checkpoint</code> : dictionary of elements below <ul> <li>epoch</li> <li>model.state_dict()</li> <li>best_acc</li> <li>optimizer.state_dict()</li> <li>scheduler.state_dict()</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/11-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/11-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/11-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-07-08-PytorchBasic/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <pre><code class="language-Python">def save_checkpoint(checkpoint, saved_dir, file_name):
    os.makedirs(saved_dir, exist_ok=True)
    output_path = os.path.join(saved_dir, file_name)
    torch.save(checkpoint, output_path) 
    # checkpoint : dictionary

def load_checkpoint(checkpoint_path, model, optimizer, scheduler, rank=-1):
    # checkpoint_path : ".../240325.pt"
    if rank != -1: # 분산학습 yes
        map_location = {"cuda:%d" % 0 : "cuda:%d" % rank}
        checkpoint = torch.load(checkpoint_path, map_location=map_location)
    else: # 분산학습 no
        checkpoint = torch.load(checkpoint_path)

    start_epoch = checkpoint['epoch']

    model.load_state_dict(checkpoint['model'])
    
    if optimizer is not None:
        optimizer.load_state_dict(checkpoint['optimizer'])
    
    if scheduler is not None:
        scheduler.load_state_dict(checkpoint['scheduler'])

    return start_epoch, model, optimizer, scheduler
</code></pre> <ul> <li><code class="language-plaintext highlighter-rouge">augmentation</code></li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/12-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/12-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/12-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-07-08-PytorchBasic/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <pre><code class="language-Python">import albumentations as A
from albumentations.pytorch.transforms import ToTensorV2

class BaseTransform(object):
    def __init__(self, crop_size = 224):
        self.transform = A.Compose(
            [   
                A.RandomResizedCrop(crop_size, crop_size),
                A.HorizontalFlip(),
                A.Normalize(),
                ToTensorV2() 
# albumentations에서는 normalize 이후에 ToTensorV2를 사용해줘야 함 (여기서 어차피 shape (C,H,W)로 변경)
            ]
        )

    def __call__(self, img):
# BaseTransform()은 nn.Module을 상속한 게 아니므로 forward를 구현해도 __call__과 연결되어 있지 않음
# 따라서 __call__()을 직접 구현해줘야 함
        return self.transform(image=img)
</code></pre> <ul> <li><code class="language-plaintext highlighter-rouge">measurement</code></li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/13-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/13-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/13-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-07-08-PytorchBasic/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <pre><code class="language-Python">class AverageMeter(object):
    def __init__(self):
        self.init()
    
    def init(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val*n
        self.count += n
        self.avg = self.sum / self.count

def topk_accuracy(pred, gt, k=1):
    # pred : shape (N, class)
    # gt : shape (N,)
    _, pred_topk = pred.topk(k, dim=1)
    n_correct = torch.sum(pred_topk.squeeze() == gt)

    return n_correct / len(gt)
</code></pre> <h3 id="multi-attention">Multi-Attention</h3> <ul> <li>FMA-Net (2024) <d-cite key="FMANet">[1]</d-cite>의 Multi-Attention 구현<br> 출처 : <a href="https://github.com/KAIST-VICLab/FMA-Net" rel="external nofollow noopener" target="_blank">FMA-Net Code</a> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/2-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/2-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-07-08-PytorchBasic/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <blockquote> <p>Multi-Attention :</p> <ul> <li> <code class="language-plaintext highlighter-rouge">CO(center-oriented)</code> attention :<br> better align \(\tilde F_{w}^{i}\) to \(F_{c}^{0}\) (center feature map of initial temporally-anchored feature)</li> <li> <code class="language-plaintext highlighter-rouge">DA(degradation-aware)</code> attention :<br> \(\tilde F_{w}^{i}\) becomes globally adaptive to spatio-temporally variant degradation by using degradation kernels \(K^{D}\)</li> </ul> </blockquote> <ul> <li> <p>CO attention :<br> \(Q=W_{q} F_{c}^{0}\)<br> \(K=W_{k} \tilde F_{w}^{i}\)<br> \(V=W_{v} \tilde F_{w}^{i}\)<br> \(COAttn(Q, K, V) = softmax(\frac{QK^{T}}{\sqrt{d}})V\)<br> 실험 결과, \(\tilde F_{w}^{i}\)가 자기 자신(self-attention)이 아니라 \(F_{c}^{0}\)과의 relation에 집중할 때 better performance</p> </li> <li> <p>DA attention :<br> CO attention과 비슷하지만,<br> Query 만들 때 \(F_{c}^{0}\) 대신 \(k^{D, i}\) 사용<br> \(\tilde F_{w}^{i}\) becomes globally adaptive to spatio-temporally-variant degradation<br> \(k^{D, i} \in R^{H \times W \times C}\) : degradation features adjusted by conv. with \(K^{D}\) (motion-aware spatio-temporally-variant degradation kernels) 에 대해<br> \(Q=W_{q} k^{D, i}\)<br> DA attention은 \(Net^{D}\) 말고 \(Net^{R}\) 에서만 사용</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">nn.Conv2d()</code> :</p> <ul> <li> \[H_{out} = \lfloor 1 + \frac{H_{in} + 2 \times pad - dilation \times (K-1) - 1}{stride} \rfloor\] </li> <li>argument : <ul> <li>groups :<br> shape (\(C_{in}\), \(C_{out}\), K, K) 대신 \(C_{in}\), \(C_{out}\) 을 groups-개로 쪼개서<br> shape (\(\frac{C_{in}}{groups}\), \(\frac{C_{out}}{groups}\), K, K)를 groups-번 실행하여 concat</li> </ul> </li> <li>variable : <ul> <li>weight : shape (\(C_{out}\), \(\frac{C_{in}}{groups}\), K, K)</li> <li>bias : shape (\(C_{out}\),)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-07-08-PytorchBasic/14-480.webp 480w,/assets/img/2024-07-08-PytorchBasic/14-800.webp 800w,/assets/img/2024-07-08-PytorchBasic/14-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-07-08-PytorchBasic/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <pre><code class="language-Python">import torch
import torch.nn as nn

class Attention(nn.Module):
    # Restormer (CVPR 2022) transposed-attention block
    # original source code: https://github.com/swz30/Restormer
    def __init__(self, dim, n_head, bias):
        super(Attention, self).__init__()
        self.n_head = n_head # multi-head for channel dim.
        self.temperature = nn.Parameter(torch.ones(n_head, 1, 1)) 
        # multi-head 별로 scale factor를 parameterize

        # W_q
        self.q = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)
        self.q_dwconv = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, groups=dim, bias=bias)

        # W_kv
        self.kv_conv = nn.Conv2d(dim, dim*2, kernel_size=1, bias=bias)
        self.kv_dwconv = nn.Conv2d(dim*2, dim*2, kernel_size=3, stride=1, padding=1, groups=dim*2, bias=bias)

        # W_o
        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)

    def forward(self, x, f):
        # first input x : shape (N, C, H, W) -&gt; makes key and value
        # second input f : shape (N, C, H, W) -&gt; makes query
        N, C, H, W = x.shape

        # Apply W_q and W_kv
        q = self.q_dwconv(self.q(f)) # query q : shape (N, C, H, W)
        kv = self.kv_dwconv(self.kv_conv(x)) # kv : shape (N, 2*C, H, W)
        k, v = kv.chunk(2, dim=1) # key k and value v : shape (N, C, H, W)

        # Multi-Head Attention
        q = einops.rearrange(q, 'b (head c) h w -&gt; b head c (h w)', head=self.n_head)
        # query q : shape (N, C, H, W) -&gt; shape (N, M, C/M, H * W)
        k = einops.rearrange(k, 'b (head c) h w -&gt; b head c (h w)', head=self.n_head)
        # key k : shape (N, C, H, W) -&gt; shape (N, M, C/M, H * W)
        v = einops.rearrange(v, 'b (head c) h w -&gt; b head c (h w)', head=self.n_head)
        # value v : shape (N, C, H, W) -&gt; shape (N, M, C/M, H * W)

        # matrix mul.을 할 spatial dim.을 normalize
        q = torch.nn.functional.normalize(q, dim=-1)
        k = torch.nn.functional.normalize(k, dim=-1)

        '''
        - q @ k.transpose(-2, -1) = similarity :
          shape (N, M, C/M, C/M)
        - self.temperature = scale factor for each head :
          shape (M, 1, 1) -&gt; shape (N, M, C/M, C/M) 
        '''
        attn = (q @ k.transpose(-2, -1)) * self.temperature 
        attn = attn.softmax(dim=-1) # convert to probability distribution

        out = (attn @ v) # shape (N, M, C/M, H*W)
        
        # Multi-Head Attention - concatenation
        out = einops.rearrange(out, 'b head c (h w) -&gt; b (head c) h w', head=self.n_head, h=H, w=W) 
        # shape (N, C, H, W)

        # Apply W_o
        out = self.project_out(out) # shape (N, C, H, W)

        return out

class LayerNorm(nn.Module):
    def __init__(self, normalized_shape):
        super(LayerNorm, self).__init__()
        
        # learnable param.
        self.weight = nn.Parameter(torch.ones(normalized_shape)) # shape (C,)
        self.bias = nn.Parameter(torch.zeros(normalized_shape)) # shape (C,)
    
    def forward(self, x):
        # x : shape (N, C, H, W)
        # LayerNorm : dim. C에 대해 normalize
        mu = x.mean(1, keepdim=True)
        sigma = x.var(1, keepdim=True, unbiased=False)
        return (x - mu) / torch.sqrt(sigma + 1e-5) * self.weight + self.bias

class MultiAttentionBlock(nn.Module):
    def __init__(self, dim, n_head, ffn_expansion_factor, bias, is_DA):
        super(MultiAttentionBlock, self).__init__()
        self.norm1 = LayerNorm(dim)
        # center-oriented attention
        self.co_attn = Attention(dim, n_head, bias) 
        self.norm2 = LayerNorm(dim)
        self.ffn1 = FeedForward(dim, bias)

        if is_DA:
            self.norm3 = LayerNorm(dim)
            # degradation-aware attention
            self.da_attn = Attention(dim, n_head, bias) 
            self.norm4 = LayerNorm(dim)
            self.ffn2 = FeedForward(dim, bias)

    def forward(self, Fw, F0_c, Kd):
        Fw = Fw + self.co_attn(self.norm1(Fw), F0_c)
        Fw = Fw + self.ffn1(self.norm2(Fw))

        if Kd is not None:
            Fw = Fw + self.da_attn(self.norm3(Fw), Kd)
            Fw = Fw + self.ffn2(self.norm4(Fw))

        return Fw
</code></pre> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-07-08-PytorchBasic.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"semyeong-yu/semyeong-yu.github.io","data-repo-id":"R_kgDOLmVJXQ","data-category":"Comments","data-category-id":"DIC_kwDOLmVJXc4CeSwc","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Semyeong Yu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5YMLX9VHFX"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5YMLX9VHFX");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>