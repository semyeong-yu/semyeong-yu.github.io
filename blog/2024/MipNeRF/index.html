<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> MipNeRF | Semyeong Yu </title> <meta name="author" content="Semyeong Yu"> <meta name="description" content="A Multiscale Representation for Anti-Aliasing Neural Radiance Fields"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://semyeong-yu.github.io/blog/2024/MipNeRF/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "MipNeRF",
            "description": "A Multiscale Representation for Anti-Aliasing Neural Radiance Fields",
            "published": "August 03, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Semyeong</span> Yu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>MipNeRF</h1> <p>A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#related-work">Related Work</a> </div> <ul> <li> <a href="#anti-aliasing-in-rendering">Anti-aliasing in Rendering</a> </li> <li> <a href="#scene-representations-for-view-synthesis">Scene Representations for View Synthesis</a> </li> </ul> <div> <a href="#method">Method</a> </div> <ul> <li> <a href="#cone-tracing-and-positional-encoding">Cone Tracing and Positional Encoding</a> </li> <li> <a href="#conical-frustum-integral-derivation">Conical Frustum Integral Derivation</a> </li> <li> <a href="#architecture">Architecture</a> </li> </ul> <div> <a href="#result">Result</a> </div> <div> <a href="#question">Question</a> </div> <div> <a href="#appendix">Appendix</a> </div> </nav> </d-contents> <h2 id="mip-nerf-a-multiscale-representation-for-anti-aliasing-neural-radiance-fields">Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</h2> <h4 id="jonathan-t-barron-ben-mildenhall-matthew-tancik-peter-hedman-ricardo-martin-brualla-pratul-p-srinivasan">Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan</h4> <blockquote> <p>paper :<br> <a href="https://arxiv.org/abs/2103.13415" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2103.13415</a><br> project website :<br> <a href="https://jonbarron.info/mipnerf/" rel="external nofollow noopener" target="_blank">https://jonbarron.info/mipnerf/</a><br> pytorch code :<br> <a href="https://github.com/bebeal/mipnerf-pytorch" rel="external nofollow noopener" target="_blank">https://github.com/bebeal/mipnerf-pytorch</a><br> <a href="https://github.com/google/mipnerf" rel="external nofollow noopener" target="_blank">https://github.com/google/mipnerf</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>ray-tracing하여 point-encoding 대신 cone-tracing하여 region-encoding 이므로 frustum의 모양과 크기 정보를 encode할 수 있어서 scale 반영 가능</li> <li>IPE 단계에서 <code class="language-plaintext highlighter-rouge">high variance (distant view)</code>일 때 <code class="language-plaintext highlighter-rouge">high freq.를 attenuate</code> (pre-filtering) 하여<br> <code class="language-plaintext highlighter-rouge">임의의 continuous-space scale</code>을 가지는 scene에 대해 <code class="language-plaintext highlighter-rouge">anti-aliased</code> representation 학습 가능<br> \(\rightarrow\) multi-resolution dataset에 대해 성능 대폭 향상<br> \(\rightarrow\) scale-aware하므로 <code class="language-plaintext highlighter-rouge">single MLP</code> 하나만으로 충분하여 빠르고 가벼움</li> <li>camera center로부터 각 pixel로 3D cone을 쏜 다음,<br> <code class="language-plaintext highlighter-rouge">frustum을 multi-variate Gaussian으로 근사</code>한 뒤,<br> Gaussian 내 좌표를 positional encoding한 것 (PE-basis-lifted Gaussian)에 대해 expected value \(E \left[ \gamma (x) \right]\) 계산<br> 주의 : frustum이 Gaussian 분포를 따르는 게 아니라, frustum 내부의 mean, variance 값을 먼저 구한 뒤 해당 mean, variance 값을 갖는 Gaussian으로 frustum을 대신(근사)할 수 있다고 생각!</li> </ol> </blockquote> <h2 id="introduction">Introduction</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/2-480.webp 480w,/assets/img/2024-08-03-MipNeRF/2-800.webp 800w,/assets/img/2024-08-03-MipNeRF/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-03-MipNeRF/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>기존 NeRF의 문제점 : <ul> <li>rendering 위해 sampling할 때 <code class="language-plaintext highlighter-rouge">single ray</code> per pixel 쏴서 composite 하므로<br> dataset에 있는 물체의 크기(resolution)가 일정하지 않을 때<br> multi-scales images에 대해 학습하더라도</li> <li> <code class="language-plaintext highlighter-rouge">blurry</code> rendering in <code class="language-plaintext highlighter-rouge">close-up</code> views<br> (because 가까이서 찍어서 zoom-out하면 물체 in <code class="language-plaintext highlighter-rouge">high resolution</code>)</li> <li> <code class="language-plaintext highlighter-rouge">aliased</code>(계단) rendering in <code class="language-plaintext highlighter-rouge">distant</code> views<br> (because 멀리서 찍어서 zoom-in하면 물체 in <code class="language-plaintext highlighter-rouge">low resolution</code>)</li> <li>그렇다고 multiple rays per pixel through its footprint로 brute-force super-sampling(offline rendering)하는 것은 정확하긴 하겠지만 too costly 비현실적</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">Minmap</code> Approach :<br> classic 컴퓨터 그래픽스 분야에서 rendering할 때 aliasing을 없애기 위한 <code class="language-plaintext highlighter-rouge">pre-filtering</code> 기법<br> 본 논문인 Mip-NeRF가 여기서 영감을 얻음<br> signal(e.g. image)을 diff. <code class="language-plaintext highlighter-rouge">downsampling scales</code>로 나타낸 뒤 pixel footprint를 근거로 ray에 사용하기 위한 <code class="language-plaintext highlighter-rouge">적절한 scale을 고른다</code><br> render time에 할 복잡할 일을 pre-computation phase에 미리 하는 것일 뿐이긴 하지만, 주어진 texture마다 한 번만 minmap을 만들면 된다는 장점이 있다</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/4-480.webp 480w,/assets/img/2024-08-03-MipNeRF/4-800.webp 800w,/assets/img/2024-08-03-MipNeRF/4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-03-MipNeRF/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Mip-NeRF : <ul> <li>represent pre-filtered scene at <code class="language-plaintext highlighter-rouge">continuous space of scales</code> </li> <li>ray 대신 <code class="language-plaintext highlighter-rouge">conical frustum</code> 사용해서 <code class="language-plaintext highlighter-rouge">anti-aliased</code> rendering with fine details</li> <li>multi-scale variant of dataset에 대해 평균 error rate 60% 감소</li> <li>NeRF가 hierarchical sampling을 위해 coarse and fine MLP를 분리했다면, Mip-NeRF는 <code class="language-plaintext highlighter-rouge">scale-aware</code>하므로 <code class="language-plaintext highlighter-rouge">single multi-scale MLP만으로 충분</code><br> 따라서 NeRF보다 7% 빠르고, param. 수는 절반이고, sampling도 더 효율적</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/1-480.webp 480w,/assets/img/2024-08-03-MipNeRF/1-800.webp 800w,/assets/img/2024-08-03-MipNeRF/1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-03-MipNeRF/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> ray 대신 cone을 쏘고, point-encoding 대신 frustum region-encoding </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/3-480.webp 480w,/assets/img/2024-08-03-MipNeRF/3-800.webp 800w,/assets/img/2024-08-03-MipNeRF/3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-03-MipNeRF/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <p>차이 : 기존 NeRF는 <code class="language-plaintext highlighter-rouge">a single point</code>를 encode하고, Mip-NeRF는 <code class="language-plaintext highlighter-rouge">a region of space</code>를 encode</p> </li> <li> <p>기존 NeRF :<br> camera center로부터 각 pixel로 ray를 하나 쏜 다음 point sampling한 뒤 positional encoding<br> point-sampled feature는 ray가 보는 <code class="language-plaintext highlighter-rouge">volume의 모양과 크기를 무시</code>하는 것임<br> 예를 들어 training할 때 camera1로부터 t 사이의 간격이 평균 10cm로 학습된 scene에 대해<br> camera2로 inference를 할 때 t 사이의 간격이 평균 1cm로 sampling된다면<br> 10개의 점은 같은 point-based feature를 갖게 되어 scale을 고려하지 못함<br> 이러한 ambiguity가 기존 NeRF의 성능 하락의 요인</p> </li> <li> <p>Mip-NeRF :<br> volume 정보를 반영하기 위해 camera center로부터 각 pixel로 3D cone을 쏜 다음, 3D point 및 그 주위의 Gaussian region을 encode하기 위해 IPE</p> </li> <li> <p>IPE (<code class="language-plaintext highlighter-rouge">integrated positional encoding</code>) :<br> region을 encode하기 위한 방식<br> <code class="language-plaintext highlighter-rouge">frustum을 multi-variate Gaussian으로 근사</code>한 뒤,<br> Gaussian 내 좌표를 positional encoding한 것 (PE-basis-lifted Gaussian)에 대해 expected value \(E \left[ \gamma (x) \right]\) 계산</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/5-480.webp 480w,/assets/img/2024-08-03-MipNeRF/5-800.webp 800w,/assets/img/2024-08-03-MipNeRF/5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-03-MipNeRF/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="related-work">Related Work</h2> <h3 id="anti-aliasing-in-rendering">Anti-aliasing in Rendering</h3> <blockquote> <p><code class="language-plaintext highlighter-rouge">anti-aliasing</code>을 위한 고전적인 방법으로는 두 가지가 있다.</p> </blockquote> <ol> <li> <code class="language-plaintext highlighter-rouge">supersampling</code> : <ul> <li>rendering할 때 <code class="language-plaintext highlighter-rouge">multiple rays per pixel</code>을 쏴서 Nyquist frequency에 가깝게 sampling rate를 높임 (super-sampling)</li> <li> <code class="language-plaintext highlighter-rouge">expensive</code> as runtime increases linearly with the super-sampling rate, so used only in <code class="language-plaintext highlighter-rouge">offline</code> rendering</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">pre-filtering</code> : <ul> <li>target sampling rate에 맞춰서 Nyquist frequency를 줄이기 위해 scene에 <code class="language-plaintext highlighter-rouge">lowpass-filter</code>를 씌운 버전 사용</li> <li>scene을 미리 <code class="language-plaintext highlighter-rouge">downsampling multi-scales</code> (sparse voxel octree 또는 minmap)로 나타낸 뒤, <code class="language-plaintext highlighter-rouge">ray 대신 cone</code>을 추적하여 cone과 scene이 만나는 곳의 cone’s footprint에 대응되는 적절한 scale을 골라서 사용 (<code class="language-plaintext highlighter-rouge">target sampling rate에 맞는 적절한 scale</code>)</li> <li>scene에 filter 씌운 버전을 한 번만 미리 계산하면 되므로, better for <code class="language-plaintext highlighter-rouge">real-time</code> rendering</li> </ul> </li> </ol> <blockquote> <p>그런데 아래의 이유로 고전적인 multi-scale representation은 적용 불가능</p> <ul> <li>input scene의 <code class="language-plaintext highlighter-rouge">geometry를 미리 알 수 없으므로</code> pre-filtering 할 수가 없어서<br> 대신 pre-filtering 방식 자체를 training할 때 학습해야 한다</li> <li>input scene의 <code class="language-plaintext highlighter-rouge">scale이 continuous</code>하므로 a fixed number of scales (discrete)과 상황이 다르다</li> </ul> </blockquote> <p>\(\rightarrow\) 결론 : 따라서 Mip-NeRF는 training하는 동안, <code class="language-plaintext highlighter-rouge">임의의 scale</code>에 대해 query 받을 수 있는, scene의 <code class="language-plaintext highlighter-rouge">pre-filtered representation</code>을 학습한다.</p> <h3 id="scene-representations-for-view-synthesis">Scene Representations for View Synthesis</h3> <ul> <li> <p>만약 images of scene are captured <code class="language-plaintext highlighter-rouge">densely</code> 라면, novel view synthesis 위해, intermediate representation of scene을 reconstruct하지 않고 <code class="language-plaintext highlighter-rouge">light field interpolation</code> 기법 사용</p> </li> <li>만약 images of scene are captured <code class="language-plaintext highlighter-rouge">sparsely</code> 라면, novel view synthesis 위해, <code class="language-plaintext highlighter-rouge">explicit representation</code> of the scene’s 3D geometry and appearance를 reconstruct <ul> <li> <code class="language-plaintext highlighter-rouge">polygon-mesh-based</code> representation (<code class="language-plaintext highlighter-rouge">discrete, classic</code>) :<br> with either diffuse or view-dependent textures<br> can be stored efficiently, but mesh geometry optimization에 gradient descent를 이용하는 것은 불연속점 및 극소점 때문에 어렵</li> <li> <code class="language-plaintext highlighter-rouge">volumetric</code> representation : <ul> <li> <code class="language-plaintext highlighter-rouge">voxel-grid-based</code> representation (<code class="language-plaintext highlighter-rouge">discrete, classic</code>) : deep learning으로 학습 가능, but 고해상도의 scene에는 부적합</li> <li> <code class="language-plaintext highlighter-rouge">coordinate-based</code> neural representation (<code class="language-plaintext highlighter-rouge">continuous, recent</code>) : 3D 좌표를 그 위치에서의 scene의 특징(e.g. volume density, radiance)으로 mapping하는 continuous function을 MLP로 예측 <ul> <li>implicit surface representation</li> <li><code class="language-plaintext highlighter-rouge">implicit volumetric NeRF representation</code></li> </ul> </li> </ul> </li> </ul> </li> <li>문제점 :<br> 그동안 view synthesis를 할 때 sampling 및 aliasing에는 덜 주목했다<br> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">classic discrete</code> representation의 경우 앞에서 소개한 minmap, octree와 같은 고전적인 multi-scale pre-filtering 기법을 쓰면 aliasing 없이 rendering 가능하다<br> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">coordinate-based</code> representation의 경우 scale이 continuous하므로 고전적인 anti-aliasing 기법은 사용할 수 없다<br> \(\rightarrow\) sparse voxel octree 기반의 multi-scale representation으로 implicit surface의 continuous neural representation을 구현한 논문 <d-cite key="continuouspriori">[1]</d-cite> 있긴 하지만, scene geometry의 priori를 알아야 한다는 제약이 있다<br> \(\rightarrow\) Mip-NeRF는 training하는 동안, <code class="language-plaintext highlighter-rouge">임의의 scale</code>에 대해 query 받을 수 있는, scene의 <code class="language-plaintext highlighter-rouge">anti-aliased (pre-filtered)</code> representation을 학습한다.</li> </ul> <h2 id="method">Method</h2> <h3 id="cone-tracing-and-positional-encoding">Cone Tracing and Positional Encoding</h3> <ul> <li>Cone Tracing : <ul> <li>Let \(d\) is cone direction vector from \(o\) to image plane</li> <li>Let \(\hat r =\) radius at image plane \(o + d\) \(= \frac{2}{\sqrt{12}}\) of pixel-width<br> so that image plane에서의 cone의 \(x, y\) 축 variance가 pixel’s footprint의 variance와 같아지도록<br> \(\hat r\)은 ray의 radius 변화율, 즉 frustum의 넓이를 결정</li> <li>\(t \in [t_0, t_1]\) 일 때 conical frustum 내의 \(x\)는 아래 범위의 값을 가질 때 indicator function \(F(x, o, d, \hat r, t_0, t_1)=1\)이다<br> \(F(x, o, d, \hat r, t_0, t_1) = 1 \left\{ (t_0 \lt \frac{d^T(x-o)}{\| d \|^2} \lt t_1) \land (\frac{d^T(x-o)}{\| d \| \| x-o \|} \gt \frac{1}{\sqrt{1+(\frac{\hat r}{\| d \|})^2}}) \right\}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/6-480.webp 480w,/assets/img/2024-08-03-MipNeRF/6-800.webp 800w,/assets/img/2024-08-03-MipNeRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-03-MipNeRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <p>Region Encoding :<br> conical frustum 내에 있는 모든 좌표 \(x\)에 대해 직접<br> expected value \(E \left[ \gamma (x) \right] = \frac{\int \gamma (x) F(x, o, d, \hat r, t_0, t_1) dx}{\int F(x, o, d, \hat r, t_0, t_1) dx}\) 계산하면<br> region을 encode할 수 있는데<br> 여기서 분자의 적분식은 closed-form solution이 없음<br> \(\rightarrow\) 직접 계산하지 말고<br> <code class="language-plaintext highlighter-rouge">conical-frustum을 multi-variate Gaussian으로 근사</code>한 뒤<br> Gaussian 내에 있는 모든 좌표 \(x\)에 대해<br> expected value \(E \left[ \gamma (x) \right]\) 계산</p> </li> <li>frustum을 multi-variate Gaussian으로 근사 : <ul> <li>conical-frustum은 대칭적인 원이기 때문에<br> \(o, d\) 뿐만 아니라 아래의 3가지 정보만 알면 Gaussian을 특정할 수 있다 <ul> <li> <code class="language-plaintext highlighter-rouge">mean distance along ray from o</code> \(\mu_{t}\)</li> <li> <code class="language-plaintext highlighter-rouge">variance along ray</code> \(\sigma_{t}^2\)</li> <li> <code class="language-plaintext highlighter-rouge">variance perpendicular to ray</code> \(\sigma_{r}^2\)</li> </ul> </li> <li>Let mid-point \(t_{\mu} = \frac{t_0+t_1}{2}\)<br> Let half-width \(t_{\sigma}=\frac{t_1-t_0}{2}\)</li> <li>아래 수식의 유도과정은 하위에 별도로 정리함<br> \(\mu_{t} = t_{\mu} + \frac{2t_{\mu}t_{\sigma}^2}{3t_{\mu}^2+t_{\sigma}^2}\)<br> \(\sigma_{t}^2 = \frac{t_{\sigma}^2}{3} - \frac{4t_{\sigma}^4(12t_{\mu}^2-t_{\sigma}^2)}{15(3t_{\mu}^2+t_{\sigma}^2)^2}\)<br> \(\sigma_{r}^2 = \hat r^2 (\frac{t_{\mu}^2}{4} + \frac{5t_{\sigma}^2}{12} - \frac{4t_{\sigma}^4}{15(3t_{\mu}^2+t_{\sigma}^2)})\)</li> <li>위의 3가지 param.를 가지는 Gaussian은 <code class="language-plaintext highlighter-rouge">t-coordinate</code>에서 정의했는데<br> 아래 수식에 의해 <code class="language-plaintext highlighter-rouge">world-coordinate</code>으로 변환할 수 있다<br> \(\mu = o + \mu_{t}d\)<br> \(\Sigma = \sigma_{t}^2(dd^T) + \sigma_{r}^2(I-\frac{dd^T}{\| d \|^2})\)<br> where \(dd^T =\) \(d\) 의 outer product은 \(d\) 방향으로의 투영을 의미하는 rank-1 matrix<br> where \(I-\frac{dd^T}{\| d \|^2}\) 는 \(\frac{d}{\| d\ \|}\) 와 수직인 subspace로의 투영을 의미하는 rank-2 matrix</li> </ul> </li> <li>Integrated Positional Encoding (IPE) : <ul> <li>목표 : 위에서 계산한 \(\mu, \Sigma\) 의 Gaussian 내에 있는 모든 좌표 \(x\)에 대해 expected value \(E \left[ \gamma (x) \right]\) 계산</li> <li>우선 <code class="language-plaintext highlighter-rouge">PE (positional-encoding) basis</code> P를 재정의<br> \(P = \begin{pmatrix} \begin{matrix} 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2\end{matrix} &amp; \cdots &amp; \begin{matrix} 2^{L-1} &amp; 0 &amp; 0 \\ 0 &amp; 2^{L-1} &amp; 0 \\ 0 &amp; 0 &amp; 2^{L-1}\end{matrix} \end{pmatrix}^T\)<br> \(\gamma (x) = \begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}\)</li> <li>\(E \left[ \gamma (x) \right]\) 는 expectation over \(\gamma (x) = \begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}\) 이므로<br> \(x\) in Gaussian of \(\mu, \Sigma\) \(\rightarrow\) \(\gamma (x)\) in Gaussian of \(\mu_{r}, \Sigma_{r}\) 로 변환해야 한다<br> 즉, <code class="language-plaintext highlighter-rouge">PE basis P로 lift</code>한 뒤의 mean과 covariance를 구해야 한다<br> Since \(Cov[Ax, By] = A Cov[x, y] B^T\),<br> \(\mu_{r} = P \mu\)<br> \(\Sigma_{r} = P \Sigma P^T\)</li> <li>최종적으로 \(E \left[ \gamma (x) \right]\) , 즉 <code class="language-plaintext highlighter-rouge">expectation over lifted multi-variate Gaussian</code> of \(\mu_{r}, \Sigma_{r}\) 을 구하면 된다<br> Since \(E_{k \sim N(\mu, \sigma^2)}[e^{itk}] = exp(i \mu t - \frac{1}{2} \sigma^2 t^2)\) and \(sin(k) = \frac{e^{ik}-e^{-ik}}{2i}\) and \(cos(k) = \frac{e^{ik}+e^{-ik}}{2}\),<br> \(E_{k \sim N(\mu, \sigma^2)}[sin(k)] = sin(\mu)exp(-\frac{1}{2}\sigma^2)\) and \(E_{k \sim N(\mu, \sigma^2)}[cos(k)] = cos(\mu)exp(-\frac{1}{2}\sigma^2)\) for each axis-k<br> (positional-encoding은 각 dim.을 independently encode하므로 marginal distribution of \(\gamma (x)\) 에 의존)<br> \(\rightarrow\)<br> \(\gamma (\mu, \Sigma) = E_{x \sim N(\mu, \Sigma)} [\gamma (x)] = E_{Px \sim N(\mu_{r}, \Sigma_{r})} [\begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}]\)<br> \(= \begin{bmatrix} sin(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \\ cos(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \end{bmatrix}\)<br> where \(\circledast\) is element-wise multiplication</li> <li>\(diag(\Sigma_{r})\) 만 필요하므로 \(\Sigma_{r}\) 전부 계산하지 말고 <code class="language-plaintext highlighter-rouge">efficiently diagonal만 계산</code><br> \(diag(\Sigma_{r}) = diag(P \Sigma P^T) = \left[ diag(\Sigma), 4 diag(\Sigma), \ldots , 4^{L-1}diag(\Sigma) \right]^T\)<br> where 3d-vector \(diag(\Sigma) = \sigma_{t}^2(d \circledast d) + \sigma_{r}^2(1-\frac{d \circledast d}{\| d \|^2})\)<br> diagonal만 직접 계산하면, IPE feature는 PE feature랑 비슷하게 cost 소모</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/7-480.webp 480w,/assets/img/2024-08-03-MipNeRF/7-800.webp 800w,/assets/img/2024-08-03-MipNeRF/7-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-03-MipNeRF/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>IPE vs PE : <ul> <li>PE :<br> point를 encode<br> 0~L까지의 <code class="language-plaintext highlighter-rouge">모든 frequencies에 대해 동일하게</code> encode<br> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">high-freq.</code> PE features are <code class="language-plaintext highlighter-rouge">aliased</code><br> (PE period가 interval width보다 작은 경우 PE over interval oscillates repeatedly)</li> <li>IPE :<br> interval region을 integrate하여 encode<br> IPE feature를 만드는 데 사용된 interval \(t \in [t_0, t_1]\) width보다 period가 작은 <code class="language-plaintext highlighter-rouge">high freq.</code>의 경우 <code class="language-plaintext highlighter-rouge">attenuate</code>하여 <code class="language-plaintext highlighter-rouge">anti-aliasing</code><br> by \(exp(-\frac{1}{2}(\omega \sigma)^2)\) term</li> <li>위와 같은 특성 덕분에 IPE는 interval 내 공간의 모양과 크기를 smoothly encode할 수 있는 anti-aliased PE 기법이다!</li> <li>high freq.는 IPE 단계 자체에서 attenuate되므로 <code class="language-plaintext highlighter-rouge">L을 hyper-param.로 두지 않고 extremely large fixed-value</code>로 두면 된다</li> </ul> </li> <li>IPE의 의미 :<br> 이게 Mip-NeRF의 핵심!! <ul> <li>수식 :<br> PE-basis P 는 다양한 frequency \(\omega\) 로 구성되어 있고<br> 각 element는 \(E_{x \sim N(\mu, \Sigma)} [\gamma_{\omega} (x)] = sin(\omega \mu) exp(-\frac{1}{2}(\omega \sigma)^2)\)</li> <li>distant view :<br> <code class="language-plaintext highlighter-rouge">distant views (low-resolution)</code>, 즉 멀리 있는 <code class="language-plaintext highlighter-rouge">wide frustum</code> (high variance \(\sigma\))의 경우에는<br> <code class="language-plaintext highlighter-rouge">detail-info.</code> (high freq. \(\omega\))는 <code class="language-plaintext highlighter-rouge">training에 사용하지 않겠다</code><br> \(\rightarrow\) more attenuation for high \(\sigma\) and high \(\omega\)</li> <li>close view :<br> <code class="language-plaintext highlighter-rouge">close views (high-resolution)</code>, 즉 가까이 있는 <code class="language-plaintext highlighter-rouge">narrow frustum</code> (low variance \(\sigma\))의 경우에는<br> <code class="language-plaintext highlighter-rouge">detail-info.</code> (high freq. \(\omega\))를 training할 때 좀 더 <code class="language-plaintext highlighter-rouge">허용</code> </li> <li>위와 같이 scale을 반영할 수 있으므로 blurry 및 aliased rendering 문제 해결 가능!</li> </ul> </li> <li>수식 <code class="language-plaintext highlighter-rouge">Summary</code> : <ul> <li>frustum을 근사하는 multi-variate Gaussian의 mean, variance \(\mu, \sigma\) 를 구한다</li> <li>PE-basis P로 lift한 Gaussian의 mean, variance \(\mu_{r}, \Sigma_{r}\) 를 구한다<br> \(P = \begin{pmatrix} \begin{matrix} 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2\end{matrix} &amp; \cdots &amp; \begin{matrix} 2^{L-1} &amp; 0 &amp; 0 \\ 0 &amp; 2^{L-1} &amp; 0 \\ 0 &amp; 0 &amp; 2^{L-1}\end{matrix} \end{pmatrix}^T\)<br> \(\gamma (x) = \begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}\)<br> \(\mu_{r} = P \mu\) and \(\Sigma_{r} = P \Sigma P^T\)</li> <li>\(E_{x \sim N(\mu_{r}, \Sigma_{r})} [\gamma (x)] = \begin{bmatrix} sin(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \\ cos(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \end{bmatrix}\)<br> (efficiently \(\Sigma_{r}\) 의 diagonal만 직접 계산)</li> </ul> </li> </ul> <h3 id="conical-frustum-integral-derivation">Conical Frustum Integral Derivation</h3> <ul> <li> <p>우선 <code class="language-plaintext highlighter-rouge">Cartesian-coordinate</code>에서 <code class="language-plaintext highlighter-rouge">conical-coordinate</code>으로 변환<br> \((x, y, z) = \varphi (r, t, \theta) = t \cdot (r cos \theta , r sin \theta , 1)\)<br> where \(\theta \in [0, 2 \pi)\) and \(t \geq 0\) and \(\| r \| \leq \hat r\)<br> Then,<br> \(dx dy dz = | det(D \varphi) | dr dt d\theta\)<br> \(= \begin{vmatrix} t cos\theta &amp; t sin\theta &amp; 0 \\ r cos\theta &amp; r sin\theta &amp; 1 \\ - rt sin\theta &amp; rt cos\theta &amp; 0 \end{vmatrix} dr dt d\theta\)<br> \(= (rt^2cos^2\theta + rt^2sin\theta) dr dt d\theta\)<br> \(= rt^2 dr dt d\theta\)</p> </li> <li> <p>conical frustum의 volume \(V = \int \int \int dx dy dz = \int_{0}^{2\pi} \int_{t_0}^{t_1} \int_{0}^{\hat r} r t^2 dr dt d\theta = \pi \hat r^2 \frac{t_1^3 - t_0^3}{3}\) 에 대해<br> conical frustum에서 uniformly-sampling한 points의 <code class="language-plaintext highlighter-rouge">probability density function</code>은 \(\frac{rt^2}{V}\) 이다</p> </li> <li> <code class="language-plaintext highlighter-rouge">t-axis</code> : <ul> <li> \[E \left[ t \right] = \int_{0}^{2\pi} \int_{t_0}^{t_1} \int_{0}^{\hat r} t \cdot \frac{rt^2}{V} dr dt d\theta = \frac{3(t_1^4 - t_0^4)}{4(t_1^3 - t_0^3)}\] </li> <li> \[E \left[ t^2 \right] = \int_{0}^{2\pi} \int_{t_0}^{t_1} \int_{0}^{\hat r} t^2 \cdot \frac{rt^2}{V} dr dt d\theta = \frac{3(t_1^5- t_0^5)}{5(t_1^3 - t_0^3)}\] </li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">x-axis</code> (\(x = t r cos \theta\)) : <ul> <li> \[E \left[ t r cos\theta \right] = \int_{0}^{2\pi} \int_{t_0}^{t_1} \int_{0}^{\hat r} t r cos\theta \cdot \frac{rt^2}{V} dr dt d\theta = 0\] </li> <li> \[E \left[ (t r cos \theta)^2 \right] = \int_{0}^{2\pi} \int_{t_0}^{t_1} \int_{0}^{\hat r} t^2 r^2 cos^2 \theta \cdot \frac{rt^2}{V} dr dt d\theta = \frac{\hat r^2}{4} \frac{3(t_1^5 - t_0^5)}{5(t_1^3 - t_0^3)}\] </li> </ul> </li> <li> <p><code class="language-plaintext highlighter-rouge">y-axis</code> (\(y = t r sin \theta\)) :<br> conical frustum은 x, y에 대해 symmetric하므로 위에서 구한 x-axis에서의 값과 동일</p> </li> <li>이제 conical frustum 내부에 있는 random point에 대한 mean, covariance 값을 구할 수 있다 <ul> <li> <code class="language-plaintext highlighter-rouge">mean distance along ray from o</code> :<br> \(\mu_{t} = E \left[ t \right] = \frac{3(t_1^4 - t_0^4)}{4(t_1^3 - t_0^3)}\)</li> <li> <code class="language-plaintext highlighter-rouge">variance along ray</code> :<br> \(\sigma_{t}^2 = E \left[ t^2 \right] - (E \left[ t \right])^2 = \frac{3(t_1^5- t_0^5)}{5(t_1^3 - t_0^3)} - \mu_{t}^2\)</li> <li> <code class="language-plaintext highlighter-rouge">variance perpendicular to ray</code> :<br> \(\sigma_{r}^2 = E \left[ x^2 \right] - (E \left[ x \right])^2 = \hat r^2 \frac{3(t_1^5 - t_0^5)}{20(t_1^3 - t_0^3)}\)</li> </ul> </li> <li>그런데 \(t_0, t_1\) 이 서로 가까우면 \(\frac{(t_1^5- t_0^5)}{(t_1^3 - t_0^3)}\) 과 같은 꼴은 numerically unstable as 0 or NaN instead of accurate values 이므로 training fail<br> \(\rightarrow\)<br> \(t_{\mu} = \frac{t_0+t_1}{2}\) and \(t_{\sigma}=\frac{t_1-t_0}{2}\) 로 re-parameterize하면<br> <code class="language-plaintext highlighter-rouge">first-order term + correct(higher-order) term 꼴</code>로 정리 가능하고<br> \(t_{\sigma}\) 가 작을 때에도 stable and accurate values 가짐 <ul> <li> \[\mu_{t} = t_{\mu} + \frac{2t_{\mu}t_{\sigma}^2}{3t_{\mu}^2+t_{\sigma}^2}\] </li> <li> \[\sigma_{t}^2 = \frac{t_{\sigma}^2}{3} - \frac{4t_{\sigma}^4(12t_{\mu}^2-t_{\sigma}^2)}{15(3t_{\mu}^2+t_{\sigma}^2)^2}\] </li> <li> \[\sigma_{r}^2 = \hat r^2 (\frac{t_{\mu}^2}{4} + \frac{5t_{\sigma}^2}{12} - \frac{4t_{\sigma}^4}{15(3t_{\mu}^2+t_{\sigma}^2)})\] </li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">limitation</code> :<br> frustum의 base 반지름과 top 반지름 차이가 클수록<br> conical-frustum을 multi-variate Gaussian으로 approx.하는 건 inaccurate<br> (예를 들어, camera FOV가 클 때 camera center와 매우 가까운 frustum)<br> 대부분의 dataset에서는 흔하지 않은 case이긴 하지만,<br> macro photography with fisheye lens와 같은 특별한 case에서 MipNeRF를 쓸 때는 frustum을 multi-variate Gaussian으로 approx.하는 게 문제가 될 수 있음</li> </ul> <h3 id="architecture">Architecture</h3> <ul> <li>아래 내용들을 제외하고는 NeRF의 Architecture와 동일 <ul> <li>ray-tracing 대신 cone-tracing</li> <li>PE 대신 IPE</li> <li>point-encoding 이므로 \(n\)개의 구간에 대해 \(n\)개의 point sampling<br> \(\rightarrow\)<br> interval(region)-encoding 이므로 \(n\)개의 구간을 위해 \(n+1\)개의 point sampling</li> <li>PE feature로는 scale을 반영할 수 없으므로 두 가지 MLP (coarse-MLP, fine-MLP) 이용해서 hierarchical sampling<br> (coarse-MLP에서는 \(N_c=64\) points per ray, fine-MLP에서는 \(N_c+N_f=64+128\) points per ray)<br> \(\rightarrow\)<br> IPE feature 자체가 scale을 반영할 수 있으므로 MLP 하나를 반복해서 써서 hierarchical sampling<br> (한 번은 \(N_c=128\) points per ray, 그 다음은 \(N_f=128\) points per ray)<br> NeRF와 MipNeRF의 공정한 비교를 위해 같은 수(총 256개)의 point를 사용</li> <li>hierarchical sampling에서 piecewise-constant PDF of normalized \(w\) 에 따라 fine-sampling 하기 전에<br> weight \(w_k\) 를 바로 사용하지 않고<br> 2-tap MaxBlur filter 를 적용한 weight의 wide and smooth upper bound 를 사용<br> \(w_k^{\ast} = \frac{1}{2}(max(w_{k-1}, w_k) + max(w_k, w_{k+1})) + \alpha\)<br> where 빈 공간에서도 일부 samples 추출되도록 보장하기 위해 \(\alpha=0.01\) 설정</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/8-480.webp 480w,/assets/img/2024-08-03-MipNeRF/8-800.webp 800w,/assets/img/2024-08-03-MipNeRF/8-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-03-MipNeRF/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> single MLP 쓰니까 coarse loss와 fine loss 간의 balance 맞추기 위해 hyperparam. gamma = 0.1로 설정 </div> <ul> <li>MaxBlur filter : <ul> <li>MaxPool 대신 MaxBlurPool 쓰면 aliasing 감소 효과</li> <li>MipNeRF에서 weight에 MaxBlur filter 쓰는 이유 :<br> scene content는 아무래도 연속적으로 존재하니까<br> 인접한 samples 간의 weight \(w\) 가 갑작스럽게 변하거나 불연속적인 outlier 를 제외하여 smoothing 해주는 역할</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/10-480.webp 480w,/assets/img/2024-08-03-MipNeRF/10-800.webp 800w,/assets/img/2024-08-03-MipNeRF/10-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-03-MipNeRF/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-03-MipNeRF/9-480.webp 480w,/assets/img/2024-08-03-MipNeRF/9-800.webp 800w,/assets/img/2024-08-03-MipNeRF/9-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-03-MipNeRF/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Setting :<br> implementation on JaxNeRF<br> 1 million iter., Adam optimizer, batch_size = 4096, lr from \(5 \cdot 10^{-4}\) to \(5 \cdot 10^{-6}\)</li> </ul> <h2 id="result">Result</h2> <ul> <li>multi-scale dataset에 대해 NeRF보다 error rate 60% 감소</li> <li>single-scale dataset에 대해 NeRF보다 error rate 17% 감소</li> <li>NeRF의 param.의 절반이고, NeRF보다 7% 빠름</li> <li>brute-force super-sampling한 버전보다 22배 빠른데 accuracy 비슷</li> </ul> <h2 id="question">Question</h2> <ul> <li>Q1 : distant view (scene content in low-resolution)일 때 IPE의 \(exp(-\frac{1}{2}(\omega \sigma)^2)\) term에 의해 high freq.를 attenuate하여 anti-aliasing 가능한 건 이해했는데,<br> close view (scene content in high-resolution)일 때 blurry rendering은 어떻게 해결??</li> <li>A1 : 위에서 “Method - Cone Tracing and Positional Encoding - IPE의 의미”에 설명해둠</li> <li>Q2 : image plane에서의 cone의 \(x, y\) 축 variance가 pixel’s footprint의 variance와 같아지도록<br> \(\hat r =\) radius at image plane \(o + d\) \(= \frac{2}{\sqrt{12}}\) of pixel-width 로 설정한다는데 이 부분이 이해가 되지 않습니다</li> <li>A2 : uniform distribution을 가정했을 때 pixel의 square variance는 \(\frac{w^2}{12}\) 이고, cone at image plane의 circle variance는 \(\frac{\hat r^2}{4}\) 이므로 variance 값이 같으려면 \(\hat r = \frac{2}{\sqrt{12}} \times w\)</li> <li>Q3 : \(E \left[ \gamma (x) \right] = \frac{\int \gamma (x) F(x, o, d, \hat r, t_0, t_1) dx}{\int F(x, o, d, \hat r, t_0, t_1) dx}\) 에서 분자의 적분식을 closed-form으로 계산할 수 없어서 conical frustum을 multi-variate Gaussian으로 근사했다는데,<br> conical frustum의 모양과 크기 범위에 대한 parameter가 주어진다면 frustum 내부의 점 \(x\) 에 sin 및 cos을 씌운 \(\gamma (x)\) 의 경우 \(x\) 에 대해 공간 적분할 수 있지 않나요?</li> <li>A3 : frustum 내에 있는 모든 좌표에 \(\gamma\) 를 씌워서 공간 적분하는 것 자체가 말도 안 되게 복잡한 식이라 closed-form solution이 없기 때문에 frustum의 mean과 variance를 구해서 Gaussian으로 근사해서 expected value 구합니다</li> <li>Q4 : 논문을 보면 frustum을 multi-variate Gaussian으로 근사하기 위해서는 먼저 \(F(x, o, d, \hat r, t_0, t_1)\) 의 mean, covariance를 계산해야 한다고 쓰여있던데<br> appendix를 보면 indicator function인 F의 mean과 covariance가 아니라 conical frustum의 \(r, t, \theta\) 범위를 이용해서 공간 적분해서 \(t, x, y\) 축의 mean과 variance를 계산하지 않나요?</li> <li>A4 : 맞습니다. 논문에서 \(F(x, o, d, \hat r, t_0, t_1)\) 의 mean, covariance를 계산해야 한다고 언급되어 있는 것은 단순히 frustum 내부 범위에 속해있는 지점에 대해 적분을 통해 mean, variance를 구해야 한다는 뜻인 것 같습니다.</li> <li>Q5 : NeRF에서 rendering할 때는 EWA volume splatting과 같은 좌표계 변환을 고려하지 않아도 되나요?</li> <li>A5 : NeRF에서는 ray를 따라 MLP의 output을 alpha-compositing하여 직접 pixel 값을 얻어내므로 ray를 쓰기 위해 cam-to-world coordinate 변환만 필요하고, projection에 의한 non-linear 좌표계 변환과는 관련이 없다.<br> 반면, Gaussian Splatting에서는 rendering할 때 3D Gaussian 자체를 직접 projection해서 쓰기 때문에 3D Gaussian covariance matrix on world-coordinate을 2D Gaussian covariance matrix on image-coordinate (ray-space)으로 projection해야 하므로 non-linear 좌표계 변환이 필요하다. 이를 위해 EWA volume splatting에 따라 non-linear transformation을 Taylor approx.하여 local affine transformation으로서 Jacobian을 사용한다</li> <li>Q6 : camera origin과 pixel 중심을 잇는 ray가 image plane에 수직이 아닌 pixel의 경우 \(\hat r\) 과 \(d\) 를 어떻게 정의하지?</li> <li>A6 : \(d\) 는 camera origin부터 pixel 중심까지의 거리 vector이고,<br> cone 단면의 \(\hat r\)은 \(d\) 와 수직인 방향으로 \(\frac{2}{\sqrt{12}}\) of pixel-width 이므로<br> cone 단면이 image plane 위에 있지 않은 꼴이 됨</li> </ul> <h2 id="appendix">Appendix</h2> <p>B. The L Hyperparameter in PE and IPE 읽을 차례</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-08-03-MipNeRF.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"semyeong-yu/semyeong-yu.github.io","data-repo-id":"R_kgDOLmVJXQ","data-category":"Comments","data-category-id":"DIC_kwDOLmVJXc4CeSwc","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Semyeong Yu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5YMLX9VHFX"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5YMLX9VHFX");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>