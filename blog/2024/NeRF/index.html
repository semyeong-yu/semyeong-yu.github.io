<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> NeRF | Semyeong Yu </title> <meta name="author" content="Semyeong Yu"> <meta name="description" content="representing scenes as neural radiance fields for view synthesis"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://semyeong-yu.github.io/blog/2024/NeRF/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "NeRF",
            "description": "representing scenes as neural radiance fields for view synthesis",
            "published": "April 10, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Semyeong</span> Yu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>NeRF</h1> <p>representing scenes as neural radiance fields for view synthesis</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <ul> <li> <a href="#pipeline">Pipeline</a> </li> <li> <a href="#problem-solution">Problem &amp; Solution</a> </li> <li> <a href="#contribution">Contribution</a> </li> </ul> <div> <a href="#related-work">Related Work</a> </div> <ul> <li> <a href="#neural-3d-shape-representation">Neural 3D shape representation</a> </li> <li> <a href="#view-synthesis-and-image-based-rendering">View synthesis and image-based rendering</a> </li> </ul> <div> <a href="#neural-radiance-field-scene-representation">Neural Radiance Field Scene Representation</a> </div> <div> <a href="#volume-rendering-with-radiance-fields">Volume Rendering with Radiance Fields</a> </div> <ul> <li> <a href="#ray-from-input-image-pre-processing">Ray from input image (pre-processing)</a> </li> <li> <a href="#volume-rendering-from-mlp-output">Volume Rendering from MLP output</a> </li> </ul> <div> <a href="#optimizing-a-neural-radiance-field">Optimizing a Neural Radiance Field</a> </div> <ul> <li> <a href="#positional-encoding-pre-processing">Positional encoding (pre-processing)</a> </li> <li> <a href="#hierarchical-volume-sampling">Hierarchical volume sampling</a> </li> <li> <a href="#implementation-details-loss">Implementation details &amp; Loss</a> </li> </ul> <div> <a href="#results">Results</a> </div> <ul> <li> <a href="#datasets">Datasets</a> </li> <li> <a href="#measurement">Measurement</a> </li> <li> <a href="#comparisons">Comparisons</a> </li> <li> <a href="#discussion">Discussion</a> </li> <li> <a href="#ablation-studies">Ablation studies</a> </li> </ul> <div> <a href="#conclusion">Conclusion</a> </div> <div> <a href="#future-work">Future Work</a> </div> </nav> </d-contents> <h2 id="nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h2> <h4 id="ben-mildenhall-pratul-psrinivasan-matthew-tancik">Ben Mildenhall, Pratul P.Srinivasan, Matthew Tancik</h4> <blockquote> <p>paper :<br> <a href="https://arxiv.org/abs/2003.08934" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2003.08934</a><br> project website :<br> <a href="https://www.matthewtancik.com/nerf" rel="external nofollow noopener" target="_blank">https://www.matthewtancik.com/nerf</a><br> pytorch code :<br> <a href="https://github.com/yenchenlin/nerf-pytorch" rel="external nofollow noopener" target="_blank">https://github.com/yenchenlin/nerf-pytorch</a><br> <a href="https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file" rel="external nofollow noopener" target="_blank">https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file</a><br> tiny tensorflow code :<br> <a href="https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb" rel="external nofollow noopener" target="_blank">https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb</a><br> referenced blog :<br> <a href="https://csm-kr.tistory.com/64" rel="external nofollow noopener" target="_blank">https://csm-kr.tistory.com/64</a><br> <a href="https://yconquesty.github.io/blog/ml/nerf/nerf_rendering.html#the-rendering-formula" rel="external nofollow noopener" target="_blank">https://yconquesty.github.io/blog/ml/nerf/nerf_rendering.html#the-rendering-formula</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>여러 각도의 camera center에서 each input image pixel 방향으로 ray(r=o+td)를 쏜다.</li> <li>ray를 discrete points로 sampling한다.</li> <li>3D coordinate x와 viewing direction d를 r(x)와 r(d)로 positional encoding한다.</li> <li>r(x)를 MLP에 넣어 volume density를 얻고 여기에 r(d)까지 넣어 RGB color를 얻는다.</li> <li>coarse network와 fine network(hierarchical sampling) 각각에서 volume density와 color를 이용한 volume rendering으로 ray마다 rendering pixel color를 구한다.</li> </ol> </blockquote> <h2 id="introduction">Introduction</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/3-480.webp 480w,/assets/img/2024-04-10-NeRF/3-800.webp 800w,/assets/img/2024-04-10-NeRF/3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="pipeline">Pipeline</h3> <ul> <li> <p>(a) march camera rays and generate sampling of 5D coordinates</p> </li> <li> <p>(b) represents volumetric static scene by optimizing continuous 5D function(fully-connected network)</p> </li> </ul> <ol> <li>input: single continuous <code class="language-plaintext highlighter-rouge">5D coordinate</code><br> 3D location \(x, y, z\)<br> 2D direction \(\theta, \phi\)</li> <li>output:<br> <code class="language-plaintext highlighter-rouge">volume density</code> (differential opacity) (how much radiance is accumulated by a ray)<br> <code class="language-plaintext highlighter-rouge">view-dependent RGB color</code> (emitted radiance) \(c = (r, g, b)\)</li> </ol> <ul> <li> <p>(c) synthesizes novel view by classic <code class="language-plaintext highlighter-rouge">volume rendering</code> techniques(differentiable) to accumulate(project)(composite) the color/density samples into 2D image along rays</p> </li> <li> <p>(d) loss between synthesized and GT observed images</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/2-480.webp 480w,/assets/img/2024-04-10-NeRF/2-800.webp 800w,/assets/img/2024-04-10-NeRF/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Pipeline of NeRF architecture </div> <h3 id="problem--solution">Problem &amp; Solution</h3> <p>Problem :</p> <ol> <li>not sufficiently high-resolution representation</li> <li>inefficient in the number of samples per camera ray</li> </ol> <p>Solution :</p> <ol> <li>input <code class="language-plaintext highlighter-rouge">positional encoding</code> for MLP to represent higher frequency function</li> <li> <code class="language-plaintext highlighter-rouge">hierarchical sampling</code> to reduce the number of queries</li> </ol> <h3 id="contribution">Contribution</h3> <ul> <li>represent continuous scenes as 5D neural radiance fields with basic MLP to render high-resolution novel views</li> <li>differentiable volume rendering + hierarchical sampling</li> <li>positional encoding to map input 5D coordinate into higher dim. space for high-frequency scene representation</li> <li>overcome the storage costs of discretized voxel grids by encoding continuous volume into network’s parameters<br> =&gt; require only storage costs of sampled volumetric representations</li> </ul> <h2 id="related-work">Related Work</h2> <h3 id="neural-3d-shape-representation">Neural 3D shape representation</h3> <ul> <li> <p>deep networks that map \(xyz\) coordinates to signed distance functions or occupancy fields<br> =&gt; limit : need GT 3D geometry</p> </li> <li> <p>Niemeyer et al.<br> =&gt; input : find directly surface intersection for each ray (can calculate exact derivatie)<br> =&gt; output : diffuse color at each ray intersection location</p> </li> <li> <p>Sitzmann et al.<br> =&gt; input : each 3D coordinate<br> =&gt; output : feature vector and RGB color at each 3D coordinate<br> =&gt; rendering by RNN that marches along each ray to decide where the surface is.</p> </li> </ul> <blockquote> <p>Limit : oversmoothed renderings, so limited to simple shapes with low geometric complexity</p> </blockquote> <h3 id="view-synthesis-and-image-based-rendering">View synthesis and image-based rendering</h3> <ul> <li> <p>Given <code class="language-plaintext highlighter-rouge">dense sampling of views</code>, novel view synthesis is possible by <code class="language-plaintext highlighter-rouge">simple light field sample interpolation</code></p> </li> <li> <p>Given <code class="language-plaintext highlighter-rouge">sparser sampling of views</code>, there are 2 ways :<br> mesh-based representation and volumetric representation</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Mesh-based</code> representation with either diffuse(난반사) or view-dependent appearance :<br> Directly optimize mesh representations by differentiable rasterizers or pathtracers so that we reproject and reconstruct images</p> </li> </ul> <blockquote> <p>Limit :<br> gradient-based optimization is often difficult because of <code class="language-plaintext highlighter-rouge">local minima or discontinuities or poor loss landscape</code><br> mesh 구조를 유지하면서 <code class="language-plaintext highlighter-rouge">gradient-based optimization하는 게 어렵</code><br> needs a <code class="language-plaintext highlighter-rouge">template mesh</code> with fixed topology for initialization, which is unavailable in real-world</p> </blockquote> <ul> <li> <code class="language-plaintext highlighter-rouge">Volumetric</code> representation :<br> well-suited for gradient-based optimization and less distracting artifacts<br> train : predict a sampled volumetric representation (voxel grids) from input images<br> test : use alpha-(or learned-)compositing along rays to render novel views<br> (alpha-compositing : 아래 volume rendering section에서 설명 예정)<br> CNN compensates discretization artifacts from low resolution voxel grids or CNN allows voxel grids to vary on input time</li> </ul> <blockquote> <p>Limit :<br> good results, but limited by poor time, space complexity due to discrete sampling<br> \(\rightarrow\) discrete sampling : rendering high resol. image =&gt; finer sampling of 3D space</p> </blockquote> <blockquote> <p>Author’s solution :<br> encode <code class="language-plaintext highlighter-rouge">continuous</code> volume into network’s parameters<br> =&gt; higher quality rendering + require only storage cost of those <code class="language-plaintext highlighter-rouge">sampled</code> volumetric representations</p> </blockquote> <h2 id="neural-radiance-field-scene-representation">Neural Radiance Field Scene Representation</h2> <p>represent continuous scene by 5D MLP : (x, d) =&gt; (c, \(\sigma\))</p> <p>Here, there are 2 key-points!</p> <blockquote> <p>multiview consistent :<br> c is dependent on both x and d, but \(\sigma\) is only dependent on location x<br> 3D coordinate x =&gt; 8 fc-layers =&gt; volume-density and 256-dim. feature vector</p> </blockquote> <blockquote> <p>Lambertian reflection : diffuse(난반사) vs Specular reflection : 전반사</p> </blockquote> <blockquote> <p>non-Lambertian effects : view-dependent color change to represent specular reflection<br> feature vector is concatenated with direction d =&gt; 1 fc-layer =&gt; view-dependent RGB color</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/6-480.webp 480w,/assets/img/2024-04-10-NeRF/6-800.webp 800w,/assets/img/2024-04-10-NeRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="volume-rendering-with-radiance-fields">Volume Rendering with Radiance Fields</h2> <h3 id="ray-from-input-image-pre-processing">Ray from input image (pre-processing)</h3> <p>We use <code class="language-plaintext highlighter-rouge">Ray</code> to synthesize <code class="language-plaintext highlighter-rouge">continuous</code>-viewpoint images from <code class="language-plaintext highlighter-rouge">discrete</code> input images</p> <blockquote> <p>\(r(t) = o + td\)<br> o : camera’s center of projection<br> d : viewing direction<br> t \(\in [ t_n , t_f ]\) : distance from camera center b.w. camera’s predefined near and far planes</p> </blockquote> <blockquote> <p>How to calculate viewing direction d??</p> <ul> <li>2D pixel coordinate : \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)</li> <li>2D normalized coordinate (\(z = 1\)) by intrinsic matrix :<br> \(\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\) = \(K^{-1}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\) = \(\begin{bmatrix} 1/f_x &amp; 0 &amp; -\frac{1}{f_x}\frac{W}{2} \\ 0 &amp; 1/f_y &amp; -\frac{1}{f_y}\frac{H}{2} \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)<br> Since \(y, z\) have opposite direction between the real-world coordinate and pixel coordinate, we multiply (-1)<br> \(\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\) = \(\begin{bmatrix} 1/f_x &amp; 0 &amp; -\frac{1}{f_x}\frac{W}{2} \\ 0 &amp; -1/f_y &amp; \frac{1}{f_y}\frac{H}{2} \\ 0 &amp; 0 &amp; -1 \end{bmatrix}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)<br> Here, focal length in intrinsic matrix K is usually calculated using camear angle \(\alpha\) as \(\tan{\alpha / 2} = \frac{h/2}{f}\)</li> <li>3D coordinate by extrinsic matrix :<br> For extrinsic matrix \([R \vert t']\),<br> \(o = t'\)<br> \(d = R * \begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\)<br> Therefore, we can obtain \(r(t) = o + td\)</li> </ul> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/9-480.webp 480w,/assets/img/2024-04-10-NeRF/9-800.webp 800w,/assets/img/2024-04-10-NeRF/9-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="volume-rendering-from-mlp-output">Volume Rendering from MLP output</h3> <p>We use differential classical volume rendering</p> <blockquote> <p>Let ray \(r\) (traced through desired virtual camera) have near and far bounds \(t_n, t_f\)<br> expected color of ray \(r\) = \(C(r) = \int_{t_n}^{t_f} T(t) \sigma (r(t)) c(r(t), d) dt\)</p> </blockquote> <ul> <li>\(T(t) = \exp(- \int_{t_n}^{t} \sigma (r(s)) ds)\) :<br> <code class="language-plaintext highlighter-rouge">transmittance</code><br> transmittance = 투과도 = <code class="language-plaintext highlighter-rouge">CDF</code> that a ray <code class="language-plaintext highlighter-rouge">does not hit</code> any particles from \(t_n\) to \(t\)<br> 투과도가 클수록, 시작점 \(t_n\)부터 현재 \(t\)까지 ray가 물체의 방해를 받지 않고 통과함</li> <li>\(\sigma (r(t))\) : <code class="language-plaintext highlighter-rouge">volume density</code> along the ray (learned by MLP)<br> volume density = <code class="language-plaintext highlighter-rouge">opacity</code> = 불투명도 = <code class="language-plaintext highlighter-rouge">extinction coefficient</code> = <code class="language-plaintext highlighter-rouge">alpha value</code> for alpha-compositing</li> <li>\(c(r(t), d)\) : object’s <code class="language-plaintext highlighter-rouge">color</code> along the ray (learned by MLP)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/10-480.webp 480w,/assets/img/2024-04-10-NeRF/10-800.webp 800w,/assets/img/2024-04-10-NeRF/10-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/11-480.webp 480w,/assets/img/2024-04-10-NeRF/11-800.webp 800w,/assets/img/2024-04-10-NeRF/11-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <blockquote> <p>volume rendering 식 유도 과정</p> </blockquote> <p>occluding objects are modeled as spherical particles with radius \(r\)<br> There are \(A \cdot \Delta z \cdot \rho (z)\)개의 particles in the slice where \(\rho (z)\) is particle density (the number of particles per unit volume)</p> <p>Since solid particles do not overlap for \(\Delta z \rightarrow 0\),<br> \(A \cdot \Delta z \cdot \rho (z) \cdot \pi r^2\)만큼 area is occluded<br> 즉, cross section \(A\)에서 \(\frac{A \cdot \Delta z \cdot \rho (z) \cdot \pi r^2}{A} = \pi r^2 \cdot \rho (z) \cdot \Delta z\)의 비율만큼 occluded</p> <p>If \(\frac{A \cdot \Delta z \cdot \rho (z) \cdot \pi r^2}{A}\)만큼 rays are occluded, the light intensity decreases as<br> \(I(z + \Delta z) = (1 - \pi r^2 \rho (z) \Delta z) \times I(z)\)</p> <p>Then the light density difference \(\Delta I = I(z + \Delta z) - I(z) = - \pi r^2 \rho (z) \Delta z \cdot I(z)\)<br> 즉, \(dI(z) = - \pi r^2 \rho (z) I(z) dz = - \sigma (z) I(z) dz\)<br> where <code class="language-plaintext highlighter-rouge">volume density (or opacity)</code> is \(\sigma(z) = \pi r^2 \rho (z)\)<br> It makes sense because particle area와 particle density(particle 수)가 클수록 ray 감소량 (volume density)이 커지기 때문<br> ODE 풀면, \(I(z) = I(z_0)\exp(- \int_{z_0}^{z} \sigma (r(s)) ds)\)</p> <p>Let’s define transmittance \(T(z) = \exp(- \int_{z_0}^{z} \sigma (r(s)) ds)\)<br> where \(I(z) = I(z_0)T(z)\) means the <code class="language-plaintext highlighter-rouge">remainning</code> intensity after rays travel from \(z_0\) to \(z\)<br> where <code class="language-plaintext highlighter-rouge">transmittance</code> \(T(z)\) means <code class="language-plaintext highlighter-rouge">CDF</code> that a ray <code class="language-plaintext highlighter-rouge">does not hit</code> any particles from \(z_0\) to \(z\)</p> <p>If a ray passes empty space, there is no color<br> If a ray hits particles, there exists color (<code class="language-plaintext highlighter-rouge">radiance is emitted</code>)<br> Let’s define \(H(z) = 1 - T(z)\), which means CDF that a ray <code class="language-plaintext highlighter-rouge">hits</code> particles from \(z_0\) to \(z\)<br> CDF를 미분하면 PDF이므로<br> Then PDF is \(p_{hit}(z) = \frac{dH}{dz} = - \frac{dT}{dz} = \exp(- \int_{z_0}^{z} \sigma (r(s)) ds) \sigma (z) = T(z) \sigma (z)\)</p> <p>Let a random variable \(R\) be the emitted randiance.<br> Then PDF \(p_R(ray) = P[R = c(z)] = p_{hit}(z) = T(z) \sigma (z)\)<br> Then the <code class="language-plaintext highlighter-rouge">color of a pixel is expected radiance for ray</code> bounded from \(t_n\) to \(t_f\)<br> \(C(ray) = E[R] = \int_{t_n}^{t_f} R \cdot p_R dz = \int_{t_n}^{t_f} c \cdot p_{hit} dz = \int_{t_n}^{t_f} T(z) \sigma (z) c(z) dz\)</p> <p>\(t_n, t_f = 0., 1.\) for scaled-bounded and front-facing scenes after conversion to <code class="language-plaintext highlighter-rouge">NDC (normalized device coordinates)</code><br> NDC에 대한 설명은 따로 정리한 블로그 글 <a href="https://semyeong-yu.github.io/blog/2024/NDC/">How NDC Works?</a> 참고</p> <blockquote> <p>To apply the equation to our model by numerical quadrature,<br> we have to sample discrete points from continuous ray</p> </blockquote> <p>Instead of deterministic quadrature(typically used for rendering voxel grids, but may limit resolution),<br> author divides a ray \(\left[t_n, t_f\right]\) into N = 64 bins(intervals), and chooses one point \(t_i\) for each bin by uniform sampling<br> \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N}(t_f - t_n), t_n + \frac{i}{N}(t_f - t_n)\right]\)</p> <p>Although we use discrete N samples, <code class="language-plaintext highlighter-rouge">stratified sampling(층화 표집)</code> enables MLP to be evaluated at continuous positions by optimization</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/5-480.webp 480w,/assets/img/2024-04-10-NeRF/5-800.webp 800w,/assets/img/2024-04-10-NeRF/5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <blockquote> <p>Discretized version for N samples by Numerical Quadrature :<br> expected color \(\hat{C}(r) = \sum_{i=1}^{N} T_i (1 - \exp(- \sigma_{i} \delta_{i})) c_i\)</p> </blockquote> <ul> <li>\(\hat{C}(r)\) : differentiable about (\(\sigma_{i}, c_i\))</li> <li>\(\delta_{j} = t_{j+1} - t_j\) : the distance between adjacent samples</li> <li>\(T(t) = \exp(- \int_{t_n}^{t} \sigma (r(s)) ds) ~~ \rightarrow ~~ T_i = \exp(- \sum_{j=1}^{i-1} \sigma_{j} \delta_{j})\) where T_1 = 1</li> <li>\(\sigma (r(t)) dt ~~ \rightarrow ~~ \sigma_{j} \delta_{j} = 1 - \exp(-\sigma_{j} \delta_{j})\) by Taylor Series Expansion</li> <li> \[c(r(t), d) ~~ \rightarrow ~~ c_i\] </li> </ul> <p>또는</p> <p>\(p_{hit}(z_i) = \frac{dH}{dz} |_{z_i} ~~ \rightarrow ~~ H(z_{i+1}) - H(z_i) = (1 - T(z_{i+1})) - (1 - T(z_i)) = T(z_i) - T(z_{i+1}) = e^{- \sum_{j=1}^{i-1} \sigma_{j} \delta_{j}} - e^{- \sum_{j=1}^{i} \sigma_{j} \delta_{j}} = T(z_i)(1 - e^{- \sigma_{i} \delta_{i}})\)<br> Then the <code class="language-plaintext highlighter-rouge">color of a pixel is expected radiance for ray</code> bounded from \(t_n\) to \(t_f\)<br> \(\hat{C}(ray) = E[R] = \int_{t_n}^{t_f} R \cdot p_R dz ~~ \rightarrow ~~ \sum_{i=1}^{N} c_i \cdot p_{hit}(z_i) dz = \sum_{i=1}^{N} c_i T_i (1 - \exp(- \sigma_{i} \delta_{i}))\)</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">Final version</code> :<br> expected color \(\hat{C}(r) = \sum_{i=1}^{N} T_i \alpha_{i} c_i\)<br> where \(T_i = \prod_{j=1}^{i-1} (1-\alpha_{j})\) and \(\alpha_{i} = 1 - \exp(-\sigma_{i} \delta_{i})\)<br> which reduces to traditional <code class="language-plaintext highlighter-rouge">alpha-compositing</code> problem</p> </blockquote> <p>이 때, this volume rendering 식은 <code class="language-plaintext highlighter-rouge">differentiable</code>하므로 end-to-end learning 가능!!<br> a sequence of samples \(\boldsymbol t = {t_1, t_2, \ldots, t_N}\)에 대해<br> \(\frac{d\hat{C}}{dc_i} |_{\boldsymbol t} = T_i \alpha_{i}\) \(\frac{d\hat{C}}{d \sigma_{i}} |_{\boldsymbol t} = c_i \times (\frac{dT_i}{d \sigma_{i}} \alpha_{i} + \frac{d \alpha_{i}}{d \sigma_{i}} T_i) = c_i \times (0 + \delta_{i}e^{-\sigma_{i}\delta_{i}} T_i) = \delta_{i} T_i c_i e^{- \sigma_{i} \delta_{i}}\)</p> <blockquote> <p>alpha-compositing :<br> 여러 frame을 합쳐서 하나의 image로 합성하는 과정으로, 각 frame pixel마다 alpha 값(불투명도 값)(0~1)이 있어 겹치는 부분의 pixel 값을 결정</p> </blockquote> <p>By divide-and-conquer approach (tail recursion),<br> \(c = \alpha_{1}c_{1} + (1 - \alpha_{1})(\alpha_{2}c_{2} + (1 - \alpha_{2})(\cdots)) = \alpha_{1}c_{1} + (1 - \alpha_{1})\alpha_{2}c_{2} + (1 - \alpha_{1})(1 - \alpha_{2})(\cdots) = \cdots = \sum_{i=1}^{N}(\alpha_{i}c_{i}\prod_{j=1}^{i-1}(1-\alpha_{j}))\) where \(\alpha_{0} = 0\)</p> <p>If \(\alpha_{i} = 1 - \exp(-\sigma_{i} \delta_{i})\),<br> NeRF volume rendering 식 \(\hat{C}(r) = \sum_{i=1}^{N} T_i \alpha_{i} c_i\)과<br> alpha-compositing 식 \(c = \sum_{i=1}^{N}(\alpha_{i}c_{i}\prod_{j=1}^{i-1}(1-\alpha_{j}))\)은<br> SAME!!</p> <h2 id="optimizing-a-neural-radiance-field">Optimizing a Neural Radiance Field</h2> <h3 id="positional-encoding-pre-processing">Positional encoding (pre-processing)</h3> <p>kernel regression(dot product 및 더하기)을 사용하는 MLP의 특성상<br> If we use input directly, <code class="language-plaintext highlighter-rouge">MLP is biased to learn low-frequency function</code> (oversmoothed appearance) (no detail) (spectral bias)<br> So, low dim. input의 작은 변화에 대해 급격하게 변화하는 <code class="language-plaintext highlighter-rouge">high-frequency output은 학습 잘 못함</code></p> <p>Here, <code class="language-plaintext highlighter-rouge">fourier features</code> (sinusoidal signal은 input signal을 orthogonal space에서 표현 가능) let MLP learn high-frequency function in low-dim. domain <d-cite key="interpolation">[1]</d-cite><br> If we <code class="language-plaintext highlighter-rouge">map input into higher dim.</code> space which contains both low and high frequency info. by fourier features, MLP can fit data with <code class="language-plaintext highlighter-rouge">high-frequency variation</code><br> Due to positional encoding, MLP can behave as <code class="language-plaintext highlighter-rouge">interpolation function</code> where \(L\) determines the bandwidth of the interpolation kernel <d-cite key="interpolation">[1]</d-cite><br> \(r : R \rightarrow R^{2L}\) <br> \(r(p) = (sin(2^0\pi p), cos(2^0\pi p), \cdots, sin(2^{L-1}\pi p), cos(2^{L-1}\pi p))\)<br> \(L=10\) for \(r(x)\) where x has three coordinates<br> \(L=4\) for \(r(d)\) where d has three components of the cartesian viewing direction unit vector</p> <p>추가로, low-dim. input 정보를 high-frequency output에 반영하기 위해서는 kernel을 거친 뒤에도 orthogonal eigenvalue들이 많이 살아있어야 하는데 stationary kernel 또는 Spherical Harmonics가 이러한 역할 수행 가능</p> <h3 id="hierarchical-volume-sampling">Hierarchical volume sampling</h3> <p>Densely evaluating N points by stratified sampling is inefficient<br> =&gt; We don’t need much sampling at free space or occluded regions<br> =&gt; Hierarchical sampling enables us to allocate more samples to regions we expect to contain visible content</p> <p>We simultaneously optimize 2 networks with different sampling : <code class="language-plaintext highlighter-rouge">coarse</code> and <code class="language-plaintext highlighter-rouge">fine</code></p> <blockquote> <p>coarse sampling \(N_c\)개 : 위에서 배웠던 내용<br> author divides a ray into \(N_c\) = 64 bins(intervals), and chooses one point \(t_i\) for each bin by uniform sampling<br> \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N_c}(t_f - t_n), t_n + \frac{i}{N_c}(t_f - t_n)\right]\)</p> </blockquote> <blockquote> <p>fine sampling \(N_f\)개 : 새로운 내용<br> coarse sampling model’s output is a <code class="language-plaintext highlighter-rouge">weighted sum of all coarse-sampled colors</code><br> \(\hat{C}(r) = \sum_{i=1}^{N_c} T_i \alpha_{i} c_i = \sum_{i=1}^{N_c} w_i c_i\)<br> where we define \(w_i = T_i \alpha_{i} = T_i (1 - \exp(-\sigma_{i} \delta_{i}))\) for \(i=1,\cdots,N_c\)<br> =&gt; Given the output of <code class="language-plaintext highlighter-rouge">coarse</code> network, we try more informed (better) sampling where samples are biased toward the <code class="language-plaintext highlighter-rouge">relevant parts of the scene volume</code><br> =&gt; We sample \(N_f\)=128 <code class="language-plaintext highlighter-rouge">fine</code> points following a <code class="language-plaintext highlighter-rouge">piecewise-constant PDF</code> of normalized \(\frac{w_i}{\sum_{j=1}^{N_c} w_j}\)<br> =&gt; Here, we use <code class="language-plaintext highlighter-rouge">Inverse CDF Method</code> for sampling fine points</p> </blockquote> <blockquote> <p>Inverse transform sampling = Inverse CDF Method :<br> =&gt; PDF (probability density function) : \(f_X(x)\)<br> =&gt; CDF (cumulative distribution function) : \(F_X(x) = P(X \leq x) = \int_{-\infty}^x f_X(x) dx\)<br> idea : 모든 확률 분포의 CDF는 Uniform distribution을 따른다!<br> =&gt; 따라서 CDF의 y값을 Uniform sampling하면, 그 y에 대응되는 x에 대한 (특정 PDF를 따르는) sampling을 구현할 수 있다!<br> =&gt; 즉, CDF의 역함수를 계산할 수 있다면, 기본 난수 생성기인 Uniform sampling을 이용해서 확률 분포 X에 대한 sampling을 할 수 있다!<br> \(F_X(x)\) ~ \(U\left[0, 1\right]\)<br> \(X\) ~ \(F^{-1}(U\left[0, 1\right])\)</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/4-480.webp 480w,/assets/img/2024-04-10-NeRF/4-800.webp 800w,/assets/img/2024-04-10-NeRF/4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We evaluate <code class="language-plaintext highlighter-rouge">coarse</code> network using \(N_c\)=64 points per ray<br> We evaluate <code class="language-plaintext highlighter-rouge">fine</code> network using \(N_c+N_f\)=64+128=192 points per ray where we sample 128 fine points following PDF of <code class="language-plaintext highlighter-rouge">coarse</code> sampled points<br> In result, we use a total of 64+192=256 samples per ray to compute the final rendering color \(C(r)\)</p> <h3 id="implementation-details--loss">Implementation details &amp; Loss</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/6-480.webp 480w,/assets/img/2024-04-10-NeRF/6-800.webp 800w,/assets/img/2024-04-10-NeRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ol> <li>Prepare RGB images, corresponding camera poses, intrinsic parameters and scene bounds (use COLMAP structure-from-motion package to estimate these parameters)</li> <li>From H x W input image, randomly sample a batch of 4096 pixels</li> <li>calculate continuous ray from each pixel \(r(t) = o + kd\)</li> <li>coarse sampling of \(N_c\)=64 points per each ray \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N_c}(t_f - t_n), t_n + \frac{i}{N_c}(t_f - t_n)\right]\)</li> <li>positional encoding \(r(x)\) and \(r(d)\) for input</li> <li>obtain volume density \(\sigma\) by MLP with \(r(x, y, z)\) as input</li> <li>obtain color \(c\) by MLP with \(r(x, y, z)\) and \(r(\theta, \phi)\) as input</li> <li>obtain rendering color of each ray by volume rendering \(\hat{C}(r) = \sum_{i=1}^{N} T_i (1 - \exp(- \sigma_{i} \delta_{i})) c_i\) from two networks ‘coarse’ and ‘fine’</li> <li>compute loss</li> <li>Adam optimizer with learning rate from \(5 \times 10^{-4}\) to \(5 \times 10^{-5}\)</li> <li>optimization for a single scene typically takes around 100-300k iterations (1~2 days) to converge on a single NVIDIA V100 GPU</li> </ol> <p>Here, we use L2 norm for loss<br> \(L = \sum_{r \in R} \left[{\left\|\hat{C_c}(r)-C(r)\right\|}^2+{\left\|\hat{C_f}(r)-C(r)\right\|}^2\right]\)<br> \(C(r)\) : GT pixel RGB color<br> \(\hat{C_c}(r)\) : rendering RGB color from coarse network : to allocate better samples in fine network<br> \(\hat{C_f}(r)\) : rendering RGB color from fine network : our goal<br> \(R\) : the set of all pixels(rays) across all images</p> <h2 id="results">Results</h2> <h3 id="datasets">Datasets</h3> <p>Synthetic renderings of objects</p> <ul> <li>Diffuse Synthetic 360 : 4 Lambertian objects with simple geometry</li> <li>Realistic Synthetic 360 : 8 non-Lambertian objects with complicated geometry</li> </ul> <p>Real images of complex scenes</p> <ul> <li>Real Forward-Facing : 8 scenes captured with a handheld cellphone</li> </ul> <h3 id="measurement">Measurement</h3> <ul> <li>PSNR(Peak Signal-to-Noise Ratio) \(\uparrow\) : the ratio between the maximum possible power of a signal and the power of corrupting noise \(10\log_{10}\left(\frac{(MAX)^2}{MSE}\right)\)[dB]</li> <li>SSIM(Structural Similarity Index Map) \(\uparrow\) : compare image qualities in three ways: Lumincance(\(l\)), Contrast(\(c\)), Structural(\(s\))<br> SSIM(x, y) = \([l(x,y)]^{\alpha}[c(x,y)]^{\beta}[s(x,y)]^{\gamma}=\frac{(2\mu_{x}\mu_{y}+C_1)(2\sigma_{xy}+C_2)}{(\mu_{x}^2+\mu_{y}^2+C_1)(\sigma_{x}^2+\sigma_{y}^2+C_2)}\) where \(l(x,y)=\frac{(2\mu_{x}\mu_{y}+C_1)}{\mu_{x}^2+\mu_{y}^2+C_1}\) and \(c(x,y)=\frac{(2\sigma_{x}\sigma_{y}+C_2)}{\sigma_{x}^2+\sigma_{y}^2+C_2}\) and \(s(x,y)=\frac{\sigma_{xy}+C_3}{\sigma_{x}\sigma_{y}+C_3}\)<br> SSIM calculator :<br> https://darosh.github.io/image-ssim-js/test/browser_test.html</li> <li>LPIPS \(\downarrow\)</li> </ul> <h3 id="comparisons">Comparisons</h3> <ul> <li>Neural Volumes (NV) :<br> It synthsizes novel views of objects that lie entirely within a bounded volume in front of a distinct background.<br> It optimizes 3D conv. network to predict a discretized RGB\(\alpha\) voxel grid and a 3D warp grid.<br> It renders novel views by marching rays through the warped voxel grid</li> <li>Scene Representation Networks (SRN) :<br> It represents continuous scene as an opaque surface.<br> MLP maps each 3D coordinate to a feature vector, and we optimize RNN to predict the next step size along the ray using the feature vector.<br> The feature vector from the final step is decoded into a color for that point on the surface. Note that SRN is followup to DeepVoxels by the same authors.</li> <li>Local Light Field Fusion (LLFF) :<br> designed for producing novel views for well-sampled forward facing scenes<br> trained 3D conv. network directly predicts a discretized frustum-sampled RGB\(\alpha\) grid (multiplane image), and then renders novel views by alpha-compositing and blending</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/7-480.webp 480w,/assets/img/2024-04-10-NeRF/7-800.webp 800w,/assets/img/2024-04-10-NeRF/7-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Test for scenes from author's new synthetic dataset </div> <p>LLFF exhibits banding and ghosting artifacts<br> SRN produces blurry and distorted renderings<br> NV cannot capture the details<br> NeRF captures fine details in both geometry and appearance</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/8-480.webp 480w,/assets/img/2024-04-10-NeRF/8-800.webp 800w,/assets/img/2024-04-10-NeRF/8-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Test for read-world scenes </div> <p>LLFF may have repeated edges because of blending between multiple renderings<br> NeRF also correctly reconstruct partially-occluded regions<br> SRN does not capture any high-frequency fine detail</p> <h3 id="discussion">Discussion</h3> <h3 id="ablation-studies">Ablation Studies</h3> <h2 id="conclusion">Conclusion</h2> <p>prior : MLP outputs discretized voxel representations<br> author : MLP outputs volume density and view-dependent emitted radiance</p> <h2 id="future-work">Future Work</h2> <p>efficiency :<br> Rather than hierarchical sampling, there is still much more progress to be made for efficient optimization and rendering of neural radiance fields</p> <p>interpretability :<br> voxel grids or meshes admits reasoning about the expected quality, but it is unclear to analyze these issues when we encode scenes into the weights of MLP</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-04-10-NeRF.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"semyeong-yu/semyeong-yu.github.io","data-repo-id":"R_kgDOLmVJXQ","data-category":"Comments","data-category-id":"DIC_kwDOLmVJXc4CeSwc","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Semyeong Yu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5YMLX9VHFX"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5YMLX9VHFX");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>