<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> NeRF | Semyeong Yu </title> <meta name="author" content="Semyeong Yu"> <meta name="description" content="representing scenes as neural radiance fields for view synthesis"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@/css/mdb.min.css" integrity="" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://semyeong-yu.github.io/blog/2024/NeRF/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@/dist/medium-zoom.min.js" integrity="" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@{{version}}/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@{{version}}/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v{{version}}/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "NeRF",
            "description": "representing scenes as neural radiance fields for view synthesis",
            "published": "April 10, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Semyeong</span> Yu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>NeRF</h1> <p>representing scenes as neural radiance fields for view synthesis</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <ul> <li> <a href="#pipeline">Pipeline</a> </li> <li> <a href="#problem-solution">Problem &amp; Solution</a> </li> <li> <a href="#contribution">Contribution</a> </li> </ul> <div> <a href="#related-work">Related Work</a> </div> <ul> <li> <a href="#neural-3d-shape-representation">Neural 3D shape representation</a> </li> <li> <a href="#view-synthesis-and-image-based-rendering">View synthesis and image-based rendering</a> </li> </ul> <div> <a href="#neural-radiance-field-scene-representation">Neural Radiance Field Scene Representation</a> </div> <div> <a href="#volume-rendering-with-radiance-fields">Volume Rendering with Radiance Fields</a> </div> <ul> <li> <a href="#ray-from-input-image-pre-processing">Ray from input image (pre-processing)</a> </li> <li> <a href="#volume-rendering-from-mlp-output">Volume Rendering from MLP output</a> </li> </ul> <div> <a href="#optimizing-a-neural-radiance-field">Optimizing a Neural Radiance Field</a> </div> <ul> <li> <a href="#positional-encoding-pre-processing">Positional encoding (pre-processing)</a> </li> <li> <a href="#hierarchical-volume-sampling">Hierarchical volume sampling</a> </li> <li> <a href="#implementation-details-loss">Implementation details &amp; Loss</a> </li> </ul> <div> <a href="#results">Results</a> </div> <ul> <li> <a href="#datasets">Datasets</a> </li> <li> <a href="#measurement">Measurement</a> </li> <li> <a href="#comparisons">Comparisons</a> </li> <li> <a href="#discussion">Discussion</a> </li> <li> <a href="#ablation-studies">Ablation studies</a> </li> </ul> <div> <a href="#conclusion">Conclusion</a> </div> <div> <a href="#future-work">Future Work</a> </div> </nav> </d-contents> <h2 id="nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h2> <h4 id="ben-mildenhall-pratul-psrinivasan-matthew-tancik">Ben Mildenhall, Pratul P.Srinivasan, Matthew Tancik</h4> <blockquote> <p>paper :<br> <a href="https://arxiv.org/abs/2003.08934" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2003.08934</a><br> project website :<br> <a href="https://www.matthewtancik.com/nerf" rel="external nofollow noopener" target="_blank">https://www.matthewtancik.com/nerf</a><br> pytorch code :<br> <a href="https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file" rel="external nofollow noopener" target="_blank">https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file</a><br> tiny tensorflow code :<br> <a href="https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb" rel="external nofollow noopener" target="_blank">https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb</a><br> referenced blog :<br> <a href="https://csm-kr.tistory.com/64" rel="external nofollow noopener" target="_blank">https://csm-kr.tistory.com/64</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ol> <li>여러 각도에서 찍은 input images에서 ray(r=o+td)를 쏜다.</li> <li>ray를 discrete points로 sampling한다.</li> <li>3D coordinate x와 viewing direction d를 r(x)와 r(d)로 positional encoding한다.</li> <li>r(x)를 MLP에 넣어 volume density를 얻고 여기에 r(d)까지 넣어 RGB color를 얻는다.</li> <li>coarse network와 fine network(hierarchical sampling) 각각에서 volume density와 color를 이용한 volume rendering으로 ray마다 rendering pixel color를 구한다.</li> </ol> </blockquote> <h2 id="introduction">Introduction</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/3-480.webp 480w,/assets/img/2024-04-10-NeRF/3-800.webp 800w,/assets/img/2024-04-10-NeRF/3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="pipeline">Pipeline</h4> <ul> <li> <p>(a) march camera rays and generate sampling of 5D coordinates</p> </li> <li> <p>(b) represents volumetric static scene by optimizing continuous 5D function(fully-connected network)</p> </li> </ul> <ol> <li>input: single continuous <code class="language-plaintext highlighter-rouge">5D coordinate</code><br> 3D location \(x, y, z\)<br> 2D direction \(\theta, \phi\)</li> <li>output:<br> <code class="language-plaintext highlighter-rouge">volume density</code> (differential opacity) (how much radiance is accumulated by a ray)<br> <code class="language-plaintext highlighter-rouge">view-dependent RGB color</code> (emitted radiance) \(c = (r, g, b)\)</li> </ol> <ul> <li> <p>(c) synthesizes novel view by classic <code class="language-plaintext highlighter-rouge">volume rendering</code> techniques(differentiable) to accumulate(project)(composite) the color/density samples into 2D image along rays</p> </li> <li> <p>(d) loss between synthesized and GT observed images</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/2-480.webp 480w,/assets/img/2024-04-10-NeRF/2-800.webp 800w,/assets/img/2024-04-10-NeRF/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Pipeline of NeRF architecture </div> <h4 id="problem--solution">Problem &amp; Solution</h4> <p>Problem :</p> <ol> <li>not sufficiently high-resolution representation</li> <li>inefficient in the number of samples per camera ray</li> </ol> <p>Solution :</p> <ol> <li>input <code class="language-plaintext highlighter-rouge">positional encoding</code> for MLP to represent higher frequency function</li> <li> <code class="language-plaintext highlighter-rouge">hierarchical sampling</code> to reduce the number of queries</li> </ol> <h4 id="contribution">Contribution</h4> <ul> <li>represent continuous scenes as 5D neural radiance fields with basic MLP to render high-resolution novel views</li> <li>differentiable volume rendering + hierarchical sampling</li> <li>positional encoding to map input 5D coordinate into higher dim. space for high-frequency scene representation</li> <li>overcome the storage costs of discretized voxel grids by encoding continuous volume into network’s parameters<br> =&gt; require only storage costs of sampled volumetric representations</li> </ul> <h2 id="related-work">Related Work</h2> <h4 id="neural-3d-shape-representation">Neural 3D shape representation</h4> <ul> <li> <p>deep networks that map \(xyz\) coordinates to signed distance functions or occupancy fields<br> =&gt; limit : need GT 3D geometry</p> </li> <li> <p>Niemeyer et al.<br> =&gt; input : find directly surface intersection for each ray (can calculate exact derivatie)<br> =&gt; output : diffuse color at each ray intersection location</p> </li> <li> <p>Sitzmann et al.<br> =&gt; input : each 3D coordinate<br> =&gt; output : feature vector and RGB color at each 3D coordinate<br> =&gt; rendering by RNN that marches along each ray to decide where the surface is.</p> </li> </ul> <blockquote> <p>Limit : oversmoothed renderings, so limited to simple shapes with low geometric complexity</p> </blockquote> <h4 id="view-synthesis-and-image-based-rendering">View synthesis and image-based rendering</h4> <ul> <li> <p>Given dense sampling of views, novel view synthesis is possible by simple light field sample interpolation</p> </li> <li> <p>Given sparser sampling of views, there are 2 ways :<br> mesh-based representation and volumetric representation</p> </li> <li> <p>Mesh-based representation with either diffuse(난반사) or view-dependent appearance :<br> Directly optimize mesh representations by differentiable rasterizers or pathtracers so that we reproject and reconstruct images</p> </li> </ul> <blockquote> <p>Limit :<br> gradient-based optimization is often difficult because of local minima or poor loss landscape<br> needs a template mesh with fixed topology for initialization, which is unavailable in real-world</p> </blockquote> <ul> <li>Volumetric representation :<br> well-suited for gradient-based optimization and less distracting artifacts<br> train : predict a sampled volumetric representation (voxel grids) from input images<br> test : use alpha-(or learned-)compositing along rays to render novel views<br> \(\rightarrow\) alpha-compositing : 여러 frame을 합쳐서 하나의 image로 합성하는 과정으로, 각 이미지 픽셀마다 알파 값(불투명도 값)(0~1)이 있어 겹치는 부분의 알파 값 및 픽셀 값을 결정<br> CNN compensates discretization artifacts from low resolution voxel grids or CNN allows voxel grids to vary on input time</li> </ul> <blockquote> <p>Limit :<br> good results, but limited by poor time, space complexity due to discrete sampling<br> \(\rightarrow\) discrete sampling : rendering high resol. image =&gt; finer sampling of 3D space</p> </blockquote> <blockquote> <p>Author’s solution :<br> encode <code class="language-plaintext highlighter-rouge">continuous</code> volume into network’s parameters<br> =&gt; higher quality rendering + require only storage cost of those <code class="language-plaintext highlighter-rouge">sampled</code> volumetric representations</p> </blockquote> <h2 id="neural-radiance-field-scene-representation">Neural Radiance Field Scene Representation</h2> <p>represent continuous scene by 5D MLP : (x, d) =&gt; (c, \(\sigma\))</p> <p>Here, there are 2 key-points!</p> <blockquote> <p>multiview consistent :<br> c is dependent on both x and d, but \(\sigma\) is only dependent on location x<br> 3D coordinate x =&gt; 8 fc-layers =&gt; volume-density and 256-dim. feature vector</p> </blockquote> <blockquote> <p>Lambertian reflection : diffuse(난반사) vs Specular reflection : 전반사</p> </blockquote> <blockquote> <p>non-Lambertian effects : view-dependent color change to represent specular reflection<br> feature vector is concatenated with direction d =&gt; 1 fc-layer =&gt; view-dependent RGB color</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/6-480.webp 480w,/assets/img/2024-04-10-NeRF/6-800.webp 800w,/assets/img/2024-04-10-NeRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="volume-rendering-with-radiance-fields">Volume Rendering with Radiance Fields</h2> <h4 id="ray-from-input-image-pre-processing">Ray from input image (pre-processing)</h4> <p>We use <code class="language-plaintext highlighter-rouge">Ray</code> to synthesize <code class="language-plaintext highlighter-rouge">continuous</code>-viewpoint images from <code class="language-plaintext highlighter-rouge">discrete</code> input images</p> <blockquote> <p>\(r(t) = o + td\)<br> o : the location of camera<br> d : viewing direction</p> </blockquote> <blockquote> <p>How to calculate viewing direction d??</p> <ul> <li>pixel coordinate : \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)</li> <li>normalized coordinate by intrinsic matrix :<br> \(\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\) = \(K^{-1}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\) = \(\begin{bmatrix} 1/f_x &amp; 0 &amp; W/2 \\ 0 &amp; 1/f_y &amp; H/2 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)<br> Since y, z have opposite direction between the real-world coordinate and pixel coordinate, we multiply (-1)<br> \(\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\) = \(\begin{bmatrix} 1/f_x &amp; 0 &amp; W/2 \\ 0 &amp; -1/f_y &amp; H/2 \\ 0 &amp; 0 &amp; -1 \end{bmatrix}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)<br> Here, focal length in intrinsic matrix K is usually calculated using camear angle \(\alpha\) as \(\tan{\alpha / 2} = \frac{h/2}{f}\)</li> <li>3D coordinate by extrinsic matrix :<br> For extrinsic matrix \([R \vert t']\),<br> \(o = t'\)<br> \(d = R * \begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\)<br> Therefore, we can obtain \(r(t) = o + td\)</li> </ul> </blockquote> <h4 id="volume-rendering-from-mlp-output">Volume Rendering from MLP output</h4> <p>We use differential classical volume rendering</p> <blockquote> <p>idea : 빛이 들어올 확률이 클수록(채도?), 물체의 밀도가 높을수록(명도?) 2D 이미지 픽셀에서 색상 채널의 값이 크게 나타나므로<br> pixel’s color along a ray is equal to the integral of (transmittance) x (volume density) x (color) for all points along one ray.</p> </blockquote> <p>For ray \(r\) traced through desired virtual camera and near, far bounds \(t_n\), \(t_f\),<br> expected color of ray \(r\) = \(C(r) = \int_{t_n}^{t_f} T(t) \sigma (r(t)) c(r(t), d) dt\)</p> <ul> <li>\(T(t) = \exp(- \int_{t_n}^{t} \sigma (r(s)) ds)\) :<br> accumulated transmittance along ray from \(t_n\) to \(t\)<br> the probability that ray travels from \(t_n\) to \(t\) without hitting any particle<br> 투과도가 클수록 투명<br> T(t) is the solution of \(\frac{dy}{dt} = - \sigma (x)y\)<br> since T(t)’s rate of decrease is proportional to T(t) itself and volume density</li> <li>\(\sigma (r(t))\) : volume density along the ray (learned by MLP)</li> <li>\(c(r(t), d)\) : object’s color along the ray (learned by MLP)</li> </ul> <p>To apply the equation to our model, we have to do sampling from continuous ray to discrete points<br> Instead of deterministic quadrature(typically used for rendering voxel grids, but may limit resolution),<br> author divides a ray \(\left[t_n, t_f\right]\) into N = 64 bins(intervals), and chooses one point \(t_i\) for each bin by uniform sampling<br> \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N}(t_f - t_n), t_n + \frac{i}{N}(t_f - t_n)\right]\)<br> Although we use discrete N samples, stratified sampling(층화 표집) enables MLP to be evaluated at continuous positions by optimization</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/5-480.webp 480w,/assets/img/2024-04-10-NeRF/5-800.webp 800w,/assets/img/2024-04-10-NeRF/5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>And then discretized version for N samples :<br> expected color \(\hat{C}(r) = \sum_{i=1}^{N} T_i (1 - \exp(- \sigma_{i} \delta_{i})) c_i\)</p> <ul> <li>\(\hat{C}(r)\) : differentiable about (\(\sigma_{i}, c_i\))</li> <li>\(\delta_{j} = t_{j+1} - t_j\) : the distance between adjacent samples</li> <li>\(\sigma (r(t)) dt ~~ =&gt; ~~ \sigma_{j} \delta_{j} = 1 - \exp(-\sigma_{j} \delta_{j})\) by Taylor Series Expansion</li> <li> \[T(t) = \exp(- \int_{t_n}^{t} \sigma (r(s)) ds) ~~ =&gt; ~~ T_i = \exp(- \sum_{j=1}^{i-1} \sigma_{j} \delta_{j})\] </li> <li> \[c(r(t), d) ~~ =&gt; ~~ c_i\] </li> </ul> <p>Here, in volume rendering, author uses <code class="language-plaintext highlighter-rouge">volume density</code><br> for 불투명도 == opacity == extinction coefficient == alpha value for alpha-compositing</p> <blockquote> <p>Final version :<br> expected color \(\hat{C}(r) = \sum_{i=1}^{N} T_i \alpha_{i} c_i\)<br> where \(T_i = \prod_{j=1}^{i-1} (1-\alpha_{j})\) and \(\alpha_{j} = 1 - \exp(-\sigma_{j} \delta_{j})\)<br> which reduces to traditional alpha-compositing problem</p> </blockquote> <h2 id="optimizing-a-neural-radiance-field">Optimizing a Neural Radiance Field</h2> <h4 id="positional-encoding-pre-processing">Positional encoding (pre-processing)</h4> <p>If we use input directly, MLP is biased to learn low-frequency function (oversmoothed appearance)<br> If we map input into higher dim. space, MLP can fit data with high-frequency variation<br> \(r : R \rightarrow R^{2L}\) <br> \(r(p) = (sin(2^0\pi p), cos(2^0\pi p), \cdots, sin(2^{L-1}\pi p), cos(2^{L-1}\pi p))\)<br> \(L=10\) for \(r(x)\) where x has three coordinates<br> \(L=4\) for \(r(d)\) where d has three components of the cartesian viewing direction unit vector</p> <h4 id="hierarchical-volume-sampling">Hierarchical volume sampling</h4> <p>Densely evaluating N points by stratified sampling is inefficient<br> =&gt; We don’t need much sampling at free space or occluded regions<br> =&gt; Hierarchical sampling enables us to allocate more samples to regions we expect to contain visible content</p> <p>We simultaneously optimize 2 networks with different sampling : <code class="language-plaintext highlighter-rouge">coarse</code> and <code class="language-plaintext highlighter-rouge">fine</code></p> <blockquote> <p>coarse sampling \(N_c\)개 : 위에서 배웠던 내용<br> author divides a ray into \(N_c\) = 64 bins(intervals), and chooses one point \(t_i\) for each bin by uniform sampling<br> \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N_c}(t_f - t_n), t_n + \frac{i}{N_c}(t_f - t_n)\right]\)</p> </blockquote> <blockquote> <p>fine sampling \(N_f\)개 : 새로운 내용<br> coarse sampling model’s output is a weighted sum of all coarse-sampled colors<br> \(\hat{C}(r) = \sum_{i=1}^{N_c} T_i \alpha_{i} c_i = \sum_{i=1}^{N_c} w_i c_i\)<br> where we define \(w_i = T_i \alpha_{i} = T_i (1 - \exp(-\sigma_{i} \delta_{i}))\) for \(i=1,\cdots,N_c\)<br> =&gt; Given the output of <code class="language-plaintext highlighter-rouge">coarse</code> network, we try more informed sampling where samples are biased toward the relevant parts of the volume<br> =&gt; We sample \(N_f\)=128 <code class="language-plaintext highlighter-rouge">fine</code> points following a piecewise-constant PDF of normalized \(\frac{w_i}{\sum_{j=1}^{N_c} w_j}\)<br> =&gt; Here, we use Inverse CDF Method for sampling fine points</p> </blockquote> <blockquote> <p>Inverse transform sampling = Inverse CDF Method :<br> =&gt; PDF (probability density function) : \(f_X(x)\)<br> =&gt; CDF (cumulative distribution function) : \(F_X(x) = P(X \leq x) = \int_{-\infty}^x f_X(x) dx\)<br> idea : 모든 확률 분포의 CDF는 Uniform distribution을 따른다!<br> =&gt; 따라서 CDF의 y값을 Uniform sampling하면, 그 y에 대응되는 x에 대한 (특정 PDF를 따르는) sampling을 구현할 수 있다!<br> =&gt; 즉, CDF의 역함수를 계산할 수 있다면, 기본 난수 생성기인 Uniform sampling을 이용해서 확률 분포 X에 대한 sampling을 할 수 있다!<br> \(F_X(x)\) ~ \(U\left[0, 1\right]\)<br> \(X\) ~ \(F^{-1}(U\left[0, 1\right])\)</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/4-480.webp 480w,/assets/img/2024-04-10-NeRF/4-800.webp 800w,/assets/img/2024-04-10-NeRF/4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We evaluate <code class="language-plaintext highlighter-rouge">coarse</code> network using \(N_c\)=64 points per ray<br> We evaluate <code class="language-plaintext highlighter-rouge">fine</code> network using \(N_c+N_f\)=64+128=192 points per ray where we sample 128 points following PDF of <code class="language-plaintext highlighter-rouge">coarse</code> sampled points<br> In result, we use a total of 64+192=256 samples per ray to compute the final rendering color \(C(r)\)</p> <h4 id="implementation-details--loss">Implementation details &amp; Loss</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/6-480.webp 480w,/assets/img/2024-04-10-NeRF/6-800.webp 800w,/assets/img/2024-04-10-NeRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ol> <li>Prepare RGB images, corresponding camera poses, intrinsic parameters and scene bounds (use COLMAP structure-from-motion package to estimate these parameters)</li> <li>From H x W input image, randomly sample a batch of 4096 pixels</li> <li>calculate continuous ray from each pixel \(r(t) = o + kd\)</li> <li>coarse sampling of \(N_c\)=64 points per each ray \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N_c}(t_f - t_n), t_n + \frac{i}{N_c}(t_f - t_n)\right]\)</li> <li>positional encoding \(r(x)\) and \(r(d)\) for input</li> <li>obtain volume density \(\sigma\) by MLP with \(r(x, y, z)\) as input</li> <li>obtain color \(c\) by MLP with \(r(x, y, z)\) and \(r(\theta, \phi)\) as input</li> <li>obtain rendering color of each ray by volume rendering \(\hat{C}(r) = \sum_{i=1}^{N} T_i (1 - \exp(- \sigma_{i} \delta_{i})) c_i\) from two networks ‘coarse’ and ‘fine’</li> <li>compute loss</li> <li>Adam optimizer with learning rate from \(5 \times 10^{-4}\) to \(5 \times 10^{-5}\)</li> <li>optimization for a single scene typically takes around 100-300k iterations (1~2 days) to converge on a single NVIDIA V100 GPU</li> </ol> <p>Here, we use L2 norm for loss<br> \(L = \sum_{r \in R} \left[{\left\|\hat{C_c}(r)-C(r)\right\|}^2+{\left\|\hat{C_f}(r)-C(r)\right\|}^2\right]\)<br> \(C(r)\) : GT pixel RGB color<br> \(\hat{C_c}(r)\) : rendering RGB color from coarse network : to allocate better samples in fine network<br> \(\hat{C_f}(r)\) : rendering RGB color from fine network : our goal</p> <h2 id="results">Results</h2> <h4 id="datasets">Datasets</h4> <p>Synthetic renderings of objects</p> <ul> <li>Diffuse Synthetic 360 : 4 Lambertian objects with simple geometry</li> <li>Realistic Synthetic 360 : 8 non-Lambertian objects with complicated geometry</li> </ul> <p>Real images of complex scenes</p> <ul> <li>Real Forward-Facing : 8 scenes captured with a handheld cellphone</li> </ul> <h4 id="measurement">Measurement</h4> <ul> <li>PSNR(Peak Signal-to-Noise Ratio) \(\uparrow\) : the ratio between the maximum possible power of a signal and the power of corrupting noise \(10\log_{10}\left(\frac{(MAX)^2}{MSE}\right)\)[dB]</li> <li>SSIM(Structural Similarity Index Map) \(\uparrow\) : compare image qualities in three ways: Lumincance(\(l\)), Contrast(\(c\)), Structural(\(s\))<br> SSIM(x, y) = \([l(x,y)]^{\alpha}[c(x,y)]^{\beta}[s(x,y)]^{\gamma}=\frac{(2\mu_{x}\mu_{y}+C_1)(2\sigma_{xy}+C_2)}{(\mu_{x}^2+\mu_{y}^2+C_1)(\sigma_{x}^2+\sigma_{y}^2+C_2)}\) where \(l(x,y)=\frac{(2\mu_{x}\mu_{y}+C_1)}{\mu_{x}^2+\mu_{y}^2+C_1}\) and \(c(x,y)=\frac{(2\sigma_{x}\sigma_{y}+C_2)}{\sigma_{x}^2+\sigma_{y}^2+C_2}\) and \(s(x,y)=\frac{\sigma_{xy}+C_3}{\sigma_{x}\sigma_{y}+C_3}\)<br> SSIM calculator :<br> https://darosh.github.io/image-ssim-js/test/browser_test.html</li> <li>LPIPS \(\downarrow\)</li> </ul> <h4 id="comparisons">Comparisons</h4> <ul> <li>Neural Volumes (NV) :<br> It synthsizes novel views of objects that lie entirely within a bounded volume in front of a distinct background.<br> It optimizes 3D conv. network to predict a discretized RGB\(\alpha\) voxel grid and a 3D warp grid.<br> It renders novel views by marching rays through the warped voxel grid</li> <li>Scene Representation Networks (SRN) :<br> It represents continuous scene as an opaque surface.<br> MLP maps each 3D coordinate to a feature vector, and we optimize RNN to predict the next step size along the ray using the feature vector.<br> The feature vector from the final step is decoded into a color for that point on the surface. Note that SRN is followup to DeepVoxels by the same authors.</li> <li>Local Light Field Fusion (LLFF) :<br> designed for producing novel views for well-sampled forward facing scenes<br> trained 3D conv. network directly predicts a discretized frustum-sampled RGB\(\alpha\) grid (multiplane image), and then renders novel views by alpha-compositing and blending</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/7-480.webp 480w,/assets/img/2024-04-10-NeRF/7-800.webp 800w,/assets/img/2024-04-10-NeRF/7-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Test for scenes from author's new synthetic dataset </div> <p>LLFF exhibits banding and ghosting artifacts<br> SRN produces blurry and distorted renderings<br> NV cannot capture the details<br> NeRF captures fine details in both geometry and appearance</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/8-480.webp 480w,/assets/img/2024-04-10-NeRF/8-800.webp 800w,/assets/img/2024-04-10-NeRF/8-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Test for read-world scenes </div> <p>LLFF may have repeated edges because of blending between multiple renderings<br> NeRF also correctly reconstruct partially-occluded regions<br> SRN does not capture any high-frequency fine detail</p> <h4 id="discussion">Discussion</h4> <h4 id="ablation-studies">Ablation Studies</h4> <h2 id="conclusion">Conclusion</h2> <p>prior : MLP outputs discretized voxel representations<br> author : MLP outputs volume density and view-dependent emitted radiance</p> <h2 id="future-work">Future Work</h2> <p>efficiency :<br> Rather than hierarchical sampling, there is still much more progress to be made for efficient optimization and rendering of neural radiance fields</p> <p>interpretability :<br> voxel grids or meshes admits reasoning about the expected quality, but it is unclear to analyze these issues when we encode scenes into the weights of MLP</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"semyeong-yu/semyeong-yu.github.io","data-repo-id":"R_kgDOLmVJXQ","data-category":"Comments","data-category-id":"DIC_kwDOLmVJXc4CeSwc","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Semyeong Yu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@{{version}}/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5YMLX9VHFX"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5YMLX9VHFX");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>