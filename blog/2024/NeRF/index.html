<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> NeRF | Semyeong Yu </title> <meta name="author" content="Semyeong Yu"> <meta name="description" content="representing scenes as neural radiance fields for view synthesis"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://semyeong-yu.github.io/blog/2024/NeRF/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Semyeong</span> Yu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">NeRF</h1> <p class="post-meta"> April 10, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/nerf"> <i class="fa-solid fa-hashtag fa-sm"></i> nerf</a>   <a href="/blog/tag/rendering"> <i class="fa-solid fa-hashtag fa-sm"></i> rendering</a>   <a href="/blog/tag/3d"> <i class="fa-solid fa-hashtag fa-sm"></i> 3d</a>     ·   <a href="/blog/category/3d-view-synthesis"> <i class="fa-solid fa-tag fa-sm"></i> 3d-view-synthesis</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h1"> <a href="#nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</a> <ul> <li class="toc-entry toc-h4"><a href="#ben-mildenhall-pratul-psrinivasan-matthew-tancik">Ben Mildenhall, Pratul P.Srinivasan, Matthew Tancik</a></li> <li class="toc-entry toc-h2"> <a href="#introduction">Introduction</a> <ul> <li class="toc-entry toc-h4"><a href="#pipeline">Pipeline</a></li> <li class="toc-entry toc-h4"><a href="#problem--solution">Problem &amp; Solution</a></li> <li class="toc-entry toc-h4"><a href="#contribution">Contribution</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#related-work">Related Work</a> <ul> <li class="toc-entry toc-h4"><a href="#neural-3d-shape-representation">Neural 3D shape representation</a></li> <li class="toc-entry toc-h4"><a href="#view-synthesis-and-image-based-rendering">View synthesis and image-based rendering</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#neural-radiance-field-scene-representation">Neural Radiance Field Scene Representation</a></li> <li class="toc-entry toc-h2"> <a href="#volume-rendering-with-radiance-fields">Volume Rendering with Radiance Fields</a> <ul> <li class="toc-entry toc-h4"><a href="#ray-from-input-image-pre-processing">Ray from input image (pre-processing)</a></li> <li class="toc-entry toc-h4"><a href="#volume-rendering-from-mlp-output">Volume Rendering from MLP output</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#optimizing-a-neural-radiance-field">Optimizing a Neural Radiance Field</a> <ul> <li class="toc-entry toc-h4"><a href="#positional-encoding-pre-processing">Positional encoding (pre-processing)</a></li> <li class="toc-entry toc-h4"><a href="#hierarchical-volume-sampling">Hierarchical volume sampling</a></li> <li class="toc-entry toc-h4"><a href="#implementation-details--loss">Implementation details &amp; Loss</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#results">Results</a> <ul> <li class="toc-entry toc-h4"><a href="#datasets">Datasets</a></li> <li class="toc-entry toc-h4"><a href="#comparisons">Comparisons</a></li> </ul> </li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <h1 id="nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h1> <h4 id="ben-mildenhall-pratul-psrinivasan-matthew-tancik">Ben Mildenhall, Pratul P.Srinivasan, Matthew Tancik</h4> <p>paper :<br> https://arxiv.org/abs/2003.08934<br> code :<br> https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file<br> referenced blog :<br> https://csm-kr.tistory.com/64</p> <h2 id="introduction">Introduction</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/3-480.webp 480w,/assets/img/2024-04-10-NeRF/3-800.webp 800w,/assets/img/2024-04-10-NeRF/3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="pipeline">Pipeline</h4> <ul> <li> <p>(a) march camera rays to generate sampling of 5D coordinates</p> </li> <li> <p>(b) represents volumetric static scene by optimizing continuous 5D function(fully-connected network)</p> </li> </ul> <ol> <li>input: single continuous <code class="language-plaintext highlighter-rouge">5D coordinate</code><br> 3D location \(x, y, z\)<br> 2D direction \(\theta, \phi\)</li> <li>output:<br> <code class="language-plaintext highlighter-rouge">volume density</code> (differential opacity) (how much radiance is accumulated by a ray)<br> <code class="language-plaintext highlighter-rouge">view-dependent RGB color</code> (emitted radiance) \(c = (r, g, b)\)</li> </ol> <ul> <li> <p>(c) synthesizes novel view by classic <code class="language-plaintext highlighter-rouge">volume rendering</code> techniques(differentiable) to accumulate(project)(composite) the color/density samples into 2D image along rays</p> </li> <li> <p>(d) loss between synthesized and GT observed images</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/2-480.webp 480w,/assets/img/2024-04-10-NeRF/2-800.webp 800w,/assets/img/2024-04-10-NeRF/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Pipeline of NeRF architecture </div> <h4 id="problem--solution">Problem &amp; Solution</h4> <p>Problem :</p> <ol> <li>not sufficiently high-resolution representation</li> <li>inefficient in the number of samples per camera ray</li> </ol> <p>Solution :</p> <ol> <li>input <code class="language-plaintext highlighter-rouge">positional encoding</code> for MLP to represent higher frequency function</li> <li> <code class="language-plaintext highlighter-rouge">hierarchical sampling</code> to reduce the number of queries</li> </ol> <h4 id="contribution">Contribution</h4> <ul> <li>represent continuous scenes as 5D neural radiance fields with basic MLP to render high-resolution novel views</li> <li>differentiable volume rendering + hierarchical sampling</li> <li>positional encoding to map input 5D coordinate into higher dim. space for high-frequency scene representation</li> <li>overcome the storage costs of discretized voxel grids by encoding continuous volume into network’s parameters<br> =&gt; require only storage costs of sampled volumetric representations</li> </ul> <h2 id="related-work">Related Work</h2> <h4 id="neural-3d-shape-representation">Neural 3D shape representation</h4> <ul> <li> <p>deep networks that map \(xyz\) coordinates to signed distance functions or occupancy fields<br> =&gt; limit : need GT 3D geometry</p> </li> <li> <p>Niemeyer et al.<br> =&gt; input : find directly surface intersection for each ray (can calculate exact derivatie)<br> =&gt; output : diffuse color at each ray intersection location</p> </li> <li> <p>Sitzmann et al.<br> =&gt; input : each 3D coordinate<br> =&gt; output : feature vector and RGB color at each 3D coordinate<br> =&gt; rendering by RNN that marches along each ray to decide where the surface is.</p> </li> </ul> <blockquote> <p>Limit : oversmoothed renderings, so limited to simple shapes with low geometric complexity</p> </blockquote> <h4 id="view-synthesis-and-image-based-rendering">View synthesis and image-based rendering</h4> <ul> <li> <p>Given dense sampling of views, novel view synthesis is possible by simple light field sample interpolation</p> </li> <li> <p>Given sparser sampling of views, there are 2 ways :<br> mesh-based representation and volumetric representation</p> </li> <li> <p>Mesh-based representation with either diffuse(난반사) or view-dependent appearance :<br> Directly optimize mesh representations by differentiable rasterizers or pathtracers so that we reproject and reconstruct images</p> </li> </ul> <blockquote> <p>Limit :<br> gradient-based optimization is often difficult because of local minima or poor loss landscape<br> needs a template mesh with fixed topology for initialization, which is unavailable in real-world</p> </blockquote> <ul> <li>Volumetric representation :<br> well-suited for gradient-based optimization and less distracting artifacts<br> train : predict a sampled volumetric representation (voxel grids) from input images<br> test : use alpha-(or learned-)compositing along rays to render novel views<br> +) alpha-compositing : 여러 frame을 합쳐서 하나의 image로 합성하는 과정으로, 각 이미지 픽셀마다 알파 값(불투명도 값)(0~1)이 있어 겹치는 부분의 알파 값 및 픽셀 값을 결정<br> CNN compensates discretization artifacts from low resolution voxel grids or CNN allows voxel grids to vary on input time</li> </ul> <blockquote> <p>Limit :<br> good results, but limited by poor time, space complexity due to discrete sampling<br> +) discrete sampling : rendering high resol. image =&gt; finer sampling of 3D space</p> </blockquote> <blockquote> <p>Author’s solution :<br> encode <code class="language-plaintext highlighter-rouge">continuous</code> volume into network’s parameters<br> =&gt; higher quality rendering + require only storage cost of those <code class="language-plaintext highlighter-rouge">sampled</code> volumetric representations</p> </blockquote> <h2 id="neural-radiance-field-scene-representation">Neural Radiance Field Scene Representation</h2> <p>represent continuous scene by 5D MLP : (x, d) =&gt; (c, \(\sigma\))</p> <p>Here, there are 2 key-points!</p> <blockquote> <p>multiview consistent :<br> c is dependent on both x and d, but \(\sigma\) is only dependent on location x</p> </blockquote> <ul> <li>3D coordinate x =&gt; 8 fc-layers =&gt; volume-density and 256-dim. feature vector</li> </ul> <blockquote> <p>Lambertian reflection : diffuse(난반사) vs Specular reflection : 전반사</p> </blockquote> <blockquote> <p>non-Lambertian effects : view-dependent color change to represent specular reflection</p> </blockquote> <ul> <li>feature vector is concatenated with direction d =&gt; 1 fc-layer =&gt; view-dependent RGB color</li> </ul> <h2 id="volume-rendering-with-radiance-fields">Volume Rendering with Radiance Fields</h2> <h4 id="ray-from-input-image-pre-processing">Ray from input image (pre-processing)</h4> <p>We use <code class="language-plaintext highlighter-rouge">Ray</code> to synthesize <code class="language-plaintext highlighter-rouge">continuous</code>-viewpoint images from <code class="language-plaintext highlighter-rouge">discrete</code> input images</p> <blockquote> <p>\(r(t) = o + td\)<br> o : the location of camera<br> d : viewing direction</p> </blockquote> <blockquote> <p>How to calculate viewing direction d??</p> <ul> <li>pixel coordinate : \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)</li> <li>normalized coordinate by intrinsic matrix :<br> \(\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\) = \(K^{-1}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\) = \(\begin{bmatrix} 1/f_x &amp; 0 &amp; W/2 \\ 0 &amp; 1/f_y &amp; H/2 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)<br> Since y, z have opposite direction between the real-world coordinate and pixel coordinate, we multiply (-1)<br> \(\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\) = \(\begin{bmatrix} 1/f_x &amp; 0 &amp; W/2 \\ 0 &amp; -1/f_y &amp; H/2 \\ 0 &amp; 0 &amp; -1 \end{bmatrix}\) \(\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\)<br> Here, focal length in intrinsic matrix K is usually calculated using camear angle \(\alpha\) as \(\tan{\alpha / 2} = \frac{h/2}{f}\)</li> <li>3D coordinate by extrinsic matrix :<br> For extrinsic matrix \([R \vert t']\),<br> \(o = t'\)<br> \(d = R * \begin{bmatrix} u \\ v \\ 1 \end{bmatrix}\)<br> Therefore, we can obtain \(r(t) = o + td\)</li> </ul> </blockquote> <h4 id="volume-rendering-from-mlp-output">Volume Rendering from MLP output</h4> <p>We use differential classical volume rendering</p> <blockquote> <p>idea : 빛이 들어올 확률이 클수록(채도?), 물체의 밀도가 높을수록(명도?) 2D 이미지 픽셀에서 색상 채널의 값이 크게 나타나므로<br> pixel’s color along a ray is equal to the integral of (transmittance) x (volume density) x (color) for all points along one ray.</p> </blockquote> <p>For ray \(r\) traced through desired virtual camera and near, far bounds \(t_n\), \(t_f\),<br> expected color of ray \(r\) = \(C(r) = \int_{t_n}^{t_f} T(t) \sigma (r(t)) c(r(t), d) dt\)</p> <ul> <li>\(T(t) = \exp(- \int_{t_n}^{t} \sigma (r(s)) ds)\) :<br> accumulated transmittance along ray from \(t_n\) to \(t\)<br> the probability that ray travels from \(t_n\) to \(t\) without hitting any particle<br> 투과도가 클수록 투명<br> T(t) is the solution of \(\frac{dy}{dt} = - \sigma (x)y\)<br> since T(t)’s rate of decrease is proportional to T(t) itself and volume density</li> <li>\(\sigma (r(t))\) : volume density along the ray (learned by MLP)</li> <li>\(c(r(t), d)\) : object’s color along the ray (learned by MLP)</li> </ul> <p>To apply the equation to our model, we have to do sampling from continuous ray to discrete points<br> Instead of deterministic quadrature(typically used for rendering voxel grids, but may limit resolution),<br> author divides a ray \(\left[t_n, t_f\right]\) into N = 64 bins(intervals), and chooses one point \(t_i\) for each bin by uniform sampling<br> \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N}(t_f - t_n), t_n + \frac{i}{N}(t_f - t_n)\right]\)<br> Although we use discrete N samples, stratified sampling(층화 표집) enables MLP to be evaluated at continuous positions by optimization</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/5-480.webp 480w,/assets/img/2024-04-10-NeRF/5-800.webp 800w,/assets/img/2024-04-10-NeRF/5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>And then discretized version for N samples :<br> expected color \(\hat{C}(r) = \sum_{i=1}^{N} T_i (1 - \exp(- \sigma_{i} \delta_{i})) c_i\)</p> <ul> <li>\(\hat{C}(r)\) : differentiable about (\(\sigma_{i}, c_i\))</li> <li>\(\delta_{j} = t_{j+1} - t_j\) : the distance between adjacent samples</li> <li>\(\sigma (r(t)) dt ~~ =&gt; ~~ \sigma_{j} \delta_{j} = 1 - \exp(-\sigma_{j} \delta_{j})\) by Taylor Series Expansion</li> <li> \[T(t) = \exp(- \int_{t_n}^{t} \sigma (r(s)) ds) ~~ =&gt; ~~ T_i = \exp(- \sum_{j=1}^{i-1} \sigma_{j} \delta_{j})\] </li> <li> \[c(r(t), d) ~~ =&gt; ~~ c_i\] </li> </ul> <p>Here, in volume rendering, author uses <code class="language-plaintext highlighter-rouge">volume density</code><br> for 불투명도 == opacity == extinction coefficient == alpha value for alpha-compositing</p> <blockquote> <p>Final version :<br> expected color \(\hat{C}(r) = \sum_{i=1}^{N} T_i \alpha_{i} c_i\)<br> where \(T_i = \prod_{j=1}^{i-1} (1-\alpha_{j})\) and \(\alpha_{j} = 1 - \exp(-\sigma_{j} \delta_{j})\)<br> which reduces to traditional alpha-compositing problem</p> </blockquote> <h2 id="optimizing-a-neural-radiance-field">Optimizing a Neural Radiance Field</h2> <h4 id="positional-encoding-pre-processing">Positional encoding (pre-processing)</h4> <p>If we use input directly, MLP is biased to learn low-frequency function (oversmoothed appearance)<br> If we map input into higher dim. space, MLP can fit data with high-frequency variation<br> \(r : R \rightarrow R^{2L}\) <br> \(r(p) = (sin(2^0\pi p), cos(2^0\pi p), \cdots, sin(2^{L-1}\pi p), cos(2^{L-1}\pi p))\)<br> \(L=10\) for \(r(x)\) where x has three coordinates<br> \(L=4\) for \(r(d)\) where d has three components of the cartesian viewing direction unit vector</p> <h4 id="hierarchical-volume-sampling">Hierarchical volume sampling</h4> <p>Densely evaluating N points by stratified sampling is inefficient<br> =&gt; We don’t need much sampling at free space or occluded regions<br> =&gt; Hierarchical sampling enables us to allocate more samples to regions we expect to contain visible content</p> <p>We simultaneously optimize 2 networks with different sampling : <code class="language-plaintext highlighter-rouge">coarse</code> and <code class="language-plaintext highlighter-rouge">fine</code></p> <blockquote> <p>coarse sampling \(N_c\)개 : 위에서 배웠던 내용<br> author divides a ray into \(N_c\) = 64 bins(intervals), and chooses one point \(t_i\) for each bin by uniform sampling<br> \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N_c}(t_f - t_n), t_n + \frac{i}{N_c}(t_f - t_n)\right]\)</p> </blockquote> <blockquote> <p>fine sampling \(N_f\)개 : 새로운 내용<br> coarse sampling model’s output is a weighted sum of all coarse-sampled colors<br> \(\hat{C}(r) = \sum_{i=1}^{N_c} T_i \alpha_{i} c_i = \sum_{i=1}^{N_c} w_i c_i\)<br> where we define \(w_i = T_i \alpha_{i} = T_i (1 - \exp(-\sigma_{i} \delta_{i}))\) for \(i=1,\cdots,N_c\)<br> =&gt; Given the output of <code class="language-plaintext highlighter-rouge">coarse</code> network, we try more informed sampling where samples are biased toward the relevant parts of the volume<br> =&gt; We sample \(N_f\)=128 <code class="language-plaintext highlighter-rouge">fine</code> points following a piecewise-constant PDF of normalized \(\frac{w_i}{\sum_{j=1}^{N_c} w_j}\)<br> =&gt; Here, we use Inverse CDF Method for sampling fine points</p> </blockquote> <blockquote> <p>Inverse transform sampling = Inverse CDF Method :<br> =&gt; PDF (probability density function) : \(f_X(x)\)<br> =&gt; CDF (cumulative distribution function) : \(F_X(x) = P(X \leq x) = \int_{-\infty}^x f_X(x) dx\)<br> idea : 모든 확률 분포의 CDF는 Uniform distribution을 따른다!<br> =&gt; 따라서 CDF의 y값을 Uniform sampling하면, 그 y에 대응되는 x에 대한 (특정 PDF를 따르는) sampling을 구현할 수 있다!<br> =&gt; 즉, CDF의 역함수를 계산할 수 있다면, 기본 난수 생성기인 Uniform sampling을 이용해서 확률 분포 X에 대한 sampling을 할 수 있다!<br> \(F_X(x)\) ~ \(U\left[0, 1\right]\)<br> \(X\) ~ \(F^{-1}(U\left[0, 1\right])\)</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/4-480.webp 480w,/assets/img/2024-04-10-NeRF/4-800.webp 800w,/assets/img/2024-04-10-NeRF/4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We evaluate <code class="language-plaintext highlighter-rouge">coarse</code> network using \(N_c\)=64 points per ray<br> We evaluate <code class="language-plaintext highlighter-rouge">fine</code> network using \(N_c+N_f\)=64+128=192 points per ray where we sample 128 points following PDF of <code class="language-plaintext highlighter-rouge">coarse</code> sampled points<br> In result, we use a total of 64+192=256 samples per ray to compute the final rendering color \(C(r)\)</p> <h4 id="implementation-details--loss">Implementation details &amp; Loss</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/6-480.webp 480w,/assets/img/2024-04-10-NeRF/6-800.webp 800w,/assets/img/2024-04-10-NeRF/6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ol> <li>Prepare RGB images, corresponding camera poses, intrinsic parameters and scene bounds (use COLMAP structure-from-motion package to estimate these parameters)</li> <li>From H x W input image, randomly sample a batch of 4096 pixels</li> <li>calculate continuous ray from each pixel \(r(t) = o + kd\)</li> <li>coarse sampling of \(N_c\)=64 points per each ray \(t_i\) ~ \(U \left[t_n + \frac{i-1}{N_c}(t_f - t_n), t_n + \frac{i}{N_c}(t_f - t_n)\right]\)</li> <li>positional encoding \(r(x)\) and \(r(d)\) for input</li> <li>obtain volume density \(\sigma\) by MLP with \(r(x, y, z)\) as input</li> <li>obtain color \(c\) by MLP with \(r(x, y, z)\) and \(r(\theta, \phi)\) as input</li> <li>obtain rendering color of each ray by volume rendering \(\hat{C}(r) = \sum_{i=1}^{N} T_i (1 - \exp(- \sigma_{i} \delta_{i})) c_i\) from two networks ‘coarse’ and ‘fine’</li> <li>compute loss</li> <li>Adam optimizer with learning rate from \(5 \times 10^{-4}\) to \(5 \times 10^{-5}\)</li> <li>optimization for a single scene typically takes around 100-300k iterations (1~2 days) to converge on a single NVIDIA V100 GPU</li> </ol> <p>Here, we use L2 norm for loss<br> \(L = \sum_{r \in R} \left[{\left\|\hat{C_c}(r)-C(r)\right\|}^2+{\left\|\hat{C_f}(r)-C(r)\right\|}^2\right]\)<br> \(C(r)\) : GT pixel RGB color<br> \(\hat{C_c}(r)\) : rendering RGB color from coarse network : to allocate better samples in fine network<br> \(\hat{C_f}(r)\) : rendering RGB color from fine network : our goal</p> <h2 id="results">Results</h2> <h4 id="datasets">Datasets</h4> <p>synthetic data, llff data, deep voxel data</p> <h4 id="comparisons">Comparisons</h4> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2015/images/">a post with images</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/object-detection/">Object Detection</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/object-tracking/">Object Tracking</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/distill/">a distill-style blog post</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/image-segmentation/">Image Segmentation</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"semyeong-yu/semyeong-yu.github.io","data-repo-id":"R_kgDOLmVJXQ","data-category":"Comments","data-category-id":"DIC_kwDOLmVJXc4CeSwc","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Semyeong Yu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?5d75c11f89cd96294bf5e6dd1ee1bb30"></script> <script defer src="/assets/js/common.js?fcfacfb8c6281f5e68d5a7d348186eb1"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5YMLX9VHFX"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5YMLX9VHFX");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>