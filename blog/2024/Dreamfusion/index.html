<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> DreamFusion | Semyeong Yu </title> <meta name="author" content="Semyeong Yu"> <meta name="description" content="Text-to-3D using 2D Diffusion (ICLR 2023)"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://semyeong-yu.github.io/blog/2024/Dreamfusion/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "DreamFusion",
            "description": "Text-to-3D using 2D Diffusion (ICLR 2023)",
            "published": "August 29, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Semyeong</span> Yu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>DreamFusion</h1> <p>Text-to-3D using 2D Diffusion (ICLR 2023)</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#contribution">Contribution</a> </div> <div> <a href="#overview">Overview</a> </div> <ul> <li> <a href="#random-camera-light-sampling">Random camera, light sampling</a> </li> <li> <a href="#nerf-rendering-with-shading">NeRF Rendering with shading</a> </li> <li> <a href="#diffusion-loss-with-conditioning">Diffusion loss with conditioning</a> </li> <li> <a href="#optimization">Optimization</a> </li> </ul> <div> <a href="#rendering">Rendering</a> </div> <ul> <li> <a href="#structure">Structure</a> </li> <li> <a href="#geometry-regularizer">Geometry Regularizer</a> </li> </ul> <div> <a href="#sds-loss">SDS Loss</a> </div> <div> <a href="#pseudo-code">Pseudo Code</a> </div> <div> <a href="#experiment">Experiment</a> </div> <div> <a href="#limitation">Limitation</a> </div> <div> <a href="#question">Question</a> </div> </nav> </d-contents> <h2 id="dreamfusion-text-to-3d-using-2d-diffusion-iclr-2023">DreamFusion: Text-to-3D using 2D Diffusion (ICLR 2023)</h2> <h4 id="ben-poole-ajay-jain-jonathan-t-barron-ben-mildenhall">Ben Poole, Ajay Jain, Jonathan T. Barron, Ben Mildenhall</h4> <blockquote> <p>paper :<br> <a href="https://arxiv.org/abs/2209.14988" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2209.14988</a><br> project website :<br> <a href="https://dreamfusion3d.github.io/" rel="external nofollow noopener" target="_blank">https://dreamfusion3d.github.io/</a><br> pytorch code :<br> <a href="https://github.com/ashawkey/stable-dreamfusion" rel="external nofollow noopener" target="_blank">https://github.com/ashawkey/stable-dreamfusion</a></p> </blockquote> <h2 id="contribution">Contribution</h2> <ul> <li> <code class="language-plaintext highlighter-rouge">SDS(Score Distillation) Loss</code> 처음 제시 <ul> <li>scalable, high-quality 2D diffusion model의 능력을 3D domain renderer로 distill</li> <li>3D 또는 multi-view training data 필요없고, pre-trained 2D diffusion model만 있으면, 3D synthesis 수행 가능!</li> </ul> </li> <li>NeRF가 Diffusion(Imagen) model with text에서 내놓을 만한 그럴 듯한 image를 합성하도록 함</li> <li> <code class="language-plaintext highlighter-rouge">text-to-3D</code> synthesis 발전 시작</li> </ul> <h2 id="overview">Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/1-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/1-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-29-Dreamfusion/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Overview <ul> <li>initialize NeRF with random weight</li> <li>for each iter. <ul> <li>camera 위치와 각도, light 위치와 색상을 randomly sampling<br> \(P(camera), P(light)\)</li> <li>NeRF로 image rendering</li> <li>text embedding \(\tau\) 이용해서 NeRF param. \(\theta\) 에 대한 SDS loss 계산</li> <li>update NeRF weight</li> </ul> </li> </ul> </li> </ul> <h3 id="random-camera-light-sampling">Random camera, light sampling</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/2-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/2-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-29-Dreamfusion/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <code class="language-plaintext highlighter-rouge">camera</code> : <ul> <li>3D model을 <code class="language-plaintext highlighter-rouge">bounded sphere</code> 내부로 제한하고,<br> spherical coordinate(구 표면)에서 camera 위치를 sampling하여<br> 구의 원점을 바라보도록 camera 각도 설정</li> <li>width(64)에 0.7 ~ 1.35의 상수값을 곱하여 focal length 설정</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">light</code> : <ul> <li>camera 위치를 중심으로 한 확률분포로부터 light의 위치를 sampling하고<br> (어떤 확률분포 <code class="language-plaintext highlighter-rouge">????</code>)<br> light 색상도 sampling</li> </ul> </li> </ul> <h3 id="nerf-rendering-with-shading">NeRF Rendering with shading</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/3-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/3-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-29-Dreamfusion/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> albedo : NeRF가 예측한 color </div> <ul> <li>rendering 방법 : <ol> <li>albedo \(\rho\) 만으로 rendering<br> (기존 NeRF와 동일)</li> <li>albedo \(\rho\) 뿐만 아니라 shading하여 rendering</li> <li>albedo \(\rho\) 를 white \((1, 1, 1)\) 로 바꾼 뒤 shading하여 rendering</li> </ol> </li> <li> <code class="language-plaintext highlighter-rouge">Shading</code>의 역할 : <ul> <li>shading 없이 \(\rho\) 만으로 rendering하면<br> 평평한 3D model이 나와도 점수 높게 나옴</li> <li>shading으로 (빛 반사에 따른) shape 정보까지 고려해서 rendering하면<br> <code class="language-plaintext highlighter-rouge">volume 있는</code> 3D model이 되도록 촉구</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">NeRF MLP</code> \(\theta\) : <ul> <li>MLP output : volume density \(\tau\) 와 albedo \(\rho\)</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">Normal</code> \(n\) :<br> \(n = - \frac{\nabla_{\mu} \tau}{\| \nabla_{\mu} \tau \|}\)<br> where \(n\) 은 물체 표면의 법선벡터 <ul> <li>normal vector의 방향은<br> volume density \(\tau\) 가 가장 급격하게 변하는 방향, 즉 \(\nabla_{\mu} \tau\) 의<br> 반대 방향</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">Shading</code> \(s\) :<br> \(s = (l_p \circ \text{max}(0, n \cdot \frac{l - \mu}{\| l - \mu \|})) + l_a\)<br> where \(l_p\) 는 light 좌표 \(l\) 에서 나오는 light(광원) 색상<br> where \(l_a\) 는 ambient light(환경 조명) 색상<br> where \(\mu\) 는 shading 값을 계산할 surface 위 point 좌표<br> where \(\circ\) 는 element-wise multiplication <ul> <li>\(n \cdot (l - \mu)\) 는 표면에서의 normal vector와 표면에서 광원까지의 vector 간의 내적이며,<br> 이는 Lambertian(diffuse) reflectance(난반사)에 의해 광원의 빛이 반사되는 정도를 나타냄<br> 왜냐하면, 빛이 표면에 수직으로 들어올수록 많이 반사됨</li> <li>만약 빛이 표면 반대쪽에 있어서 또는 back-facing normal로 잘못 예측해서<br> 내적 값 \(n \cdot (l - \mu)\) 이 음수일 경우<br> 난반사에 의해 광원의 빛이 반사되는 정도는 0</li> <li>\(l_p \circ \text{난반사 정도} + l_a\) 에 의해<br> <code class="language-plaintext highlighter-rouge">광원</code>의 색상 \(l_p\) 는 물체 <code class="language-plaintext highlighter-rouge">표면의 난반사 정도에 따라</code> 반영되고<br> <code class="language-plaintext highlighter-rouge">환경 조명</code>의 색상 \(l_a\) 는 물체의 <code class="language-plaintext highlighter-rouge">모든 표면에 일정하게</code> 반영됨</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">Color</code> \(c\) :<br> \(c = \rho \circ s\) 또는 \(c = \rho\)</li> </ul> <h3 id="diffusion-loss-with-conditioning">Diffusion loss with conditioning</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/5-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/5-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-29-Dreamfusion/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <code class="language-plaintext highlighter-rouge">Latent Diffusion</code> model : <ul> <li>image \(x\) 가 아니라 encoder를 거친 image latent vector \(z\) 에 대해 noising, denoising 수행</li> <li>noisy \(z_T\) 와 text embedding vector \(\tau_{\theta}\) 를 concat한 걸<br> denoising하여 input image와 유사한 확률 분포를 갖도록 학습<br> (text embedding vector \(\tau_{\theta}\) 을 conditioning (query) 로 넣어줌)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/4-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/4-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-29-Dreamfusion/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>text embedding vector \(\tau_{\theta}\) :<br> T5-XXL text embedding을 거치기 전에<br> text prompt engineering 수행 <ul> <li>Elevation angle(고각)이 60도 이상일 때 “overhead view”</li> <li>azimuth angle(방위각)에 따라 “front view”, “side view”, “back view”</li> <li>text prompt engineering은 원래 좀 투박하게 하나?</li> </ul> </li> <li>Imagen : <ul> <li>latent diffusion model with \(64 \times 64\) resolution<br> (for fast training)</li> </ul> </li> </ul> <h3 id="sample-in-parmater-space-not-pixel-space">Sample in Parmater Space, not Pixel Space</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/10-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/10-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/10-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-29-Dreamfusion/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>\(x=g(\theta)\) : differentiable image parameterization (DIP)<br> where \(x\) 는 image이고 \(g\) 는 renderer이고 \(\theta\) 는 param. <ul> <li>more compact param. space \(\theta\) 에서 optimize ㄱㄴ<br> (더 강력한 optimization algorithm 사용 ㄱㄴ)</li> </ul> </li> <li>loss optimization으로 tractable sample 만들기 위해 diffusion model의 힘을 이용해서<br> \(x\) in pixel space 가 아니라, \(\theta\) in parameter space 를 optimize<br> s.t. \(x=g(\theta)\) 가 그럴 듯한 diffusion model sample처럼 보이도록</li> </ul> <h3 id="optimization">Optimization</h3> <ul> <li>실험적인 implementation : <ul> <li>noise level (time) sampling \(t\) :<br> \(z_t, t \sim U[0, 1]\) 에서 noise level이 너무 크거나(\(t=1\)) 너무 작을 경우(\(t=0\)) instability 생기므로<br> noise level \(t \sim U[0.02, 0.98]\) 로 sampling</li> <li>guidance weight \(w\) :<br> Imagen이 NeRF에 얼만큼 영향을 미칠지(guide할지)인데,<br> high-quality 3D model을 학습하기 위해서는<br> CFG(classifier-free guidance) weight \(w\) 를 큰 값(100)으로 설정<br> (NeRF MLP output color가 sigmoid에 의해 [0, 1]로 bounded되어있으므로 constrained optimization 문제라서 guidance weight 커도 딱히 artifacts 없음)<br> (SDS loss는 mode-seeking property를 가지고 있어서 작은 guidance weight 값을 사용할 경우 over-smoothing됨 <code class="language-plaintext highlighter-rouge">????</code>)</li> <li>seed :<br> noise level이 높을 때 smoothed density는 distinct modes를 많이 가지지 않고<br> SDS Loss는 mode-seeking property를 가지고 있으므로<br> random seed 바꿔도 실험 결과는 큰 차이 없음</li> </ul> </li> <li>implementation : <ul> <li>train : TPUv4, 15000 iter., 1.5h with Distributed Shampoo optimizer</li> <li>rendering : 각 cpu는 개별 view를 rendering하는데 사용</li> </ul> </li> </ul> <h2 id="rendering">Rendering</h2> <h3 id="structure">Structure</h3> <ul> <li>Mip-NeRF 360 구조 사용</li> <li>entire scene 대신 single object를 generate할 때<br> <code class="language-plaintext highlighter-rouge">bounded sphere</code> 내에서 NeRF view-synthesis 하면 빠르게 수렴 및 좋은 성능</li> <li>\(\gamma(d)\) 를 input으로 받아 배경 색상을 계산하는 별도의 MLP로 <code class="language-plaintext highlighter-rouge">environment map</code>을 생성한 뒤 그 위에 ray rendering하면 좋은 성능 <ul> <li>배경이 보이는 부분은 배경에서의 누적 \(\alpha\) 값이 1이도록</li> <li>물체 때문에 배경이 안 보이는 부분은 배경에서의 누적 \(\alpha\) 값이 0이도록</li> </ul> </li> </ul> <h3 id="geometry-regularizer">Geometry Regularizer</h3> <ul> <li>DreamField의 regularization : <ul> <li> <code class="language-plaintext highlighter-rouge">empty space가 불필요하게 채워지는</code> 것을 방지</li> <li>\(L_T = - \text{min} (\tau, \text{mean}(T(\theta, p)))\) :<br> 평균 <code class="language-plaintext highlighter-rouge">transmittance가 클수록</code> loss가 작음<br> where \(T(\theta, p)\) : transmittance with NeRF parameter \(\theta\) and camera pose \(p\)<br> where \(\tau\) : 최대값 상수</li> </ul> </li> <li>Ref-NeRF의 regularization : <ul> <li>normal vector \(n_i\) 의 back-facing (<code class="language-plaintext highlighter-rouge">물체 안쪽을 향하는</code>) 문제를 방지</li> <li>orientation loss \(L = \Sigma_{i} w_i max(0, n_i \cdot d)^2\) :<br> ray를 쏘면 물체의 앞면만 보이니까<br> 물체 표면의 normal vector 방향과 ray 방향의 내적이 음수여야 한다<br> 따라서 \(n_i\) 와 \(d\) 의 <code class="language-plaintext highlighter-rouge">내적이 양수일 경우</code> back-facing normal vector이므로 penalize <ul> <li>textureless shading을 쓸 때 해당 regularization이 중요<br> 만약 해당 regularization 안 쓰면<br> density field로 구한 normal 방향이 camera 반대쪽을 향하게 되어 shading이 더 어두워짐</li> </ul> </li> </ul> </li> </ul> <h2 id="sds-loss">SDS Loss</h2> <ul> <li> <p>NeRF로 rendering한 image \(x\) 에 noise를 더한 것을 \(z_t\) 로 두고<br> U-Net \(\hat \epsilon_{\phi}(z_t | y, t)\) 을 빼서 denoising하여 얻은 image의 확률분포가<br> 2D diffusion prior가 내놓는 image의 확률분포와 비슷하도록 하는 loss이며,<br> 그 차이만큼 NeRF \(\theta\) 로 back-propagation</p> </li> <li> <p>배경지식 :</p> <ul> <li>DDPM Loss : \(E_{t, x_0, \epsilon} [\| \epsilon - \hat \epsilon_{\phi}(\alpha_{t}x_0 + \sigma_{t} \epsilon, t) \|^{2}]\)<br> where \(\epsilon \sim N(0, I)\)<br> where \(\alpha_{t} = \sqrt{\bar \alpha_{t}}\)<br> where \(\sigma_{t} = \sqrt{1-\bar \alpha_{t}}\)</li> <li>만약 \(\theta\) 를 업데이트하기 위해 DDPM Loss를 직접 이용할 경우<br> diffusion training의 multiscale 특성을 이용하고<br> timestep schedule을 잘 선택한다면 <d-cite key="diffprior">[1]</d-cite> 잘 작동할 수 있다고 하지만<br> 실험해봤을 때 timestep schedule을 tune하기 어려웠고 DDPM Loss는 불안정했음</li> <li>위의 DDPM Loss는 denoising U-Net param.을 업데이트하기 위함이었고,<br> 우리는 fixed denoising U-Net을 이용하여<br> NeRF param. \(\theta\) 업데이트하기 위한 SDS Loss를 새로 만들겠다!</li> </ul> </li> </ul> <h3 id="simple-derivation-of-sds-loss">Simple Derivation of SDS Loss</h3> <ul> <li>DDPM Loss를 \(\phi\) 말고 \(\theta\) 에 대해 미분하고<br> constant \(\frac{dz_t}{dx} = \alpha_{t} \boldsymbol I\) 를 \(w(t)\) 에 넣으면</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/6-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/6-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-29-Dreamfusion/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> x는 NeRF가 생성한 image이고, y는 text embedding vector </div> <ul> <li>위의 U-Net Jacobian은 상당한 연산량을 가지는 데 비해<br> 작은 noise만 줄 뿐 큰 영향이 없으므로<br> SDS Loss에서 U-Net Jacobian term은 생략</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/7-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/7-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/7-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-29-Dreamfusion/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="derivation-of-sds-loss">Derivation of SDS Loss</h3> <ul> <li>SDS Loss gradient : <ul> <li>inspired by <code class="language-plaintext highlighter-rouge">gradient of weighted probability density distillation loss</code> <d-cite key="WaveNet">[2]</d-cite> </li> <li> \[\nabla_{\theta} L_{SDS}(\phi, x=g(\theta)) = \nabla_{\theta} E_{t, z_t|x}[w(t)\frac{\sigma_{t}}{\alpha_{t}}\text{KL}(q(z_t|g(\theta)) \| p_{\phi}(z_t | y, t))]\] </li> </ul> </li> <li>KL-divergence : <ul> <li> <a href="https://semyeong-yu.github.io/blog/2024/Diffusion/">Diffusion</a> 의 KL-divergence 부분에 따르면<br> 모르는 분포 \(q(x)\) ( \(\epsilon\) ) 을 N개 sampling하여 trained \(p(x | \theta)\)로 근사하고자 할 때,<br> \(KL(q \| p) \simeq \frac{1}{N} \sum_{n=1}^{N} {log q(x_n) - log p(x_n | \theta)}\) 이므로<br> \(\text{KL}(q(z_t|g(\theta)) \| p_{\phi}(z_t | y, t)) = E_{\epsilon}[\text{log} q(z_t | x = g(\theta)) - \text{log} p_{\phi}(z_t | y)]\)<br> \(\rightarrow\)<br> \(\nabla_{\theta}\text{KL}(q(z_t|g(\theta)) \| p_{\phi}(z_t; y, t)) = E_{\epsilon}[\nabla_{\theta}\text{log} q(z_t | x = g(\theta)) - \nabla_{\theta}\text{log} p_{\phi}(z_t | y)]\)</li> </ul> </li> <li>\(\theta\) 에 대한 \(\text{log}q\) 의 미분 : <ul> <li>gradient of <code class="language-plaintext highlighter-rouge">forward process entropy</code> w.r.t mean param. \(\theta\)<br> (variance는 고정)</li> <li>아래 수식을 \(\nabla_{\theta}log q(z_t | x = g(\theta))\) 계산에 이용<br> \(z_t = \alpha_{t} x + \sigma_{t} \epsilon \sim N(\alpha_{t} x, \sigma_{t}^2)\)<br> \(\rightarrow \text{log} q(z_t|x=g(\theta)) = -\frac{1}{2\sigma_{t}^2} \| z_t - \alpha_{t} x \|^2 + \text{constant}\)<br> \(\rightarrow \frac{d\text{log}q(z_t | x)}{dx} = \frac{\alpha_{t}}{\sigma_{t}^2}(z_t - \alpha_{t} x) = \frac{\alpha_{t}}{\sigma_{t}^2}\sigma_{t}\epsilon = \frac{\alpha_{t}}{\sigma_{t}}\epsilon\)<br> and \(\frac{d\text{log}q(z_t | x)}{dz_t} = -\frac{1}{\sigma_{t}^2}(z_t - \alpha_{t} x) = -\frac{1}{\sigma_{t}^2}\sigma_{t}\epsilon = -\frac{1}{\sigma_{t}}\epsilon\)<br> and \(\frac{dz_t}{dx} = \alpha_{t}\)</li> <li>\(\nabla_{\theta}log q(z_t | x = g(\theta)) = (\frac{d\text{log}q(z_t | x)}{dx} + \frac{d\text{log}q(z_t | x)}{dz_t}\frac{dz_t}{dx})\frac{dx}{d\theta}\)<br> \(= (\frac{\alpha_{t}}{\sigma_{t}}\epsilon - \frac{1}{\sigma_{t}}\epsilon \alpha_{t})\frac{dx}{d\theta}\)<br> \(= 0\)<br> (\(q\) 는 <code class="language-plaintext highlighter-rouge">고정된 variance의 noise</code>를 사용하므로 \(\theta\) 에 대한 entropy \(\text{log}q\) 의 미분 값은 0) <ul> <li>위의 식에서 \(\frac{d\text{log}q(z_t | x)}{dx}\) :<br> <code class="language-plaintext highlighter-rouge">parameter score function</code><br> gradient of log probability w.r.t parameter \(x\)<br> (\(x\) 에 대한 \(\text{log}q\) 의 gradient 계산)</li> <li>\(\frac{d\text{log}q(z_t | x)}{dz_t}\frac{dz_t}{dx}\) :<br> <code class="language-plaintext highlighter-rouge">path derivative</code><br> gradient of log probability w.r.t sample \(z_t\)<br> (\(q\) 를 따르는 sample \(z_t\) 를 통해 \(x\) 에 대한 \(\text{log}q\) 의 gradient 계산)</li> </ul> </li> <li>Sticking-the-Landing <d-cite key="vargrad">[3]</d-cite> 에 따르면<br> path derivative term은 냅두고<br> parameter score function term을 제거하여<br> SDS loss gradient에 \(\epsilon\) 항을 포함할 경우<br> <code class="language-plaintext highlighter-rouge">control-variates</code> 기법 <a href="https://en.wikipedia.org/wiki/Control_variates" rel="external nofollow noopener" target="_blank">Wikipedia</a>에 의해<br> \(E[\cdot]\) 으로 gradient 구할 때 <code class="language-plaintext highlighter-rouge">variance를 줄일 수</code> 있음!<br> (자세한 설명은 아래의 SDS Loss gradient Summary 부분 참고)<br> (variance가 작으면 optimization이 빨라지고 더 나은 결과를 도출할 수 있음)</li> </ul> </li> <li>\(\theta\) 에 대한 \(\text{log}p_{\phi}\) 의 미분 : <ul> <li>gradient of <code class="language-plaintext highlighter-rouge">backward process entropy</code> (denoising U-Net) w.r.t mean param. \(\theta\)</li> <li>아래 수식을 \(\nabla_{\theta}log p_{\phi}(z_t | y)\) 계산에 이용<br> \(\frac{d\text{log}q(z_t | x)}{dz_t}\) 구했듯이 \(\epsilon\) 대신 \(\epsilon_{\phi}\) 넣으면<br> \(\nabla_{z_t} \text{log}p_{\phi}(z_t | y) = \frac{d\text{log}p_{\phi}(z_t | y)}{dz_t} = -\frac{1}{\sigma_{t}}\hat \epsilon_{\phi}\)<br> and \(\frac{dz_t}{dx} = \alpha_{t}\)</li> <li> \[\nabla_{\theta}\text{log} p_{\phi}(z_t | y) = \nabla_{z_t} \text{log}p_{\phi}(z_t | y) \frac{dz_t}{dx} \frac{dx}{d\theta} = - \frac{\alpha_{t}}{\sigma_{t}} \hat \epsilon_{\phi}(z_t | y) \frac{dx}{d\theta}\] </li> </ul> </li> <li>SDS Loss gradient <code class="language-plaintext highlighter-rouge">Summary</code> : <ul> <li>SDS Loss gradient :<br> \(\nabla_{\theta} L_{SDS}(\phi, x=g(\theta)) = E_{t, z_t|x}[w(t)\frac{\sigma_{t}}{\alpha_{t}}\nabla_{\theta}\text{KL}(q(z_t|g(\theta)) \| p_{\phi}(z_t | y, t))]\)<br> \(= E_{t, \epsilon}[w(t)\frac{\sigma_{t}}{\alpha_{t}}E_{\epsilon}[\nabla_{\theta}\text{log} q(z_t | x = g(\theta)) - \nabla_{\theta}\text{log} p_{\phi}(z_t | y)]]\)<br> \(= E_{t, \epsilon}[w(t)\frac{\sigma_{t}}{\alpha_{t}} (-\frac{\alpha_{t}}{\sigma_{t}}\epsilon \frac{dx}{d\theta} + \frac{\alpha_{t}}{\sigma_{t}} \hat \epsilon_{\phi}(z_t | y) \frac{dx}{d\theta})]\)<br> \(= E_{t, \epsilon}[w(t)(\hat \epsilon_{\phi}(z_t | y) - \epsilon)\frac{dx}{d\theta}]\)</li> <li>\(\nabla_{\theta}log q(z_t | x = g(\theta))\) 의 path derivative term은 \(\epsilon\) 과 관련 있고!<br> \(\nabla_{\theta}\text{log} p_{\phi}(z_t | y)\) 은 \(\epsilon\) 의 예측, 즉 \(\hat \epsilon_{\phi}\) 와 관련 있고!<br> 둘의 KL-divergence를 loss term으로 사용한다!<br> (\(\epsilon\) 을 \(\hat \epsilon\) 의 control-variate로 생각하여 <d-cite key="vargrad">[3]</d-cite> 방식처럼 둘의 차이로 SDS Loss gradient 만들 수 있음!)</li> </ul> </li> <li>Other Papers : <ul> <li>Graikos et al. (2022) <d-cite key="diffprior">[1]</d-cite> :<br> \(KL(h(x) \| p_{\phi}(x|y))\) 로부터<br> \(E_{\epsilon, t}[\| \epsilon - \hat \epsilon_{\theta}(z_t | y; t) \|^2] - \text{log} c(x, y)\) 를 유도해서 loss로 썼지만,<br> SDS와 달리 error 제곱 꼴이라서 costly back-propagation</li> <li>DDPM-PnP 또한 auxiliary classifier \(c\) 를 썼지만,<br> SDS에서는 CFG(classifier-free-guidance) 사용<br> (별도의 image label 없이 image caption만 conditioning으로 넣어줘서 model 학습)</li> <li>지금까지 implicit model의 entropy의 gradient는 single noise level <d-cite key="ARDAE">[4]</d-cite> 에서의 amortized score model (control-variate 사용 안 함) 로 측정하였는데,<br> SDS에서는 multiple noise level을 사용함으로써 optimization 더 쉽게 ㄱㄴ<br> (multiple noise level <code class="language-plaintext highlighter-rouge">?????</code>)</li> <li>GAN-like amortized samplers는 Stein discrepancy 최소화 <d-cite key="Stein">[5]</d-cite> , <d-cite key="Stein2">[6]</d-cite> 로 학습하는데,<br> 이는 SDS loss의 score 차이와 비슷</li> </ul> </li> </ul> <h2 id="pseudo-code">Pseudo Code</h2> <pre><code class="language-Python">params = generator.init() # NeRF param.
opt_state = optimizer.init(params) # optimizer
diffusion_model = diffusion.load_model() # Imagen diffusion model
for iter in iterations:
  t = random.uniform(0., 1.) # noise level (time step)
  alpha_t, sigma_t = diffusion_model.get_coeffs(t) # determine noisy z_t's mean, std.
  eps = random.normal(img_shape) # gaussian noise (epsilon)
  x = generator(params, ...) # NeRF rendered image
  z_t = alpha_t * x + sigma_t * eps # noisy NeRF image
  epshat_t = diffusion_model.epshat(z_t, y, t) # denoising U-Net
  g = grad(weight(t) * dot(stopgradient[epshat_t - eps], x), params) # derivative of SDS loss; stopgradient since do not update diffusion model
  params, opt_state = optimizer.update(g, opt_state) # update NeRF param.
return params
</code></pre> <h2 id="experiment">Experiment</h2> <h3 id="metric">Metric</h3> <ul> <li> <code class="language-plaintext highlighter-rouge">CLIP R-Precision</code> <d-cite key="dreamfield">[7]</d-cite> : <ul> <li> <code class="language-plaintext highlighter-rouge">rendered image의 text 일관성</code>을 측정<br> (rendered image가 주어졌을 때 CLIP이 오답 texts 중 적절한 text를 찾는 accuracy로 계산)</li> <li>기존 CLIP R-Precision은 geometry quality는 측정할 수 없으므로<br> 평평한 flat geometry에 대해서도 높은 점수가 나올 수 있음</li> <li>textureless render의 R-Precision(Geo)도 추가로 측정!</li> </ul> </li> <li>PSNR :<br> zero-shot text-to-3D generation에서는<br> text에 대한 3D Ground-Truth를 만들 수 없으므로<br> GT를 필요로 하는 PSNR 같은 metric은 사용하지 못함</li> </ul> <h3 id="result">Result</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/8-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/8-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/8-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-29-Dreamfusion/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Geo(metry)의 CLIP R-Precision 점수가 높다는 것은 평평한 3D model이 아니라 shape 정보까지 고려했다는 것! </div> <ul> <li>위의 표 설명 : <ul> <li>GT Images : oracle (CLIP training에 사용된 dataset)</li> <li>CLIP-Mesh : CLIP으로 mesh를 optimize한 연구</li> </ul> </li> <li> <p>DreamFusion은 training할 때 <code class="language-plaintext highlighter-rouge">Imagen</code>을 썼고,<br> Dream Fields와 CLIP-Mesh는 training할 때 <code class="language-plaintext highlighter-rouge">CLIP</code>을 썼으므로<br> Dream Fields와 CLIP-Mesh를 사용하는 게<br> DreamFusion보다 성능이 더 좋아야 하는데,<br> 위의 표를 보면 Color와 Geometry 평가에서 DreamFusion이 높은 성능(text 일관성)을 보인다는 것을 확인할 수 있다</p> </li> <li>아쉬운 점 :<br> 비슷한 다른 모델이 있다면 PSNR, SSIM 등으로 비교할 수 있었을텐데<br> 비교군이 없어서 R-Precision으로 consistency 측정만 했음</li> </ul> <h3 id="ablation-study">Ablation Study</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-29-Dreamfusion/9-480.webp 480w,/assets/img/2024-08-29-Dreamfusion/9-800.webp 800w,/assets/img/2024-08-29-Dreamfusion/9-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-29-Dreamfusion/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>어떤 기법이 얼마나 성능에 기여했는지 파악하기 위해<br> 4가지 기법을 점진적으로 추가 <ul> <li>(i) <code class="language-plaintext highlighter-rouge">ViewAug</code> : view-points의 범위를 넓힘</li> <li>(ii) <code class="language-plaintext highlighter-rouge">ViewDep</code> : view-dependent text prompt-engineering 사용<br> (e.g. “overhead view”, “side view”)</li> <li>(iii) <code class="language-plaintext highlighter-rouge">Lighting</code> : 조명 사용</li> <li>(iv) <code class="language-plaintext highlighter-rouge">Textureless</code> : albedo를 white로 만들어서 (color 없이) shading</li> </ul> </li> <li>geometry quality를 확인하기 위해<br> 3가지 rendering 기법을 비교 <ul> <li>(Top) <code class="language-plaintext highlighter-rouge">Albedo</code> : albedo \(\rho\) 만으로 rendering<br> (기존 NeRF와 동일)</li> <li>(Middle) <code class="language-plaintext highlighter-rouge">Full Shaded</code> : albedo \(\rho\) 뿐만 아니라 shading하여 rendering</li> <li>(Bottom) <code class="language-plaintext highlighter-rouge">Textureless</code> : albedo \(\rho\) 를 white \((1, 1, 1)\) 로 바꾼 뒤 shading</li> </ul> </li> <li>결과 설명 : <ul> <li>기법 추가 없이 Albedo rendering 하면 R-Precision은 높게 나오는데<br> Geometry가 엄청 이상함 (e.g. 머리 2개 가진 개)</li> <li>ViewDep, Lighting, Textureless 기법 사용해야 정확한 <code class="language-plaintext highlighter-rouge">geometry</code>까지 recon할 수 있음</li> <li>(ii) ViewDep의 영향 :<br> geometry 개선되지만, surface가 non-smooth하고 Shaded rendering 결과가 bad</li> <li>(iii) Lighting의 영향 :<br> geometry 개선되지만, 어두운 부분은 (e.g. 해적 모자) 여전히 non-smooth</li> <li>(iv) Textureless의 영향 :<br> geometry smooth하게 만드는 데 도움 되지만, color detail (e.g. 해골 뼈)이 geometry에 carved 되는 문제 발생</li> </ul> </li> </ul> <h2 id="limitation">Limitation</h2> <ul> <li>SDS를 적용하여 만든 2D image sample은 <code class="language-plaintext highlighter-rouge">over-saturated</code> 혹은 <code class="language-plaintext highlighter-rouge">over-smoothed</code> result <ul> <li>dynamic thresholding <d-cite key="dynathres">[8]</d-cite> 을 사용하면 SDS를 image에 적용할 때의 문제를 완화시킬 수 있다고 알려져 있긴 하지만, NeRF context에 대해서는 해결하지 못함<br> (dynamic thresholding이 뭔지 아직 몰라서 읽어 봐야 됨 <code class="language-plaintext highlighter-rouge">?????</code>)</li> </ul> </li> <li>SDS를 적용하여 만든 2D image sample은 <code class="language-plaintext highlighter-rouge">diversity</code> 부족<br> (random seed 바꿔도 3D result에 큰 차이 없음)</li> </ul> <p>This may be fundamental to our use of reverse KL divergence, which has been previously noted to have mode-seeking properties in the context of variational inference and probability density distillation <code class="language-plaintext highlighter-rouge">?????</code></p> <ul> <li>\(64 \times 64\) Imagen (<code class="language-plaintext highlighter-rouge">low resol.</code>)을 사용하여 3D model의 fine-detail이 부족할 수 있음 <ul> <li>diffusion model 또는 NeRF를 더 큰 걸 사용하면 문제 해결할 수 있지만, 그만큼 겁나 느려지지…</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">2D image로부터 3D recon.</code>하는 게 원래 어렵고 애매한 task임<br> e.g. inverse rendering, dreamfusion <ul> <li>같은 2D images로부터 무수히 많은 3D worlds가 존재할 수 있으니까</li> <li>optimization landscape가 highly non-convex하므로 local minima에 빠지지 않기 위한 기법들 필요<br> (local minima : e.g. 모든 scene content가 하나의 flat surface에 painted된 경우)</li> <li>more <code class="language-plaintext highlighter-rouge">robust 3D prior</code>가 도움 될 것임</li> </ul> </li> </ul> <h2 id="latest-papers">Latest Papers</h2> <ul> <li>본 논문 DreamFusion과 관련된 논문들 <ul> <li>ProlificDreamer</li> <li>CLIP Goes 3D</li> <li>Magic3D</li> <li>Fantasia3D</li> <li>CLIP-Forge</li> <li>CLIP-NeRF</li> <li>Text2Mesh</li> <li>DDS (Delta Denoising Score)</li> </ul> </li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 : diffusion의 mode-seeking property?</p> </li> <li>A1 : <ul> <li>A1-1 : diffusion에서 forward process는 mean-seeking property 가지고 있고, backward process는 mode-seeking property 가지고 있는 걸로 알고 있는데 이거랑 관련 있을까요?</li> <li>A1-2 : 그 아까 말씀하신 mode-seeking property에 대해 찾아봤는데 diffusion의 Langevin Dynamics 등의 샘플링 방법이 갖고 있는 특징으로 특정 mode에서 sample을 집중적으로 생성하는 특징을 의미하네요. 제 생각에 “mode-seeking property를 갖고있어 guidance weight가 작으면 over-smoothing 된다”는 말은 diffusion model에서는 특정 모드에 집중되어 sample들이 생성되는데 낮은 guidance weight를 쓰면 여러 모드 사이를 부드럽게(?) 연결하려는 (over-smoothing) 말이지 않을까 싶네요.(즉 너무 매끄럽거나 디테일이 떨어지는 이미지 생성)</li> </ul> </li> <li> <p>Q2 : reverse KL-divergence를 최소화하는 과정의 경우 mode-seeking property (확률 높은 중요한 부분 찾는 경향)가 있다는데,<br> reverse KL-divergence와 mode-seeking property가 무슨 관계인가요?</p> </li> <li> <p>A2 : TBD</p> </li> <li> <p>Q3 : SDS loss로 image rendering한 samples의 경우 diversity가 부족하고 그 이유가 mode-seeking property라는 거 같은데,<br> 오히려 diversity가 부족한 게 단점이 아니라,<br> mode-seeking property로 중요한 부분을 잘 캐치해서 consistent하게 그려내는 게 장점이 될 수 있지 않나요?</p> </li> <li> <p>A3 : TBD<br> While modes of generative models in high dimensions are often far from typical samples (Nalisnick et al., 2018), the multiscale nature of diffusion model training may help to avoid these pathologies. <code class="language-plaintext highlighter-rouge">?????</code></p> </li> <li> <p>Q4 : \(\theta\) 에 대한 \(\text{log}q\) 의 미분에서 path derivative term은 냅두고 parameter score function term은 제거해서 control-variates에 의해 variance를 줄였다고 하는데,<br> parameter score function term을 걍 제거해버리는 게 좀 야매 아닌가요?</p> </li> <li>A4 : TBD</li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-08-29-Dreamfusion.bib"></d-bibliography> <div id="disqus_thread" style="max-width: 800px; margin: 0 auto;"> <script type="text/javascript">var disqus_shortname="semyeong-yu",disqus_identifier="/blog/2024/Dreamfusion",disqus_title="DreamFusion";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Semyeong Yu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5YMLX9VHFX"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5YMLX9VHFX");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>