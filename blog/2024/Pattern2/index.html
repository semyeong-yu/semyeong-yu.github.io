<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> EE534 Pattern Recognition Final | Semyeong Yu </title> <meta name="author" content="Semyeong Yu"> <meta name="description" content="Lecture Summary (24F)"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://semyeong-yu.github.io/blog/2024/Pattern2/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "EE534 Pattern Recognition Final",
            "description": "Lecture Summary (24F)",
            "published": "October 28, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Semyeong</span> Yu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>EE534 Pattern Recognition Final</h1> <p>Lecture Summary (24F)</p> </d-title> <d-article> <blockquote> <p>Lecture :<br> 24F EE534 Pattern Recognition<br> by KAIST Munchurl Kim <a href="https://www.viclab.kaist.ac.kr/" rel="external nofollow noopener" target="_blank">VICLab</a></p> </blockquote> <h2 id="chapter-6-linear-discriminant-functions">Chapter 6. Linear Discriminant Functions</h2> <h3 id="linearly-non-separable-svm">Linearly Non-Separable SVM</h3> <ul> <li>new constraint :<br> \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}\)<br> \(\xi_{i}\) 를 도입하여 이제는 inside margin or misclassified 도 가능하지만 대신 \(C \sum_{i=1}^{N} \xi_{i}\) 를 loss에 넣어서 큰 \(\xi_{i}\) 값을 penalize <ul> <li>\(\xi = 0\) : outside margin or support vector</li> <li>\(0 \lt \xi \leq 1\) : inside margin (correctly classified, but margin violation)</li> <li>\(\xi \gt 1\) : misclassified</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/2.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/2.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>방법 1) 1-norm-soft-margin <ul> <li>constrained primal form :<br> minimize \(J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \xi_{i}\)<br> subject to \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}\) and \(\xi_{i} \geq 0\) <ul> <li>unconstrained primal form :<br> 이 때 위의 두 가지 constraints는 \(\xi_{i} = \text{max}(0, 1 - y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}))\) 로 하나로 합칠 수 있음<br> 따라서<br> minimize \(J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \text{max}(0, 1 - y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}))\)</li> <li>regularization param. \(C\) : <ul> <li>small \(C\) : 큰 \(\xi_{i}\) 값도 허용하므로 margin 커짐</li> <li>large \(C\) : 큰 \(\xi_{i}\) 값은 허용 안 하므로 margin 작아짐</li> <li>\(C = \infty\) : non-zero \(\xi_{i}\) 값 허용 안 하므로 hard margin (no sample inside margin)<br> (Linearly Separable SVM 에 해당함)</li> </ul> </li> </ul> </li> <li>Lagrangian :<br> minimize \(L(\boldsymbol w, w_{0}, \xi, \boldsymbol \lambda, \boldsymbol \mu) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \xi_{i} - \sum_{i}^{N} \mu_{i} \xi_{i} - \sum_{i}^{N} \lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (1 - \xi_{i}))\)<br> subject to \(\xi_{i}, \mu_{i}, \lambda_{i} \geq 0\) <ul> <li> \[\nabla_{\boldsymbol w} L = 0 \rightarrow \boldsymbol w = \sum_{i=1}^{N} \lambda_{i} y_{i} \boldsymbol x_{i}\] </li> <li> \[\nabla_{w_{0}} L = 0 \rightarrow \sum_{i=1}^{N} \lambda_{i} y_{i} = 0\] </li> <li> \[\nabla_{\xi_{i}} L = 0 \rightarrow C - \mu_{i} - \lambda_{i} = 0\] </li> </ul> </li> <li>KKT condition 중 slackness condition : <ul> <li> \[\lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (1 - \xi_{i})) = 0\] </li> <li> \[\mu_{i} \xi_{i} = 0\] </li> </ul> </li> <li>dual form :<br> 위의 세 가지 식을 대입하여 \(\boldsymbol w, w_{0}, \xi_{i}, \mu_{i}\) 를 소거하면<br> maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}\)<br> subject to \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\) and \(0 \leq \lambda_{i} \leq C\)</li> <li>Summary : <ul> <li>Step 1) optimal \(\lambda_{i}^{\ast}\) 구하기<br> \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\) and \(0 \leq \lambda_{i} \leq C\) 이용해서<br> \(\nabla_{\lambda_{i}} L = 0\) 으로 아래의 dual form 풀어서<br> (maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}\))<br> optimal \(\lambda_{i}\) 얻음</li> <li>Step 2) optimal \(\boldsymbol w^{\ast}, w_{0}^{\ast}\) 구하기 <ul> <li>\(\boldsymbol w^{\ast} = \sum_{i=1}^{N_{s}} \lambda_{i}^{\ast} y_{i} x_{i}\)<br> (\(N_{s}\) : support vector 개수)<br> (hyperplane 결정할 때는 \(\lambda_{i} \gt 0\) 중에 \(\xi = 0\) 인 support vectors만 고려!!)</li> <li>\(w_{0}^{\ast} = \frac{1}{y_{j}} - \sum_{i=1}^{N_{s}} \lambda_{i}^{\ast} y_{i} x_{i}^{T} x_{j} = \frac{1}{y_{j}} - \boldsymbol w^{\ast T} x_{j}\)<br> (support vector \(x_{j}\) 1개 사용)<br> 또는<br> \(w_{0}^{\ast} = \frac{1}{N_{s}} \sum_{j=1}^{N_{s}} (\frac{1}{y_{j}} - \sum_{i=1}^{N_{s}} \lambda_{i}^{\ast} y_{i} x_{i}^{T} x_{j}) = \frac{1}{N_{s}} \sum_{j=1}^{N_{s}} (\frac{1}{y_{j}} - \boldsymbol w^{\ast T} x_{j})\)<br> (support vector \(x_{j}\) \(N_{s}\)-개 모두 사용하여 average value)</li> </ul> </li> <li>Tip : hard margin (no sample inside margin) 의 경우<br> 육안으로 어떤 sample이 support vector일지 판단 가능하다면<br> complementary slackness condition (\(\lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (1 - \xi_{i})) = 0\)) 에서<br> support vector만 \(\lambda_{i} \gt 0\) 이므로<br> 연립해서 \(\boldsymbol w^{\ast}, w_{0}^{\ast}\) 바로 구할 수 있음</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/1.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/1.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/1.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/1.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>방법 2) 2-norm-soft-margin <ul> <li>차이점 1) primal form<br> minimize \(J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \xi_{i}\)<br> subject to \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}\) and \(\xi_{i} \geq 0\)<br> 대신<br> minimize \(J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + \frac{1}{2} C \sum_{i=1}^{N} \xi_{i}^{2}\)<br> subject to \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}\) and \(\xi_{i} \geq 0\)</li> <li>차이점 2) Lagrangian<br> \(\nabla_{\xi_{i}} L(\boldsymbol w, w_{0}, \boldsymbol \xi, \boldsymbol \lambda, \boldsymbol \mu) = 0\) 했을 때<br> \(C - \mu_{i} - \lambda_{i} = 0\)<br> 대신<br> \(C \xi_{i} - \mu_{i} - \lambda_{i} = 0\)</li> <li>차이점 3) dual form<br> maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}\)<br> subject to \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\) and \(0 \leq \lambda_{i} \leq C\)<br> 대신<br> maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j} - \frac{1}{2C} \sum_{i=1}^{N} (\lambda_{i} + \mu_{i})^{2}\)<br> subject to \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\) and \(0 \leq \lambda_{i} \leq C\)</li> </ul> </li> <li>Remark : <ul> <li>Linearly Non-Separable SVM에서<br> \(C \rightarrow \infty\) 하면 Linearly Separable SVM<br> e.g. non-linear에서는 \(0 \leq \lambda_{i} \leq C\) 인데, linear에서는 \(0 \leq \lambda_{i} \lt \infty\)</li> <li>SVM의 한계 :<br> high computational complexity<br> (SVM training은 주로 batch mode로 진행되어 memory를 많이 필요로 하므로<br> training dataset을 subset으로 나눠서 training 진행)</li> <li>지금까지는 SVM for two-category만 살펴봤는데,<br> M-class 의 경우 M개의 discriminant function \(g_{i}(x)\) 를 design하여<br> assign \(x\) to class \(w_{i}\) if \(i = \text{argmax}_{k} g_{k}(x)\)</li> </ul> </li> </ul> <h3 id="v-svm">v-SVM</h3> <ul> <li>v-SVM : <ul> <li>hyperplane<br> \(\boldsymbol w^{T} \boldsymbol x_{i} + w_{0} = \pm 1\)<br> 대신<br> \(\boldsymbol w^{T} \boldsymbol x_{i} + w_{0} = \pm \rho\)<br> where \(\rho \geq 0\) : var. to be optimized</li> <li>margin<br> margin은 \(\frac{2 \rho}{\| w \|}\) 이므로<br> margin을 maximize하려면<br> \(\| w \|\) minimize 뿐만 아니라 \(\rho\) maximize하면 되므로<br> 둘 다 primal form loss term에 넣음</li> <li>primal form<br> minimize \(J(\boldsymbol w, \xi, \rho) = \frac{1}{2} \| \boldsymbol w \|^{2} - v \rho + \frac{1}{N} \sum_{i=1}^{N} \xi_{i}\)<br> subject to \(y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq \rho - \xi_{i}\) and \(\xi_{i} \geq 0\) and \(\rho \geq 0\)</li> <li>Lagrangian<br> \(L(\boldsymbol w, w_{0}, \boldsymbol \xi, \rho, \boldsymbol \lambda, \boldsymbol \mu, \delta) = \frac{1}{2} \| \boldsymbol w \|^{2} - v \rho + \frac{1}{N} \sum_{i=1}^{N} \xi_{i} - \sum_{i}^{N} \mu_{i} \xi_{i} - \sum_{i}^{N} \lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (\rho - \xi_{i})) - \delta \rho\) <ul> <li>\(\nabla_{\boldsymbol w} L = 0\) 했을 때<br> \(\boldsymbol w = \sum_{i=1}^{N} \lambda_{i} y_{i} \boldsymbol x_{i}\)</li> <li>\(\nabla_{w_{0}} L = 0\) 했을 때<br> \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\)</li> <li>\(\nabla_{\xi_{i}} L = 0\) 했을 때<br> \(\mu_{i} + \lambda_{i} = \frac{1}{N}\)</li> <li>\(\nabla_{\rho} L = 0\) 했을 때<br> \(\sum_{i=1}^{N} \lambda_{i} - \delta = v\)</li> </ul> </li> <li>KKT condition 중 complementary slackness<br> For \(\lambda_{i} \geq 0\) and \(\mu_{i} \geq 0\) and \(\delta \geq 0\), <ul> <li> \[\lambda_{i} (y_{i}(\boldsymbol w^{T} \boldsymbol x + w_{0}) - (\rho - \xi_{i})) = 0\] </li> <li> \[\mu_{i} \xi_{i} = 0\] </li> <li> \[\delta \rho = 0\] </li> </ul> </li> <li>dual form<br> maximize \(L(\lambda) = - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}\)<br> subject to \(\sum_{i=1}^{N} \lambda_{i} y_{i} = 0\) and \(0 \leq \lambda_{i} \leq \frac{1}{N}\) and \(\sum_{i=1}^{N} \lambda_{i} = \delta + v \geq v\) <ul> <li>\(\lambda\) 만 explicitly 남아 있고,<br> margin var. \(\rho\) 와 slack var. \(\xi_{i}\) 는 constraint의 bounds에 implicitly 들어 있음</li> <li>v-SVM에서는 \(\sum_{i=1}^{N} \lambda_{i}\) term이 없으므로<br> optimal \(\lambda_{i}\) 는 quadratically homogeneous solution</li> <li>새로운 constraint \(\sum_{i=1}^{N} \lambda_{i} \geq v\) 있음</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/3.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/3.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Remark <ul> <li>v-SVM의 경우 \(0 \leq v \leq 1\) 이어야 optimizable</li> <li>C-SVM에 비해 v-SVM은<br> error rate와 support vector 수 bound 측면에서 장점 <code class="language-plaintext highlighter-rouge">???</code> </li> <li>\(\rho \gt 0\) 일 때 \(\delta = 0\) 이므로<br> \(\sum_{i=1}^{N} \lambda_{i} = v\)</li> <li>loss (error)에 기여하는 애들은<br> \(\xi_{i} \gt 0\), 즉 \(\mu_{i} = 0\), 즉 \(\lambda_{i} = \frac{1}{N}\) 이다<br> 따라서 error rate = \(\sum_{i=1}^{N_{error}} \lambda_{i} = \frac{N_{error}}{N} \leq \sum_{i=1}^{N} \lambda_{i} = v\)<br> 즉, error rate \(\frac{N_{error}}{N} \leq v\) 이고<br> total number errors \(N_{error} \leq N v\)</li> <li>Since \(0 \lt \lambda_{i} \lt 1\) for support vector \(i\),<br> \(v = \sum_{i=1}^{N} \lambda_{i} = \sum_{i=1}^{N_{s}} \lambda_{i} \leq \sum_{i=1}^{N_{s}} \frac{1}{N} = \frac{N_{s}}{N}\)<br> 즉, \(vN \leq N_{s}\)<br> (\(\lambda_{i} \gt 0\) 중에 \(\xi = 0\) 인 support vectors만 고려하면 \(\sum_{i=1}^{N} \lambda_{i} = \sum_{i=1}^{N_{s}}\) !!)</li> <li>\(\frac{N_{error}}{N} \leq v \leq \frac{N_{s}}{N}\) 이므로<br> \(v\) optimize하면 error rate와 support vector 개수도 bound 알 수 있음</li> <li>support vector 수 \(N_{s}\) 는 classifier performance에 있어서 매우 중요<br> (\(N_{s}\) 가 클수록 inner product 많이 계산해야 돼서 computational cost 높아짐)<br> (\(N_{s}\) 가 크면 training set 이외의 data에 대한 performance가 제한되어 poor generalization)</li> </ul> </li> </ul> <h3 id="kernel-method-for-svm">Kernel Method for SVM</h3> <ul> <li> <p>discriminant function :<br> \(x\) 의 inner product 꼴<br> \(g(\boldsymbol x) = \boldsymbol w^{T} \boldsymbol x + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \boldsymbol x_{i}^{T} \boldsymbol x + w_{0}\)</p> </li> <li> <p>Cover’s theorem :<br> non-linearly separable D-dim. space는<br> linearly separable space of high enough dim. 으로 transform 될 수 있다<br> (separating hyperplane의 optimality는 관심사 아님)</p> </li> <li> <p>Kernel Method for SVM :<br> discriminant function \(g(\boldsymbol x) = \boldsymbol w^{T} \boldsymbol x + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \boldsymbol x_{i}^{T} \boldsymbol x + w_{0}\) 에서<br> kernel \(K(\boldsymbol x_{i}, \boldsymbol x) = \boldsymbol x_{i}^{T} \boldsymbol x\)<br> (inner product b.w. support vector and input vector)<br> 대신<br> 다른 kernel \(K(\boldsymbol x_{i}, \boldsymbol x) = \Phi(\boldsymbol x_{i})^{T} \Phi(\boldsymbol x)\) 을 써서<br> non-linearly separable samples도 분류해보자!</p> <ul> <li>Step 1)<br> input vector \(\boldsymbol x\) 와 training samples \(\boldsymbol x_{i}\) 를 <code class="language-plaintext highlighter-rouge">high-dim.으로 project</code> by function \(\Phi(\cdot)\)</li> <li>Step 2)<br> transformed vector \(\Phi (\boldsymbol x)\) 와 \(\Phi (\boldsymbol x_{i})\) 에 대해 linear SVM 적용<br> \(g(\boldsymbol x) = \boldsymbol w^{T} \Phi (\boldsymbol x) + w_{0}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/4.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/4.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <code class="language-plaintext highlighter-rouge">Kernel Trick</code> :<br> \(\boldsymbol x_{i}^{T} \boldsymbol x_{j}\) 대신 \(K(\boldsymbol x_{i}, \boldsymbol x_{j}) = \Phi(\boldsymbol x_{i})^{T} \Phi(\boldsymbol x_{j})\) 쓰면 됨!! <ul> <li>optimization of dual form :<br> maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}\) <br> 대신<br> maximize \(L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} K(\boldsymbol x_{i}, \boldsymbol x_{j})\)<br> where \($K(\boldsymbol x_{i}, \boldsymbol x_{j}) = \Phi(\boldsymbol x_{i})^{T} \Phi(\boldsymbol x_{j})\)</li> <li>hyperplane :<br> \(g(\boldsymbol x) = \boldsymbol w^{T} \boldsymbol x + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \boldsymbol x_{i}^{T} \boldsymbol x + w_{0} = 0\)<br> 대신<br> \(g(\boldsymbol x) = \boldsymbol w^{T} \Phi(\boldsymbol x) + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \Phi(\boldsymbol x_{i})^{T} \Phi(\boldsymbol x) + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} K(\boldsymbol x_{i}, \boldsymbol x) + w_{0} = 0\)<br> where \(\boldsymbol w = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \Phi(\boldsymbol x_{i})\)</li> </ul> </li> <li>Remark : <ul> <li>polynomial learning machine, radial-basis function network, two-layer perceptron(single hidden layer) 와 같은<br> kernel-based learning machine을 만들 때<br> support vector learning algorithm을 사용 <ul> <li>polynomial :<br> \(K(x, z) = (x^{T} z + 1)^{q}\) for \(q \gt 0\)</li> <li>radial-basis function :<br> \(K(x, z) = \text{exp}(-\frac{\| x - z \|^{2}}{\sigma^{2}})\)</li> <li>hyperbolic tangent :<br> \(K(x, z) = \text{tanh}(\beta x^{T} z + \gamma)\) where typical value is \(\beta = 2\) and \(\gamma = 1\)</li> </ul> </li> </ul> </li> <li>문제 풀이 예시 :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/6-480.webp 480w,/assets/img/2024-10-28-Pattern2/6-800.webp 800w,/assets/img/2024-10-28-Pattern2/6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/5-480.webp 480w,/assets/img/2024-10-28-Pattern2/5-800.webp 800w,/assets/img/2024-10-28-Pattern2/5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/7-480.webp 480w,/assets/img/2024-10-28-Pattern2/7-800.webp 800w,/assets/img/2024-10-28-Pattern2/7-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/8-480.webp 480w,/assets/img/2024-10-28-Pattern2/8-800.webp 800w,/assets/img/2024-10-28-Pattern2/8-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/9-480.webp 480w,/assets/img/2024-10-28-Pattern2/9-800.webp 800w,/assets/img/2024-10-28-Pattern2/9-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="chapter-6-multilayer-neural-networks">Chapter 6. Multilayer Neural Networks</h2> <ul> <li>activation function : <ul> <li>unipolar sigmoid :<br> \(\phi (x) = \frac{1}{1 + exp(-x)}\)<br> \(\phi^{'} (x) = \phi (x) (1 - \phi (x))\)</li> <li>bipolar sigmoid (tanh) :<br> \(\phi (x) = \text{tanh} (x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\)<br> \(\phi^{'} (x) = 1 - \text{tanh}^{2} (x) = 1 - \phi^{2} (x)\) <ul> <li>tanh가 sigmoid보다 gradient 더 커서 같은 \(\eta\) 일 때 학습 더 많이 함</li> </ul> </li> <li>ReLU</li> </ul> </li> <li>weight initialization : <ul> <li>zero-mean uniform distribution \(U(0, \sigma^{2})\)<br> where \(\sigma^{2}\) is chosen so that std of induced local fields of neurons lie in the linear transition interval of sigmoid activation function</li> </ul> </li> <li>weight update :<br> \(w_{ji}(n+1) = w_{ji}(n) + \eta \delta_{j} (n) y_{i} (n) + \alpha (w_{ji} (n) - w_{ji} (n-1))\)<br> where \(\eta\) : learning rate<br> where \(\alpha\) : momentum constant<br> (momentum in back-prop has stabilizing effect when gradient has oscillate in sign)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/11.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/11.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/11.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/11.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="back-propagation-algorithm">Back-propagation Algorithm</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/10.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/10.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/10.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/10.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="issues-on-neural-networks">Issues on Neural Networks</h3> <ul> <li>Stopping criteria : <ul> <li>Euclidean norm of gradient reaches sufficiently small threshold</li> <li>absolute rate of change in average squared error per epoch is sufficiently small</li> <li>generalization performance (tested after each iter.) has peaked</li> </ul> </li> <li>Weight Update : <ul> <li>sample-by-sample mode :<br> weights are updated after presenting each training sample<br> \(w_{ji}(n+1) = w_{ji}(n) + \eta \delta_{j} (n) y_{i} (n) + \alpha (w_{ji} (n) - w_{ji} (n-1))\) <ul> <li>very sensitive to each sample so that the weight update term is very noisy</li> </ul> </li> <li>batch mode :<br> weights are updated after presenting entire set of training samples<br> \(w_{ji}(t+1) = w_{ji}(t) + \eta \frac{1}{N} \sum_{n=1}^{N} \delta_{j} (n) y_{i} (n) + \alpha (w_{ji} (t) - w_{ji} (t-1))\)</li> </ul> </li> <li>k-fold cross validation :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/12.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/12.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/12.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/12.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/13.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/13.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/13.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/13.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Normalization : Whitening <ul> <li>mean removal</li> <li>de-correlation</li> <li>scaling for equal covariance<br> (then input var. in training set becomes uncorrelated)<br> (then gradient descent converges faster)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/14.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/14.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/14.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/14.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Gradient Vanish 해결 방법 : <ul> <li>방법 1) Batch Normalization<br> Batch Normalization을 해서 input이 \(0\) 주위의 가파른 부분에 머무르도록 함<br> 근데 sigmoid의 경우 gradient가 [0, 0.25] 이고, tanh의 경우 gradient가 [0, 1] 이므로<br> 여전히 gradient vanishing 문제 발생</li> <li>방법 2) ReLU activation<br> ReLU의 gradient는 0 또는 1이므로<br> gradient value 1의 경우 gradient vanishing 문제 없음</li> <li>방법 3) Residual Network<br> skip connection 사용하여<br> non-linear activation 통과하지 않고 바로 gradient가 흘러들어갈 수 있도록 함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/15.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/15.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/15.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/15.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/16.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/16.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/16.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/16.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Bias-Variance dilemma : <ul> <li>prediction error의 종류로는 bias, variance, irreducible error가 있음<br> MSE \(E_{x, y, D}[\epsilon^{2} (x)] = E_{x, y, D}[(h_{D}(x) - y)^{2}]\)<br> \(= E_{x, D}[(h_{D}(x) - \bar h(x))^{2}] + E_{x}[(\bar h(x) - \bar y(x))^{2}] + E_{x, y}[(\bar y(x) - y)^{2}]\) <ul> <li>variance of model : \(E_{x, D}[(h_{D}(x) - \bar h(x))^{2}]\)<br> where \(h_{D}(x)\) : model output<br> where \(\bar h(x)\) : model mean <ul> <li>different test dataset으로 테스트했을 때 변하는 정도</li> <li>particular training dataset에 overfitting된 정도</li> </ul> </li> <li>bias of model : \(\bar h(x) - \bar y(x)\)<br> where \(\bar y(x)\) : label mean <ul> <li>data를 아무리 많이 학습시켜도 model 특성 때문에 발생하는 inherent error<br> e.g. linear classifier is biased to a particular kind of solution</li> </ul> </li> <li>data noise : \(E_{x, y}[(\bar y(x) - y)^{2}]\) <ul> <li>ambiguity due to data distribution and feature representation</li> </ul> </li> </ul> </li> <li>trade-off : <ul> <li>too simple model : high bias<br> \(\rightarrow\) 해결법 : pick more complex model</li> <li>too complex model : high variance<br> \(\rightarrow\) naive 해결법 : use more data, but it requires high cost</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/17.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/17.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/17.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/17.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>SSE (Sum of Squared Errors) :<br> 위의 유도 결과에서<br> 두 번째 term은 model weight와 무관하므로<br> back-propagation은 첫 번째 term만 minimize<br> 즉, \(y_{k}(x; W) \approx P(w_{k} | x)\)<br> 이 때, 잘 approx.하려면 model이 충분한 layers, neurons를 가지고 있어야 함 <ul> <li>\(y_{k}(x; W)\) : model output</li> <li> <table> <tbody> <tr> <td>$$P(w_{k}</td> <td>x)$$ : true posteriori probability (Bayes linear discriminant function)</td> </tr> </tbody> </table> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/18.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/18.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/18.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/18.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="chapter-7-stochastic-methods-for-pattern-classification">Chapter 7. Stochastic Methods for Pattern Classification</h2> <ul> <li>Learning : <ul> <li>deterministic learning :<br> 무조건 loss 작아지는 방향으로 이동<br> e.g. gradient descent</li> <li>stochastic learning :<br> 작은 확률이겠지만 energy 커지는 방향으로 이동하는 것도 허용<br> \(\rightarrow\) local minima에 빠지는 문제 해결하여 global minimum에 도달 가능 e.g. Simulated Annealing, Boltzmann Machine</li> </ul> </li> <li>Boltzmann Machine : <ul> <li>neuron : <ul> <li>visible neuron : \(\alpha = \{ \alpha^{i}, \alpha^{o} \}\)</li> <li>hidden neuron : \(\beta\)</li> </ul> </li> <li>probability \(P(\alpha) = \sum_{\beta} P(\alpha \beta) = \sum_{\beta} \frac{e^{- E_{\alpha \beta} / T}}{Z} = \sum_{\beta} \frac{e^{- E_{\alpha \beta} / T}}{\sum_{\alpha \beta} e^{- E_{\alpha \beta} / T}}\)<br> where energy \(E_{\gamma} = - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} w_{ij} s_{i} s_{j}\)<br> where \(w_{ij} = w_{ji}\) is symmetric with \(w_{ii} = 0\)<br> 즉, energy \(E_{\gamma}\) 가 낮을수록 configuration \(\alpha\) 일 확률 \(P(\alpha)\) 가 높음</li> <li>goal :<br> \(P(\alpha)\) 가 \(Q(\alpha)\) 와 최대한 가까워지도록<br> weight \(w_{ij}\) 를 학습 <ul> <li>marginal (estimated) distribution of generated samples \(P(\alpha)\) :<br> network가 free-running일 때 (all neurons가 자유롭게 업데이트될 수 있을 때)<br> visible neuron이 \(\alpha\) 일 확률</li> <li>observed (desired) distribution of training samples \(Q(\alpha)\) :<br> network의 visible neuron이 \(\alpha\) 로 clamped되었을 때<br> visible neuron이 \(\alpha\) 일 확률</li> <li>KL-divergence \(D_{KL} (Q(\alpha), P(\alpha)) = \sum_{\alpha} Q(\alpha) \text{log} \frac{Q(\alpha)}{P(\alpha)}\) 를 minimize하면<br> \(\Delta w_{ij} = - \eta \frac{\partial D_{KL}}{\partial w_{ij}} = \eta \sum_{\alpha} \frac{Q(\alpha)}{P(\alpha)} \frac{\partial P(\alpha)}{\partial w_{ij}} = \frac{\eta}{2T}(\sum_{\alpha \beta} Q(\alpha) P(\beta | \alpha) s_{i} s_{j} - \sum_{\alpha^{'} \beta^{'}} P(\alpha^{'} \beta^{'}) s_{i} s_{j}) = \frac{\eta}{2T}(E_{Q}[s_{i} s_{j}]_{\text{clamped by} \alpha} - E[s_{i} s_{j}]_{\text{free}})\)<br> (pf는 아래에)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/19.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/19.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/19.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/19.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Boltzmann Machine with I-O Association : <ul> <li>neuron : <ul> <li>visible neuron : <ul> <li>input neuron : \(\alpha\)</li> <li>output neuron : \(\gamma\)</li> </ul> </li> <li>hidden neuron : \(\beta\)</li> </ul> </li> <li>goal :<br> \(P(\gamma | \alpha)\) 가 \(Q(\gamma | \alpha)\) 와 최대한 가까워지도록<br> weight \(w_{ij}\) 를 학습 <ul> <li>KL-divergence \(D_{KL} (Q(\gamma | \alpha), P(\gamma | \alpha)) = D_{KL} (Q(\alpha \gamma), P(\alpha \gamma)) - D_{KL} (Q(\alpha), P(\alpha))\) 를 minimize하면<br> \(\Delta w_{ij} = \frac{\eta}{2T}(E_{Q}[s_{i} s_{j}]_{\text{clamped by} \alpha \gamma} - E[s_{i} s_{j}]_{\text{clamped by} \alpha})\)<br> (pf는 아래에)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/20.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/20.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/20.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/20.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/21.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/21.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/21.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/21.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/22.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/22.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/22.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/22.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Restricted Boltzmann Machine (RBM) :<br> Boltzmann Machine with bi-partite graph of visible and hidden units <ul> <li>probability \(P(v) = \sum_{h} P(v, h) = \sum_{h} \frac{e^{- E(v, h)}}{Z}\)<br> where (intractable) partition function \(Z = \sum_{v, h} e^{- E(v, h)}\)<br> where energy \(E(v, h) = - h^{T} W v - b^{T} v - c^{T} h\) for \(T=1\)</li> <li>goal :<br> training dataset의 distribution을 학습!! <ul> <li>훈련 끝나고나면 해당 distribution을 따르는 new sample을 generate할 수 있음!! image inpainting에도 적용 가능</li> <li>label과의 joint distribution을 학습하여 classification 수행 가능!! feed-forward layer를 initialize할 때 RBM weight 사용 가능</li> <li>feature extractor 역할 수행 가능!!</li> </ul> </li> </ul> </li> <li>Training RBM 수식 유도 : <ul> <li>weight \(\theta = [W, b, c]\) 에 대해<br> \(\hat \theta = \text{argmax}_{\theta} \text{ln} P(v | \theta) = \text{argmax}_{\theta} \text{ln} \sum_{h} \frac{e^{- E(v, h | \theta)}}{Z} = \text{argmax}_{\theta} \text{ln} \sum_{h} e^{- E(v, h | \theta)} - \text{ln} Z\)<br> \(\rightarrow\)<br> \(\frac{\partial - \text{ln} P(v | \theta)}{\partial \theta} = \cdots = \sum_{h} P(h | v, \theta) \frac{\partial E(v, h | \theta)}{\partial \theta} - \sum_{v, h} P(v, h | \theta) \frac{\partial E(v, h | \theta)}{\partial \theta} = E_{h \sim P(h | v, \theta)}[\frac{\partial E(v, h | \theta)}{\partial \theta}] - E_{(v, h) \sim P(v, h | \theta)}[\frac{\partial E(v, h | \theta)}{\partial \theta}]\) <ul> <li>오른쪽 expectation 식 :<br> model distribution \(P(v, h | \theta)\) 의 모든 경우의 수에 대해 expectation \(E[\cdot]\) 계산하려면 \(2^{m+n}\) 로 costly하므로<br> MCMC (Markov chain Monte Carlo) 기법으로 해당 distribution에서 <code class="language-plaintext highlighter-rouge">Gibbs sampling</code> 수행하여<br> 오른쪽 expectation 식을 sample mean으로 approx.</li> </ul> </li> <li>RBM의 경우 visible units끼리, hidden units끼리는 connection 없으므로<br> given visible units에 대해 hidden units끼리는 <code class="language-plaintext highlighter-rouge">conditionally independent</code>하므로<br> 같은 layer에 있는 units는 쉽게 jointly(동시에) Gibbs sampling 가능<br> \(\rightarrow\)<br> <code class="language-plaintext highlighter-rouge">Gibbs sampling</code>은 두 단계로 요약 가능 (block Gibbs sampling) <ul> <li>Step 1)<br> sample \(h\) based on \(P(h | v)\)</li> <li>Step 2)<br> sample \(v\) based on \(P(v | h)\)</li> </ul> </li> <li>Conditional Distribution :<br> proof는 아래 아래 사진에 <ul> <li> \[P(h_{i} = 1 | v) = \sigma (\sum_{j=1}^{m} w_{ij} v_{j} + c_{i}) = \sigma (\boldsymbol w_{i} \cdot \boldsymbol v + c_{i})\] </li> <li> \[P(v_{j} = 1 | h) = \sigma (\sum_{i=1}^{n} w_{ij} h_{i} + b_{j}) = \sigma (\boldsymbol w_{j} \cdot \boldsymbol h + b_{j})\] </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/27.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/27.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/27.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/27.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/23.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/23.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/23.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/23.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Training RBM 수식 유도 (continued) : <ul> <li>goal :<br> \(P(v | \theta)\) 가 \(Q(v)\) 와 최대한 가까워지도록<br> weight \(W, b, c\) 를 학습 <ul> <li> </li> <li>Gradient of Log-Likelihood :<br> (proof는 아래 사진에)<br> Let’s define<br> \(\boldsymbol h (\boldsymbol v_{0}) = P(\boldsymbol h = \boldsymbol 1 | \boldsymbol v_{0}, \theta) = \sigma(\boldsymbol W \boldsymbol v_{0} + \boldsymbol b)\) <br> \(\boldsymbol v_{0} \in S\) : clamped training sample where \(S\) is training dataset<br> \(\boldsymbol v_{k}\) : generated sample by RBM<br> KL-divergence \(D_{KL} (Q(v), P(v | \theta)) = \sum_{v} Q(v) \text{log} \frac{Q(v)}{P(v | \theta)}\) 를 minimize하려면 <ul> <li> \[\Delta W \propto \frac{1}{N} \sum_{\boldsymbol v_{0} \in S} \boldsymbol h (\boldsymbol v_{0}) \boldsymbol v_{0}^{T} - \boldsymbol h (\boldsymbol v_{k}) \boldsymbol v_{k}^{T}\] </li> <li> \[\Delta b \propto \frac{1}{N} \sum_{\boldsymbol v_{0} \in S} \boldsymbol v_{0}^{T} - \boldsymbol v_{k}^{T}\] </li> <li> \[\Delta c \propto \frac{1}{N} \sum_{\boldsymbol v_{0} \in S} \boldsymbol h (\boldsymbol v_{0}) - \boldsymbol h (\boldsymbol v_{k})\] </li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/24.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/24.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/24.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/24.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>k-step Contrastive Divergence :<br> model expectation은 exponential cost 가지므로<br> sampling from RBM distribution<br> 대신<br> k-step sampling from Gibbs chain initialized with training data<br> Then, \(\frac{\partial - \text{ln} P(v | \theta)}{\partial \theta} \approx \sum_{h} P(h | v_{0}) \frac{\partial E(v_{0}, h)}{\partial \theta} - \sum_{h} P(h | v_{k}) \frac{\partial E(v_{k}, h)}{\partial \theta}\)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/26.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/26.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/26.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/26.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Training RBM Summary : <ul> <li>Step 1) <code class="language-plaintext highlighter-rouge">Positive Phase</code><br> training sample \(\boldsymbol v_{0}\) 에서 시작하여<br> sample \(h_{i}\) from \(Q(h_{i} = 1 | v_{j}) = \sigma (\boldsymbol w_{i} \cdot \boldsymbol v + c_{i})\)<br> \(\rightarrow\)<br> \(h_{i} = \begin{cases} 1 &amp; \text{if} &amp; \text{rand}(0, 1) \lt Q(h_{i}=1 | v_{j}) \\ 0 &amp; \text{O.W.} &amp; \text{} \end{cases}\)</li> <li>Step 2) <code class="language-plaintext highlighter-rouge">Negative Phase</code> (Recon. Phase)<br> sample \(v_{j}\) from \(P(v_{j} = 1 | h_{i}) = \sigma (\boldsymbol w_{j} \cdot \boldsymbol h + b_{j})\)<br> \(\rightarrow\)<br> \(v_{j} = \begin{cases} 1 &amp; \text{if} &amp; \text{rand}(0, 1) \lt P(v_{j}=1 | h_{i}) \\ 0 &amp; \text{O.W.} &amp; \text{} \end{cases}\)</li> <li>Step 3) 위의 과정을 k-step 반복<br> \(\boldsymbol v_{0}\) 으로부터 \(\boldsymbol h(\boldsymbol v_{0})\) 얻고<br> \(\cdots\) (Gibbs sampling k-step 반복) \(\cdots\)<br> \(\boldsymbol v_{k}\) 으로부터 \(\boldsymbol h(\boldsymbol v_{k})\) 얻음</li> <li>Step 4) <code class="language-plaintext highlighter-rouge">Update Weights</code> <ul> <li> \[\Delta W \propto \frac{1}{N} \sum_{\boldsymbol v_{0} \in S} \boldsymbol h (\boldsymbol v_{0}) \boldsymbol v_{0}^{T} - \boldsymbol h (\boldsymbol v_{k}) \boldsymbol v_{k}^{T}\] </li> <li> \[\Delta b \propto \frac{1}{N} \sum_{\boldsymbol v_{0} \in S} \boldsymbol v_{0}^{T} - \boldsymbol v_{k}^{T}\] </li> <li> \[\Delta c \propto \frac{1}{N} \sum_{\boldsymbol v_{0} \in S} \boldsymbol h (\boldsymbol v_{0}) - \boldsymbol h (\boldsymbol v_{k})\] </li> </ul> </li> </ul> </li> <li>RBM Code :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/25.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/25.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/25.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/25.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Example :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/28.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/28.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/28.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/28.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-28-Pattern2/29.PNG-480.webp 480w,/assets/img/2024-10-28-Pattern2/29.PNG-800.webp 800w,/assets/img/2024-10-28-Pattern2/29.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-28-Pattern2/29.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="chapter-8-non-metric-methods-for-pattern-classification">Chapter 8. Non-metric Methods for Pattern Classification</h2> <p>training data 전체에 부합하는 parametric boundary 찾는 대신<br> feature space를 여러 tree level에서 sub-regions로 나눠서 classify independently</p> <ul> <li>Decision Tree :<br> not unique for partition <ul> <li>5p TBD</li> </ul> </li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="disqus_thread" style="max-width: 800px; margin: 0 auto;"> <script type="text/javascript">var disqus_shortname="semyeong-yu",disqus_identifier="/blog/2024/Pattern2",disqus_title="EE534 Pattern Recognition Final";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Semyeong Yu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5YMLX9VHFX"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5YMLX9VHFX");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>