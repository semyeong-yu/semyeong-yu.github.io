<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> SfMLearner | Semyeong Yu </title> <meta name="author" content="Semyeong Yu"> <meta name="description" content="Unsupervised Learning of Depth and Ego-Motion from Video"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://semyeong-yu.github.io/blog/2024/SfMLearner/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8.0.6/dist/styles.min.css"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-bundle.min.css" integrity="sha256-yUoNxsvX+Vo8Trj3lZ/Y5ZBf8HlBFsB6Xwm7rH75/9E=" crossorigin="anonymous"> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "SfMLearner",
            "description": "Unsupervised Learning of Depth and Ego-Motion from Video",
            "published": "April 06, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Semyeong</span> Yu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>SfMLearner</h1> <p>Unsupervised Learning of Depth and Ego-Motion from Video</p> </d-title> <d-article> <h1 id="unsupervised-learning-of-depth-and-ego-motion-from-video">Unsupervised Learning of Depth and Ego-Motion from Video</h1> <h4 id="tinghui-zhou-matthew-brown-noah-snavely-david-g-lowe">Tinghui Zhou, Matthew Brown, Noah Snavely, David G. Lowe</h4> <blockquote> <p>paper :<br> <a href="https://arxiv.org/abs/1704.07813" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/1704.07813</a><br> code :<br> <a href="https://github.com/tinghuiz/SfMLearner" rel="external nofollow noopener" target="_blank">https://github.com/tinghuiz/SfMLearner</a></p> </blockquote> <blockquote> <p>핵심 요약 :</p> <ul> <li>monocular camera</li> <li>unsupervised learning by reconstruction loss (view synthesis)</li> <li>view synthesis (reconstruct target view from source view) by projection and warping</li> <li>projection할 때 아래에서 구한 depth와 pose 이용</li> <li>use 3 networks <ul> <li>single-view depth CNN</li> <li>multi-view pose CNN</li> <li>explainability soft mask</li> </ul> </li> </ul> </blockquote> <h2 id="introduction">Introduction</h2> <ul> <li>SfM : Structure from Motion<br> end-to-end unsupervised learning from monocular video (only one camera lens)</li> <li> <code class="language-plaintext highlighter-rouge">single-view</code> depth estimation by per-pixel depth map</li> <li> <code class="language-plaintext highlighter-rouge">multi-view</code> camera motion (= <code class="language-plaintext highlighter-rouge">ego-motion</code> = <code class="language-plaintext highlighter-rouge">pose</code>) by <code class="language-plaintext highlighter-rouge">6-DoF transformation matrices</code> </li> <li> <code class="language-plaintext highlighter-rouge">unsupervised</code> learning : 직접적인 GT data가 아니라 <code class="language-plaintext highlighter-rouge">view synthesis (reconstruction term)를 supervision</code>으로 씀</li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li>simultaneous estimation of structure and motion through deep learning</li> <li>end-to-end learning of transformation matrix without learning geometry explicitly</li> <li>learning of 3D single-view from registered 2D views</li> <li>unsupervised/self-supervised learning from video</li> </ul> <h2 id="method">Method</h2> <h4 id="approach">Approach</h4> <p>Assumption :<br> Scenes, which we are interested in, are mostly rigid, so changes across different frames are dominated by camera motion</p> <h4 id="view-synthesis-as-supervision">View Synthesis as supervision</h4> <ul> <li>View Synthesis : as supervision of depth and pose (추후 설명 예정)</li> <li>loss function (reconstruction term) :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/1-480.webp 480w,/assets/img/2024-04-06-SfMLearner/1-800.webp 800w,/assets/img/2024-04-06-SfMLearner/1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-06-SfMLearner/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>\(p\) : index of target view’s pixel coordinates<br> \(s\) : index of source views<br> \(I_{t}(p)\) : target view<br> \(\hat I_{s}(p)\) : source view warped to target coordinate frame (= reconstructed target view) using predicted depth \(\hat D_{t}\) and \(4 \times 4\) camera transformation matrix \(\hat T_{t \rightarrow s}\) and source view \(I_{s}\)</p> <ul> <li>pipeline for depth and pose estimation :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/2-480.webp 480w,/assets/img/2024-04-06-SfMLearner/2-800.webp 800w,/assets/img/2024-04-06-SfMLearner/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-06-SfMLearner/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="differentiable-depth-image-based-rendering">Differentiable depth image-based rendering</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/3-480.webp 480w,/assets/img/2024-04-06-SfMLearner/3-800.webp 800w,/assets/img/2024-04-06-SfMLearner/3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-06-SfMLearner/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ol> <li> <p><code class="language-plaintext highlighter-rouge">Depth CNN</code>을 통해 <code class="language-plaintext highlighter-rouge">target view</code> (single view)로부터 <code class="language-plaintext highlighter-rouge">depth prediction</code> \(\hat D_{t}\) 얻기</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Pose CNN</code>을 통해 <code class="language-plaintext highlighter-rouge">target &amp; source view</code> (multi-view)로부터 \(4 \times 4\) <code class="language-plaintext highlighter-rouge">camera transformation matrix</code> \(\hat T_{t \rightarrow s}\) 얻기</p> </li> <li>target view의 pixels를 source view coordinate으로 <code class="language-plaintext highlighter-rouge">project</code>하기<br> 값이 아니라 <code class="language-plaintext highlighter-rouge">대응되는 위치</code>를 구하기 위해<br> projection할 때 depth와 pose 이용 <ul> <li>monocular camera이므로 두 카메라 사이의 상대적인 위치를 설명하는 \([R \vert t]\)는 고려 안함</li> <li>\(K^{-1}p_{t}\) : target view coordinate에서 2D 좌표 \(\rightarrow\) 3D 좌표</li> <li>\(\hat D_{t}(p_{t})K^{-1}p_{t}\) : target view의 3D depth map (= 2D depth \(\times\) 3D 좌표)<br> full 3D volumetric은 아니고, surface만 나타내는 3D target</li> <li>\(\hat T_{t \rightarrow s} \hat D_{t}(p_{t})K^{-1}p_{t}\) : 3D depth map projected from target view to source view</li> <li>\(K \hat T_{t \rightarrow s} \hat D_{t}(p_{t})K^{-1}p_{t}\) : source view coordinate에서 3D 좌표 \(\rightarrow\) 2D 좌표<br> <code class="language-plaintext highlighter-rouge">target view의 pixel 좌푯값을 source view의 좌푯값으로 project하는 데 중간에 depth map이 왜 필요한 거지???</code> </li> </ul> </li> <li>source view coordinate에서 differentiable bilinear <code class="language-plaintext highlighter-rouge">interpolation</code>으로 value 얻은 뒤 <code class="language-plaintext highlighter-rouge">warp to target coordinate</code> (= <code class="language-plaintext highlighter-rouge">reconstructed target view</code>)<br> source view의 pixel 값들을 이용해서 reconstruct target view</li> </ol> <h4 id="modeling-the-model-limitation">Modeling the model limitation</h4> <p>Assumption :</p> <ol> <li> <p>objects are static except camera (changes are dominated by camera motion)<br> 물체들이 움직이지 않아야 Depth CNN과 Pose CNN이 같은 coordinate에 대해 project할 수 있다.</p> </li> <li> <p>there is no occlusion/disocclusion between target view and source view<br> target view와 source views 중 하나라도 물체가 가려져서 안보인다면 projection 정보가 없어 학습에 문제가 된다.</p> </li> <li> <p>surface is Lambertain so that photo-consistency error is meaningful<br> 어떤 방향에서 보든 표면이 isotropic 똑같은 밝기로 보인다고 가정 \(\rightarrow\) photo-consistency에 차이가 있을 경우 이는 다른 surface를 의미함</p> </li> </ol> <h4 id="overcoming-the-gradient-locality-at-loss-term">Overcoming the gradient locality at loss term</h4> <ol> <li> <p>To improve robustness, train additional network which predicts <code class="language-plaintext highlighter-rouge">explainability soft mask</code> \(\hat E_{s}\) (= <code class="language-plaintext highlighter-rouge">per-pixel weight</code>), and add it to reconstruction loss term.<br> deep-learning model은 black-box이므로 explainablity는 중요한 요소</p> </li> <li> <p>trivial sol. \(\hat E_{s} = 0\)을 방지하기 위해, add <code class="language-plaintext highlighter-rouge">regularization</code> term that encourages nonzero prediction of \(\hat E_{s}\)</p> </li> <li> <p>직접 pixel intensity difference로 reconstruction loss를 얻으므로, GT depth &amp; pose로 project하여 얻은 \(p_{s}\) 가 low-texture region or far region에 있을 경우 training 방해 (common issue in motion estimation)<br> \(\rightarrow\) 해결 1. use conv. encoder-decoder with small bottleneck<br> \(\rightarrow\) 해결 2. add <code class="language-plaintext highlighter-rouge">multi-scale</code> and <code class="language-plaintext highlighter-rouge">smoothness loss</code> term<br> (less sensitive to architecture choice, so 이 논문은 해결 2. 적용)</p> </li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/4-480.webp 480w,/assets/img/2024-04-06-SfMLearner/4-800.webp 800w,/assets/img/2024-04-06-SfMLearner/4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-06-SfMLearner/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> s : source view image index / p : target view pixel index </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/5-480.webp 480w,/assets/img/2024-04-06-SfMLearner/5-800.webp 800w,/assets/img/2024-04-06-SfMLearner/5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-06-SfMLearner/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> l : multi-scale / s : source view image index </div> <h4 id="network-architecture">Network Architecture</h4> <ul> <li>Network 1. <code class="language-plaintext highlighter-rouge">Single-view Depth CNN</code><br> input : target view<br> output : per-pixel depth map<br> DispNet encoder-decoder architecture</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/6-480.webp 480w,/assets/img/2024-04-06-SfMLearner/6-800.webp 800w,/assets/img/2024-04-06-SfMLearner/6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-06-SfMLearner/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Network 2. <code class="language-plaintext highlighter-rouge">Multi-view Pose CNN</code> (아래 figure의 파란 부분)<br> input : target view concatenated with all source views<br> output : 6-DoF relative poses between target view and each source view<br> (Pose CNN estimates <code class="language-plaintext highlighter-rouge">6 channels (3 Euler angles + 3D translation vector)</code> for each source view, and then it is converted to \(4 \times 4\) <code class="language-plaintext highlighter-rouge">transformation matrix</code>)</li> </ul> <p><code class="language-plaintext highlighter-rouge">어떻게 transformation matrix로 변환???</code></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/7-480.webp 480w,/assets/img/2024-04-06-SfMLearner/7-800.webp 800w,/assets/img/2024-04-06-SfMLearner/7-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-06-SfMLearner/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/7-480.webp 480w,/assets/img/2024-04-06-SfMLearner/7-800.webp 800w,/assets/img/2024-04-06-SfMLearner/7-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-06-SfMLearner/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Network 3. <code class="language-plaintext highlighter-rouge">Explainablity soft mask</code> (= <code class="language-plaintext highlighter-rouge">reconstruction weight per pixel</code>) (위의 figure의 빨간 부분)<br> output : multi-scale explainability masks<br> (it estimates <code class="language-plaintext highlighter-rouge">2 channels</code> for each source view at each prediction layer)</li> </ul> <p><code class="language-plaintext highlighter-rouge">weight per pixel인데 왜 2 channels are needed for explainability mask???</code></p> <h2 id="experiments">Experiments</h2> <p>Train : BN, Adam optimizer, monocular camera (one camera lens), resize input image<br> Test : arbitrary input image size</p> <h4 id="single-view-depth-estimation">Single-view depth estimation</h4> <ul> <li>train model on the split (exclude frames from test sequences and exclude static scene’s pixels with mean optical flow magnitude &lt; 1)</li> <li>pre-trained on Cityscapes dataset / fine-tuned on KITTI dataset / test on Make3D dataset</li> <li>may improve if we also use left-right cycle consistency loss</li> <li>ablation study 결과, explainablity mask를 추가하고 fine-tuning하는 게 더 좋은 성능 도출</li> </ul> <h4 id="multi-view-pose-estimation">Multi-view pose estimation</h4> <ul> <li>trained on KITTI odometry(change in position over time by motion sensor) dataset</li> <li>measurement :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/8-480.webp 480w,/assets/img/2024-04-06-SfMLearner/8-800.webp 800w,/assets/img/2024-04-06-SfMLearner/8-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-06-SfMLearner/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>ATE : Absolute Trajectory Error<br> left/right turning magnitude : coordinate diff. in the side-direction between start and ending frame at test<br> Mean Odom. : mean of car motion for 5-frame snippets from GT odometry dataset<br> ORB-SLAM(full) : recover odometry using all frames for loop closure and re-localization<br> ORB-SLAM(short) : Ours에서처럼, use 5-frame snippets as input<br> \(\rightarrow\) 특히 small left/right turning magnitude (car is mostly driving forward) 상황에서 Ours가 ORB-SLAM(short)보다 성능 더 좋으므로 monocular SLAM system의 local estimation module을 Ours가 대체할 수 있을 것이라 예상​<br> (<code class="language-plaintext highlighter-rouge">SLAM 논문 아직 안 읽어봄. 읽어보자.</code>)</p> <h4 id="visualizing-explainability-prediction">Visualizing Explainability Prediction</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-06-SfMLearner/9-480.webp 480w,/assets/img/2024-04-06-SfMLearner/9-800.webp 800w,/assets/img/2024-04-06-SfMLearner/9-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-06-SfMLearner/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> highlighted pixels at explainability mask : predicted to be unexplainable </div> <p>explainability = per-pixel weight (confidence 느낌) for reconstruction</p> <p>row 1 ~ 3 : due to motion (dynamic objects are unexplainable)<br> row 4 ~ 5 : due to occlusion/visibility (disappeared objects are unexplainable)<br> row 6 ~ 7 : due to other factors (e.g. depth CNN has low confidence on thin structures)</p> <h2 id="discussion">Discussion</h2> <h4 id="contribution">Contribution</h4> <ul> <li>end-to-end <code class="language-plaintext highlighter-rouge">unsupervised</code> learning from <code class="language-plaintext highlighter-rouge">monocular</code> sequences<br> (기존에는 gt depth로 depth supervision 또는 calibrated stereo images로 pose supervision이었지만, 본 논문은 <code class="language-plaintext highlighter-rouge">view synthesis (reconstruction)을 supervision으로</code> 써서 unsupervised learning으로도 comparable performance 달성)</li> <li>depth CNN recognizes common structural features of objects, and pose CNN uses image correspondence with estimating camera motion</li> </ul> <h4 id="limitation">Limitation</h4> <ol> <li> <p><code class="language-plaintext highlighter-rouge">dynamic objects (X) / occlusion (X) / must be Lambertain surface / vast open scenes (X) / when objects are close to the front of camera (X) / thin structure (X)</code><br> \(\rightarrow\) 위의 한계들을 개선하고자 explainablity mask (= per-pixel reconstruction confidence 느낌) 도입했지만, it is implicit consideration</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">assume that camera intrinsic K is given</code>, so not generalized to the random videos with unknown camera types</p> </li> <li> <p>predict simplified 3D depth map of <code class="language-plaintext highlighter-rouge">surface</code> (<code class="language-plaintext highlighter-rouge">not full 3D volumetric representation</code>)</p> </li> </ol> <p>중간중간에 있는 질문들은 아직 이해하지 못해서 남겨놓은 코멘트입니다.<br> 추후에 다시 읽어보고 이해했다면 업데이트할 예정입니다.<br> 혹시 알고 계신 분이 있으면 댓글로 남겨주시면 감사하겠습니다!</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="disqus_thread" style="max-width: 800px; margin: 0 auto;"> <script type="text/javascript">var disqus_shortname="semyeong-yu",disqus_identifier="/blog/2024/SfMLearner",disqus_title="SfMLearner";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Semyeong Yu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5YMLX9VHFX"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5YMLX9VHFX");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>