<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Deblurring 3D Gaussian Splatting | Semyeong Yu </title> <meta name="author" content="Semyeong Yu"> <meta name="description" content="ECCV 2024"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://semyeong-yu.github.io/blog/2024/Deblurring3DGS/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Deblurring 3D Gaussian Splatting",
            "description": "ECCV 2024",
            "published": "October 30, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Semyeong</span> Yu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Deblurring 3D Gaussian Splatting</h1> <p>ECCV 2024</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#related-works">Related Works</a> </div> <div> <a href="#defocus-blur">Defocus Blur</a> </div> <div> <a href="#camera-motion-blur">Camera motion Blur</a> </div> <div> <a href="#compensation-for-sparse-point-cloud">Compensation for Sparse Point Cloud</a> </div> <div> <a href="#experiment">Experiment</a> </div> <div> <a href="#results">Results</a> </div> <div> <a href="#ablation-study">Ablation Study</a> </div> <div> <a href="#limitation-and-future-work">Limitation and Future Work</a> </div> <div> <a href="#code-review">Code Review</a> </div> <div> <a href="#question">Question</a> </div> </nav> </d-contents> <h2 id="deblurring-3d-gaussian-splatting-eccv-2024">Deblurring 3D Gaussian Splatting (ECCV 2024)</h2> <h4 id="byeonghyeon-lee-howoong-lee-xiangyu-sun-usman-ali-eunbyung-park">Byeonghyeon Lee, Howoong Lee, Xiangyu Sun, Usman Ali, Eunbyung Park</h4> <blockquote> <p>paper :<br> <a href="https://arxiv.org/abs/2401.00834" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2401.00834</a><br> project website :<br> <a href="https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/" rel="external nofollow noopener" target="_blank">https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/</a><br> code :<br> <a href="https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting" rel="external nofollow noopener" target="_blank">https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting</a></p> </blockquote> <blockquote> <p>핵심 :</p> <ol> <li>defocus blur 구현 :<br> MLP로 covariance(rotation, scaling)의 변화량을 모델링해서<br> covariance를 키워서<br> defocus-blurred image 얻음</li> <li>camera motion blur 구현 :<br> MLP로 position 및 covariance의 변화량을 모델링해서<br> M개의 3DGS sets를 만든 뒤<br> 이걸로 만든 M개의 sharp imgs를 average해서<br> camera-motion-blurred image 얻음</li> <li>위의 MLP를 training에서만 사용하므로<br> still real-time rendering at inference</li> <li>sparse point clouds 보상하기 위해 points 추가<br> 또한 먼 거리에 있는 3DGS는 덜 prune out</li> </ol> </blockquote> <h3 id="introduction">Introduction</h3> <ul> <li>3DGS : <ul> <li>novel-view로 inference할 때<br> NeRF는 새로운 각도를 MLP에 넣어야만 color, opacity 얻을 수 있지만<br> 3DGS는 spherical harmonics, explicit 기법이라 새로운 각도에 대해서도 바로 color, opacity 얻을 수 있어서<br> volume rendering이 빠름</li> <li>differentiable splatting-based rasterization with parallelism</li> </ul> </li> <li>본 논문 : <ul> <li>핵심 : <ul> <li>각 3DGS의 <code class="language-plaintext highlighter-rouge">covariance</code>를 수정하여 <code class="language-plaintext highlighter-rouge">blur(adjacent pixels의 혼합)를 모델링하는 작은 MLP</code> 사용</li> <li>training 시에는 MLP output 곱해서 blurry image를 생성하고<br> inference 시에는 MLP 사용하지 않아서 real-time으로 sharp image 생성</li> </ul> </li> <li>문제 : <ul> <li>3DGS는 initial point cloud에 많이 의존하는데<br> given images가 <code class="language-plaintext highlighter-rouge">blurry</code>하면 SfM은 유효한 feature를 식별하지 못해서 <code class="language-plaintext highlighter-rouge">매우 적은 수의 point</code> cloud를 추출함</li> <li>심지어 depth가 크면 SfM은 맨 끝에 있는 점을 거의 추출하지 않음</li> </ul> </li> <li>해결 : <ul> <li>sparse point cloud를 방지하고자<br> <code class="language-plaintext highlighter-rouge">N-nearest-neighbor interpolation으로 points 추가</code> </li> <li>먼 거리의 평면에 많은 Gaussian을 유지하기 위해<br> <code class="language-plaintext highlighter-rouge">위치에 따라 Gaussian pruning</code> </li> </ul> </li> <li>contribution :<br> SOTA qualtiy인데 훨씬 빠른 rendering speed (\(\gt 200\) FPS)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/1m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/1m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/1m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/1m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Overall Architecture </div> <h3 id="related-works">Related Works</h3> <ul> <li>Image Deblurring : <ul> <li>\(g(x) = \sum_{s \in S_{h}} h(x, s) f(x) + n(x)\)<br> where \(g(x)\) : blurry image and \(f(x)\) : latent sharp image<br> where \(h(x, s)\) : blur kernel or PSF (Point Spread Function)<br> where \(n(x)\) : additive white Gaussian noise (occurs in nature images)</li> <li>지금까지 2D image deblurring은 많이 연구되어 왔는데<br> 3D scene deblurring은 3D view consistency 부족 때문에 연구하기 어려웠음</li> </ul> </li> <li>Fast NeRF : <ul> <li>방법 1)<br> use additional data-structure to reduce the size and number of MLP layers<br> but, fail to reach real-time view synthesis <ul> <li>grid-based :<br> Hexplane, TensoRF, K-planes, Mip-grid, Masked wavelet representation, Direct voxel grid optimization, F2-nerf</li> <li>hash-based :<br> InstantNGP, Zip-nerf</li> </ul> </li> <li>방법 2)<br> trained param.을 faster representation으로 bake해서 real-time rendering <ul> <li>Baking neural radiance fields, Merf, Bakedsdf</li> </ul> </li> </ul> </li> <li>Deblurring NeRF : <ul> <li>DoF-NeRF <d-cite key="DofNeRF">[1]</d-cite> : <ul> <li>단점 :<br> train하기 위해 all-in-focus image와 blurry image 모두 필요<br> (all-in-focus image : 화면 전체가 초점이 맞춰져 있는 image)</li> </ul> </li> <li>Deblur-NeRF <d-cite key="DeblurNeRF">[2]</d-cite> : <ul> <li>장점 :<br> train할 때 all-in-focus image 필요 없음</li> <li>핵심 :<br> additional small MLP 사용해서<br> per-pixel blur kernel 예측</li> </ul> </li> <li>DP-NeRF <d-cite key="DpNeRF">[3]</d-cite> and PDRF <d-cite key="PDRF">[4]</d-cite> : <ul> <li>Deblur-NeRF 발전시킴</li> </ul> </li> <li>Hybrid <d-cite key="Hybrid">[5]</d-cite> and Sharp-NeRF <d-cite key="SharpNeRF">[6]</d-cite> and BAD-NeRF <d-cite key="BADNeRF">[7]</d-cite> : <ul> <li>camera motion blur와 defocus blur 중 하나만 다룸</li> </ul> </li> </ul> </li> <li>Deblurring NeRF 요약 : <ul> <li>deblur task 잘 수행하지만<br> NeRF 자체가 rendering time이 오래 걸림<br> \(\rightarrow\)<br> real-time differentiable rasterizer 이용하는<br> 3DGS로 deblur task 수행하자!</li> </ul> </li> </ul> <h3 id="background">Background</h3> <ul> <li> <p>3DGS <a href="https://semyeong-yu.github.io/blog/2024/GS/">Link</a> 참고</p> </li> <li> <p>Blur :</p> <ul> <li>Defocus Blur :<br> 렌즈의 <code class="language-plaintext highlighter-rouge">초점이 맞지 않아서</code> 흐려진 경우<br> e.g. 인물 사진에서 인물만 초점이 맞고 배경은 흐릿한 경우</li> <li>Camera Motion Blur :<br> 셔터가 열려 있는 동안 카메라가 움직이거나 피사체가 <code class="language-plaintext highlighter-rouge">움직여서</code> 흐려진 경우<br> e.g. 달리는 자동차를 촬영한 경우</li> </ul> </li> </ul> <h3 id="defocus-blur">Defocus Blur</h3> <ul> <li>Motivation : <ul> <li>Defocus Blur는 일반적으로<br> 실제 image와 PSF(point spread func.)(2D Gaussian function) 간의<br> convolution으로 모델링<br> 즉, a pixel이 주위 pixels에 영향을 미칠 경우 blur</li> <li>여기서 영감을 받아<br> <code class="language-plaintext highlighter-rouge">covariance(크기)가 큰 3DGS는 Blur</code>를 유발하고<br> <code class="language-plaintext highlighter-rouge">covariance(크기)가 작은 3DGS는 Sharp</code> image에 기여한다고 가정<br> (covariance(dispersion)가 클수록 Gaussian이 더 많은 pixels에 걸쳐 있으니까<br> 더 많은 이웃한 pixels 간의 interference 표현 가능)</li> <li>그렇다면 covariance \(\Sigma = R S S^{T} R^{T}\) 를 변경하여 Blur를 모델링해야겠다!</li> </ul> </li> <li>Defocus Blur를 모델링하는 MLP :<br> \((\delta r_{j}, \delta s_{j}) = F_{\theta}(\gamma(x_{j}), r_{j}, s_{j}, \gamma(v))\)<br> where input : \(j\)-th Gaussian’s position, rotation, scale, view-direction<br> where output : \(j\)-th Gaussian’s rotation change, scale change<br> (\(\gamma\) : positional encoding) <ul> <li>transformed 3DGS : <ul> <li>rotation quaternion : \(\hat r_{j} = r_{j} \cdot \text{min}(1.0, \lambda_{s} \delta r_{j} + (1 - \lambda_{s}))\)</li> <li>scaling : \(\hat s_{j} = s_{j} \cdot \text{min}(1.0, \lambda_{s} \delta s_{j} + (1 - \lambda_{s}))\) <ul> <li>\(\cdot\) : element-wise multiplication</li> <li>\(\lambda_{s}\) 로 scale하고 \((1 - \lambda_{s})\) 로 shift : for optimization stability <code class="language-plaintext highlighter-rouge">???</code> </li> <li>rotation 및 scaling 변화량의 <code class="language-plaintext highlighter-rouge">최솟값을 1로 clip</code> :<br> \(\hat s_{j} \geq s_{j}\) 이므로 transformed 3DGS는 <code class="language-plaintext highlighter-rouge">더 큰 covariance</code>를 가져서<br> <code class="language-plaintext highlighter-rouge">Defocus Blur</code>의 근본 원인인 주변 정보의 interference을 모델링할 수 있게 됨</li> </ul> </li> </ul> </li> <li>inference :<br> scaling factor로 covariance 변화시키는 게 blur kernel의 역할을 하므로<br> <code class="language-plaintext highlighter-rouge">training</code> 시에는 <code class="language-plaintext highlighter-rouge">transformed 3DGS</code>가 <code class="language-plaintext highlighter-rouge">blurry</code> image를 생성하지만<br> <code class="language-plaintext highlighter-rouge">inference</code> 시에는 MLP를 사용하지 않은 <code class="language-plaintext highlighter-rouge">기존 3DGS</code>가 <code class="language-plaintext highlighter-rouge">sharp</code> image를 생성<br> \(\rightarrow\)<br> training할 때는 MLP forwarding과 간단한 element-wise multiplication만 추가 비용이고,<br> inference할 때는 MLP를 사용하지 않아 Vanilla-3DGS와 모든 단계가 동일하므로<br> <code class="language-plaintext highlighter-rouge">추가 비용 없이 real-time rendering</code> 가능</li> </ul> </li> </ul> <h3 id="selective-blurring">Selective Blurring</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/2m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/2m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/2m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/2m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>초점에 의한 Defocus Blur는 <code class="language-plaintext highlighter-rouge">영역마다 흐린 수준이 다름</code><br> 본 논문에서는 <code class="language-plaintext highlighter-rouge">각 3DGS마다</code> 다르게 \(\delta_{r}, \delta_{s}\) 를 추정하므로<br> Gaussian의 covariance를 선택적으로 확대시킬 수 있어서<br> 영역에 따라 다르게 blurring 할 수 있으므로<br> <code class="language-plaintext highlighter-rouge">pixel 단위의 blurring</code>을 보다 유연하게 모델링 가능 <ul> <li>defocus blur가 심한 영역에 있는 3DGS는 \(\delta_{s}\) 가 더 크도록</li> <li>당연히 단일 유형의 Gaussian Blur kernel을 써서 평균 blurring을 모델링하는 것보다<br> 본 논문에서처럼 3DGS마다 다른 Blur kernel을 적용하여 pixel 단위 blurring을 모델링하는 게 더 좋음!</li> </ul> </li> </ul> <h3 id="camera-motion-blur">Camera motion Blur</h3> <ul> <li> <p>셔터가 열려 있는 exposure time 동안<br> camera movement가 있으면<br> light intensities from multipe sources가 inter-mixed되어<br> Camera motion Blur 발생</p> </li> <li> <p>Camera motion Blur를 모델링하는 MLP :<br> \({(\delta x_{j}^{(i)}, \delta r_{j}^{(i)}, \delta s_{j}^{(i)})}_{i=1}^{M} = F_{\theta}(\gamma(x_{j}), r_{j}, s_{j}, \gamma(v))\)</p> <ul> <li>transformed 3DGS : <ul> <li>3D position : \(\hat x_{j}^{(i)} = x_{j} + \lambda_{p} \delta x_{j}^{(i)}\) (shift)</li> <li>rotation quaternion : \(\hat r_{j}^{(i)} = r_{j} \cdot \delta r_{j}^{(i)}\) (element-wise multiplication)</li> <li>scaling : \(\hat s_{j}^{(i)} = s_{j} \cdot \delta s_{j}^{(i)}\) (element-wise multiplication) <ul> <li>Camera motion Blur의 경우<br> Defocus Blur와 달리 covariance를 무조건 키워야 되는 게 아니므로<br> min-clip by 1.0 없음</li> </ul> </li> </ul> </li> <li>Camera motion Blur :<br> \(I_{b} = \frac{1}{M} \sum_{i=1}^{M} I_{i}\) <ul> <li>셔터가 열려 있는 동안 카메라가 움직이는 각 discrete moment는<br> 각 3DGS set에 대응됨</li> <li>\(j\)-th Gaussian 의 <code class="language-plaintext highlighter-rouge">camera movement</code>를 나타내기 위해<br> <code class="language-plaintext highlighter-rouge">M개의 auxiliary 3DGS sets</code> 만들어서<br> <code class="language-plaintext highlighter-rouge">M개의 clean images</code> rendering해서<br> <code class="language-plaintext highlighter-rouge">average</code>해서 camera-motion-blurred image 얻음</li> </ul> </li> <li>inference :<br> 마찬가지로 <code class="language-plaintext highlighter-rouge">inference</code> 시에는 MLP를 사용하지 않은 <code class="language-plaintext highlighter-rouge">기존 3DGS</code>가 clean image를 생성<br> \(\rightarrow\)<br> inference할 때는 MLP로 \(M\)-개의 3DGS sets 만들지 않고<br> Vanilla-3DGS와 모든 단계가 동일하므로<br> <code class="language-plaintext highlighter-rouge">추가 비용 없이 real-time rendering</code> 가능</li> </ul> </li> </ul> <h3 id="compensation-for-sparse-point-cloud">Compensation for Sparse Point Cloud</h3> <ul> <li> <p>문제 1)<br> 3DGS는 initial point cloud에 많이 의존하는데<br> given input multi-view images가 <code class="language-plaintext highlighter-rouge">blurry</code>하면<br> SfM은 유효한 feature를 식별하지 못해서<br> 매우 적은 수의 <code class="language-plaintext highlighter-rouge">sparse point cloud</code>를 추출함</p> </li> <li> <p>해결 :</p> <ul> <li>sparse point cloud를 방지하고자<br> \(N_{st}\) iter. 후에 \(N_{p}\)-개의 points를 uniform \(U(\alpha, \beta)\) 에서 sampling하여 추가<br> where \(\alpha\) : 기존 point cloud 위치의 최솟값<br> where \(\beta\) : 기존 point cloud 위치의 최댓값</li> <li>새로운 point의 <code class="language-plaintext highlighter-rouge">색상은 KNN(K-Nearest-Neighbor) interpolation</code>으로 할당</li> <li>새로운 points를 uniform 분포에서 sampling해서 <code class="language-plaintext highlighter-rouge">빈 공간에 불필요한 points</code>가 생길 수 있으므로<br> nearest neighbor까지의 거리가 threshold \(t_{d}\) 를 초과하는 points는 <code class="language-plaintext highlighter-rouge">폐기</code> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/3m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/3m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/3m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/3m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/12m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/12m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/12m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/12m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> 가운데는 without adding points, 오른쪽은 with adding extra points </div> <ul> <li> <p>문제 2)<br> 심지어 depth of field가 크면<br> SfM은 맨 끝에 있는 점을 거의 추출하지 않음</p> </li> <li> <p>해결 :<br> Deblur-NeRF dataset은 forward-facing scene으로만 구성되어 있으므로<br> dataset에 기록된 <code class="language-plaintext highlighter-rouge">z-axis 값</code>은 <code class="language-plaintext highlighter-rouge">relative depth</code> from any viewpoint라고 볼 수 있음</p> <ul> <li>방법 1) 먼 거리에 있는 3DGS 수 늘리기<br> 먼 거리의 평면에 있는 3DGS에 대해 denisfy<br> \(\rightarrow\)<br> 과도한 densification은 Blur 모델링을 방해하고 추가 계산 비용 필요</li> <li>방법 2) <code class="language-plaintext highlighter-rouge">먼 거리에 있는 3DGS는 덜 prune out</code><br> pruning threshold를 깊이에 따라 다르게 scaling<br> as \(t_{p}, 0.9 t_{p}, \cdots , \frac{1}{w_{p}} t_{p}\)<br> (먼 거리의 3DGS일수록 낮은 threshold) <br> \(\rightarrow\)<br> real-time rendering을 고려했을 때<br> 유연한 pruning으로도 먼 거리의 3DGS sparsity를 보상하기에 충분하다는 걸 경험적으로 발견</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/4m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/4m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/4m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/4m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="experiment">Experiment</h3> <ul> <li>Setting : <ul> <li>dataset : Deblur-NeRF dataset <ul> <li>have both synthetic and real images</li> <li>has camera motion blur or defocus blur</li> </ul> </li> <li>GPU : NVIDIA RTX 4090 GPU (24GB)</li> <li>optimzier : Adam</li> <li>iter. : \(20,000\)</li> <li>Blur를 모델링하는 small MLP : <ul> <li>lr : \(1e^{-3}\)</li> <li>hidden layer : 4 <ul> <li>3 layers : shared</li> <li>1 layer : head for each \(\delta\)</li> </ul> </li> <li>hidden unit : 64</li> <li>activation : ReLU</li> <li>initialization : Xavier</li> <li>scaling factor for \(\delta\) : \(\lambda_{s}, \lambda_{p} = 1 e^{-2}\)</li> </ul> </li> <li>sparse point cloud를 보상하기 위해 <ul> <li>\(N_{st} = 2,500\) iter. 후에 \(N_{p}\) 개의 point 추가<br> \(N_{p}\) 는 기존 point cloud 규모에 비례하며 최대 200,000개</li> <li>색상은 \(K = 4\) 의 KNN interpolation으로 할당</li> <li>nearest neighbor까지의 거리가 \(t_{d} = 2\) 을 초과하는 point는 폐기</li> </ul> </li> <li>먼 거리에 있는 3DGS는 덜 pruning하기 위해<br> pruning threshold를 깊이에 따라 다르게 scaling <ul> <li>pruning threshold \(t_{p} = 5 e^{-3}\) and densification threshold \(2 e^{-4}\)<br> for real defocus blur dataset</li> <li>pruning threshold \(t_{p} = 1 e^{-2}\) and densification threshold \(5 e^{-4}\)<br> for real camera motion blur dataset</li> <li>pruning threshold multiplier \(w_{p} = 3\)</li> </ul> </li> <li>camera motion blur를 구현하기 위해<br> \(M = 5\) 개의 3DGS sets 만들어서<br> \(M = 5\) 개의 clean images를 average</li> </ul> </li> </ul> <h3 id="results">Results</h3> <ul> <li>Results : <ul> <li> <code class="language-plaintext highlighter-rouge">SOTA deblurring NeRF</code>만큼 <code class="language-plaintext highlighter-rouge">PSNR</code> 높음</li> <li> <code class="language-plaintext highlighter-rouge">3DGS</code>만큼 <code class="language-plaintext highlighter-rouge">FPS</code> 높음</li> <li>비교 대상으로 쓰인 논문들 : <ul> <li>Deblur-NeRF, Sharp-NeRF, DP-NeRF, PDRF</li> <li>original 3DGS</li> <li>Restormer로 input training images 먼저 deblur한 뒤 original 3DGS</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/5m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/5m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/5m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/5m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/6m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/6m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/6m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/6m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> real-world Defocus Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/7m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/7m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/7m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/7m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> real-world Defocus Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/8m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/8m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/8m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/8m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> synthesized Defocus Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/9m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/9m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/9m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/9m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> synthesized Defocus Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/13m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/13m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/13m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/13m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> real-world Camera motion Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/14m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/14m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/14m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/14m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> real-world Camera motion Blur Dataset </div> <h3 id="ablation-study">Ablation Study</h3> <ul> <li>Ablation Study : <ul> <li>Extra points allocation</li> <li>Depth-based pruning</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/10m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/10m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/10m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/10m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Extra points allocation </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/11m.PNG-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/11m.PNG-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/11m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/11m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Depth-based pruning </div> <h3 id="limitation-and-future-work">Limitation and Future Work</h3> <ul> <li>Limitation : <ul> <li>volumetric rendering 기반의 NeRF-based deblurring 기법들을<br> rasterization 기반의 3DGS에 적용하기 어렵<br> \(\rightarrow\)<br> MLP로 <code class="language-plaintext highlighter-rouge">world-space</code>에서의 rays 또는 kernels를 변형하는 대신<br> MLP로 <code class="language-plaintext highlighter-rouge">rasterized image space</code>에서의 kernels를 변형하면<br> Deblurring 3DGS 구현 가능<br> \(\rightarrow\)<br> 하지만 kernel interpolation 방향으로 가면<br> pixel interpolation은 추가 비용이 발생하며<br> 3DGS의 geometry를 implicitly 변형하는 것일 뿐이므로<br> 해당 방법은 3DGS로 blur를 모델링하는 최적의 방법이 아닐 것이다<br> 이를 개선하기 위한 future works 필요</li> </ul> </li> </ul> <h3 id="code-review">Code Review</h3> <ul> <li>blur kernel 함수 :<br> Defocus Blur 및 Camera motion Blur <ul> <li>정의 : <a href="https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/scene/blur_kernel.py#L74" rel="external nofollow noopener" target="_blank">https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/scene/blur_kernel.py#L74</a> </li> <li>호출 : <a href="https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/gaussian_renderer/__init__.py#L101" rel="external nofollow noopener" target="_blank">https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/gaussian_renderer/<strong>init</strong>.py#L101</a> </li> </ul> </li> <li>sparse point cloud 보상하기 위해 add points : <ul> <li><a href="https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/scene/gaussian_model.py#L444" rel="external nofollow noopener" target="_blank">https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting/blob/main/scene/gaussian_model.py#L444</a></li> </ul> </li> </ul> <h3 id="question">Question</h3> <ul> <li> <p>Q1 :<br> small MLP는 어떤 architecture로 구성되어 있나요?<br> Dust3R 기반의 논문들을 보면<br> transformer 등 pre-trained complex model 가져와서 쓰는데<br> feed-forward 방식으로 학습하므로<br> 빠르면서도 성능이 좋습니다. 이를 적용할 수 있지 않을까요?</p> </li> <li> <p>A1 :<br> 일단 본 논문에서는 fast training 유지하기 위해 shallow MLP가 simply fc layers로 구성되어 있습니다<br> 말씀해주신대로 simple shallow MLP 대신 더 좋은 network 쓰면 성능이 더 좋아질 것 같다고 생각하는데,<br> deblurring task를 다룬 본 논문 이후의 논문들을 아직 읽어보지 않아서<br> 혹시 읽어보고 좋은 아이디어 있다면 공유하도록 하겠습니다.</p> </li> <li> <p>Q2 :<br> 본 논문이 deblurring task를 위해 pre-trained 3DGS를 가져와서 fine-tuning하는 것인가요?</p> </li> <li> <p>A2 :<br> 아닙니다. 기존 3DGS에 blur를 모델링하는 MLP만 추가해서 scratch부터 training하고,<br> 이로써 input image가 더러워도(blurry하더라도) clean image를 rendering할 수 있게 됩니다.<br> 그리고 기존 3DGS와 같이 per-scene 방식으로 학습하는 것으로 알고 있습니다.</p> </li> <li> <p>Q3 :<br> blurry input image를 dataset에서 미리 빼버리면 deblurring을 해야 하는 상황이 없어지잖아요<br> 이처럼 제가 생각하기에는 굳이 deblurring을 해야 하나 라는 생각이 듭니다.</p> </li> <li> <p>A3 :<br> 일단 deblurring이라는 게 super-resolution처럼 하나의 task로 생각할 수 있습니다<br> input image가 blurry 할 수 있는데 말씀해주신대로 이를 dataset에서 미리 뺀다는 것 자체가 manual effort를 필요로 합니다 (이를 model이 대신 해준다면 좋겠죠)<br> 그리고 만약 주어진 모델이 deblurring을 수행할 수 있다면 다른 모델의 앞단에 쓰여서 blur를 제거하는 pre-processing 용도로도 쓰일 수 있습니다.<br> 이로써 input images가 현실에서 있을 법한 더러운(blurry) 이미지더라도 상관 없이 input으로 쓸 수 있습니다.<br> 2D image 또는 video를 deblurring하는 논문들은 이미 많이 있는데<br> 3D scene deblurring의 경우에는 3D view consistency 때문에 어려움이 있었습니다.<br> 그러다가 3DGS 등장 이후로 처음 3DGS deblurring을 시도한 논문이 본 논문이라고 보시면 될 것 같습니다.</p> </li> <li> <p>Q4 :<br> 그렇다면 deblurring task라는 게 uncertainty를 해결하는 것이라고 볼 수 있을까요? 아니면 이것과는 별개의 task로 봐야 할까요?</p> </li> <li> <p>A4 :<br> (3D recon. 및 novel view synthesis에서 uncertainty라는 용어가 자주 등장하는데, 관련 논문들을 아직 많이 읽어보지 않아서 확실하게 답변드리지 못하겠습니다.)</p> </li> <li> <p>Q5 :<br> dataset에 있는 image들이 blurry하지 않고 clean(sharp) 하더라도<br> camera explore 하면서 novel view에 대해 rendering을 하다보면 rendered image에 blur가 생길 수 있을 것 같은데<br> deblurring이라는 게 이러한 blur도 제거해주나요?</p> </li> <li> <p>A5 :<br> 일단 본 논문에서 deblur를 하는 원리는 covariance를 조정하는 MLP로 blur를 모델링하여<br> 해당 MLP(blur 담당)를 사용하지 않는 inference에서는 deblurred image가 rendering되는 것입니다<br> 하지만 input이 blurry해서 생긴 blur가 아니라 rendering하다보니 생기는 artifacts로서의 blur의 경우라면<br> 해당 MLP가 artifacts로서의 blur도 잘 모델링해줄지는 모르겠습니다. 더 찾아봐야 할 것 같습니다.</p> </li> <li> <p>Q6 :<br> 혹시 본 논문을 읽으면서 생각해보셨던 limitation이 있을까요? 논문에 적혀있는 것 말고 개인적인 생각이 있으신지 궁금합니다.<br> 저는 뭔가 본 논문의 알고리즘이 artificial하다는 생각이 들었습니다.</p> </li> <li> <p>A6 :<br> (개인적으로 생각해본 limitation 답변 못 드림 ㅠㅠ 앞으로는 논문 읽을 때 novelty 말고도 limitation이 무엇일지 생각하는 습관 길러보자!)<br> 기존 deblur nerf에서는 deblur kernel을 이용해서 여러 ray를 쏴서 2D 상에서 pixel들을 interpolate해서 blur를 모델링하는데<br> deblurring 3DGS에서는 3D 상에서 Gaussian covariance를 키우는 방식으로 interpolate를 비슷하게 구현했다는 논리(가정)이고<br> 결과적으로 성능이 좋게 나왔으니 본인들 주장(가정)이 맞았다 인 것 같아서 말씀해주신대로 artificial한 느낌이 들긴 하네요</p> </li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-10-30-Deblurring3DGS.bib"></d-bibliography> <div id="disqus_thread" style="max-width: 800px; margin: 0 auto;"> <script type="text/javascript">var disqus_shortname="semyeong-yu",disqus_identifier="/blog/2024/Deblurring3DGS",disqus_title="Deblurring 3D Gaussian Splatting";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Semyeong Yu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5YMLX9VHFX"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5YMLX9VHFX");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>