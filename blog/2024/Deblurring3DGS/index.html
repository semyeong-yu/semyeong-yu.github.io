<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Deblurring 3D Gaussian Splatting | Semyeong Yu </title> <meta name="author" content="Semyeong Yu"> <meta name="description" content="ECCV 2024"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://semyeong-yu.github.io/blog/2024/Deblurring3DGS/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Deblurring 3D Gaussian Splatting",
            "description": "ECCV 2024",
            "published": "October 30, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Semyeong</span> Yu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Deblurring 3D Gaussian Splatting</h1> <p>ECCV 2024</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#related-works">Related Works</a> </div> <div> <a href="#defocus-blur">Defocus Blur</a> </div> <div> <a href="#camera-motion-blur">Camera motion Blur</a> </div> <div> <a href="#compensation-for-sparse-point-cloud">Compensation for Sparse Point Cloud</a> </div> <div> <a href="#experiment">Experiment</a> </div> <div> <a href="#limitation-and-future-work">Limitation and Future Work</a> </div> <div> <a href="#code-review">Code Review</a> </div> </nav> </d-contents> <h2 id="deblurring-3d-gaussian-splatting-eccv-2024">Deblurring 3D Gaussian Splatting (ECCV 2024)</h2> <h4 id="byeonghyeon-lee-howoong-lee-xiangyu-sun-usman-ali-eunbyung-park">Byeonghyeon Lee, Howoong Lee, Xiangyu Sun, Usman Ali, Eunbyung Park</h4> <blockquote> <p>paper :<br> <a href="https://arxiv.org/abs/2401.00834" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2401.00834</a><br> project website :<br> <a href="https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/" rel="external nofollow noopener" target="_blank">https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/</a><br> code :<br> <a href="https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting" rel="external nofollow noopener" target="_blank">https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting</a></p> </blockquote> <h3 id="introduction">Introduction</h3> <ul> <li>3DGS : <ul> <li>novel-view로 inference할 때<br> NeRF는 새로운 각도를 MLP에 넣어야만 color, opacity 얻을 수 있지만<br> 3DGS는 spherical harmonics, explicit 기법이라 새로운 각도에 대해서도 바로 color, opacity 얻을 수 있어서<br> volume rendering이 빠름</li> <li>differentiable splatting-based rasterization with parallelism</li> </ul> </li> <li>본 논문 : <ul> <li>핵심 : <ul> <li>각 3DGS의 <code class="language-plaintext highlighter-rouge">covariance</code>를 수정하여 <code class="language-plaintext highlighter-rouge">blur(adjacent pixels의 혼합)를 모델링하는 작은 MLP</code> 사용</li> <li>training 시에는 MLP output 곱해서 blurry image를 생성하고<br> inference 시에는 MLP 사용하지 않아서 real-time으로 sharp image 생성</li> </ul> </li> <li>문제 : <ul> <li>3DGS는 initial point cloud에 많이 의존하는데<br> given images가 <code class="language-plaintext highlighter-rouge">blurry</code>하면 SfM은 유효한 feature를 식별하지 못해서 <code class="language-plaintext highlighter-rouge">매우 적은 수의 point</code> cloud를 추출함</li> <li>심지어 depth가 크면 SfM은 맨 끝에 있는 점을 거의 추출하지 않음</li> </ul> </li> <li>해결 : <ul> <li>sparse point cloud를 방지하고자<br> <code class="language-plaintext highlighter-rouge">N-nearest-neighbor interpolation으로 points 추가</code> </li> <li>먼 거리의 평면에 많은 Gaussian을 유지하기 위해<br> <code class="language-plaintext highlighter-rouge">위치에 따라 Gaussian pruning</code> </li> </ul> </li> <li>contribution :<br> SOTA qualtiy인데 훨씬 빠른 rendering speed (\(\gt 200\) FPS)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/1-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/1-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Overall Architecture </div> <h3 id="related-works">Related Works</h3> <ul> <li>Image Deblurring : <ul> <li>\(g(x) = \sum_{s \in S_{h}} h(x, s) f(x) + n(x)\)<br> where \(g(x)\) : blurry image and \(f(x)\) : latent sharp image<br> where \(h(x, s)\) : blur kernel or PSF (Point Spread Function)<br> where \(n(x)\) : additive white Gaussian noise (occurs in nature images)</li> <li>지금까지 2D image deblurring은 많이 연구되어 왔는데<br> 3D scene deblurring은 3D view consistency 부족 때문에 연구하기 어려웠음</li> </ul> </li> <li>Fast NeRF : <ul> <li>방법 1)<br> use additional data-structure to reduce the size and number of MLP layers<br> but, fail to reach real-time view synthesis <ul> <li>grid-based :<br> Hexplane, TensoRF, K-planes, Mip-grid, Masked wavelet representation, Direct voxel grid optimization, F2-nerf</li> <li>hash-based :<br> InstantNGP, Zip-nerf</li> </ul> </li> <li>방법 2)<br> trained param.을 faster representation으로 bake해서 real-time rendering <ul> <li>Baking neural radiance fields, Merf, Bakedsdf</li> </ul> </li> </ul> </li> <li>Deblurring NeRF :<br> 자세한 건 <a href="https://semyeong-yu.github.io/blog/2024/DeblurNeRF/">Link</a> 참조 <ul> <li>DoF-NeRF <d-cite key="DofNeRF">[1]</d-cite> : <ul> <li>단점 :<br> train하기 위해 all-in-focus image와 blurry image 모두 필요<br> (all-in-focus image : 화면 전체가 초점이 맞춰져 있는 image)</li> </ul> </li> <li>Deblur-NeRF <d-cite key="DeblurNeRF">[2]</d-cite> : <ul> <li>장점 :<br> train할 때 all-in-focus image 필요 없음</li> <li>핵심 :<br> additional small MLP 사용해서<br> per-pixel blur kernel 예측</li> </ul> </li> <li>DP-NeRF <d-cite key="DpNeRF">[3]</d-cite> and PDRF <d-cite key="PDRF">[4]</d-cite> : <ul> <li>Deblur-NeRF 발전시킴</li> </ul> </li> <li>Hybrid <d-cite key="Hybrid">[5]</d-cite> and Sharp-NeRF <d-cite key="SharpNeRF">[6]</d-cite> and BAD-NeRF <d-cite key="BADNeRF">[7]</d-cite> : <ul> <li>camera motion blur와 defocus blur 중 하나만 다룸</li> </ul> </li> </ul> </li> <li>Deblurring NeRF 요약 : <ul> <li>deblur task 잘 수행하지만<br> NeRF 자체가 rendering time이 오래 걸림<br> \(\rightarrow\)<br> real-time differentiable rasterizer 이용하는<br> 3DGS로 deblur task 수행하자!</li> </ul> </li> </ul> <h3 id="background">Background</h3> <ul> <li> <p>3DGS <a href="https://semyeong-yu.github.io/blog/2024/GS/">Link</a> 참고</p> </li> <li> <p>Blur :</p> <ul> <li>Defocus Blur :<br> 렌즈의 <code class="language-plaintext highlighter-rouge">초점이 맞지 않아서</code> 흐려진 경우<br> e.g. 인물 사진에서 인물만 초점이 맞고 배경은 흐릿한 경우</li> <li>Camera Motion Blur :<br> 셔터가 열려 있는 동안 카메라가 움직이거나 피사체가 <code class="language-plaintext highlighter-rouge">움직여서</code> 흐려진 경우<br> e.g. 달리는 자동차를 촬영한 경우</li> </ul> </li> </ul> <h3 id="defocus-blur">Defocus Blur</h3> <ul> <li>Motivation : <ul> <li>Defocus Blur는 일반적으로<br> 실제 image와 PSF(point spread func.)(2D Gaussian function) 간의<br> convolution으로 모델링<br> 즉, a pixel이 주위 pixels에 영향을 미칠 경우 blur</li> <li>여기서 영감을 받아<br> <code class="language-plaintext highlighter-rouge">covariance(크기)가 큰 3DGS는 Blur</code>를 유발하고<br> <code class="language-plaintext highlighter-rouge">covariance(크기)가 작은 3DGS는 Sharp</code> image에 기여한다고 가정<br> (covariance(dispersion)가 클수록 Gaussian이 더 많은 pixels에 걸쳐 있으니까<br> 더 많은 이웃한 pixels 간의 interference 표현 가능)</li> <li>그렇다면 covariance \(\Sigma = R S S^{T} R^{T}\) 를 변경하여 Blur를 모델링해야겠다!</li> </ul> </li> <li>Defocus Blur를 모델링하는 MLP :<br> \((\delta r_{j}, \delta s_{j}) = F_{\theta}(\gamma(x_{j}), r_{j}, s_{j}, \gamma(v))\)<br> where input : \(j\)-th Gaussian’s position, rotation, scale, view-direction<br> where output : \(j\)-th Gaussian’s rotation change, scale change<br> (\(\gamma\) : positional encoding) <ul> <li>transformed 3DGS : <ul> <li>rotation quaternion : \(\hat r_{j} = r_{j} \cdot \text{min}(1.0, \lambda_{s} \delta r_{j} + (1 - \labmda_{s}))\)</li> <li>scaling : \(\hat s_{j} = s_{j} \cdot \text{min}(1.0, \lambda_{s} \delta s_{j} + (1 - \labmda_{s}))\) <ul> <li>\(\cdot\) : element-wise multiplication</li> <li>\(\lambda_{s}\) 로 scale하고 \((1 - \lambda_{s})\) 로 shift : for optimization stability <code class="language-plaintext highlighter-rouge">???</code> </li> <li>MLP output \(\delta r_{j}, \delta s_{j}\) 의 <code class="language-plaintext highlighter-rouge">최솟값을 1로 clip</code> :<br> \(\hat s_{j} \geq s_{j}\) 이므로 transformed 3DGS는 <code class="language-plaintext highlighter-rouge">더 큰 covariance</code>를 가져서<br> <code class="language-plaintext highlighter-rouge">Defocus Blur</code>의 근본 원인인 주변 정보의 interference을 모델링할 수 있게 됨</li> </ul> </li> </ul> </li> <li>inference :<br> scaling factor로 covariance 변화시키는 게 blur kernel의 역할을 하므로<br> <code class="language-plaintext highlighter-rouge">training</code> 시에는 <code class="language-plaintext highlighter-rouge">transformed 3DGS</code>가 <code class="language-plaintext highlighter-rouge">blurry</code> image를 생성하지만<br> <code class="language-plaintext highlighter-rouge">inference</code> 시에는 MLP를 사용하지 않은 <code class="language-plaintext highlighter-rouge">기존 3DGS</code>가 <code class="language-plaintext highlighter-rouge">sharp</code> image를 생성<br> \(\rightarrow\)<br> training할 때는 MLP forwarding과 간단한 element-wise multiplication만 추가 비용이고,<br> inference할 때는 MLP를 사용하지 않아 Vanilla-3DGS와 모든 단계가 동일하므로<br> <code class="language-plaintext highlighter-rouge">추가 비용 없이 real-time rendering</code> 가능</li> </ul> </li> </ul> <h3 id="selective-blurring">Selective Blurring</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/2-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/2-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>초점에 의한 Defocus Blur는 <code class="language-plaintext highlighter-rouge">영역마다 흐린 수준이 다름</code><br> 본 논문에서는 <code class="language-plaintext highlighter-rouge">각 3DGS마다</code> 다르게 \(\delta_{r}, \delta_{s}\) 를 추정하므로<br> Gaussian의 covariance를 선택적으로 확대시킬 수 있어서<br> 영역에 따라 다르게 blurring 할 수 있으므로<br> <code class="language-plaintext highlighter-rouge">pixel 단위의 blurring</code>을 보다 유연하게 모델링 가능 <ul> <li>defocus blur가 심한 영역에 있는 3DGS는 \(\delta_{s}\) 가 더 크도록</li> <li>당연히 단일 유형의 Gaussian Blur kernel을 써서 평균 blurring을 모델링하는 것보다<br> 본 논문에서처럼 3DGS마다 다른 Blur kernel을 적용하여 pixel 단위 blurring을 모델링하는 게 더 좋음!</li> </ul> </li> </ul> <h3 id="camera-motion-blur">Camera motion Blur</h3> <ul> <li> <p>셔터가 열려 있는 exposure time 동안<br> camera movement가 있으면<br> light intensities from multipe sources가 inter-mixed되어<br> Camera motion Blur 발생</p> </li> <li> <p>Camera motion Blur를 모델링하는 MLP :<br> \({(\delta x_{j}^{(i)}, \delta r_{j}^{(i)}, \delta s_{j}^{(i)})}_{i=1}^{M} = F_{\theta}(\gamma(x_{j}), r_{j}, s_{j}, \gamma(v))\)</p> <ul> <li>transformed 3DGS : <ul> <li>3D position : \(\hat x_{j}^{(i)} = x_{j} + \lambda_{p} \delta x_{j}^{(i)}\) (shift)</li> <li>rotation quaternion : \(\hat r_{j}^{(i)} = r_{j} \cdot \delta r_{j}^{(i)}\)</li> <li>scaling : \(\hat s_{j}^{(i)} = s_{j} \cdot \delta s_{j}^{(i)}\) <ul> <li>Camera motion Blur의 경우<br> Defocus Blur와 달리 covariance를 무조건 키워야 되는 게 아니므로<br> min-clip by 1.0 없음</li> </ul> </li> </ul> </li> <li>Camera motion Blur :<br> \(I_{b} = \frac{1}{M} \sum_{i=1}^{M} I_{i}\) <ul> <li>셔터가 열려 있는 동안 카메라가 움직이는 각 discrete moment는<br> 각 3DGS set에 대응됨</li> <li>\(j\)-th Gaussian 의 <code class="language-plaintext highlighter-rouge">camera movement</code>를 나타내기 위해<br> <code class="language-plaintext highlighter-rouge">M개의 auxiliary 3DGS sets</code> 만들어서<br> <code class="language-plaintext highlighter-rouge">M개의 clean images</code> rendering해서<br> <code class="language-plaintext highlighter-rouge">average</code>해서 camera-motion-blurred image 얻음</li> </ul> </li> <li>inference :<br> 마찬가지로 <code class="language-plaintext highlighter-rouge">inference</code> 시에는 MLP를 사용하지 않은 <code class="language-plaintext highlighter-rouge">기존 3DGS</code>가 clean image를 생성<br> \(\rightarrow\)<br> inference할 때는 MLP로 \(M\)-개의 3DGS sets 만들지 않고<br> Vanilla-3DGS와 모든 단계가 동일하므로<br> <code class="language-plaintext highlighter-rouge">추가 비용 없이 real-time rendering</code> 가능</li> </ul> </li> </ul> <h3 id="compensation-for-sparse-point-cloud">Compensation for Sparse Point Cloud</h3> <ul> <li> <p>문제 1)<br> 3DGS는 initial point cloud에 많이 의존하는데<br> given images가 <code class="language-plaintext highlighter-rouge">blurry</code>하면 SfM은 유효한 feature를 식별하지 못해서 <code class="language-plaintext highlighter-rouge">매우 적은 수의 point</code> cloud를 추출함</p> </li> <li> <p>해결 :</p> <ul> <li>sparse point cloud를 방지하고자<br> \(N_{st}\) iter. 후에 \(N_{p}\)-개의 points를 uniform \(U(\alpha, \beta)\) 에서 sampling하여 추가<br> where \(\alpha\) : 기존 point cloud 위치의 최솟값<br> where \(\beta\) : 기존 point cloud 위치의 최댓값</li> <li>새로운 point의 <code class="language-plaintext highlighter-rouge">색상은 KNN(K-Nearest-Neighbor) interpolation</code>으로 할당</li> <li>새로운 points를 uniform 분포에서 sampling해서 <code class="language-plaintext highlighter-rouge">빈 공간에 불필요한 points</code>가 생길 수 있으므로<br> nearest neighbor까지의 거리가 threshold \(t_{d}\) 를 초과하는 points는 <code class="language-plaintext highlighter-rouge">폐기</code> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/3-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/3-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <p>문제 2)<br> 심지어 depth가 크면 SfM은 맨 끝에 있는 점을 거의 추출하지 않음</p> </li> <li> <p>해결 :</p> <ul> <li>방법 1) 먼 거리에 있는 3DGS 수 늘리기<br> 먼 거리의 평면에 있는 3DGS에 대해 denisfy<br> \(\rightarrow\)<br> 과도한 densification은 Blur 모델링을 방해하고 추가 계산 비용 필요</li> <li>방법 2) <code class="language-plaintext highlighter-rouge">먼 거리에 있는 3DGS는 덜 pruning</code><br> pruning threshold를 깊이에 따라 다르게 scaling<br> as \(t_{p}, 0.9 t_{p}, \cdots , w_{p} t_{p}\)<br> (먼 거리의 3DGS일수록 낮은 threshold) <br> \(\rightarrow\)<br> real-time rendering을 고려했을 때<br> 유연한 pruning으로도 먼 거리의 3DGS sparsity를 보상하기에 충분하다는 걸 경험적으로 발견</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/4-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/4-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="experiment">Experiment</h3> <ul> <li>Setting : <ul> <li>GPU : NVIDIA RTX 4090 GPU (24GB)</li> <li>optimzier : Adam</li> <li>iter. : \(30,000\)</li> <li>Blur를 모델링하는 small MLP : <ul> <li>lr : \(10^{-3}\)</li> <li>hidden layer : 3</li> <li>hidden unit : 64</li> <li>activation : ReLU</li> <li>initialization : Xavier</li> </ul> </li> <li>sparse point cloud를 보상하기 위해 <ul> <li>\(N_{st} = 2,500\) iter. 후에 \(N_{p} = 100,000\) 개의 point 추가</li> <li>색상은 \(K = 4\) 의 KNN interpolation으로 할당</li> <li>nearest neighbor까지의 거리가 \(t_{d} = 10\) 을 초과하는 point는 폐기</li> </ul> </li> <li>먼 거리에 있는 3DGS는 덜 pruning하기 위해<br> pruning threshold를 깊이에 따라 다르게 scaling<br> as \(t_{p}, 0.9 t_{p}, \cdots , w_{p} t_{p}\)<br> where \(t_{p} = 5 \times 10^{-3}\) and \(w_{p} = 0.3\)</li> </ul> </li> <li>Results :<br> SOTA deblurring NeRF만큼 PSNR이 높은데<br> 3DGS만큼 FPS도 높음</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/5-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/5-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/6-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/6-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/7-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/7-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/7-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> real-world Defocus Blur Dataset </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/8-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/8-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/8-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/9-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/9-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/9-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> synthesized Defocus Blur Dataset </div> <ul> <li>Ablation Study : <ul> <li>Extra points allocation</li> <li>Depth-based pruning</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/10-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/10-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/10-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Extra points allocation </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-30-Deblurring3DGS/11-480.webp 480w,/assets/img/2024-10-30-Deblurring3DGS/11-800.webp 800w,/assets/img/2024-10-30-Deblurring3DGS/11-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-30-Deblurring3DGS/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Depth-based pruning </div> <h3 id="limitation-and-future-work">Limitation and Future Work</h3> <p>TBD <code class="language-plaintext highlighter-rouge">???</code></p> <h3 id="code-review">Code Review</h3> <p>TBD <code class="language-plaintext highlighter-rouge">???</code></p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-10-30-Deblurring3DGS.bib"></d-bibliography> <div id="disqus_thread" style="max-width: 800px; margin: 0 auto;"> <script type="text/javascript">var disqus_shortname="semyeong-yu",disqus_identifier="/blog/2024/Deblurring3DGS",disqus_title="Deblurring 3D Gaussian Splatting";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Semyeong Yu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5YMLX9VHFX"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5YMLX9VHFX");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>