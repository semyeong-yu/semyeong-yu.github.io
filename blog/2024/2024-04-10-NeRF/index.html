<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h1> <h4 id="ben-mildenhall-pratul-psrinivasan-matthew-tancik">Ben Mildenhall, Pratul P.Srinivasan, Matthew Tancik</h4> <h2 id="introduction">Introduction</h2> <h4 id="pipeline">Pipeline</h4> <ol> <li> <p>march camera rays to generate a sampled set of 3D points</p> </li> <li> <p>represents volumetric static scene by optimizing continuous 5D function(fully-connected network)</p> </li> </ol> <ul> <li>input: single continuous <code class="language-plaintext highlighter-rouge">5D coordinate</code> (\(x, y, z, theta, phi\))</li> <li>output: <code class="language-plaintext highlighter-rouge">volume density</code> (differential opacity) (how much radiance is accumulated by a ray) <code class="language-plaintext highlighter-rouge">view-dependent RGB color</code> (emitted radiance) \(c = (r, g, b)\)</li> </ul> <ol> <li>synthesizes novel view by classic <code class="language-plaintext highlighter-rouge">volume rendering</code> techniques(differentiable) to accumulate(project) the color/density samples into 2D image along rays</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-04-10-NeRF/1-480.webp 480w,/assets/img/2024-04-10-NeRF/1-800.webp 800w,/assets/img/2024-04-10-NeRF/1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-04-10-NeRF/1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all. </div> <h4 id="problem--solution">Problem &amp; Solution</h4> <p>Problem :</p> <ol> <li>not sufficiently high-resolution representation</li> <li>inefficient in the number of samples per camera ray</li> </ol> <p>Solution :</p> <ol> <li>input <code class="language-plaintext highlighter-rouge">positional encoding</code> for MLP to represent higher frequency function</li> <li> <code class="language-plaintext highlighter-rouge">hierarchical sampling</code> to reduce the number of queries</li> </ol> <h4 id="contribution">Contribution</h4> <ul> <li>represent continuous scenes as 5D neural radiance fields with basic MLP to render high-resolution novel views</li> <li>differentiable volume rendering + hierarchical sampling</li> <li>positional encoding to map input 5D coordinate into higher dim. space for high-frequency scene representation</li> <li>overcome the storage costs of discretized voxel grids by encoding continuous volume into network’s parameters =&gt; require only storage costs of sampled volumetric representations</li> </ul> <h2 id="related-work">Related Work</h2> <h4 id="neural-3d-shape-representation">Neural 3D shape representation</h4> <ul> <li> <p>deep networks that map \(xyz\) coordinates to signed distance functions or occupancy fields =&gt; limit : need GT 3D geometry</p> </li> <li> <p>Niemeyer et al. =&gt; input : find directly surface intersection for each ray (can calculate exact derivatie) =&gt; output : diffuse color at each ray intersection location</p> </li> <li> <p>Sitzmann et al. =&gt; input : each 3D coordinate =&gt; output : feature vector and RGB color at each 3D coordinate =&gt; rendering by RNN that marches along each ray to decide where the surface is.</p> </li> </ul> <blockquote> <p>Limit : oversmoothed renderings, so limited to simple shapes with low geometric complexity</p> </blockquote> <h4 id="view-synthesis-and-image-based-rendering">View synthesis and image-based rendering</h4> <ul> <li> <p>Given dense sampling of views, novel view synthesis is possible by simple light field sample interpolation</p> </li> <li> <p>Given sparser sampling of views, there are 2 ways : mesh-based representation and volumetric representation</p> </li> <li> <p>Mesh-based representation with either diffuse(난반사) or view-dependent appearance : Directly optimize mesh representations by differentiable rasterizers or pathtracers so that we reproject and reconstruct images</p> </li> </ul> <blockquote> <p>Limit : gradient-based optimization is often difficult because of local minima or poor loss landscape needs a template mesh with fixed topology for initialization, which is unavailable in real-world</p> </blockquote> <ul> <li>Volumetric representation : well-suited for gradient-based optimization and less distracting artifacts</li> </ul> <ol> <li> <p>train : predict a sampled volumetric representation (voxel grids) from input images test : use alpha-(or learned-)compositing along rays to render novel views +) alpha-compositing : 여러 frame을 합쳐서 하나의 image로 합성하는 과정으로, 각 이미지 픽셀마다 알파 값(투명도 값)(0~1)이 있어 겹치는 부분의 알파 값 및 픽셀 값을 결정</p> </li> <li> <p>CNN compensates discretization artifacts from low resolution voxel grids or CNN allows voxel grids to vary on input time</p> </li> </ol> <blockquote> <p>Limit : good results, but limited by poor time, space complexity due to discrete sampling +) discrete sampling : rendering high resol. image =&gt; finer sampling of 3D space</p> </blockquote> <blockquote> <p>Author’s solution : encode <code class="language-plaintext highlighter-rouge">continuous</code> volume into network’s parameters =&gt; higher quality rendering + require only storage cost of those <code class="language-plaintext highlighter-rouge">sampled</code> volumetric representations</p> </blockquote> <h2 id="neural-radiance-field-scene-representation">Neural Radiance Field Scene Representation</h2> <p>approximate continuous scene with MLP \(F : (x, d) =&gt; (c,\)sigma\()\)</p> <ul> <li>input : x - 3D location</li> </ul> </body></html>