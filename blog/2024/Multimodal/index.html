<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Multi-Modal Study | Semyeong Yu </title> <meta name="author" content="Semyeong Yu"> <meta name="description" content="Paper Review"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://semyeong-yu.github.io/blog/2024/Multimodal/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Multi-Modal Study",
            "description": "Paper Review",
            "published": "August 05, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Semyeong</span> Yu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Multi-Modal Study</h1> <p>Paper Review</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#storyimager-a-unified-and-efficient-framework-for-coherent-story-visualization-and-completion">StoryImager - A Unified and Efficient Framework for Coherent Story Visualization and Completion</a> </div> <div> <a href="#intelligent-grimm-open-ended-visual-storytelling-via-latent-diffusion-models">Intelligent Grimm - Open-ended Visual Storytelling via Latent Diffusion Models</a> </div> <div> <a href="#generating-realistic-images-from-in-the-wild-sounds">Generating Realistic Images from In-the-wild Sounds</a> </div> <div> <a href="#vivit-a-video-vision-transformer">ViViT - A Video Vision Transformer</a> </div> </nav> </d-contents> <h2 id="multi-modal-study">Multi-Modal Study</h2> <h3 id="storyimager---a-unified-and-efficient-framework-for-coherent-story-visualization-and-completion">StoryImager - A Unified and Efficient Framework for Coherent Story Visualization and Completion</h3> <h4 id="ming-tao-bing-kun-bao-hao-tang-yaowei-wang-changsheng-xu">Ming Tao, Bing-Kun Bao, Hao Tang, Yaowei Wang, Changsheng Xu</h4> <blockquote> <p>paper :<br> <a href="https://arxiv.org/abs/2404.05979" rel="external nofollow noopener" target="_blank">StoryImager</a></p> </blockquote> <ul> <li> <code class="language-plaintext highlighter-rouge">Story Visualization</code> Task : <ul> <li>input : prince image / cat image / prompts</li> <li>output : story images</li> <li>video generation과는 다른 게, story visualization은 웹툰 같다고 생각하면 됨<br> story visualization은 image 간의 consistency를 유지하긴 하지만, video generation처럼 frame끼리 연속성을 보장할 필요는 없음</li> </ul> </li> <li>StoryImager: <ul> <li>task : coherent story visualization and completion</li> <li>기존 모델은 visualization과 continuation을 위한 model을 별도로 필요했는데,<br> 본 논문은 single model (<code class="language-plaintext highlighter-rouge">통합적인 framework</code>) 제시<br> by <code class="language-plaintext highlighter-rouge">global consistency</code> 반영!</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/StoryImager/1-480.webp 480w,/assets/img/2024-08-05-Multimodal/StoryImager/1-800.webp 800w,/assets/img/2024-08-05-Multimodal/StoryImager/1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-05-Multimodal/StoryImager/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Architecture :<br> maintain <code class="language-plaintext highlighter-rouge">global consistency</code><br> by context-feature-extractor<br> and FS-CAM (frame-story cross-attention module)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/StoryImager/2-480.webp 480w,/assets/img/2024-08-05-Multimodal/StoryImager/2-800.webp 800w,/assets/img/2024-08-05-Multimodal/StoryImager/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-05-Multimodal/StoryImager/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <p><code class="language-plaintext highlighter-rouge">random masking</code> :<br> make a story board from story images<br> \(\rightarrow\) VAE<br> \(\rightarrow\) random masking to VAE latent space</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">context-feature-extractor</code> :</p> <ul> <li>word-embeddings \(\rightarrow\) transformer<br> \(\rightarrow\) prior embeddings<br> \(\rightarrow\) MLP<br> \(\rightarrow\) frame-aware latent prior<br> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">story board images가 story prompts를 반영하도록</code> 하기 위해 masked VAE latent space와 concat해서 이를 FS-CAM에서 story board로 사용</li> <li>word-embeddings \(\rightarrow\) transformer<br> \(\rightarrow\) context embeddings<br> \(\rightarrow\) transformer<br> \(\rightarrow\) global context<br> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">global 정보</code> 주기 위해 FS-CAM에서 text prompts로 사용</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/StoryImager/4-480.webp 480w,/assets/img/2024-08-05-Multimodal/StoryImager/4-800.webp 800w,/assets/img/2024-08-05-Multimodal/StoryImager/4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-05-Multimodal/StoryImager/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>FS-CAM (<code class="language-plaintext highlighter-rouge">frame-story cross-attention module</code>) :<br> 개별 story board - 개별 text prompts 를 locally cross-attention한 것과,<br> 전체 story board - 전체 text prompts 를 globally cross-attention한 것을<br> concat</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/StoryImager/3-480.webp 480w,/assets/img/2024-08-05-Multimodal/StoryImager/3-800.webp 800w,/assets/img/2024-08-05-Multimodal/StoryImager/3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-05-Multimodal/StoryImager/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="intelligent-grimm---open-ended-visual-storytelling-via-latent-diffusion-models">Intelligent Grimm - Open-ended Visual Storytelling via Latent Diffusion Models</h3> <h4 id="chang-liu-haoning-wu-yujie-zhong-xiaoyun-zhang-yanfeng-wang-weidi-xie">Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, Yanfeng Wang, Weidi Xie</h4> <blockquote> <p>paper :<br> <a href="https://arxiv.org/abs/2306.00973" rel="external nofollow noopener" target="_blank">Intelligent Grimm</a></p> </blockquote> <ul> <li>Intelligent Grimm : <ul> <li>NIPS 2023에서 novelty 부족으로 reject 당했다가 보완해서 CVPR 2024에 accept</li> <li>task : open-ended visual storytelling</li> <li>StoryGen : unseen characters에 대해서도 추가적인 character-specific-optimization 없이 story visualization 가능</li> <li>StorySalon : online-video, open-source e-book 등 소싱해서 만든 dataset</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">context encoding</code> : <ul> <li>SDM : pre-trained stable diffusion model<br> CLIP : pre-trained CLIP encoder<br> VAE : pre-trained VAE</li> <li>visual condition feature = [SDM(image1, CLIP(text1)), SDM(image2, CLIP(text2)), …, SDM(imagek-1, CLIP(textk-1))]<br> k-th frame image 만들기 위해 cross-attention 하는 데 사용</li> </ul> </li> <li>visual-language contextual fusion :<br> <code class="language-plaintext highlighter-rouge">cross-attention</code> 사용<br> 아래 Fig. (b)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/IntelligentGrimm/1-480.webp 480w,/assets/img/2024-08-05-Multimodal/IntelligentGrimm/1-800.webp 800w,/assets/img/2024-08-05-Multimodal/IntelligentGrimm/1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-05-Multimodal/IntelligentGrimm/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <code class="language-plaintext highlighter-rouge">conditional generation</code> : <ul> <li>prev. frame t-1 \(\rightarrow\) add noise<br> \(\rightarrow\) denoising one step<br> \(\rightarrow\) diffusion U-Net</li> <li>prev. text \(\rightarrow\) text encoder<br> \(\rightarrow\) diffusion U-Net</li> <li>diffusion U-Net (self-attention, text-attention)<br> \(\rightarrow\) <code class="language-plaintext highlighter-rouge">denoising one step에 해당되는 feature</code>를 추출</li> <li>extracted image-diffusion-denoising-feature<br> &amp; random noise<br> &amp; current text \(\rightarrow\) text encoder<br> \(\rightarrow\) StoryGen model (self-attention, image-attention, text-attention)<br> \(\rightarrow\) current frame t</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">multi-frame conditioning</code> :<br> story의 경우 frame t에 영향을 주는 image들이 frame t-1, frame t-2, … 일 수 있음<br> 이 때, 과거 frames에 모두 같은 random noise를 줄 경우 성능 좋지 않아서<br> <code class="language-plaintext highlighter-rouge">현재에 가까운 과거 frame일수록 noise를 덜 주는 식으로 temporal order를 부여</code>하면 성능 좋음</li> </ul> <h3 id="generating-realistic-images-from-in-the-wild-sounds">Generating Realistic Images from In-the-wild Sounds</h3> <h4 id="taegyeong-lee-jeonghun-kang-hyeonyu-kim-taehwan-kim">Taegyeong Lee, Jeonghun Kang, Hyeonyu Kim, Taehwan Kim</h4> <blockquote> <p>paper :<br> <a href="https://arxiv.org/abs/2309.02405" rel="external nofollow noopener" target="_blank">Image from in-the-wild Sounds</a></p> </blockquote> <ul> <li> <p>novelty :<br> 이전까지는 wild sound와 image 간의 pair가 없어서 limited categories와 music의 sound로부터 image를 생성하는 연구만 진행되었음<br> 본 논문은 sound와 image 간의 large paired dataset이 없더라도<br> wild sound로부터 image를 생성하는 task를 최초로 제시</p> </li> <li> <p>method :</p> <ul> <li>stage (a) :<br> <code class="language-plaintext highlighter-rouge">audio captioning</code>을 통해 sound를 text로 변환한 audio caption과<br> sound의 dynamic 특성을 반영하기 위한 <code class="language-plaintext highlighter-rouge">audio attention</code>과<br> 제대로 image visualization하기 위한 <code class="language-plaintext highlighter-rouge">sentence attention</code>을<br> 함께 사용하여 positional encoding을 거친 뒤 vector w를 <code class="language-plaintext highlighter-rouge">initialize</code><br> (이 때, Audio-Captioning-Transformer model의 decoder에서 나오는 확률값을 audio attention이라고 정의함)</li> <li>stage (b) :<br> audio caption으로부터 만든 vector z와<br> stage (a)의 vector w로부터<br> new latent vector z’를 만들고,<br> <code class="language-plaintext highlighter-rouge">stable-diffusion model</code>을 이용하여 이로부터 image를 생성한다<br> 여기서 image와 vector z 간의 <code class="language-plaintext highlighter-rouge">CLIPscore similarity</code>를 이용해서 audio caption으로부터 만든 vector z를 optimize하고<br> image와 audio 간의 <code class="language-plaintext highlighter-rouge">AudioCLIP similarity</code>를 이용해서 <code class="language-plaintext highlighter-rouge">audio를 직접 optimize</code>한다<br> (image가 text에 맞게 생성되도록 image를 점점 변화시키면서 생성하는 Style-CLIP에서 영감을 받아 이를 diffusion model에 적용)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/ImageSound/1-480.webp 480w,/assets/img/2024-08-05-Multimodal/ImageSound/1-800.webp 800w,/assets/img/2024-08-05-Multimodal/ImageSound/1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-05-Multimodal/ImageSound/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>local minimum에 빠지지 않기 위해 audio attention과 sentence attention을 이용한 stage (a)의 initialization이 매우 중요</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/ImageSound/2-480.webp 480w,/assets/img/2024-08-05-Multimodal/ImageSound/2-800.webp 800w,/assets/img/2024-08-05-Multimodal/ImageSound/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-05-Multimodal/ImageSound/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <p>Q1 :<br> image는 pixel 단위로 값이 있어서 feature map을 통해 vector로 만들 수 있고, text는 word 단위로 embedding을 통해 vector를 만들 수 있습니다. AudioCLIP을 통해 audio를 직접 optimize했다는데 audio는 무엇을 기준으로 vector로 만들어서 optimize 가능한 건가요?</p> </li> <li> <p>A1 :<br> audio는 melspectrogram을 만든 뒤 ViT에서 image 다루듯이 똑같이 patch로 쪼개서 vector로 만든다<br> AudioCLIP similarity의 경우 audio encoding과 image encoding과 text encoding 간의 contrastive learning을 통해 구할 수 있다</p> </li> </ul> <h3 id="vivit---a-video-vision-transformer">ViViT - A Video Vision Transformer</h3> <h4 id="anurag-arnab-mostafa-dehghani-georg-heigold-chen-sun-mario-lucic-cordelia-schmid">Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, Cordelia Schmid</h4> <blockquote> <p>paper :<br> <a href="https://arxiv.org/abs/2103.15691" rel="external nofollow noopener" target="_blank">ViViT</a></p> </blockquote> <ul> <li> <p>video of (T, H, W, C)를 sampling하여<br> token sequence of (n_t, n_h, n_w, C) 을 만들고<br> positional embedding을 더한 뒤 (N, d)로 reshape해서 transformer의 input으로 넣어줌</p> </li> <li> <p>uniform frame sampling :<br> ViT에서처럼 각 2D frame을 독립적으로 embedding 후 모든 token을 concat</p> </li> <li> <p>Tubelet sampling :<br> temporal info.를 반영하기 위해 tokenization 단계에서 spatial, temporal info.를 fuse</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/ViViT/1-480.webp 480w,/assets/img/2024-08-05-Multimodal/ViViT/1-800.webp 800w,/assets/img/2024-08-05-Multimodal/ViViT/1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-05-Multimodal/ViViT/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-08-05-Multimodal/ViViT/2-480.webp 480w,/assets/img/2024-08-05-Multimodal/ViViT/2-800.webp 800w,/assets/img/2024-08-05-Multimodal/ViViT/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-08-05-Multimodal/ViViT/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <p>Model 1 :<br> CNN과 달리 transformer layer는 token 수에 비례하게 quadratic complexity를 가지므로 input frame에 linearly 필요</p> </li> <li> <p>Model 2 (factorized encoder) :<br> spatial과 temporal을 두 개의 transformer encoder로 구성하여 많은 연산량 필요</p> </li> <li> <p>Model 3 (factorized self-attention) :<br> 여전히 두 개의 encoder로 특정 dim만 뽑아서 attention 연산</p> </li> <li> <p>Model 4 (factorized dot-product attention) :<br> spatial head의 경우 spatial-dim.에 대해서만 attention 수행</p> </li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="disqus_thread" style="max-width: 800px; margin: 0 auto;"> <script type="text/javascript">var disqus_shortname="semyeong-yu",disqus_identifier="/blog/2024/Multimodal",disqus_title="Multi-Modal Study";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Semyeong Yu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5YMLX9VHFX"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5YMLX9VHFX");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>