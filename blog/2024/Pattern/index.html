<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> EE534 Pattern Recognition | Semyeong Yu </title> <meta name="author" content="Semyeong Yu"> <meta name="description" content="Lecture Summary (24F)"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://semyeong-yu.github.io/blog/2024/Pattern/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "EE534 Pattern Recognition",
            "description": "Lecture Summary (24F)",
            "published": "September 10, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Semyeong</span> Yu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>EE534 Pattern Recognition</h1> <p>Lecture Summary (24F)</p> </d-title> <d-article> <blockquote> <p>Lecture :<br> 24F EE534 Pattern Recognition<br> by KAIST Munchurl Kim <a href="https://www.viclab.kaist.ac.kr/" rel="external nofollow noopener" target="_blank">VICLab</a></p> </blockquote> <h2 id="chapter-1-overview">Chapter 1. Overview</h2> <h3 id="discriminative-vs-generative">Discriminative vs Generative</h3> <ul> <li>Discriminative model : <ul> <li> <table> <tbody> <tr> <td>learn $$P(Y</td> <td>X)\(to maximize\)P(Y</td> <td>X)$$ directly</td> </tr> </tbody> </table> </li> <li>e.g. logistic regression, SVM, nearest neighbor, CRF, Decision Tree and Random Forest, traditional NN</li> </ul> </li> <li>Generative model : <ul> <li> <table> <tbody> <tr> <td>learn $$P(X</td> <td>Y)\(and\)P(Y)\(to maximize\)P(X, Y) = P(X</td> <td>Y)P(Y)$$</td> </tr> <tr> <td>where can learn $$P(Y</td> <td>X) \propto P(X</td> <td>Y)P(Y)$$ indirectly</td> </tr> </tbody> </table> </li> <li>e.g. Bayesian network, Autoregressive model, GAN, Diffuson model</li> </ul> </li> </ul> <h2 id="chapter-2-bayes-decision-theory">Chapter 2. Bayes Decision Theory</h2> <h3 id="bayes-decision-rule">Bayes Decision Rule</h3> <ul> <li>conditional probability density :<br> Let \(w\) be state (class)<br> Let \(x\) be data (continous-valued sample) <ul> <li>prior : \(P(w=w_k)\)</li> <li> <table> <tbody> <tr> <td>likelihood : PDF $$P(x</td> <td>w_k)$$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>posterior : $$P(w_k</td> <td>x) = \frac{P(x</td> <td>w_k)P(w_k)}{P(x)}$$ (Bayes Rule)</td> <td> </td> </tr> <tr> <td>where $$P(w_1</td> <td>x) + P(w_2</td> <td>x) + \cdots + P(w_N</td> <td>x) = 1$$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>evidence : $$P(x) = \sum_{k=1}^N P(x</td> <td>w_k)P(w_k) = \sum_{k=1}^N P(x, w_k)$$</td> </tr> </tbody> </table> </li> </ul> </li> <li>Bayes Decision Rule :<br> posterior 더 큰 쪽 고름! <ul> <li>Two-class (\(w_1, w_2\)) problem :<br> choose \(w_1\)<br> if \(P(w_1 | x) \gt P(w_2 | x)\)<br> if \(P(x|w_1)P(w_1) \gt P(x|w_2)P(w_2)\)<br> if \(\frac{P(x|w_1)}{P(x|w_2)} \gt \frac{P(w_2)}{P(w_1)}\)<br> (likehood ratio \(\gt\) threshold)</li> <li>multi-class problem :<br> choose \(w_i\) where \(P(w_i | x)\) is the largest</li> </ul> </li> </ul> <h3 id="minimum-error">minimum error</h3> <ul> <li>minimum error :<br> GT가 \(w_1, w_2\) 이고, Predicted가 \(R_1, R_2\) 일 때, <ul> <li>\(P(error) = \int_{-\infty}^{\infty} P(error, x)dx = \int_{-\infty}^{\infty} P(error|x)P(x)dx\)<br> \(= \int_{R_2}P(w_1|x)P(x)dx + \int_{R_1}P(w_2|x)P(x)dx\)<br> \(= \int_{R_2}P(x|w_1)P(w_1)dx + \int_{R_1}P(x|w_2)P(w_2)dx\)<br> \(= \begin{cases} A+B+D &amp; \text{if} &amp; x_B \\ A+B+C+D &amp; \text{if} &amp; x^{\ast} \end{cases}\)<br> where \(A+B+D\) is minimum error and \(C\) is reducible error<br> (아래 그림 참고)</li> <li>\(P(correct)\)<br> \(= \int \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_1}P(x|w_1)P(w_1)dx + \int_{R_2}P(x|w_2)P(w_2)dx\)</li> <li>\(P(error) = 1 - P(correct)\)<br> \(= 1 - \int \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_2}P(x|w_1)P(w_1)dx + \int_{R_1}P(x|w_2)P(w_2)dx\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/1-480.webp 480w,/assets/img/2024-09-10-Pattern/1-800.webp 800w,/assets/img/2024-09-10-Pattern/1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>minimum error with rejection :<br> decision이 확실하지 않을 때는 classification 자체를 reject하는 게 적절<br> (classification error도 줄어들고, correct classification도 줄어듬) <ul> <li>feature space \(x\) 를 rejection region \(R\) 과 acceptance region \(A\) 으로 나눠서<br> rejection region \(R=\{ x | \text{max}_{i} P(w_i | x) \leq 1 - t\}\) 에서는 reject decision<br> acceptance region \(A=\{ x | \text{max}_{i} P(w_i | x) \gt 1 - t\}\) 에서는 \(w_1\) or \(w_2\) 로 classification decision 수행</li> <li>\(P_c(t) = P(correct)\)<br> \(= \int_{A} \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_1}P(x|w_1)P(w_1)dx + \int_{R_2}P(x|w_2)P(w_2)dx\)</li> <li>\(P_r(t) = P(reject)\)<br> \(= \int_{R}P(x|w_1)P(w_1)dx + \int_{R}P(x|w_2)P(w_2)dx\)<br> \(= \int_{R} P(x)dx\)</li> <li>\(P_e(t) = P(error)\)<br> \(= P(error, w_1) + P(error, w_2)\)<br> \(= 1 - P_r(t) - P_c(t)\) by 아래 식 대입<br> where \(P(error, w_1) = \int_{-\infty}^{\infty} P(x|w_1)P(w_1)dx - P(reject, w_1) - P(correct, w_1)\)<br> where \(P(error, w_2) = \int_{-\infty}^{\infty} P(x|w_2)P(w_2)dx - P(reject, w_2) - P(correct, w_2)\)<br> where \(\int_{-\infty}^{\infty} P(x|w_1)P(w_1)dx + \int_{-\infty}^{\infty} P(x|w_2)P(w_2)dx = \int_{-\infty}^{\infty} P(x)dx = 1\)</li> </ul> </li> <li>Summary : <ul> <li> <table> <tbody> <tr> <td>$$P(w_i</td> <td>x)$$ : rejection/acceptance region 구하는 데 사용</td> </tr> </tbody> </table> </li> <li>\(P(x|w_i)P(w_i)\) : \(P(correct, w_i), P(reject, w_i), P(error, w_i)\) 구해서<br> \(P_c(t), P_r(t), P_e(t)\) 구하는 데 사용</li> <li> \[P_c(t) + P_r(t) + P_e(t) = 1\] </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/2-480.webp 480w,/assets/img/2024-09-10-Pattern/2-800.webp 800w,/assets/img/2024-09-10-Pattern/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/3-480.webp 480w,/assets/img/2024-09-10-Pattern/3-800.webp 800w,/assets/img/2024-09-10-Pattern/3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="bayes-decision-rule-w-bayes-risk">Bayes Decision Rule w. Bayes risk</h3> <ul> <li>Bayes risk (minimum overall risk) :<br> \(\Omega = \{ w_1, \cdots w_c \}\) 에서 \(w_j\) 는 \(j\) -th class<br> \(A = \{ \alpha_{1}, \cdots, \alpha_{c} \}\) 에서 \(\alpha_{i}\) 는 class \(w_i\) 라고 예측하는 action<br> \(\lambda(\alpha_{i} | w_j) = \lambda_{ij}\) : class \(w_j\) 가 GT일 때, class \(w_i\) 로 pred. 했을 때의 loss <ul> <li>conditional risk for taking action \(\alpha_{i}\) :<br> 특정 input \(x\) 에 대해<br> \(R(\alpha_{i}|x) = \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j|x)\)</li> <li>overall risk for taking action \(\alpha_{i}\) :<br> 모든 input \(x\) 에 대해 적분<br> \(R(\alpha_{i}) = \int R(\alpha_{i}|x)P(x)dx\)<br> \(= \int \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j|x) P(x)dx\)<br> \(= \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j) \int P(x|w_j)dx\)<br> \(= \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j)\)<br> \(= \sum_{j=1}^c \lambda_{ij}P(w_j)\)<br> where pdf(likelihood) 합 \(\int P(x|w_j)dx = 1\)</li> <li>모든 input \(x\) 에 대해 가장 loss가 최소인 class \(w_i\) 로 예측하면,<br> minimum overall risk (= Bayes risk) 를 가짐</li> </ul> </li> <li>Bayes Decision Rule for Bayes risk : <ul> <li>Two-class (\(w_1, w_2\)) problem :<br> choose \(w_1\)<br> if \(R(\alpha_{1} | x) \lt R(\alpha_{2} | x)\)<br> if \(\lambda_{11}P(w_1 | x) + \lambda_{12}P(w_2 | x) \lt \lambda_{21}P(w_1 | x) + \lambda_{22}P(w_2 | x)\)<br> if \((\lambda_{21} - \lambda_{11})P(w_1 | x) \gt (\lambda_{12} - \lambda_{22})P(w_2 | x)\)<br> if \(\frac{P(x | w_1)}{P(x | w_2)} \gt \frac{(\lambda_{12} - \lambda_{22})P(w_2)}{(\lambda_{21} - \lambda_{11})P(w_1)}\)<br> if \(\frac{P(x | w_1)}{P(x | w_2)} \gt \frac{P(w_2)}{P(w_1)}\) for \(\lambda_{11}=\lambda_{22}=0\) and \(\lambda_{12}=\lambda_{21}\)<br> (likehood ratio \(\gt\) threshold) (위의 Bayes Decision Rule에서 구한 식과 same)</li> <li> <table> <tbody> <tr> <td>loss $$\lambda(\alpha_{i}</td> <td>w_j) = \begin{cases} 0 &amp; \text{if} &amp; i=j &amp; (\text{no penalty}) \ 1 &amp; \text{if} &amp; i \neq j &amp; (\text{equal penalty}) \end{cases}$$ 일 때</td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td>conditional risk $$R(\alpha_{i}</td> <td>x) = \sum_{j=1}^c \lambda(\alpha_{i}</td> <td>w_j)P(w_j</td> <td>x) = \sum_{j=1, j \neq i}^c P(w_j</td> <td>x) = 1 - P(w_i</td> <td>x)$$ 이므로</td> </tr> <tr> <td>Bayes Decision Rule에서 conditional risk $$R(\alpha_{i}</td> <td>x)\(최소화는 posterior\)P(w_i</td> <td>x)$$ 최대화와 같음</td> <td> </td> <td> </td> <td> </td> </tr> </tbody> </table> </li> <li>multi-class problem :<br> classifieer (discriminant function) (space-partitioning function) \(g(x)\) 에 대해<br> choose \(w_i\) where \(g_{i}(x)\) is the largest<br> s.t. decision boundary is \(g_{i}(x) = g_{j}(x)\) where they are the two largest discriminant functions<br> e.g. Bayes classifier : \(g_{i}(x) = - R(\alpha_{i} | x)\) or \(g_{i}(x) = P(w_i | x)\) or \(g_{i}(x) = P(x | w_i)P(w_i)\) or \(g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i)\)</li> </ul> </li> </ul> <h3 id="discriminant-function-for-gaussian-pdf">Discriminant Function for Gaussian PDF</h3> <ul> <li> <p>\(G(\boldsymbol x) = \frac{1}{(2\pi)^{\frac{d}{2}} | \Sigma |^{\frac{1}{2}}}e^{-\frac{1}{2}(\boldsymbol x - \boldsymbol \mu)^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu)}\)<br> where \(d \times d\) covariance \(\Sigma = E[(\boldsymbol x - \boldsymbol \mu)(\boldsymbol x - \boldsymbol \mu)^T] = E[\boldsymbol x \boldsymbol x^{T}] - \boldsymbol \mu \boldsymbol \mu^{T} = S - \boldsymbol \mu \boldsymbol \mu^{T}\)<br> where \(S = E[\boldsymbol x \boldsymbol x^{T}]\) : standard autocorrelation matrix</p> </li> <li> <p>Discriminant function for Gaussian PDF :<br> likelihood \(P(x | w_i)\) 를 Gaussian PDF로 둘 경우,<br> \(g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{d}{2} \text{ln}(2\pi) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\)</p> <ul> <li>case 1) \(\Sigma_{i} = \sigma^{2} \boldsymbol I\) (모든 classes에 대해 equal covariance) (등방성(sphere)) <br> \(g_{i}(x) = -\frac{\| \boldsymbol x - \boldsymbol \mu_{i} \|^2}{2 \sigma^{2}} + \text{ln}P(w_i)\)<br> \(i\) 와 관련된 term만 남기면<br> \(g_{i}(x) = \frac{1}{\sigma^{2}} \boldsymbol \mu_{i}^T \boldsymbol x - \frac{1}{2\sigma^{2}} \boldsymbol \mu_{i}^T \boldsymbol \mu_{i} + \text{ln}P(w_i)\)<br> \(= \boldsymbol w_i^T \boldsymbol x + \boldsymbol w_{i0}\) (linear) <ul> <li>decision boundary :<br> hyperplane \(g(x) = g_{i}(x) - g_{j}(x) = (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T(\boldsymbol x - \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) + \frac{\sigma^{2}}{(\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T} \text{ln}\frac{P(w_i)}{P(w_j)})\)<br> \(= \boldsymbol w^T (\boldsymbol x - \boldsymbol x_0) = 0\)</li> <li>\(\boldsymbol x_0\) 를 지나고 \(\boldsymbol w = \boldsymbol \mu_{i} - \boldsymbol \mu_{j}\) 에 수직인 hyperplane</li> <li>\(\boldsymbol x_0 = \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) - \frac{\sigma^{2}}{\| \boldsymbol \mu_{i} - \boldsymbol \mu_{j} \|^2} \text{ln}\frac{P(w_i)}{P(w_j)} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})\) 이므로<br> \(\boldsymbol x_0\) 의 위치는 \(\boldsymbol \mu_{i}\) 와 \(\boldsymbol \mu_{j}\) 의 중점에서 \(\begin{cases} \boldsymbol \mu_{j} \text{쪽으로 이동} &amp; \text{if} &amp; P(w_i) \gt P(w_j) \\ \boldsymbol \mu_{i} \text{쪽으로 이동} &amp; \text{if} &amp; P(w_i) \lt P(w_j) \end{cases}\)<br> (\(P(w_i)\) 와 \(P(w_j)\) 중 더 작은 쪽으로 이동)<br> (\(\sigma^{2}\) 이 (\(\| \mu_{i} - \mu_{j} \|^2\) 에 비해 비교적) 작은 경우 \(P(w_i)\) 와 \(P(w_j)\) 에 따른 \(x_0\) shift는 미약)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/4-480.webp 480w,/assets/img/2024-09-10-Pattern/4-800.webp 800w,/assets/img/2024-09-10-Pattern/4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Discriminant function for Gaussian PDF :<br> likelihood \(P(x | w_i)\) 를 Gaussian PDF로 둘 경우,<br> \(g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{d}{2} \text{ln}(2\pi) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\) <ul> <li>case 2) \(\Sigma_{i} = \Sigma\) (symmetric) (모든 classes에 대해 equal covariance) (비등방성(hyper-ellipsoidal))<br> \(g_{i}(x) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) + \text{ln}P(w_i)\)<br> \(i\) 와 관련된 term만 남기면<br> \(g_{i}(x) = \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol x - \frac{1}{2} \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol \mu_{i} + \text{ln}P(w_i)\)<br> \(= \boldsymbol w_i^T \boldsymbol x + \boldsymbol w_{i0}\) (linear) <ul> <li>decision boundary :<br> hyperplane \(g(x) = g_{i}(x) - g_{j}(x) = (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T \Sigma^{-1} (\boldsymbol x - \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) + \frac{1}{(\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T \Sigma^{-1}} \text{ln}\frac{P(w_i)}{P(w_j)})\)<br> \(= \boldsymbol w^T (\boldsymbol x - \boldsymbol x_0) = 0\)</li> <li>\(\boldsymbol x_0\) 를 지나고 \(\boldsymbol w = \Sigma^{-1} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})\) 에 수직인 hyperplane</li> <li>\(\boldsymbol x_0 = \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) - \frac{\text{ln}\frac{P(w_i)}{P(w_j)}}{(\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T \Sigma^{-1} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})\) 이므로<br> 마찬가지로 \(\boldsymbol x_0\) 의 위치는 \(\boldsymbol \mu_{i}\) 와 \(\boldsymbol \mu_{j}\) 의 중점에서 \(P(w_i)\) 와 \(P(w_j)\) 중 더 작은 쪽으로 이동</li> <li>\(\boldsymbol w = \Sigma^{-1} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})\) 는<br> vector \(\boldsymbol \mu_{i} - \boldsymbol \mu_{j}\) 를 \(\Sigma^{-1}\) 로 회전시킨 vector를 의미</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/5-480.webp 480w,/assets/img/2024-09-10-Pattern/5-800.webp 800w,/assets/img/2024-09-10-Pattern/5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Discriminant function for Gaussian PDF :<br> likelihood \(P(x | w_i)\) 를 Gaussian PDF로 둘 경우,<br> \(g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{d}{2} \text{ln}(2\pi) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\) <ul> <li>case 2) \(\Sigma_{i}\) is arbitrary (symmetric) (class마다 covariance 다름) (비등방성(hyper-ellipsoidal))<br> \(g_{i}(x) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\)<br> \(\Sigma_{i}\) 가 \(i\) 에 대한 term이므로<br> \(g_{i}(x) = - \frac{1}{2} \boldsymbol x^T \Sigma^{-1} \boldsymbol x + \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol x - \frac{1}{2} \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol \mu_{i} - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\)<br> \(= - \frac{1}{2} \boldsymbol x^T \Sigma^{-1} \boldsymbol x + \boldsymbol w_i^T \boldsymbol x + \boldsymbol w_{i0}\) (quadratic) 는<br> quadratic discriminant function in \(x\) <ul> <li>decision surface :<br> hyperquadratic (hyperplane, hypersphere, hyperellipsoidal, hyperparaboloid, hyperhyperboloid)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/6-480.webp 480w,/assets/img/2024-09-10-Pattern/6-800.webp 800w,/assets/img/2024-09-10-Pattern/6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="bayes-rule-for-discrete-case">Bayes Rule for Discrete Case</h3> <ul> <li> <table> <tbody> <tr> <td>pdf 적분 $$\int p(x</td> <td>w_j) dx$$ 대신</td> <td> </td> </tr> <tr> <td>확률 합 $$lim_{\Delta x \rightarrow 0} \Sigma_{k=-\infty}^{\infty} p(x_k</td> <td>w_j) \Delta x\(\)\rightarrow\(\)\Sigma_{k=1}^m P(v_k</td> <td>w_j)$$</td> </tr> </tbody> </table> </li> <li> <p>Bayes Decision Rule은 discrete case에서도 same<br> Bayes risk minimize 위해 conditional risk \(R(\alpha_{i} | x)\) minimize<br> (posterior maximize와 same)</p> </li> <li>\(\boldsymbol x = [x_1, x_2, \ldots, x_d]^T\) 에서 \(x_i\) 가 0 혹은 1의 값을 갖는 Bernoulli random var.일 때 <ul> <li>class \(w_1\) 일 때 :<br> \(x_i \sim p_i^{x_i}(1-p_i)^{1-x_i}\)<br> \(P(\boldsymbol x | w_1) = P([x_1, x_2, \ldots, x_d]^T | w_1) = \prod_{i=1}^d P(x_i | w_1) = \prod_{i=1}^d p_i^{x_i}(1-p_i)^{1-x_i}\)</li> <li>class \(w_2\) 일 때 :<br> \(x_i \sim q_i^{x_i}(1-q_i)^{1-x_i}\)<br> \(P(\boldsymbol x | w_2) = P([x_1, x_2, \ldots, x_d]^T | w_2) = \prod_{i=1}^d P(x_i | w_2) = \prod_{i=1}^d q_i^{x_i}(1-q_i)^{1-x_i}\)</li> <li>likelihood ratio :<br> \(\frac{P(\boldsymbol x | w_1)}{P(\boldsymbol x | w_2)} = \prod_{i=1}^d (\frac{p_i}{q_i})^{x_i}(\frac{1-p_i}{1-q_i})^{1-x_i}\)</li> <li>discriminant function :<br> choose \(w_1\)<br> if \(g(x) = \text{ln} \frac{P(\boldsymbol x | w_1)P(w_1)}{P(\boldsymbol x | w_2)P(w_2)} = \sum_{i=1}^d(x_i \text{ln}\frac{p_i}{q_i} + (1-x_i)\text{ln}\frac{1-p_i}{1-q_i}) + \text{ln}\frac{P(w_1)}{P(w_2)} = \sum_{i=1}^d w_ix_i + w_0 = \boldsymbol w^T \boldsymbol x + w_0 \gt 0\)<br> where \(w_i = \text{ln}\frac{p_i(1-q_i)}{q_i(1-p_i)}\) and \(w_0 = \sum_{i=1}^d(\text{ln}\frac{1-p_i}{1-q_i}) + \text{ln}\frac{P(w_1)}{P(w_2)}\) <ul> <li>case 1-1) \(p_i = q_i\)<br> \(w_i = 0\) , so \(x_i\) 는 class 결정에 영향 없음</li> <li>case 1-2) \(p_i \gt q_i\)<br> \(w_i \gt 0\) , so \(x_i = 1\) 은 class \(w_1\) 선택에 보탬</li> <li>case 1-3) \(p_i \lt q_i\)<br> \(w_i \lt 0\) , so \(x_i = 1\) 은 class \(w_2\) 선택에 보탬</li> <li>case 2-1) \(P(w_1)\) 값 증가 (\(\gt P(w_2)\))<br> \(w_0\) 값이 커지므로 class \(w_1\) 선택에 보탬</li> <li>case 2-2) \(P(w_1)\) 값 감소 (\(\lt P(w_2)\))<br> \(w_0\) 값이 작아지므로 class \(w_1\) 선택에 보탬</li> </ul> </li> </ul> </li> </ul> <h2 id="chapter-2-linear-transformation">Chapter 2. Linear Transformation</h2> <h3 id="linear-transformation">Linear Transformation</h3> <ul> <li> \[y = A^Tx\] <ul> <li>mean and variance :<br> \(\mu_{y} = A^T \mu_{x}\)<br> \(\Sigma_{y} = E[(y - \mu_{y})(y - \mu_{y})^T] = A^T \Sigma_{x} A\)</li> <li>Mahalanobis distance :<br> \(d_y^2 = (y - \mu_{y})^T\Sigma_{y}^{-1}(y - \mu_{y}) = \cdots = d_x^2\)<br> <code class="language-plaintext highlighter-rouge">linear transformation</code>을 해도 Mahalanobis distance는 <code class="language-plaintext highlighter-rouge">그대로</code>임<br> (Euclidean distance \((x - \mu_{x})^T(x - \mu_{x})\) 는 linear transformation을 하면 variant)</li> <li>Gaussian distribution :<br> \(x \sim N(\mu_{x}, \Sigma_{x})\) 일 때<br> \(P(y) = (2 \pi)^{- \frac{d}{2}} | \Sigma_{y} |^{-\frac{1}{2}} \exp(-\frac{1}{2}(y - \mu_{y})^T \Sigma_{y}^{-1} (y - \mu_{y})) = (2 \pi)^{- \frac{d}{2}} | A |^{-1} | \Sigma_{x} |^{-\frac{1}{2}} \exp(-\frac{1}{2}(x - \mu_{x})^T \Sigma_{x}^{-1} (x - \mu_{x})) = \frac{1}{|A|} P(x)\)</li> </ul> </li> </ul> <h3 id="orthonormal-transformation">Orthonormal Transformation</h3> <ul> <li>\(x = \sum_{i=1}^d y_i \phi_{i}\)<br> where \(\{ \phi_{i}, \cdots, \phi_{d} \}\) is orthonormal basis<br> Equivalently,<br> \(y_i = x^T \phi_{i}\)<br> where vector \(x\) 를 i-th eigenvector \(\phi_{i}\) 에 project한 게 \(y_i\) <ul> <li>approx. \(x\) : <ul> <li>\(\{ y_{m+1}, \cdots, y_{d} \}\) 를 pre-defined constants \(\{ b_{m+1}, \cdots, b_{d} \}\) 로 대체했을 때<br> \(\hat x(m) = \sum_{i=1}^m y_i \phi_{i} + \sum_{i=m+1}^d b_i \phi_{i}\)</li> </ul> </li> <li>optimal \(b_i\) : <ul> <li>error \(\Delta x(m) = x - \hat x(m) = \sum_{i=m+1}^d (y_i - b_i) \phi_{i}\)<br> MSE \(\bar \epsilon^{2}(m) = E[| \Delta x(m) |^2] = E[\Delta x^T(m) \Delta x(m)] = \sum_{i=m+1}^d E[(y_i - b_i)^2]\)</li> <li>orthonormal basis \(\phi_{i}, \phi_{j}\) 에 대해<br> \(\frac{\partial}{\partial b_i} E[(y_i - b_i)^2] = -2(E[y_i] - b_i) = 0\) 이므로<br> MSE 최소화하는 optimal \(b_i = E[y_i]\)</li> </ul> </li> <li>optimal \(\phi_{i}\) : <ul> <li>\(x = \sum_{j=1}^d y_j \phi_{j}\) 의 양변에 \(\phi_{i}^T\) 를 곱하면<br> \(y_i = x^T \phi_{i}\) 이고<br> optimal \(b_i = E[y_i]\) 이므로<br> MSE \(\bar \epsilon^{2}(m) = \sum_{i=m+1}^d E[(y_i - b_i)^2] = \sum_{i=m+1}^d E[(x^T \phi_{i} - E[x^T \phi_{i}])^T(x^T \phi_{i} - E[x^T \phi_{i}])] = \sum_{i=m+1}^d \text{Var}(\phi_{i}^{T} x) = \sum_{i=m+1}^d \phi_{i}^T \Sigma_{x} \phi_{i}\)</li> <li>orthonormality equality constraint \(\phi_{i}^T\phi_{i} = \| \phi_{i} \| = 1\) 을 만족하면서 MSE \(\bar \epsilon^{2}(m)\) 를 최소화하는 \(\phi_{i}\) 는 Lagrange multiplier Method <a href="https://semyeong-yu.github.io/blog/2024/Lagrange/">Link</a> 로 찾을 수 있다<br> \(\rightarrow\)<br> goal : minimize \(U(m) = \sum_{i=m+1}^d \phi_{i}^T \Sigma_{x} \phi_{i} + \sum_{i=m+1}^d \lambda_{i}(1 - \phi_{i}^T\phi_{i})\)<br> \(\frac{\partial}{\partial x}(x^TAx) = (A + A^T)x = 2Ax\) for symmetric \(A\) 이므로<br> \(\frac{\partial}{\partial \phi_{i}} U(m) = 2(\Sigma_{x}\phi_{i} - \lambda_{i}\phi_{i}) = 0\) 이므로<br> MSE 최소화하는 optimal \(\phi_{i}\) 는 \(\Sigma_{x}\phi_{i} = \lambda_{i}\phi_{i}\) 을 만족하므로<br> \(\phi_{i}\) 와 \(\lambda_{i}\) 는 covariance matrix \(\Sigma_{x}\) 의 eigenvector and eigenvalue 이다</li> </ul> </li> </ul> </li> <li>Eigenvector and Eigenvalue : <ul> <li>\(\Sigma \Phi = \Phi \Lambda\) where \(\Phi \Phi^{T} = I\)</li> <li>If \(\Sigma\) is non-singular (\(| \Sigma | \neq 0\)),<br> all eigenvalues \(\lambda\) are non-zero</li> <li>If \(\Sigma\) is positive-definite (\(x^T \Sigma x \geq 0\) for all \(x \neq 0\)),<br> all eigenvalues \(\lambda\) are positive</li> <li>If \(\Sigma\) is real and symmetric,<br> all eigenvalues \(\lambda\) are real<br> and eigenvectors(w. distinct eigenvalues) are orthogonal <ul> <li>pf)<br> \(\Sigma \phi_{i} = \lambda_{i} \phi_{i}\) and \(\Sigma \phi_{j} = \lambda_{j} \phi_{j}\)<br> \(\phi_{j}^T \Sigma \phi_{i} - \phi_{i}^T \Sigma \phi_{j} = \phi_{j}^T \lambda_{i} \phi_{i} - \phi_{i}^T \lambda_{j} \phi_{j}\)<br> \(0 = (\lambda_{i} - \lambda_{j}) \phi_{j}^T \phi_{i}\) since \(\Sigma\) is symmetric<br> \(\rightarrow \phi_{j}^T \phi_{i} = 0\) (eigenvectors are orthogonal)</li> </ul> </li> </ul> </li> <li>Orthonormal Transformation :<br> \(y = \Phi^{T} x\)<br> for \(\Phi = [\phi_{1}, \cdots \phi_{d}]\) and \(\Phi \Phi^{T} = I\) <ul> <li>vector \(x\) 를 i-th eigenvector \(\phi_{i}\) 에 project한 게 \(y_{i}\)<br> 즉, vector \(x\) 를 new coordinate \(\Phi = [\phi_{1}, \cdots \phi_{d}]\) 으로 나타낸 게 vector \(y\)</li> <li>eigenvector는 principal axis를 나타내고, eigenvalue는 해당 방향으로 퍼진 정도를 나타냄</li> <li>\(y\) 의 covariance matrix인 \(\Sigma_{y}\) 는 <code class="language-plaintext highlighter-rouge">diagonal matrix</code><br> (uncorrelated random vector \(y\)) <ul> <li>\(\Sigma_{y}\)<br> \(= \Phi^{T} \Sigma_{x} \Phi\)<br> \(= \Phi^{T} \Phi \Lambda\) since \(\Sigma \Phi = \Phi \Lambda\)<br> \(= \Phi^{-1} \Phi \Lambda\) since eigenvector matrix is orthogonal matrix (\(\Phi^{T} = \Phi^{-1}\))<br> \(= \Lambda\)</li> </ul> </li> <li>distance : <ul> <li>Mahalanobis distance는 any linear transformation에 대해 보존됨</li> <li> <code class="language-plaintext highlighter-rouge">Euclidean distance</code>는 linear transformation 중 orthonormal transformation일 때만 <code class="language-plaintext highlighter-rouge">보존</code>됨<br> \(\| y \|^2 = y^Ty = x^T \Phi \Phi^{T} x = x^T \Phi \Phi^{-1} x = x^T x = \| x \|^2\)</li> </ul> </li> </ul> </li> </ul> <h3 id="whitening-transformation">Whitening Transformation</h3> <ul> <li>Whitening Transformation :<br> \(y = \Lambda^{-\frac{1}{2}} \Phi^{T} x = (\Phi \Lambda^{-\frac{1}{2}})^T x\)<br> (Orthonormal Transformation을 한 뒤 추가로 \(\Lambda^{-\frac{1}{2}}\) 로 transformation) <ul> <li>\(y\) 의 covariance matrix인 \(\Sigma_{y}\) 는 <code class="language-plaintext highlighter-rouge">identity matrix</code> \(I\) <ul> <li>\(\Sigma_{y}\)<br> \(= (\Lambda^{-\frac{1}{2}} \Phi^{T}) \Sigma_{x} (\Phi \Lambda^{-\frac{1}{2}})\)<br> \(= \Lambda^{-\frac{1}{2}} \Lambda \Lambda^{-\frac{1}{2}}\)<br> \(= I\)</li> </ul> </li> <li>\(\Lambda^{-\frac{1}{2}}\) 은 principal components의 scale을 \(\frac{1}{\sqrt{\lambda_{i}}}\) 배 하는 효과</li> <li>Whitening Transformation을 한 번 하고나면,<br> 그 후에 any Orthonormal Transformation(\(y = \Phi^{T} x\) for \(\Psi \Psi^{T} = I\))을 해도<br> covariance matrix는 항상 \(\Psi I \Psi^{T} = I\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/7-480.webp 480w,/assets/img/2024-09-10-Pattern/7-800.webp 800w,/assets/img/2024-09-10-Pattern/7-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="sample-separation">Sample Separation</h3> <ul> <li>Sample Separation :<br> uncorrelated normal samples \(\sim N(0, I)\) 로부터 correlated sample \(\sim N(\mu_{x}, \Sigma_{x})\) 만들기 <ul> <li>How? :<br> given data \(x\) 에서 \(\mu_{x}\) 를 뺀 뒤 Whitening Transformation 적용하면 \(N(0, I)\) 이므로 이 과정을 역으로 실행</li> <li>Step 1) Normal distribution으로부터 N개의 \(d\) -dim. independent vectors를 sampling<br> \(y_1, y_2, \cdots, y_N \sim N(0, I)\)</li> <li>Step 2) Inverse-Whitening-Transformation 적용하여 Normal distribution을 x-space로 변환 \(x_k = \Phi \Lambda^{\frac{1}{2}} y_k\)<br> for given \(\Sigma_{x}\)<br> and its eigen-decomposition \(\Sigma_{x} \Phi = \Phi \Lambda\)</li> <li>Step 3) x-space의 samples에 \(\mu_{x}\) 더함<br> \(x_k = \Phi \Lambda^{\frac{1}{2}} y_k + \mu_{x} \sim N(\mu_{x}, \Sigma_{x})\)<br> for given \(\mu_{x}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/8-480.webp 480w,/assets/img/2024-09-10-Pattern/8-800.webp 800w,/assets/img/2024-09-10-Pattern/8-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="chapter-3-maximum-likelihood-and-bayesian-parameter-estimation">Chapter 3. Maximum-likelihood and Bayesian Parameter Estimation</h2> <ul> <li>parameter estimation : <ul> <li>Maximum Likelihood Estimation (MLE) :<br> (true) parameters are <code class="language-plaintext highlighter-rouge">unknown</code>, but <code class="language-plaintext highlighter-rouge">fixed</code><br> estimators are random variable</li> <li>Bayesian Estimation :<br> parameters are <code class="language-plaintext highlighter-rouge">random variables</code> and <code class="language-plaintext highlighter-rouge">prior is known</code> </li> </ul> </li> </ul> <h3 id="maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</h3> <ul> <li> <p>Assumption :<br> training data \(D_j\) \(\sim\) likelihood \(p(D_j | w_j) = N(\mu_{j}, \Sigma_{j})\)<br> (i.i.d random samples)</p> </li> <li>MLE : <ul> <li>likelihood :<br> \(\hat \theta = \text{argmax}_{\theta} p(D=\{ x_1, x_2, \ldots, x_n \} | \theta) = \text{argmax}_{\theta} \prod_{k=1}^n p(x_k | \theta)\)</li> <li>log-likelihood :<br> \(\hat \theta = \text{argmax}_{\theta} p(D=\{ x_1, x_2, \ldots, x_n \} | \theta) = \text{argmax}_{\theta} \sum_{k=1}^n \text{ln} p(x_k | \theta)\)</li> </ul> </li> <li>Gaussian likelihood : <ul> <li>unknown \(\mu\) : <ul> <li>likelihood :<br> \(p(x_k | \mu) = (2 \pi)^{-\frac{d}{2}} | \Sigma |^{-\frac{1}{2}} \text{exp}(-\frac{1}{2}(x_k - \mu)^T \Sigma^{-1} (x_k - \mu))\)<br> \(p(D=\{ x_1, x_2, \ldots, x_N \} | \mu) = \prod_{k=1}^N p(x_k | \mu) = (2 \pi)^{-\frac{dN}{2}} | \Sigma |^{-\frac{N}{2}} \text{exp}(-\frac{1}{2} \sum_{k=1}^N (x_k - \mu)^T \Sigma^{-1} (x_k - \mu))\)</li> <li>log-likelihood :<br> \(\text{ln} p(D | \mu) = -\frac{dN}{2} \text{ln}(2 \pi) -\frac{N}{2} \text{ln} | \Sigma | -\frac{1}{2} \sum_{k=1}^N (x_k - \mu)^T \Sigma^{-1} (x_k - \mu)\)</li> <li>matrix derivative :<br> \(\frac{d}{dx}(Ax) = A\)<br> \(\frac{d}{dx}(y^TAx) = A^Ty\)<br> \(\frac{d}{dx}(x^TAx) = (A+A^T)x\)<br> \(\frac{d}{dA}(x^TAx) = xx^T\)<br> \(\frac{\partial |A|}{\partial A} = (\text{adj}(A))^T = |A|(A^{-1})^T\)<br> \(\frac{\partial \text{ln}|A|}{\partial A} = (A^{-1})^T = (A^T)^{-1}\) where \(|A| = \frac{1}{|A^{-1}|}\)</li> <li>MLE problem :<br> \(\nabla_{\mu} \text{ln} p(D | \mu) = -\frac{1}{2} \sum_{k=1}^N ((\Sigma^{-1} + (\Sigma^{-1})^T) (x_k - \mu)) \times (-1) = (\Sigma^{-1} + (\Sigma^{-1})^T)(\sum_{k=1}^N x_k - \sum_{k=1}^N \mu) = 0\)<br> \(\sum_{k=1}^N x_k - N \mu = 0\)<br> \(\hat \mu_{\text{MLE}} = \frac{1}{N} \sum_{k=1}^N x_k\)</li> <li>Summary : <ul> <li>\(\hat \mu_{\text{MLE}} = \frac{1}{N} \sum_{k=1}^N x_k\)<br> (true mean의 MLE estimator는 sample mean)</li> <li>\(E[\hat \mu_{\text{MLE}}] = \mu\)<br> (\(\hat \mu_{\text{MLE}}\) 는 <code class="language-plaintext highlighter-rouge">unbiased</code> estimator)</li> </ul> </li> </ul> </li> <li>unknown \(\mu\) and \(\Sigma\) : <ul> <li>log-likelihood :<br> \(\text{ln} p(D | \mu) = -\frac{dN}{2} \text{ln}(2 \pi) -\frac{N}{2} \text{ln} | \Sigma | -\frac{1}{2} \sum_{k=1}^N (x_k - \mu)^T \Sigma^{-1} (x_k - \mu) = -\frac{dN}{2} \text{ln}(2 \pi) + \frac{N}{2} \text{ln} | \Sigma^{-1} | -\frac{1}{2} \sum_{k=1}^N (x_k - \mu)^T \Sigma^{-1} (x_k - \mu)\)</li> <li>MLE problem :<br> \(\nabla_{\Sigma^{-1}} \text{ln} p(D | \mu) = \frac{N}{2}\Sigma^{T} - \frac{1}{2} \sum_{k=1}^N (x_k - \mu)(x_k - \mu)^T = 0\)<br> \(N \Sigma^{T} = \sum_{k=1}^N (x_k - \mu)(x_k - \mu)^T\)<br> \(\mu = \hat \mu_{\text{MLE}}\) 대입하고, \(\Sigma\) 는 symmetric(\(\Sigma^{T} = \Sigma\))하므로<br> \(\hat \Sigma_{\text{MLE}} = \frac{1}{N} \sum_{k=1}^N (x_k - \hat \mu_{\text{MLE}})(x_k - \hat \mu_{\text{MLE}})^T\)</li> <li>Summary : <ul> <li>\(\hat \Sigma_{\text{MLE}} = \frac{1}{N} \sum_{k=1}^N (x_k - \hat \mu_{\text{MLE}})(x_k - \hat \mu_{\text{MLE}})^T\)<br> (\(\mu\) 먼저 estimate한 뒤 \(\Sigma\) estimate)</li> <li>\(E[\hat \Sigma_{\text{MLE}}] = \frac{1}{N} E[\sum_{k=1}^N (x_k - \hat \mu_{\text{MLE}})(x_k - \hat \mu_{\text{MLE}})^T] = \frac{N-1}{N} \Sigma \neq \Sigma\)<br> (\(\hat \Sigma_{\text{MLE}}\) 는 <code class="language-plaintext highlighter-rouge">biased</code> estimator) <ul> <li>pf) 아래 식 이용<br> \(E[x_i x_j^T] = \begin{cases} \Sigma + \mu \mu^{T} &amp; \text{if} &amp; i = j \\ \mu \mu^{T} &amp; \text{if} &amp; i \neq j \end{cases}\)<br> since \(\Sigma = E[(x - \mu)(x - \mu)^T] = \cdots = E[xx^T] - \mu \mu^{T}\)<br> since \(0 = E[(x_i - \mu)(x_j - \mu)^T] = E[x_i x_j^T] - \mu \mu^{T}\) by independence \(i \neq j\)</li> </ul> </li> <li>\(\text{lim}_{N \rightarrow \infty}E[\hat \Sigma_{\text{MLE}}] = \text{lim}_{N \rightarrow \infty} \frac{N-1}{N} \Sigma = \Sigma\)<br> (\(\hat \Sigma_{\text{MLE}}\) 는 <code class="language-plaintext highlighter-rouge">asymptotically unbiased</code> estimator)<br> 또는<br> \(\hat \Sigma_{\text{MLE}} = \frac{1}{N-1} \sum_{k=1}^N (x_k - \hat \mu_{\text{MLE}})(x_k - \hat \mu_{\text{MLE}})^T\)<br> (위처럼 설정하면 \(\hat \Sigma_{\text{MLE}}\) 는 <code class="language-plaintext highlighter-rouge">unbiased</code> estimator)</li> </ul> </li> </ul> </li> </ul> </li> <li>MLE : <ul> <li>MLE is <code class="language-plaintext highlighter-rouge">asymptotically consistent</code><br> if \(\text{lim}_{N \rightarrow \infty} P(\| \hat \theta_{\text{MLE}} - \theta_{\text{true}} \| \leq \epsilon) = 1\) for arbitrary small \(\epsilon\)<br> (sample 수 \(N\) 이 크면 param. estimate은 true value랑 거의 비슷)<br> by central limit theorem and the fact that MLE is related to the sum of random var.</li> <li>MLE is <code class="language-plaintext highlighter-rouge">asymptotically efficient</code><br> since MLE는 Cramer-Rao lower bound(any estimate이 달성할 수 있는 the lowest value of variance)</li> </ul> </li> </ul> <h3 id="bayesian-estimation">Bayesian Estimation</h3> <ul> <li>Summary : <ul> <li>MLE (maximum likelihood estimation) : <ul> <li>when \(\theta\) is unknown, but fixed</li> <li> <table> <tbody> <tr> <td>maximize likelihood $$\hat \theta_{MLE} = \text{argmax}_{\theta} p(D</td> <td>\theta)$$</td> </tr> </tbody> </table> </li> </ul> </li> <li>MAP (maximum a posterior) : <ul> <li>when \(\theta\) is random var. and prior \(p(\theta)\) is known</li> <li> <table> <tbody> <tr> <td>maximize posterior $$\hat \theta_{MAP} = \text{argmax}_{\theta} p(\theta</td> <td>D) = \text{argmax}_{\theta} \text{ln} p(D</td> <td>\theta) + \text{ln} p(\theta)$$</td> </tr> </tbody> </table> </li> </ul> </li> <li>If prior \(p(\theta)\) is constant (uniform distribution),<br> MLE와 MAP는 same</li> </ul> </li> <li> <p>prior \(p(\theta)\) 와 posterior \(p(\theta | D)\) 가 같은 확률 분포의 형태를 가질 경우<br> prior \(p(\theta)\) 를<br> likehood \(p(D | \theta)\) 에 대한 <code class="language-plaintext highlighter-rouge">conjugate prior</code>라고 말한다</p> </li> <li>Gaussian case : <ul> <li>random var. \(\mu\) : <ul> <li>likelihood and conjugate prior :<br> \(x_k \sim\) \(p(x_k | \mu) = N(\mu, \sigma^{2})\)<br> where \(\mu \sim\) \(p(\mu) = N(\mu_{0}, \sigma_{0}^{2})\)</li> <li>posterior (수식 유도는 아래에 별도로) :<br> \(p(\mu | D, \sigma^{2}) \propto N(\mu_{N}, \sigma_{N}^{2})\)<br> \(= \frac{1}{\sqrt{2 \pi} \sigma_{N}} e^{-\frac{1}{2}(\frac{\mu - \mu_{N}}{\sigma_{N}})^2}\)<br> where \(\hat \mu_{MAP} = \text{argmax}_{\mu} p(\mu | D, \sigma^{2}) = \mu_{N} = (\frac{N}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}}) \hat \mu_{MLE} + \frac{\frac{\sigma^{2}}{\sigma_{0}^{2}}}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}} \mu_{0}\)<br> where \(\sigma_{N}^{2} = \frac{\sigma^{2}}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}}\) <ul> <li>Bayesian Learning :<br> \(N\), 즉 sample 수가 많아질수록<br> \(\mu_{N}\) 은 \(\hat \mu_{MLE}\) 에 가까워지고<br> \(\sigma_{N}^{2}\), 즉 uncertainty about \(\mu_{N}\) 은 감소<br> 따라서 \(N \rightarrow \infty\) 이면<br> posterior \(p(\mu | D, \sigma^{2})\) 는 \(\mu_{N} = \hat \mu_{MLE}\) 에서의 Dirac delta function</li> <li>\(\hat \mu_{MAP} = \mu_{N}\) 에서<br> \((\frac{N}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}}) \hat \mu_{MLE}\) 는 empirical data samples 부분이고<br> \(\frac{\frac{\sigma^{2}}{\sigma_{0}^{2}}}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}} \mu_{0}\) 는 prior info. 부분</li> <li>만약 \(\sigma_{0}^{2} = 0\) 이라면<br> prior : variance \(\sigma_{0}^{2}\) 가 매우 작으므로, certain that \(\mu = \mu_{0}\)<br> So,<br> posterior : \(\mu_{N} = \mu_{0}\) (data samples는 \(\mu_{N}\) 에 영향 없음)</li> <li>만약 \(\sigma^{2} \ll \sigma_{0}^{2}\) 이라면<br> prior : variance \(\sigma_{0}^{2}\) 가 매우 크므로, so uncertain that \(\mu = \mu_{0}\)<br> So,<br> posterior : \(\mu_{N} = \hat \mu_{MLE}\) (data samples가 \(\mu_{N}\) 에 대부분의 영향 미침)</li> </ul> </li> <li>posterior (수식 유도) :<br> \(p(\mu | D, \sigma^{2})\)<br> \(\propto p(D | \mu, \sigma^{2}) p(\mu) = \frac{1}{(2 \pi \sigma^{2})^{\frac{N}{2}}(2 \pi \sigma_{0}^{2})^{\frac{1}{2}}}e^{-\frac{1}{2\sigma^{2}}\sum_{i=1}^N(x_i - \mu)^2 -\frac{1}{2\sigma_{0}^{2}}(\mu - \mu_{0})^2}\)<br> \(\propto e^{-\frac{1}{2\sigma^{2}}\sum_{i=1}^N(x_i - \mu)^2 -\frac{1}{2\sigma_{0}^{2}}(\mu - \mu_{0})^2}\)<br> \(\propto e^{-\frac{1}{2\sigma^{2}}(N \mu^{2} - 2 \mu \sum_{i=1}^N x_i) -\frac{1}{2\sigma_{0}^{2}}(\mu^{2} - 2 \mu \mu_{0})}\)<br> \(= e^{-\frac{1}{2}(\mu^{2}(\frac{N}{\sigma^{2}} + \frac{1}{\sigma_{0}^{2}}) - 2 \mu (\frac{N \hat \mu_{MLE}}{\sigma^{2}} + \frac{\mu_{0}}{\sigma_{0}^{2}}))}\)<br> \(\propto \frac{1}{\sqrt{2 \pi} \sigma_{N}} e^{-\frac{1}{2}(\frac{\mu - \mu_{N}}{\sigma_{N}})^2}\)<br> where \(\hat \mu_{MAP} = \mu_{N} = (\frac{N\sigma_{0}^{2}}{N\sigma_{0}^{2} + \sigma^{2}}) \hat \mu_{MLE} + \frac{\sigma^{2}}{N\sigma_{0}^{2} + \sigma^{2}} \mu_{0}\)<br> where \(\sigma_{N}^{2} = \frac{\sigma_{0}^{2} \sigma^{2}}{N\sigma_{0}^{2} + \sigma^{2}}\)</li> </ul> </li> </ul> </li> </ul> <h3 id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h3> <ul> <li>dimensionality reduction w/o losing much info.</li> </ul> <p>28p TBD</p> <h3 id="singular-value-decomposition-svd">Singular Value Decomposition (SVD)</h3> <ul> <li>쓰임 : matrix factorization <ul> <li>low-rank approximation of matrix</li> <li>pseudo-inverse of non-square matrix</li> </ul> </li> <li>Singular Value Decomposition (SVD) :<br> \(A = U \Sigma V^T = \begin{bmatrix} u_1 &amp; u_2 &amp; \cdots &amp; u_m \end{bmatrix} \begin{bmatrix} \sigma_{1} &amp; \cdots &amp; 0 &amp; \cdots &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; \sigma_{r} &amp; \cdots &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; 0 &amp; \cdots &amp; \sigma_{m} &amp; 0 &amp; \cdots &amp; 0 \end{bmatrix} \begin{bmatrix} v_1^{T} \\ v_2^{T} \\ \vdots \\ v_n^{T} \end{bmatrix}\)<br> where \(U\) is \(m \times m\) orthonormal matrix (\(UU^T = I\))<br> where \(V\) is \(n \times n\) orthonormal matrix (\(VV^T = I\))<br> where \(\Sigma\) is \(m \times n\) diagonal matrix (\(\sigma_{1} \geq \sigma_{2} \geq \cdots \geq \sigma_{m} \geq 0\)) <ul> <li>\(AA^T = U \Sigma \Sigma^{T} U^T\)<br> \(U\) 는 \(AA^T\) 의 eigenvector matrix<br> \(\sigma_{i} = \sqrt{\lambda_{i}}\) where \(\lambda_{i}\) 는 \(AA^T\) 의 eigenvalue</li> <li>\(A^TA = V \Sigma^{T} \Sigma V^T\)<br> \(V\) 는 \(A^TA\) 의 eigenvector matrix<br> \(\sigma_{i} = \sqrt{\lambda_{i}}\) where \(\lambda_{i}\) 는 \(A^TA\) 의 eigenvalue</li> </ul> </li> <li>rank and span :<br> \(r = \text{rank}(A) = \text{rank}(\Sigma) \leq \text{min}(m, n)\) 에 대해<br> (\(\sigma_{1} \geq \sigma_{2} \geq \cdots \geq \sigma_{r} \geq \sigma_{r+1} = \cdots = \sigma_{m} = 0\)) <ul> <li>\(\text{col}(A)\) is spanned by \(\begin{bmatrix} u_1, \cdots, u_r \end{bmatrix}\)<br> \(AA^Tu_i = \sigma_{i}^{2} u_i\) for \(i = 1, \cdots, r\)</li> <li>\(\text{null}(A^T)\) is spanned by \(\begin{bmatrix} u_{r+1}, \cdots, u_m \end{bmatrix}\)<br> \(AA^Tu_i = 0\) for \(i = r+1, \cdots, m\), 즉 \(A^T u_i = 0\)</li> <li>\(\text{row}(A) = \text{col}(A^T)\) is spanned by \(\begin{bmatrix} v_{1}^{T}, \cdots, v_{r}^{T} \end{bmatrix}\)<br> \(A^TAv_i = \sigma_{i}^{2} v_i\) for \(i = 1, \cdots, r\)</li> <li>\(\text{null}(A)\) is spanned by \(\begin{bmatrix} v_{r+1}^{T}, \cdots, v_{n}^{T} \end{bmatrix}\)<br> \(A^TAv_i = 0\) for \(i = r+1, \cdots, n\), 즉 \(A v_i = 0\)</li> <li>\(A = U \Sigma V^T\) \(\rightarrow\) \(A V = U \Sigma\)<br> \(A v_i = \sigma_{i} u_i\) for \(i = 1, \cdots, r\)<br> \(A v_i = 0\) for \(i = r+1, \cdots, n\)</li> </ul> </li> <li> <p>SVD (rank에 맞게 줄인 버전) :<br> \(A = \begin{bmatrix} u_1 &amp; u_2 &amp; \cdots &amp; u_r \end{bmatrix} \begin{bmatrix} \sigma_{1} &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \sigma_{2} &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; 0 &amp; \sigma_{r} \end{bmatrix} \begin{bmatrix} v_1^{T} \\ v_2^{T} \\ \vdots \\ v_r^{T} \end{bmatrix}\)<br> \(= \sum_{i=1}^{r} \sigma_{i} u_i v_{i}^{T}\)</p> </li> <li> <p>Eigen Decomposition :<br> matrix \(A\) 가 square and real and symmetric일 경우<br> \(A P = P D\)<br> where \(P\) is orthonormal eigenvector matrix (\(P^{-1} = P^T\) and \(PP^T = I\))<br> where \(D\) is diagonal eigenvalue matrix</p> </li> <li>SVD 계산 과정 Summary : <ul> <li>Step 1)<br> \(A^TA\) 를 Eigen Decomposition해서,<br> eigenvector matrix \(V\) 와 eigenvalue matrix \(\Sigma\) 를 구함<br> 이 때, \(\Sigma\) 의 singular value는 \(A^TA\) 의 eigenvalue에 \(\sqrt{\cdot}\) 씌워서 구함</li> <li>Step 2)<br> eigenvector matrix \(V\) 의 columns를 normalize하고<br> \(V^TV=I\) 맞는지 확인</li> <li>Step 3)<br> \(Av_i = \sigma_{i} u_i\), 즉 \(u_i = \sigma_{i}^{-1} A v_i\) 으로<br> eigenvector \(u_i\) 구하고<br> normalize해서 \(UU^T = I\) 맞는지 확인</li> <li>Step 4)<br> \(A = U \Sigma V^T\) 맞는지 최종 확인</li> </ul> </li> <li>Relation b.w. PCA and SVD :<br> matrix \(A\) 의 columns가 zero-mean centered일 때 <ul> <li>PCA :<br> covariance matrix \(C = E[AA^T] = \frac{1}{N-1}AA^T\)의 eigenvectors를 구한 뒤<br> eigenvalue 큰 순으로 잘라서 principal eigenvectors의 합으로 표현</li> <li>SVD :<br> \(AA^T = U \Sigma^{2} U^T = V \Sigma^{2} V^T\) (\(U = V\) since \(AA^T\) is square matrix) 이므로<br> \(AA^T\)의 eigenvector matrix \(V\) 를 구한 뒤<br> rank \(r \leq \text{min}(m, n)\)에 맞게 줄여서 \(A = U \Sigma V^T = \sum_{i=1}^{r} \sigma_{i} u_i v_{i}^{T}\) 로 표현</li> <li>Relation :<br> 둘은 mathematically 동일!<br> \(C = V \frac{\Sigma^{2}}{N-1} V^T\)</li> </ul> </li> </ul> <h2 id="chapter-4-non-parametric-techniques">Chapter 4. Non-parametric Techniques</h2> <ul> <li>parametric approach :<br> pdf가 어떤 form인지 미리 알아야 함<br> e.g. \(N(\mu, \Sigma)\) 의 \(\mu\) 를 estimate<br> 하지만 실제로는 pdf prior form 모를 때가 많다 <ul> <li>해결 방법 1)<br> Non-parametric approach로 samples로부터 pdf (density \(\hat p(x) \approx p(x)\)) 를 직접 estimate<br> e.g. Parzen window<br> e.g. k-NN</li> <li>해결 방법 2)<br> 아예 posterior를 직접 estimate<br> e.g. Direct Decision Rule - Nearest-Neighbor</li> </ul> </li> </ul> <h3 id="density-estimation">Density Estimation</h3> <ul> <li>\(P(x \in R) = \int_{R} p(x)dx\) :<br> pdf \(p(x)\) 를 안다면 위의 식으로 a sample \(x\) 가 region \(R\) 안에 속할 확률을 구할 수 있다<br> But, pdf 모를 때는? <ul> <li>\(P \approx \frac{k}{n}\) <br> where \(n\) 개의 samples 중 region \(R\) 안에 속하는 samples가 \(k\)개</li> <li>For very small region \(R\),<br> \(P(x \in R) \approx \hat p(x) \cdot V\)<br> where V is volume enclosed by \(R\)</li> <li>즉, \(\hat p(x) = \frac{k/n}{V}\) is pdf estimator of \(p(x)\) <ul> <li>case 1) fixed V (volume of region \(R\) is fixed)<br> sample 수 많아지면 \(\text{lim}_{n \rightarrow \infty} k/n = P\) 로 수렴<br> So, \(\hat p(x)\) is averaged ver. of \(p(x)\)</li> <li>case 2) \(V \rightarrow 0\) (volume of region \(R\) shrinks to 0)<br> region \(R\)의 volume이 매우 작으므로 \(k \rightarrow 0\)<br> So, \(p(x)\) 는 zero에 가깝고, \(\hat p(x)\) 는 very noisy</li> <li>case 3) 실제 상황<br> sample 수 \(n\) is limited<br> volume \(V\) 는 arbitrarily small일 수 없음<br> 따라서 samples에 따라 \(\frac{k}{n}\)과 averaging by \(V\) 에 어느 정도 variance가 있음</li> </ul> </li> <li>\(\hat p(x)\) 가 \(p(x)\) 로 converge하려면 아래의 세 가지 조건 만족해야 함 <ul> <li>\(\text{lim}_{n \rightarrow \infty} V = 0\)<br> no averaging in the limit</li> <li>If \(V\) is fixed, \(\text{lim}_{n \rightarrow \infty} k = \infty\)<br> 그래야 \(\frac{k/n}{V}\) 가 probability \(p(x)\) 로 수렴<br> 만약 \(\text{lim}_{n \rightarrow \infty} k = c\) 라면 \(\text{lim}_{n \rightarrow \infty} \hat p(x) = 0\)</li> <li>\(\text{lim}_{n \rightarrow \infty} \frac{k}{n} = 0\)<br> So, \(\text{lim}_{n \rightarrow \infty} \hat p(x) = \text{lim}_{n \rightarrow \infty} \frac{k/n}{V}\) is density function</li> </ul> </li> <li>\(\text{lim}_{n \rightarrow \infty} \hat p(x) = p(x)\) 위해 <ul> <li>e.g. \(V = \frac{V_0}{\sqrt{n}}\) (Parzen window method)<br> so that \(\text{lim}_{n \rightarrow \infty} V = 0\)</li> <li>e.g. \(k = \sqrt{n}\)<br> so that \(\text{lim}_{n \rightarrow \infty} k = \infty\) and \(\text{lim}_{n \rightarrow \infty} \frac{k}{n} = 0\) and \(\text{lim}_{n \rightarrow \infty} V = \text{lim}_{n \rightarrow \infty} \frac{\sqrt{n}}{n \hat p(x)} = 0\)</li> </ul> </li> </ul> </li> </ul> <h3 id="density-estimation---parzen-window">Density Estimation - Parzen window</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/10-480.webp 480w,/assets/img/2024-09-10-Pattern/10-800.webp 800w,/assets/img/2024-09-10-Pattern/10-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/9-480.webp 480w,/assets/img/2024-09-10-Pattern/9-800.webp 800w,/assets/img/2024-09-10-Pattern/9-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <p>Let’s define Parzen window as <code class="language-plaintext highlighter-rouge">unit hypercube</code><br> \(\phi (u) = \begin{cases} 1 &amp; \text{if} &amp; | u_i | \leq \frac{1}{2} \\ 0 &amp; \text{O.W.} \end{cases}\) for \(i = 1, \ldots, d\)</p> </li> <li>\(\phi (u)\) 는 indicator function처럼 쓰여서<br> \(x\) 를 중심으로 하고 \((h_n)^d\) 의 범위를 갖는 cube 안에 들어오는 sample 개수 \(k\) 를 세는 데 사용 <ul> <li>\(k = \sum_{i=1}^n \phi (\frac{x - x_i}{h_n})\) and \(V = (h_n)^d\)<br> \(\rightarrow\)<br> \(\hat p(x) = \frac{k/n}{V} = \frac{1}{n} \sum_{i=1}^n \frac{\phi (\frac{x - x_i}{h_n})}{(h_n)^d}\) :<br> interpolated function at position \(x\) from samples \(x_i\)</li> </ul> </li> <li>Let \(\delta_{n} (x) = \frac{1}{V} \phi (\frac{x}{h_n})\)<br> Then \(\hat p(x) = \frac{1}{n} \sum_{i=1}^n \delta_{n} (x - x_i)\) <ul> <li>\(h_n\) (\(V\)) 이 클 경우 :<br> \(\delta_{n}(x)\) 의 variance가 커서<br> 이를 합친 \(\hat p(x) = \frac{1}{n} \sum_{i=1}^n \delta_{n} (x - x_i)\) 는 오히려 smoothed ver. of \(p(x)\) at \(x\)<br> (too little resolution)</li> <li>\(h_n\) (\(V\)) 이 작을 경우 :<br> \(\delta_{n}(x)\) 의 variance가 작아서<br> 이를 합친 \(\hat p(x) = \frac{1}{n} \sum_{i=1}^n \delta_{n} (x - x_i)\) 는 noisy estimate of \(p(x)\) (variation이 큼)<br> (too much statistical variation)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/11-480.webp 480w,/assets/img/2024-09-10-Pattern/11-800.webp 800w,/assets/img/2024-09-10-Pattern/11-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>만약 \(h_n \rightarrow 0\) (\(\text{lim}_{n \rightarrow \infty} V = 0\)) 이라면 <ul> <li>\(\text{lim}_{n \rightarrow \infty} \delta_{n} (x) = \delta (x)\) (Dirac delta func.)</li> <li>\(\text{lim}_{n \rightarrow \infty} E[\hat p(x)] = \text{lim}_{n \rightarrow \infty} \frac{1}{n} \sum_{i=1}^n E_{x_i}[\frac{1}{V} \phi (\frac{x - x_i}{h_n})]\)<br> \(= \text{lim}_{n \rightarrow \infty} \frac{1}{n} \cdot n \cdot \int \frac{1}{V} \phi (\frac{x - s}{h_n}) p(s) ds\)<br> \(= \text{lim}_{n \rightarrow \infty} \frac{1}{V} \phi(\frac{x}{h_n}) \ast p(x)\) by definition of convolution<br> \(= \text{lim}_{n \rightarrow \infty} \delta_{n}(x) \ast p(x)\)<br> \(= \delta (x) \ast p(x)\)<br> \(= p(x)\)</li> </ul> </li> </ul> <h3 id="density-estimation---knn-method">Density Estimation - kNN method</h3> <ul> <li>고정된 \(k_n\) 값에 대해<br> \(k_n\) nearest neighbors 찾을 때까지 \(V_n\) expand<br> \(\rightarrow\)<br> training samples가 sparse한 곳에서 \(\hat p(x) \rightarrow 0\) 인 걸 방지</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/12-480.webp 480w,/assets/img/2024-09-10-Pattern/12-800.webp 800w,/assets/img/2024-09-10-Pattern/12-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>k-NN estimation :<br> probability \(\frac{k}{n}\) 은 고정하고<br> \(k\)-개의 sample이 들어 있는 volume \(V\) 의 크기를 통해 density estimation</li> </ul> <h3 id="classification-based-on-parzen-window-and-k-nn">Classification based on Parzen-window and k-NN</h3> <ul> <li>classification based on Parzen-window method : <ul> <li>density estimate<br> \(\hat p(x) = \frac{1}{n} \sum_{i=1}^n (\frac{1}{V_n} \phi (\frac{x-x_i}{h_n}))\)</li> <li>classification<br> choose \(w_1\)<br> if \(\frac{\hat p(x | w_1)}{\hat p(x | w_2)} = \frac{\frac{1}{n_1} \sum_{i=1}^{n_1} (\frac{1}{V_{n_1}} \phi (\frac{x-x_i}{h_n}))}{\frac{1}{n_2} \sum_{i=1}^{n_2} (\frac{1}{V_{n_2}} \phi (\frac{x-x_i}{h_n}))} \gt \frac{(\lambda_{12} - \lambda_{22})P(w_2)}{(\lambda_{21} - \lambda_{11})P(w_1)}\)</li> </ul> </li> <li>classification based on k-NN method : <ul> <li>density estimate<br> \(\hat p(x) = \frac{k_n / n}{V_n}\)</li> <li>classification<br> choose \(w_1\)<br> if \(\frac{\hat p(x | w_1)}{\hat p(x | w_2)} = \frac{\frac{k_1 / n_1}{V_{n_1}}}{\frac{k_2 / n_2}{V_{n_2}}} \gt \frac{(\lambda_{12} - \lambda_{22})P(w_2)}{(\lambda_{21} - \lambda_{11})P(w_1)}\)<br> if \(\frac{V_{n_2}}{V_{n_1}} \gt \frac{n_1(\lambda_{12} - \lambda_{22})P(w_2)}{n_2(\lambda_{21} - \lambda_{11})P(w_1)}\)<br> (\(k_1 = k_2\) is fixed for k-NN)</li> </ul> </li> </ul> <h3 id="direct-estimation-of-posteriori">Direct Estimation of Posteriori</h3> <ul> <li>NNR (Nearest Neighbor Rule) : <ul> <li>Step 1)<br> estimate posteriori \(P(w_i | x)\) directly from training set</li> <li>Step 2)<br> classification based on estimated \(P(w_i | x)\)</li> </ul> </li> </ul> <p>22p</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="disqus_thread" style="max-width: 800px; margin: 0 auto;"> <script type="text/javascript">var disqus_shortname="semyeong-yu",disqus_identifier="/blog/2024/Pattern",disqus_title="EE534 Pattern Recognition";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Semyeong Yu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5YMLX9VHFX"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5YMLX9VHFX");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>