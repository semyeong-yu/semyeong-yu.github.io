<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> EE534 Pattern Recognition Midterm | Semyeong Yu </title> <meta name="author" content="Semyeong Yu"> <meta name="description" content="Lecture Summary (24F)"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://semyeong-yu.github.io/blog/2024/Pattern/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "EE534 Pattern Recognition Midterm",
            "description": "Lecture Summary (24F)",
            "published": "September 10, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Semyeong</span> Yu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>EE534 Pattern Recognition Midterm</h1> <p>Lecture Summary (24F)</p> </d-title> <d-article> <blockquote> <p>Lecture :<br> 24F EE534 Pattern Recognition<br> by KAIST Munchurl Kim <a href="https://www.viclab.kaist.ac.kr/" rel="external nofollow noopener" target="_blank">VICLab</a></p> </blockquote> <h2 id="chapter-1-overview">Chapter 1. Overview</h2> <h3 id="discriminative-vs-generative">Discriminative vs Generative</h3> <ul> <li>Discriminative model : <ul> <li>learn posterior directly</li> <li>e.g. logistic regression, SVM, nearest neighbor, CRF, Decision Tree and Random Forest, traditional NN</li> </ul> </li> <li>Generative model : <ul> <li>learn likelihood and prior to maximize posterior</li> <li>e.g. Bayesian network, Autoregressive model, GAN, Diffuson model</li> </ul> </li> </ul> <h2 id="chapter-2-bayes-decision-theory">Chapter 2. Bayes Decision Theory</h2> <h3 id="bayes-decision-rule">Bayes Decision Rule</h3> <ul> <li>conditional probability density :<br> Let \(w\) be state (class)<br> Let \(x\) be data (continous-valued sample) <ul> <li>prior : \(P(w=w_k)\)</li> <li> <table> <tbody> <tr> <td>likelihood : PDF $$P(x</td> <td>w_k)$$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>posterior : $$P(w_k</td> <td>x) = \frac{P(x</td> <td>w_k)P(w_k)}{P(x)}$$ (Bayes Rule)</td> <td> </td> </tr> <tr> <td>where $$P(w_1</td> <td>x) + P(w_2</td> <td>x) + \cdots + P(w_N</td> <td>x) = 1$$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>evidence : $$P(x) = \sum_{k=1}^N P(x</td> <td>w_k)P(w_k) = \sum_{k=1}^N P(x, w_k)$$</td> </tr> </tbody> </table> </li> </ul> </li> <li>Bayes Decision Rule :<br> posterior 더 큰 쪽 고름! <ul> <li>Two-class (\(w_1, w_2\)) problem :<br> choose \(w_1\)<br> if \(P(w_1 | x) \gt P(w_2 | x)\)<br> if \(P(x|w_1)P(w_1) \gt P(x|w_2)P(w_2)\)<br> if \(\frac{P(x|w_1)}{P(x|w_2)} \gt \frac{P(w_2)}{P(w_1)}\)<br> (likehood ratio \(\gt\) threshold)</li> <li>multi-class problem :<br> choose \(w_i\) where \(P(w_i | x)\) is the largest</li> </ul> </li> </ul> <h3 id="minimum-error">minimum error</h3> <ul> <li>minimum error :<br> GT가 \(w_1, w_2\) 이고, Predicted가 \(R_1, R_2\) 일 때, <ul> <li>\(P(error) = \int_{-\infty}^{\infty} P(error, x)dx = \int_{-\infty}^{\infty} P(error|x)P(x)dx\)<br> \(= \int_{R_2}P(w_1|x)P(x)dx + \int_{R_1}P(w_2|x)P(x)dx\)<br> \(= \int_{R_2}P(x|w_1)P(w_1)dx + \int_{R_1}P(x|w_2)P(w_2)dx\)<br> \(= \begin{cases} A+B+D &amp; \text{if} &amp; x_B \\ A+B+C+D &amp; \text{if} &amp; x^{\ast} \end{cases}\)<br> where \(A+B+D\) is minimum error and \(C\) is reducible error<br> (아래 그림 참고)</li> <li>\(P(correct)\)<br> \(= \int \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_1}P(x|w_1)P(w_1)dx + \int_{R_2}P(x|w_2)P(w_2)dx\)</li> <li>\(P(error) = 1 - P(correct)\)<br> \(= 1 - \int \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_2}P(x|w_1)P(w_1)dx + \int_{R_1}P(x|w_2)P(w_2)dx\)<br> since \(\int_{-\infty}^{\infty} P(x|w_1)P(w_1)dx + \int_{-\infty}^{\infty} P(x|w_2)P(w_2)dx = 1\)<br> (\(\int_{-\infty}^{\infty} P(x|w_1)P(w_1)dx \neq 1\) 임에 주의!)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/1-480.webp 480w,/assets/img/2024-09-10-Pattern/1-800.webp 800w,/assets/img/2024-09-10-Pattern/1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>minimum error with rejection :<br> decision이 확실하지 않을 때는 classification 자체를 reject하는 게 적절<br> (classification error도 줄어들고, correct classification도 줄어듬) <ul> <li>feature space \(x\) 를 rejection region \(R\) 과 acceptance region \(A\) 으로 나눠서<br> rejection region \(R=\{ x | \text{max}_{i} P(w_i | x) \leq 1 - t\}\) 에서는 reject decision<br> acceptance region \(A=\{ x | \text{max}_{i} P(w_i | x) \gt 1 - t\}\) 에서는 \(w_1\) or \(w_2\) 로 classification decision 수행</li> <li>\(P_c(t) = P(correct)\)<br> \(= \int_{A} \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_1}P(x|w_1)P(w_1)dx + \int_{R_2}P(x|w_2)P(w_2)dx\)</li> <li>\(P_r(t) = P(reject)\)<br> \(= \int_{R}P(x|w_1)P(w_1)dx + \int_{R}P(x|w_2)P(w_2)dx\)<br> \(= \int_{R} P(x)dx\)</li> <li>\(P_e(t) = P(error)\)<br> \(= P(error, w_1) + P(error, w_2)\)<br> \(= 1 - P_r(t) - P_c(t)\) by 아래 식 대입<br> where \(P(error, w_1) = \int_{-\infty}^{\infty} P(x|w_1)P(w_1)dx - P(reject, w_1) - P(correct, w_1)\)<br> where \(P(error, w_2) = \int_{-\infty}^{\infty} P(x|w_2)P(w_2)dx - P(reject, w_2) - P(correct, w_2)\)<br> where \(\int_{-\infty}^{\infty} P(x|w_1)P(w_1)dx + \int_{-\infty}^{\infty} P(x|w_2)P(w_2)dx = \int_{-\infty}^{\infty} P(x)dx = 1\)</li> </ul> </li> <li>Summary : <ul> <li> <table> <tbody> <tr> <td>$$P(w_{i}</td> <td>x)$$ : rejection/acceptance region 구하는 데 사용</td> </tr> </tbody> </table> </li> <li>\(P(x|w_i)P(w_i)\) : \(P(correct, w_i), P(reject, w_i), P(error, w_i)\) 구해서<br> \(P_c(t), P_r(t), P_e(t)\) 구하는 데 사용</li> <li> \[P_c(t) + P_r(t) + P_e(t) = 1\] </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/2-480.webp 480w,/assets/img/2024-09-10-Pattern/2-800.webp 800w,/assets/img/2024-09-10-Pattern/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/3-480.webp 480w,/assets/img/2024-09-10-Pattern/3-800.webp 800w,/assets/img/2024-09-10-Pattern/3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="bayes-decision-rule-w-bayes-risk">Bayes Decision Rule w. Bayes risk</h3> <ul> <li>Bayes risk (minimum overall risk) :<br> \(\Omega = \{ w_1, \cdots w_c \}\) 에서 \(w_j\) 는 \(j\) -th class<br> \(A = \{ \alpha_{1}, \cdots, \alpha_{c} \}\) 에서 \(\alpha_{i}\) 는 class \(w_i\) 라고 예측하는 action<br> \(\lambda(\alpha_{i} | w_j) = \lambda_{ij}\) : class \(w_j\) 가 GT일 때, class \(w_i\) 로 pred. 했을 때의 loss <ul> <li>conditional risk for taking action \(\alpha_{i}\) :<br> 특정 input \(x\) 에 대해<br> \(R(\alpha_{i}|x) = \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j|x)\)</li> <li>overall risk for taking action \(\alpha_{i}\) :<br> 모든 input \(x\) 에 대해 적분<br> \(R(\alpha_{i}) = \int R(\alpha_{i}|x)P(x)dx\)<br> \(= \int \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j|x) P(x)dx\)<br> \(= \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j) \int P(x|w_j)dx\)<br> \(= \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j)\)<br> \(= \sum_{j=1}^c \lambda_{ij}P(w_j)\)<br> where pdf(likelihood) 합 \(\int P(x|w_j)dx = 1\)</li> <li>모든 input \(x\) 에 대해 가장 loss가 최소인 class \(w_i\) 로 예측하면,<br> minimum overall risk (= Bayes risk) 를 가짐</li> </ul> </li> <li>Bayes Decision Rule for Bayes risk : <ul> <li>Two-class (\(w_1, w_2\)) problem :<br> choose \(w_1\)<br> if \(R(\alpha_{1} | x) \lt R(\alpha_{2} | x)\)<br> if \(\lambda_{11}P(w_1 | x) + \lambda_{12}P(w_2 | x) \lt \lambda_{21}P(w_1 | x) + \lambda_{22}P(w_2 | x)\)<br> if \((\lambda_{21} - \lambda_{11})P(w_1 | x) \gt (\lambda_{12} - \lambda_{22})P(w_2 | x)\)<br> if \(\frac{P(x | w_1)}{P(x | w_2)} \gt \frac{(\lambda_{12} - \lambda_{22})P(w_2)}{(\lambda_{21} - \lambda_{11})P(w_1)}\)<br> if \(\frac{P(x | w_1)}{P(x | w_2)} \gt \frac{P(w_2)}{P(w_1)}\) for \(\lambda_{11}=\lambda_{22}=0\) and \(\lambda_{12}=\lambda_{21}\)<br> (likehood ratio \(\gt\) threshold) (위의 Bayes Decision Rule에서 구한 식과 same)</li> <li>loss가 \(\lambda (\alpha_{i}|w_{j}) = \begin{cases} 0 &amp; \text{if} &amp; i = j &amp; (\text{no penalty}) \\ 1 &amp; \text{if} &amp; i \neq j &amp; (\text{equal penalty}) \end{cases}\) 일 경우<br> conditional risk :<br> \(R(\alpha_{i} | x) = \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j|x) = \sum_{j=1, j \neq i}^c P(w_j|x) = 1 - P(w_i | x)\) 이므로<br> Bayes Decision Rule에서 conditional risk \(R(\alpha_{i} | x)\) 최소화는 posterior \(P(w_i | x)\) 최대화와 같음</li> <li>multi-class problem :<br> classifieer (discriminant function) (space-partitioning function) \(g(x)\) 에 대해<br> choose \(w_i\) where \(g_{i}(x)\) is the largest<br> s.t. decision boundary is \(g_{i}(x) = g_{j}(x)\) where they are the two largest discriminant functions<br> e.g. Bayes classifier : \(g_{i}(x) = - R(\alpha_{i} | x)\) or \(g_{i}(x) = P(w_i | x)\) or \(g_{i}(x) = P(x | w_i)P(w_i)\) or \(g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i)\)</li> </ul> </li> </ul> <h3 id="discriminant-function-for-gaussian-pdf">Discriminant Function for Gaussian PDF</h3> <ul> <li> <p>\(G(\boldsymbol x) = \frac{1}{(2\pi)^{\frac{d}{2}} | \Sigma |^{\frac{1}{2}}}e^{-\frac{1}{2}(\boldsymbol x - \boldsymbol \mu)^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu)}\)<br> where \(d \times d\) covariance \(\Sigma = E[(\boldsymbol x - \boldsymbol \mu)(\boldsymbol x - \boldsymbol \mu)^T] = E[\boldsymbol x \boldsymbol x^{T}] - \boldsymbol \mu \boldsymbol \mu^{T} = S - \boldsymbol \mu \boldsymbol \mu^{T}\)<br> where \(S = E[\boldsymbol x \boldsymbol x^{T}]\) : standard autocorrelation matrix</p> </li> <li> <p>Discriminant function for Gaussian PDF :<br> likelihood \(P(x | w_i)\) 를 Gaussian PDF로 둘 경우,<br> \(g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{d}{2} \text{ln}(2\pi) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\)</p> <ul> <li>case 1) \(\Sigma_{i} = \sigma^{2} \boldsymbol I\) (모든 classes에 대해 equal covariance) (등방성(sphere)) <br> \(g_{i}(x) = -\frac{\| \boldsymbol x - \boldsymbol \mu_{i} \|^2}{2 \sigma^{2}} + \text{ln}P(w_i)\)<br> \(i\) 와 관련된 term만 남기면<br> \(g_{i}(x) = \frac{1}{\sigma^{2}} \boldsymbol \mu_{i}^T \boldsymbol x - \frac{1}{2\sigma^{2}} \boldsymbol \mu_{i}^T \boldsymbol \mu_{i} + \text{ln}P(w_i)\)<br> \(= \boldsymbol w_i^T \boldsymbol x + \boldsymbol w_{i0}\) (linear) <ul> <li>decision boundary :<br> hyperplane \(g(x) = g_{i}(x) - g_{j}(x) = (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T(\boldsymbol x - \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) + \frac{\sigma^{2}}{(\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T} \text{ln}\frac{P(w_i)}{P(w_j)})\)<br> \(= \boldsymbol w^T (\boldsymbol x - \boldsymbol x_0) = 0\)</li> <li>\(\boldsymbol x_0\) 를 지나고 \(\boldsymbol w = \boldsymbol \mu_{i} - \boldsymbol \mu_{j}\) 에 수직인 hyperplane</li> <li>\(\boldsymbol x_0 = \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) - \frac{\sigma^{2}}{\| \boldsymbol \mu_{i} - \boldsymbol \mu_{j} \|^2} \text{ln}\frac{P(w_i)}{P(w_j)} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})\) 이므로<br> \(\boldsymbol x_0\) 의 위치는 \(\boldsymbol \mu_{i}\) 와 \(\boldsymbol \mu_{j}\) 의 중점에서 \(\begin{cases} \boldsymbol \mu_{j} \text{쪽으로 이동} &amp; \text{if} &amp; P(w_i) \gt P(w_j) \\ \boldsymbol \mu_{i} \text{쪽으로 이동} &amp; \text{if} &amp; P(w_i) \lt P(w_j) \end{cases}\)<br> (\(P(w_i)\) 와 \(P(w_j)\) 중 더 작은 쪽으로 이동)<br> (\(\sigma^{2}\) 이 (\(\| \mu_{i} - \mu_{j} \|^2\) 에 비해 비교적) 작은 경우 \(P(w_i)\) 와 \(P(w_j)\) 에 따른 \(x_0\) shift는 미약)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/4-480.webp 480w,/assets/img/2024-09-10-Pattern/4-800.webp 800w,/assets/img/2024-09-10-Pattern/4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Discriminant function for Gaussian PDF :<br> likelihood \(P(x | w_i)\) 를 Gaussian PDF로 둘 경우,<br> \(g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{d}{2} \text{ln}(2\pi) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\) <ul> <li>case 2) \(\Sigma_{i} = \Sigma\) (symmetric) (모든 classes에 대해 equal covariance) (비등방성(hyper-ellipsoidal))<br> \(g_{i}(x) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) + \text{ln}P(w_i)\)<br> \(i\) 와 관련된 term만 남기면<br> \(g_{i}(x) = \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol x - \frac{1}{2} \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol \mu_{i} + \text{ln}P(w_i)\)<br> \(= \boldsymbol w_i^T \boldsymbol x + \boldsymbol w_{i0}\) (linear) <ul> <li>decision boundary :<br> hyperplane \(g(x) = g_{i}(x) - g_{j}(x) = (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T \Sigma^{-1} (\boldsymbol x - \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) + \frac{1}{(\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T \Sigma^{-1}} \text{ln}\frac{P(w_i)}{P(w_j)})\)<br> \(= \boldsymbol w^T (\boldsymbol x - \boldsymbol x_0) = 0\)</li> <li>\(\boldsymbol x_0\) 를 지나고 \(\boldsymbol w = \Sigma^{-1} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})\) 에 수직인 hyperplane</li> <li>\(\boldsymbol x_0 = \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) - \frac{\text{ln}\frac{P(w_i)}{P(w_j)}}{(\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T \Sigma^{-1} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})\) 이므로<br> 마찬가지로 \(\boldsymbol x_0\) 의 위치는 \(\boldsymbol \mu_{i}\) 와 \(\boldsymbol \mu_{j}\) 의 중점에서 \(P(w_i)\) 와 \(P(w_j)\) 중 더 작은 쪽으로 이동</li> <li>\(\boldsymbol w = \Sigma^{-1} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})\) 는<br> vector \(\boldsymbol \mu_{i} - \boldsymbol \mu_{j}\) 를 \(\Sigma^{-1}\) 로 회전시킨 vector를 의미</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/5-480.webp 480w,/assets/img/2024-09-10-Pattern/5-800.webp 800w,/assets/img/2024-09-10-Pattern/5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Discriminant function for Gaussian PDF :<br> likelihood \(P(x | w_i)\) 를 Gaussian PDF로 둘 경우,<br> \(g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{d}{2} \text{ln}(2\pi) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\) <ul> <li>case 3) \(\Sigma_{i}\) is arbitrary (symmetric) (class마다 covariance 다름) (비등방성(hyper-ellipsoidal))<br> \(g_{i}(x) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\)<br> \(\Sigma_{i}\) 가 \(i\) 에 대한 term이므로 \(- \frac{1}{2} \boldsymbol x^T \Sigma_{i}^{-1} \boldsymbol x\) term이 안 지워져서<br> \(g_{i}(x) = - \frac{1}{2} \boldsymbol x^T \Sigma_{i}^{-1} \boldsymbol x + \boldsymbol \mu_{i}^T \Sigma_{i}^{-1} \boldsymbol x - \frac{1}{2} \boldsymbol \mu_{i}^T \Sigma_{i}^{-1} \boldsymbol \mu_{i} - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\)<br> \(= - \frac{1}{2} \boldsymbol x^T \Sigma_{i}^{-1} \boldsymbol x + \boldsymbol w_i^T \boldsymbol x + \boldsymbol w_{i0}\) (quadratic) 는<br> quadratic discriminant function in \(x\) <ul> <li>decision surface :<br> hyperquadratic (hyperplane, hypersphere, hyperellipsoidal, hyperparaboloid, hyperhyperboloid)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/6-480.webp 480w,/assets/img/2024-09-10-Pattern/6-800.webp 800w,/assets/img/2024-09-10-Pattern/6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="bayes-rule-for-discrete-case">Bayes Rule for Discrete Case</h3> <ul> <li> <table> <tbody> <tr> <td>pdf 적분 $$\int p(x</td> <td>w_{j}) dx$$ 대신</td> <td> </td> </tr> <tr> <td>확률 합 $$\text{lim}<em>{\Delta x \rightarrow 0} \Sigma</em>{k=-\infty}^{\infty} p(x_k</td> <td>w_j) \Delta x \rightarrow \Sigma_{k=1}^m P(v_k</td> <td>w_j)$$</td> </tr> </tbody> </table> </li> <li> <p>Bayes Decision Rule은 discrete case에서도 same<br> Bayes risk minimize 위해 conditional risk \(R(\alpha_{i} | x)\) minimize<br> (posterior maximize와 same)</p> </li> <li>\(\boldsymbol x = [x_1, x_2, \ldots, x_d]^T\) 에서 \(x_i\) 가 0 혹은 1의 값을 갖는 Bernoulli random var.일 때 <ul> <li>class \(w_1\) 일 때 :<br> \(x_i \sim p_i^{x_i}(1-p_i)^{1-x_i}\)<br> \(P(\boldsymbol x | w_1) = P([x_1, x_2, \ldots, x_d]^T | w_1) = \prod_{i=1}^d P(x_i | w_1) = \prod_{i=1}^d p_i^{x_i}(1-p_i)^{1-x_i}\)</li> <li>class \(w_2\) 일 때 :<br> \(x_i \sim q_i^{x_i}(1-q_i)^{1-x_i}\)<br> \(P(\boldsymbol x | w_2) = P([x_1, x_2, \ldots, x_d]^T | w_2) = \prod_{i=1}^d P(x_i | w_2) = \prod_{i=1}^d q_i^{x_i}(1-q_i)^{1-x_i}\)</li> <li>likelihood ratio :<br> \(\frac{P(\boldsymbol x | w_1)}{P(\boldsymbol x | w_2)} = \prod_{i=1}^d (\frac{p_i}{q_i})^{x_i}(\frac{1-p_i}{1-q_i})^{1-x_i}\)</li> <li>discriminant function :<br> choose \(w_1\)<br> if \(g(x) = \text{ln} \frac{P(\boldsymbol x | w_1)P(w_1)}{P(\boldsymbol x | w_2)P(w_2)} = \sum_{i=1}^d(x_i \text{ln}\frac{p_i}{q_i} + (1-x_i)\text{ln}\frac{1-p_i}{1-q_i}) + \text{ln}\frac{P(w_1)}{P(w_2)} = \sum_{i=1}^d w_ix_i + w_0 = \boldsymbol w^T \boldsymbol x + w_0 \gt 0\)<br> where \(w_i = \text{ln}\frac{p_i(1-q_i)}{q_i(1-p_i)}\) and \(w_0 = \sum_{i=1}^d(\text{ln}\frac{1-p_i}{1-q_i}) + \text{ln}\frac{P(w_1)}{P(w_2)}\) <ul> <li>case 1-1) \(p_i = q_i\)<br> \(w_i = 0\) , so \(x_i\) 는 class 결정에 영향 없음</li> <li>case 1-2) \(p_i \gt q_i\)<br> \(w_i \gt 0\) , so \(x_i = 1\) 은 class \(w_1\) 선택에 보탬</li> <li>case 1-3) \(p_i \lt q_i\)<br> \(w_i \lt 0\) , so \(x_i = 1\) 은 class \(w_2\) 선택에 보탬</li> <li>case 2-1) \(P(w_1)\) 값 증가 (\(\gt P(w_2)\))<br> \(w_0\) 값이 커지므로 class \(w_1\) 선택에 보탬</li> <li>case 2-2) \(P(w_1)\) 값 감소 (\(\lt P(w_2)\))<br> \(w_0\) 값이 작아지므로 class \(w_2\) 선택에 보탬</li> </ul> </li> </ul> </li> </ul> <h2 id="chapter-2-linear-transformation">Chapter 2. Linear Transformation</h2> <h3 id="linear-transformation">Linear Transformation</h3> <ul> <li> \[y = A^Tx\] <ul> <li>mean and variance :<br> \(\mu_{y} = A^T \mu_{x}\)<br> \(\Sigma_{y} = E[(y - \mu_{y})(y - \mu_{y})^T] = A^T \Sigma_{x} A\)</li> <li>Mahalanobis distance :<br> \(d_y^2 = (y - \mu_{y})^T\Sigma_{y}^{-1}(y - \mu_{y}) = \cdots = d_x^2\)<br> <code class="language-plaintext highlighter-rouge">linear transformation</code>을 해도 Mahalanobis distance는 <code class="language-plaintext highlighter-rouge">그대로</code>임<br> (Euclidean distance \((x - \mu_{x})^T(x - \mu_{x})\) 는 linear transformation을 하면 variant)</li> <li>Gaussian distribution :<br> \(x \sim N(\mu_{x}, \Sigma_{x})\) 일 때<br> \(P(y) = (2 \pi)^{- \frac{d}{2}} | \Sigma_{y} |^{-\frac{1}{2}} \exp(-\frac{1}{2}(y - \mu_{y})^T \Sigma_{y}^{-1} (y - \mu_{y})) = (2 \pi)^{- \frac{d}{2}} | A |^{-1} | \Sigma_{x} |^{-\frac{1}{2}} \exp(-\frac{1}{2}(x - \mu_{x})^T \Sigma_{x}^{-1} (x - \mu_{x})) = \frac{1}{|A|} P(x)\)</li> </ul> </li> </ul> <h3 id="orthonormal-transformation">Orthonormal Transformation</h3> <ul> <li>\(x = \sum_{i=1}^d y_i \phi_{i}\)<br> where \(\{ \phi_{i}, \cdots, \phi_{d} \}\) is orthonormal basis<br> Equivalently,<br> \(y_i = x^T \phi_{i}\)<br> where vector \(x\) 를 i-th eigenvector \(\phi_{i}\) 에 project한 게 \(y_i\) <ul> <li>approx. \(x\) : <ul> <li>\(\{ y_{m+1}, \cdots, y_{d} \}\) 를 pre-defined constants \(\{ b_{m+1}, \cdots, b_{d} \}\) 로 대체했을 때<br> \(\hat x(m) = \sum_{i=1}^m y_i \phi_{i} + \sum_{i=m+1}^d b_i \phi_{i}\)</li> </ul> </li> <li>optimal \(b_i\) : <ul> <li>error \(\Delta x(m) = x - \hat x(m) = \sum_{i=m+1}^d (y_i - b_i) \phi_{i}\)<br> MSE \(\bar \epsilon^{2}(m) = E[| \Delta x(m) |^2] = E[\Delta x^T(m) \Delta x(m)] = \sum_{i=m+1}^d E[(y_i - b_i)^2]\)</li> <li>orthonormal basis \(\phi_{i}, \phi_{j}\) 에 대해<br> \(\frac{\partial}{\partial b_i} E[(y_i - b_i)^2] = -2(E[y_i] - b_i) = 0\) 이므로<br> MSE 최소화하는 optimal \(b_i = E[y_i]\)</li> </ul> </li> <li>optimal \(\phi_{i}\) : <ul> <li>\(x = \sum_{j=1}^d y_j \phi_{j}\) 의 양변에 \(\phi_{i}^T\) 를 곱하면<br> \(y_i = x^T \phi_{i}\) 이고<br> optimal \(b_i = E[y_i]\) 이므로<br> MSE \(\bar \epsilon^{2}(m) = \sum_{i=m+1}^d E[(y_i - b_i)^2] = \sum_{i=m+1}^d E[(x^T \phi_{i} - E[x^T \phi_{i}])^T(x^T \phi_{i} - E[x^T \phi_{i}])] = \sum_{i=m+1}^d \text{Var}(\phi_{i}^{T} x) = \sum_{i=m+1}^d \phi_{i}^T \Sigma_{x} \phi_{i}\)</li> <li>orthonormality equality constraint \(\phi_{i}^T\phi_{i} = \| \phi_{i} \| = 1\) 을 만족하면서 MSE \(\bar \epsilon^{2}(m)\) 를 최소화하는 \(\phi_{i}\) 는 Lagrange multiplier Method <a href="https://semyeong-yu.github.io/blog/2024/Lagrange/">Link</a> 로 찾을 수 있다<br> \(\rightarrow\)<br> goal : minimize \(U(m) = \sum_{i=m+1}^d \phi_{i}^T \Sigma_{x} \phi_{i} + \sum_{i=m+1}^d \lambda_{i}(1 - \phi_{i}^T\phi_{i})\)<br> \(\frac{\partial}{\partial x}(x^TAx) = (A + A^T)x = 2Ax\) for symmetric \(A\) 이므로<br> \(\frac{\partial}{\partial \phi_{i}} U(m) = 2(\Sigma_{x}\phi_{i} - \lambda_{i}\phi_{i}) = 0\) 이므로<br> MSE 최소화하는 optimal \(\phi_{i}\) 는 \(\Sigma_{x}\phi_{i} = \lambda_{i}\phi_{i}\) 을 만족하므로<br> \(\phi_{i}\) 와 \(\lambda_{i}\) 는 covariance matrix \(\Sigma_{x}\) 의 eigenvector and eigenvalue 이다</li> </ul> </li> </ul> </li> <li>Eigenvector and Eigenvalue : <ul> <li>\(\Sigma \Phi = \Phi \Lambda\) where \(\Phi \Phi^{T} = I\)</li> <li>If \(\Sigma\) is non-singular (\(| \Sigma | \neq 0\)),<br> all eigenvalues \(\lambda\) are non-zero</li> <li>If \(\Sigma\) is positive-definite (\(x^T \Sigma x \geq 0\) for all \(x \neq 0\)),<br> all eigenvalues \(\lambda\) are positive</li> <li>If \(\Sigma\) is real and symmetric,<br> all eigenvalues \(\lambda\) are real<br> and eigenvectors(w. distinct eigenvalues) are orthogonal <ul> <li>pf)<br> \(\Sigma \phi_{i} = \lambda_{i} \phi_{i}\) and \(\Sigma \phi_{j} = \lambda_{j} \phi_{j}\)<br> \(\phi_{j}^T \Sigma \phi_{i} - \phi_{i}^T \Sigma \phi_{j} = \phi_{j}^T \lambda_{i} \phi_{i} - \phi_{i}^T \lambda_{j} \phi_{j}\)<br> \(0 = (\lambda_{i} - \lambda_{j}) \phi_{j}^T \phi_{i}\) since \(\Sigma\) is symmetric<br> \(\rightarrow \phi_{j}^T \phi_{i} = 0\) (eigenvectors are orthogonal)</li> </ul> </li> </ul> </li> <li>Orthonormal Transformation :<br> \(y = \Phi^{T} x\)<br> for \(\Phi = [\phi_{1}, \cdots \phi_{d}]\) and \(\Phi \Phi^{T} = I\) <ul> <li>vector \(x\) 를 i-th eigenvector \(\phi_{i}\) 에 project한 게 \(y_{i}\)<br> 즉, vector \(x\) 를 new coordinate \(\Phi = [\phi_{1}, \cdots \phi_{d}]\) 으로 나타낸 게 vector \(y\)</li> <li>eigenvector는 principal axis를 나타내고, eigenvalue는 해당 방향으로 퍼진 정도를 나타냄</li> <li>\(y\) 의 covariance matrix인 \(\Sigma_{y}\) 는 <code class="language-plaintext highlighter-rouge">diagonal matrix</code><br> (uncorrelated random vector \(y\)) <ul> <li>\(\Sigma_{y}\)<br> \(= \Phi^{T} \Sigma_{x} \Phi\)<br> \(= \Phi^{T} \Phi \Lambda\) since \(\Sigma \Phi = \Phi \Lambda\)<br> \(= \Phi^{-1} \Phi \Lambda\) since eigenvector matrix is orthogonal matrix (\(\Phi^{T} = \Phi^{-1}\))<br> \(= \Lambda\)</li> </ul> </li> <li>distance : <ul> <li>Mahalanobis distance는 any linear transformation에 대해 보존됨</li> <li> <code class="language-plaintext highlighter-rouge">Euclidean distance</code>는 linear transformation 중 orthonormal transformation일 때만 <code class="language-plaintext highlighter-rouge">보존</code>됨<br> \(\| y \|^2 = y^Ty = x^T \Phi \Phi^{T} x = x^T \Phi \Phi^{-1} x = x^T x = \| x \|^2\)</li> </ul> </li> </ul> </li> </ul> <h3 id="whitening-transformation">Whitening Transformation</h3> <ul> <li>Whitening Transformation :<br> \(y = \Lambda^{-\frac{1}{2}} \Phi^{T} x = (\Phi \Lambda^{-\frac{1}{2}})^T x\)<br> (Orthonormal Transformation을 한 뒤 추가로 \(\Lambda^{-\frac{1}{2}}\) 로 transformation) <ul> <li>\(y\) 의 covariance matrix인 \(\Sigma_{y}\) 는 <code class="language-plaintext highlighter-rouge">identity matrix</code> \(I\) <ul> <li>\(\Sigma_{y}\)<br> \(= (\Lambda^{-\frac{1}{2}} \Phi^{T}) \Sigma_{x} (\Phi \Lambda^{-\frac{1}{2}})\)<br> \(= \Lambda^{-\frac{1}{2}} \Lambda \Lambda^{-\frac{1}{2}}\)<br> \(= I\)</li> </ul> </li> <li>\(\Lambda^{-\frac{1}{2}}\) 은 principal components의 scale을 \(\frac{1}{\sqrt{\lambda_{i}}}\) 배 하는 효과</li> <li>Whitening Transformation을 한 번 하고나면,<br> 그 후에 any Orthonormal Transformation(\(y = \Phi^{T} x\) for \(\Psi \Psi^{T} = I\))을 해도<br> covariance matrix는 항상 \(\Psi I \Psi^{T} = I\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/7-480.webp 480w,/assets/img/2024-09-10-Pattern/7-800.webp 800w,/assets/img/2024-09-10-Pattern/7-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="sample-separation">Sample Separation</h3> <ul> <li>Sample Separation :<br> uncorrelated normal samples \(\sim N(0, I)\) 로부터 correlated sample \(\sim N(\mu_{x}, \Sigma_{x})\) 만들기 <ul> <li>How? :<br> given data \(x\) 에서 \(\mu_{x}\) 를 뺀 뒤 Whitening Transformation 적용하면 \(N(0, I)\) 이므로 이 과정을 역으로 실행</li> <li>Step 1) Normal distribution으로부터 N개의 \(d\) -dim. independent vectors를 sampling<br> \(y_1, y_2, \cdots, y_N \sim N(0, I)\)</li> <li>Step 2) Inverse-Whitening-Transformation 적용하여 Normal distribution을 x-space로 변환 \(x_k = \Phi \Lambda^{\frac{1}{2}} y_k\)<br> for given \(\Sigma_{x}\)<br> and its eigen-decomposition \(\Sigma_{x} \Phi = \Phi \Lambda\)</li> <li>Step 3) x-space의 samples에 \(\mu_{x}\) 더함<br> \(x_k = \Phi \Lambda^{\frac{1}{2}} y_k + \mu_{x} \sim N(\mu_{x}, \Sigma_{x})\)<br> for given \(\mu_{x}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/8-480.webp 480w,/assets/img/2024-09-10-Pattern/8-800.webp 800w,/assets/img/2024-09-10-Pattern/8-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="chapter-3-maximum-likelihood-and-bayesian-parameter-estimation">Chapter 3. Maximum-likelihood and Bayesian Parameter Estimation</h2> <ul> <li>parameter estimation : <ul> <li>Maximum Likelihood Estimation (MLE) :<br> (true) parameters are <code class="language-plaintext highlighter-rouge">unknown</code>, but <code class="language-plaintext highlighter-rouge">fixed</code><br> estimators are random variable</li> <li>Bayesian Estimation :<br> parameters are <code class="language-plaintext highlighter-rouge">random variables</code> and <code class="language-plaintext highlighter-rouge">prior is known</code> </li> </ul> </li> </ul> <h3 id="maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</h3> <ul> <li> <p>Assumption :<br> training data \(D_j\) \(\sim\) likelihood \(p(D_j | w_j) = N(\mu_{j}, \Sigma_{j})\)<br> (i.i.d random samples)</p> </li> <li>MLE : <ul> <li>likelihood :<br> \(\hat \theta = \text{argmax}_{\theta} p(D=\{ x_1, x_2, \ldots, x_n \} | \theta) = \text{argmax}_{\theta} \prod_{k=1}^n p(x_k | \theta)\)</li> <li>log-likelihood :<br> \(\hat \theta = \text{argmax}_{\theta} p(D=\{ x_1, x_2, \ldots, x_n \} | \theta) = \text{argmax}_{\theta} \sum_{k=1}^n \text{ln} p(x_k | \theta)\)</li> </ul> </li> <li>Gaussian likelihood : <ul> <li>unknown \(\mu\) : <ul> <li>likelihood :<br> \(p(x_k | \mu) = (2 \pi)^{-\frac{d}{2}} | \Sigma |^{-\frac{1}{2}} \text{exp}(-\frac{1}{2}(x_k - \mu)^T \Sigma^{-1} (x_k - \mu))\)<br> \(p(D=\{ x_1, x_2, \ldots, x_N \} | \mu) = \prod_{k=1}^N p(x_k | \mu) = (2 \pi)^{-\frac{dN}{2}} | \Sigma |^{-\frac{N}{2}} \text{exp}(-\frac{1}{2} \sum_{k=1}^N (x_k - \mu)^T \Sigma^{-1} (x_k - \mu))\)</li> <li>log-likelihood :<br> \(\text{ln} p(D | \mu) = -\frac{dN}{2} \text{ln}(2 \pi) -\frac{N}{2} \text{ln} | \Sigma | -\frac{1}{2} \sum_{k=1}^N (x_k - \mu)^T \Sigma^{-1} (x_k - \mu)\)</li> <li>matrix derivative :<br> \(\frac{d}{dx}(Ax) = A\)<br> \(\frac{d}{dx}(y^TAx) = A^Ty\)<br> \(\frac{d}{dx}(x^TAx) = (A+A^T)x\)<br> \(\frac{d}{dA}(x^TAx) = xx^T\)<br> \(\frac{\partial |A|}{\partial A} = (\text{adj}(A))^T = |A|(A^{-1})^T\)<br> \(\frac{\partial \text{ln}|A|}{\partial A} = (A^{-1})^T = (A^T)^{-1}\) where \(|A| = \frac{1}{|A^{-1}|}\)</li> <li>MLE problem :<br> \(\nabla_{\mu} \text{ln} p(D | \mu) = -\frac{1}{2} \sum_{k=1}^N ((\Sigma^{-1} + (\Sigma^{-1})^T) (x_k - \mu)) \times (-1) = (\Sigma^{-1} + (\Sigma^{-1})^T)(\sum_{k=1}^N x_k - \sum_{k=1}^N \mu) = 0\)<br> \(\sum_{k=1}^N x_k - N \mu = 0\)<br> \(\hat \mu_{\text{MLE}} = \frac{1}{N} \sum_{k=1}^N x_k\)</li> <li>Summary : <ul> <li>\(\hat \mu_{\text{MLE}} = \frac{1}{N} \sum_{k=1}^N x_k\)<br> (true mean의 MLE estimator는 sample mean)</li> <li>\(E[\hat \mu_{\text{MLE}}] = \mu\)<br> (\(\hat \mu_{\text{MLE}}\) 는 <code class="language-plaintext highlighter-rouge">unbiased</code> estimator)</li> </ul> </li> </ul> </li> <li>unknown \(\mu\) and \(\Sigma\) : <ul> <li>log-likelihood :<br> \(\text{ln} p(D | \mu) = -\frac{dN}{2} \text{ln}(2 \pi) -\frac{N}{2} \text{ln} | \Sigma | -\frac{1}{2} \sum_{k=1}^N (x_k - \mu)^T \Sigma^{-1} (x_k - \mu) = -\frac{dN}{2} \text{ln}(2 \pi) + \frac{N}{2} \text{ln} | \Sigma^{-1} | -\frac{1}{2} \sum_{k=1}^N (x_k - \mu)^T \Sigma^{-1} (x_k - \mu)\)</li> <li>MLE problem :<br> \(\nabla_{\Sigma^{-1}} \text{ln} p(D | \mu) = \frac{N}{2}\Sigma^{T} - \frac{1}{2} \sum_{k=1}^N (x_k - \mu)(x_k - \mu)^T = 0\)<br> \(N \Sigma^{T} = \sum_{k=1}^N (x_k - \mu)(x_k - \mu)^T\)<br> \(\mu = \hat \mu_{\text{MLE}}\) 대입하고, \(\Sigma\) 는 symmetric(\(\Sigma^{T} = \Sigma\))하므로<br> \(\hat \Sigma_{\text{MLE}} = \frac{1}{N} \sum_{k=1}^N (x_k - \hat \mu_{\text{MLE}})(x_k - \hat \mu_{\text{MLE}})^T\)</li> <li>Summary : <ul> <li>\(\hat \Sigma_{\text{MLE}} = \frac{1}{N} \sum_{k=1}^N (x_k - \hat \mu_{\text{MLE}})(x_k - \hat \mu_{\text{MLE}})^T\)<br> (\(\mu\) 먼저 estimate한 뒤 \(\Sigma\) estimate)</li> <li>\(E[\hat \Sigma_{\text{MLE}}] = \frac{1}{N} E[\sum_{k=1}^N (x_k - \hat \mu_{\text{MLE}})(x_k - \hat \mu_{\text{MLE}})^T] = \frac{N-1}{N} \Sigma \neq \Sigma\)<br> (\(\hat \Sigma_{\text{MLE}}\) 는 <code class="language-plaintext highlighter-rouge">biased</code> estimator) <ul> <li>pf) 아래 식 이용<br> \(E[x_i x_j^T] = \begin{cases} \Sigma + \mu \mu^{T} &amp; \text{if} &amp; i = j \\ \mu \mu^{T} &amp; \text{if} &amp; i \neq j \end{cases}\)<br> since \(\Sigma = E[(x - \mu)(x - \mu)^T] = \cdots = E[xx^T] - \mu \mu^{T}\)<br> since \(0 = E[(x_i - \mu)(x_j - \mu)^T] = E[x_i x_j^T] - \mu \mu^{T}\) by independence \(i \neq j\)</li> </ul> </li> <li>\(\text{lim}_{N \rightarrow \infty}E[\hat \Sigma_{\text{MLE}}] = \text{lim}_{N \rightarrow \infty} \frac{N-1}{N} \Sigma = \Sigma\)<br> (\(\hat \Sigma_{\text{MLE}}\) 는 <code class="language-plaintext highlighter-rouge">asymptotically unbiased</code> estimator)<br> 또는<br> \(\hat \Sigma_{\text{MLE}} = \frac{1}{N-1} \sum_{k=1}^N (x_k - \hat \mu_{\text{MLE}})(x_k - \hat \mu_{\text{MLE}})^T\)<br> (위처럼 설정하면 \(\hat \Sigma_{\text{MLE}}\) 는 <code class="language-plaintext highlighter-rouge">unbiased</code> estimator)</li> </ul> </li> </ul> </li> </ul> </li> <li>MLE : <ul> <li>MLE is <code class="language-plaintext highlighter-rouge">asymptotically consistent</code><br> if \(\text{lim}_{N \rightarrow \infty} P(\| \hat \theta_{\text{MLE}} - \theta_{\text{true}} \| \leq \epsilon) = 1\) for arbitrary small \(\epsilon\)<br> (sample 수 \(N\) 이 크면 param. estimate은 true value랑 거의 비슷)<br> by central limit theorem and the fact that MLE is related to the sum of random var.</li> <li>MLE is <code class="language-plaintext highlighter-rouge">asymptotically efficient</code><br> since MLE는 Cramer-Rao lower bound(any estimate이 달성할 수 있는 the lowest value of variance)</li> </ul> </li> </ul> <h3 id="bayesian-estimation">Bayesian Estimation</h3> <ul> <li>Summary : <ul> <li>MLE (maximum likelihood estimation) : <ul> <li>\(\theta\) is unknown, but fixed</li> <li> <table> <tbody> <tr> <td>maximize likelihood $$\theta_{MLE} = \text{argmax}_{\theta} p(D</td> <td>\theta)$$</td> </tr> </tbody> </table> </li> </ul> </li> <li>MAP (maximum a posterior) : <ul> <li>\(\theta\) is random var. and prior \(p(\theta)\) is known</li> <li> <table> <tbody> <tr> <td>maximize posterior $$\theta_{MAP} = \text{argmax}_{\theta} p(\theta</td> <td>D) = \text{argmax}_{\theta} \text{ln} p(D</td> <td>\theta) + \text{ln} p(\theta)$$</td> </tr> </tbody> </table> </li> </ul> </li> <li>If prior \(p(\theta)\) is constant (uniform distribution),<br> MLE와 MAP는 same</li> </ul> </li> <li> <p>prior \(p(\theta)\) 와 posterior \(p(\theta | D)\) 가 같은 확률 분포의 형태를 가질 경우<br> prior \(p(\theta)\) 를<br> likehood \(p(D | \theta)\) 에 대한 <code class="language-plaintext highlighter-rouge">conjugate prior</code>라고 말한다</p> </li> <li>Gaussian case : <ul> <li>random var. \(\mu\) : <ul> <li>likelihood and conjugate prior :<br> \(x_k \sim\) \(p(x_k | \mu) = N(\mu, \sigma^{2})\)<br> where \(\mu \sim\) \(p(\mu) = N(\mu_{0}, \sigma_{0}^{2})\)</li> <li>posterior (수식 유도는 아래에 별도로) :<br> \(p(\mu | D, \sigma^{2}) \propto N(\mu_{N}, \sigma_{N}^{2})\)<br> \(= \frac{1}{\sqrt{2 \pi} \sigma_{N}} e^{-\frac{1}{2}(\frac{\mu - \mu_{N}}{\sigma_{N}})^2}\)<br> where \(\hat \mu_{MAP} = \text{argmax}_{\mu} p(\mu | D, \sigma^{2}) = \mu_{N} = (\frac{N}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}}) \hat \mu_{MLE} + \frac{\frac{\sigma^{2}}{\sigma_{0}^{2}}}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}} \mu_{0}\)<br> where \(\sigma_{N}^{2} = \frac{\sigma^{2}}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}}\) <ul> <li>Bayesian Learning :<br> \(N\), 즉 sample 수가 많아질수록<br> \(\mu_{N}\) 은 \(\hat \mu_{MLE}\) 에 가까워지고<br> \(\sigma_{N}^{2}\), 즉 uncertainty about \(\mu_{N}\) 은 감소<br> 따라서 \(N \rightarrow \infty\) 이면<br> posterior \(p(\mu | D, \sigma^{2})\) 는 \(\mu_{N} = \hat \mu_{MLE}\) 에서의 Dirac delta function</li> <li>\(\hat \mu_{MAP} = \mu_{N}\) 에서<br> \((\frac{N}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}}) \hat \mu_{MLE}\) 는 empirical data samples 부분이고<br> \(\frac{\frac{\sigma^{2}}{\sigma_{0}^{2}}}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}} \mu_{0}\) 는 prior info. 부분</li> <li>만약 \(\sigma_{0}^{2} = 0\) 이라면<br> prior : variance \(\sigma_{0}^{2}\) 가 매우 작으므로, certain that \(\mu = \mu_{0}\)<br> So,<br> posterior : \(\mu_{N} = \mu_{0}\) (data samples는 \(\mu_{N}\) 에 영향 없음)</li> <li>만약 \(\sigma^{2} \ll \sigma_{0}^{2}\) 이라면<br> prior : variance \(\sigma_{0}^{2}\) 가 매우 크므로, so uncertain that \(\mu = \mu_{0}\)<br> So,<br> posterior : \(\mu_{N} = \hat \mu_{MLE}\) (data samples가 \(\mu_{N}\) 에 대부분의 영향 미침)</li> </ul> </li> <li>posterior (수식 유도) :<br> \(p(\mu | D, \sigma^{2})\)<br> \(\propto p(D | \mu, \sigma^{2}) p(\mu) = \frac{1}{(2 \pi \sigma^{2})^{\frac{N}{2}}(2 \pi \sigma_{0}^{2})^{\frac{1}{2}}}e^{-\frac{1}{2\sigma^{2}}\sum_{i=1}^N(x_i - \mu)^2 -\frac{1}{2\sigma_{0}^{2}}(\mu - \mu_{0})^2}\)<br> \(\propto e^{-\frac{1}{2\sigma^{2}}\sum_{i=1}^N(x_i - \mu)^2 -\frac{1}{2\sigma_{0}^{2}}(\mu - \mu_{0})^2}\)<br> \(\propto e^{-\frac{1}{2\sigma^{2}}(N \mu^{2} - 2 \mu \sum_{i=1}^N x_i) -\frac{1}{2\sigma_{0}^{2}}(\mu^{2} - 2 \mu \mu_{0})}\)<br> \(= e^{-\frac{1}{2}(\mu^{2}(\frac{N}{\sigma^{2}} + \frac{1}{\sigma_{0}^{2}}) - 2 \mu (\frac{N \hat \mu_{MLE}}{\sigma^{2}} + \frac{\mu_{0}}{\sigma_{0}^{2}}))}\)<br> \(\propto \frac{1}{\sqrt{2 \pi} \sigma_{N}} e^{-\frac{1}{2}(\frac{\mu - \mu_{N}}{\sigma_{N}})^2}\)<br> where \(\hat \mu_{MAP} = \mu_{N} = (\frac{N\sigma_{0}^{2}}{N\sigma_{0}^{2} + \sigma^{2}}) \hat \mu_{MLE} + \frac{\sigma^{2}}{N\sigma_{0}^{2} + \sigma^{2}} \mu_{0}\)<br> where \(\sigma_{N}^{2} = \frac{\sigma_{0}^{2} \sigma^{2}}{N\sigma_{0}^{2} + \sigma^{2}}\)</li> </ul> </li> </ul> </li> </ul> <h3 id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h3> <ul> <li>PCA :<br> “curse of dimensionality” 문제 완화하고자<br> dimensionality reduction w/o losing much info. <ul> <li>Step 1)<br> 각 dim.에서의 mean을 빼서 zero-mean data로 만듦</li> <li>Step 2)<br> variance \(E_{x}[(u^T x)^2]\) 를 최대화하는 direction \(u\) 가 first principal component<br> (\(u^T x\) : vector \(x\) 를 unit vector \(u\) 에 projection한 것) <ul> <li>Step 2-1)<br> \(E_{x}[(u^T x)^2] = E_{x}[(u^T x)(u^T x)^T] = u^T E_{x}[xx^T] u = u^T C u\)<br> where \(C = E_{x}[xx^T]\) (or \(\hat C = \frac{1}{N-1} xx^T\)) : covariance matrix of \(x\) w. zero-mean</li> <li>Step 2-2)<br> \(u = \text{argmax}_{u} E_{x}[(u^T x)^2] = \text{argmax}_{u} u^T C u\) subject to \(\| u \| = | u^T u | = 1\)<br> By Lagrange mutliplier Method <a href="https://semyeong-yu.github.io/blog/2024/Lagrange/">Link</a>,<br> \(u = \text{argmax}_{u} (u^T C u - \lambda (u^T u - 1))\)<br> \(\rightarrow\)<br> \(\frac{d}{du}(u^T C u - \lambda (u^T u - 1)) = 2(Cu - \lambda u) = 0\)<br> \(\rightarrow\)<br> \(C u = \lambda u\)<br> where \(u\) is eigenvector of \(C = E_{x}[xx^T]\) (or \(\hat C = \frac{1}{N-1} xx^T\)) with eigenvalue \(\lambda\)</li> </ul> </li> <li>Step 3)<br> 정리하면 <ul> <li>Step 3-0) \(C\) :<br> \(C = E_{x}[xx^T]\) (or \(\hat C = \frac{1}{N-1} xx^T\))는<br> covariance matrix of \(x\) w. zero-mean</li> <li>Step 3-1) \(u\) :<br> principal component \(u\) 는 \(C\) 의 eigenvector</li> <li>Step 3-2) \(\lambda\) :<br> \(\lambda\) 는 \(C\) 의 eigenvalue 이자,<br> direction \(u\) 에 대한 variance (\(\text{max}_{u} u^T C u = \lambda\))</li> </ul> </li> <li>Step 4)<br> \(x = a_1 u_1 + \cdots + a_d u_d \approx \sum_{k=1}^{d^{'} \lt d} a_k u_k\)<br> (any vector \(x\) in original space는<br> linear comb. of eigenvectors로 표현할 수 있는데,<br> 덜 중요한 eigenvector 쳐내서 (dim. reduction) 근사 가능)</li> </ul> </li> </ul> <h3 id="multiple-discriminant-analysis-mda">Multiple Discriminant Analysis (MDA)</h3> <ul> <li>Dimensionality reduction : <ul> <li>PCA :<br> data를 가장 잘 <code class="language-plaintext highlighter-rouge">represent</code>하는 projection 찾기<br> maximize variance<br> representative<br> unsupervised learning</li> <li>MDA :<br> data를 가장 잘 <code class="language-plaintext highlighter-rouge">separate</code>하는 projection 찾기<br> maximize between-variance and minimize within-variance<br> discriminative<br> supervised learning</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/26-480.webp 480w,/assets/img/2024-09-10-Pattern/26-800.webp 800w,/assets/img/2024-09-10-Pattern/26-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/26.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Fisher’s LDA (Linear Discriminant Analysis) : <ul> <li>goal :<br> Project onto 1D line 할 때<br> \(y_k = \boldsymbol w^T \boldsymbol x_k\) where \(\| \boldsymbol \| = 1\) <ul> <li>maximize separation <code class="language-plaintext highlighter-rouge">b.w.</code> means of projected classes</li> <li>minimize variance <code class="language-plaintext highlighter-rouge">within</code> each projected class</li> </ul> </li> <li>Step 1-1)<br> To maximize between-class separation and minimize within-class variance,<br> maximize \(J(\boldsymbol w) = \frac{(\mu_{1} - \mu_{2})^2}{\sigma_{1}^2 + \sigma_{2}^2} = \frac{(\boldsymbol w^T \boldsymbol \mu_{1} - \boldsymbol w^T \boldsymbol \mu_{2})^2}{\boldsymbol w^T (\Sigma_{1} + \Sigma_{2}) \boldsymbol w}\)<br> where exact mean \(\mu_{i} = E_{\boldsymbol x}[y_i] = E_{\boldsymbol x}[\boldsymbol w^T \boldsymbol x_i] = \int \boldsymbol w^T \boldsymbol x_i p(\boldsymbol x_i) d \boldsymbol x_i = \boldsymbol w^T \int \boldsymbol x_i p(\boldsymbol x_i) d \boldsymbol x_i = \boldsymbol w^T \boldsymbol \mu_{i}\)<br> where exact variance \(\sigma_{i}^2 = E_{\boldsymbol x}[(y_i - \mu_{i})^2] = \int (y_i - \mu_{i})^2 p(\boldsymbol x_i) d \boldsymbol x_i = \int (\boldsymbol w^T \boldsymbol x_i - \boldsymbol w^T \boldsymbol \mu_{i})^2 p(\boldsymbol x_i) d \boldsymbol x_i = \boldsymbol w^T \int (\boldsymbol x_i - \boldsymbol \mu_{i}) (\boldsymbol x_i - \boldsymbol \mu_{i})^{T} p(\boldsymbol x_i) d \boldsymbol x_i \boldsymbol w = \boldsymbol w^T \Sigma_{i} \boldsymbol w\)</li> <li>Step 1-2)<br> Find optimal \(w\) <ul> <li> \[\frac{\partial J}{\partial \boldsymbol w} = \frac{\partial J}{\partial \mu_{1}} \frac{\partial \mu_{1}}{\partial \boldsymbol w} + \frac{\partial J}{\partial \mu_{2}} \frac{\partial \mu_{2}}{\partial \boldsymbol w} + \frac{\partial J}{\partial \sigma_{1}^2} \frac{\partial \sigma_{1}^2}{\partial \boldsymbol w} + \frac{\partial J}{\partial \sigma_{2}^2} \frac{\partial \sigma_{2}^2}{\partial \boldsymbol w} = 0\] <ul> <li> \[\frac{\partial J}{\partial \mu_{1}} \frac{\partial \mu_{1}}{\partial \boldsymbol w} = \frac{2(\mu_{1} - \mu_{2})}{\sigma_{1}^2 + \sigma_{2}^2} \boldsymbol \mu_{1}\] </li> <li> \[\frac{\partial J}{\partial \mu_{2}} \frac{\partial \mu_{2}}{\partial \boldsymbol w} = - \frac{2(\mu_{1} - \mu_{2})}{\sigma_{1}^2 + \sigma_{2}^2} \boldsymbol \mu_{2}\] </li> <li> \[\frac{\partial J}{\partial \sigma_{1}^2} \frac{\partial \sigma_{1}^2}{\partial \boldsymbol w} = - \frac{(\mu_{1} - \mu_{2})^2}{(\sigma_{1}^2 + \sigma_{2}^2)^2} (\Sigma_{1} + \Sigma_{1}^{T}) \boldsymbol w\] </li> <li> \[\frac{\partial J}{\partial \sigma_{2}^2} \frac{\partial \sigma_{2}^2}{\partial \boldsymbol w} = - \frac{(\mu_{1} - \mu_{2})^2}{(\sigma_{1}^2 + \sigma_{2}^2)^2} (\Sigma_{2} + \Sigma_{2}^{T}) \boldsymbol w\] </li> </ul> </li> <li> \[\frac{\partial J}{\partial \boldsymbol w} = \frac{2(\mu_{1} - \mu_{2})}{\sigma_{1}^2 + \sigma_{2}^2}(\boldsymbol \mu_{1} - \boldsymbol \mu_{2}) - \frac{2(\mu_{1} - \mu_{2})^2}{(\sigma_{1}^2 + \sigma_{2}^2)^2} (\Sigma_{1} + \Sigma_{2}) \boldsymbol w = 0\] </li> <li>optimal \(\boldsymbol w = \frac{\sigma_{1}^2 + \sigma_{2}^2}{\mu_{1} - \mu_{2}}(\Sigma_{1} + \Sigma_{2})^{-1}(\boldsymbol \mu_{1} - \boldsymbol \mu_{2})\)<br> where \(\boldsymbol w\) 의 방향은 \(\boldsymbol \mu_{1} - \boldsymbol \mu_{2}\) 를 \((\Sigma_{1} + \Sigma_{2})^{-1}\) 만큼 rotate시킨 방향</li> <li>근데, exact mean \(\mu_{i}\) and variance \(\sigma_{i}\) are unknown!<br> \(\rightarrow\) 아래의 방식으로 해결</li> </ul> </li> <li>Step 2-1)<br> Another form of \(J(\boldsymbol w)\)<br> maximize \(J(\boldsymbol w) = \frac{\boldsymbol w^{T} S_{B} \boldsymbol w}{\boldsymbol w^{T} S_{w} \boldsymbol w} = \frac{\text{among group variance}}{\text{within group variance}}\) <ul> <li>within-class scatter matrix \(S_w = S_1 + S_2\) : \(d \times d\) matrix with rank \(d\)<br> where scatter matrix \(S_i = \sum_{\boldsymbol x \in D_i} (\boldsymbol x - \boldsymbol m_i) (\boldsymbol x - \boldsymbol m_i)^{T}\)<br> where sample mean \(\boldsymbol m_i = \frac{1}{n_i} \sum_{\boldsymbol x \in D_i} \boldsymbol x\)</li> <li>between-class scatter matrix \(S_B = (\boldsymbol m_1 - \boldsymbol m_2) (\boldsymbol m_1 - \boldsymbol m_2)^{T}\) : \(d \times d\) matrix with rank \(1\)</li> </ul> </li> <li>Step 2-2)<br> Find optimal \(w\) <ul> <li> \[\frac{\partial J}{\partial \boldsymbol w} = 0\] <ul> <li> \[(S_B + S_{B}^{T}) \boldsymbol w (\boldsymbol w^{T} S_{w} \boldsymbol w) - (\boldsymbol w^{T} S_{B} \boldsymbol w) (S_w + S_{w}^{T}) \boldsymbol w = 0\] </li> </ul> </li> <li>Since \(S_B\) and \(S_w\) are symmetric and positive-definite,<br> \(S_{w}^{-1} S_B \boldsymbol w = \lambda \boldsymbol w\)<br> where \(\lambda = (\boldsymbol w^{T} S_{B} \boldsymbol w) (\boldsymbol w^{T} S_{w} \boldsymbol w)^{-1} = J(\boldsymbol w)\) <ul> <li>\(\text{rank}(S_{w}^{-1} S_B) = \text{rank}(S_B) = 1\) 이므로 \(\lambda = \lambda_{1} \gt \lambda_{2} = \cdots = \lambda_{d} = 0\)<br> 즉, \(\lambda\) 는 only nonzero eigenvalue of \(S_{w}^{-1} S_B\)<br> 그리고 \(\boldsymbol w\) 는 \(\lambda\) 에 대응되는 eigenvector</li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/24-480.webp 480w,/assets/img/2024-09-10-Pattern/24-800.webp 800w,/assets/img/2024-09-10-Pattern/24-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/24.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Summary on Fisher’s LDA (Linear Discriminant Analysis) : <ul> <li>slowest way : <ul> <li>inverse matrix \(S_{w}^{-1}\) 구함</li> <li>matrix multiplication \(S_{w}^{-1}S_B\) 수행</li> <li>\(S_{w}^{-1}S_B\) 의 nonzero eigenvalue \(\lambda\) 에 대응되는 eigenvector \(\boldsymbol w\) 구하기</li> </ul> </li> <li>better way : <ul> <li>MATLAB eigs(A, B) 함수로 generalized eigenvalue problem \(S_{B} \boldsymbol w = \lambda S_{w} \boldsymbol w\) 풀기</li> </ul> </li> <li>smartest way : <ul> <li>\(\lambda \boldsymbol w = S_{w}^{-1} S_{B} \boldsymbol w = S_{w}^{-1} (\boldsymbol m_{1} - \boldsymbol m_{2}) (\boldsymbol m_{1} - \boldsymbol m_{2})^{T} \boldsymbol w = S_{w}^{-1} (\boldsymbol m_{1} - \boldsymbol m_{2}) k \propto S_{w}^{-1} (\boldsymbol m_{1} - \boldsymbol m_{2})\)<br> for constant \(k = (\boldsymbol m_{1} - \boldsymbol m_{2})^{T} \boldsymbol w = (\text{1 x d})(\text{d x 1}) = \text{1 x 1}\)</li> <li>즉,<br> optimal \(\boldsymbol w = q S_{w}^{-1} (\boldsymbol m_{1} - \boldsymbol m_{2})\)<br> for constant \(q = \frac{1}{\sqrt{(\boldsymbol m_{1} - \boldsymbol m_{2})^{T} S_{w}^{-2} (\boldsymbol m_{1} - \boldsymbol m_{2})}}\) s.t. \(\boldsymbol w^{T} \boldsymbol w = 1\)</li> <li>\(\boldsymbol w\) 의 방향은 \(\boldsymbol \mu_{1} - \boldsymbol \mu_{2}\) 를 \(S_{w}^{-1}\) 만큼 rotate시킨 방향</li> </ul> </li> </ul> </li> <li>trade-off : <ul> <li>high-dim. :<br> high performance (low error rate), but high computational complexity</li> <li>low-dim. :<br> low performance (high error rate), but low computational complexity</li> </ul> </li> <li>Multiple Discriminant Analysis (MDA) : <ul> <li>goal :<br> \(c\)-class의 경우 \(c-1\) 개의 discriminant function으로 \(c-1\) 개의 1D projection 수행<br> 즉, Project onto \(c-1\) dim. : \(\boldsymbol y = W^{T} \boldsymbol x = [y_1, y_2, \cdots, y_{c-1}]\) by projection matrix \(W = [w_1 | w_2 | \cdots | w_{c-1}]\)</li> <li>Fisher’s LDA에서와 유사하게 <ul> <li>within-class scatter matrix : \(S_w = \sum_{i=1}^c S_i\) : \(d \times d\) matrix with rank \(d\)<br> where \(S_i = \sum_{\boldsymbol x \in D_i} (\boldsymbol x - \boldsymbol m_i) (\boldsymbol x - \boldsymbol m_i)^{T}\) : scatter matrix of each class<br> where \(\boldsymbol m_{i} = \frac{1}{n_i} \sum_{\boldsymbol x \in D_i} \boldsymbol x\) : sample mean of each class in \(d\)-dim.</li> <li>between-class scatter matrix : \(S_B = \sum_{i=1}^c n_i (\boldsymbol m_i - \boldsymbol m) (\boldsymbol m_i - \boldsymbol m)^{T}\) : \(d \times d\) matrix with rank \(\leq c-1\)<br> where \(n_i\) : weight for unbalanced data<br> where \(\boldsymbol m = \frac{1}{n} \sum \boldsymbol x = \frac{1}{n} \sum_{j=1}^c n_j \boldsymbol m_j\) : global mean in \(d\)-dim.</li> <li>projected within-class scatter matrix : \(\tilde S_w = \cdots = W^{T} S_w W\)</li> <li>projected between-class scatter matrix : \(\tilde S_B = \cdots = W^{T} S_B W\)</li> </ul> </li> <li> \[\text{max}_{\| W \| = 1} \frac{| \tilde S_B |}{| \tilde S_w |} = \text{max}_{\| W \| = 1} \frac{| W^{T} S_B W |}{| W^{T} S_w W |} = \text{max}_{\| W \| = 1} \frac{\text{among group variance}}{\text{within group variance}}\] </li> <li>Fisher’s LDA에서와 유사한 과정으로 풀면<br> \(S_{w}^{-1} S_{B} W = \Gamma W\)<br> for \(\Gamma = \begin{bmatrix} \lambda_{1} &amp; \cdots &amp; 0 \\ 0 &amp; \ddots &amp; 0 \\ 0 &amp; 0 &amp; \lambda_{c-1} \end{bmatrix}\)<br> for \(\text{rank}(S_{w}^{-1} S_{B}) = \text{rank}(S_B) \leq c-1\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/25-480.webp 480w,/assets/img/2024-09-10-Pattern/25-800.webp 800w,/assets/img/2024-09-10-Pattern/25-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/25.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Limitation of LDA and MDA : <ul> <li>dim. reduction only to \(c-1\) dim. (unlike PCA)</li> <li>unimodal Gaussian likelihoods를 가정하는 parametric method라서<br> 만약 실제 distribution이 Gaussian 분포와 멀다면<br> MDA projection은 기존 분포의 복잡한 구조를 잘 보존하지 못함</li> <li>LDA, MDA는 mean difference에 의존하여 discriminate하기 때문에<br> \(\mu_{1} = \mu_{2}\) 라면 \(J(\boldsymbol w) = 0\) 이라서 LDA, MDA 적용 불가능</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/27-480.webp 480w,/assets/img/2024-09-10-Pattern/27-800.webp 800w,/assets/img/2024-09-10-Pattern/27-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/27.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/28-480.webp 480w,/assets/img/2024-09-10-Pattern/28-800.webp 800w,/assets/img/2024-09-10-Pattern/28-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/28.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> mean difference가 크지 않으면 LDA보다 PCA가 class separation 유리한 상황도 있음 </div> <h3 id="singular-value-decomposition-svd">Singular Value Decomposition (SVD)</h3> <ul> <li>쓰임 : matrix factorization <ul> <li>low-rank approximation of matrix</li> <li>pseudo-inverse of non-square matrix</li> </ul> </li> <li>Singular Value Decomposition (SVD) :<br> \(A = U \Sigma V^T = \begin{bmatrix} u_1 &amp; u_2 &amp; \cdots &amp; u_m \end{bmatrix} \begin{bmatrix} \sigma_{1} &amp; \cdots &amp; 0 &amp; \cdots &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; \sigma_{r} &amp; \cdots &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; 0 &amp; \cdots &amp; \sigma_{m} &amp; 0 &amp; \cdots &amp; 0 \end{bmatrix} \begin{bmatrix} v_1^{T} \\ v_2^{T} \\ \vdots \\ v_n^{T} \end{bmatrix}\)<br> where \(U\) is \(m \times m\) orthonormal matrix (\(UU^T = I\))<br> where \(V\) is \(n \times n\) orthonormal matrix (\(VV^T = I\))<br> where \(\Sigma\) is \(m \times n\) diagonal matrix (\(\sigma_{1} \geq \sigma_{2} \geq \cdots \geq \sigma_{m} \geq 0\)) <ul> <li>\(AA^T = U \Sigma \Sigma^{T} U^T\)<br> \(U\) 는 \(AA^T\) 의 eigenvector matrix<br> \(\sigma_{i} = \sqrt{\lambda_{i}}\) where \(\lambda_{i}\) 는 \(AA^T\) 의 eigenvalue</li> <li>\(A^TA = V \Sigma^{T} \Sigma V^T\)<br> \(V\) 는 \(A^TA\) 의 eigenvector matrix<br> \(\sigma_{i} = \sqrt{\lambda_{i}}\) where \(\lambda_{i}\) 는 \(A^TA\) 의 eigenvalue</li> </ul> </li> <li>rank and span :<br> \(r = \text{rank}(A) = \text{rank}(\Sigma) \leq \text{min}(m, n)\) 에 대해<br> (\(\sigma_{1} \geq \sigma_{2} \geq \cdots \geq \sigma_{r} \geq \sigma_{r+1} = \cdots = \sigma_{m} = 0\)) <ul> <li>\(\text{col}(A)\) is spanned by \(\begin{bmatrix} u_1, \cdots, u_r \end{bmatrix}\)<br> \(AA^Tu_i = \sigma_{i}^{2} u_i\) for \(i = 1, \cdots, r\)</li> <li>\(\text{null}(A^T)\) is spanned by \(\begin{bmatrix} u_{r+1}, \cdots, u_m \end{bmatrix}\)<br> \(AA^Tu_i = 0\) for \(i = r+1, \cdots, m\), 즉 \(A^T u_i = 0\)</li> <li>\(\text{row}(A) = \text{col}(A^T)\) is spanned by \(\begin{bmatrix} v_{1}^{T}, \cdots, v_{r}^{T} \end{bmatrix}\)<br> \(A^TAv_i = \sigma_{i}^{2} v_i\) for \(i = 1, \cdots, r\)</li> <li>\(\text{null}(A)\) is spanned by \(\begin{bmatrix} v_{r+1}^{T}, \cdots, v_{n}^{T} \end{bmatrix}\)<br> \(A^TAv_i = 0\) for \(i = r+1, \cdots, n\), 즉 \(A v_i = 0\)</li> <li>\(A = U \Sigma V^T\) \(\rightarrow\) \(A V = U \Sigma\)<br> \(A v_i = \sigma_{i} u_i\) for \(i = 1, \cdots, r\)<br> \(A v_i = 0\) for \(i = r+1, \cdots, n\)</li> </ul> </li> <li> <p>SVD (rank에 맞게 줄인 버전) :<br> \(A = \begin{bmatrix} u_1 &amp; u_2 &amp; \cdots &amp; u_r \end{bmatrix} \begin{bmatrix} \sigma_{1} &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \sigma_{2} &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; 0 &amp; \sigma_{r} \end{bmatrix} \begin{bmatrix} v_1^{T} \\ v_2^{T} \\ \vdots \\ v_r^{T} \end{bmatrix}\)<br> \(= \sum_{i=1}^{r} \sigma_{i} u_i v_{i}^{T}\)</p> </li> <li> <p>Eigen Decomposition :<br> matrix \(A\) 가 square and real and symmetric일 경우<br> \(A P = P D\)<br> where \(P\) is orthonormal eigenvector matrix (\(P^{-1} = P^T\) and \(PP^T = I\))<br> where \(D\) is diagonal eigenvalue matrix</p> </li> <li>SVD 계산 과정 Summary : <ul> <li>Step 1)<br> \(A^TA\) 를 Eigen Decomposition해서,<br> eigenvector matrix \(V\) 와 eigenvalue matrix \(\Sigma\) 를 구함<br> 이 때, \(\Sigma\) 의 singular value는 \(A^TA\) 의 eigenvalue에 \(\sqrt{\cdot}\) 씌워서 구함</li> <li>Step 2)<br> eigenvector matrix \(V\) 의 columns를 normalize하고<br> \(V^TV=I\) 맞는지 확인</li> <li>Step 3)<br> \(Av_i = \sigma_{i} u_i\), 즉 \(u_i = \sigma_{i}^{-1} A v_i\) 으로<br> eigenvector \(u_i\) 구하고<br> normalize해서 \(UU^T = I\) 맞는지 확인</li> <li>Step 4)<br> \(A = U \Sigma V^T\) 맞는지 최종 확인</li> </ul> </li> <li>Relation b.w. PCA and SVD :<br> matrix \(A\) 의 columns가 zero-mean centered일 때 <ul> <li>PCA :<br> covariance matrix \(C = E[AA^T] = \frac{1}{N-1}AA^T\)의 eigenvectors를 구한 뒤<br> eigenvalue 큰 순으로 잘라서 principal eigenvectors의 합으로 표현</li> <li>SVD :<br> \(AA^T = U \Sigma^{2} U^T = V \Sigma^{2} V^T\) (\(U = V\) since \(AA^T\) is square matrix) 이므로<br> \(AA^T\)의 eigenvector matrix \(V\) 를 구한 뒤<br> rank \(r \leq \text{min}(m, n)\)에 맞게 줄여서 \(A = U \Sigma V^T = \sum_{i=1}^{r} \sigma_{i} u_i v_{i}^{T}\) 로 표현</li> <li>Relation :<br> 둘은 mathematically 동일!<br> \(C = V \frac{\Sigma^{2}}{N-1} V^T\)</li> </ul> </li> </ul> <h2 id="chapter-4-non-parametric-techniques">Chapter 4. Non-parametric Techniques</h2> <ul> <li>parametric approach :<br> pdf가 어떤 form인지 미리 알아야 함<br> e.g. \(N(\mu, \Sigma)\) 의 \(\mu\) 를 estimate<br> 하지만 실제로는 pdf prior form 모를 때가 많다 <ul> <li>해결 방법 1)<br> Non-parametric approach로 samples로부터 pdf (density \(\hat p(x) \approx p(x)\)) 를 직접 estimate<br> e.g. Parzen window<br> e.g. k-NN</li> <li>해결 방법 2)<br> 아예 posterior를 직접 estimate<br> e.g. Direct Decision Rule - Nearest-Neighbor</li> </ul> </li> </ul> <h3 id="density-estimation">Density Estimation</h3> <ul> <li>\(P(x \in R) = \int_{R} p(x)dx\) :<br> pdf \(p(x)\) 를 안다면 위의 식으로 a sample \(x\) 가 region \(R\) 안에 속할 확률을 구할 수 있다<br> But, pdf 모를 때는? <ul> <li>\(P \approx \frac{k}{n}\) <br> where \(n\) 개의 samples 중 region \(R\) 안에 속하는 samples가 \(k\)개</li> <li>For very small region \(R\),<br> \(P(x \in R) \approx \hat p(x) \cdot V\)<br> where V is volume enclosed by \(R\)</li> <li>즉, \(\hat p(x) = \frac{k/n}{V}\) is pdf estimator of \(p(x)\) <ul> <li>case 1) fixed V (volume of region \(R\) is fixed)<br> sample 수 많아지면 \(\text{lim}_{n \rightarrow \infty} k/n = P\) 로 수렴<br> So, \(\hat p(x)\) is averaged ver. of \(p(x)\)</li> <li>case 2) \(V \rightarrow 0\) (volume of region \(R\) shrinks to 0)<br> region \(R\)의 volume이 매우 작으므로 \(k \rightarrow 0\)<br> So, \(p(x)\) 는 zero에 가깝고, \(\hat p(x)\) 는 very noisy</li> <li>case 3) 실제 상황<br> sample 수 \(n\) is limited<br> volume \(V\) 는 arbitrarily small일 수 없음<br> 따라서 samples에 따라 \(\frac{k}{n}\)과 averaging by \(V\) 에 어느 정도 variance가 있음</li> </ul> </li> <li>\(\hat p(x)\) 가 \(p(x)\) 로 converge하려면 아래의 세 가지 조건 만족해야 함 <ul> <li>\(\text{lim}_{n \rightarrow \infty} V = 0\)<br> no averaging in the limit</li> <li>If \(V\) is fixed, \(\text{lim}_{n \rightarrow \infty} k = \infty\)<br> 그래야 \(\frac{k/n}{V}\) 가 probability \(p(x)\) 로 수렴<br> 만약 \(\text{lim}_{n \rightarrow \infty} k = c\) 라면 \(\text{lim}_{n \rightarrow \infty} \hat p(x) = 0\)</li> <li>\(\text{lim}_{n \rightarrow \infty} \frac{k}{n} = 0\)<br> So, \(\text{lim}_{n \rightarrow \infty} \hat p(x) = \text{lim}_{n \rightarrow \infty} \frac{k/n}{V}\) is density function</li> </ul> </li> <li>\(\text{lim}_{n \rightarrow \infty} \hat p(x) = p(x)\) 위해 <ul> <li>e.g. \(V = \frac{V_0}{\sqrt{n}}\) (Parzen window method)<br> so that \(\text{lim}_{n \rightarrow \infty} V = 0\)</li> <li>e.g. \(k = \sqrt{n}\)<br> so that \(\text{lim}_{n \rightarrow \infty} k = \infty\) and \(\text{lim}_{n \rightarrow \infty} \frac{k}{n} = 0\) and \(\text{lim}_{n \rightarrow \infty} V = \text{lim}_{n \rightarrow \infty} \frac{\sqrt{n}}{n \hat p(x)} = 0\)</li> </ul> </li> </ul> </li> </ul> <h3 id="density-estimation---parzen-window">Density Estimation - Parzen window</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/10-480.webp 480w,/assets/img/2024-09-10-Pattern/10-800.webp 800w,/assets/img/2024-09-10-Pattern/10-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/9-480.webp 480w,/assets/img/2024-09-10-Pattern/9-800.webp 800w,/assets/img/2024-09-10-Pattern/9-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <p>Let’s define Parzen window as <code class="language-plaintext highlighter-rouge">unit hypercube</code><br> \(\phi (u) = \begin{cases} 1 &amp; \text{if} &amp; | u_i | \leq \frac{1}{2} \\ 0 &amp; \text{O.W.} \end{cases}\) for \(i = 1, \ldots, d\)</p> </li> <li>\(\phi (u)\) 는 indicator function처럼 쓰여서<br> \(x\) 를 중심으로 하고 \((h_n)^d\) 의 범위를 갖는 cube 안에 들어오는 sample 개수 \(k\) 를 세는 데 사용 <ul> <li>\(k = \sum_{i=1}^n \phi (\frac{x - x_i}{h_n})\) and \(V = (h_n)^d\)<br> \(\rightarrow\)<br> \(\hat p(x) = \frac{k/n}{V} = \frac{1}{n} \sum_{i=1}^n \frac{\phi (\frac{x - x_i}{h_n})}{(h_n)^d}\) :<br> interpolated function at position \(x\) from samples \(x_i\)</li> </ul> </li> <li>Let \(\delta_{n} (x) = \frac{1}{V} \phi (\frac{x}{h_n})\) or gaussian distribution<br> Then \(\hat p(x) = \frac{1}{n} \sum_{i=1}^n \delta_{n} (x - x_i)\) <ul> <li>\(h_n\) (\(V\)) 이 클 경우 :<br> \(\delta_{n}(x)\) 의 variance가 커서<br> 이를 합친 \(\hat p(x) = \frac{1}{n} \sum_{i=1}^n \delta_{n} (x - x_i)\) 는 오히려 smoothed ver. of \(p(x)\) at \(x\)<br> (too little resolution)</li> <li>\(h_n\) (\(V\)) 이 작을 경우 :<br> \(\delta_{n}(x)\) 의 variance가 작아서<br> 이를 합친 \(\hat p(x) = \frac{1}{n} \sum_{i=1}^n \delta_{n} (x - x_i)\) 는 noisy estimate of \(p(x)\) (variation이 큼)<br> (too much statistical variation)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/11-480.webp 480w,/assets/img/2024-09-10-Pattern/11-800.webp 800w,/assets/img/2024-09-10-Pattern/11-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>만약 \(h_n \rightarrow 0\) (\(\text{lim}_{n \rightarrow \infty} V = 0\)) 이라면 <ul> <li>\(\text{lim}_{n \rightarrow \infty} \delta_{n} (x) = \delta (x)\) (Dirac delta func.)</li> <li>\(\text{lim}_{n \rightarrow \infty} E[\hat p(x)] = \text{lim}_{n \rightarrow \infty} \frac{1}{n} \sum_{i=1}^n E_{x_i}[\frac{1}{V} \phi (\frac{x - x_i}{h_n})]\)<br> \(= \text{lim}_{n \rightarrow \infty} \frac{1}{n} \cdot n \cdot \int \frac{1}{V} \phi (\frac{x - s}{h_n}) p(s) ds\)<br> \(= \text{lim}_{n \rightarrow \infty} \frac{1}{V} \phi(\frac{x}{h_n}) \ast p(x)\) by definition of convolution<br> \(= \text{lim}_{n \rightarrow \infty} \delta_{n}(x) \ast p(x)\)<br> \(= \delta (x) \ast p(x)\)<br> \(= p(x)\)</li> </ul> </li> </ul> <h3 id="density-estimation---knn-method">Density Estimation - kNN method</h3> <ul> <li>고정된 \(k_n\) 값에 대해<br> \(k_n\) nearest neighbors 찾을 때까지 \(V_n\) expand<br> \(\rightarrow\)<br> training samples가 sparse한 곳에서 \(\hat p(x) \rightarrow 0\) 인 걸 방지</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/12-480.webp 480w,/assets/img/2024-09-10-Pattern/12-800.webp 800w,/assets/img/2024-09-10-Pattern/12-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>k-NN estimation :<br> probability \(\frac{k}{n}\) 은 고정하고<br> \(k\)-개의 sample이 들어 있는 volume \(V\) 의 크기를 통해 density estimation</li> </ul> <h3 id="classification-based-on-parzen-window-and-k-nn">Classification based on Parzen-window and k-NN</h3> <ul> <li>classification based on Parzen-window method : <ul> <li>density estimate<br> \(\hat p(x) = \frac{1}{n} \sum_{i=1}^n (\frac{1}{V_n} \phi (\frac{x-x_i}{h_n}))\)</li> <li>classification<br> choose \(w_1\)<br> if \(\frac{\hat p(x | w_1)}{\hat p(x | w_2)} = \frac{\frac{1}{n_1} \sum_{i=1}^{n_1} (\frac{1}{V_{n_1}} \phi (\frac{x-x_i}{h_n}))}{\frac{1}{n_2} \sum_{i=1}^{n_2} (\frac{1}{V_{n_2}} \phi (\frac{x-x_i}{h_n}))} \gt \frac{(\lambda_{12} - \lambda_{22})P(w_2)}{(\lambda_{21} - \lambda_{11})P(w_1)}\)</li> </ul> </li> <li>classification based on k-NN method : <ul> <li>density estimate<br> \(\hat p(x) = \frac{k_n / n}{V_n}\)</li> <li>classification<br> choose \(w_1\)<br> if \(\frac{\hat p(x | w_1)}{\hat p(x | w_2)} = \frac{\frac{k_1 / n_1}{V_{n_1}}}{\frac{k_2 / n_2}{V_{n_2}}} \gt \frac{(\lambda_{12} - \lambda_{22})P(w_2)}{(\lambda_{21} - \lambda_{11})P(w_1)}\)<br> if \(\frac{V_{n_2}}{V_{n_1}} \gt \frac{n_1(\lambda_{12} - \lambda_{22})P(w_2)}{n_2(\lambda_{21} - \lambda_{11})P(w_1)}\)<br> (\(k_1 = k_2\) is fixed for k-NN)</li> </ul> </li> </ul> <h3 id="direct-estimation-of-posteriori">Direct Estimation of Posteriori</h3> <ul> <li>NNR (Nearest Neighbor Rule) : <ul> <li>Step 1)<br> estimate posteriori \(\hat P(w_i | x)\) directly from training set <ul> <li>classify하고 싶은 data \(x\) 를 중심으로 volume \(V\) 둠</li> <li>likelihood pdf \(\hat P(x | w_i) = \frac{k_i / n_i}{V}\)<br> where \(n_i\) : 총 \(n\) samples 중 class \(w_i\) 에 속하는 samples 수<br> where \(k_i\) : \(V\) 안에 속하는 \(k\) samples 중 class \(w_i\) 에 속하는 samples 수 (\(\sum_{i=1}^c k_i = k\))</li> <li>class probability \(\hat P(w) = \frac{n_i}{n}\)</li> <li> <table> <tbody> <tr> <td>joint pdf $$\hat P (x, w_{i}) = \hat P (x</td> <td>w_{i}) \hat P (w_{i}) = \frac{k_{i}/n}{V}$$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>posterior $$\hat P (w_{i}</td> <td>x) = \frac{\hat P (x, w_{i})}{\sum_{j=1}^c \hat P(x, w_{j})} = \frac{(k_{i}/n) / V}{\sum_{j=1}^c (k_{j} / n) / V} = \frac{k_{i}}{k}$$</td> </tr> </tbody> </table> </li> </ul> </li> <li>Step 2)<br> classification based on estimated \(\hat P(w_i | x)\) <ul> <li> <table> <tbody> <tr> <td>choose \(w_i\) where $$i = \text{argmax}<em>{i} \hat P (w</em>{i}</td> <td>x) = \text{argmax}<em>{i} k</em>{i}$$</td> </tr> </tbody> </table> </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/13-480.webp 480w,/assets/img/2024-09-10-Pattern/13-800.webp 800w,/assets/img/2024-09-10-Pattern/13-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>k-NNR :<br> volume \(V\) 를 고정하는 게 아니라 \(V\) 에 속하는 sample 수 \(k\) 를 고정 <ul> <li>1-NNR :<br> assign test sample \(x\) to the same class as the nearest training sample \(x^{'}\)</li> <li>3-NNR :<br> assign test sample \(x\) to the class where the nearest training samples 3개 중 2개 이상이 속한 class</li> <li>k-NNR :<br> 무승부 방지 위해 \(k\) 는 항상 홀수로 설정</li> </ul> </li> </ul> <h3 id="asymptotic-analysis-of-nnr">Asymptotic Analysis of NNR</h3> <ul> <li>Error Bound for NNR (Nearest Negibor Rule) : <ul> <li>probability of error : <ul> <li>exact :<br> \(P(e) = \int (1 - \sum_{i=1}^c P^2(w_i | x)) p(x)dx\)</li> <li>approximate :<br> if all \(c\) classes have equal probability<br> \(P(e) = 1 - \frac{1}{c}\)</li> </ul> </li> <li>error bound :<br> \(P^{\ast} \leq P(e) \leq 2 P^{\ast}\)<br> for Bayes rate \(P^{\ast}\)</li> </ul> </li> </ul> <p>증명해보자</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/14-480.webp 480w,/assets/img/2024-09-10-Pattern/14-800.webp 800w,/assets/img/2024-09-10-Pattern/14-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Asymptotic Error Rate of NNR :<br> Let \(x\) be test data<br> Let \(x^{'}\) be the nearest neighbor for 1-NNR<br> Let \(\theta^{'}\) be the labeled class of \(x^{'}\) (pred)<br> Let \(\theta\) be the true class (gt) <ul> <li>Define \(P(e)\) and \(P^{\ast}(e)\) : <ul> <li>Step 1-1)<br> \(P(e|x, x^{'}) = \sum_{i} P(\theta = w_i, \theta^{'} \neq w_i|x, x^{'}) = \sum_{i} P(\theta = w_i|x) P(\theta^{'} \neq w_i|x^{'}) = \sum_{i} P(\theta = w_i|x) (1 - P(\theta^{'} = w_i|x^{'}))\) <ul> <li>As \(n \rightarrow \infty\), \(x^{'} \rightarrow x\)<br> So, \(\text{lim}_{n \rightarrow \infty} P(\theta^{'} = w_i | x^{'}) = P(\theta = w_i | x)\)</li> </ul> </li> <li>Step 1-2)<br> \(\text{lim}_{n \rightarrow \infty} P(e | x, x^{'}) = P(e | x) = \text{lim}_{n \rightarrow \infty} \sum_{i} P(\theta = w_i | x) (1 - P(\theta^{'} = w_i | x^{'})) = \sum_{i} P(\theta = w_i | x) (1 - P(\theta = w_i | x)) = \sum_{i} P(\theta = w_i | x) - \sum_{i} (P(\theta = w_i | x))^2 = 1 - \sum_{i} (P(\theta = w_i | x))^2\)</li> <li>Step 1-3)<br> \(P(e) = \int P(e|x) p(x) dx = \int (1 - \sum_{i}^c (P(\theta = w_i | x))^2) p(x) dx\)<br> Here, \(\sum_{i}^c (P(\theta = w_i | x))^2\) is maximized when</li> <li>Step 2-1)<br> Let \(P^{\ast}(e | x)\) be the minimum value of \(P(e|x)\)<br> Then \(P^{\ast}(e | x) = 1 - P(w_m | x)\)<br> where \(P(w_m | x) = \text{max}_{i} P(w_i | x)\)</li> <li>Step 2-2)<br> \(P^{\ast}(e)\) :<br> \(P^{\ast}(e) = \int P^{\ast}(e | x) p(x) dx = \int (1 - P(w_m | x)) p(x) dx\)</li> </ul> </li> <li>Lower Bound of \(P(e)\) : <ul> <li>정의한대로<br> \(P^{\ast}(e) \leq P(e)\)</li> </ul> </li> <li>Upper Bound of \(P(e)\) : <ul> <li>\(P(e)\) is maximized<br> (\(\sum_{i}^c (P(\theta = w_i | x))^2\) is minimized)<br> when 가장 높은 확률의 class \(w_m\) 빼고는 posterior 확률이 같을 때<br> as \(P(w_i | x) = \begin{cases} \frac{P^{\ast}(e|x)}{c-1} &amp; \text{if} &amp; i \neq m \\ 1 - P^{\ast}(e|x) &amp; \text{if} &amp; i = m \end{cases}\)</li> <li>Lower Bound of \(\sum_{i}^c (P(\theta = w_i | x))^2\) :<br> \(\sum_{i}^c (P(\theta = w_i | x))^2 = P^2(w_m | x) + \sum_{i=1, i \neq m}^c P^2(w_i | x)\)<br> \(\geq 1 - P^{\ast}(e|x) + (c-1)(\frac{P^{\ast}(e|x)}{c-1})^2 = 1 - 2 P^{\ast}(e|x) + \frac{c}{c-1} (P^{\ast}(e|x))^2\)<br> So,<br> \(1 - \sum_{i}^c (P(\theta = w_i | x))^2 \leq 2 P^{\ast}(e|x) - \frac{c}{c-1} (P^{\ast}(e|x))^2 = P^{\ast}(e|x) (2 - \frac{c}{c-1} P^{\ast}(e|x))\)</li> <li>Upper Bound of \(P(e)\) :<br> \(P(e) = \int P(e|x) p(x) dx = \int (1 - \sum_{i}^c (P(\theta = w_i | x))^2) p(x) dx \leq \int P^{\ast}(e|x) (2 - \frac{c}{c-1} P^{\ast}(e|x)) p(x) dx = P^{\ast}(e)(2 - \frac{c}{c-1} P^{\ast}(e))\)</li> </ul> </li> <li>Error Bounds of \(P(e)\) :<br> \(P^{\ast}(e) \leq P(e) \leq P^{\ast}(e)(2 - \frac{c}{c-1} P^{\ast}(e))\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/15-480.webp 480w,/assets/img/2024-09-10-Pattern/15-800.webp 800w,/assets/img/2024-09-10-Pattern/15-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="chapter-5-linear-discriminant-functions">Chapter 5. Linear Discriminant Functions</h2> <ul> <li>pattern recognition : <ul> <li>Chapter 2. Bayes Decision theory :<br> choose class whose posterior is maximized<br> \(\rightarrow\) likelihood ratio와 threshold를 비교<br> \(\rightarrow\) likelihood가 Gaussian pdf일 때의 Discriminant Functions 다룸<br> (\(\Sigma_{i}\) 가 어떤 값이냐에 따라 linear or quadratic)</li> <li>Chapter 3. Parametric approach :<br> PDF form 아는 상태에서 estimate parameters<br> e.g. MLE or MAP</li> <li>Chapter 4. Non-parametric approach :<br> PDF form 모르는 상태에서 estimate density<br> e.g. Parzen-window or k-NN or direct estimation (NNR)</li> <li>Chapter 5. Linear Discriminant Function :<br> discriminant function form 아는 상태에서 estimate discriminant func. parameters<br> (PDF form 몰라도 됨 \(\rightarrow\) Non-parametric approach)</li> </ul> </li> </ul> <h3 id="linear-discriminant-function">Linear Discriminant Function</h3> <ul> <li>Two-category case : <ul> <li>Decision Boundary by Linear Discriminant Function :<br> \(g(\boldsymbol x) = \boldsymbol w^T \boldsymbol x + w_0 = 0\)</li> <li>point \(\boldsymbol x = \boldsymbol x_{p} + r \frac{\boldsymbol w}{\| \boldsymbol w \|}\)<br> where \(\boldsymbol x_{p}\) : projection of \(x\) onto Decision Surface<br> where \(r\) : signed distance</li> <li>signed distance :<br> \(r = \frac{g(\boldsymbol x)}{\| \boldsymbol w \|} = \frac{\boldsymbol w^T \boldsymbol x + w_0}{\| \boldsymbol w \|}\)<br> \(r \gt 0\) : point \(x\) is on positive side (class \(w_1\))<br> \(r \lt 0\) : point \(x\) is on negative side (class \(w_2\))<br> e.g. 원점에서의 거리 : \(r = \frac{w_0}{\| \boldsymbol w \|}\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/16-480.webp 480w,/assets/img/2024-09-10-Pattern/16-800.webp 800w,/assets/img/2024-09-10-Pattern/16-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Multi-category case : <ul> <li>\(c\)-개의 Linear Discriminant Function :<br> \(g_{i}(\boldsymbol x) = \boldsymbol w_{i}^{T} \boldsymbol x + w_{i0}\) for \(i=1, \ldots, c\)</li> <li>Decision Rule :<br> class \(w_i\) if \(g_{i}(\boldsymbol x) \gt g_{j}(\boldsymbol x)\) for all \(j \neq i\)</li> <li>\(c\)-개의 Decision Boundary by Linear Discriminant Function :<br> \(g_{i}(\boldsymbol x) = g_{j}(\boldsymbol x)\) for adjacent \(i, j\)<br> \(\rightarrow\) \(\boldsymbol w_{i}^{T} \boldsymbol x + w_{i0} = \boldsymbol w_{j}^{T} \boldsymbol x + w_{j0}\)<br> \(\rightarrow\) \((\boldsymbol w_{i} - \boldsymbol w_{j})^{T} \boldsymbol x + (w_{i0} - w_{j0}) = 0\)<br> \(\rightarrow\) \(\boldsymbol w^{T} \boldsymbol x + w_0 = 0\)<br> where \(\boldsymbol w = \boldsymbol w_{i} - \boldsymbol w_{j}\) and \(w_0 = w_{i0} - w_{j0}\) <ul> <li>\(\boldsymbol w_{i} - \boldsymbol w_{j}\) :<br> Decision Boundary(Hyperplane)와 수직인 vector</li> <li>\(r = \frac{g_{i}(\boldsymbol x) - g_{j}(\boldsymbol x)}{\| \boldsymbol w_{i} - \boldsymbol w_{j} \|}\) :<br> signed distance from \(\boldsymbol x\) to Decision Boundary</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/17-480.webp 480w,/assets/img/2024-09-10-Pattern/17-800.webp 800w,/assets/img/2024-09-10-Pattern/17-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/17.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="general-discriminant-function">General Discriminant Function</h3> <ul> <li>Discriminant Function : <ul> <li>Linear Discriminant Function :<br> \(g(\boldsymbol x) = \boldsymbol w^T \boldsymbol x + w_0 = \sum_{i=1}^d w_{i} x_{i} + w_0\)<br> e.g. likelihood \(P(x|w_i)\) 가 Gaussian PDF이고 \(\Sigma_{i}\) 가 \(\sigma^{2} I\) 또는 \(\Sigma\) 일 때<br> (본 포스팅 Discriminant Function for Gaussian PDF 참고)</li> <li>Quadratic Discriminant Function :<br> \(g(\boldsymbol x) = \sum_{i=1}^d \sum_{j=1}^d w_{ij} x_{i} x_{j} + \sum_{i=1}^d w_{i} x_{i} + w_0\)<br> e.g. likelihood \(P(x|w_i)\) 가 Gaussian PDF이고 \(\Sigma_{i}\) 가 arbitrary일 때<br> (본 포스팅 Discriminant Function for Gaussian PDF 참고)</li> <li>Polynomial Discriminant Function :<br> \(g(\boldsymbol x) = \sum_{i=1}^{\hat d} a_i y_i(\boldsymbol x) = \boldsymbol a^{T} \boldsymbol y\)<br> where \(\hat d\) : arbitrary function \(y_{i}(x)\) 개수<br> e.g. 2D-to-3D : \((x_1, x_2) \rightarrow \boldsymbol y = (x_1, x_2, x_1 x_2)\)</li> </ul> </li> <li>Two-category case (Linearly Separable) : <ul> <li>Decision Rule :<br> \(g(\boldsymbol x) = \boldsymbol a^{T} \boldsymbol y\)<br> \(\gt 0\) : class \(w_1\)<br> \(\lt 0\) : class \(w_2\)</li> <li>Solution Region of \(\boldsymbol a\) :<br> training point vector와 수직인 vector들을 이용해서<br> solution region 찾은 뒤<br> decision boundary \(g(\boldsymbol x) = \boldsymbol a^{T} \boldsymbol y = 0\) 와 수직인 \(\boldsymbol a\) 는 해당 solution region에 속함</li> <li>Normalize :<br> 이 때, class \(w_2\) 에 속하는 training points에 (-)를 곱해서 normalize 할 수 있음<br> s.t. \(\boldsymbol a^{T} \boldsymbol y \gt 0\) for both \(w_1\) and \(w_2\)</li> <li>Solution Region of \(\boldsymbol a\) with margin \(b\) :<br> Normalize했을 때<br> \(\boldsymbol a^{T} \boldsymbol y \gt 0\) for both \(w_1\) and \(w_2\)<br> 대신<br> \(\boldsymbol a^{T} \boldsymbol y \gt b \gt 0\) for both \(w_1\) and \(w_2\) 이도록<br> margin을 두면<br> solution region은 각 방향으로 \(\frac{b}{\| y_i \|}\) 만큼 줄어듬</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/18-480.webp 480w,/assets/img/2024-09-10-Pattern/18-800.webp 800w,/assets/img/2024-09-10-Pattern/18-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/18.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> class w_1 의 training points 2개와 class w_2 의 training points 2개가 있을 때, Solution Region 찾기 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/19-480.webp 480w,/assets/img/2024-09-10-Pattern/19-800.webp 800w,/assets/img/2024-09-10-Pattern/19-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/19.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> margin 뒀을 때, Solution Region 찾기 </div> <h3 id="gradient-descent">Gradient Descent</h3> <ul> <li>Gradient Descent : <ul> <li>Solution Region 내에 있는, discriminant function \(\boldsymbol a^{T} \boldsymbol y\) 의 \(\boldsymbol a\) 를 찾기 위해<br> iteratively \(\boldsymbol a\) 를 업데이트</li> <li>\(\boldsymbol a(k+1) = \boldsymbol a(k) - \eta (k) \cdot \frac{\partial J(\boldsymbol a(k))}{\partial \boldsymbol a(k)}\)<br> where \(\eta (k)\) is learning-rate<br> where \(J(\boldsymbol a)\) is criterion(loss) func.</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/20-480.webp 480w,/assets/img/2024-09-10-Pattern/20-800.webp 800w,/assets/img/2024-09-10-Pattern/20-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/20.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Gradient Descent : <ul> <li>Loss :<br> Normalize했을 경우, learnable weight vector \(\boldsymbol a\) 에 대해<br> well-classified sample은 \(\boldsymbol a^{T} \boldsymbol y \gt 0\) 이고,<br> misclassified sample은 \(\boldsymbol a^{T} \boldsymbol y \lt 0\) 이므로<br> \(J(\boldsymbol a) = \Sigma_{\boldsymbol y \in Y(\boldsymbol a)} (- \boldsymbol a^{T} \boldsymbol y)\)<br> where \(Y(\boldsymbol a)\) : weight \(\boldsymbol a\) 가 잘못 분류한 sample들을 모아놓은 set <ul> <li>continuous and piece-wise linear<br> (연속이지만, misclassified sample 수 바뀌는 지점에서 미분 불가능)</li> <li>\(J(\boldsymbol a) \geq 0\)<br> (Normalize했을 경우)</li> </ul> </li> <li>Gradient :<br> \(\nabla J(\boldsymbol a(k)) = \frac{\partial J(\boldsymbol a(k))}{\partial \boldsymbol a} = \Sigma_{\boldsymbol y \in Y(\boldsymbol a)} (- \boldsymbol y)\)</li> <li>Update :<br> \(\boldsymbol a(k+1) = \boldsymbol a(k) - \eta(k) \cdot \nabla J(\boldsymbol a(k)) = \boldsymbol a(k) + \eta(k) \cdot \Sigma_{\boldsymbol y \in Y(\boldsymbol a)} (\boldsymbol y)\)<br> where \(\boldsymbol y \in Y(\boldsymbol a)\) : batch <ul> <li>sample/batch :<br> 하나씩 update하면 sample-by-sample relaxation<br> 묶어서 update하면 batch relaxation</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/21-480.webp 480w,/assets/img/2024-09-10-Pattern/21-800.webp 800w,/assets/img/2024-09-10-Pattern/21-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/21.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> 원래 분홍색 boundary일 때는 y가 아래에 있어서 misclassified이었는데, 초록색 boundary로 업데이트하면 y가 위에 있어서 well-classified </div> <ul> <li>이외의 Loss : <ul> <li>misclassified sample 수를 loss로 두면<br> piece-wise constant여서 gradient descent에 부적합</li> <li>\(\Sigma_{\boldsymbol y \in Y(\boldsymbol a)} (- \boldsymbol a^{T} \boldsymbol y)\) 를 loss로 두면<br> piece-wise linear여서 gradient descent 가능</li> <li>\(\Sigma_{\boldsymbol y \in Y(\boldsymbol a)} (- \boldsymbol a^{T} \boldsymbol y)^2\) 를 loss로 두면 <ul> <li>장점 : continuous gradient를 가져서 smooth loss landscape를 가지고 gradient descent에 적합</li> <li>단점 : <ul> <li>solution region 내 optimal weight vector \(\boldsymbol a\) 를 찾아야 하는데<br> solution region의 경계 쪽이 smooth해서 경계 쪽으로 converge할 수 있음</li> <li>dominated by the longest misclassified sample vector (\(\| \boldsymbol y \|\) 값이 매우 큰 경우)</li> </ul> </li> <li>해결 :<br> \(\| \boldsymbol y \|\) 값이 너무 커지는 걸 방지하는 relaxation procedure<br> (아래에서 설명)</li> </ul> </li> <li>\(\boldsymbol a^{T} \boldsymbol y\) 대신 \(\boldsymbol a^{T} \boldsymbol y - b\) 로 margin을 두면<br> better</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/22-480.webp 480w,/assets/img/2024-09-10-Pattern/22-800.webp 800w,/assets/img/2024-09-10-Pattern/22-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/22.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Relaxation procedure : <ul> <li>Loss :<br> \(J(\boldsymbol a) = \frac{1}{2} \Sigma_{\boldsymbol y \in Y(\boldsymbol a)} \frac{(\boldsymbol a^{T} \boldsymbol y - b)^2}{\| \boldsymbol y \|^{2}}\) <ul> <li>penalize misclassified samples \(\boldsymbol y \in Y(\boldsymbol a)\) where \(\boldsymbol a^{T} \boldsymbol y - b \lt 0\) with margin \(b\)</li> </ul> </li> <li>Gradient :<br> \(\nabla J(\boldsymbol a) = \frac{\partial J(\boldsymbol a)}{\partial \boldsymbol a} = \Sigma_{\boldsymbol y \in Y(\boldsymbol a)} \frac{\boldsymbol a^{T} \boldsymbol y - b}{\| \boldsymbol y \|^{2}} \boldsymbol y\)</li> <li>Update : <ul> <li>\(\boldsymbol a(k+1) = \boldsymbol a(k) - \eta(k) \cdot \nabla J(\boldsymbol a) = \boldsymbol a(k) + \eta(k) \cdot \Sigma_{\boldsymbol y \in Y(\boldsymbol a)} \frac{b - \boldsymbol a^{T}(k) \boldsymbol y}{\| \boldsymbol y \|^{2}} \boldsymbol y = \boldsymbol a(k) + \eta(k) \cdot \Sigma_{\boldsymbol y \in Y(\boldsymbol a)} r(k) \cdot \frac{\boldsymbol y}{\| \boldsymbol y \|}\)<br> where \(r(k) = \frac{b - \boldsymbol a^{T}(k) \boldsymbol y}{\| \boldsymbol y \|}\) : distance from \(\boldsymbol a (k)\) to hyperplane \(\boldsymbol a^{T} \boldsymbol y - b = 0\)<br> where \(\frac{\boldsymbol y}{\| \boldsymbol y \|}\) : unit normal vector of hyperplane \(\boldsymbol a^{T} \boldsymbol y - b = 0\)</li> <li>위 update 식 양변에 \(\boldsymbol y\) 를 곱해서 정리하면<br> \(\boldsymbol a^{T}(k+1) \boldsymbol y - b = (1 - \eta(k)) (\boldsymbol a^{T}(k) \boldsymbol y - b)\)</li> <li>geometric 해석 :<br> \(\boldsymbol a(k+1)\) 은 \(\boldsymbol a(k)\) 를 \(\frac{\boldsymbol y}{\| \boldsymbol y \|}\) 방향으로 (hyperplane에 가까워지게)<br> \(\eta(k) r(k)\) 거리만큼 이동 <ul> <li>\(\eta \lt 1\) : under-relaxation<br> 완화, but still misclassified (\(\boldsymbol a^{T} \boldsymbol y - b \lt 0\))</li> <li>\(\eta = 1\) : relaxed<br> \(\boldsymbol a(k)\) 가 hyperplane touch (\(\boldsymbol a^{T} \boldsymbol y - b = 0\))</li> <li>\(1 \lt \eta \lt 2\) : over-relaxation<br> overshooting이긴 하지만 결국 \(\boldsymbol a (k+1)\) converge</li> </ul> </li> </ul> </li> <li>Stop : <ul> <li> \[\| \boldsymbol a(k+1) - \boldsymbol a(k) \| \lt \epsilon\] </li> <li> \[k \leq k_{max}\] </li> <li>\(J(\boldsymbol a(k)) \leq J_{T}\) since we minimize \(J(\boldsymbol a(k))\)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/29-480.webp 480w,/assets/img/2024-09-10-Pattern/29-800.webp 800w,/assets/img/2024-09-10-Pattern/29-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/29.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/30-480.webp 480w,/assets/img/2024-09-10-Pattern/30-800.webp 800w,/assets/img/2024-09-10-Pattern/30-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/30.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Non-separable behavior :<br> If \(\eta(k) \rightarrow 0\) as \(k \rightarrow \infty\),<br> non-separable problems에서도 괜찮은 performance <code class="language-plaintext highlighter-rouge">왜???</code> <ul> <li>\(\eta(k)\) 가 너무 빨리 0이 되면<br> 덜 배워서 prematurely converge (less than optimal result)</li> <li>\(\eta(k)\) 가 너무 느리게 0이 되면<br> non-separable하게 만드는 training samples에 너무 민감</li> <li>\(\eta(k) = \frac{\eta(1)}{k}\) 로 설정하여<br> \(k\) 증가 (performance 증가)할수록 learning rate \(\eta(k)\) 감소</li> </ul> </li> </ul> <h3 id="least-squared-error-solution">Least Squared-Error Solution</h3> <ul> <li>Least Squared-Error Solution : <ul> <li>two-category :<br> \(J(\boldsymbol a) = \| \boldsymbol Y \boldsymbol a - \boldsymbol b \|^{2}\)<br> where \(\boldsymbol Y = \begin{bmatrix} \boldsymbol 1 &amp; X_1 \\ - \boldsymbol 1 &amp; - X_2 \end{bmatrix}\) : \(n \times (d+1)\) sample matrix<br> where \(\boldsymbol a = \begin{bmatrix} w_0 \\ \boldsymbol w \end{bmatrix}\) : \((d+1) \times 1\) weight vector<br> where \(\boldsymbol b\) : \(n \times 1\) teacher(margin) vector <ul> <li>optmize :<br> \(\nabla_{\boldsymbol a} J(\boldsymbol a) = 2 \boldsymbol Y^{T} (\boldsymbol Y \boldsymbol a - \boldsymbol b) = 0\)<br> \(\rightarrow\)<br> optimal \(\boldsymbol a = (\boldsymbol Y^{T} \boldsymbol Y)^{-1} \boldsymbol Y^{T} \boldsymbol b = \boldsymbol Y^{\dagger} \cdot \boldsymbol b\)<br> (one-shot solution)<br> where \(\boldsymbol Y^{\dagger} = (\boldsymbol Y^{T} \boldsymbol Y)^{-1} \boldsymbol Y^{T}\) : pseudo-inverse of \(\boldsymbol Y\)</li> </ul> </li> <li>multi-category :<br> \(J(\boldsymbol a) = \| \boldsymbol Y \boldsymbol A - \boldsymbol B \|^{2}\)<br> where \(\boldsymbol Y\) : \(n \times (d+1)\) data matrix<br> where \(\boldsymbol A\) : \((d+1) \times C\) weight vector<br> where \(\boldsymbol B\) : \(n \times C\) teacher(margin) vector <ul> <li>optimize :<br> optimal \(\boldsymbol A = \boldsymbol Y^{\dagger} \cdot \boldsymbol B\)<br> where \(\boldsymbol Y^{\dagger} = (\boldsymbol Y^{T} \boldsymbol Y)^{-1} \boldsymbol Y^{T}\) : pseudo-inverse of \(\boldsymbol Y\)</li> </ul> </li> </ul> </li> <li> <p>이 때, \(\boldsymbol Y^{\dagger} = (\boldsymbol Y^{T} \boldsymbol Y)^{-1} \boldsymbol Y^{T}\) 에서<br> \(\boldsymbol Y^{T} \boldsymbol Y\) 가 singular여서 또는 large matrix 연산 다루기 버거워서<br> \((\boldsymbol Y^{T} \boldsymbol Y)^{-1}\) 를 직접 계산하기 어렵다면<br> 아래와 같이 iteratively update by gradient descent</p> </li> <li>Least Squared-Error Solution (Widrow-Hoff) : <ul> <li>Loss :<br> \(J(\boldsymbol a, \boldsymbol b) = \| \boldsymbol Y \boldsymbol a - \boldsymbol b \|^{2}\)</li> <li>Gradient :<br> \(\frac{\partial J(\boldsymbol a, \boldsymbol b)}{\partial \boldsymbol a} = 2 \boldsymbol Y^{T} (\boldsymbol Y \boldsymbol a - \boldsymbol b)\)<br> \(\frac{\partial J(\boldsymbol a, \boldsymbol b)}{\partial \boldsymbol b} = -2 (\boldsymbol Y \boldsymbol a - \boldsymbol b)\)</li> <li>Stop : <ul> <li>updated only when \(\boldsymbol Y \boldsymbol a - \boldsymbol b \lt 0\) to keep \(\boldsymbol b\) as positive margin</li> <li> \[\| J(\boldsymbol a(k+1), \boldsymbol b(k+1)) - J(\boldsymbol a(k), \boldsymbol b(k)) \| \lt \epsilon\] </li> <li> \[k \leq k_{max}\] </li> <li>\(J(\boldsymbol a(k), \boldsymbol b(k)) \leq J_{T}\) since we minimize \(J\)</li> </ul> </li> </ul> </li> <li>Least Squared-Error와 Fisher’s Linear Discriminant Function의 관계 : <ul> <li>\(J(\boldsymbol a) = \| \boldsymbol Y \boldsymbol a - \boldsymbol b \|^{2}\)<br> where \(\boldsymbol Y = \begin{bmatrix} \boldsymbol 1 &amp; X_1 \\ - \boldsymbol 1 &amp; - X_2 \end{bmatrix}\) : \(n \times (d+1)\) data matrix<br> where \(\boldsymbol a = \begin{bmatrix} w_0 \\ \boldsymbol w \end{bmatrix}\) : \((d+1) \times 1\) weight vector<br> where \(\boldsymbol b = \begin{bmatrix} \frac{n}{n_1} \boldsymbol 1 \\ \frac{n}{n_2} \boldsymbol 1 \end{bmatrix}\) : \(n \times 1\) teacher(margin) vector<br> (\(\boldsymbol b\) 를 위와 같이 설정함으로써 LSE와 Fisher’s LDA의 Relation 설명 가능!)</li> <li>\(\boldsymbol Y \boldsymbol a = \boldsymbol b\)<br> \(\boldsymbol Y^T \boldsymbol Y \boldsymbol a = \boldsymbol Y^T \boldsymbol b\)<br> \(\begin{bmatrix} \boldsymbol 1^T &amp; - \boldsymbol 1^T \\ X_{1}^{T} &amp; - X_{2}^{T} \end{bmatrix} \begin{bmatrix} \boldsymbol 1 &amp; X_1 \\ - \boldsymbol 1 &amp; - X_2 \end{bmatrix} \begin{bmatrix} w_0 \\ \boldsymbol w \end{bmatrix} = \begin{bmatrix} \boldsymbol 1^T &amp; - \boldsymbol 1^T \\ X_{1}^{T} &amp; - X_{2}^{T} \end{bmatrix} \begin{bmatrix} \frac{n}{n_1} \boldsymbol 1 \\ \frac{n}{n_2} \boldsymbol 1 \end{bmatrix}\)<br> \(\begin{bmatrix} n_{1} + n_{2} &amp; (n_{1} \boldsymbol m_{1} + n_{2} \boldsymbol m_{2})^{T} \\ n_{1} \boldsymbol m_{1} + n_{2} \boldsymbol m_{2} &amp; S_w + n_{1} \boldsymbol m_{1} \boldsymbol m_{1}^{T} + n_{2} \boldsymbol m_{2} \boldsymbol m_{2}^{T} \end{bmatrix} \begin{bmatrix} w_0 \\ \boldsymbol w \end{bmatrix} = \begin{bmatrix} \frac{n}{n_1} n_{1} - \frac{n}{n_2} n_{2} \\ \frac{n}{n_1} n_1 \boldsymbol m_{1} - \frac{n}{n_2} n_2 \boldsymbol m_{2} \end{bmatrix}\)<br> \(\begin{bmatrix} n &amp; (n \boldsymbol m)^{T} \\ n \boldsymbol m &amp; S_w + n_{1} \boldsymbol m_{1} \boldsymbol m_{1}^{T} + n_{2} \boldsymbol m_{2} \boldsymbol m_{2}^{T} \end{bmatrix} \begin{bmatrix} w_0 \\ \boldsymbol w \end{bmatrix} = \begin{bmatrix} 0 \\ n(\boldsymbol m_{1} - \boldsymbol m_{2}) \end{bmatrix} \cdots (\ast)\)<br> <code class="language-plaintext highlighter-rouge">???</code> 가장 왼쪽 matrix의 (2, 2) 항이 왜 저렇게 나옴 <code class="language-plaintext highlighter-rouge">???</code> </li> <li>\(\ast\) 의 first row를 정리하면<br> \(w_0 = - \boldsymbol m^{T} \boldsymbol w\)<br> where \(\boldsymbol m = \frac{n_{1} \boldsymbol m_{1} + n_{2} \boldsymbol m_{2}}{n}\) : mean of all samples</li> <li>위의 식과 \(\boldsymbol m = \frac{n_{1} \boldsymbol m_{1} + n_{2} \boldsymbol m_{2}}{n}\) 를 이용해서<br> \(\ast\) 의 second row를 정리하면<br> \((\frac{1}{n} S_w + \frac{n_{1} n_{2}}{n^{2}} (\boldsymbol m_{1} - \boldsymbol m_{2}) (\boldsymbol m_{1} - \boldsymbol m_{2})^{T}) \cdot \boldsymbol w = \boldsymbol m_{1} - \boldsymbol m_{2}\)<br> \(\frac{1}{n} S_w \cdot \boldsymbol w + \frac{n_{1} n_{2}}{n^{2}} (\boldsymbol m_{1} - \boldsymbol m_{2}) (\boldsymbol m_{1} - \boldsymbol m_{2})^{T} \cdot \boldsymbol w = \boldsymbol m_{1} - \boldsymbol m_{2}\)<br> \(\frac{1}{n} S_w \cdot \boldsymbol w = (1 - \alpha) (\boldsymbol m_{1} - \boldsymbol m_{2})\)<br> where constant \(\alpha = \frac{n_{1} n_{2}}{n^{2}} k\)<br> where constant \(k = (\boldsymbol m_{1} - \boldsymbol m_{2})^{T} \cdot \boldsymbol w = (1 \times d)(d \times 1) = (1 \times 1)\)<br> So, \(\boldsymbol w = (1 - \alpha) n S_{w}^{-1} (\boldsymbol m_{1} - \boldsymbol m_{2})\)<br> 이는 Fisher’s LDA (\(\boldsymbol w = q S_{w}^{-1} (\boldsymbol m_{1} - \boldsymbol m_{2})\)) 와 same!!</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/31-480.webp 480w,/assets/img/2024-09-10-Pattern/31-800.webp 800w,/assets/img/2024-09-10-Pattern/31-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/31.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>discriminant function : <ul> <li>Bayes linear discriminant function :<br> \(g_{opt} = P(w_{1} | \boldsymbol x) - P(w_{2} | \boldsymbol x)\)<br> \(P(w_{1} | \boldsymbol x) + P(w_{2} | \boldsymbol x) = 1\)<br> \(P(w_{1} | \boldsymbol x) = \frac{1 + g_{opt}}{2}\) and \(P(w_{2} | \boldsymbol x) = \frac{1 - g_{opt}}{2}\)</li> <li>LS-based linear discriminant function :<br> \(g = \boldsymbol a^{T} \boldsymbol y\) <ul> <li>If \(\boldsymbol b = \boldsymbol 1_{n}\),<br> \(J_{s}(\boldsymbol a) = \sum_{\boldsymbol y \in D_{1}} (\boldsymbol a^{T} \boldsymbol y - 1)^{2} + \sum_{\boldsymbol y \in D_{2}} (\boldsymbol a^{T} \boldsymbol y + 1)^{2}\)<br> \(\rightarrow\)<br> \(J(\boldsymbol a) = \text{lim}_{n \rightarrow \infty} \frac{1}{n} J_{s}(\boldsymbol a) = \cdots = P(w_1) E_{1}[(\boldsymbol a^{T} \boldsymbol y - 1)^{2}] + P(w_2) E_{2}[(\boldsymbol a^{T} \boldsymbol y + 1)^{2}] = \cdots = \int (\boldsymbol a^{T} \boldsymbol y - g_{opt}(\boldsymbol x))^{2} p(\boldsymbol x) dx + 1 - \int g_{opt}^{2}(\boldsymbol x) p(\boldsymbol x) dx = \epsilon^{2} + 1 - \int g_{opt}^{2}(\boldsymbol x) p(\boldsymbol x) dx\)<br> where MSE \(\epsilon^{2} = \int (g - g_{opt})^{2} p(\boldsymbol x) dx\)<br> \(\rightarrow\) <br> \(1 - \int g_{opt}^{2}(\boldsymbol x) p(\boldsymbol x) dx\) 는 \(\boldsymbol a\) 와 무관한 term이기 때문에<br> optimal \(\boldsymbol a = \text{argmin}_{\boldsymbol a} J(\boldsymbol a) = \text{argmin}_{\boldsymbol a} \epsilon^{2}\)</li> </ul> </li> <li>정리하면,<br> \(\boldsymbol b = \boldsymbol 1_{n}\) and \(n \rightarrow \infty\) 일 때<br> optimal \(\boldsymbol a = \boldsymbol Y^{\dagger} \boldsymbol 1_{n}\) 에 대해<br> LS-based linear discrimant function \(g(\boldsymbol x) = \boldsymbol a^{T} \boldsymbol y\) 은<br> Bayes linear discrimant function \(g_{opt}(\boldsymbol x) = P(w_{1} | \boldsymbol x) - P(w_{2} | \boldsymbol x)\) 을<br> approx.한다</li> <li>remark : <ul> <li>decision boundary \(g_{opt}\) 근처에 있는 samples보다는 <code class="language-plaintext highlighter-rouge">???</code><br> high probability \(p(\boldsymbol x)\) 를 가지는 samples에 가중치를 두어 LS-based solution이 Bayes에 가까워지도록 함</li> <li>\(g = \boldsymbol a^{T} \boldsymbol y\) 에서 \(\boldsymbol x\) 의 transform (\(\boldsymbol y\)) 를 얼마나 잘 선택하느냐에 따라 approx. 퀄이 달라짐</li> <li>linear discriminant function이라서 상황에 따라 \(g_{opt}\) 도 poor decision boundary를 가질 수 있음</li> </ul> </li> </ul> </li> </ul> <h3 id="linear-support-vector-machine-svm">Linear Support Vector Machine (SVM)</h3> <ul> <li>decision rule : <ul> <li>discriminant function :<br> discriminant function \(g(\boldsymbol x) = \text{sign}(\boldsymbol w^{T} \boldsymbol x + b)\)</li> <li>decision boundary :<br> decision boundary \(\boldsymbol w^{T} \boldsymbol x + b = 0\) that maximizes the margin</li> <li>margin :<br> margin \(\text{argmin}_{\boldsymbol x \in D} d(\boldsymbol x) = \text{argmin}_{\boldsymbol x in D} \frac{| \boldsymbol w^{T} \boldsymbol x + b |}{\| \boldsymbol w \|}\)<br> where margin은 support vectors(extreme vectors)가 결정</li> <li>goal :<br> \(\text{argmax}_{\boldsymbol w, b} \text{margin} = \text{argmax}_{\boldsymbol w, b} \text{argmin}_{\boldsymbol x \in D} d(\boldsymbol x) = \text{argmax}_{\boldsymbol w, b} \text{argmin}_{\boldsymbol x \in D} \frac{| \boldsymbol w^{T} \boldsymbol x + b |}{\| \boldsymbol w \|}\)<br> TBD</li> </ul> </li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="disqus_thread" style="max-width: 800px; margin: 0 auto;"> <script type="text/javascript">var disqus_shortname="semyeong-yu",disqus_identifier="/blog/2024/Pattern",disqus_title="EE534 Pattern Recognition Midterm";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Semyeong Yu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5YMLX9VHFX"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5YMLX9VHFX");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>