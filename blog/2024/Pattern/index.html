<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> EE534 Pattern Recognition | Semyeong Yu </title> <meta name="author" content="Semyeong Yu"> <meta name="description" content="Lecture Summary (24F)"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://semyeong-yu.github.io/blog/2024/Pattern/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "EE534 Pattern Recognition",
            "description": "Lecture Summary (24F)",
            "published": "September 10, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Semyeong</span> Yu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>EE534 Pattern Recognition</h1> <p>Lecture Summary (24F)</p> </d-title> <d-article> <blockquote> <p>Lecture :<br> 24F EE534 Pattern Recognition<br> by KAIST Munchurl Kim <a href="https://www.viclab.kaist.ac.kr/" rel="external nofollow noopener" target="_blank">VICLab</a></p> </blockquote> <h2 id="chapter-1-overview">Chapter 1. Overview</h2> <h3 id="discriminative-vs-generative">Discriminative vs Generative</h3> <ul> <li>Discriminative model : <ul> <li> <table> <tbody> <tr> <td>learn $$P(Y</td> <td>X)\(to maximize\)P(Y</td> <td>X)$$ directly</td> </tr> </tbody> </table> </li> <li>e.g. logistic regression, SVM, nearest neighbor, CRF, Decision Tree and Random Forest, traditional NN</li> </ul> </li> <li>Generative model : <ul> <li> <table> <tbody> <tr> <td>learn $$P(X</td> <td>Y)\(and\)P(Y)\(to maximize\)P(X, Y) = P(X</td> <td>Y)P(Y)$$</td> </tr> <tr> <td>where can learn $$P(Y</td> <td>X) \propto P(X</td> <td>Y)P(Y)$$ indirectly</td> </tr> </tbody> </table> </li> <li>e.g. Bayesian network, Autoregressive model, GAN, Diffuson model</li> </ul> </li> </ul> <h2 id="chapter-2-bayes-decision-theory">Chapter 2. Bayes Decision Theory</h2> <h3 id="bayes-decision-rule">Bayes Decision Rule</h3> <ul> <li>conditional probability density :<br> Let \(w\) be state (class)<br> Let \(x\) be data (continous-valued sample) <ul> <li>prior : \(P(w=w_k)\)</li> <li> <table> <tbody> <tr> <td>likelihood : pdf $$P(x</td> <td>w_k)$$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>posterior : $$ P(w_k</td> <td>x) = \frac{P(x</td> <td>w_k)P(w_k)}{P(x)} $$ (Bayes Rule)</td> <td> </td> </tr> <tr> <td>where $$ P(w_1</td> <td>x) + P(w_2</td> <td>x) + \cdots + P(w_N</td> <td>x) = 1 $$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>evidence : $$ P(x) = \sum_{k=1}^N P(x</td> <td>w_k)P(w_k) = \sum_{k=1}^N P(x, w_k) $$</td> </tr> </tbody> </table> </li> </ul> </li> <li>Bayes Decision Rule :<br> posterior 더 큰 쪽 고름! <ul> <li>Two-class (\(w_1, w_2\)) problem :<br> choose \(w_1\)<br> if \(P(w_1 | x) \gt P(w_2 | x)\)<br> if \(P(x|w_1)P(w_1) \gt P(x|w_2)P(w_2)\)<br> if \(\frac{P(x|w_1)}{P(x|w_2)} \gt \frac{P(w_2)}{P(w_1)}\)<br> (likehood ratio \(\gt\) threshold)</li> <li>multi-class problem :<br> choose \(w_i\) where \(P(w_i | x)\) is the largest</li> </ul> </li> </ul> <h3 id="minimum-error">minimum error</h3> <ul> <li>minimum error :<br> GT가 \(w_1, w_2\) 이고, Predicted가 \(R_1, R_2\) 일 때, <ul> <li>\(P(error) = \int_{-\infty}^{\infty} P(error, x)dx = \int_{-\infty}^{\infty} P(error|x)P(x)dx\)<br> \(= \int_{R_2}P(w_1|x)P(x)dx + \int_{R_1}P(w_2|x)P(x)dx\)<br> \(= \int_{R_2}P(x|w_1)P(w_1)dx + \int_{R_1}P(x|w_2)P(w_2)dx\)<br> \(= \begin{cases} A+B+D &amp; \text{if} &amp; x_B \\ A+B+C+D &amp; \text{if} &amp; x^{\ast} \end{cases}\)<br> where \(A+B+D\) is minimum error and \(C\) is reducible error<br> (아래 그림 참고)</li> <li>\(P(correct)\)<br> \(= \int \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_1}P(x|w_1)P(w_1)dx + \int_{R_2}P(x|w_2)P(w_2)dx\)</li> <li>\(P(error) = 1 - P(correct)\)<br> \(= 1 - \int \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_2}P(x|w_1)P(w_1)dx + \int_{R_1}P(x|w_2)P(w_2)dx\)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/1-480.webp 480w,/assets/img/2024-09-10-Pattern/1-800.webp 800w,/assets/img/2024-09-10-Pattern/1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>minimum error with rejection :<br> decision이 확실하지 않을 때는 classification 자체를 reject하는 게 적절<br> (classification error도 줄어들고, correct classification도 줄어듬) <ul> <li>feature space \(x\) 를 rejection region \(R\) 과 acceptance region \(A\) 으로 나눠서<br> rejection region \(R=\{ x | \text{max}_{i} P(w_i | x) \leq 1 - t\}\) 에서는 reject decision<br> acceptance region \(A=\{ x | \text{max}_{i} P(w_i | x) \gt 1 - t\}\) 에서는 \(w_1\) or \(w_2\) 로 classification decision 수행</li> <li>\(P_c(t) = P(correct)\)<br> \(= \int_{A} \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_1}P(x|w_1)P(w_1)dx + \int_{R_2}P(x|w_2)P(w_2)dx\)</li> <li>\(P_r(t) = P(reject)\)<br> \(= \int_{R}P(x|w_1)P(w_1)dx + \int_{R}P(x|w_2)P(w_2)dx\)<br> \(= \int_{R} P(x)dx\)</li> <li>\(P_e(t) = P(error)\)<br> \(= P(error, w_1) + P(error, w_2)\)<br> \(= 1 - P_r(t) - P_c(t)\) by 아래 식 대입<br> where \(P(error, w_1) = \int_{-\infty}^{\infty} P(x|w_1)P(w_1)dx - P(reject, w_1) - P(correct, w_1)\)<br> where \(P(error, w_2) = \int_{-\infty}^{\infty} P(x|w_2)P(w_2)dx - P(reject, w_2) - P(correct, w_2)\)<br> where \(\int_{-\infty}^{\infty} P(x|w_1)P(w_1)dx + \int_{-\infty}^{\infty} P(x|w_2)P(w_2)dx = \int_{-\infty}^{\infty} P(x)dx = 1\)</li> </ul> </li> <li>Summary : <ul> <li> <table> <tbody> <tr> <td>$$P(w_i</td> <td>x)$$ : rejection/acceptance region 구하는 데 사용</td> </tr> </tbody> </table> </li> <li>\(P(x|w_i)P(w_i)\) : \(P(correct, w_i), P(reject, w_i), P(error, w_i)\) 구해서<br> \(P_c(t), P_r(t), P_e(t)\) 구하는 데 사용</li> <li> \[P_c(t) + P_r(t) + P_e(t) = 1\] </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/2-480.webp 480w,/assets/img/2024-09-10-Pattern/2-800.webp 800w,/assets/img/2024-09-10-Pattern/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/3-480.webp 480w,/assets/img/2024-09-10-Pattern/3-800.webp 800w,/assets/img/2024-09-10-Pattern/3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="bayes-decision-rule-w-bayes-risk">Bayes Decision Rule w. Bayes risk</h3> <ul> <li>Bayes risk (minimum overall risk) :<br> \(\Omega = \{ w_1, \cdots w_c \}\) 에서 \(w_j\) 는 \(j\) -th class<br> \(A = \{ \alpha_{1}, \cdots, \alpha_{c} \}\) 에서 \(\alpha_{i}\) 는 class \(w_i\) 라고 예측하는 action<br> \(\lambda(\alpha_{i} | w_j) = \lambda_{ij}\) : class \(w_j\) 가 GT일 때, class \(w_i\) 로 pred. 했을 때의 loss <ul> <li>conditional risk for taking action \(\alpha_{i}\) :<br> 특정 input \(x\) 에 대해<br> \(R(\alpha_{i}|x) = \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j|x)\)</li> <li>overall risk for taking action \(\alpha_{i}\) :<br> 모든 input \(x\) 에 대해 적분<br> \(R(\alpha_{i}) = \int R(\alpha_{i}|x)P(x)dx\)<br> \(= \int \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j|x) P(x)dx\)<br> \(= \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j) \int P(x|w_j)dx\)<br> \(= \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j)\)<br> \(= \sum_{j=1}^c \lambda_{ij}P(w_j)\)<br> where pdf(likelihood) 합 \(\int P(x|w_j)dx = 1\)</li> <li>모든 input \(x\) 에 대해 가장 loss가 최소인 class \(w_i\) 로 예측하면,<br> minimum overall risk (= Bayes risk) 를 가짐</li> </ul> </li> <li>Bayes Decision Rule for Bayes risk : <ul> <li>Two-class (\(w_1, w_2\)) problem :<br> choose \(w_1\)<br> if \(R(\alpha_{1} | x) \lt R(\alpha_{2} | x)\)<br> if \(\lambda_{11}P(w_1 | x) + \lambda_{12}P(w_2 | x) \lt \lambda_{21}P(w_1 | x) + \lambda_{22}P(w_2 | x)\)<br> if \((\lambda_{21} - \lambda_{11})P(w_1 | x) \gt (\lambda_{12} - \lambda_{22})P(w_2 | x)\)<br> if \(\frac{P(x | w_1)}{P(x | w_2)} \gt \frac{(\lambda_{12} - \lambda_{22})P(w_2)}{(\lambda_{21} - \lambda_{11})P(w_1)}\)<br> if \(\frac{P(x | w_1)}{P(x | w_2)} \gt \frac{P(w_2)}{P(w_1)}\) for \(\lambda_{11}=\lambda_{22}=0\) and \(\lambda_{12}=\lambda_{21}\)<br> (likehood ratio \(\gt\) threshold) (위의 Bayes Decision Rule에서 구한 식과 same)</li> <li> <table> <tbody> <tr> <td>loss $$\lambda(\alpha_{i}</td> <td>w_j) = \begin{cases} 0 &amp; \text{if} &amp; i=j &amp; (\text{no penalty}) \ 1 &amp; \text{if} &amp; i \neq j &amp; (\text{equal penalty}) \end{cases}$$ 일 때</td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td>conditional risk $$R(\alpha_{i}</td> <td>x) = \sum_{j=1}^c \lambda(\alpha_{i}</td> <td>w_j)P(w_j</td> <td>x) = \sum_{j=1, j \neq i}^c P(w_j</td> <td>x) = 1 - P(w_i</td> <td>x)$$ 이므로</td> </tr> <tr> <td>Bayes Decision Rule에서 conditional risk $$R(\alpha_{i}</td> <td>x)\(최소화는 posterior\)P(w_i</td> <td>x)$$ 최대화와 같음</td> <td> </td> <td> </td> <td> </td> </tr> </tbody> </table> </li> <li>multi-class problem :<br> classifieer (discriminant function) (space-partitioning function) \(g(x)\) 에 대해<br> choose \(w_i\) where \(g_{i}(x)\) is the largest<br> s.t. decision boundary is \(g_{i}(x) = g_{j}(x)\) where they are the two largest discriminant functions<br> e.g. Bayes classifier : \(g_{i}(x) = - R(\alpha_{i} | x)\) or \(g_{i}(x) = P(w_i | x)\) or \(g_{i}(x) = P(x | w_i)P(w_i)\) or \(g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i)\)</li> </ul> </li> </ul> <h3 id="discriminant-function-for-gaussian-pdf">Discriminant Function for Gaussian PDF</h3> <ul> <li> <p>\(G(\boldsymbol x) = \frac{1}{(2\pi)^{\frac{d}{2}} | \Sigma |^{\frac{1}{2}}}e^{-\frac{1}{2}(\boldsymbol x - \boldsymbol \mu)^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu)}\)<br> where \(d \times d\) covariance \(\Sigma = E[(\boldsymbol x - \boldsymbol \mu)(\boldsymbol x - \boldsymbol \mu)^T] = E[\boldsymbol x \boldsymbol x^{T}] - \boldsymbol \mu \boldsymbol \mu^{T} = S - \boldsymbol \mu \boldsymbol \mu^{T}\)<br> where \(S = E[\boldsymbol x \boldsymbol x^{T}]\) : standard autocorrelation matrix</p> </li> <li> <p>Discriminant function for Gaussian PDF :<br> likelihood \(P(x | w_i)\) 를 Gaussian PDF로 둘 경우,<br> \(g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{d}{2} \text{ln}(2\pi) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\)</p> <ul> <li>case 1) \(\Sigma_{i} = \sigma^{2} \boldsymbol I\) (모든 classes에 대해 equal covariance) (등방성(sphere)) <br> \(g_{i}(x) = -\frac{\| \boldsymbol x - \boldsymbol \mu_{i} \|^2}{2 \sigma^{2}} + \text{ln}P(w_i)\)<br> \(i\) 와 관련된 term만 남기면<br> \(g_{i}(x) = \frac{1}{\sigma^{2}} \boldsymbol \mu_{i}^T \boldsymbol x - \frac{1}{2\sigma^{2}} \boldsymbol \mu_{i}^T \boldsymbol \mu_{i} + \text{ln}P(w_i)\)<br> \(= \boldsymbol w_i^T \boldsymbol x + \boldsymbol w_{i0}\) (linear) <ul> <li>decision boundary :<br> hyperplane \(g(x) = g_{i}(x) - g_{j}(x) = (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T(\boldsymbol x - \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) + \frac{\sigma^{2}}{(\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T} \text{ln}\frac{P(w_i)}{P(w_j)})\)<br> \(= \boldsymbol w^T (\boldsymbol x - \boldsymbol x_0) = 0\)</li> <li>\(\boldsymbol x_0\) 를 지나고 \(\boldsymbol w = \boldsymbol \mu_{i} - \boldsymbol \mu_{j}\) 에 수직인 hyperplane</li> <li>\(\boldsymbol x_0 = \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) - \frac{\sigma^{2}}{\| \boldsymbol \mu_{i} - \boldsymbol \mu_{j} \|^2} \text{ln}\frac{P(w_i)}{P(w_j)} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})\) 이므로<br> \(\boldsymbol x_0\) 의 위치는 \(\boldsymbol \mu_{i}\) 와 \(\boldsymbol \mu_{j}\) 의 중점에서 \(\begin{cases} \boldsymbol \mu_{j} \text{쪽으로 이동} &amp; \text{if} &amp; P(w_i) \gt P(w_j) \\ \boldsymbol \mu_{i} \text{쪽으로 이동} &amp; \text{if} &amp; P(w_i) \lt P(w_j) \end{cases}\)<br> (\(P(w_i)\) 와 \(P(w_j)\) 중 더 작은 쪽으로 이동)<br> (\(\sigma^{2}\) 이 (\(\| \mu_{i} - \mu_{j} \|^2\) 에 비해 비교적) 작은 경우 \(P(w_i)\) 와 \(P(w_j)\) 에 따른 \(x_0\) shift는 미약)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/4-480.webp 480w,/assets/img/2024-09-10-Pattern/4-800.webp 800w,/assets/img/2024-09-10-Pattern/4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Discriminant function for Gaussian PDF :<br> likelihood \(P(x | w_i)\) 를 Gaussian PDF로 둘 경우,<br> \(g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{d}{2} \text{ln}(2\pi) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\) <ul> <li>case 2) \(\Sigma_{i} = \Sigma\) (symmetric) (모든 classes에 대해 equal covariance) (비등방성(hyper-ellipsoidal))<br> \(g_{i}(x) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) + \text{ln}P(w_i)\)<br> \(i\) 와 관련된 term만 남기면<br> \(g_{i}(x) = \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol x - \frac{1}{2} \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol \mu_{i} + \text{ln}P(w_i)\)<br> \(= \boldsymbol w_i^T \boldsymbol x + \boldsymbol w_{i0}\) (linear) <ul> <li>decision boundary :<br> hyperplane \(g(x) = g_{i}(x) - g_{j}(x) = (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T \Sigma^{-1} (\boldsymbol x - \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) + \frac{1}{(\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T \Sigma^{-1}} \text{ln}\frac{P(w_i)}{P(w_j)})\)<br> \(= \boldsymbol w^T (\boldsymbol x - \boldsymbol x_0) = 0\)</li> <li>\(\boldsymbol x_0\) 를 지나고 \(\boldsymbol w = \Sigma^{-1} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})\) 에 수직인 hyperplane</li> <li>\(\boldsymbol x_0 = \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) - \frac{\text{ln}\frac{P(w_i)}{P(w_j)}}{(\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T \Sigma^{-1} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})\) 이므로<br> 마찬가지로 \(\boldsymbol x_0\) 의 위치는 \(\boldsymbol \mu_{i}\) 와 \(\boldsymbol \mu_{j}\) 의 중점에서 \(P(w_i)\) 와 \(P(w_j)\) 중 더 작은 쪽으로 이동</li> <li>\(\boldsymbol w = \Sigma^{-1} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})\) 는<br> vector \(\boldsymbol \mu_{i} - \boldsymbol \mu_{j}\) 를 \(\Sigma^{-1}\) 로 회전시킨 vector를 의미</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/5-480.webp 480w,/assets/img/2024-09-10-Pattern/5-800.webp 800w,/assets/img/2024-09-10-Pattern/5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Discriminant function for Gaussian PDF :<br> likelihood \(P(x | w_i)\) 를 Gaussian PDF로 둘 경우,<br> \(g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{d}{2} \text{ln}(2\pi) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\) <ul> <li>case 2) \(\Sigma_{i}\) is arbitrary (symmetric) (class마다 covariance 다름) (비등방성(hyper-ellipsoidal))<br> \(g_{i}(x) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\)<br> \(\Sigma_{i}\) 가 \(i\) 에 대한 term이므로<br> \(g_{i}(x) = - \frac{1}{2} \boldsymbol x^T \Sigma^{-1} \boldsymbol x + \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol x - \frac{1}{2} \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol \mu_{i} - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)\)<br> \(= - \frac{1}{2} \boldsymbol x^T \Sigma^{-1} \boldsymbol x + \boldsymbol w_i^T \boldsymbol x + \boldsymbol w_{i0}\) (quadratic) 는<br> quadratic discriminant function in \(x\) <ul> <li>decision surface :<br> hyperquadratic (hyperplane, hypersphere, hyperellipsoidal, hyperparaboloid, hyperhyperboloid)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-10-Pattern/6-480.webp 480w,/assets/img/2024-09-10-Pattern/6-800.webp 800w,/assets/img/2024-09-10-Pattern/6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-10-Pattern/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="bayes-rule-for-discrete-case">Bayes Rule for Discrete Case</h3> <ul> <li> <table> <tbody> <tr> <td>pdf 적분 $$\int p(x</td> <td>w_j) dx$$ 대신</td> <td> </td> </tr> <tr> <td>확률 합 $$lim_{\Delta x \rightarrow 0} \Sigma_{k=-\infty}^{\infty} p(x_k</td> <td>w_j) \Delta x\(\)\rightarrow\(\)\Sigma_{k=1}^m P(v_k</td> <td>w_j)$$</td> </tr> </tbody> </table> </li> <li> <p>Bayes Decision Rule은 discrete case에서도 same<br> Bayes risk minimize 위해 conditional risk \(R(\alpha_{i} | x)\) minimize<br> (posterior maximize와 same)</p> </li> <li>\(\boldsymbol x = [x_1, x_2, \ldots, x_d]^T\) 에서 \(x_i\) 가 0 혹은 1의 값을 갖는 Bernoulli random var.일 때 <ul> <li>class \(w_1\) 일 때 :<br> \(x_i \sim p_i^{x_i}(1-p_i)^{1-x_i}\)<br> \(P(\boldsymbol x | w_1) = P([x_1, x_2, \ldots, x_d]^T | w_1) = \prod_{i=1}^d P(x_i | w_1) = \prod_{i=1}^d p_i^{x_i}(1-p_i)^{1-x_i}\)</li> <li>class \(w_2\) 일 때 :<br> \(x_i \sim q_i^{x_i}(1-q_i)^{1-x_i}\)<br> \(P(\boldsymbol x | w_2) = P([x_1, x_2, \ldots, x_d]^T | w_2) = \prod_{i=1}^d P(x_i | w_2) = \prod_{i=1}^d q_i^{x_i}(1-q_i)^{1-x_i}\)</li> <li>likelihood ratio :<br> \(\frac{P(\boldsymbol x | w_1)}{P(\boldsymbol x | w_2)} = \prod_{i=1}^d (\frac{p_i}{q_i})^{x_i}(\frac{1-p_i}{1-q_i})^{1-x_i}\)</li> <li>discriminant function :<br> \(g(x) = \text{ln} \frac{P(\boldsymbol x | w_1)P(w_1)}{P(\boldsymbol x | w_2)P(w_2)} = TBD\)</li> </ul> </li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="disqus_thread" style="max-width: 800px; margin: 0 auto;"> <script type="text/javascript">var disqus_shortname="semyeong-yu",disqus_identifier="/blog/2024/Pattern",disqus_title="EE534 Pattern Recognition";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Semyeong Yu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5YMLX9VHFX"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5YMLX9VHFX");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>