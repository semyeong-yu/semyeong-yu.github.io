<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 4D Gaussian Splatting | Semyeong Yu </title> <meta name="author" content="Semyeong Yu"> <meta name="description" content="4D Gaussian Splatting for Real-Time Dynamic Scene Rendering (CVPR 2024)"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://semyeong-yu.github.io/blog/2024/4DGS/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "4D Gaussian Splatting",
            "description": "4D Gaussian Splatting for Real-Time Dynamic Scene Rendering (CVPR 2024)",
            "published": "September 14, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Semyeong</span> Yu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>4D Gaussian Splatting</h1> <p>4D Gaussian Splatting for Real-Time Dynamic Scene Rendering (CVPR 2024)</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#abstract">Abstract</a> </div> <div> <a href="#contribution">Contribution</a> </div> <div> <a href="#related-works">Related Works</a> </div> <ul> <li> <a href="#novel-view-synthesis">Novel View Synthesis</a> </li> <li> <a href="#neural-rendering-w-point-clouds">Neural Rendering w. Point Clouds</a> </li> <li> <a href="#dynamic-nerf-with-deformation-fields">Dynamic NeRF with Deformation Fields</a> </li> </ul> <div> <a href="#method">Method</a> </div> <ul> <li> <a href="#overview-gaussian-deformation-field-network">Overview (Gaussian Deformation Field Network)</a> </li> <li> <a href="#spatial-temporal-structure-encoder">Spatial-Temporal Structure Encoder</a> </li> <li> <a href="#extremely-tiny-multi-head-gaussian-deformation-decoder">Extremely Tiny Multi-head Gaussian Deformation Decoder</a> </li> <li> <a href="#optimization">Optimization</a> </li> </ul> <div> <a href="#experiment">Experiment</a> </div> <ul> <li> <a href="#dataset">Dataset</a> </li> <li> <a href="#results">Results</a> </li> <li> <a href="#ablation-study">Ablation Study</a> </li> <li> <a href="#discussion">Discussion</a> </li> <li> <a href="#limitation">Limitation</a> </li> <li> <a href="#conclusion">Conclusion</a> </li> </ul> <div> <a href="#code-flow">Code Flow</a> </div> <div> <a href="#question">Question</a> </div> <div> <a href="#appendix">Appendix</a> </div> </nav> </d-contents> <h2 id="4d-gaussian-splatting-for-real-time-dynamic-scene-rendering">4D Gaussian Splatting for Real-Time Dynamic Scene Rendering</h2> <h4 id="guanjun-wu-taoran-yi-jiemin-fang-lingxi-xie-xiaopeng-zhang-wei-wei-wenyu-liu-qi-tian-xinggang-wang">Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, Xinggang Wang</h4> <blockquote> <p>paper :<br> <a href="https://arxiv.org/abs/2310.08528" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2310.08528</a><br> project website :<br> <a href="https://guanjunwu.github.io/4dgs/index.html" rel="external nofollow noopener" target="_blank">https://guanjunwu.github.io/4dgs/index.html</a><br> code :<br> <a href="https://github.com/hustvl/4DGaussians" rel="external nofollow noopener" target="_blank">https://github.com/hustvl/4DGaussians</a><br> referenced blog :<br> <a href="https://xoft.tistory.com/54" rel="external nofollow noopener" target="_blank">https://xoft.tistory.com/54</a></p> </blockquote> <blockquote> <p>핵심 요약 :<br> <code class="language-plaintext highlighter-rouge">3DGS를 dynamic scene에 적용하고자 할 때</code><br> x, y, z, t를 input으로 갖는 encoder로서<br> <code class="language-plaintext highlighter-rouge">4D scene을 2D planes로 표현하는 HexPlane 기법을 이용하겠다!</code></p> </blockquote> <h2 id="abstract">Abstract</h2> <ul> <li> <p>spatially-temporally-sparse input으로부터<br> complex point motion을 정확하게 모델링하면서<br> high efficiency로 real-time dynamic scene을 rendering하는 건 매우 challenging task</p> </li> <li>3DGS를 각 frame에 적용하는 게 아니라 4DGS라는 새로운 모델 제시 <ul> <li> <code class="language-plaintext highlighter-rouge">오직 3DGS 한 세트</code> 필요</li> <li>4DGS framework : <ul> <li> <code class="language-plaintext highlighter-rouge">Spatial-Temporal Structure Encoder</code> :<br> HexPlane <d-cite key="neuralvoxel1">[22]</d-cite> 에서 영감을 받아<br> decomposed neural voxel encoding algorithm을 이용해서<br> <code class="language-plaintext highlighter-rouge">4D neural voxel을 2D voxel planes로 decompose</code>하여<br> 2D voxel plane (param.)에 Gaussian <code class="language-plaintext highlighter-rouge">point-clouds (pts)의 spatial-temporal 정보를 encode</code> </li> <li> <code class="language-plaintext highlighter-rouge">Extremely Tiny Multi-head Gaussian Deformation Decoder</code> :<br> 가벼운 MLP를 이용해서<br> <code class="language-plaintext highlighter-rouge">Gaussian deformation을 예측</code>함</li> </ul> </li> </ul> </li> <li>4DGS :<br> real-time (82 FPS) rendering at high (800 \(\times\) 800) resolution on RTX 3090 GPU</li> </ul> <h2 id="contribution">Contribution</h2> <ul> <li> <p>Gaussian <code class="language-plaintext highlighter-rouge">motion</code>과 <code class="language-plaintext highlighter-rouge">shape</code>-deformation을 모두 모델링할 수 있는 4DGS framework 제시<br> w. efficient <code class="language-plaintext highlighter-rouge">Gaussian deformation field</code> network</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">multi-resolution</code> encoding<br> (connect nearby 3D Gaussians to build rich Gaussian features)<br> by efficient <code class="language-plaintext highlighter-rouge">spatial-temporal structure encoder</code></p> </li> <li> <p>SOTA <code class="language-plaintext highlighter-rouge">performance</code>이면서 <code class="language-plaintext highlighter-rouge">real-time</code> rendering on <code class="language-plaintext highlighter-rouge">dynamic</code> scenes<br> e.g. 82 FPS at resol. 800 \(\times\) 800 for synthetic dataset<br> e.g. 30 FPS at resol. 1352 \(\times\) 1014 for real dataset</p> </li> <li> <p>4D scenes에서의 editing 및 tracking에 활용 가능</p> </li> </ul> <h2 id="related-works">Related Works</h2> <h3 id="novel-view-synthesis">Novel View Synthesis</h3> <ul> <li>static scene : <ul> <li>light fields <d-cite key="lightfield">[1]</d-cite>, mesh <d-cite key="mesh1">[2]</d-cite> <d-cite key="mesh2">[3]</d-cite> <d-cite key="mesh3">[4]</d-cite> <d-cite key="mesh4">[5]</d-cite>, voxels <d-cite key="voxel1">[6]</d-cite> <d-cite key="voxel2">[7]</d-cite> <d-cite key="voxel3">[8]</d-cite>, multi-planes <d-cite key="multiplane">[9]</d-cite> 이용한 methods</li> <li>NeRF-based methods <a href="https://semyeong-yu.github.io/blog/2024/NeRF/">NeRF</a> <a href="https://semyeong-yu.github.io/blog/2024/MipNeRF/">MipNeRF</a> <d-cite key="nerf++">[10]</d-cite> </li> </ul> </li> <li>dynamic scene : <ul> <li>NeRF-based methods <d-cite key="dynerf1">[11]</d-cite> <d-cite key="dynerf2">[12]</d-cite> <d-cite key="dynerf3">[13]</d-cite> </li> <li> <code class="language-plaintext highlighter-rouge">explicit voxel grid</code> <d-cite key="voxeltemp1">[14]</d-cite> <d-cite key="voxeltemp2">[15]</d-cite> <d-cite key="voxeltemp3">[16]</d-cite> <d-cite key="voxeltemp4">[17]</d-cite> :<br> temporal info. 모델링하기 위해 explicit voxel grid 사용</li> <li> <code class="language-plaintext highlighter-rouge">flow-based</code> methods <d-cite key="flow1">[18]</d-cite> <d-cite key="flow2">[19]</d-cite> <d-cite key="voxeltemp3">[16]</d-cite> <d-cite key="flow3">[20]</d-cite> <d-cite key="flow4">[21]</d-cite> :<br> nearby frames를 blending하는 warping algorithm 사용</li> <li> <code class="language-plaintext highlighter-rouge">decomposed neural voxels</code> <d-cite key="neuralvoxel1">[22]</d-cite> <d-cite key="neuralvoxel2">[23]</d-cite> <d-cite key="neuralvoxel3">[24]</d-cite> <d-cite key="neuralvoxel4">[25]</d-cite> <d-cite key="neuralvoxel5">[26]</d-cite> <d-cite key="neuralvoxel6">[27]</d-cite> :<br> 빠른 training on dynamic scenes 가능<br> (Fig 1.의 (b))</li> <li> <code class="language-plaintext highlighter-rouge">multi-view</code> setups 다루기 위한 methods <d-cite key="multi1">[28]</d-cite> <d-cite key="multi2">[29]</d-cite> <d-cite key="multi3">[30]</d-cite> <d-cite key="multi4">[31]</d-cite> <d-cite key="multi5">[32]</d-cite> <d-cite key="multi6">[33]</d-cite> </li> <li>본 논문 (4DGS) :<br> 위에서 언급된 methods는 빠른 training은 가능했지만 real-time rendering on dynamic scenes는 여전히 어려웠음<br> \(\rightarrow\)<br> 본 논문은 빠른 training 및 rendering pipeline 제시<br> (Fig 1.의 (c))</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/1-480.webp 480w,/assets/img/2024-09-14-4DGS/1-800.webp 800w,/assets/img/2024-09-14-4DGS/1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-14-4DGS/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig 1. dynamic scene rendering methods </div> <ul> <li>Fig 1. 설명 :<br> dynamic scene을 rendering하는 여러 방법들 소개 <ul> <li>(a) : Deformation-based (Canonical Mapping) Volume Rendering<br> point-deformation-field를 이용해서<br> sampled points를 canonical space로 mapping<br> (하나의 ray 위의 sampled points가 다같이 canonical space로 mapping되므로<br> 각 point의 서로 다른 속도를 잘 모델링하지 못함)</li> <li>(b) : Time-aware Volume Rendering<br> 각 timestamp에서의 각 point의 feature를 직접 개별적으로 계산<br> (path는 그대로)</li> <li>(c) : 4DGS <br> compact <code class="language-plaintext highlighter-rouge">Gaussian-deformation-field</code>를 이용해서<br> 기존의 3D Gaussians를 특정 timestamp의 3D Gaussians로 변환<br> ((a)와 유사하긴 하지만<br> 각 Gaussian이 <code class="language-plaintext highlighter-rouge">ray에 의존하지 않고 서로 다른 속도로 이동</code> 가능)</li> </ul> </li> </ul> <h3 id="neural-rendering-w-point-clouds">Neural Rendering w. Point Clouds</h3> <ul> <li> <p>3D scenes를 나타내기 위해 meshes, point-clouds, voxels, hybrid ver. 등 여러 분야가 연구되어 왔는데<br> 그 중 point-cloud representation을 volume rendering과 결합하면<br> dynamic novel view synthesis task도 잘 수행 가능</p> </li> <li> <p>3DGS :<br> <code class="language-plaintext highlighter-rouge">explicit</code> representation이라서,<br> <code class="language-plaintext highlighter-rouge">differentiable</code> <code class="language-plaintext highlighter-rouge">point</code>-based splatting이라서,<br> <code class="language-plaintext highlighter-rouge">real-time</code> renderer라서 주목받음</p> </li> <li> <p>3DGS on dynamic scenes :</p> <ul> <li>Dynamic3DGS <d-cite key="dyna3DGS">[34]</d-cite> : <ul> <li>3D Gaussian 개수를 고정하고<br> 각 timestamp \(t_i\) 마다 각 3D Gaussian의 position, variance를 tracking</li> <li>문제점 : <ul> <li>need dense multi-view input images</li> <li>prev. frame의 모델링이 부적절하면 전체적인 성능이 떨어짐</li> <li>linear memory consumption \(O(tN)\)<br> for \(t\)-time steps and \(N\)-3D Gaussians</li> </ul> </li> </ul> </li> <li>4DGS (본 논문) : <ul> <li>very compact network로 3D Gaussian motion을 모델링하기 때문에<br> training 효율적이고 real-time rendering</li> <li>memory consumption \(O(N+F)\)<br> for \(N\)-3D Gaussians, \(F\)-parameters of Gaussian-deformation-field network</li> </ul> </li> <li>4DGS (Zeyu Yang) <d-cite key="4DGS1">[35]</d-cite> : <ul> <li>marginal temporal Gaussian 분포를 기존의 3D Gaussian 분포에 추가하여<br> 3D Gaussians를 4D로 uplift</li> <li>However, 그러면 각 3D Gaussian은 오직 their local temporal space에만 focus</li> </ul> </li> <li>Deformable-3DGS (Ziyi Yang) <d-cite key="deformable3DGS">[36]</d-cite> : <ul> <li>본 논문처럼 MLP deformation network를 도입하여 dynamic scenes의 motion을 모델링</li> <li>본 논문 (4DGS)도 이와 유사하지만 training을 효율적으로 만듦</li> </ul> </li> <li>Spacetime-GS (Zhan Li) <d-cite key="spacetimeGS">[37]</d-cite> : <ul> <li>각 3D Gaussian을 individually tracking</li> </ul> </li> </ul> </li> </ul> <h3 id="dynamic-nerf-with-deformation-fields">Dynamic NeRF with Deformation Fields</h3> <ul> <li> <p>모든 dynamic NeRF는 아래의 식을 따른다<br> \(c, \sigma = M(x, d, t, \lambda)\)<br> where \(c \in R^3, \sigma \in R, x \in R^3, d \in R^2, t \in R, \lambda \in R\)<br> where \(\lambda\) is optional input (frame-dependent code to build topological and appearance changes) <d-cite key="dynerf2">[12]</d-cite> <d-cite key="wild">[38]</d-cite></p> </li> <li> <p>deformation NeRF-based methods는<br> Fig 1. (a)에서처럼<br> deformation network \(\phi_{t} : (x, t) \rightarrow \Delta x\) 로 world-to-canonical mapping 한 뒤<br> RGB color와 volume density를 뽑는다<br> \(c, \sigma = NeRF(x+\Delta x, d, \lambda)\)</p> </li> <li> <p>4DGS (본 논문)은<br> <code class="language-plaintext highlighter-rouge">Gaussian deformation field</code> network \(F\) 이용해서<br> time \(t\) 에서의 <code class="language-plaintext highlighter-rouge">canonical-to-world mapping</code>을 직접 계산한 뒤<br> differential splatting(rendering) 수행</p> </li> </ul> <h2 id="method">Method</h2> <h3 id="overview-gaussian-deformation-field-network">Overview (Gaussian Deformation Field Network)</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/2-480.webp 480w,/assets/img/2024-09-14-4DGS/2-800.webp 800w,/assets/img/2024-09-14-4DGS/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-14-4DGS/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig 2. Pipeline of this model (Gaussian Deformation Field Network) </div> <ul> <li>Fig 2. 설명 : <ul> <li>static 3D Gaussian set을 만듦</li> <li>각 3D Gaussian의 center 좌표 \(x, y, z\) 와 timestamp \(t\) 를<br> Gaussian Deformation Field Network의 input으로 준비</li> <li>Spatial-Temporal Structure Encoder :<br> multi-resolution voxel planes를 query하여<br> voxel feature를 계산<br> (temporal 및 spatial feature를 둘 다 encode 가능)</li> <li>Tiny Multi-head Gaussian Deformation Decoder :<br> position, rotation, scaling head에서 각각 해당 feature를 decode하여<br> 각 3D Gaussian의 position, rotation, scaling 변화량을 얻어서<br> timestamp \(t\) 에서의 변형된 3D Gaussians를 얻음</li> </ul> </li> </ul> <h3 id="spatial-temporal-structure-encoder">Spatial-Temporal Structure Encoder</h3> <ul> <li> <p>근처에 있는 3D Gaussians끼리는 항상 spatial 및 temporal 정보를 비슷하게 공유하고 있다.<br> 따라서 HexPlane 기법에서는 각 Gaussian이 따로 변형되는 게 아니라,<br> 여러 <code class="language-plaintext highlighter-rouge">adjacent 3D Gaussian</code>들이 군집처럼 연결되어 함께 변형되므로<br> motion과 shape-deformation을 정확하게 예측할 수 있다<br> 이로써 변형된 geometry를 더 정확히 모델링하고 avulsion(벗겨짐?)을 방지할 수 있음</p> </li> <li> <p>기존 논문 설명 (Backgrounds) :</p> <ul> <li>TensoRF : <a href="https://semyeong-yu.github.io/blog/2024/TensoRF/">Link</a> </li> <li>HexPlane <d-cite key="neuralvoxel1">[22]</d-cite> :<br> 4차원(\(XYZT\))을 모델링하기 위해<br> 3개 타입의 rank로 decomposition (\(XY\) 평면 - \(ZT\) 평면, \(XZ\) 평면 - \(YT\) 평면, \(YZ\) 평면 - \(XT\) 평면)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/3-480.webp 480w,/assets/img/2024-09-14-4DGS/3-800.webp 800w,/assets/img/2024-09-14-4DGS/3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-14-4DGS/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> HexPlane Overview </div> <ul> <li>Spatial-Temporal Structure Encoder (1) : <ul> <li>vanilla 4D neural voxel은 memory를 많이 잡아먹기 때문에<br> 4D neural voxel(\(XYZT\))을 6개의 multi-resol. planes로 decompose하는<br> 4D K-Planes module <d-cite key="neuralvoxel2">[23]</d-cite> 사용</li> <li>3D Gaussians는 bounding plane voxels에 포함되어<br> Gaussians의 deformation도 nearby temporal voxels에 encode될 수 있음 <code class="language-plaintext highlighter-rouge">????</code> </li> <li>기존 논문들 <d-cite key="voxeltemp1">[14]</d-cite> <d-cite key="neuralvoxel1">[22]</d-cite> <d-cite key="neuralvoxel2">[23]</d-cite> <d-cite key="neuralvoxel5">[26]</d-cite> 에서 영감을 받아<br> Spatial-Temporal Structure Encoder는<br> multi-resolution HexPlane \(R(i, j)\) 와 tiny MLP \(\phi_{d}\) 로 구성됨</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/4-480.webp 480w,/assets/img/2024-09-14-4DGS/4-800.webp 800w,/assets/img/2024-09-14-4DGS/4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-14-4DGS/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Spatial-Temporal Structure Encoder (2) : <ul> <li>multi-resolution HexPlane \(R(i, j)\) :<br> 본 논문에서는 TensoRF와 달리 Grid resol.을 점점 증가시키지 않고, 애초에 multi-resolution으로 decomposition의 rank를 구성함<br> \(f_{h} = \cup_{l} \prod \text{interp}(R_{l}(i, j))\) <ul> <li>where<br> \(f_h \in R^{h \ast l}\) : feature of decomposed neural voxel \(R_{l}(i, j) \in R^{h \times lN_i \times lN_j}\) : 2D voxel plane<br> \(h\) : hidden dim.<br> \(\{ i, j \} \in \{ (x, y), (x, z), (y, z), (x, t), (y, t), (z, t) \}\) : 6 종류의 planes<br> \(N\) : voxel grid의 basic resol.<br> \(l \in \{ 1, 2 \}\) : upsampling scale (multi-resol.)<br> \(\text{interp}\) : bilinear interpolation (plane의 grid의 네 꼭짓점으로부터 interpolation으로 voxel feature 뽑아냄)<br> \(\prod\) : product over planes (K-Planes <d-cite key="neuralvoxel2">[23]</d-cite> 참고)<br> \(\cup_{l}\) : multi-resol.에 대해 concat 또는 add</li> <li> <a href="https://github.com/hustvl/4DGaussians/blob/master/scene/hexplane.py" rel="external nofollow noopener" target="_blank">Github Code</a> 에서 <ul> <li>forward()</li> <li>get_density()<br> self.grids : nn.ModuleList() of init_grid_param<br> and<br> init_grid_param() : range(in_dim) = [0, 1, 2, 3] (x, y, z, t) 중에 grid_nd = 2개(plane)를 뽑아서<br> grid_coefs \(R_{l}(i, j) for \{ i, j \} \in \{ (x, y), (x, z), (y, z), (x, t), (y, t), (z, t) \}\) 반환<br> grid_coefs : nn.ParameterList() of nn.Parameter \(R_{l}(i, j)\) with shape \((1, D_{out}, \text{resol. of second dimension}, \text{resol. of first dimension})\)<br> e.g. \(R_{l}(x, t)\), 즉 \(XT\) plane은 nn.Parameter \((1, D_{out}, \text{resol.}[3], \text{resol.}[0])\) 으로 표현</li> <li>interpolate_ms_features : \(f_{h} = \cup_{l} \prod \text{interp}(R_{l}(i, j))\) 반환</li> <li>grid_sample_wrapper : \(\text{interp}(R_{l}(i, j))\) 반환</li> <li>grid_sampler : F.grid_sample <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html" rel="external nofollow noopener" target="_blank">Link</a><br> second argument(pts) 좌표에서의 값을 구하기 위해 first argument(grid \(R_{l}(i, j)\))의 값을 interpolate<br> 그럼 이제 3D scene을 <code class="language-plaintext highlighter-rouge">4D neural voxel</code> 대신 <code class="language-plaintext highlighter-rouge">2D voxel plane</code> \(R_{l}(i, j)\) 이라는 param.들로 표현 가능</li> </ul> </li> </ul> </li> </ul> </li> <li>Spatial-Temporal Structure Encoder (3) : <ul> <li>tiny MLP \(\phi_{d}\) :<br> \(f_d = \phi_{d} (f_h)\)<br> merge all the features</li> <li>공간상(e.g. \(XY\) 평면) 또는 시간상(e.g. \(XT\) 평면)으로 인접한 voxel은<br> HexPlane \(R(i, j)\) 에서 유사한 feature를 가져서 유사한 Gaussian param. 변화량을 가지므로<br> optimization 진행됨에 따라<br> Gaussian의 covariance가 줄어들면서 작은 3D Gaussian들이 모여서 dense해진다 <code class="language-plaintext highlighter-rouge">?????</code> </li> </ul> </li> </ul> <h3 id="extremely-tiny-multi-head-gaussian-deformation-decoder">Extremely Tiny Multi-head Gaussian Deformation Decoder</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/5-480.webp 480w,/assets/img/2024-09-14-4DGS/5-800.webp 800w,/assets/img/2024-09-14-4DGS/5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-14-4DGS/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Multi-head Gaussian Deformation Decoder : <ul> <li>매우 작은 multi-head decoder로 position, scaling, rotation 변화량을 얻음<br> \(\Delta \chi = \phi_{x}(f_d)\)<br> \(\Delta r = \phi_{r}(f_d)\)<br> \(\Delta s = \phi_{s}(f_d)\)</li> <li>그러면 변형된 deformed 3D Gaussians 계산할 수 있음<br> \((\chi ' , r ' , s ') = (\chi + \Delta \chi, r + \Delta r, s + \Delta s)\) 에 대해<br> next time \(t\) 의 deformed 3D Gaussian set은<br> \(G ' = \{ \chi ' , r ' , s ', \sigma, c \}\)</li> <li>근데 실제 implementation 할 때는 speed 증가 위해<br> scaling(size), rotation, color, opacity는 고정하고<br> position 변화량만 구함</li> </ul> </li> </ul> <h3 id="optimization">Optimization</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/6-480.webp 480w,/assets/img/2024-09-14-4DGS/6-800.webp 800w,/assets/img/2024-09-14-4DGS/6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-14-4DGS/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <p><code class="language-plaintext highlighter-rouge">warm-up</code> :<br> 처음 3000 iter. 동안은<br> Gaussian Deformation Field Network 없이<br> 3DGS의 SfM points initialization 이용하여<br> <code class="language-plaintext highlighter-rouge">static 3DGS</code> optimize 하고,<br> 그 후에 dynamic scene에 대해 4DGS framework를 fine-tuning 형태로 학습</p> </li> <li> <p>Loss :<br> \(L = | \hat I - I | + L_{tv}\)</p> <ul> <li>L1 recon. loss</li> <li> <code class="language-plaintext highlighter-rouge">total-variational loss</code> : <ul> <li>sparse input images일 경우에 적게 관측된 view에서는 noise 및 outlier 때문에 overfitting 및 local minima 문제가 발생할 수 있으므로<br> <code class="language-plaintext highlighter-rouge">regularization</code> term 필요</li> <li>pixel 값 간의 급격한 변화를 억제하기 위해<br> \(I_{i+1, j} - I_{i, j}\) 항과 \(I_{i, j+1} - I_{i, j}\) 항을 loss에 추가</li> </ul> </li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <ul> <li> <p>single RTX 3090 GPU</p> </li> <li> <p>Synthetic Dataset :</p> <ul> <li>designed for monocular settings</li> <li>camera poses for each timestamp은 거의 randomly generated 수준</li> <li>scene 당 50-200 frames</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/12-480.webp 480w,/assets/img/2024-09-14-4DGS/12-800.webp 800w,/assets/img/2024-09-14-4DGS/12-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-14-4DGS/12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Real-world Dataset : <ul> <li>by HyperNeRF <d-cite key="dynerf2">[12]</d-cite> and Neu3D <d-cite key="neuralvoxel4">[25]</d-cite> </li> <li>HyperNeRF dataset :<br> one or two cameras<br> with straightforward camera motion<br> (1,2개의 camera를 직관적인 경로로 움직이며 촬영)</li> <li>Neu3D dataset :<br> 15 to 20 static cameras<br> with extended periods and complex camera motions<br> (15-20개의 많은 정적인 camera로 오랜 시간 동안씩 촬영하며 복잡한 경로로 camera를 움직임)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/13-480.webp 480w,/assets/img/2024-09-14-4DGS/13-800.webp 800w,/assets/img/2024-09-14-4DGS/13-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-14-4DGS/13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="results">Results</h3> <ul> <li>Metrics : <ul> <li>quality :<br> PSNR<br> LPIPS<br> SSIM<br> DSSIM<br> MS-SSIM</li> <li>speed :<br> FPS<br> training times</li> <li>memory :<br> storage</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/7-480.webp 480w,/assets/img/2024-09-14-4DGS/7-800.webp 800w,/assets/img/2024-09-14-4DGS/7-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-14-4DGS/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/8-480.webp 480w,/assets/img/2024-09-14-4DGS/8-800.webp 800w,/assets/img/2024-09-14-4DGS/8-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-14-4DGS/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Im4D <d-cite key="multi2">[29]</d-cite> 는 본 논문과 유사하게 high-quality이지만<br> multi-cam 방식을 쓰기 때문에 monocular scene을 모델링하기 어렵</li> </ul> <h3 id="ablation-study">Ablation Study</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/9-480.webp 480w,/assets/img/2024-09-14-4DGS/9-800.webp 800w,/assets/img/2024-09-14-4DGS/9-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-14-4DGS/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Spatial-Temporal Structure Encoder : <ul> <li>explicit HexPlane encoder는<br> 3DGS의 spatial 및 temporal 정보를 모두 encode 하면서<br> purely explicit method <d-cite key="dyna3DGS">[34]</d-cite> 보다 storage 공간 아낄 수 있음</li> <li>만약에 HexPlane encoder 없이 shallow MLP encoder만 쓰면<br> 복잡한 deformation 모델링 어렵</li> </ul> </li> <li>3D Gaussian Initialization : <ul> <li>처음에 warm-up으로 SfM points initialization 한 뒤 static 3DGS optimize 부터 해야<br> 아래의 장점들 있음 <ul> <li>3DGS 일부가 dynamic part에 분포되도록 함</li> <li>3DGS를 미리 학습해야 deformation field가 dynamic part에 더 집중 가능</li> <li>deformation field 학습 시 numerical errors를 방지하여 훈련 과정이 더 stable</li> </ul> </li> </ul> </li> <li>Multi-head Gaussian Deformation Decoder : <ul> <li>3D Gaussian motion을 modeling함으로써 dynamic scene을 잘 표현할 수 있도록 해줌</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/15-480.webp 480w,/assets/img/2024-09-14-4DGS/15-800.webp 800w,/assets/img/2024-09-14-4DGS/15-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-14-4DGS/15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Neural Voxel Encoder : <ul> <li>implicit MLP-based neural voxel encoder (voxel grid)가 아니라<br> explicit Dynamic 3DGS 기법을 사용할 경우<br> rendering quality는 떨어지지만 FPS 및 storage는 향상</li> </ul> </li> <li>Two-stage Training : <ul> <li>static 3DGS stage \(\rightarrow\) dynamic 4DGS stage (fine-tuning) 으로<br> 분할해서 학습할 경우 성능 향상<br> (참고로 D-NeRF, DyNeRF에서는 point-clouds가 주어지지 않아서 어려운 task를 다룸)</li> </ul> </li> <li>Image-based Loss : <ul> <li>LPIPS loss, SSIM loss 같은 image-based loss를 사용할 경우<br> training speed도 느려지고 quality도 떨어짐</li> <li>그 이유는<br> image-based loss로 motion 부분을 fine-tuning하는 건 어렵고 복잡</li> </ul> </li> <li>Model Capacity (MLP size) : <ul> <li>voxel plane resol. 또는 MLP 크기가 증가할수록<br> quality 향상되지만 FPS 및 storage 악화</li> </ul> </li> <li>Fast Training : <ul> <li>7k iter. 까지만 학습해도(training 시간 짧음) 괜찮은 PSNR 달성</li> </ul> </li> </ul> <h3 id="discussion">Discussion</h3> <ul> <li>Tracking with 3D Gaussians : <ul> <li>Dynamic3DGS <d-cite key="dyna3DGS">[34]</d-cite> 와 달리<br> 본 논문은 monocular setting에서도 low storage로 3D object tracking 가능<br> (e.g. 10MB for 3DGS and 8MB for deformation field network)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/10-480.webp 480w,/assets/img/2024-09-14-4DGS/10-800.webp 800w,/assets/img/2024-09-14-4DGS/10-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-14-4DGS/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Composition(Editing) with 4D Gaussians : <ul> <li>Dynamic3DGS <d-cite key="dyna3DGS">[34]</d-cite> 에서처럼<br> 4DGS editing 가능</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/11-480.webp 480w,/assets/img/2024-09-14-4DGS/11-800.webp 800w,/assets/img/2024-09-14-4DGS/11-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-14-4DGS/11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Rendering Speed (FPS) : <ul> <li>3DGS 수와 FPS는 반비례 관계인데<br> Gaussians 수가 30,000개 이하이면 single RTX 3090 GPU에서 90 FPS 까지 가능</li> <li>이처럼 real-time FPS를 달성하려면<br> resolution, Gaussian 수, Gaussian deformation field network 용량, hardware constraints 등 여러 요인을 조절해야 함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-09-14-4DGS/14-480.webp 480w,/assets/img/2024-09-14-4DGS/14-800.webp 800w,/assets/img/2024-09-14-4DGS/14-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-09-14-4DGS/14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="limitation">Limitation</h3> <ul> <li>아래의 경우엔 학습 잘 안 됨 <ul> <li> <code class="language-plaintext highlighter-rouge">large motions</code>일 경우</li> <li> <code class="language-plaintext highlighter-rouge">background points</code>가 없을 경우</li> <li> <code class="language-plaintext highlighter-rouge">camera pose</code>가 <code class="language-plaintext highlighter-rouge">unprecise</code>(부정확)할 경우</li> </ul> </li> <li> <p>추가적인 supervision 없이<br> <code class="language-plaintext highlighter-rouge">static</code> Gaussians와 <code class="language-plaintext highlighter-rouge">dynamic</code> Gaussians의 joint motion을 구분하는 건 아직 어려운 과제</p> </li> <li> <code class="language-plaintext highlighter-rouge">urban(large)-scale</code> recon.일 경우엔<br> 3DGS 수가 훨씬 많아서<br> Gaussian deformation field network를 query하기에 너무 무거우므로 좀 더 compact한 algorithm이 필요</li> </ul> <h3 id="conclusion">Conclusion</h3> <ul> <li> <p>4DGS framework for <code class="language-plaintext highlighter-rouge">real-time</code> <code class="language-plaintext highlighter-rouge">dynamic</code> scene rendering</p> </li> <li>efficient deformation field network to model motions and shape-deformation <ul> <li>Spatial-temporal structure encoder :<br> adjacent Gaussians가 비슷하게 encode되도록 spatial-temporal 정보를 encode</li> <li>Multi-head Gaussian deformation decoder :<br> position, scaling, rotation을 각각 modeling</li> </ul> </li> <li>dynamic scenes 모델링 뿐만 아니라<br> 4D object tracking 및 editing에도 활용 가능</li> </ul> <h2 id="code-flow">Code Flow</h2> <ul> <li>TBD</li> </ul> <h2 id="question">Question</h2> <ul> <li>Q1 : 본 논문을 한 문장으로 요약하자면,<br> 3DGS를 dynamic scene에 적용하고자 할 때 4D 정보를 효율적으로 encode하기 위해 2D planes로 scene을 표현하는 HexPlane 기법을 이용하겠다!인데,<br> 본 논문이 novelty가 있는지 의구심이 듭니다.</li> <li> <p>A1 : 3DGS 논문 자체가 나온 지 얼마 안 돼서<br> 기존 논문(HexPlane) 아이디어를 3DGS에 적용하는 논문들이 아직까지는 많이 채택되는 것 같다.</p> </li> <li>Q2 : 본 포스팅에서 코드 리뷰는 encoder (HexPlane) 쪽만 진행하였는데,<br> Multi-head Gaussian deformation decoder로 position, scaling, rotation 변화량을 구해서<br> Deformed(변형된) 3DGS를 구하는 부분의 코드도 보고 싶습니다.</li> <li>A2 : 포스팅의 “Extremely Tiny Multi-head Gaussian Deformation Decoder” 부분에 해당 내용을 추가하였습니다.</li> </ul> <h2 id="appendix">Appendix</h2> <ul> <li>TBD</li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-09-14-4DGS.bib"></d-bibliography> <div id="disqus_thread" style="max-width: 800px; margin: 0 auto;"> <script type="text/javascript">var disqus_shortname="semyeong-yu",disqus_identifier="/blog/2024/4DGS",disqus_title="4D Gaussian Splatting";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Semyeong Yu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5YMLX9VHFX"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5YMLX9VHFX");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>