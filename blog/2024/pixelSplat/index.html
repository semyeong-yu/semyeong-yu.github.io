<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> pixelSplat | Semyeong Yu </title> <meta name="author" content="Semyeong Yu"> <meta name="description" content="3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction (CVPR 2024)"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://semyeong-yu.github.io/blog/2024/pixelSplat/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "pixelSplat",
            "description": "3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction (CVPR 2024)",
            "published": "October 25, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Semyeong</span> Yu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>pixelSplat</h1> <p>3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction (CVPR 2024)</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#abstract">Abstract</a> </div> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#background">Background</a> </div> <div> <a href="#scale-ambiguity">Scale Ambiguity</a> </div> <div> <a href="#gaussian-parameter-prediction">Gaussian Parameter Prediction</a> </div> <div> <a href="#experiments">Experiments</a> </div> </nav> </d-contents> <h2 id="pixelsplat---3d-gaussian-splats-from-image-pairs-for-scalable-generalizable-3d-reconstruction-cvpr-2024">pixelSplat - 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction (CVPR 2024)</h2> <h4 id="david-charatan-sizhe-li-andrea-tagliasacchi-vincent-sitzmann">David Charatan, Sizhe Li, Andrea Tagliasacchi, Vincent Sitzmann</h4> <blockquote> <p>paper :<br> <a href="https://arxiv.org/abs/2312.12337" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2312.12337</a><br> project website :<br> <a href="https://davidcharatan.com/pixelsplat/" rel="external nofollow noopener" target="_blank">https://davidcharatan.com/pixelsplat/</a><br> code :<br> <a href="https://github.com/dcharatan/pixelsplat" rel="external nofollow noopener" target="_blank">https://github.com/dcharatan/pixelsplat</a><br> reference :<br> NeRF and 3DGS Study</p> </blockquote> <h3 id="abstract">Abstract</h3> <ul> <li>pixelSplat :<br> reconstruct a 3DGS primitive-based parameterization of 3D radiance field from only two images</li> </ul> <h3 id="introduction">Introduction</h3> <ul> <li>Problem : <ul> <li> <code class="language-plaintext highlighter-rouge">scale ambiguity</code> :<br> camera pose has arbitrary scale factor</li> <li> <code class="language-plaintext highlighter-rouge">local minima</code> :<br> primitive param.을 random initialization으로부터 직접 optimize하면 local minima 문제 발생</li> </ul> </li> <li>Contribution : <ul> <li>two-view image encoder :<br> <code class="language-plaintext highlighter-rouge">two-view Epipolar Sampling, Epipolar Attention</code> 덕분에<br> scale ambiguity 문제 극복</li> <li>pixel-aligned Gaussian param. prediction module :<br> depth를 <code class="language-plaintext highlighter-rouge">sampling</code>하기 때문에<br> local minima 문제 극복</li> </ul> </li> <li>Solution : <ul> <li>feed-forward model 이, a pair of images로부터,<br> 3DGS primitives로 parameterized되는 3D radiance field recon.을 학습</li> </ul> </li> <li>model : <ul> <li> <code class="language-plaintext highlighter-rouge">per-scene model</code> :<br> <code class="language-plaintext highlighter-rouge">각각의 scene</code>을 학습하기 위해 <code class="language-plaintext highlighter-rouge">정해진 하나의 points set</code>을 <code class="language-plaintext highlighter-rouge">iteratively</code> update<br> \(\rightarrow\)<br> local minima 등 문제 있어서<br> 3DGS에서는 non-differentiable Adaptive Density Control 기법으로 해결하려 하지만<br> 이는 일반화 불가능</li> <li> <code class="language-plaintext highlighter-rouge">feed-forward model</code> :<br> <code class="language-plaintext highlighter-rouge">scene마다 얻은 points set</code>을 <code class="language-plaintext highlighter-rouge">한 번에 feed-forward</code>로 넣어서 학습<br> differentiable하게 일반화 가능 <ul> <li>attention</li> <li>MASt3R(-SfM), Spann3R, Splatt3R, DUSt3R (잘 모름. 더 서치해봐야 함.)</li> </ul> </li> </ul> </li> </ul> <h3 id="background">Background</h3> <ul> <li>local minima : <ul> <li>언제 발생? :<br> random location에 initialize된 Gaussian primitives가<br> 최종 목적지까지 a few std보다 더 <code class="language-plaintext highlighter-rouge">많이 움직</code>여야 될 때<br> gradient가 vanish 되어버려서<br> 또는<br> 최종 목적지까지 loss가 monotonically decrease하지 않을 때<br> local minima 발생</li> <li>해결법? :<br> 3DGS에서는<br> non-differentiable pruning and splitting 기법인<br> Adaptive Density Control을 사용하지만<br> 본 논문에서는<br> <code class="language-plaintext highlighter-rouge">differentiable</code> parameterization of Gaussian primitives 소개</li> </ul> </li> </ul> <h3 id="scale-ambiguity">Scale Ambiguity</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/4-480.webp 480w,/assets/img/2024-10-25-pixelSplat/4-800.webp 800w,/assets/img/2024-10-25-pixelSplat/4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-25-pixelSplat/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Scale Ambiguity 문제 : <ul> <li> <code class="language-plaintext highlighter-rouge">SfM 단계</code>에서 camera pose를 계산할 때<br> real-world-scale pose \(T_{j}^{m}\) 을<br> metric pose \(s_{i} T_{j}^{m}\) 으로 scale하여 사용 <ul> <li>\(s_{i}\) :<br> arbitrary scale factor</li> <li>metric pose \(s_{i} T_{j}^{m}\) :<br> real-world-scale pose의 translation component를 \(s_{i}\) 만큼 scale</li> </ul> </li> <li>single image의 camera pose \(s_{i} T_{j}^{m}\) 로부터<br> arbitrary scale factor \(s_{i}\) 를 복원하는 건 불가능</li> </ul> </li> <li>Scale Ambiguity 해결 : <ul> <li>two-view encoder에서 <code class="language-plaintext highlighter-rouge">a pair of images</code> 로부터<br> arbitrary scale factor \(s_{i}\) 복원</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/1-480.webp 480w,/assets/img/2024-10-25-pixelSplat/1-800.webp 800w,/assets/img/2024-10-25-pixelSplat/1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-25-pixelSplat/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Two-view encoder Overview : <ul> <li>Step 1) Per-Image Encoder</li> <li>Step 2) Epipolar Sampling</li> <li>Step 3) Epipolar Attention</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/2-480.webp 480w,/assets/img/2024-10-25-pixelSplat/2-800.webp 800w,/assets/img/2024-10-25-pixelSplat/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-25-pixelSplat/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Step 1) Per-Image Encoder </div> <ul> <li>Step 1) <code class="language-plaintext highlighter-rouge">Per-Image Encoder</code> :<br> each view (two images)를 각각 feature \(F\), \(\tilde F\) 로 encode</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/3-480.webp 480w,/assets/img/2024-10-25-pixelSplat/3-800.webp 800w,/assets/img/2024-10-25-pixelSplat/3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-25-pixelSplat/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Step 2) Epipolar Sampling </div> <ul> <li>Step 2) <code class="language-plaintext highlighter-rouge">Epipolar Sampling</code> :<br> Features 1 from Image 1의 <code class="language-plaintext highlighter-rouge">ray</code>로 <code class="language-plaintext highlighter-rouge">query</code> 만들고,<br> Features 2 from Image 2의 <code class="language-plaintext highlighter-rouge">epipolar samples</code> 및 <code class="language-plaintext highlighter-rouge">depth</code> 로 <code class="language-plaintext highlighter-rouge">key, value</code> 만들어서,<br> attention으로 depth scale을 잘 학습하는 게 목적<br> (attention : depth 정보와 함께, Image 1의 ray가 Image 2의 epipolar line 위 어떤 sample에 더 많이 attention하는지)<br> (epipolar line은 학습하는 게 아니라 수학 식으로 계산) <ul> <li>Query :<br> \(q = Q \cdot F [u]\)<br> where \(F\) : Features 1 from Image 1 <br> where \(F [u]\) : ray feature at each pixel (in pixel coordinate)</li> <li>Key, Value :<br> \(s = \tilde F [\tilde u_{l}] \oplus \gamma (\tilde d_{\tilde u_{l}})\)<br> where \(\tilde F\) : Features 2 from Image 2<br> where \(\tilde F [\tilde u_{l}]\) : samples on epipolar line<br> where \(\tilde d_{\tilde u_{l}}\) : Image 2의 camera 원점까지의 거리 <ul> <li> \[k_{l} = K \cdot s\] </li> <li> \[v_{l} = V \cdot s\] </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/5-480.webp 480w,/assets/img/2024-10-25-pixelSplat/5-800.webp 800w,/assets/img/2024-10-25-pixelSplat/5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-25-pixelSplat/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Step 3) Epipolar Attention 중 Epipolar Cross-Attention </div> <ul> <li>Step 3) Epipolar Attention : <ul> <li> <code class="language-plaintext highlighter-rouge">Epipolar Cross-Attention</code> :<br> 앞서 만든 \(q, k_{l}, v_{l}\) 로 <code class="language-plaintext highlighter-rouge">cross-attention 수행</code>하여<br> per-pixel <code class="language-plaintext highlighter-rouge">correpondence b.w. ray and epipolar sample</code> 찾음으로써<br> per-pixel feature \(F [u]\) 가 이제<br> arbitrary scale factor \(s\) 에 consistent한<br> <code class="language-plaintext highlighter-rouge">scaled depth를 encode</code>하도록 update <ul> <li>\(F [u] += Att(q, k_{l}, v_{l})\)<br> where \(+=\) : skip-connection<br> where \(Att\) : softmax attention</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">Per-Image Self-Attention</code> :<br> Cross-Attention 끝난 뒤 마지막에 Per-Image Self-Attention 수행하여<br> propagate scaled depth estimates<br> to parts of the image feature maps<br> that may not have any epipolar correspondences <ul> <li> \[F += SelfAttention(F)\] </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/9-480.webp 480w,/assets/img/2024-10-25-pixelSplat/9-800.webp 800w,/assets/img/2024-10-25-pixelSplat/9-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-25-pixelSplat/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="gaussian-parameter-prediction">Gaussian Parameter Prediction</h3> <ul> <li>앞선 과정들 덕분에<br> scale-aware feature map \(F, \tilde F\) 를 이용하여<br> Gaussian param. \(g_{k} = (\mu_{k}, \Sigma_{k}, \alpha_{k}, S_{k})\) 를 예측 <ul> <li>2D image 상의 <code class="language-plaintext highlighter-rouge">모든 각 pixel은 3D 상의 point에 대응</code>되어<br> 최종적인 Gaussian primitives set은<br> just union of each image</li> </ul> </li> <li>3D position Prediction : <ul> <li>방법 1) Baseline :<br> predict point estimate of 3D position \(\mu\) <ul> <li>\(\boldsymbol \mu = \boldsymbol o + d_{u} \boldsymbol d\)<br> where \(u\) : 2D pixel coordiante<br> where \(\boldsymbol d = R K^{-1} [u, 1]^{T}\) : ray direction<br> where \(d_{u} = g_{\theta}(F [u])\) : depth obtained by neural network</li> <li>문제 :<br> depth 자체를 neural network로 추정하는 건 local minima 문제 발생하기 쉬움</li> </ul> </li> <li>방법 2) 본 논문 방식 :<br> predict <code class="language-plaintext highlighter-rouge">probability density</code> of 3D position \(\mu\) <ul> <li>핵심 :<br> neural network로<br> depth 자체를 예측하는 게 아니라<br> differentiable probability distribution of likelihood of depth along ray를 예측</li> <li>Step 1)<br> depth를 \(Z\)-bins로 discretize<br> \(b_{z} = ((1 - \frac{z}{Z})(\frac{1}{d_{near}} - \frac{1}{d_{far}}) + \frac{1}{d_{far}})^{-1} \in [d_{near}, d_{far}]\)<br> for \(z \in [0, Z]\) : depth index</li> <li>Step 2)<br> discrete probability \(\phi\) 로부터 index \(z\) 를 sampling<br> \(z \sim p_{\phi}(z)\)</li> <li>Step 3)<br> ray를 쏴서(unproject) Gaussian mean \(\mu\) 계산<br> \(\boldsymbol \mu = \boldsymbol o + (b_{z} + \delta_{z}) \boldsymbol d\)<br> where \(\phi\) : depth(\(z\)) probability obtained by neural network<br> where \(\delta_{z}\) : depth offset obtained by neural network</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/6-480.webp 480w,/assets/img/2024-10-25-pixelSplat/6-800.webp 800w,/assets/img/2024-10-25-pixelSplat/6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-25-pixelSplat/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Gaussian Parameter Prediction : <ul> <li>scale-aware feature map \(F, \tilde F\) 과 neural network \(f\) 를 이용하여<br> \(\phi, \delta, \Sigma, S = f(F [u])\)<br> where \(\phi, \delta, \Sigma, S\) : depth probability, depth offset, covariance, spherical harmonics coeff. <ul> <li> <code class="language-plaintext highlighter-rouge">3D position</code>(mean) :<br> \(\phi, \delta\) 이용해서<br> \(\boldsymbol \mu = \boldsymbol o + (b_{z} + \delta_{z}) \boldsymbol d\)</li> <li> <code class="language-plaintext highlighter-rouge">Covariance</code> :<br> \(\Sigma\)</li> <li> <code class="language-plaintext highlighter-rouge">Spherical Harmonics Coeff.</code> :<br> \(S\)</li> <li> <code class="language-plaintext highlighter-rouge">Opacity</code> :<br> \(\phi\) 이용해서<br> \(\alpha = \phi_{z}\)<br> \(=\) probability of sampled depth \(z\)<br> (so that we make sampling differentiable)</li> </ul> </li> <li>각 pixel마다 3DGS point에 대응되므로<br> pixel-aligned Gaussians라고 부름</li> </ul> </li> </ul> <h3 id="experiments">Experiments</h3> <ul> <li>Setup : <ul> <li>Dataset :<br> camera pose is computed by SfM <ul> <li>RealEstate 10k</li> <li>ACID</li> </ul> </li> <li>Baseline : <ul> <li>pixelNeRF</li> <li>GPNR</li> <li>Method of Du et al.</li> </ul> </li> <li>Metric : <ul> <li>PSNR</li> <li>SSIM</li> <li>LPIPS</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/7-480.webp 480w,/assets/img/2024-10-25-pixelSplat/7-800.webp 800w,/assets/img/2024-10-25-pixelSplat/7-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-25-pixelSplat/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Result : <ul> <li>performance much better</li> <li>inference time faster</li> <li>less memory per ray</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/8-480.webp 480w,/assets/img/2024-10-25-pixelSplat/8-800.webp 800w,/assets/img/2024-10-25-pixelSplat/8-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-25-pixelSplat/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-10-25-pixelSplat/10-480.webp 480w,/assets/img/2024-10-25-pixelSplat/10-800.webp 800w,/assets/img/2024-10-25-pixelSplat/10-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-10-25-pixelSplat/10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Ablation Study </div> <ul> <li>Ablation Study : <ul> <li>Epipolar Encoder : Epipolar Sampling and Epipolar Attention</li> <li>Depth Encoding : freq.-based positional encoding \(\gamma(\tilde d_{\tilde u_{l}})\)</li> <li>Probabilistic Sampling : depth index \(z \sim p_{\phi}(z)\)</li> <li>Depth Regularization : <code class="language-plaintext highlighter-rouge">???</code> </li> </ul> </li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> <div id="disqus_thread" style="max-width: 800px; margin: 0 auto;"> <script type="text/javascript">var disqus_shortname="semyeong-yu",disqus_identifier="/blog/2024/pixelSplat",disqus_title="pixelSplat";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Semyeong Yu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5YMLX9VHFX"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5YMLX9VHFX");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>