<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Quark | Semyeong Yu </title> <meta name="author" content="Semyeong Yu"> <meta name="description" content="Real-time, High-resolution, and General Neural View Synthesis (SIGGRAPH 2024)"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://semyeong-yu.github.io/blog/2024/Quark/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Quark",
            "description": "Real-time, High-resolution, and General Neural View Synthesis (SIGGRAPH 2024)",
            "published": "December 23, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Semyeong</span> Yu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Quark</h1> <p>Real-time, High-resolution, and General Neural View Synthesis (SIGGRAPH 2024)</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#contribution">Contribution</a> </div> <div> <a href="#related-works">Related Works</a> </div> <div> <a href="#overview">Overview</a> </div> <div> <a href="#method">Method</a> </div> <div> <a href="#result">Result</a> </div> <div> <a href="#discussion">Discussion</a> </div> </nav> </d-contents> <h2 id="quark---real-time-high-resolution-and-general-neural-view-synthesis">Quark - Real-time, High-resolution, and General Neural View Synthesis</h2> <h4 id="john-flynn-michael-broxton-lukas-murmann-lucy-chai-matthew-duvall-clément-godard-kathryn-heal-srinivas-kaza-stephen-lombardi-xuan-luo-supreeth-achar-kira-prabhu-tiancheng-sun-lynn-tsai-ryan-overbeck">John Flynn, Michael Broxton, Lukas Murmann, Lucy Chai, Matthew DuVall, Clément Godard, Kathryn Heal, Srinivas Kaza, Stephen Lombardi, Xuan Luo, Supreeth Achar, Kira Prabhu, Tiancheng Sun, Lynn Tsai, Ryan Overbeck</h4> <blockquote> <p>paper :<br> <a href="https://arxiv.org/abs/2411.16680" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2411.16680</a><br> project website :<br> <a href="https://quark-3d.github.io/" rel="external nofollow noopener" target="_blank">https://quark-3d.github.io/</a><br> reference :<br> Presentation of https://charlieppark.kr from 3D-Nerd Community</p> </blockquote> <h2 id="contribution">Contribution</h2> <ul> <li>Architecture : <ul> <li>3D space에서 ray를 쏘거나(NeRF) Gaussian list를 구해서(3DGS) alpha-compositing하는 게 아니라<br> <code class="language-plaintext highlighter-rouge">layered RGB image(or depth map)</code>를 구해서 alpha-compositing</li> <li> <code class="language-plaintext highlighter-rouge">target view가 어떤 input view에 얼만큼 attention해야 하는지</code>를 <code class="language-plaintext highlighter-rouge">iteratively refine</code>하여<br> Blend Weights를 구해서 input view들을 interpolate하는 방식</li> <li>refinement로 Blend Weights 구해서<br> input images를 blend하여 layered RGB images를 구하므로<br> input view가 멀리멀리 sparse하게 떨어져 있어야<br> recon.할 때 모든 영역 커버 가능</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">Generalizable</code> : <ul> <li>pre-trained model 가져온 뒤<br> pre-trained model이 학습하지 못했던 <code class="language-plaintext highlighter-rouge">unseen scene</code>에 대해<br> <code class="language-plaintext highlighter-rouge">fine-tuning 없이</code> <code class="language-plaintext highlighter-rouge">refinement</code>로<br> layered depth map 쫘르륵 얻어내서 novel view recon. 가능!</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">Real-time</code> <code class="language-plaintext highlighter-rouge">Reconstruction</code> and Rendering : <ul> <li>3DGS에서는 real-time rendering이었는데<br> 본 논문은 recon. 자체도 real-time<br> (inference하는 데 총 33ms at 1080p with single A100 GPU)</li> </ul> </li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li>Generalizable : <ul> <li>IBRNet :<br> rendering 시간은 오래 걸리지만 generalizable</li> <li>ENeRF :<br> cost volume, depth-guided sampling, volume rendering 사용</li> <li>GPNR :<br> 2-view VFT, Epipolar Transformer 사용</li> <li>CO3D - NeRFormer :<br> 반복 between attention on feature-dim. and attention on ray-direction-dim.</li> </ul> </li> <li>Quark의 직계 조상 paper : <ul> <li>DeepView <d-cite key="DeepView">[1]</d-cite> : <ul> <li>MPI (여러 depth에 대해 image를 중첩한 multi-plane image)</li> <li>한계 : input view와 target view 간의 camera 이동이 크면 안 됨</li> </ul> </li> <li>Immersive light field video with a layered mesh representation <d-cite key="Immersive">[2]</d-cite> : <ul> <li>MSI (여러 depth에 대해 곡면 image를 중첩한 multi-spherical image) (= layered mesh)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/2.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/2.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-12-23-Quark/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="overview">Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/3.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/3.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-12-23-Quark/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>I/O : <ul> <li>input : sparse multi-view images (\(\in R^{M \times H \times W \times 3}\))<br> (sensitive to view selection)<br> (pose 정보 필요)</li> <li>output : novel view image</li> <li>Quark는 pretrained model (pretrained with 8 input views of scenes(Spaces, RFF, Nex-Shiny, and SWORD)) 가져와서<br> unseen scene에 대한 refinement로 novel target view synthesis 가능 (generalizable) <ul> <li>Spaces : Quark의 직계 조상 격인 DeepView에서 사용한 dataset</li> <li>RFF : NeRF에서 사용한 Real Forward Facing dataset</li> <li>Nex-Shiny : NeX에서 사용한 shiny object이 포함된 dataset</li> <li>SWORD : real-world scene dataset</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/1.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/1.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/1.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-12-23-Quark/1.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Architecture :<br> U-Net style <ul> <li>Encoder :<br> Obtain feature pyramid \(I_{\downarrow 8}, I_{\downarrow 4}, I_{\downarrow 2}, I_{\downarrow 0}\) from input image</li> <li>Iterative Updates : <ul> <li>pre-trained model을 가져와서 학습하는데,<br> layered depth map을 업데이트하는 방법은<br> gradient descent 이용한 <code class="language-plaintext highlighter-rouge">fine-tuning이 아니라</code><br> input view feature 이용한 <code class="language-plaintext highlighter-rouge">refinement</code>임!!</li> <li>U-Net skip-connection과 비슷하지만 <code class="language-plaintext highlighter-rouge">Update &amp; Fuse 단계가 novel</code><br> (아래에서 별도로 설명)</li> </ul> </li> <li>Upsample &amp; Activate : <ul> <li>image resolution으로 upsample한 뒤<br> Layered Depth Map at target view 구함 <ul> <li>Depth \(d \in R^{L \times H \times W \times 1}\)<br> (이 때, depth map은 linear in disparity (가까운 high-freq. 영역에서 더 촘촘히))</li> <li>Opacity \(\sigma \in R^{L \times H \times W \times 1}\)</li> <li>Blend Weights \(\beta \in R^{L \times H \times W \times M}\)<br> by attention softmax weight</li> </ul> </li> </ul> </li> <li>Rendering : <ul> <li>input images \(\in R^{M \times H \times W \times 3}\) 를 Layered Depth Map (target view)로 back-project한 뒤<br> Blend Weights \(\beta\) 로 input images를 blend해서 per-layer RGB 얻음</li> <li>Opacity \(\sigma\) 로 per-layer RGB를 alpha-compositing해서 final RGB image at target view 얻고,<br> Opacity \(\sigma\) 로 Depth \(d\) 를 alpha-compositing해서 Depth Map 얻음</li> <li>training할 때는 stadard differentiable rendering 사용하지만<br> inference할 때는 1080p resol. at 1.3 ms per frame 위해 CUDA-optimized renderer 사용</li> </ul> </li> </ul> </li> </ul> <h2 id="method">Method</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/4.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/4.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-12-23-Quark/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Update &amp; Fuse : <ul> <li>Step 1) Render to Input Views <ul> <li>from <code class="language-plaintext highlighter-rouge">layer space (target view)</code> to <code class="language-plaintext highlighter-rouge">image space (input view)</code><br> (feature pyramid \(I_{\downarrow k}\) 와 합치기 위해!)</li> <li>feature volume \(V^{(n)}\)<br> \(\rightarrow\) obtain appearance \(a\), density \(\sigma\), depth map \(d\)<br> (depth map \(d = \delta + \text{tanh}\) 는 depth anchor \(\delta\) 근처의 depth)<br> \(\rightarrow\) project from target-view into input-view by \(P_{\theta}\)<br> \(\rightarrow\) obtain rendered feature \(\tilde I\) by alpha-compositing \(O\) at input-view<br> (\(\tilde I\) : intermediate LDM(layered depth map))</li> </ul> </li> <li>Step 2) Update Block <ul> <li> <code class="language-plaintext highlighter-rouge">rendered feature</code> \(\tilde I\) 를<br> <code class="language-plaintext highlighter-rouge">feature pyramid</code> \(I_{\downarrow k}\), <code class="language-plaintext highlighter-rouge">input view-direction</code> \(\gamma\) 등 input image에 대한 정보와 섞음 <ul> <li>input view-direction 넣어줄 때 Ray Encoding \(\gamma\) 수행 : <ul> <li>obtain difference vector (아래 그림 참고)<br> (input view가 target view에서 멀리 떨어져 있을수록 값이 큼)<br> \(\rightarrow\) tanh and Sinusoidal PE</li> <li>tanh 사용하므로<br> difference vector가 0 근처일 때<br> 즉, input view가 target view 근처일 때 gradient 많이 반영</li> <li>input view’s ray가 frustum 밖으로 벗어나더라도<br> near, far plane과의 교점을 구할 수 있으므로<br> Ray Encoding 가능</li> <li>view-direction 넣어줘야<br> view-dependent color 만들 수 있고<br> reflection, non-lambertian surface 잘 구현 가능</li> </ul> </li> </ul> </li> </ul> </li> <li>Step 3) Back-project <ul> <li>from <code class="language-plaintext highlighter-rouge">image space (input view)</code> to <code class="language-plaintext highlighter-rouge">layer space (target view)</code><br> (feature volume \(V^{(n)}\) 과 합치기 위해!)</li> <li>back-project from input-view into target-view by \(P_{\theta}^{T} (I, d)\)<br> \(\rightarrow\) obtain residual feature \(\Delta\)</li> </ul> </li> <li>Step 4) One-to-Many Attention <ul> <li> <code class="language-plaintext highlighter-rouge">feature volume</code> \(V^{(n)}\) 을 <code class="language-plaintext highlighter-rouge">query</code>로,<br> Step 1~3)에서 얻은 <code class="language-plaintext highlighter-rouge">residual feature</code> \(\Delta\) 를 <code class="language-plaintext highlighter-rouge">key, value</code>로 하여<br> One-to-Many attention 수행하여<br> updated feature volume \(V^{(n+1)}\) 얻음<br> Then, target view가 input view의 feature들을 aggregate하여 이용할 수 있게 됨!!<br> 즉, target view가 어떤 input view에 얼만큼 attention해야 하는지! <ul> <li> <code class="language-plaintext highlighter-rouge">query</code> : <code class="language-plaintext highlighter-rouge">target view</code> 정보 at target view space</li> <li> <code class="language-plaintext highlighter-rouge">key, value</code> : <code class="language-plaintext highlighter-rouge">input view</code> 정보 at target view space</li> <li> <code class="language-plaintext highlighter-rouge">One-to-Many attention</code> : <ul> <li>cross-attention과 비슷하지만<br> <code class="language-plaintext highlighter-rouge">redundant matrix multiplication 없애서</code><br> complexity 줄여서<br> real-time reconstruction에 기여!</li> <li>\(\text{MultiHead}(Q, K, V) = \text{concat}(\text{head}_{1}, \cdots, \text{head}_{h}) W^{O}\)<br> where \(\text{head}_{i} = \text{Attention}(QW_{i}^{Q}, KW_{i}^{K}, VW_{i}^{V})\) 식을 써보면<br> \(W_{i}^{Q} (W_{i}^{K})^{T}\) 항에서 \(W^{Q}\) 와 \(W^{K}\) 가 redundant 하고<br> \(\text{concat}(\cdots W_{i}^{V}) W^{O}\) 항에서 \(W^{V}\) 와 \(W^{O}\) 가 redundant 하므로<br> \(\text{head}_{i} = \text{Attention}(QW_{i}^{Q}, K, V)\) 로 바꿔서<br> \(W^{Q}\) 와 \(W^{O}\) 만 사용</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/5.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/5.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/5.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-12-23-Quark/5.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> difference vector for input view-direction (Ray Encoding) </div> <h2 id="result">Result</h2> <ul> <li>Training : <ul> <li>Dataset : Spaces, RFF, Nex-Shiny, SWORD</li> <li>Loss : \(\text{10 * L1} + \text{LPIPS}\)</li> <li>Input : 8 views (randomly sampled from 16 views nearest to object)</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/6.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/6.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/6.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-12-23-Quark/6.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Inference time : recon.까지 포함해서 총 33ms at 1080p single A100 GPU</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/7.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/7.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/7.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-12-23-Quark/7.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Generalizable method와의 비교 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/8.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/8.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/8.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-12-23-Quark/8.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/9.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/9.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/9.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-12-23-Quark/9.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Non-Generalizable method와의 비교 </div> <h2 id="discussion">Discussion</h2> <ul> <li>Limitation : <ul> <li>view selection :<br> training할 때 sparse(8개) input views를 사용하는데, <code class="language-plaintext highlighter-rouge">view selection</code>에 매우 민감 (중요함) (heuristic)</li> <li>Blend Weights :<br> target view RGB image를 rendering하기 전에 input RGB images를 blend하는데, <ul> <li> <code class="language-plaintext highlighter-rouge">view dependency</code>를 잘 캡처 못한다<br> \(\rightarrow\) Ray Encoding으로 해소하긴 함</li> <li>input images의 <code class="language-plaintext highlighter-rouge">focal length</code>가 각기 다르면 잘 recon.하지 못한다</li> </ul> </li> <li>sparse input :<br> real-time rendering 뿐만 아니라 real-time recon. 위해<br> 적은 수(8 ~ 16)의 <code class="language-plaintext highlighter-rouge">sparse inputs</code> 사용</li> <li>light, shadow 고려 X</li> <li>conv. network를 일반화에 사용했을 때 생기는 깨지는 artifacts 발생 (홈페이지 영상 참고)</li> </ul> </li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-12-23-Quark.bib"></d-bibliography> <div id="disqus_thread" style="max-width: 800px; margin: 0 auto;"> <script type="text/javascript">var disqus_shortname="semyeong-yu",disqus_identifier="/blog/2024/Quark",disqus_title="Quark";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Semyeong Yu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5YMLX9VHFX"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5YMLX9VHFX");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>