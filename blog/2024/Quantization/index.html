<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Quantization | Semyeong Yu </title> <meta name="author" content="Semyeong Yu"> <meta name="description" content="Quantization in Pytorch and ONNX"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://semyeong-yu.github.io/blog/2024/Quantization/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8.0.6/dist/styles.min.css"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-bundle.min.css" integrity="sha256-yUoNxsvX+Vo8Trj3lZ/Y5ZBf8HlBFsB6Xwm7rH75/9E=" crossorigin="anonymous"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Semyeong</span> Yu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Quantization</h1> <p class="post-meta"> March 02, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/quantization"> <i class="fa-solid fa-hashtag fa-sm"></i> quantization</a>     ·   <a href="/blog/category/quantization"> <i class="fa-solid fa-tag fa-sm"></i> quantization</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h1"> <a href="#introduction">Introduction</a> <ul> <li class="toc-entry toc-h3"><a href="#the-increasing-demand-for-deep-learning-model-efficiency">The Increasing Demand for Deep-learning Model Efficiency</a></li> <li class="toc-entry toc-h3"><a href="#some-ways-to-achieve-faster-inference-speed">Some Ways to Achieve Faster Inference Speed</a></li> <li class="toc-entry toc-h3"><a href="#the-reason-why-quantization-can-be-a-powerful-technique-among-them">The Reason why Quantization can be a Powerful Technique among them</a></li> <li class="toc-entry toc-h3"><a href="#a-brief-explanation-of-quantization">A Brief Explanation of Quantization</a></li> </ul> </li> <li class="toc-entry toc-h1"> <a href="#method">Method</a> <ul> <li class="toc-entry toc-h2"> <a href="#quantization">Quantization</a> <ul> <li class="toc-entry toc-h3"><a href="#overview-of-quantization">Overview of Quantization</a></li> <li class="toc-entry toc-h3"><a href="#principle-of-quantization">Principle of Quantization</a></li> <li class="toc-entry toc-h3"><a href="#types-of-quantization">Types of Quantization</a></li> <li class="toc-entry toc-h3"><a href="#model-fusion">Model Fusion</a></li> <li class="toc-entry toc-h3"><a href="#code-implementation-with-torchaoquantization-library">code implementation with torch.ao.quantization library</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#post-training-dynamic-quantization-in-pytorch">Post-Training Dynamic Quantization in Pytorch</a> <ul> <li class="toc-entry toc-h3"><a href="#code-implementation-with-torchaoquantization-library-1">code implementation with torch.ao.quantization library</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#post-training-static-quantization-in-pytorch">Post-Training Static Quantization in Pytorch</a> <ul> <li class="toc-entry toc-h3"><a href="#code-implementation-with-torchaoquantization-library-2">code implementation with torch.ao.quantization library</a></li> <li class="toc-entry toc-h3"><a href="#code-implementation-with-pytorch_quantization-library">code implementation with pytorch_quantization library</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#qatquantization-aware-training-in-pytorch">QAT(Quantization-Aware Training) in Pytorch</a> <ul> <li class="toc-entry toc-h3"><a href="#code-implementation-using-torchaoquantization-library">code implementation using torch.ao.quantization library</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#quantization-in-onnx">Quantization in ONNX</a> <ul> <li class="toc-entry toc-h3"><a href="#definition-of-onnx">Definition of ONNX</a></li> <li class="toc-entry toc-h3"><a href="#necessity-of-onnx">Necessity of ONNX</a></li> <li class="toc-entry toc-h3"><a href="#inference-in-onnx">Inference in ONNX</a></li> <li class="toc-entry toc-h3"><a href="#code-implementation-using-onnxruntime-library">code implementation using onnxruntime library</a></li> <li class="toc-entry toc-h3"><a href="#quantization-in-onnx-1">Quantization in ONNX</a></li> <li class="toc-entry toc-h3"><a href="#code-implementation-using-onnxruntime-library-cpu-qdq-format">code implementation using onnxruntime library (CPU, QDQ format)</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h1"> <a href="#result">Result</a> <ul> <li class="toc-entry toc-h2"> <a href="#shufflenetv2">shufflenetv2</a> <ul> <li class="toc-entry toc-h3"><a href="#pytorch">Pytorch</a></li> <li class="toc-entry toc-h3"><a href="#experiment">Experiment</a></li> <li class="toc-entry toc-h3"><a href="#result-1">Result</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#resnet18">resnet18</a> <ul> <li class="toc-entry toc-h3"><a href="#pytorch-1">Pytorch</a></li> <li class="toc-entry toc-h3"><a href="#exp-1--qat---the-effect-of-learning-rate">Exp 1. QAT - The effect of learning rate</a></li> <li class="toc-entry toc-h3"><a href="#exp-2-qat---the-effect-of-epoch-to-freeze-quantization-parameters-and-bn-stat">Exp 2. QAT - The effect of epoch to freeze quantization parameters and bn stat</a></li> <li class="toc-entry toc-h3"><a href="#exp-3-qat---the-effect-of-observer-to-calibrate-the-quantization-parameters">Exp 3. QAT - The effect of observer to calibrate the quantization parameters</a></li> <li class="toc-entry toc-h3"><a href="#experiment-1">Experiment</a></li> <li class="toc-entry toc-h3"><a href="#result-2">Result</a></li> <li class="toc-entry toc-h3"><a href="#generated-landmark">Generated landmark</a></li> <li class="toc-entry toc-h3"><a href="#onnx">ONNX</a></li> <li class="toc-entry toc-h3"><a href="#visualization-of-onnx-via-netronapp">Visualization of ONNX via netron.app</a></li> <li class="toc-entry toc-h3"><a href="#int8-quantization-of-onnx-runtime">INT8 Quantization of ONNX Runtime</a></li> <li class="toc-entry toc-h3"><a href="#experiment-2">Experiment</a></li> <li class="toc-entry toc-h3"><a href="#result-3">Result</a></li> <li class="toc-entry toc-h3"><a href="#generated-landmark-1">Generated landmark</a></li> <li class="toc-entry toc-h3"><a href="#debugging">Debugging</a></li> <li class="toc-entry toc-h3"><a href="#fp16-conversion-of-onnx-runtime">FP16 Conversion of ONNX Runtime</a></li> <li class="toc-entry toc-h3"><a href="#experiment-3">Experiment</a></li> <li class="toc-entry toc-h3"><a href="#result-4">Result</a></li> <li class="toc-entry toc-h3"><a href="#generated-landmark-2">Generated landmark</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h1"><a href="#conclusion">Conclusion</a></li> <li class="toc-entry toc-h1"> <a href="#reference">Reference</a> <ul> <li class="toc-entry toc-h2"> <a href="#pytorch-2">Pytorch</a> <ul> <li class="toc-entry toc-h3"><a href="#torchaoquantization">torch.ao.quantization</a></li> <li class="toc-entry toc-h3"><a href="#pytorch_quantization">pytorch_quantization</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#onnx-1">ONNX</a></li> <li class="toc-entry toc-h2"><a href="#other-references">Other references</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <h1 id="introduction"><strong>Introduction</strong></h1> <h3 id="the-increasing-demand-for-deep-learning-model-efficiency">The Increasing Demand for Deep-learning Model Efficiency</h3> <p>In the area of deep learning, where neural networks have remarkable capabilites to learn complex patterns from massive datasets, there has emerged an ongoing pursuit for model efficiency. We need to achieve quick inference speed and less memory consumption in order to apply our deep-learning models to a wider range of users and applications. Especially, there is an increasing demand for deploying our models on mobile devices or edge devices and running the models in real-time, but the devices have resource-constrained hardware. Therefore it became important to strike a balance between model accuracy and computational cost.</p> <h3 id="some-ways-to-achieve-faster-inference-speed">Some Ways to Achieve Faster Inference Speed</h3> <p>There are several ways to achieve faster inference speed of deep-learning model.</p> <ul> <li>Hardware accelerators : There are specialized hardware accelerators such as GPUs and TPUs which can be utilized to efficiently execute optimized deep-learning operations. We can achieve faster inference speed because the hardware accelerators are designed to utilize multi-threading or mult-processing to parallelize inference across multiple cores.</li> <li>Optimized kernels : Kernel optimization refers to the process of improving the performance of codes that form the core computational operations of a software application. There are optimized kernels such as cuDNN and Intel MKL-DNN to perform optimized deep-learning operations. For example, we can achieve faster inference speed by vectorization or hardware-specific assembly-level optimization.</li> <li>Model architecture : We can make a model of compact architecture such as NasNet, MobileNet, and FBNet. The models have more suitable architectures to deploy on mobile devices. Another example is that we can reduce computation by using bottleneck architecture like depth-wise separable convolution. By separating depth-wise convolution(= per channel convolution) and point-wise convolution(= 1x1 convolution), we extract many feature maps(as much as output channels) only at point-wise convolution with reduced computation. Eventually, we can reduce the number of parameters by about kernel_size x kernel_size times and we can achieve faster inference speed.</li> <li>Model network optimization : We can optimize the existing model architecture via some optimization techniques. For example, we can reduce computation by pruning unnecessary layers, channels, or filters, which results in faster inference speed.</li> <li>Quantization : We can optimize the existing model via reducing the precision of weights or activations. For example, we can reduce inference time and model size by quantizing weights or activations into lower bit-width.</li> </ul> <h3 id="the-reason-why-quantization-can-be-a-powerful-technique-among-them">The Reason why Quantization can be a Powerful Technique among them</h3> <p>It should be considered that the real world’s environment, on which our model is deployed, has limited resources. In real world, we cannot produce model with expensive hardware in large quantities, so we have to fix hardware at a proper price. Then our goal is to fit our model to the specific target hardware, usually for edge devices. In the case of kernel optimization, it has diminishing returns : Improving inference speed by optimizing computational operations in kernel can become increasingly difficult and may require more effort for relatively minor gains in performance. It’s because bottlenecks would exist at higher level such as I/O operations. Also, let us assume that we already designed the architecture of efficient small model as a backbone, but we need more improvement in inference time and model size.</p> <p>How can we improve a existing model with fixed hardware, kernel, and model architecture? In that case, eventually quantization can be a powerful technique for inference speedup. Since quantization is lossy compression, it is important to achieve inference speedup with minimal accuracy drop.</p> <h3 id="a-brief-explanation-of-quantization">A Brief Explanation of Quantization</h3> <p>Quantization is the process of reducing the precision of numerical values in neural network model : for example, from FP32 to INT8. By reducing the precision of weights or activations of deep-learning model, we can compress the model’s size and computational cost.</p> <p>We will discuss how quantization works and look through various quantization techniques such as Post-Training-Quantization and Quantization-Aware-Training. In addition, we are also going to discuss how we quantize a model on different frameworks such as Pytorch and ONNX.</p> <p>Nowadays, it is important to consider the balance between model accuracy and computational cost. By understanding the process of quantization, you will have the knowledge to use its potential and may efficiently bridge the gap between powerful AI models and resource-constrained real-world environments.</p> <h1 id="method">Method</h1> <h2 id="quantization">Quantization</h2> <h3 id="overview-of-quantization">Overview of Quantization</h3> <p>In general, we use FP32 (= 32-bit floating-point) representation in deep-learning models because it provides a high level of numerical precision at the backpropagation during the training phase. However, performing operations in high bit-depth can be slow during the inference phase when it is deployed on the small device with resource-constrained hardware.</p> <p>In the real world’s environment with resource-constrained hardware, we need small model size, small RAM bandwidth, and inference speedup with less accuracy drop. To achieve this goal, quantization can be a powerful technique.</p> <p>Quantization is to perform computation and storage at reduced precision using lower bits.</p> <p>We can quantize a model from FP32 to FP16, INT8, or INT4. Here, INT8 (= 8-bit integer) quantization is a common choice due to a balance between accuracy drop and efficiency improvement. By INT8 quantization, we can also utilize the advantages of modern specialized hardware accelerators such as NVIDIA GPU, TPU, and Qualcomm DSP so that they perform efficient INT8 arithmetic operations. If you quantize a model from FP32 to INT8, model size is typically reduced by 4 times and inference speed is improved by 2~4 times and required memory bandwidth is reduced by 2~4 times.</p> <h3 id="principle-of-quantization">Principle of Quantization</h3> <p>Let me explain the main principle of quantization.</p> <p>First, we specify the float range to be quantized and clip values outside the range.</p> <p>Then we take the quantization equation \(x_q=clip(round({x\over s})+z)\) for the Quantization Layer and the dequantization equation \(x=s(x_q-z)\) for the Dequantization Layer.</p> <p>Here, <strong>s</strong> is a scale factor which determines the range mapping and <strong>z</strong> is a zero-point integer such that \(x=0\) in FP32 corresponds to \(x_q=z\) in INT8.</p> <p>When we quantize weights or activations of a model by the above equation in the case of INT8 quantization, we have to map the range of FP32 precision into the range of INT8 precision as shown in the picture below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled1-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled1-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-03-02-Quantization/Untitled1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-03-02-Quantization/Untitled.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Scale Quantization (Symmetric) from FP32 to IN8 vs Affine Quantization (Asymmetric) from FP32 to INT8 </div> <p>There are two types of range-mapping techniques in quantization according to the way of choosing scale factor <strong>s</strong> and zero-point integer <strong>z.</strong></p> <ul> <li> <p>Affine Quantization Mapping : INT8 range is from -128 to 127, which is asymmetric.</p> \[s = \frac{\left| \beta\ - \alpha \right|}{2^{bit}-1}, z = - round(\frac{\alpha}{s})-2^{bit-1}\] <p>\(\alpha, \beta\) = min, max of original weight/activation values</p> <p>\(\alpha_q, \beta_q\) = min, max of quantized weight/activation values ( \(\beta_q-\alpha_q=2^{bit}-1\) )</p> <ul> <li>advantage : Affine quantization generally offers tighter clipping range since \(\alpha,\ \beta\) are assigned to min, max of observed values itself. This can result in good quantization resolution. Also, it is particularly useful for quantizing non-negative activations of which distribution is not symmetric around zero. You can calibrate the zero-point integer to match the data distribution in order to reduce quantization errors.</li> <li>disadvantage : Affine quantization needs extra computations with calibration of zero-point integer and requires hardware-specific tweaks.</li> </ul> </li> <li> <p>Scale Quantization Mapping : INT8 range is from -127 to 127, which is symmetric.</p> \[s = {\left| \beta \right| \over 2^{bit-1}-1}, z=0\] <p>\(\beta, \beta\) = min, max of original weight/activation values where \(\vert \alpha \vert \leq \vert \beta \vert\)</p> <p>\(\beta_q, \beta_q\) = min, max of quantized weight/activation values (\(\beta_q = 2^{bit-1}-1\))</p> <ul> <li>advantage : Symmetric quantization eliminates the need to calculate zero-point integer and it is simpler than asymmetric quantization. Thus, symmetric quantization is more hardware-friendly and produces higher speedup.</li> <li>disadvantage : For skewed signals like non-negative activations, this can result in bad quantization resolution because the clipping range includes negative values that never show up in the input.</li> </ul> </li> </ul> <p>To quantize each layer by MinMax, we need to know the value of \(\alpha,\ \beta\) to determine scale factor <strong>s</strong> and zero-point integer <strong>z.</strong> Thus, we insert observer into each layer of a model and the observers gather statistics from the activations and weights of a neural network during the forward pass of calibration process. These statistics are then used to determine scale-factor and zero-point integer.</p> <p>The above equations are based on MinMax observer, but there are also many other observers in Pytorch framework as shown in the picture below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled2-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled2-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-03-02-Quantization/Untitled2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled3-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled3-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-03-02-Quantization/Untitled3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Different observers to determine scale factor and zero-point integer </div> <h3 id="types-of-quantization">Types of Quantization</h3> <p>Quantization techniques can be grouped into two classes depending on to which stages of neural network model’s development pipeline they are applicable. One is PTQ (Post-Training Quantization) which is applicable after training is finished. The other is QAT (Quantization-Aware Training) which is applicable during training.</p> <ul> <li> <p>PTQ (Post-Training Quantization) : It is to quantize a model which was already trained in high precision. The quantization has nothing to do with training.</p> <p>If the clipping range of activation is determined during inference, it is called Post-Training Dynamic Quantization.</p> <p>If the clipping range of activation is determined before inference, it is called Post-Training Static Quantization.</p> </li> <li> <p>QAT (Quantization-Aware Training) : It is to fine-tune a model with integrating the quantization effects. The model is exposed to quantization during training by inserting observers and fake-quantization modules(e.g. QuantStub and DeQuantStub) in the forward/backward-pass. Here, fake-quantization modules mimic the behavior of quantized operations while working with full precision representations, allowing developers to simulate the effects of quantization during training and evaluation.</p> <p>QAT is more complicated than PTQ since it needs training process. However, QAT outperforms PTQ since the weights of the model is fine-tuned to the quantization task. Therefore, QAT may be an appropriate method for small models such as MobileNet since small models on edge device are more sensitive to quantization error.</p> </li> </ul> <h3 id="model-fusion">Model Fusion</h3> <p>In addition, we usually fuse modules of a model before quantization since it would have less accuracy drop : typically Conv2d-BatchNorm or Conv2d-ReLU or Conv2d-BatchNorm-ReLU or Linear-ReLU. It’s because the overall number of layers to be quantized and the number of operations are reduced if you fuse modules. This reduction of quantization overhead can mitigate the cumulative quantization error, resulting in a less accuracy drop.</p> <h3 id="code-implementation-with-torchaoquantization-library">code implementation with torch.ao.quantization library</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># One Example of Model Fusion : timm resnet18
# torch.ao.quantization.fuse_modules() is used     for PTQ
# torch.ao.quantization.fuse_modules_qat() is used for QAT
</span> 
<span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">fuse_modules_qat</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[[</span><span class="sh">"</span><span class="s">conv1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">bn1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">act1</span><span class="sh">"</span><span class="p">]],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">name1</span><span class="p">,</span> <span class="n">module1</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_children</span><span class="p">():</span>
    <span class="k">if</span> <span class="sh">"</span><span class="s">layer</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">name1</span> <span class="ow">and</span> <span class="n">module1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">name2</span><span class="p">,</span> <span class="n">module2</span> <span class="ow">in</span> <span class="n">module1</span><span class="p">.</span><span class="nf">named_children</span><span class="p">():</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">fuse_modules_qat</span><span class="p">(</span><span class="n">module2</span><span class="p">,</span> <span class="p">[[</span><span class="sh">"</span><span class="s">conv1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">bn1</span><span class="sh">"</span><span class="p">],</span> <span class="p">[</span><span class="sh">"</span><span class="s">conv2</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">bn2</span><span class="sh">"</span><span class="p">]],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">name3</span><span class="p">,</span> <span class="n">module3</span> <span class="ow">in</span> <span class="n">module2</span><span class="p">.</span><span class="nf">named_children</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">name3</span> <span class="o">==</span> <span class="sh">"</span><span class="s">downsample</span><span class="sh">"</span> <span class="ow">and</span> <span class="n">module3</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                    <span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">fuse_modules_qat</span><span class="p">(</span><span class="n">module3</span><span class="p">,</span> <span class="p">[[</span><span class="sh">"</span><span class="s">0</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">1</span><span class="sh">"</span><span class="p">]],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <h2 id="post-training-dynamic-quantization-in-pytorch">Post-Training Dynamic Quantization in Pytorch</h2> <p>If the clipping range of activation is determined during inference, it is called dynamic quantization. Only weights of a trained model are quantized before inference and activations of the model should be quantized dynamically during inference. So, observer which can compute quantization parameters in real-time manner should be used such as MinMax and Percentile.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled4-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled4-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-03-02-Quantization/Untitled4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Post-Training Dynamic Quantization </div> <ul> <li> <p>advantage :</p> <p>Inference accuarcy may be higher than Static Quantization since scale factor and zero-point integer of activations are determined in real-time during inference such that they fit our input data.</p> <p>Post-Training Dynamic Quantization is appropriate to dynamic models like LSTMs or BERT. It’s because the disbribution of activation values of dynamic models can vary significantly depending on the input data.</p> </li> <li> <p>disadvantage :</p> <p>The scale factor and zero-point integer of activations should be computed dynamically at inference runtime. This results in the increase of the cost of inference and has less improvement of inference latency than Static Quantization.</p> </li> </ul> <p>Note that inference of a quantized model is still executed on CPU for Pytorch framework. (For other frameworks, GPU may work.)</p> <h3 id="code-implementation-with-torchaoquantization-library-1">code implementation with torch.ao.quantization library</h3> <p>It is very simple to implement Post-Training Dynamic Quantization as shown below.</p> <p>You can specify submodules which will be quantized using “qconfig_spec” argument.</p> <p>That’s all!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">quantized_model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">quantize_dynamic</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">qconfig_spec</span><span class="o">=</span><span class="p">{</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">},</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">quint8</span><span class="p">)</span>
</code></pre></div></div> <h2 id="post-training-static-quantization-in-pytorch">Post-Training Static Quantization in Pytorch</h2> <p>If the clipping range of activation is determined before inference, it is called static quantization. Both weights and activations of a trained model are quantized before inference. Here, by calibration, observers observe the range of stored values to determine quantization parameters.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled5-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled5-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-03-02-Quantization/Untitled5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled6-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled6-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-03-02-Quantization/Untitled6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Post-Training Static Quantization - calibrate &amp; quantize </div> <ul> <li> <p>advantage :</p> <p>It is relatively easy to find scale factor and zero-point integer in advance before inference.</p> <p>Post-Training Static Quantization is appropriate to CNN models since their throughput is limited by memory bandwidth for activations and we can figure out the disbribution of activation values of CNN models during calibration.</p> </li> <li> <p>disadvantage :</p> <p>Smaller model like Mobile CNN is more sensitive to quantization errors, so Post-Training Quantization may have significant accuracy drop. It’s the moment when we need Quantization-Aware Training.</p> </li> </ul> <p>Note that inference of a quantized model is still executed on CPU for Pytorch framework. (For other frameworks like tflite, both CPU and GPU may work.)</p> <h3 id="code-implementation-with-torchaoquantization-library-2">code implementation with torch.ao.quantization library</h3> <ul> <li>Step 1. Module Fusion :</li> </ul> <p>Fuse modules for less accuracy drop</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">fuse_modules</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[[</span><span class="sh">"</span><span class="s">conv1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">bn1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">act1</span><span class="sh">"</span><span class="p">]],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">name1</span><span class="p">,</span> <span class="n">module1</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_children</span><span class="p">():</span>
    <span class="k">if</span> <span class="sh">"</span><span class="s">layer</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">name1</span> <span class="ow">and</span> <span class="n">module1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">name2</span><span class="p">,</span> <span class="n">module2</span> <span class="ow">in</span> <span class="n">module1</span><span class="p">.</span><span class="nf">named_children</span><span class="p">():</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">fuse_modules</span><span class="p">(</span><span class="n">module2</span><span class="p">,</span> <span class="p">[[</span><span class="sh">"</span><span class="s">conv1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">bn1</span><span class="sh">"</span><span class="p">],</span> <span class="p">[</span><span class="sh">"</span><span class="s">conv2</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">bn2</span><span class="sh">"</span><span class="p">]],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">name3</span><span class="p">,</span> <span class="n">module3</span> <span class="ow">in</span> <span class="n">module2</span><span class="p">.</span><span class="nf">named_children</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">name3</span> <span class="o">==</span> <span class="sh">"</span><span class="s">downsample</span><span class="sh">"</span> <span class="ow">and</span> <span class="n">module3</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                    <span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">fuse_modules</span><span class="p">(</span><span class="n">module3</span><span class="p">,</span> <span class="p">[[</span><span class="sh">"</span><span class="s">0</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">1</span><span class="sh">"</span><span class="p">]],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># insert torch.ao.quantization.QuantStub() layer 
#	and torch.ao.quantization.DeQuantStub() layer 
# at the beginning and end of forward() respectively.
</span></code></pre></div></div> <ul> <li>Step 2. Prepare :</li> </ul> <p>Insert observer and prepare the quantization process</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 'x86' or 'fbgemm' for server inference
# 'qnnpack' for mobile inference
</span><span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">quantized</span><span class="p">.</span><span class="n">engine</span> <span class="o">=</span> <span class="sh">'</span><span class="s">fbgemm</span><span class="sh">'</span> 
<span class="n">model</span><span class="p">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">get_default_qconfig</span><span class="p">(</span><span class="sh">'</span><span class="s">fbgemm</span><span class="sh">'</span><span class="p">)</span>
<span class="c1"># You can use different observers for quantized_model.qconfig
</span><span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <ul> <li>Step 3. Calibration :</li> </ul> <p>Forward-pass to determine the scale factor and zero-point integer based on the given input calibration dataset.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># forward pass of model in .eval() phase
# observer computes scale factor and zero-point integer by calibration
</span><span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cpu:0</span><span class="sh">"</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">gt</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">calibrate_loader</span><span class="p">):</span>
		<span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cpu:0</span><span class="sh">"</span><span class="p">))</span>
		<span class="n">gt</span> <span class="o">=</span> <span class="n">gt</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cpu:0</span><span class="sh">"</span><span class="p">))</span>
	  <span class="nf">model</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
</code></pre></div></div> <ul> <li>Step 4. Convert :</li> </ul> <p>Convert from FP32 to reduced precision based on the calibrated quantization parameters</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">convert</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <h3 id="code-implementation-with-pytorch_quantization-library">code implementation with pytorch_quantization library</h3> <ul> <li>Step 1. Initialize quantizable modules :</li> </ul> <p>pytorch_quantization library supports only the quantization of the layers shown below.</p> <ul> <li>QuantConv1d, QuantConv2d, QuantConv3d, QuantConvTranspose1d, QuantConvTranspose2d, QuantConvTranspose3d</li> <li>QuantLinear</li> <li>QuantAvgPool1d, QuantAvgPool2d, QuantAvgPool3d, QuantMaxPool1d, QuantMaxPool2d, QuantMaxPool3d</li> </ul> <p>If you want to quantize another layer, you should implement the quantized version of custom modules. (In my case, I implemented QuantHardswish and QuantConvReLU2d.)</p> <p>For the model instance that you create after quant_modules.initialize(), it automatically converts the default modules and custom modules into their quantizable version via monkey-patching.</p> <ul> <li>Limitation : pytorch_quantization library “immediately” converts the modules of a model into their quantized version as soon as the model is loaded after quant_modules.initialize(). Therefore the modules to be quantized must be the modules of a model to be loaded.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pytorch_quantization</span> <span class="kn">import</span> <span class="n">quant_modules</span>

<span class="n">custom_quant_modules</span> <span class="o">=</span> <span class="p">[(</span><span class="n">nn</span><span class="p">,</span> <span class="sh">"</span><span class="s">Hardswish</span><span class="sh">"</span><span class="p">,</span> <span class="n">QuantHardswish</span><span class="p">),</span> 
		<span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">intrinsic</span><span class="p">.</span><span class="n">modules</span><span class="p">.</span><span class="n">fused</span><span class="p">,</span> <span class="sh">"</span><span class="s">ConvReLU2d</span><span class="sh">"</span><span class="p">,</span> <span class="n">QuantConvReLU2d</span><span class="p">)]</span>
<span class="n">quant_modules</span><span class="p">.</span><span class="nf">initialize</span><span class="p">(</span><span class="n">custom_quant_modules</span><span class="o">=</span><span class="n">custom_quant_modules</span><span class="p">)</span>

<span class="c1"># create a model instance
# then modules are substituted into quantizable version 
# automatically via monkey-patching
</span></code></pre></div></div> <ul> <li>Step 2. Prepare :</li> </ul> <p>I utilized histogram-based calibration for activations, but you can also try another calibration method.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">quant_desc_input</span> <span class="o">=</span> <span class="nc">QuantDescriptor</span><span class="p">(</span><span class="n">calib_method</span><span class="o">=</span><span class="sh">'</span><span class="s">histogram</span><span class="sh">'</span><span class="p">)</span>
<span class="n">quant_nn</span><span class="p">.</span><span class="n">QuantConv2d</span><span class="p">.</span><span class="nf">set_default_quant_desc_input</span><span class="p">(</span><span class="n">quant_desc_input</span><span class="p">)</span>
<span class="n">quant_nn</span><span class="p">.</span><span class="n">QuantLinear</span><span class="p">.</span><span class="nf">set_default_quant_desc_input</span><span class="p">(</span><span class="n">quant_desc_input</span><span class="p">)</span>
<span class="n">QuantHardswish</span><span class="p">.</span><span class="nf">set_default_quant_desc_input</span><span class="p">(</span><span class="n">quant_desc_input</span><span class="p">)</span>
<span class="n">QuantConvReLU2d</span><span class="p">.</span><span class="nf">set_default_quant_desc_input</span><span class="p">(</span><span class="n">quant_desc_input</span><span class="p">)</span>
</code></pre></div></div> <ul> <li>Step 3. Calibration :</li> </ul> <p>Forward-pass to determine the scale factor and zero-point integer</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># If there is calibrator, disable quantization and enable calibrator
# Otherwise, disable the module itself
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
<span class="c1"># Enable calibrators to collect statistics
</span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_modules</span><span class="p">():</span>
    <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">quant_nn</span><span class="p">.</span><span class="n">TensorQuantizer</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">module</span><span class="p">.</span><span class="n">_calibrator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">module</span><span class="p">.</span><span class="nf">disable_quant</span><span class="p">()</span> <span class="c1"># use full precision data to calibrate
</span>            <span class="n">module</span><span class="p">.</span><span class="nf">enable_calib</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">module</span><span class="p">.</span><span class="nf">disable</span><span class="p">()</span>

<span class="c1"># Calibration  
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">ground_truth</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
    <span class="nf">model</span><span class="p">(</span><span class="n">image</span><span class="p">.</span><span class="nf">cuda</span><span class="p">())</span> <span class="c1"># forward pass of model in .eval() phase
</span>    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">7</span><span class="p">:</span>
        <span class="k">break</span>

<span class="c1"># If there is calibrator, enable quantization and disable calibrator
# Otherwise, enable the module itself
</span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_modules</span><span class="p">():</span>
    <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">quant_nn</span><span class="p">.</span><span class="n">TensorQuantizer</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">module</span><span class="p">.</span><span class="n">_calibrator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">module</span><span class="p">.</span><span class="nf">enable_quant</span><span class="p">()</span>
            <span class="n">module</span><span class="p">.</span><span class="nf">disable_calib</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">module</span><span class="p">.</span><span class="nf">enable</span><span class="p">()</span>

<span class="c1"># After calibration, quantizers obtain amax set, which is
# absolute maximum input value representable in the quantized space
# In default, amax for weight is per channel 
#         and amax for activation is per tensor.
</span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_modules</span><span class="p">():</span>
    <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">quant_nn</span><span class="p">.</span><span class="n">TensorQuantizer</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">module</span><span class="p">.</span><span class="n">_calibrator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">.</span><span class="n">_calibrator</span><span class="p">,</span> <span class="n">calib</span><span class="p">.</span><span class="n">MaxCalibrator</span><span class="p">):</span>
                <span class="n">module</span><span class="p">.</span><span class="nf">load_calib_amax</span><span class="p">(</span><span class="n">strict</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># method = "percentile" or "mse" or "entropy"
</span>								<span class="n">module</span><span class="p">.</span><span class="nf">load_calib_amax</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="sh">"</span><span class="s">percentile</span><span class="sh">"</span><span class="p">,</span> 
									<span class="n">percentile</span><span class="o">=</span><span class="mf">99.99</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> 
				<span class="c1"># You can check 
</span>				<span class="nf">print</span><span class="p">(</span><span class="sa">F</span><span class="sh">"</span><span class="si">{</span><span class="n">name</span><span class="si">:</span><span class="mi">40</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">module</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>		
</code></pre></div></div> <h2 id="qatquantization-aware-training-in-pytorch">QAT(Quantization-Aware Training) in Pytorch</h2> <p>It has similar steps with Post-Training Static Quantization, but the difference is that the weights of a model are updated via fine-tuning to quantization task. To emulate the quantization, we insert fake-quantization modules at the beginning and end of forward() function.</p> <ul> <li> <p>advantage :</p> <p>The weights of model is updated to fit the quantization task (a.k.a fine-tuning), so it has usually higher accuracy than Post-Training Quantization.</p> </li> <li> <p>disadvantage :</p> <p>It needs additional resources due to training process, so it is more complicated.</p> </li> </ul> <p>Note that training of a model can be executed on both CPU and GPU, but inference of a quantized model is still executed on CPU for Pytorch framework. To compare the inference time(latency) between original model and quantized model, I did inference on CPU for both models.</p> <h3 id="code-implementation-using-torchaoquantization-library">code implementation using torch.ao.quantization library</h3> <ul> <li>Step 1. Module Fusion :</li> </ul> <p>Fuse modules for less accuracy drop</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">fuse_modules_qat</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[[</span><span class="sh">"</span><span class="s">conv1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">bn1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">act1</span><span class="sh">"</span><span class="p">]],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">for</span> <span class="n">name1</span><span class="p">,</span> <span class="n">module1</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">named_children</span><span class="p">():</span>
    <span class="k">if</span> <span class="sh">"</span><span class="s">layer</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">name1</span> <span class="ow">and</span> <span class="n">module1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">name2</span><span class="p">,</span> <span class="n">module2</span> <span class="ow">in</span> <span class="n">module1</span><span class="p">.</span><span class="nf">named_children</span><span class="p">():</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">fuse_modules_qat</span><span class="p">(</span><span class="n">module2</span><span class="p">,</span> <span class="p">[[</span><span class="sh">"</span><span class="s">conv1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">bn1</span><span class="sh">"</span><span class="p">],</span> <span class="p">[</span><span class="sh">"</span><span class="s">conv2</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">bn2</span><span class="sh">"</span><span class="p">]],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">name3</span><span class="p">,</span> <span class="n">module3</span> <span class="ow">in</span> <span class="n">module2</span><span class="p">.</span><span class="nf">named_children</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">name3</span> <span class="o">==</span> <span class="sh">"</span><span class="s">downsample</span><span class="sh">"</span> <span class="ow">and</span> <span class="n">module3</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                    <span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">fuse_modules_qat</span><span class="p">(</span><span class="n">module3</span><span class="p">,</span> <span class="p">[[</span><span class="sh">"</span><span class="s">0</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">1</span><span class="sh">"</span><span class="p">]],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># To emulate the quantization process,
# insert torch.ao.quantization.QuantStub() layer 
#	and torch.ao.quantization.DeQuantStub() layer 
# at the beginning and end of forward() respectively.
</span></code></pre></div></div> <ul> <li>Step 2. Prepare :</li> </ul> <p>Insert observer and prepare the quantization process</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 'x86' or 'fbgemm' for server inference
# 'qnnpack' for mobile inference
</span><span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">quantized</span><span class="p">.</span><span class="n">engine</span> <span class="o">=</span> <span class="sh">'</span><span class="s">fbgemm</span><span class="sh">'</span>
<span class="n">model</span><span class="p">.</span><span class="n">qconfig</span> <span class="o">=</span> 
	<span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">get_default_qat_qconfig</span><span class="p">(</span><span class="sh">'</span><span class="s">fbgemm</span><span class="sh">'</span><span class="p">)</span>
<span class="c1"># You can use different observers for quantized_model.qconfig
</span><span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">prepare_qat</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">(),</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <ul> <li>Step 3. Calibration + Fine-Tuning (Training) :</li> </ul> <p>First, enable the observers and fake-quantization modules.</p> <p>The observers and fake-quantization modules will compute the scale factor and zero-point integer during calibration.</p> <p>Second, fine-tune the model until loss converges (training).</p> <p>Here, note that you should finish the calibration around the beginning of epochs. So, disable the observers and freeze the BatchNorm stats around the beginning of epochs (epoch 4, 3 in my case) so that we can focus on updating the weights of the model with the observer’s fixed quantization parameters.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">quantized_model</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">enable_observer</span><span class="p">)</span>
<span class="n">quantized_model</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">enable_fake_quant</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
	<span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="nf">train_one_epoch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
	<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">train loss : {.8f} acc : {:.5f}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">acc</span><span class="p">))</span>
	
	<span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="nf">test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
	<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">test loss : {.8f} acc : {:.5f}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">acc</span><span class="p">))</span>
	
	<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">inference_mode</span><span class="p">():</span>
		<span class="k">if</span> <span class="n">epoch</span> <span class="o">&gt;=</span> <span class="mi">4</span><span class="p">:</span>
			<span class="n">quantized_model</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">disable_observer</span><span class="p">)</span>
		<span class="k">if</span> <span class="n">epoch</span> <span class="o">&gt;=</span> <span class="mi">3</span><span class="p">:</span>
			<span class="n">quantized_model</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">intrinsic</span><span class="p">.</span><span class="n">qat</span><span class="p">.</span><span class="n">freeze_bn_stats</span><span class="p">)</span>
</code></pre></div></div> <ul> <li>Step 4. Convert :</li> </ul> <p>Convert from FP32 to reduced precision based on the calibrated quantization parameters</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ao</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="nf">convert</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div> <h2 id="quantization-in-onnx">Quantization in ONNX</h2> <h3 id="definition-of-onnx">Definition of ONNX</h3> <p>ONNX(Open Neural Network Exchange) is an open standard to facilitate interoperability between different DNN frameworks.</p> <h3 id="necessity-of-onnx">Necessity of ONNX</h3> <p>Deploying models on specific hardware can be challenging due to the difference in hardware’s architecture and runtime environment. ONNX deals with this challenge by providing a standardized way to represent deep-learning models so that they can be easily transferred across various frameworks and easily deployed on specific hardware device. It bridges the gap between model development on framework and model deployment on hardware.</p> <ul> <li>Framework-Agnostic : ONNX allows you to train deep-learning models in one framework such as PyTorch and TensorFlow, and then export them to the ONNX format. This enables you to choose the best framework which is familiar with you and suitable for the model development. And then you can deploy the model on different target hardware without extensive modifications.</li> <li>Hardware Optimization : Different hardware have varying architectures and optimizations. Here, hardware-specific optimizations are incorporated into onnxruntime, so onnxruntime allows the model to take full advantage of the underlying hardware-specific capabilities and perform inference efficiently on the target device. In detail, ONNX Runtime works with different hardware acceleration libraries through its extensible hardware-specific Execution Providers listed below.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled7-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled7-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled7-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-03-02-Quantization/Untitled7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="inference-in-onnx">Inference in ONNX</h3> <p>You can perform forward-pass of an ONNX model following the code below.</p> <h3 id="code-implementation-using-onnxruntime-library">code implementation using onnxruntime library</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">onnxruntime</span>

<span class="k">def</span> <span class="nf">benchmark</span><span class="p">(</span><span class="n">model_path</span><span class="p">):</span>
    <span class="n">session</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="p">.</span><span class="nc">InferenceSession</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
    <span class="n">input_name</span> <span class="o">=</span> <span class="n">session</span><span class="p">.</span><span class="nf">get_inputs</span><span class="p">()[</span><span class="mi">0</span><span class="p">].</span><span class="n">name</span>

    <span class="n">total</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">runs</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span> <span class="c1"># B, C, H, W
</span>    <span class="c1"># Warming up
</span>    <span class="n">_</span> <span class="o">=</span> <span class="n">session</span><span class="p">.</span><span class="nf">run</span><span class="p">([],</span> <span class="p">{</span><span class="n">input_name</span><span class="p">:</span> <span class="n">input_data</span><span class="p">})</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">runs</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">session</span><span class="p">.</span><span class="nf">run</span><span class="p">([],</span> <span class="p">{</span><span class="n">input_name</span><span class="p">:</span> <span class="n">input_data</span><span class="p">})</span>
        <span class="n">end</span> <span class="o">=</span> <span class="p">(</span><span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1000</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">end</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">end</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">ms</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">total</span> <span class="o">/=</span> <span class="n">runs</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Avg: </span><span class="si">{</span><span class="n">total</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">ms</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="quantization-in-onnx-1">Quantization in ONNX</h3> <p>Quantization in ONNX Runtime refers to INT8 or UINT8 linear quantization of an ONNX model. There are two ways to represent quantized ONNX models.</p> <ul> <li> <p>Operator-oriented (QOperator) :</p> <p>All the quantized operators have their own ONNX definitions like QLinearConv and MatMulInteger.</p> </li> <li> <p>Tensor-oriented (QDQ) :</p> <p>This format inserts &lt;tensor - QuantizeLinear - DequantizeLinear&gt; between the original operators to simulate the quantization and dequantization process. In the case of activations, QuantizeLinear layer is used for quantizing and DequantizeLinear layer is used for dequantizing respectively. In the case of weight, only DequantizeLinear layer is inserted.</p> <p>In Dynamic Quantization, a ComputeQuantizationParameters functions proto is inserted to calculate quantization parameters on the fly. In Static Quantization, QuantizeLinear and DeQuantizeLinear operators carry the quantization parameters (scale factor and zero-point integer) of activations or weights.</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled8-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled8-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled8-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-03-02-Quantization/Untitled8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Visualization of ONNX model via netron.app Left : QOperator format of a quantized ONNX model Right : QDQ format of a quantized ONNX model </div> <ul> <li>For more details, refer to the link below.</li> </ul> <p><a href="https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html" rel="external nofollow noopener" target="_blank">https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html</a></p> <h3 id="code-implementation-using-onnxruntime-library-cpu-qdq-format">code implementation using onnxruntime library (CPU, QDQ format)</h3> <p>I will introduce how to quantize an ONNX model using CPU hardware and QDQ format.</p> <p>It follows the link below.</p> <p><a href="https://github.com/microsoft/onnxruntime-inference-examples/tree/main/quantization/image_classification/cpu" rel="external nofollow noopener" target="_blank">https://github.com/microsoft/onnxruntime-inference-examples/tree/main/quantization/image_classification/cpu</a></p> <ul> <li>Step 1. Pre-process</li> </ul> <p>Pre-processing is to prepare ONNX model for better quantization. It consists of three optional steps : Symbolic shape inference, Model optimization, and ONNX shape inference.</p> <p>Both Symbolic shape inference and ONNX shape inference figure out the tensor shapes. Model optimization performs module fusion.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">onnxruntime</span>
<span class="kn">from</span> <span class="n">onnxruntime.quantization.shape_inference</span> <span class="kn">import</span> <span class="n">quant_pre_process</span>

<span class="c1"># whether you skip Model optimization since model size is greater than 2GB
</span><span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--skip_optimization</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># whether you skip ONNX shape inference
</span><span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--skip_onnx_shape</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># whether you skip Symbolic shape inference
</span><span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--skip_symbolic_shape</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--auto_merge</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--int_max</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">31</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--guess_output_rank</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--verbose</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--save_as_external_data</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--all_tensors_to_one_file</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--external_data_location</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--external_data_size_threshold</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>

<span class="nf">quant_pre_process</span><span class="p">(</span>
        <span class="n">input_onnxmodel_path</span><span class="p">,</span>
        <span class="n">output_onnxmodel_path</span><span class="p">,</span>
        <span class="n">args</span><span class="p">.</span><span class="n">skip_optimization</span><span class="p">,</span>
        <span class="n">args</span><span class="p">.</span><span class="n">skip_onnx_shape</span><span class="p">,</span>
        <span class="n">args</span><span class="p">.</span><span class="n">skip_symbolic_shape</span><span class="p">,</span>
        <span class="n">args</span><span class="p">.</span><span class="n">auto_merge</span><span class="p">,</span>
        <span class="n">args</span><span class="p">.</span><span class="n">int_max</span><span class="p">,</span>
        <span class="n">args</span><span class="p">.</span><span class="n">guess_output_rank</span><span class="p">,</span>
        <span class="n">args</span><span class="p">.</span><span class="n">verbose</span><span class="p">,</span>
        <span class="n">args</span><span class="p">.</span><span class="n">save_as_external_data</span><span class="p">,</span>
        <span class="n">args</span><span class="p">.</span><span class="n">all_tensors_to_one_file</span><span class="p">,</span>
        <span class="n">args</span><span class="p">.</span><span class="n">external_data_location</span><span class="p">,</span>
        <span class="n">args</span><span class="p">.</span><span class="n">external_data_size_threshold</span>
<span class="p">)</span>
</code></pre></div></div> <ul> <li>Step 2. Quantize</li> </ul> <p>Model optimization may also be performed during quantization by default for historical reasons. However, it’s highly recommended to perform model optimization during pre-process(Step 1) and turn off model optimization during quantization(Step 2) for the ease of debugging.</p> <p>(i) Dynamic Quantization :</p> <p>It calculates quantization parameters for activations dynamically</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">onnxruntime</span>
<span class="kn">from</span> <span class="n">onnxruntime.quantization</span> <span class="kn">import</span> <span class="n">QuantFormat</span><span class="p">,</span> <span class="n">QuantType</span><span class="p">,</span> <span class="n">quantize_dynamic</span>

<span class="c1"># Tensor-oriented QDQ format of quantized ONNX model
</span><span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--quant_format</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">QuantFormat</span><span class="p">.</span><span class="n">QDQ</span><span class="p">)</span>

<span class="c1"># You can use per-channel quantization if accuracy drop is significant
</span><span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--per_channel</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># If accuracy drop is significant, it may be caused by saturation (clamped)
# Then you can try reduce_range
# reduce_range == True : quantize weights with 7-bits. 
#                        It may improve accuracy for non-VNNI machine
</span><span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--reduce_range</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># nodes_to_exclude : specify nodes which you will freeze and will not quantize
</span>
<span class="nf">quantize_dynamic</span><span class="p">(</span>
        <span class="n">input_onnxmodel_path</span><span class="p">,</span>
        <span class="n">output_onnxmodel_path</span><span class="p">,</span>
        <span class="n">per_channel</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">per_channel</span><span class="p">,</span>
        <span class="n">reduce_range</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">reduce_range</span><span class="p">,</span>
        <span class="n">weight_type</span><span class="o">=</span><span class="n">QuantType</span><span class="p">.</span><span class="n">QInt8</span><span class="p">,</span> 
        <span class="n">nodes_to_exclude</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">/layer4/layer4.0/conv1/Conv</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/layer4/layer4.0/conv2/Conv</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/layer4/layer4.0/downsample/downsample.0/Conv</span><span class="sh">'</span><span class="p">,</span>  
                          <span class="sh">'</span><span class="s">/layer4/layer4.1/conv1/Conv</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/layer4/layer4.1/conv2/Conv</span><span class="sh">'</span><span class="p">,</span>
                          <span class="sh">'</span><span class="s">/fc/Gemm</span><span class="sh">'</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div></div> <p>(ii) Static Quantization :</p> <p>It calculates quantization parameters using calibration input data before inference.</p> <p>ONNX Runtime quantization tool supports three calibration methods: MinMax, Entropy and Percentile.</p> <p>ONNX Runtime quantization on CPU can run U8U8, U8S8, and S8S8. Here, U8S8 means that activation and weight are quantized to UINT8(unsigned) and INT8(signed) respectively. And S8S8 with QDQ is the default setting since it may have balance between performance and accuracy.</p> <p>Note that S8S8 with QOperator will be slow on x86-64 CPUs and should be avoided in general. Also, note that ONNX Runtime quantization on GPU only supports S8S8.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">onnxruntime</span>
<span class="kn">from</span> <span class="n">onnxruntime.quantization</span> <span class="kn">import</span> <span class="n">QuantFormat</span><span class="p">,</span> <span class="n">QuantType</span><span class="p">,</span> <span class="n">quantize_static</span>

<span class="c1"># Tensor-oriented QDQ format of quantized ONNX model
</span><span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--quant_format</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">QuantFormat</span><span class="p">.</span><span class="n">QDQ</span><span class="p">)</span>

<span class="c1"># You can use per-channel quantization if accuracy drop is significant
</span><span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--per_channel</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">True</span>

<span class="c1"># If accuracy drop is significant, it may be caused by saturation (clamped)
# Then you can try reduce_range or U8U8
# reduce_range == True : quantize weights with 7-bits. 
#                        It may improve accuracy for non-VNNI machine
</span><span class="n">parser</span><span class="p">.</span><span class="nf">add_argument</span><span class="p">(</span><span class="sh">"</span><span class="s">--reduce_range</span><span class="sh">"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># nodes_to_exclude : specify nodes which you will freeze and will not quantize
</span>
<span class="c1"># Create a set of inputs called calibration data
</span><span class="n">dr</span> <span class="o">=</span> <span class="n">resnet18_data_reader</span><span class="p">.</span><span class="nc">ResNet18DataReader</span><span class="p">(</span>
        <span class="n">calibration_dir_path</span><span class="p">,</span> <span class="n">input_onnxmodel_path</span>
    <span class="p">)</span>

<span class="nf">quantize_static</span><span class="p">(</span>
        <span class="n">input_onnxmodel_path</span><span class="p">,</span>
        <span class="n">output_onnxmodel_path</span><span class="p">,</span>
        <span class="n">dr</span><span class="p">,</span>
        <span class="n">quant_format</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">quant_format</span><span class="p">,</span>
        <span class="n">per_channel</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">per_channel</span><span class="p">,</span>
        <span class="n">reduce_range</span><span class="o">=</span><span class="n">args</span><span class="p">.</span><span class="n">reduce_range</span><span class="p">,</span>
        <span class="n">activation_type</span><span class="o">=</span><span class="n">QuantType</span><span class="p">.</span><span class="n">QUInt8</span><span class="p">,</span>
        <span class="n">weight_type</span><span class="o">=</span><span class="n">QuantType</span><span class="p">.</span><span class="n">Int8</span><span class="p">,</span> 
        <span class="n">nodes_to_exclude</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">/layer4/layer4.0/conv1/Conv</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/layer4/layer4.0/act1/Relu</span><span class="sh">'</span><span class="p">,</span>
                          <span class="sh">'</span><span class="s">/layer4/layer4.0/conv2/Conv</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/layer4/layer4.0/downsample/downsample.0/Conv</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/layer4/layer4.0/Add</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/layer4/layer4.0/act2/Relu</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/layer4/layer4.1/conv1/Conv</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/layer4/layer4.1/act1/Relu</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/layer4/layer4.1/conv2/Conv</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/layer4/layer4.1/Add</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/layer4/layer4.1/act2/Relu</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/global_pool/pool/GlobalAveragePool</span><span class="sh">'</span><span class="p">,</span> 
                          <span class="sh">'</span><span class="s">/global_pool/flatten/Flatten</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">/fc/Gemm</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">extra_options</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">CalibMovingAverage</span><span class="sh">'</span><span class="p">:</span><span class="bp">True</span><span class="p">,</span> <span class="sh">'</span><span class="s">SmoothQuant</span><span class="sh">'</span><span class="p">:</span><span class="bp">True</span><span class="p">}</span>
<span class="p">)</span>
</code></pre></div></div> <ul> <li>Step 3. Debugging</li> </ul> <p>Quantization is a lossy compression, so it may drop a model’s accuracy. To improve the problematic parts, you can compare the weights and activations tensors between the original computation graph and the quantized comptuation graph. By debugging, you can identify where they differ most and avoid quantizing these nodes using “nodes_to_exclude” argument in Step 2. Quantize.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">onnxruntime.quantization.qdq_loss_debug</span> <span class="kn">import</span> <span class="p">(</span>
	<span class="n">collect_activations</span><span class="p">,</span> <span class="n">compute_activation_error</span><span class="p">,</span> <span class="n">compute_weight_error</span><span class="p">,</span>
	<span class="n">create_activation_matching</span><span class="p">,</span> <span class="n">create_weight_matching</span><span class="p">,</span>
	<span class="n">modify_model_output_intermediate_tensors</span><span class="p">)</span>

<span class="c1"># Comparing weights of float model vs qdq model
</span><span class="n">matched_weights</span> <span class="o">=</span> <span class="nf">create_weight_matching</span><span class="p">(</span><span class="n">float_model_path</span><span class="p">,</span> <span class="n">qdq_model_path</span><span class="p">)</span>
<span class="n">weights_error</span> <span class="o">=</span> <span class="nf">compute_weight_error</span><span class="p">(</span><span class="n">matched_weights</span><span class="p">)</span>
<span class="k">for</span> <span class="n">weight_name</span><span class="p">,</span> <span class="n">err</span> <span class="ow">in</span> <span class="n">weights_error</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Cross model error of </span><span class="sh">'</span><span class="si">{</span><span class="n">weight_name</span><span class="si">}</span><span class="sh">'</span><span class="s">: </span><span class="si">{</span><span class="n">err</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Augmenting models to save intermediate activations
</span><span class="nf">modify_model_output_intermediate_tensors</span><span class="p">(</span><span class="n">float_model_path</span><span class="p">,</span> <span class="n">aug_float_model_path</span><span class="p">)</span>
<span class="nf">modify_model_output_intermediate_tensors</span><span class="p">(</span><span class="n">qdq_model_path</span><span class="p">,</span> <span class="n">aug_qdq_model_path</span><span class="p">)</span>

<span class="c1"># Running the augmented floating point model to collect activations
</span><span class="n">dr</span> <span class="o">=</span> <span class="n">resnet18_data_reader</span><span class="p">.</span><span class="nc">ResNet18DataReader</span><span class="p">(</span>
        <span class="n">calibration_dir_path</span><span class="p">,</span> <span class="n">float_model_path</span>
<span class="p">)</span>
<span class="n">float_activations</span> <span class="o">=</span> <span class="nf">collect_activations</span><span class="p">(</span><span class="n">aug_float_model_path</span><span class="p">,</span> <span class="n">dr</span><span class="p">)</span>

<span class="c1"># Running the augmented qdq model to collect activations
</span><span class="n">dr</span><span class="p">.</span><span class="nf">rewind</span><span class="p">()</span>
<span class="n">qdq_activations</span> <span class="o">=</span> <span class="nf">collect_activations</span><span class="p">(</span><span class="n">aug_qdq_model_path</span><span class="p">,</span> <span class="n">dr</span><span class="p">)</span>

<span class="c1"># Comparing activations of float model vs qdq model
</span><span class="n">act_matching</span> <span class="o">=</span> <span class="nf">create_activation_matching</span><span class="p">(</span><span class="n">qdq_activations</span><span class="p">,</span> <span class="n">float_activations</span><span class="p">)</span>
<span class="n">act_error</span> <span class="o">=</span> <span class="nf">compute_activation_error</span><span class="p">(</span><span class="n">act_matching</span><span class="p">)</span>
<span class="k">for</span> <span class="n">act_name</span><span class="p">,</span> <span class="n">err</span> <span class="ow">in</span> <span class="n">act_error</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Cross model error of </span><span class="sh">'</span><span class="si">{</span><span class="n">act_name</span><span class="si">}</span><span class="sh">'</span><span class="s">: </span><span class="si">{</span><span class="n">err</span><span class="p">[</span><span class="sh">'</span><span class="s">xmodel_err</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
  <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">QDQ error of </span><span class="sh">'</span><span class="si">{</span><span class="n">act_name</span><span class="si">}</span><span class="sh">'</span><span class="s">: </span><span class="si">{</span><span class="n">err</span><span class="p">[</span><span class="sh">'</span><span class="s">qdq_err</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <ul> <li>Summary :</li> </ul> <p>Assume that you implemented Step 1. into preprocess.py and Step 2. into quantize.py and Step 3. into debug.py with proper I/O. Then you can run them in terminal as shown below.</p> <p>(i) If you do not optimize ONNX model during quantization (recommended)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 1. Pre-process
</span><span class="n">python</span> <span class="n">preprocess</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="nb">input</span> <span class="n">original</span><span class="p">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">output</span> <span class="n">preprocess</span><span class="p">.</span><span class="n">onnx</span>
<span class="c1"># Step 2. Quantize without optimization
</span><span class="n">python</span> <span class="n">quantize</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="nb">input</span> <span class="n">preprocess</span><span class="p">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">output</span> <span class="n">quantized</span><span class="p">.</span><span class="n">onnx</span>
<span class="c1"># Step 3. Debug
</span><span class="n">python</span> <span class="n">debug</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">float_model</span> <span class="n">preprocess</span><span class="p">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">qdq_model</span> <span class="n">quantized</span><span class="p">.</span><span class="n">onnx</span>
</code></pre></div></div> <p>(ii) If you optimize ONNX model during quantization (default)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Step 1. Pre-process
</span><span class="n">python</span> <span class="n">preprocess</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="nb">input</span> <span class="n">original</span><span class="p">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">output</span> <span class="n">preprocess</span><span class="p">.</span><span class="n">onnx</span>
<span class="c1"># Step 2. Quantize with optimization
</span><span class="n">python</span> <span class="n">quantize</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="nb">input</span> <span class="n">preprocess</span><span class="p">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">output</span> <span class="n">quantized_2</span><span class="p">.</span><span class="n">onnx</span>
<span class="c1"># Step 3. Debug
</span><span class="n">python</span> <span class="n">preprocess</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="nb">input</span> <span class="n">original</span><span class="p">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">output</span> <span class="n">preprocess_2</span><span class="p">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">skip_symbolic_shape</span> <span class="bp">True</span>
<span class="n">python</span> <span class="n">debug</span><span class="p">.</span><span class="n">py</span> <span class="o">--</span><span class="n">float_model</span> <span class="n">preprocess_2</span><span class="p">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">qdq_model</span> <span class="n">quantized_2</span><span class="p">.</span><span class="n">onnx</span>
</code></pre></div></div> <h1 id="result">Result</h1> <h2 id="shufflenetv2">shufflenetv2</h2> <h3 id="pytorch">Pytorch</h3> <h3 id="experiment">Experiment</h3> <ul> <li> <p>dataset :<br> calibration : batch_size = 32, iteration = 32<br> inference : batch_size = 128, iteration = 61</p> </li> <li> <p>hardware : cpu</p> </li> </ul> <table> <thead> <tr> <th style="text-align: left">method</th> <th style="text-align: left">model size [MB]</th> <th style="text-align: left">inference time [s]</th> <th style="text-align: left">loss</th> <th style="text-align: left">ocualr_nme [%]</th> <th style="text-align: left">pupil_nme [%]</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Original Model (partial fuse)</td> <td style="text-align: left">7.15</td> <td style="text-align: left">306.59</td> <td style="text-align: left">0.01121577</td> <td style="text-align: left">3.51807</td> <td style="text-align: left">4.87317</td> </tr> <tr> <td style="text-align: left">Original Model (all fuse)</td> <td style="text-align: left">7.01</td> <td style="text-align: left">227.92</td> <td style="text-align: left">0.01121577</td> <td style="text-align: left">3.51807</td> <td style="text-align: left">4.87317</td> </tr> <tr> <td style="text-align: left">Static PTQ calibrated with dummy input (partial fuse)</td> <td style="text-align: left">2.19</td> <td style="text-align: left">174.01</td> <td style="text-align: left">0.04872545</td> <td style="text-align: left">15.87943</td> <td style="text-align: left">21.99286</td> </tr> <tr> <td style="text-align: left">Static PTQ calibrated with our input dataset (partial fuse)</td> <td style="text-align: left">2.19</td> <td style="text-align: left">173.31</td> <td style="text-align: left">0.01834808</td> <td style="text-align: left">5.68243</td> <td style="text-align: left">7.87280</td> </tr> <tr> <td style="text-align: left">Static PTQ calibrated with our input dataset (all fuse)</td> <td style="text-align: left">2.01</td> <td style="text-align: left">161.74</td> <td style="text-align: left">0.01228440</td> <td style="text-align: left">3.83984</td> <td style="text-align: left">5.31811</td> </tr> </tbody> </table> <h3 id="result-1">Result</h3> <ol> <li>By Post-Training Static Quantization, model size of shufflenetv2 was reduced by about 3.5 times.</li> <li>By Post-Training Static Quantization, inference speed was improved by about 1.5 times.</li> <li> <p>To minimize the accuracy drop, it is better to use our dataset as input of calibration rather than dummy input.</p> <p>It’s because, during calibration, we can figure out the range of activations similarly to when the model was trained and inferred.</p> </li> <li>To minimize the accuracy drop, it is better to fuse all the layers of Conv-Bn or Conv-Bn-ReLU or Conv-ReLU because the number of layers to be quantized are reduced.</li> </ol> <h2 id="resnet18">resnet18</h2> <h3 id="pytorch-1">Pytorch</h3> <h3 id="exp-1--qat---the-effect-of-learning-rate">Exp 1. QAT - The effect of learning rate</h3> <p>The proper learning rate of fine-tuning is lr = 1e-8 which is 0.1 times the learning rate of pre-trained model 1e-7.</p> <p>I tested various values of learning rate to find the optimal value. For example, the left graph (lr = 1e-10) below shows underfitting and slow convergence due to small learning rate. However, the right graph (lr = 1e-8) below shows proper convergence.</p> <p>loss graph</p> <ul> <li>pink : test loss before quantization</li> <li>blue : training loss before quantization</li> <li>orange : test loss after quantization</li> </ul> <p>nme graph</p> <ul> <li>green : test nme before quantization</li> <li>red : training nme before quantization</li> <li>blue : test nme after quantization</li> </ul> <p>left : lr = 1e-10<br> right : lr = 1e-8</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled9-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled9-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled9-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-03-02-Quantization/Untitled9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="exp-2-qat---the-effect-of-epoch-to-freeze-quantization-parameters-and-bn-stat">Exp 2. QAT - The effect of epoch to freeze quantization parameters and bn stat</h3> <p>There is no significant difference in the effect of epochs on which you will freeze the quantization parameters(observers) and BatchNorm stats.</p> <p>I tested various values of epoch to freeze the observers and bn stats, but there was no significant difference in the resulting converged value of loss or nme.</p> <p>loss graph</p> <ul> <li>pink : test loss before quantization</li> <li>blue : training loss before quantization</li> <li>orange : test loss after quantization</li> </ul> <p>nme graph</p> <ul> <li>green : test nme before quantization</li> <li>red : training nme before quantization</li> <li>blue : test nme after quantization</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled10-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled10-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled10-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-03-02-Quantization/Untitled10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="exp-3-qat---the-effect-of-observer-to-calibrate-the-quantization-parameters">Exp 3. QAT - The effect of observer to calibrate the quantization parameters</h3> <p>MovingAverageMinMaxObserver with U8S8 calibrated the quantization parameters(scale factor and zero-point integer) better than the default observer as shown in the graph below.</p> <p>I tested only three kinds of observers : default observer, histogram observer, and MovingAverageMinMaxObserver with U8S8. Among them, the last one performed the best, but there are also many other types of observers and other observers may perform better. The observers are listed in the link below.</p> <p><a href="https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/observer.py" rel="external nofollow noopener" target="_blank">https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/observer.py</a></p> <p>loss graph</p> <ul> <li>pink : test loss before quantization</li> <li>blue : training loss before quantization</li> <li>orange : test loss after quantization</li> </ul> <p>nme graph</p> <ul> <li>green : test nme before quantization</li> <li>red : training nme before quantization</li> <li>blue : test nme after quantization</li> </ul> <p>left : default observer<br> right : MovingAverageMinMaxObserver with U8S8</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled11-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled11-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled11-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-03-02-Quantization/Untitled11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="experiment-1">Experiment</h3> <ul> <li> <p>dataset :<br> calibration : batch_size = 32, iteration = 32<br> inference : batch_size = 128, iteration = 61</p> </li> <li>hardware : cpu</li> <li>learning rate : 1e-8</li> </ul> <table> <thead> <tr> <th style="text-align: left">method</th> <th style="text-align: left">model size [MB]</th> <th style="text-align: left">inference time [s]</th> <th style="text-align: left">loss</th> <th style="text-align: left">ocualr_nme [%]</th> <th style="text-align: left">pupil_nme [%]</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Original Model</td> <td style="text-align: left">44.011</td> <td style="text-align: left">76.06</td> <td style="text-align: left">0.04312033</td> <td style="text-align: left">2.73897</td> <td style="text-align: left">3.79658</td> </tr> <tr> <td style="text-align: left">Static PTQ with default observer</td> <td style="text-align: left">11.121</td> <td style="text-align: left">22.30</td> <td style="text-align: left">0.07286325</td> <td style="text-align: left">4.54174</td> <td style="text-align: left">6.29569</td> </tr> <tr> <td style="text-align: left">QAT with default observer</td> <td style="text-align: left">11.121</td> <td style="text-align: left">22.04</td> <td style="text-align: left">0.07202724</td> <td style="text-align: left">4.48688</td> <td style="text-align: left">6.21954</td> </tr> <tr> <td style="text-align: left">QAT with MovingAverage MinMaxObserver (U8S8)</td> <td style="text-align: left">11.121</td> <td style="text-align: left">21.75</td> <td style="text-align: left">0.05595706</td> <td style="text-align: left">3.50271</td> <td style="text-align: left">4.85493</td> </tr> </tbody> </table> <h3 id="result-2">Result</h3> <ol> <li>By both Post-Training Static Quantization and Quantization-Aware Training, model size of resnet18 was reduced by about 4 times.</li> <li>By both Post-Training Static Quantization and Quantization-Aware Training, inference speed was improved by about 3.5 times.</li> <li>QAT performs a little bit better than Static PTQ due to the fine-tuning process.</li> <li>MovingAverageMinMaxObserver with U8S8 performed better than the default observer. However, there may exist another better observer since I only tested three kinds of observers.</li> </ol> <h3 id="generated-landmark">Generated landmark</h3> <p>You can see that the inference output of QAT model is better than that of Static PTQ model.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled12-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled12-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled12-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-03-02-Quantization/Untitled12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>When the input is a tight face, the inference output of QAT model seems nearly similar to that of original model.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled13-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled13-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled13-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-03-02-Quantization/Untitled13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="onnx">ONNX</h3> <h3 id="visualization-of-onnx-via-netronapp">Visualization of ONNX via netron.app</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled14-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled14-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled14-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-03-02-Quantization/Untitled14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled15-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled15-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled15-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-03-02-Quantization/Untitled15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled16-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled16-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled16-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-03-02-Quantization/Untitled16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Visualization of ONNX : Fused Model &amp; Quantized Model (QDQ format) </div> <h3 id="int8-quantization-of-onnx-runtime">INT8 Quantization of ONNX Runtime</h3> <p>INT8 Quantization follows the link below.</p> <p><a href="https://github.com/microsoft/onnxruntime-inference-examples/tree/main/quantization/image_classification/cpu" rel="external nofollow noopener" target="_blank">https://github.com/microsoft/onnxruntime-inference-examples/tree/main/quantization/image_classification/cpu</a></p> <h3 id="experiment-2">Experiment</h3> <ul> <li>dataset : inference : batch_size = 1, iteration = 7799</li> <li>hardware : cpu</li> </ul> <p>This table shows how I quantized each ONNX model adjusting combination of some parameters.</p> <table> <thead> <tr> <th style="text-align: left">IN8 Quantization</th> <th style="text-align: left">freezed layer</th> <th style="text-align: left">per channel</th> <th style="text-align: left">reduce range</th> <th style="text-align: left">CalibTensor Range Symmetric</th> <th style="text-align: left">Calib Moving Average</th> <th style="text-align: left">Smooth Quant</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Pre-processed Model</td> <td style="text-align: left"> </td> <td style="text-align: left"> </td> <td style="text-align: left"> </td> <td style="text-align: left"> </td> <td style="text-align: left"> </td> <td style="text-align: left"> </td> </tr> <tr> <td style="text-align: left">Quantized Model 1</td> <td style="text-align: left"> </td> <td style="text-align: left">T</td> <td style="text-align: left">F</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> </tr> <tr> <td style="text-align: left">Quantized Model 2</td> <td style="text-align: left"> </td> <td style="text-align: left">F</td> <td style="text-align: left">F</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> </tr> <tr> <td style="text-align: left">Quantized Model 3</td> <td style="text-align: left"> </td> <td style="text-align: left">T</td> <td style="text-align: left">F</td> <td style="text-align: left">F</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> </tr> <tr> <td style="text-align: left">Quantized Model 4</td> <td style="text-align: left"> </td> <td style="text-align: left">T</td> <td style="text-align: left">F</td> <td style="text-align: left">F</td> <td style="text-align: left">F</td> <td style="text-align: left">F</td> </tr> <tr> <td style="text-align: left">Quantized Model 5</td> <td style="text-align: left">fc layer</td> <td style="text-align: left">T</td> <td style="text-align: left">F</td> <td style="text-align: left">F</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> </tr> <tr> <td style="text-align: left">Quantized Model 6</td> <td style="text-align: left">ReLU &amp; Add</td> <td style="text-align: left">T</td> <td style="text-align: left">F</td> <td style="text-align: left">F</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> </tr> <tr> <td style="text-align: left">Quantized Model 7</td> <td style="text-align: left">layer 4 &amp; fc layer</td> <td style="text-align: left">T</td> <td style="text-align: left">F</td> <td style="text-align: left">F</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> </tr> <tr> <td style="text-align: left">Quantized Model 8</td> <td style="text-align: left">layer 4 &amp; fc layer</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> <td style="text-align: left">F</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> </tr> <tr> <td style="text-align: left">Quantized Model 9</td> <td style="text-align: left">layer 4 &amp; fc layer</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> <td style="text-align: left">F</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> </tr> <tr> <td style="text-align: left">Quantized Model 10</td> <td style="text-align: left">layer 4 &amp; fc layer</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> <td style="text-align: left">F</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> </tr> <tr> <td style="text-align: left">Quantized Model 11</td> <td style="text-align: left">layer 1 &amp; layer 4 &amp; fc layer</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> <td style="text-align: left">F</td> <td style="text-align: left">T</td> <td style="text-align: left">T</td> </tr> </tbody> </table> <table> <thead> <tr> <th style="text-align: left">INT8 Quantization</th> <th style="text-align: left">model size [MB]</th> <th style="text-align: left">inference time [s]</th> <th style="text-align: left">loss</th> <th style="text-align: left">ocualr_nme [%]</th> <th style="text-align: left">pupil_nme [%]</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Pre-processed Model</td> <td style="text-align: left">44.044</td> <td style="text-align: left">83.05</td> <td style="text-align: left">0.00867057</td> <td style="text-align: left">2.73897</td> <td style="text-align: left">3.79658</td> </tr> <tr> <td style="text-align: left">Quantized Model 1</td> <td style="text-align: left">11.177</td> <td style="text-align: left">90.04</td> <td style="text-align: left">2.04831275</td> <td style="text-align: left">594.30187</td> <td style="text-align: left">823.93989</td> </tr> <tr> <td style="text-align: left">Quantized Model 2</td> <td style="text-align: left">11.113</td> <td style="text-align: left">91.34</td> <td style="text-align: left">2.08157768</td> <td style="text-align: left">603.68516</td> <td style="text-align: left">836.92611</td> </tr> <tr> <td style="text-align: left">Quantized Model 3</td> <td style="text-align: left">11.177</td> <td style="text-align: left">93.21</td> <td style="text-align: left">0.19241423</td> <td style="text-align: left">58.71200</td> <td style="text-align: left">81.41132</td> </tr> <tr> <td style="text-align: left">Quantized Model 4</td> <td style="text-align: left">11.177</td> <td style="text-align: left">90.74</td> <td style="text-align: left">0.20964802</td> <td style="text-align: left">65.31481</td> <td style="text-align: left">90.57096</td> </tr> <tr> <td style="text-align: left">Quantized Model 5</td> <td style="text-align: left">11.377</td> <td style="text-align: left">94.70</td> <td style="text-align: left">0.19232825</td> <td style="text-align: left">58.67378</td> <td style="text-align: left">81.35814</td> </tr> <tr> <td style="text-align: left">Quantized Model 6</td> <td style="text-align: left">11.226</td> <td style="text-align: left">100.37</td> <td style="text-align: left">0.19220746</td> <td style="text-align: left">58.66837</td> <td style="text-align: left">81.35119</td> </tr> <tr> <td style="text-align: left">Quantized Model 7</td> <td style="text-align: left">35.923</td> <td style="text-align: left">90.62</td> <td style="text-align: left">0.12938738</td> <td style="text-align: left">39.69218</td> <td style="text-align: left">55.05709</td> </tr> <tr> <td style="text-align: left">Quantized Model 8</td> <td style="text-align: left">35.923</td> <td style="text-align: left">87.56</td> <td style="text-align: left">0.12887142</td> <td style="text-align: left">39.57574</td> <td style="text-align: left">54.89788</td> </tr> <tr> <td style="text-align: left">Quantized Model 9</td> <td style="text-align: left">35.923</td> <td style="text-align: left">61.08</td> <td style="text-align: left">0.12887123</td> <td style="text-align: left">39.57566</td> <td style="text-align: left">54.89777</td> </tr> <tr> <td style="text-align: left">Quantized Model 10</td> <td style="text-align: left">35.923</td> <td style="text-align: left">60.07</td> <td style="text-align: left">0.12887123</td> <td style="text-align: left">39.57566</td> <td style="text-align: left">54.89777</td> </tr> <tr> <td style="text-align: left">Quantized Model 11</td> <td style="text-align: left">36.384</td> <td style="text-align: left">65.18</td> <td style="text-align: left">0.11555405</td> <td style="text-align: left">35.54718</td> <td style="text-align: left">49.31533</td> </tr> </tbody> </table> <h3 id="result-3">Result</h3> <ol> <li>By ONNX Quantization, model size of resnet18 was reduced by about 4 times for Quantized Model 1~6. (However, there is significant accuracy drop.)</li> <li> <p>By ONNX Quantization, there is no notable improvement on inference time.</p> <p>The performance improvement depends on your model and hardware. The performance gain from quantization has two aspects: compute and memory. Old hardware has none or few of the instructions needed to perform efficient inference in int8. And quantization has overhead (from quantizing and dequantizing), so it is not rare to get worse performance on some devices.</p> <p>x86-64 with VNNI, GPU with Tensor Core int8 support, and ARM with dot-product instructions can get better performance in general. But, there may not exist a notable improvement on inference time.</p> </li> <li> <p>By ONNX Quantization, it is not rare to see significant accuracy drop. Then you can try U8U8.</p> <p>When to try U8U8 data type? :</p> <p>On x86-64 machines with AVX2 and AVX512 extensions, ONNX Runtime uses the VPMADDUBSW instruction for U8S8 for performance. However, this instruction might suffer from saturation issues: it can happen that the output does not fit into a 8-bit, 8-bit integer and has to be clamped (saturated) to fit. Generally, this is not a big issue for the final result. However, if you encounter a significant accuracy drop, it may be caused by saturation. In this case, you can try U8U8 with reduce_range.</p> </li> <li> <p>By ONNX Quantization, it is not rare to see significant accuracy drop. Then you can try reduce_range = True or per_channel = True.</p> <p>When to use reduce_range and per_channel quantization? :</p> <p>Reduce-range will quantize the weights to 7-bits. It is designed for the U8S8 format on AVX2 and AVX512 (non-VNNI) machines to mitigate saturation issues. This is not needed on machines supporting VNNI.</p> <p>Per-channel quantization can improve the accuracy for models whose weight ranges are large. You can try it if the accuracy drop is large. In addition, on AVX2 and AVX512 machines, you will generally need to enable reduce_range as well if per_channel is enabled.</p> </li> <li>By ONNX Quantization, it is not rare to see significant accuracy drop. To improve the problematic parts, you can compare the weights and activations tensors between the original computation graph and the quantized comptuation graph. By debugging, you can identify where they differ most and avoid quantizing these nodes.</li> <li> <p>I only tried ONNX Runtime quantization on CPU, but you can also try quantization on GPU and you can use many other Execution Providers.</p> <p>The ONNX Runtime material below suggests that you can try quantization on GPU if there is a significant accuracy drop.</p> <p><a href="https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html#quantization-on-gpu" rel="external nofollow noopener" target="_blank">https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html#quantization-on-gpu</a></p> <p>If you are interested in other Execution Providers, refer to the link below.</p> <p><a href="https://onnxruntime.ai/docs/execution-providers/" rel="external nofollow noopener" target="_blank">https://onnxruntime.ai/docs/execution-providers/</a></p> </li> <li>If the Post-Training Quantization method cannot meet accuracy goal, you can try using QAT (Quantization-Aware Training) to retrain the model. However, ONNX Runtime does not provide retraining at this time, so you should re-train your models with the original framework (in my case, Pytorch) and convert them back to ONNX.</li> </ol> <h3 id="generated-landmark-1">Generated landmark</h3> <p>You can see that accuracy drop of ONNX quantization is significant.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled17-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled17-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled17-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-03-02-Quantization/Untitled17.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="debugging">Debugging</h3> <p>Briefly, I will explain how to debug the weights of a model. To learn how to debug the activations of a model, refer to the link below.</p> <p><a href="https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/quantization/qdq_loss_debug.py" rel="external nofollow noopener" target="_blank">https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/quantization/qdq_loss_debug.py</a></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">create_weight_matching</span><span class="p">(</span><span class="sh">"</span><span class="s">path/float.onnx</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">path/qdq.onnx</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># This function dequantizes a quantized weight following
#	the linear dequantization equation x = s * (x_q - z)
# Then it returns dictA = {"onnx::Conv_193" : 
#					 									 {"float": w1, "dequantized" : w2},
#											 		 "onnx::Conv_194" : 
#														 {"float": w1, "dequantized" : w2}}
# Here, w1 means the fp32 weight of original model
# and w2 means the dequantized weight of quantized model
</span>
<span class="nf">compute_weight_error</span><span class="p">(</span><span class="n">dictA</span><span class="p">)</span>
<span class="c1"># This function computes SQNR = P_signal / P_noise 
#                             = 20log(|w1|/|w1-w2|)
# Then it returns dictB = {"onnx::Conv_193" : SQNR1,
#                          "onnx::Conv_194" : SQNR2}
# If the SQNR value is larger, then it means the error is smaller.
</span></code></pre></div></div> <p>By using the above functions, you can figure out which node has the significant quantization error and avoid quantizing those nodes.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled18-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled18-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled18-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-03-02-Quantization/Untitled18.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>SQNR of each node by debugging</p> <p>In my case, SQNR value for weight matrix of each node was in the range of [23, 37], which means that the value of \(\frac{\vert w1 \vert}{\vert w1-w2 \vert}\) is in the range of [14, 70].</p> <p>Also, SQNR value for bias vector of each node was in the range of [46, 71], which means that the value of \(\frac{\vert w1 \vert}{\vert w1-w2 \vert}\) is in the range of [199, 3548].</p> <h3 id="fp16-conversion-of-onnx-runtime">FP16 Conversion of ONNX Runtime</h3> <p>FP16 Conversion follows the link below.</p> <p><a href="https://onnxruntime.ai/docs/performance/model-optimizations/float16.html" rel="external nofollow noopener" target="_blank">https://onnxruntime.ai/docs/performance/model-optimizations/float16.html</a></p> <p>ONNX Runtime INT8 Quantization was not successful, so I also tried FP16 Conversion of an ONNX model.</p> <h3 id="experiment-3">Experiment</h3> <ul> <li>hardware : cpu</li> </ul> <table> <thead> <tr> <th style="text-align: left">FP16 Conversion</th> <th style="text-align: left">model size [MB]</th> <th style="text-align: left">inference time [s]</th> <th style="text-align: left">loss</th> <th style="text-align: left">ocualr_nme [%]</th> <th style="text-align: left">pupil_nme [%]</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Original Model</td> <td style="text-align: left">43.922</td> <td style="text-align: left">207.34</td> <td style="text-align: left">0.00867057</td> <td style="text-align: left">2.73897</td> <td style="text-align: left">3.79658</td> </tr> <tr> <td style="text-align: left">Converted Model</td> <td style="text-align: left">21.969</td> <td style="text-align: left">190.82</td> <td style="text-align: left">0.00867078</td> <td style="text-align: left">2.73904</td> <td style="text-align: left">3.79668</td> </tr> </tbody> </table> <h3 id="result-4">Result</h3> <ol> <li>By ONNX Runtime FP16 Conversion, the model size of resnet18 was reduced by about 2 times.</li> <li>By ONNX Runtime FP16 Conversion on CPU, there was no notable improvement on inference time since CPU cannot utilize FP16 speedup. However, the inference speed will be improved if you use GPUs.</li> <li>Accuracy drop almost does not occur after FP16 Conversion.</li> </ol> <h3 id="generated-landmark-2">Generated landmark</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-03-02-Quantization/Untitled19-480.webp 480w,/assets/img/2024-03-02-Quantization/Untitled19-800.webp 800w,/assets/img/2024-03-02-Quantization/Untitled19-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-03-02-Quantization/Untitled19.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h1 id="conclusion">Conclusion</h1> <p>In conclusion, I provided a comprehensive overview of the diverse quantization techniques available in Pytorch and ONNX. We have mainly discussed three techniques : Post-Training Dynamic Quantization, Post-Training Static Quantization, and Quantization-Aware Training.</p> <p>Remember that quantization is required to efficiently deploy your model on resouce-constrained devices even if there is a trade-off of accuracy drop. Therefore, quantization has to be done targeting your specific hardware and there are actually various quantization details depending on the hardware. Starting with studying quantization in Pytorch and ONNX, I encourage you to dive deeper into quantization that fits your hardware.</p> <p>Moreover, as the field of deep learning continues to expand, quantization will persist as a critical factor for the widespread deployment of neural network models on various hardware platforms and real-world applications. Therefore, I encourage you to continuously have interest in performance improvement including quantization so that you can successfully bridge the gap between powerful AI models and resource-constrained real-world environments.</p> <h1 id="reference">Reference</h1> <h2 id="pytorch-2">Pytorch</h2> <h3 id="torchaoquantization">torch.ao.quantization</h3> <p>[1] Quantization :</p> <p><a href="https://github.com/Lornatang/PyTorch/blob/1a998bf1baded12869c466f8dcfb7b6130f57d02/docs/source/quantization.rst#L583" rel="external nofollow noopener" target="_blank">https://github.com/Lornatang/PyTorch/blob/1a998bf1baded12869c466f8dcfb7b6130f57d02/docs/source/quantization.rst#L583</a></p> <p>[2] Quantization :</p> <p><a href="https://pytorch.org/docs/stable/quantization.html" rel="external nofollow noopener" target="_blank">https://pytorch.org/docs/stable/quantization.html</a></p> <p>[3] Principle of Quantization :</p> <p><a href="https://pytorch.org/blog/quantization-in-practice/" rel="external nofollow noopener" target="_blank">https://pytorch.org/blog/quantization-in-practice/</a></p> <p>[4] PTQ, QAT :</p> <p><a href="https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html" rel="external nofollow noopener" target="_blank">https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html</a></p> <p>[5] Post-Training Static Quantization of resnet18 : <a href="https://github.com/Sanjana7395/static_quantization" rel="external nofollow noopener" target="_blank">https://github.com/Sanjana7395/static_quantization</a></p> <p>[6] Quantization-Aware Training of resnet18 :</p> <p><a href="https://gaussian37.github.io/dl-pytorch-quantization/" rel="external nofollow noopener" target="_blank">https://gaussian37.github.io/dl-pytorch-quantization/</a></p> <p>[7] Freeze observer, bn stat in QAT :</p> <p><a href="https://github.com/pytorch/vision/blob/main/references/classification/train_quantization.py" rel="external nofollow noopener" target="_blank">https://github.com/pytorch/vision/blob/main/references/classification/train_quantization.py</a></p> <p>[8] Observers :</p> <p><a href="https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/observer.py" rel="external nofollow noopener" target="_blank">https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/observer.py</a></p> <p>[9] fused modules :</p> <p><a href="https://pytorch.org/docs/stable/_modules/torch/ao/nn/intrinsic/modules/fused.html#ConvReLU2d" rel="external nofollow noopener" target="_blank">https://pytorch.org/docs/stable/_modules/torch/ao/nn/intrinsic/modules/fused.html#ConvReLU2d</a></p> <p>[11] Quantized Transfer Learning : <a href="https://tutorials.pytorch.kr/intermediate/quantized_transfer_learning_tutorial.html#part-1-training-a-custom-classifier-based-on-a-quantized-feature-extractor" rel="external nofollow noopener" target="_blank">https://tutorials.pytorch.kr/intermediate/quantized_transfer_learning_tutorial.html#part-1-training-a-custom-classifier-based-on-a-quantized-feature-extractor</a></p> <h3 id="pytorch_quantization">pytorch_quantization</h3> <p>[12] Quantization :</p> <p><a href="https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization" rel="external nofollow noopener" target="_blank">https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization</a></p> <p>[14] quant_modules :</p> <p><a href="https://www.ccoderun.ca/programming/doxygen/tensorrt/namespacepytorch__quantization_1_1quant__modules.html" rel="external nofollow noopener" target="_blank">https://www.ccoderun.ca/programming/doxygen/tensorrt/namespacepytorch__quantization_1_1quant__modules.html</a></p> <p>[16] Post-Training Static Quantization of resnet50 :</p> <p><a href="https://github.com/NVIDIA/TensorRT/blob/master/tools/pytorch-quantization/examples/torchvision/classification_flow.py" rel="external nofollow noopener" target="_blank">https://github.com/NVIDIA/TensorRT/blob/master/tools/pytorch-quantization/examples/torchvision/classification_flow.py</a></p> <h2 id="onnx-1">ONNX</h2> <p>[17] netron : <a href="https://netron.app/" rel="external nofollow noopener" target="_blank">https://netron.app/</a></p> <p>[18] ONNX :</p> <p><a href="https://pytorch.org/docs/stable/onnx.html" rel="external nofollow noopener" target="_blank">https://pytorch.org/docs/stable/onnx.html</a></p> <p><a href="https://gaussian37.github.io/dl-pytorch-deploy/#onnxruntime%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EB%AA%A8%EB%8D%B8-%EC%82%AC%EC%9A%A9-1" rel="external nofollow noopener" target="_blank">https://gaussian37.github.io/dl-pytorch-deploy/#onnxruntime을-이용한-모델-사용-1</a></p> <p>[19] inference in ONNX : <a href="https://seokhyun2.tistory.com/83" rel="external nofollow noopener" target="_blank">https://seokhyun2.tistory.com/83</a></p> <p>[20] ONNX Quantization :</p> <p><a href="https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html" rel="external nofollow noopener" target="_blank">https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html</a></p> <p>[21] ONNX Quantization on CPU :</p> <p><a href="https://github.com/microsoft/onnxruntime-inference-examples/tree/main/quantization/image_classification/cpu" rel="external nofollow noopener" target="_blank">https://github.com/microsoft/onnxruntime-inference-examples/tree/main/quantization/image_classification/cpu</a></p> <p>[22] ONNX Quantization on CPU - quantize :</p> <p><a href="https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/quantization/quantize.py" rel="external nofollow noopener" target="_blank">https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/quantization/quantize.py</a></p> <p>[23] ONNX Quantization on CPU - debug :</p> <p><a href="https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/quantization/qdq_loss_debug.py#L361" rel="external nofollow noopener" target="_blank">https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/quantization/qdq_loss_debug.py#L361</a></p> <p>[24] ONNX Quantization by ONNX Optimizer :</p> <p><a href="https://github.com/onnx/optimizer" rel="external nofollow noopener" target="_blank">https://github.com/onnx/optimizer</a></p> <p>[25] ONNX Quantization by neural-compressor :</p> <p><a href="https://github.com/intel/neural-compressor/blob/master/examples/onnxrt/image_recognition/onnx_model_zoo/resnet50/quantization/ptq_static/main.py" rel="external nofollow noopener" target="_blank">https://github.com/intel/neural-compressor/blob/master/examples/onnxrt/image_recognition/onnx_model_zoo/resnet50/quantization/ptq_static/main.py</a></p> <p>[26] TensorRT Execution Provider :</p> <p><a href="https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html" rel="external nofollow noopener" target="_blank">https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html</a></p> <p><a href="https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/observer.py" rel="external nofollow noopener" target="_blank">https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/observer.py</a></p> <p>[27] Huggingface Optimum : Export to ONNX, Quantization, Graph Optimization :</p> <p><a href="https://blog.naver.com/wooy0ng/223007164371" rel="external nofollow noopener" target="_blank">https://blog.naver.com/wooy0ng/223007164371</a></p> <p>[28] Export to ONNX (Brevitas) :</p> <p><a href="https://xilinx.github.io/brevitas/getting_started" rel="external nofollow noopener" target="_blank">https://xilinx.github.io/brevitas/getting_started</a></p> <p>[29] Export to ONNX (from Pytorch to ONNX) :</p> <p><a href="https://tutorials.pytorch.kr/advanced/super_resolution_with_onnxruntime.html" rel="external nofollow noopener" target="_blank">https://tutorials.pytorch.kr/advanced/super_resolution_with_onnxruntime.html</a></p> <p><a href="https://yunmorning.tistory.com/17" rel="external nofollow noopener" target="_blank">https://yunmorning.tistory.com/17</a></p> <p><a href="https://mmclassification.readthedocs.io/en/latest/tools/pytorch2onnx.html" rel="external nofollow noopener" target="_blank">https://mmclassification.readthedocs.io/en/latest/tools/pytorch2onnx.html</a></p> <p><a href="https://discuss.pytorch.org/t/onnx-export-of-quantized-model/76884/33" rel="external nofollow noopener" target="_blank">https://discuss.pytorch.org/t/onnx-export-of-quantized-model/76884/33</a></p> <p><a href="https://discuss.pytorch.org/t/onnx-export-of-quantized-model/76884/26?page=2" rel="external nofollow noopener" target="_blank">https://discuss.pytorch.org/t/onnx-export-of-quantized-model/76884/26?page=2</a></p> <p>[30] Conversion to TensorRT (from ONNX to TensorRT) :</p> <p><a href="https://mmclassification.readthedocs.io/en/latest/tools/onnx2tensorrt.html" rel="external nofollow noopener" target="_blank">https://mmclassification.readthedocs.io/en/latest/tools/onnx2tensorrt.html</a></p> <h2 id="other-references">Other references</h2> <p>[31] Quantization :</p> <p><a href="https://gaussian37.github.io/dl-concept-quantization/" rel="external nofollow noopener" target="_blank">https://gaussian37.github.io/dl-concept-quantization/</a></p> <p><a href="https://velog.io/@jooh95/%EB%94%A5%EB%9F%AC%EB%8B%9D-Quantization%EC%96%91%EC%9E%90%ED%99%94-%EC%A0%95%EB%A6%AC" rel="external nofollow noopener" target="_blank">https://velog.io/@jooh95/딥러닝-Quantization양자화-정리</a></p> <p><a href="https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Enhanced-low-precision-pipeline-to-accelerate-inference-with/post/1335626" rel="external nofollow noopener" target="_blank">https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Enhanced-low-precision-pipeline-to-accelerate-inference-with/post/1335626</a></p> <p>[32] timm resnet18 :</p> <p><a href="https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/resnet.py" rel="external nofollow noopener" target="_blank">https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/resnet.py</a></p> <p>[33] Depth-wise Separable Convolution : <a href="https://coding-yoon.tistory.com/122" rel="external nofollow noopener" target="_blank">https://coding-yoon.tistory.com/122</a></p> <p>[34] TensorRT :</p> <p><a href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#fit" rel="external nofollow noopener" target="_blank">https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#fit</a></p> <p>[35] Attention Round : <a href="https://arxiv.org/abs/2207.03088" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2207.03088</a></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/NeRF/">NeRF</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/FMANet/">FMANet</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/GS/">3D Gaussian Splatting</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/PytorchBasic/">Pytorch Basic Code (DDP)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/SfMLearner/">SfMLearner</a> </li> <div id="disqus_thread" style="max-width: 800px; margin: 0 auto;"> <script type="text/javascript">var disqus_shortname="semyeong-yu",disqus_identifier="/blog/2024/Quantization",disqus_title="Quantization";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> </div> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Semyeong Yu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?5d75c11f89cd96294bf5e6dd1ee1bb30"></script> <script defer src="/assets/js/common.js?fcfacfb8c6281f5e68d5a7d348186eb1"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5YMLX9VHFX"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5YMLX9VHFX");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8.0.6/dist/index.min.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-element-bundle.min.js" integrity="sha256-BPrwikijIybg9OQC5SYFFqhBjERYOn97tCureFgYH1E=" crossorigin="anonymous"></script> </body> </html>