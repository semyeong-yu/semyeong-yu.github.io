<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Feed Forward Bullet Time Reconstruction | Semyeong Yu </title> <meta name="author" content="Semyeong Yu"> <meta name="description" content="Feed Forward Bullet Time Reconstruction of Dynamic Scenes from Monocular Videos (CVPR 2025)"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://semyeong-yu.github.io/blog/2025/BTimer/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Feed Forward Bullet Time Reconstruction",
            "description": "Feed Forward Bullet Time Reconstruction of Dynamic Scenes from Monocular Videos (CVPR 2025)",
            "published": "January 10, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Semyeong</span> Yu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Feed Forward Bullet Time Reconstruction</h1> <p>Feed Forward Bullet Time Reconstruction of Dynamic Scenes from Monocular Videos (CVPR 2025)</p> </d-title> <d-article> <h2 id="feed-forward-bullet-time-reconstruction-of-dynamic-scenes-from-monocular-videos">Feed Forward Bullet Time Reconstruction of Dynamic Scenes from Monocular Videos</h2> <h4 id="hanxue-liang-jiawei-ren-ashkan-mirzaei-antonio-torralba-ziwei-liu-igor-gilitschenski-sanja-fidler-cengiz-oztireli-huan-ling-zan-gojcic-jiahui-huang">Hanxue Liang, Jiawei Ren, Ashkan Mirzaei, Antonio Torralba, Ziwei Liu, Igor Gilitschenski, Sanja Fidler, Cengiz Oztireli, Huan Ling, Zan Gojcic, Jiahui Huang</h4> <blockquote> <p>paper :<br> <a href="https://arxiv.org/abs/2412.03526" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2412.03526</a><br> project website :<br> <a href="https://research.nvidia.com/labs/toronto-ai/bullet-timer/" rel="external nofollow noopener" target="_blank">https://research.nvidia.com/labs/toronto-ai/bullet-timer/</a></p> </blockquote> <h2 id="contribution">Contribution</h2> <ul> <li>model : <ul> <li> <code class="language-plaintext highlighter-rouge">motion-aware</code> <code class="language-plaintext highlighter-rouge">feed-forward</code> model for real-time recon. and novel-view-synthesis of <code class="language-plaintext highlighter-rouge">dynamic</code> scenes</li> <li>obtain <code class="language-plaintext highlighter-rouge">scalability</code> and <code class="language-plaintext highlighter-rouge">generalization</code> by using both static and dynamic scene datasets<br> (static and dynamic recon.에 모두 사용 가능)</li> <li>Procedure : <ul> <li>Step 1) pre-train on large static scene dataset</li> <li>Step 2) video duration or FPS에 구애받지 않고 scale effectively across datasets</li> <li>Step 3) output multi-view volumetric video representation</li> </ul> </li> <li>recon. a bullet-time scene within 150ms with SOTA performance on a single GPU<br> from 12 context frames of \(256 \times 256\) resolution</li> <li>bullet (target) timestamp \(t_{b}\) 를 원하는 each timestamp in video로 설정하면<br> full video를 recon.할 수 있으므로<br> 각 frame을 <code class="language-plaintext highlighter-rouge">parallel</code>하게 recon. 가능</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/2m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/2m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/2m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-01-10-BTimer/2m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <code class="language-plaintext highlighter-rouge">BulletTimer</code> (Novelty 1.) :<br> main model <ul> <li>recon. at <code class="language-plaintext highlighter-rouge">arbitrary</code> target (bullet) timestamp and <code class="language-plaintext highlighter-rouge">arbitrary</code> novel-view<br> by adding <code class="language-plaintext highlighter-rouge">bullet-time embedding</code> to all the context (input) frames<br> and <code class="language-plaintext highlighter-rouge">aggregating</code> pred. from all the context frames</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">NTE Module</code> (Novelty 2.) :<br> <code class="language-plaintext highlighter-rouge">pre-processing</code> (FPI) <ul> <li> <code class="language-plaintext highlighter-rouge">fast motion</code>에 대응하기 위해<br> model에 feed하기 전에<br> <code class="language-plaintext highlighter-rouge">intermediate (interpolated) frames를 predict</code> </li> <li>inference할 때<br> arbitrary target (bullet) timestamp에 대해<br> recon.할 수 있도록 도움</li> </ul> </li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li> <p>Dynamic scene recon. from monocular video :<br> still challenging<br> due to inherently ill-posed (해가 무수히 많음) nature of dynamic recon. from limited observations</p> </li> <li> <p>Static scene recon. :</p> <ul> <li>optimization-based (per-scene) :<br> NeRF, HyperNeRF</li> <li>learning-based (feed-forward) :<br> MonoNeRF, GS-LRM</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/5m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/5m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/5m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-01-10-BTimer/5m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Dynamic scene recon. :<br> dynamic scene은 complex motion 때문에 ambiguity 존재<br> 이를 해소하는 데 도움될 data prior 필요 <ul> <li>optimization-based (per-scene) : <ul> <li>use contraint (data prior) like depth and optical flow <d-cite key="33">[1]</d-cite>, <d-cite key="36">[2]</d-cite>, <d-cite key="37">[3]</d-cite>, <d-cite key="68">[4]</d-cite><br> \(\rightarrow\)<br> given data와 위의 data prior 간의 싱크를 맞추는 게 challenging <d-cite key="34">[5]</d-cite>, <d-cite key="63">[6]</d-cite> </li> <li>per-scene approach는 time-consuming and thus scale 어렵</li> </ul> </li> <li>learning-based (<code class="language-plaintext highlighter-rouge">feed-forward</code>) : <ul> <li>directly predict recon. in feed-forward manner<br> so, can <code class="language-plaintext highlighter-rouge">learn strong inherent prior directly from data</code> <d-cite key="7">[7]</d-cite>, <d-cite key="10">[8]</d-cite>, <d-cite key="12">[9]</d-cite>, <d-cite key="25">[10]</d-cite>, <d-cite key="53">[11]</d-cite> </li> <li>근데 dynamic scene 모델링하기 복잡하고 4D supervision data 부족해서 적용하는데 한계 있었음</li> <li>지금 시점 기준 L4GM <d-cite key="53">[11]</d-cite> 이 유일한 feed-forward dynamic recon. model인데,<br> synthetic object-centric dataset으로 훈련돼서<br> fixed camera view-point와 multi-view supervision을 필요로 한다는 한계와<br> real-world scene에 generalize하기 어렵다는 한계가 있었음</li> </ul> </li> </ul> </li> <li>Feed-Forward Dynamic scene recon. : <ul> <li>본 논문은<br> <code class="language-plaintext highlighter-rouge">pixel-aligned 3DGS</code> <d-cite key="79">[12]</d-cite> 를 기반으로<br> novel BulletTimer and NTE module 제안</li> </ul> </li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li>Dynamic 3D Representation : <ul> <li>TBD</li> </ul> </li> <li>Novel-View-Synthesis : <ul> <li>TBD</li> </ul> </li> <li>Feed-Forward Reconstruction : <ul> <li>TBD</li> </ul> </li> </ul> <h2 id="overview">Overview</h2> <ul> <li> <p>notation :<br> context frames \(I_{c} \subset I\)<br> camera poses \(P_{c} \subset P\)<br> context timestamps \(T_{c} \subset T\)<br> bullet timestamp \(t_{b} \in [\text{min}(T_{c}), \text{max}(T_{c})]\)<br> recon. at timestamp \(t \notin T\) by NTE module</p> </li> <li> <p>Architecture :</p> <ul> <li>Training :<br> BTimer와 NTE Module을 별도로 각각 train<br> (not end-to-end)</li> <li>Inference :<br> NTE Module로 pre-process한 뒤<br> BTimer 사용</li> </ul> </li> </ul> <h2 id="method">Method</h2> <h3 id="btimer-bullet-timer">BTimer (Bullet Timer)</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/3m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/3m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/3m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-01-10-BTimer/3m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Model Design : <ul> <li>encode (input) :<br> \(i\)-th frame \(I_{i} \in I_{c}\) 을 \(8 \times 8\) 짜리 patches로 나눈 뒤<br> \(j\)-th patch에 대해<br> per-patch input token \(f_{ij} |_{j=1}^{HW / 64} = f_{ij}^{rgb} + f_{ij}^{pose} + f_{i}^{time}\) 만든 뒤<br> concatenate input tokens from all context frames<br> and feed into Transformer <ul> <li> <code class="language-plaintext highlighter-rouge">image</code> encoder :<br> GS-LRM <d-cite key="79">[12]</d-cite> 에서 영감을 받아,<br> <code class="language-plaintext highlighter-rouge">ViT</code> model을 backbone으로 사용</li> <li> <code class="language-plaintext highlighter-rouge">camera pose</code> encoder :<br> <code class="language-plaintext highlighter-rouge">camera Plucker embedding</code> <d-cite key="70">[13]</d-cite> </li> <li> <code class="language-plaintext highlighter-rouge">time</code> encoder :<br> PE (Positional Encoding) 및 linear layer를 거쳐<br> \(t_{i}\) 와 \(t_{b}\) 를 각각 \(f_{i}^{ctx}\) 와 \(f_{i}^{bullet}\) 으로 encode한 뒤<br> \(f_{i}^{time} = f_{i}^{ctx} + f_{i}^{bullet}\) <ul> <li> <code class="language-plaintext highlighter-rouge">context (input)</code> timestamp \(t_{i}\) from context (input) frame \(I_{i}\)</li> <li> <code class="language-plaintext highlighter-rouge">bullet (target)</code> timestamp \(t_{b}\) that is <code class="language-plaintext highlighter-rouge">shared</code> across context (input) frames</li> </ul> </li> </ul> </li> <li>decode (output) :<br> transformer의 per-patch output token \(f_{ij}^{out}\) 을<br> <code class="language-plaintext highlighter-rouge">per-patch 3DGS param. at bullet timestamp</code> \(G_{ij} \in R^{8 \times 8 \times 12}\) 로 regression <ul> <li>each Gaussian has 12 param. as color \(c \in R^{3}\), scale \(s \in R^{3}\), rotation unit quaternion \(q \in R^{4}\), opacity \(\sigma \in R\), and ray distance \(\tau \in R\)</li> <li>3D position is obtained by pixel-aligned unprojection \(\mu = o + \tau d\)<br> (\(o\) and \(d\) are obtained from camera pose \(P_{i}\))</li> </ul> </li> </ul> </li> <li> <p>Loss :<br> \(L_{RGB} = L_{MSE} + \lambda L_{LPIPS}\) with \(\lambda = 0.5\)</p> </li> <li>Timestamp :<br> context (input) frames와 bullet (target supervision) frame 을 잘 고르는 게 중요 <ul> <li> <code class="language-plaintext highlighter-rouge">In-context Supervision</code> : <ul> <li>bullet timestamp is randomly selected from context frames<br> \(t_{b} \in T_{c}\)</li> <li>model이 context timestamp에 대해 정확히 recon. 가능하도록</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">Interpolation Supervision</code> : <ul> <li>bullet timestamp lies between two adjacent context frames<br> \(t_{b} \notin T_{c}\)</li> <li>model이 dynamic parts를 interpolate할 수 있도록</li> <li>pixel-aligned 3DGS의 inductive bias 때문에<br> motion이 복잡하고 빠를 때 intermediate timestamp에 대해 예측 잘 못 함<br> \(\rightarrow\)<br> 먼저 NTE module의 도움을 받아 pre-process한 뒤<br> BTimer 사용</li> <li>local minimum 방지 및 view 간 consistency 상승</li> </ul> </li> </ul> </li> <li>Inference : <ul> <li>bullet (target) timestamp \(t_{b}\) 를 원하는 each timestamp in video로 설정하면<br> full video를 recon.할 수 있으므로<br> 각 frame을 <code class="language-plaintext highlighter-rouge">parallel</code>하게 recon. 가능</li> <li> <code class="language-plaintext highlighter-rouge">???</code><br> For a video longer than the number of training context views \(| I_{c} |\),<br> at timestamp \(t\), apart from including this exact timestamp and setting \(t_{b} = t\),<br> we uniformly distribute the remaining \(| I_{c} | − 1\) required context frames across the whole duration of the video<br> to form the input batch with \(| I_{c} |\) frames</li> </ul> </li> </ul> <h3 id="nte-module-novel-time-enhancer">NTE Module (Novel Time Enhancer)</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/4m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/4m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/4m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-01-10-BTimer/4m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>NTE Module Design :<br> decoder-only LVSM <d-cite key="27">[13]</d-cite> 에서 영감을 받아,<br> BTimer model과 구조 똑같지만,<br> I/O가 다름 <ul> <li>input : <ul> <li>context frame : <ul> <li>context (input) timestamp embedding<br> (BTimer model과 달리 bullet timestamp embedding은 안 넣음)</li> <li>camera pose Plucker embedding</li> <li>context (input) frame</li> </ul> </li> <li>intermediate frame : <ul> <li>bullet (target) timestamp embedding</li> <li>target camera pose Plucker embedding</li> </ul> </li> </ul> </li> <li>output : <ul> <li>Transformer의 per-patch output tokens 중 target token을<br> unpatchify and project directly to RGB values by linear layer<br> \(\rightarrow\)<br> RGB frame for any bullet (target) timestamp<br> (this RGB frame은 NTE module network의 direct output이고, 3DGS로 rendering한 게 아님!!)<br> \(\rightarrow\)<br> NTE Module의 output은<br> BTimer에서 bullet timestamp의 image로 쓰임</li> </ul> </li> <li>Implementation : <ul> <li>LVSM <d-cite key="27">[13]</d-cite> 에서처럼<br> 안정적인 훈련을 위해 <code class="language-plaintext highlighter-rouge">QK-norm</code> 사용<br> (Q와 K의 내적 과정에서 값이 너무 크거나 작으면 gradient explode or vanish 발생할 수 있으므로<br> Q와 K를 normalize)</li> <li>target token에 attention할 수 있도록<br> <code class="language-plaintext highlighter-rouge">masked attention</code> 사용</li> <li> <d-cite key="50">[14]</d-cite> <p>에서처럼<br> 빠른 inference를 위해 <code class="language-plaintext highlighter-rouge">KV-Cache</code> 사용<br> (training할 때는 전체 input sequence에 대해 K, Q, V를 계산하지만,<br> inference할 때는 prev. token에서 계산한 K, V를 cache에 저장한 채 매번 Q만 새로 계산함으로써 input sequence 전체에 대해 K, V를 매번 계산할 필요 없어 계산 비용 감소)</p> </li> <li>NTE Module has negligible overhead on runtime</li> </ul> </li> </ul> </li> <li> <p>Loss :<br> \(L_{RGB} = L_{MSE} + \lambda L_{LPIPS}\) with \(\lambda = 0.5\)</p> </li> <li>BTimer and NTE Module : <ul> <li>NTE Module로 직접 RGB image 예측하여<br> novel-view-synthesis 할 수 있긴 한데, 그럼 성능 안 좋음<br> (Ablation Study에 있음)</li> <li>feed-forward transformer (NTE Module)로 FPI pre-process한 뒤<br> <code class="language-plaintext highlighter-rouge">feed-forward transformer</code> (BTimer)로 data info. 포착하여<br> <code class="language-plaintext highlighter-rouge">3DGS param. 예측</code>한 뒤<br> 3DGS rasterization으로 novel-view-synthesis</li> </ul> </li> </ul> <h3 id="curriculum-training-at-scale">Curriculum Training at Scale</h3> <ul> <li>Generalizability : <ul> <li>data 다양성이 많을수록 model generalizability가 높아짐<br> static dataset은 많이 존재하고<br> dynamic dataset은 적게 존재하지만 motion awareness 및 temporal consistency 확보 가능</li> <li>본 논문의 model인 BTimer는<br> generalizable to both static and dynamic scenes <ul> <li>static scene : equalize all \(t_{b}\)</li> <li>dynamic scene : recon. at arbitrary bullet \(t_{b}\)</li> <li>different domain에서는 different model 필요로 하는<br> GS-LRM <d-cite key="79">[12]</d-cite> or MVSplat <d-cite key="10">[8]</d-cite> 과는 다름</li> </ul> </li> </ul> </li> <li>Curriculum Training : <ul> <li>Stage 1) <code class="language-plaintext highlighter-rouge">Low-res to High-res Static Pretraining</code> <ul> <li>static dataset으로 pre-train <ul> <li>both synthetic and real-world</li> <li>390K training samples</li> <li>normalize datasets to be bounded in \(10^{3}\) cube</li> <li>종류 : <ul> <li>Objaverse</li> <li>RE10K</li> <li>MVImgNet</li> <li>DL3DV</li> </ul> </li> </ul> </li> <li>no time embedding<br> (static scene이니까)</li> <li>data distribution이 복잡하기 때문에<br> coarse 세팅 (low-resol.(\(128 \times 128\)) and few-view(\(| I_{c} | = 4\)))에서 시작해서<br> 점점 fine 세팅 (high-resol.(\(256 \times 256 \rightarrow 512 \times 512\)))으로 train</li> </ul> </li> <li>Stage 2) <code class="language-plaintext highlighter-rouge">Dynamic Scene Co-training</code> <ul> <li>dynamic dataset으로 fine-tuning <ul> <li>종류 : <ul> <li>Kubric</li> <li>PointOdyssey</li> <li>DynamicReplica</li> <li>Spring</li> </ul> </li> </ul> </li> <li>4D dynamic dataset이 부족하기 때문에<br> 안정적인 훈련을 위해<br> static dataset을 함께 사용하여 co-training</li> <li>Internet video로부터 camera pose를 매기는 customized pipeline 구축하여<br> real-world data에 대한 robustness 향상 <ul> <li>먼저 PANDA-70M dataset에서 random select한 video를 20s 길이의 clips로 자르기</li> <li>SAM으로 video의 dynamic objects를 mask out</li> <li>DROID-SLAM으로 video camera pose를 estimate</li> <li>reprojection error 측정하여 low-quality의 video 및 pose는 필터링</li> <li>최종적으로 obtain 40K clips with high-quality camera trajectories</li> </ul> </li> </ul> </li> <li>Stage 3) <code class="language-plaintext highlighter-rouge">Long-context Window Fine-tuning</code> <ul> <li>NTE Module 말고 BTimer model에만 적용</li> <li>context (input) image 수를<br> \(| I_{c} | = 4\) 에서 \(| I_{c} | = 12\) 로 늘려서<br> long video recon.하는 데 도움</li> </ul> </li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <h3 id="implementation">Implementation</h3> <ul> <li>Implementation : <ul> <li>Backbone Transformer :<br> FlashAttention-3 <d-cite key="13">[15]</d-cite> and FlexAttention <d-cite key="24">[16]</d-cite> </li> <li>3DGS Rasterization :<br> gsplat library <d-cite key="74">[17]</d-cite> </li> <li>Training Schedule : <ul> <li>BTimer :<br> totally 4 days on 64 A100 GPUs <ul> <li>Stage 1)<br> \(128^{2}\) resol. 90K iter. init lr \(4 \times 10^{-4}\)<br> \(\rightarrow\)<br> \(256^{2}\) resol. 90K iter. init lr \(2 \times 10^{-4}\)<br> \(\rightarrow\)<br> \(512^{2}\) resol. 50K iter. init lr \(1 \times 10^{-4}\)</li> <li>Stage 2)<br> 10K iter.</li> <li>Stage 3)<br> 5K iter.</li> </ul> </li> <li>NTE Module : <ul> <li>Stage 1)<br> \(128^{2}\) resol. 140K iter. init lr \(4 \times 10^{-4}\)<br> \(\rightarrow\)<br> \(256^{2}\) resol. 60K iter. init lr \(2 \times 10^{-4}\)<br> \(\rightarrow\)<br> \(512^{2}\) resol. 30K iter. init lr \(1 \times 10^{-4}\)</li> <li>Stage 2)<br> 20K iter.</li> </ul> </li> </ul> </li> <li>Inference time : <ul> <li>BTimer : <ul> <li>20 ms for 4-view \(256^{2}\) recon.</li> <li>150 ms for 12-view \(256^{2}\) recon.</li> <li>4.2 s for 12-view \(512 \times 896\) recon.</li> </ul> </li> <li>NTE : <ul> <li>0.44 s for 4-view \(512 \times 896\) recon. w/o KV cache</li> </ul> </li> </ul> </li> </ul> </li> </ul> <h3 id="results">Results</h3> <ul> <li>Dynamic Novel-View-Synthesis (Quantitative) : <ul> <li>DyCheck Benchmark <d-cite key="22">[18]</d-cite> : <ul> <li>dataset :<br> DyCheck iPhone dataset (7 dynamic scenes by 3 synchronized cameras)</li> <li>baseline :<br> TiNeuVox, NSFF, T-NeRF, Nerfies, HyperNeRF, PGDVS, direct depth warp <ul> <li>BTimer는 per-scene optimization method에 competitive performance 달성</li> <li>BTimer는 consistent depth estimate 없이도 PGDVS보다 성능 좋음</li> </ul> </li> </ul> </li> <li>NVIDIA Dynamic Scene Benchmark <d-cite key="75">[19]</d-cite> : <ul> <li>dataset :<br> NVIDIA Dynamic Scene dataset (9 dynamic scenes by 12 forward-facing synchronized cameras)</li> <li>baseline :<br> HyperNeRF, DynNeRF, NSFF, RoDynRF, MonoNeRF, 4D-GS, Casual-FVS <ul> <li>feed-forward 방식이므로 optimization time 필요 없음</li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/6m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/6m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/6m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-01-10-BTimer/6m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Dynamic Novel-View-Synthesis (Qualitative) : <ul> <li>test on real-world scene 위해<br> DAVIS dataset의 monocular videos 이용하고,<br> customized pipeline으로 camera pose estimate해서 사용</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/8m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/8m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/8m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-01-10-BTimer/8m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/7m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/7m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/7m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-01-10-BTimer/7m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Static Novel-View-Synthesis : <ul> <li>RealEstate10K Benchmark : <ul> <li>baseline : pixelSplat, MVSplat, GPNR, GS-LRM</li> </ul> </li> <li>Tanks &amp; Temples Benchmark :<br> from InstantSplat Benchmark <ul> <li>baseline : GS-LRM (SOTA)</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/10m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/10m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/10m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-01-10-BTimer/10m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> single dataset(Ours-Static)보다 mixed-dataset(Ours-Full) 사용하는 게 generalization 및 성능 훨씬 좋음 </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/9m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/9m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/9m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-01-10-BTimer/9m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="ablation-study">Ablation Study</h3> <ul> <li>Ablation 1) Context Frames : <ul> <li>train할 때 context frames 더 많이 쓰면 3DGS prediction이 progressively 많아지므로 more complete scene recon. 가능</li> <li>inference할 때 서로 멀리 떨어진 context frames를 arbitrarily 골라서 커버하는 view 범위 넓힘</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/12m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/12m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/12m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-01-10-BTimer/12m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Ablation 2) Curriculum Training : <ul> <li>Stage 1)<br> single dataset 말고 multiple dataset 써야<br> geometry와 sharp detail 잡는 데 도움</li> <li>Stage 2)<br> static scene을 섞어서 co-train해야<br> geometry 및 rich detail 잡는 데 도움</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/11m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/11m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/11m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-01-10-BTimer/11m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Ablation 3) Interpolation Supervision : <ul> <li>temporal and multi-view consistency 챙기는 데 도움</li> <li>bullet timestamp가 input frames에 없을 때를 훈련하지 않으면<br> white-edge artifacts 생김<br> (interpolation loss를 cheat하려고 camera에 너무 가까운 3DGS를 만들기 때문)</li> </ul> </li> <li>Ablation 4) NTE Module : <ul> <li>motion이 빠르고 복잡할 때 도움<br> (ghosting artifacts 해소)</li> <li>BTimer 없이<br> 3D info. 쓰지 않는 NTE Module만으로 novel-view-synthesis 수행하면<br> input camera trajectory와 먼 novel-view에 대해서는 잘 recon. 못 함</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/13m.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/13m.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/13m.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-01-10-BTimer/13m.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="conclusion">Conclusion</h2> <ul> <li>Limitation : <ul> <li> <code class="language-plaintext highlighter-rouge">geometry</code> : <ul> <li>SOTA depth prediction model <d-cite key="71">[20]</d-cite> 만큼 정확하게<br> geometry (depth map)을 recover하지는 않음</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">memory issue</code> :<br> transformer를 사용하다보니 memory 많이 소요 <ul> <li>need 3 days on 64 A100 GPU (40GB VRAM)</li> <li>up to \(512 \times 904\) spatial resol.</li> <li>up to 12 context frames</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">pose</code> : <ul> <li>need camera pose param.</li> <li>future work :<br> DUSt3R, NoPoSplat처럼 pose-free일 순 없을까?</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">non-generative</code> : <ul> <li>본 논문은 feed-forward Transformer 모델이고<br> generative model이 아니기 때문에<br> cannot generate unseen region<br> (unseen view를 예측하는 view extrapolation 불가능)</li> <li>future work :<br> generative prior 사용하여 view extrapolation 수행</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">novelty</code> : <ul> <li>사실 arbitrary bullet timestamp를 input token에 추가한 뒤<br> 모든 input frames를 transformer에 때려넣고 원하는 bullet timestamp에서의 frame을 뽑아내는 video interpolation 방식으로 보이고,<br> 다만 차이점은 각 frame image를 transformer output으로 구하는 게 아니라 각 frame의 3DGS param.를 transformer output으로 구하는 것이고..<br> 모델 자체의 novelty보다는 implementation을 잘 해서 결과 좋게 낸 것 같다..</li> <li>(static, dynamic) data를 많이 쓰고 stage 별 training을 통해 높은 performance를 달성할 수 있었고<br> feed-forward 방식을 통해 빠른 속도를 달성</li> </ul> </li> </ul> </li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br> NTE Module이 마지막에 linear layer로 RGB value를 예측함으로써<br> pixel-space에서 RGB image at bullet timestamp 를 interpolate하고<br> 이를 BTimer에 사용하는데,<br> latent-space에서 interpolation 다룬 뒤 BTimer에 넘기면 더 성능 좋아질 수 있지 않을까?</p> </li> <li> <p>A1 :<br> TBD</p> </li> <li> <p>Q2 :<br> NTE Module이 예측한 pixel-space RGB image가 BTimer의 input으로 들어가는데,<br> NTE Module output이 부정확하면 drift 연쇄적으로 BTimer의 결과에도 악영향 미칠 거 같아.<br> refinement, uncertainty(confidence) 등으로 NTE Module output의 부정확성을 감소시켜 성능 높일 수 있을까?</p> </li> <li> <p>A2 :<br> TBD</p> </li> <li> <p>Q3 :<br> limitation 중에 unseen view는 recon.하지 못한다는 게 있는데 (view extrapolation 불가능)<br> 본 논문이 generalizability를 가진다는 말은<br> static and dynamic unseen dataset (scene)에 대응할 수 있어서인거지?</p> </li> <li> <p>A3 :<br> TB</p> </li> <li> <p>Q4 :<br> BTimer와 NTE Module을 각각 별도로 train하므로 not end-to-end인데<br> end-to-end training할 수는 없을까?</p> </li> <li> <p>A4 :<br> TBD</p> </li> <li> <p>Q5 :<br> pixelSplat에서는 a pair of images를 transformer의 input으로 넣어 둘의 관계를 파악하여 static scene recon.하는데<br> dynamic scene recon.에서는 motion 정보를 캡처해야 하기 때문에<br> transformer의 input으로 두 장이 아니라 여러 장의 image를 넣어주어야 하는거야?</p> </li> <li> <p>A5 :<br> TBD</p> </li> <li> <p>Q6 :<br> interpolation supervision으로 context (input) frame이 아닌 그 사이의 frame에 대해 rendering할 때 GT는 무엇으로 두나요?</p> </li> <li> <p>A6 :<br> context (input) frame의 image와 camera pose를 직접 interpolate하여 사용 <code class="language-plaintext highlighter-rouge">???</code></p> </li> <li> <p>Q7 :<br> 다른 논문들을 보면 4DGS처럼 canonical time에 대한 시간에 따른 Gaussian 변화량을 MLP로 학습하거나,<br> 또는 Dynamic Gaussian Marbles처럼 prev. frame의 GS가 next frame의 GS에 미치는 영향을 학습하기 위해 global adjustment해서 gaussian trajectory를 학습함으로써<br> GS끼리 정보를 주고받습니다.<br> 본 논문에서는 모든 input frames를 BTimer에 parallel하게 넣어준 뒤 bullet (target) timestamp마다 3DGS param.를 따로 뽑아내는데<br> 그럼 3DGS끼리는 정보를 공유하지 않는 건가요?</p> </li> <li> <p>A7 :<br> 네, 일단 BTimer 이 논문에서는 모든 input frames를 BTimer에 parallel하게 때려넣은 뒤 self-attention에 의존해서 t를 포함한 frames 간의 관계를 학습하는 것 같습니다.</p> </li> <li> <p>Q8 :<br> 어차피 3DGS끼리 정보를 공유하지 않는 거면 굳이 3DGS를 사용한 이유가 있나요?</p> </li> <li> <p>A8 :<br> novel-view-synthesis task에서 novel camera pose에 대한 image를 뽑아내려면 3D info.를 이용해야 recon.이 잘 될 것이기 때문에 3DGS를 이용합니다.<br> 논문에서 언급되어 있듯이 NTE Module만을 이용해서 from 2D to 2D로 novel-view-synthesis task를 수행하면 quality가 좋지 않았다고 합니다.</p> </li> <li> <p>Q9 :<br> camera pose의 영향을 많이 받을 것 같아요. 만약에 input frame 3에서 보였던 물체가 frame 밖을 벗어나거나 occlusion 때문에 input frame 4에서 안 보이게 되었을 때에도 잘 recon.하려면 prev. frame의 3D info. 정보를 결합해서 반영하는 식이어야 할 것 같은데, 각 bullet timestamp의 3D info.끼리 어떻게 relate되는지에 대한 내용이 없으니까 이와 같은 상황에 잘 대응할 수 있는지 궁금합니다.</p> </li> <li> <p>A9 :<br> TBD</p> </li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-01-10-BTimer.bib"></d-bibliography> <div id="disqus_thread" style="max-width: 800px; margin: 0 auto;"> <script type="text/javascript">var disqus_shortname="semyeong-yu",disqus_identifier="/blog/2025/BTimer",disqus_title="Feed Forward Bullet Time Reconstruction";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Semyeong Yu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5YMLX9VHFX"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5YMLX9VHFX");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>