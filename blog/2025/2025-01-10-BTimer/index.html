<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="feed-forward-bullet-time-reconstruction-of-dynamic-scenes-from-monocular-videos">Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos</h2> <h4 id="hanxue-liang-jiawei-ren-ashkan-mirzaei-antonio-torralba-ziwei-liu-igor-gilitschenski-sanja-fidler-cengiz-oztireli-huan-ling-zan-gojcic-jiahui-huang">Hanxue Liang, Jiawei Ren, Ashkan Mirzaei, Antonio Torralba, Ziwei Liu, Igor Gilitschenski, Sanja Fidler, Cengiz Oztireli, Huan Ling, Zan Gojcic, Jiahui Huang</h4> <blockquote> <p>paper :<br> <a href="https://arxiv.org/abs/2412.03526" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2412.03526</a><br> project website :<br> <a href="https://research.nvidia.com/labs/toronto-ai/bullet-timer/" rel="external nofollow noopener" target="_blank">https://research.nvidia.com/labs/toronto-ai/bullet-timer/</a></p> </blockquote> <h2 id="contribution">Contribution</h2> <ul> <li> <code class="language-plaintext highlighter-rouge">NTE Module</code> and <code class="language-plaintext highlighter-rouge">BulletTimer</code> : <ul> <li> <code class="language-plaintext highlighter-rouge">motion-aware</code> <code class="language-plaintext highlighter-rouge">feed-forward</code> model for real-time recon. and novel-view-synthesis of <code class="language-plaintext highlighter-rouge">dynamic</code> scenes</li> <li>recon. at given target (bullet) timestamp by aggregating info. from all the context frames</li> <li>obtain <code class="language-plaintext highlighter-rouge">scalability</code> and <code class="language-plaintext highlighter-rouge">generalization</code> by using both static and dynamic scene datasets</li> <li>recon. a bullet-time scene within 150ms with SOTA performance<br> comparable to optimization-based (per-scene) approaches</li> </ul> </li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li> <p>Dynamic scene recon. from monocular video :<br> still challenging<br> due to inherently ill-posed (해가 무수히 많음) nature of dynamic recon. from limited observations</p> </li> <li>Static scene recon. : <ul> <li>optimization-based (per-scene) :<br> NeRF, HyperNeRF</li> <li>learning-based (feed-forward) :<br> MonoNeRF, GS-LRM</li> </ul> </li> <li>Dynamic scene recon. :<br> dynamic scene은 complex motion 때문에 ambiguity 존재<br> 이를 해소하는 데 도움될 data prior 필요 <ul> <li>optimization-based (per-scene) : <ul> <li>use contraint (data prior) like depth and optical flow <d-cite key="33">[1]</d-cite>, <d-cite key="36">[2]</d-cite>, <d-cite key="37">[3]</d-cite>, <d-cite key="68">[4]</d-cite><br> \(\rightarrow\)<br> given data와 위의 data prior 간의 싱크를 맞추는 게 challenging <d-cite key="34">[5]</d-cite>, <d-cite key="63">[6]</d-cite> </li> <li>per-scene approach는 time-consuming and thus scale 어렵</li> </ul> </li> <li>learning-based (<code class="language-plaintext highlighter-rouge">feed-forward</code>) : <ul> <li>directly predict recon. in feed-forward manner<br> so, can <code class="language-plaintext highlighter-rouge">learn strong inherent prior directly from data</code> </li> <li>근데 dynamic scene 모델링하기 복잡하고 4D supervision data 부족해서 적용하는데 한계 있었음</li> <li>지금 시점 기준 L4GM이 유일한 feed-forward dynamic recon. model인데,<br> synthetic object-centric dataset으로 훈련돼서<br> fixed camera view-point와 multi-view supervision을 필요로 한다는 한계와<br> real-world scene에 generalize하기 어렵다는 한계가 있었음</li> </ul> </li> </ul> </li> </ul> <p>1 10 12 25 53 TBD</p> <h2 id="related-works">Related Works</h2> <p>TBD</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/2.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/2.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-12-23-Quark/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="overview">Overview</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/3.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/3.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-12-23-Quark/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="method">Method</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-12-23-Quark/4.PNG-480.webp 480w,/assets/img/2024-12-23-Quark/4.PNG-800.webp 800w,/assets/img/2024-12-23-Quark/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2024-12-23-Quark/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="method-1">Method</h2> <h3 id="btimer">BTimer</h3> <h3 id="nte-module">NTE Module</h3> <h3 id="curriculum-training">Curriculum Training</h3> <h2 id="experiment">Experiment</h2> <h2 id="ablation-study">Ablation Study</h2> <h2 id="conclusion">Conclusion</h2> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br> TBD</p> </li> <li> <p>A1 :<br> TBD</p> </li> </ul> </body></html>