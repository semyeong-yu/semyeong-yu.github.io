<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="feed-forward-bullet-time-reconstruction-of-dynamic-scenes-from-monocular-videos">Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos</h2> <h4 id="hanxue-liang-jiawei-ren-ashkan-mirzaei-antonio-torralba-ziwei-liu-igor-gilitschenski-sanja-fidler-cengiz-oztireli-huan-ling-zan-gojcic-jiahui-huang">Hanxue Liang, Jiawei Ren, Ashkan Mirzaei, Antonio Torralba, Ziwei Liu, Igor Gilitschenski, Sanja Fidler, Cengiz Oztireli, Huan Ling, Zan Gojcic, Jiahui Huang</h4> <blockquote> <p>paper :<br> <a href="https://arxiv.org/abs/2412.03526" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2412.03526</a><br> project website :<br> <a href="https://research.nvidia.com/labs/toronto-ai/bullet-timer/" rel="external nofollow noopener" target="_blank">https://research.nvidia.com/labs/toronto-ai/bullet-timer/</a></p> </blockquote> <h2 id="contribution">Contribution</h2> <ul> <li>model : <ul> <li> <code class="language-plaintext highlighter-rouge">motion-aware</code> <code class="language-plaintext highlighter-rouge">feed-forward</code> model for real-time recon. and novel-view-synthesis of <code class="language-plaintext highlighter-rouge">dynamic</code> scenes</li> <li>obtain <code class="language-plaintext highlighter-rouge">scalability</code> and <code class="language-plaintext highlighter-rouge">generalization</code> by using both static and dynamic scene datasets<br> (static and dynamic recon.에 모두 사용 가능)</li> <li>Procedure : <ul> <li>Step 1) pre-train on large static scene dataset</li> <li>Step 2) video duration or FPS에 구애받지 않고 scale effectively across datasets</li> <li>Step 3) output multi-view volumetric video representation</li> </ul> </li> <li>recon. a bullet-time scene within 150ms with SOTA performance on a single GPU<br> from 12 context frames of \(256 \times 256\) resolution</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/2.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/2.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-01-10-BTimer/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <code class="language-plaintext highlighter-rouge">BulletTimer</code> (Novelty 1.) :<br> main model <ul> <li>recon. at <code class="language-plaintext highlighter-rouge">arbitrary target (bullet) timestamp</code><br> by adding <code class="language-plaintext highlighter-rouge">bullet-time embedding</code> to all the context (input) frames<br> and <code class="language-plaintext highlighter-rouge">aggregating</code> pred. from all the context frames</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">NTE Module</code> (Novelty 2.) :<br> <code class="language-plaintext highlighter-rouge">pre-processing</code> (FPI) <ul> <li> <code class="language-plaintext highlighter-rouge">fast motion</code>에 대응하기 위해<br> model에 feed하기 전에<br> <code class="language-plaintext highlighter-rouge">intermediate (interpolated) frames를 predict</code> </li> <li>inference할 때<br> arbitrary target (bullet) timestamp에 대해<br> recon.할 수 있도록 도움</li> </ul> </li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li> <p>Dynamic scene recon. from monocular video :<br> still challenging<br> due to inherently ill-posed (해가 무수히 많음) nature of dynamic recon. from limited observations</p> </li> <li> <p>Static scene recon. :</p> <ul> <li>optimization-based (per-scene) :<br> NeRF, HyperNeRF</li> <li>learning-based (feed-forward) :<br> MonoNeRF, GS-LRM</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/5.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/5.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/5.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-01-10-BTimer/5.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Dynamic scene recon. :<br> dynamic scene은 complex motion 때문에 ambiguity 존재<br> 이를 해소하는 데 도움될 data prior 필요 <ul> <li>optimization-based (per-scene) : <ul> <li>use contraint (data prior) like depth and optical flow <d-cite key="33">[1]</d-cite>, <d-cite key="36">[2]</d-cite>, <d-cite key="37">[3]</d-cite>, <d-cite key="68">[4]</d-cite><br> \(\rightarrow\)<br> given data와 위의 data prior 간의 싱크를 맞추는 게 challenging <d-cite key="34">[5]</d-cite>, <d-cite key="63">[6]</d-cite> </li> <li>per-scene approach는 time-consuming and thus scale 어렵</li> </ul> </li> <li>learning-based (<code class="language-plaintext highlighter-rouge">feed-forward</code>) : <ul> <li>directly predict recon. in feed-forward manner<br> so, can <code class="language-plaintext highlighter-rouge">learn strong inherent prior directly from data</code> <d-cite key="7">[7]</d-cite>, <d-cite key="10">[8]</d-cite>, <d-cite key="12">[9]</d-cite>, <d-cite key="25">[10]</d-cite>, <d-cite key="53">[11]</d-cite> </li> <li>근데 dynamic scene 모델링하기 복잡하고 4D supervision data 부족해서 적용하는데 한계 있었음</li> <li>지금 시점 기준 L4GM <d-cite key="53">[11]</d-cite> 이 유일한 feed-forward dynamic recon. model인데,<br> synthetic object-centric dataset으로 훈련돼서<br> fixed camera view-point와 multi-view supervision을 필요로 한다는 한계와<br> real-world scene에 generalize하기 어렵다는 한계가 있었음</li> </ul> </li> </ul> </li> <li>Feed-Forward Dynamic scene recon. : <ul> <li>본 논문은<br> <code class="language-plaintext highlighter-rouge">pixel-aligned 3DGS</code> <d-cite key="79">[12]</d-cite> 를 기반으로<br> novel BulletTimer and NTE module 제안</li> </ul> </li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li>Dynamic 3D Representation : <ul> <li>TBD</li> </ul> </li> <li>Novel-View-Synthesis : <ul> <li>TBD</li> </ul> </li> <li>Feed-Forward Reconstruction : <ul> <li>TBD</li> </ul> </li> </ul> <h2 id="overview">Overview</h2> <ul> <li> <p>notation :<br> context frames \(I_{c} \subset I\)<br> camera poses \(P_{c} \subset P\)<br> context timestamps \(T_{c} \subset T\)<br> bullet timestamp \(t_{b} \in [\text{min}(T_{c}), \text{max}(T_{c})]\)<br> recon. at timestamp \(t \notin T\) by NTE module</p> </li> <li> <p>Architecture :</p> <ul> <li>Training :<br> BTimer와 NTE Module을 별도로 각각 train<br> (not end-to-end)</li> <li>Inference :<br> NTE Module로 pre-process한 뒤<br> BTimer 사용</li> </ul> </li> </ul> <h2 id="method">Method</h2> <h3 id="btimer-bullet-timer">BTimer (Bullet Timer)</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/3.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/3.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-01-10-BTimer/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Model Design : <ul> <li>encode (input) :<br> \(i\)-th frame \(I_{i} \in I_{c}\) 을 \(8 \times 8\) 짜리 patches로 나눈 뒤<br> \(j\)-th patch에 대해<br> per-patch input token \(f_{ij} \|_{j=1}^{HW / 64} = f_{ij}^{rgb} + f_{ij}^{pose} + f_{i}^{time}\) 만든 뒤<br> concatenate input tokens from all context frames<br> and feed into Transformer <ul> <li> <code class="language-plaintext highlighter-rouge">image</code> encoder :<br> GS-LRM <d-cite key="79">[12]</d-cite> 에서 영감을 받아,<br> <code class="language-plaintext highlighter-rouge">ViT</code> model을 backbone으로 사용</li> <li> <code class="language-plaintext highlighter-rouge">camera pose</code> encoder :<br> <code class="language-plaintext highlighter-rouge">camera Plucker embedding</code> <d-cite key="70">[13]</d-cite> </li> <li> <code class="language-plaintext highlighter-rouge">time</code> encoder :<br> PE (Positional Encoding) 및 linear layer를 거쳐<br> \(t_{i}\) 와 \(t_{b}\) 를 각각 \(f_{i}^{ctx}\) 와 \(f_{i}^{bullet}\) 으로 encode한 뒤<br> \(f_{i}^{time} = f_{i}^{ctx} + f_{i}^{bullet}\) <ul> <li> <code class="language-plaintext highlighter-rouge">context (input)</code> timestamp \(t_{i}\) from context (input) frame \(I_{i}\)</li> <li> <code class="language-plaintext highlighter-rouge">bullet (target)</code> timestamp \(t_{b}\) that is <code class="language-plaintext highlighter-rouge">shared</code> across context (input) frames</li> </ul> </li> </ul> </li> <li>decode (output) :<br> transformer의 per-patch output token \(f_{ij}^{out}\) 을<br> <code class="language-plaintext highlighter-rouge">per-patch 3DGS param. at bullet timestamp</code> \(G_{ij} \in R^{8 \times 8 \times 12}\) 로 regression <ul> <li>each Gaussian has 12 param. as color \(c \in R^{3}\), scale \(s \in R^{3}\), rotation unit quaternion \(q \in R^{4}\), opacity \(\sigma \in R\), and ray distance \(\tau \in R\)</li> <li>3D position is obtained by pixel-aligned unprojection \(\mu = o + \tau d\)<br> (\(o\) and \(d\) are obtained from camera pose \(P_{i}\))</li> </ul> </li> </ul> </li> <li> <p>Loss :<br> \(L_{RGB} = L_{MSE} + \lambda L_{LPIPS}\) with \(\lambda = 0.5\)</p> </li> <li>Timestamp :<br> context (input) frames와 bullet (target supervision) frame 을 잘 고르는 게 중요 <ul> <li> <code class="language-plaintext highlighter-rouge">In-context Supervision</code> : <ul> <li>bullet timestamp is randomly selected from context frames<br> \(t_{b} \in T_{c}\)</li> <li>model이 context timestamp에 대해 정확히 recon. 가능하도록</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">Interpolation Supervision</code> : <ul> <li>bullet timestamp lies between two adjacent context frames<br> \(t_{b} \notin T_{c}\)</li> <li>model이 dynamic parts를 interpolate할 수 있도록</li> <li>pixel-aligned 3DGS의 inductive bias 때문에<br> motion이 복잡하고 빠를 때 intermediate timestamp에 대해 예측 잘 못 함<br> \(\rightarrow\)<br> 먼저 NTE module의 도움을 받아 pre-process한 뒤<br> BTimer 사용</li> <li>local minimum 방지 및 view 간 consistency 상승</li> </ul> </li> </ul> </li> <li>Inference : <ul> <li>bullet (target) timestamp \(t_{b}\) 를 원하는 each timestamp in video로 설정하면<br> full video를 recon.할 수 있으므로<br> 각 frame을 <code class="language-plaintext highlighter-rouge">parallel</code>하게 recon. 가능</li> <li> <code class="language-plaintext highlighter-rouge">???</code><br> For a video longer than the number of training context views \(\| I_{c} \|\),<br> at timestamp \(t\), apart from including this exact timestamp and setting \(t_{b} = t\),<br> we uniformly distribute the remaining \(\| I_{c} \| − 1\) required context frames across the whole duration of the video<br> to form the input batch with \(\| I_{c} \|\) frames</li> </ul> </li> </ul> <h3 id="nte-module-novel-time-enhancer">NTE Module (Novel Time Enhancer)</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/4.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/4.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-01-10-BTimer/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>NTE Module Design :<br> decoder-only LVSM <d-cite key="27">[13]</d-cite> 에서 영감을 받아,<br> BTimer model과 구조 똑같지만,<br> I/O가 다름 <ul> <li>input : <ul> <li>context frame : <ul> <li>context (input) timestamp embedding<br> (BTimer model과 달리 bullet timestamp embedding은 안 넣음)</li> <li>camera pose Plucker embedding</li> <li>context (input) frame</li> </ul> </li> <li>intermediate frame : <ul> <li>bullet (target) timestamp embedding</li> <li>target camera pose Plucker embedding</li> </ul> </li> </ul> </li> <li>output : <ul> <li>Transformer의 per-patch output tokens 중 target token을<br> unpatchify and project directly to RGB values by linear layer<br> \(\rightarrow\)<br> RGB frame for any bullet (target) timestamp<br> (this RGB frame은 NTE module network의 direct output이고, 3DGS로 rendering한 게 아님!!)<br> \(\rightarrow\)<br> NTE Module의 output은<br> BTimer에서 bullet timestamp의 image로 쓰임</li> </ul> </li> <li>Implementation : <ul> <li>LVSM <d-cite key="27">[13]</d-cite> 에서처럼<br> 안정적인 훈련을 위해 <code class="language-plaintext highlighter-rouge">QK-norm</code> 사용 <code class="language-plaintext highlighter-rouge">???</code> </li> <li>target token에 attention할 수 있도록<br> <code class="language-plaintext highlighter-rouge">masked attention</code> 사용</li> <li> <d-cite key="50">[14]</d-cite> <p>에서처럼<br> 빠른 inference를 위해 <code class="language-plaintext highlighter-rouge">KV-Cache</code> <code class="language-plaintext highlighter-rouge">???</code> 사용</p> </li> <li>NTE Module has negligible overhead on runtime</li> </ul> </li> </ul> </li> <li> <p>Loss :<br> \(L_{RGB} = L_{MSE} + \lambda L_{LPIPS}\) with \(\lambda = 0.5\)</p> </li> <li>BTimer and NTE Module : <ul> <li>NTE Module로 직접 RGB image 예측하여<br> novel-view-synthesis 할 수 있긴 한데, 그럼 성능 안 좋음</li> <li>feed-forward transformer (NTE Module)로 FPI pre-process한 뒤<br> <code class="language-plaintext highlighter-rouge">feed-forward transformer</code> (BTimer)로 data info. 포착하여<br> <code class="language-plaintext highlighter-rouge">3DGS param. 예측</code>한 뒤<br> 3DGS rasterization으로 novel-view-synthesis</li> </ul> </li> </ul> <h3 id="curriculum-training-at-scale">Curriculum Training at Scale</h3> <ul> <li>Generalizability : <ul> <li>data 다양성이 많을수록 model generalizability가 높아짐<br> static dataset은 많이 존재하고<br> dynamic dataset은 적게 존재하지만 motion awareness 및 temporal consistency 확보 가능</li> <li>본 논문의 model인 BTimer는<br> generalizable to both static and dynamic scenes <ul> <li>static scene : equalize all \(t_{b}\)</li> <li>dynamic scene : recon. at arbitrary bullet \(t_{b}\)</li> <li>different domain에서는 different model 필요로 하는<br> GS-LRM <d-cite key="79">[12]</d-cite> or MVSplat <d-cite key="10">[8]</d-cite> 과는 다름</li> </ul> </li> </ul> </li> <li>Curriculum Training : <ul> <li>Stage 1) <code class="language-plaintext highlighter-rouge">Low-res to High-res Static Pretraining</code> <ul> <li>static dataset으로 pre-train <ul> <li>both synthetic and real-world</li> <li>390K training samples</li> <li>normalize datasets to be bounded in \(10^{3}\) cube</li> <li>종류 : <ul> <li>Objaverse</li> <li>RE10K</li> <li>MVImgNet</li> <li>DL3DV</li> </ul> </li> </ul> </li> <li>no time embedding<br> (static scene이니까)</li> <li>data distribution이 복잡하기 때문에<br> coarse 세팅 (low-resol.(\(128 \times 128\)) and few-view(\(\| I_{c} \| = 4\)))에서 시작해서<br> 점점 fine 세팅 (high-resol.(\(256 \times 256 \rightarrow 512 \times 512\)))으로 train</li> </ul> </li> <li>Stage 2) <code class="language-plaintext highlighter-rouge">Dynamic Scene Co-training</code> <ul> <li>dynamic dataset으로 fine-tuning <ul> <li>종류 : <ul> <li>Kubric</li> <li>PointOdyssey</li> <li>DynamicReplica</li> <li>Spring</li> </ul> </li> </ul> </li> <li>4D dynamic dataset이 부족하기 때문에<br> 안정적인 훈련을 위해<br> static dataset을 함께 사용하여 co-training</li> <li>Internet video로부터 camera pose를 매기는 customized pipeline 구축하여<br> real-world data에 대한 robustness 향상 <ul> <li>먼저 PANDA-70M dataset에서 random select한 video를 20s 길이의 clips로 자르기</li> <li>SAM으로 video의 dynamic objects를 mask out</li> <li>DROID-SLAM으로 video camera pose를 estimate</li> <li>reprojection error 측정하여 low-quality의 video 및 pose는 필터링</li> <li>최종적으로 obtain 40K clips with high-quality camera trajectories</li> </ul> </li> </ul> </li> <li>Stage 3) <code class="language-plaintext highlighter-rouge">Long-context Window Fine-tuning</code> <ul> <li>NTE Module 말고 BTimer model에만 적용</li> <li>context (input) image 수를<br> \(\| I_{c} \| = 4\) 에서 \(\| I_{c} \| = 12\) 로 늘려서<br> long video recon.하는 데 도움</li> </ul> </li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <ul> <li>Implementation : <ul> <li>TBD</li> </ul> </li> <li>Results on Dynamic Novel-View-Synthesis : <ul> <li>Quantitative :<br> TBD</li> <li>Qualitative :<br> TBD</li> </ul> </li> <li>Compatibility with Static Novel-View-Synthesis : <ul> <li>TBD</li> </ul> </li> </ul> <h2 id="ablation-study">Ablation Study</h2> <ul> <li>Ablation 1) Context Frames : <ul> <li>TBD</li> </ul> </li> <li>Ablation 2) Curriculum Training : <ul> <li>TBD</li> </ul> </li> <li>Ablation 3) Interpolation Supervision : <ul> <li>TBD</li> </ul> </li> <li>Ablation 4) NTE Module : <ul> <li>TBD</li> </ul> </li> </ul> <h2 id="conclusion">Conclusion</h2> <ul> <li>Limitation : <ul> <li> <code class="language-plaintext highlighter-rouge">memory issue</code> : <ul> <li>need 3 days on 64 A100 GPU (40GB VRAM)</li> <li>up to \(512 \times 904\) spatial resol.</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">pose</code> : <ul> <li>need camera pose param.</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">non-generative</code> : <ul> <li>본 논문은 feed-forward Transformer 모델이고<br> generative model이 아니기 때문에<br> cannot generate unseen region</li> </ul> </li> </ul> </li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br> NTE Module이 마지막에 linear layer로 RGB value를 예측함으로써<br> pixel-space에서 RGB image at bullet timestamp 를 interpolate하고<br> 이를 BTimer에 사용하는데,<br> latent-space에서 interpolation 다룬 뒤 BTimer에 넘기면 더 성능 좋아질 수 있지 않을까?</p> </li> <li> <p>A1 :<br> TBD</p> </li> <li> <p>Q2 :<br> NTE Module이 예측한 pixel-space RGB image가 BTimer의 input으로 들어가는데,<br> NTE Module output이 부정확하면 drift 연쇄적으로 BTimer의 결과에도 악영향 미칠 듯<br> refinement, uncertainty 등으로 NTE Module output의 부정확성을 감소시켜 성능 높일 수 있을까?</p> </li> <li> <p>A2 :<br> TBD</p> </li> <li> <p>Q3 :<br> limitation 중에 unseen region은 recon.하지 못한다는 게 있는데<br> 본 논문이 generalizability를 가진다는 말은<br> static and dynamic unseen dataset (scene)에 대응할 수 있어서인거지?</p> </li> <li> <p>A3 :<br> TB</p> </li> <li> <p>Q4 :<br> BTimer와 NTE Module을 각각 별도로 train하므로 not end-to-end인데<br> end-to-end training할 수는 없을까?</p> </li> <li> <p>A4 :<br> TBD</p> </li> <li> <p>Q5 :<br> Pixelsplat에서는 a pair of images를 transformer의 input으로 넣어 둘의 관계를 파악하여 static scene recon.하는데<br> dynamic scene recon.에서는 motion 정보를 캡처해야 하기 때문에<br> transformer의 input으로 두 장이 아니라 여러 장의 image를 넣어주어야 하는거야?</p> </li> <li> <p>A5 :<br> TBD</p> </li> </ul> </body></html>