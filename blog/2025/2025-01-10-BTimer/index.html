<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="feed-forward-bullet-time-reconstruction-of-dynamic-scenes-from-monocular-videos">Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos</h2> <h4 id="hanxue-liang-jiawei-ren-ashkan-mirzaei-antonio-torralba-ziwei-liu-igor-gilitschenski-sanja-fidler-cengiz-oztireli-huan-ling-zan-gojcic-jiahui-huang">Hanxue Liang, Jiawei Ren, Ashkan Mirzaei, Antonio Torralba, Ziwei Liu, Igor Gilitschenski, Sanja Fidler, Cengiz Oztireli, Huan Ling, Zan Gojcic, Jiahui Huang</h4> <blockquote> <p>paper :<br> <a href="https://arxiv.org/abs/2412.03526" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2412.03526</a><br> project website :<br> <a href="https://research.nvidia.com/labs/toronto-ai/bullet-timer/" rel="external nofollow noopener" target="_blank">https://research.nvidia.com/labs/toronto-ai/bullet-timer/</a></p> </blockquote> <h2 id="contribution">Contribution</h2> <ul> <li>model : <ul> <li> <code class="language-plaintext highlighter-rouge">motion-aware</code> <code class="language-plaintext highlighter-rouge">feed-forward</code> model for real-time recon. and novel-view-synthesis of <code class="language-plaintext highlighter-rouge">dynamic</code> scenes</li> <li>obtain <code class="language-plaintext highlighter-rouge">scalability</code> and <code class="language-plaintext highlighter-rouge">generalization</code> by using both static and dynamic scene datasets<br> (static and dynamic recon.에 모두 사용 가능)</li> <li>Procedure : <ul> <li>Step 1) pre-train on large static scene dataset</li> <li>Step 2) video duration or FPS에 구애받지 않고 scale effectively across datasets</li> <li>Step 3) output multi-view volumetric video representation</li> </ul> </li> <li>recon. a bullet-time scene within 150ms with SOTA performance on a single GPU<br> from 12 context frames of \(256 \times 256\) resolution</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/2.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/2.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-01-10-BTimer/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <code class="language-plaintext highlighter-rouge">BulletTimer</code> (Novelty 1.) :<br> main model <ul> <li>recon. at <code class="language-plaintext highlighter-rouge">arbitrary target (bullet) timestamp</code><br> by adding <code class="language-plaintext highlighter-rouge">bullet-time embedding</code> to all the context (input) frames<br> and <code class="language-plaintext highlighter-rouge">aggregating</code> pred. from all the context frames</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">NTE Module</code> (Novelty 2.) :<br> <code class="language-plaintext highlighter-rouge">pre-processing</code> (FPI) <ul> <li> <code class="language-plaintext highlighter-rouge">fast motion</code>에 대응하기 위해<br> model에 feed하기 전에<br> <code class="language-plaintext highlighter-rouge">intermediate (interpolated) frames를 predict</code> </li> <li>inference할 때<br> arbitrary target (bullet) timestamp에 대해<br> recon.할 수 있도록 도움</li> </ul> </li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li> <p>Dynamic scene recon. from monocular video :<br> still challenging<br> due to inherently ill-posed (해가 무수히 많음) nature of dynamic recon. from limited observations</p> </li> <li>Static scene recon. : <ul> <li>optimization-based (per-scene) :<br> NeRF, HyperNeRF</li> <li>learning-based (feed-forward) :<br> MonoNeRF, GS-LRM</li> </ul> </li> <li>Dynamic scene recon. :<br> dynamic scene은 complex motion 때문에 ambiguity 존재<br> 이를 해소하는 데 도움될 data prior 필요 <ul> <li>optimization-based (per-scene) : <ul> <li>use contraint (data prior) like depth and optical flow <d-cite key="33">[1]</d-cite>, <d-cite key="36">[2]</d-cite>, <d-cite key="37">[3]</d-cite>, <d-cite key="68">[4]</d-cite><br> \(\rightarrow\)<br> given data와 위의 data prior 간의 싱크를 맞추는 게 challenging <d-cite key="34">[5]</d-cite>, <d-cite key="63">[6]</d-cite> </li> <li>per-scene approach는 time-consuming and thus scale 어렵</li> </ul> </li> <li>learning-based (<code class="language-plaintext highlighter-rouge">feed-forward</code>) : <ul> <li>directly predict recon. in feed-forward manner<br> so, can <code class="language-plaintext highlighter-rouge">learn strong inherent prior directly from data</code> <d-cite key="7">[7]</d-cite>, <d-cite key="10">[8]</d-cite>, <d-cite key="12">[9]</d-cite>, <d-cite key="25">[10]</d-cite>, <d-cite key="53">[11]</d-cite> </li> <li>근데 dynamic scene 모델링하기 복잡하고 4D supervision data 부족해서 적용하는데 한계 있었음</li> <li>지금 시점 기준 L4GM <d-cite key="53">[11]</d-cite> 이 유일한 feed-forward dynamic recon. model인데,<br> synthetic object-centric dataset으로 훈련돼서<br> fixed camera view-point와 multi-view supervision을 필요로 한다는 한계와<br> real-world scene에 generalize하기 어렵다는 한계가 있었음</li> </ul> </li> </ul> </li> <li>Feed-Forward Dynamic scene recon. : <ul> <li>본 논문은<br> <code class="language-plaintext highlighter-rouge">pixel-aligned 3DGS</code> <d-cite key="79">[12]</d-cite> 를 기반으로<br> novel BulletTimer and NTE module 제안</li> </ul> </li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li>Dynamic 3D Representation : <ul> <li>TBD</li> </ul> </li> <li>Novel-View-Synthesis : <ul> <li>TBD</li> </ul> </li> <li>Feed-Forward Reconstruction : <ul> <li>TBD</li> </ul> </li> </ul> <h2 id="overview">Overview</h2> <ul> <li>notation :<br> context frames \(I_{c} \subset I\)<br> camera poses \(P_{c} \subset P\)<br> context timestamps \(T_{c} \subset T\)<br> bullet timestamp \(t_{b} \in [\text{min}(T_{c}), \text{max}(T_{c})]\)<br> recon. at timestamp \(t \notin T\) by NTE module</li> </ul> <h2 id="method">Method</h2> <h3 id="btimer-bullet-timer">BTimer (Bullet Timer)</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/3.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/3.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-01-10-BTimer/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Model Design : <ul> <li>encode (input) :<br> \(i\)-th frame \(I_{i} \in I_{c}\) 을 \(8 \times 8\) 짜리 patches로 나눈 뒤<br> \(j\)-th patch에 대해<br> per-patch input token \(f_{ij} \|_{j=1}^{HW / 64} = f_{ij}^{rgb} + f_{ij}^{pose} + f_{i}^{time}\) 만든 뒤<br> concatenate input tokens from all context frames<br> and feed into Transformer <ul> <li> <code class="language-plaintext highlighter-rouge">image</code> encoder :<br> GS-LRM <d-cite key="79">[12]</d-cite> 에서 영감을 받아,<br> <code class="language-plaintext highlighter-rouge">ViT</code> model을 backbone으로 사용</li> <li> <code class="language-plaintext highlighter-rouge">camera pose</code> encoder :<br> <code class="language-plaintext highlighter-rouge">camera Plucker embedding</code> <d-cite key="70">[13]</d-cite> </li> <li> <code class="language-plaintext highlighter-rouge">time</code> encoder :<br> PE (Positional Encoding) 및 linear layer를 거쳐<br> \(t_{i}\) 와 \(t_{b}\) 를 각각 \(f_{i}^{ctx}\) 와 \(f_{i}^{bullet}\) 으로 encode한 뒤<br> \(f_{i}^{time} = f_{i}^{ctx} + f_{i}^{bullet}\) <ul> <li> <code class="language-plaintext highlighter-rouge">context (input)</code> timestamp \(t_{i}\) from context (input) frame \(I_{i}\)</li> <li> <code class="language-plaintext highlighter-rouge">bullet (target)</code> timestamp \(t_{b}\) that is <code class="language-plaintext highlighter-rouge">shared</code> across context (input) frames</li> </ul> </li> </ul> </li> <li>decode (output) :<br> transformer의 per-patch output token \(f_{ij}^{out}\) 을<br> <code class="language-plaintext highlighter-rouge">per-patch 3DGS param. at bullet timestamp</code> \(G_{ij} \in R^{8 \times 8 \times 12}\) 로 decode <ul> <li>each Gaussian has 12 param. as color \(c \in R^{3}\), scale \(s \in R^{3}\), rotation unit quaternion \(q \in R^{4}\), opacity \(\sigma \in R\), and ray distance \(\tau \in R\)</li> <li>3D position is obtained by pixel-aligned unprojection \(\mu = o + \tau d\)<br> (\(o\) and \(d\) are obtained from camera pose \(P_{i}\))</li> </ul> </li> </ul> </li> <li> <p>Loss :<br> \(L_{RGB} = L_{MSE} + \lambda L_{LPIPS}\) with \(\lambda = 0.5\)</p> </li> <li>Timestamp :<br> context (input) frames와 bullet (target supervision) frame 을 잘 고르는 게 중요 <ul> <li> <code class="language-plaintext highlighter-rouge">In-context Supervision</code> : <ul> <li>bullet timestamp is randomly selected from context frames<br> \(t_{b} \in T_{c}\)</li> <li>model이 context timestamp에 대해 정확히 recon. 가능하도록</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">Interpolation Supervision</code> : <ul> <li>bullet timestamp lies between two adjacent context frames<br> \(t_{b} \notin T_{c}\)</li> <li>model이 dynamic parts를 interpolate할 수 있도록</li> <li>pixel-aligned 3DGS의 inductive bias 때문에<br> motion이 복잡하고 빠를 때 intermediate timestamp에 대해 예측 잘 못 함<br> \(\rightarrow\)<br> 먼저 NTE module의 도움을 받아 pre-process한 뒤<br> BTimer 사용</li> </ul> </li> </ul> </li> <li>Inference : <ul> <li>bullet (target) timestamp \(t_{b}\) 를 원하는 each timestamp in video로 설정하면<br> full video를 recon.할 수 있으므로<br> 각 frame을 <code class="language-plaintext highlighter-rouge">parallel</code>하게 recon. 가능</li> <li> <code class="language-plaintext highlighter-rouge">???</code><br> For a video longer than the number of training context views \(\| I_{c} \|\),<br> at timestamp \(t\), apart from including this exact timestamp and setting \(t_{b} = t\),<br> we uniformly distribute the remaining \(\| I_{c} \| − 1\) required context frames across the whole duration of the video<br> to form the input batch with \(\| I_{c} \|\) frames</li> </ul> </li> </ul> <h3 id="nte-module-novel-time-enhancer">NTE Module (Novel Time Enhancer)</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-01-10-BTimer/4.PNG-480.webp 480w,/assets/img/2025-01-10-BTimer/4.PNG-800.webp 800w,/assets/img/2025-01-10-BTimer/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-01-10-BTimer/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>NTE Module Design :<br> decoder-only LVSM <d-cite key="27">[13]</d-cite> 에서 영감을 받아,<br> BTimer model과 구조 똑같지만,<br> I/O가 다름 <ul> <li>input : <ul> <li>context frame : <ul> <li>context (input) timestamp embedding<br> (BTimer model과 달리 bullet timestamp embedding은 안 넣음)</li> <li>camera pose Plucker embedding</li> <li>context (input) frame</li> </ul> </li> <li>intermediate frame : <ul> <li>bullet (target) timestamp embedding</li> <li>target camera pose Plucker embedding</li> </ul> </li> </ul> </li> <li>output : <ul> <li>Transformer의 per-patch output tokens 중 target token을<br> unpatchify and project directly to RGB values by linear layer<br> \(\rightarrow\)<br> RGB frame for any bullet (target) timestamp<br> (this RGB frame은 NTE module network의 direct output이고, 3DGS로 rendering한 게 아님!!)<br> \(\rightarrow\)<br> NTE Module의 output은<br> BTimer에서 bullet timestamp의 image로 쓰임</li> </ul> </li> <li>Implementation : <ul> <li>LVSM <d-cite key="27">[13]</d-cite> 에서처럼<br> 안정적인 훈련을 위해 <code class="language-plaintext highlighter-rouge">QK-norm</code> 사용 <code class="language-plaintext highlighter-rouge">???</code> </li> <li>target token에 attention할 수 있도록<br> <code class="language-plaintext highlighter-rouge">masked attention</code> 사용</li> <li> <d-cite key="50">[14]</d-cite> <p>에서처럼<br> 빠른 inference를 위해 <code class="language-plaintext highlighter-rouge">KV-Cache</code> <code class="language-plaintext highlighter-rouge">???</code> 사용</p> </li> <li>NTE Module has negligible overhead on runtime</li> </ul> </li> </ul> </li> <li> <p>Loss :<br> \(L_{RGB} = L_{MSE} + \lambda L_{LPIPS}\) with \(\lambda = 0.5\)</p> </li> <li>BTimer and NTE Module : <ul> <li>NTE Module로 직접 RGB image 예측하여<br> novel-view-synthesis 할 수 있긴 한데, 그럼 성능 안 좋음</li> <li>feed-forward transformer (NTE Module)로 FPI pre-process한 뒤<br> <code class="language-plaintext highlighter-rouge">feed-forward transformer</code> (BTimer)로 data info. 포착하여<br> <code class="language-plaintext highlighter-rouge">3DGS param. 예측</code>한 뒤<br> 3DGS rasterization으로 novel-view-synthesis</li> </ul> </li> </ul> <h3 id="curriculum-training-at-scale">Curriculum Training at Scale</h3> <p>motion awareness 및 temporal consistency 보장하기 위해<br> both static and dynamic scene dataset를 포함하는 learning curriculum 설계</p> <ul> <li>TBD</li> </ul> <h2 id="experiment">Experiment</h2> <h2 id="ablation-study">Ablation Study</h2> <h2 id="conclusion">Conclusion</h2> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br> NTE Module이 마지막에 linear layer로 RGB value를 예측함으로써<br> pixel-space에서 RGB image at bullet timestamp 를 예측하는데,<br> latent-space에서 interpolation 다루면 더 성능 좋아질 수 있지 않을까?</p> </li> <li> <p>A1 :<br> TBD</p> </li> <li> <p>Q2 :<br> NTE Module이 예측한 pixel-space RGB image가 BTimer의 input으로 들어가는데,<br> NTE Module output이 부정확하면 drift 연쇄적으로 BTimer의 결과에도 악영향 미칠 듯<br> refinement, uncertainty 등으로 NTE Module output의 부정확성을 감소시켜 성능 높일 수 있을까?</p> </li> <li> <p>A2 :<br> TBD</p> </li> </ul> </body></html>