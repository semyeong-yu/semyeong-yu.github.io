<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> SplineGS | Semyeong Yu </title> <meta name="author" content="Semyeong Yu"> <meta name="description" content="Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://semyeong-yu.github.io/blog/2025/SplineGS/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "SplineGS",
            "description": "Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video",
            "published": "February 06, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Semyeong</span> Yu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>SplineGS</h1> <p>Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#contribution">Contribution</a> </div> <div> <a href="#related-works">Related Works</a> </div> <div> <a href="#method">Method</a> </div> <ul> <li> <a href="#architecture">Architecture</a> </li> <li> <a href="#motion-adaptive-spline-for-3dgs">Motion-Adaptive Spline for 3DGS</a> </li> <li> <a href="#camera-pose-estimation">Camera Pose Estimation</a> </li> <li> <a href="#loss">Loss</a> </li> </ul> <div> <a href="#experiment">Experiment</a> </div> <ul> <li> <a href="#implementation">Implementation</a> </li> <li> <a href="#result">Result</a> </li> <li> <a href="#ablation-study">Ablation Study</a> </li> </ul> <div> <a href="#conclusion">Conclusion</a> </div> <div> <a href="#question">Question</a> </div> </nav> </d-contents> <h2 id="splinegs---robust-motion-adaptive-spline-for-real-time-dynamic-3d-gaussians-from-monocular-video">SplineGS - Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video</h2> <h4 id="jongmin-park-minh-quan-viet-bui-juan-luis-gonzalez-bello-jaeho-moon-jihyong-oh-munchurl-kim">Jongmin Park, Minh-Quan Viet Bui, Juan Luis Gonzalez Bello, Jaeho Moon, Jihyong Oh, Munchurl Kim</h4> <blockquote> <p>paper :<br> <a href="https://arxiv.org/abs/2412.09982" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2412.09982</a><br> project website :<br> <a href="https://kaist-viclab.github.io/splinegs-site/" rel="external nofollow noopener" target="_blank">https://kaist-viclab.github.io/splinegs-site/</a></p> </blockquote> <h2 id="contribution">Contribution</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/1.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/1.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/1.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-02-06-SplineGS/1.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>summary : <ul> <li>per-Scene model (maybe <code class="language-plaintext highlighter-rouge">???</code>)</li> <li> <code class="language-plaintext highlighter-rouge">COLMAP-free</code> novel-view-synthesis for <code class="language-plaintext highlighter-rouge">dynamic</code> scenes from <code class="language-plaintext highlighter-rouge">in-the-wild monocular</code> videos, <code class="language-plaintext highlighter-rouge">thousands</code> time faster than SOTA<br> by applying <code class="language-plaintext highlighter-rouge">spline-based</code> model to dynamic <code class="language-plaintext highlighter-rouge">3DGS trajectories</code> </li> </ul> </li> <li>novelty : <ul> <li>Motion-Adaptive Spline (MAS) :<br> continuous dynamic <code class="language-plaintext highlighter-rouge">3DGS trajectories</code> (deformation) 을 효율적으로 모델링하기 위해<br> <code class="language-plaintext highlighter-rouge">cubic Hermite splines</code> with a small number of control points 사용 <ul> <li>control point : <ul> <li>learnable param.</li> <li>determines each piecewise cubic func.’s curvature and direction</li> </ul> </li> </ul> </li> <li>Motion-Adaptive Control points Pruning (MACP) :<br> quality, efficiency 모두 챙기기 위해 계속 <code class="language-plaintext highlighter-rouge">control points를 prune</code>하여 수 조절</li> <li>joint optimization strategy :<br> <code class="language-plaintext highlighter-rouge">photometric and geometric consistency</code> 이용해서<br> (external estimators 필요 X)<br> <code class="language-plaintext highlighter-rouge">camera param.</code> 와 <code class="language-plaintext highlighter-rouge">3DGS param.</code>를 jointly optimize<br> (COLMAP-free!)</li> </ul> </li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li>dynamic novel-view-synthesis : <ul> <li>implicit representation <code class="language-plaintext highlighter-rouge">(MLP) 이용하여 deformation 모델링</code> in canonical space <d-cite key="Deform1">[1]</d-cite>, <d-cite key="Deform2">[2]</d-cite>, <d-cite key="Deform3">[3]</d-cite>, <d-cite key="Deform4">[4]</d-cite>, <d-cite key="Deform5">[5]</d-cite> <ul> <li>단점 : 아무리 tiny MLP더라도 computational <code class="language-plaintext highlighter-rouge">overhead</code> and low speed</li> </ul> </li> <li>4D space-time domain을 <code class="language-plaintext highlighter-rouge">multiple 2D planes로 decompose</code>하는 grid-based model <d-cite key="Grid1">[6]</d-cite>, <a href="https://semyeong-yu.github.io/blog/2024/4DGS/">4DGS</a>, <d-cite key="Grid3">[7]</d-cite>, <d-cite key="Grid4">[8]</d-cite> <ul> <li>단점 : grid representation으로는 scene의 dynamic 특징의 <code class="language-plaintext highlighter-rouge">fine detail을 fully capture할 수 없음</code> </li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">polynomial trajectories</code> 적용 <d-cite key="trajectory">[9]</d-cite> <ul> <li>장점 : efficient</li> <li>단점 : polynomial trajectory의 <code class="language-plaintext highlighter-rouge">fixed degree</code>는 complex motion을 표현하는 flexibility 측면에서 제한적임</li> </ul> </li> </ul> </li> <li>spline : <ul> <li>minimal number of control points로 complex shape를 smooth and continuous representation으로 표현할 수 있음</li> </ul> </li> <li>SplineGS (본 논문) : <ul> <li>논문 <d-cite key="Mosca">[10]</d-cite>, <d-cite key="GauFRe">[11]</d-cite>에서처럼<br> 각각 static bg와 moving object를 표현하기 위해<br> 3DGS를 <code class="language-plaintext highlighter-rouge">static 3DGS와 dynamic 3DGS의 union</code>으로 확장 <ul> <li>static region :<br> diffuse and specular features는 보존한 채<br> time-encoded feature는 제거</li> <li>dynamic region :<br> mean \(\mu_{i}\) 는 deformation modeling에 의해 결정되는 time-dependent var.<br> rotation \(q_{i}\) 와 scale \(s_{i}\) 도 time-dependent var.</li> </ul> </li> <li>논문 <a href="https://semyeong-yu.github.io/blog/2025/STGS/">STGS</a>에서처럼<br> final pixel <code class="language-plaintext highlighter-rouge">color</code>를 예측할 때 splatted feature rendering 사용 (<code class="language-plaintext highlighter-rouge">SH coeff. 대신 feature</code>!)</li> </ul> </li> </ul> <h2 id="method">Method</h2> <h3 id="architecture">Architecture</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-06-SplineGS/2.PNG-480.webp 480w,/assets/img/2025-02-06-SplineGS/2.PNG-800.webp 800w,/assets/img/2025-02-06-SplineGS/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-02-06-SplineGS/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>goal :<br> jointly optimize 3DGS param. and camera param. <ul> <li>camera param. :<br> extrinsic \([\hat R_{t} | \hat T_{t}] \in R^{3 \times 4}\) for each time \(t\)<br> and shared intrinsic \(\hat K \in R^{3 \times 3}\) across all \(t\)</li> <li>how :<br> two-stage optimization<br> (warm-up stage and main traning stage) <ul> <li> <code class="language-plaintext highlighter-rouge">warm-up stage</code> :<br> optimize <code class="language-plaintext highlighter-rouge">coarse camera param.</code><br> using photometric and geometric consistency<br> (<code class="language-plaintext highlighter-rouge">SfM 사용하지 않기 위해!</code>)</li> <li> <code class="language-plaintext highlighter-rouge">main training stage</code> :<br> initialize 3DGS based on the estimated camera poses<br> and<br> jointly optimize 3DGS param. and camera param. with MAS and MACP</li> </ul> </li> </ul> </li> </ul> <h3 id="motion-adaptive-spline-for-3dgs">Motion-Adaptive Spline for 3DGS</h3> <p>time \(t\) 에서 each dynamic 3DGS의 mean \(\mu(t)\) (continuous trajectory)를 모델링하기 위해<br> cubic Hermite spline function with a set of learnable control points 사용 (MAS)<br> 즉, each dynamic Gaussian마다 a set of control points가 있고 얘네들의 spline curve로 Gaussian mean \(\mu(t)\) 을 결정!</p> <ul> <li>Motion-Adaptive Spline (MAS) :<br> \(\mu(t) = S(t, \boldsymbol P)\) <ul> <li>input : <ul> <li>time \(t\)</li> <li> <table> <tbody> <tr> <td>a set of \(N_{c}\) learnable control points $$\boldsymbol P = { \boldsymbol p_{k}</td> <td>\boldsymbol p_{k} \in R^{3} }\(where\)k \in [0, N_{c}-1]$$</td> </tr> </tbody> </table> </li> </ul> </li> <li>piece-wise cubic Hermite spline function \(S(\cdot)\) :<br> \(S(t, \boldsymbol P) = (2t_{r}^{3} - 3t_{r}^{2} + 1) \boldsymbol p_{\lfloor t_{s} \rfloor} + (t_{r}^{3} - 2t_{r}^{2} + t_{r}) \boldsymbol m_{\lfloor t_{s} \rfloor} + (-2t_{r}^{3} + 3t_{r}^{2}) \boldsymbol p_{\lfloor t_{s} \rfloor + 1} + (t_{r}^{3} - t_{r}^{2}) \boldsymbol m_{\lfloor t_{s} \rfloor + 1}\) <ul> <li>\(N_{f}\) : frame (timestamp) 개수</li> <li>\(N_{c}\) : control point 개수 (estimated by MACP)</li> <li> \[t \in [0, N_{f} - 1]\] </li> <li>\(t_{s} = t \frac{N_{c} - 1}{N_{f} - 1} \in [0, N_{c} - 1]\)<br> e.g. 3.7</li> <li>\(t_{r} = t_{s} - \lfloor t_{s} \rfloor\)<br> e.g. 0.7</li> <li>\(\boldsymbol m_{k} = (\boldsymbol p_{k+1} - \boldsymbol p_{k-1})/2\) : approx. tangent(기울기) of control point \(\boldsymbol p_{k}\)</li> <li> <code class="language-plaintext highlighter-rouge">piece-wise cubic Hermite spline function</code> :<br> \(\lfloor t_{s} \rfloor = 3\) 에서의 control point 및 tangent와<br> \(\lfloor t_{s} \rfloor + 1 = 4\) 에서의 control point 및 tangent와<br> 그 사이 어디쯤 있는지 \(t_{r} = 0.7\) 를 이용하여<br> \(\lfloor t_{s} \rfloor = 3\) 과 \(\lfloor t_{s} \rfloor + 1 = 4\) 사이의 piece-wise cubic Hermite spline function을 그림</li> </ul> </li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">Initialization of 3D Control Points</code> :<br> intialization은 quality에 매우 중요!<br> long-range <code class="language-plaintext highlighter-rouge">2D track</code> <d-cite key="cotracker">[12]</d-cite>과 <code class="language-plaintext highlighter-rouge">depth</code> <d-cite key="unidepth">[13]</d-cite> prior 사용 <ul> <li>notation : <ul> <li>2D track by <d-cite key="cotracker">[12]</d-cite> : \(\mathcal{T} = \left\{ \varphi_{t}^{tr} | \varphi_{t}^{tr} \in R^{2} \right\}_{t \in [0, N_{f} - 1]}\)<br> where \(\varphi_{t}^{tr}\) : 2D track on pixel-coordinate at time \(t\)</li> <li>projection func. from 3D camera-space to 2D image-space by intrinsic \(K\) : \(\pi_{K}(\cdot)\)</li> </ul> </li> <li>Step 1)<br> <code class="language-plaintext highlighter-rouge">unproject 2D track</code> \(\mathcal{T}\) on image-space into 3D track curve on world-space<br> using depth \(d_{t}\) and coarsely-estimated extrinsic \([\hat K_{t} | \hat T_{t}]\)<br> \(W_{t}(\varphi_{t}^{tr}) = \hat R_{t}^{T} \pi_{\hat K}^{-1}(\varphi_{t}^{tr}, d_{t}(\varphi_{t}^{tr})) - \hat R_{t}^{T} \hat T_{t}\) <ul> <li>we estimate camera param. \(\hat K, \hat R, \hat T\) from only frames (without any GT)</li> </ul> </li> <li>Step 2)<br> initialize per-Gaussian control points set \(\boldsymbol P\)<br> by least-square approx. s.t. <code class="language-plaintext highlighter-rouge">spline curve</code> \(S(t, \boldsymbol P)\) fits the initial <code class="language-plaintext highlighter-rouge">tracker curve</code> \(W_{t}(\varphi_{t}^{tr})\)<br> \(\text{min}_{\boldsymbol P} \sum_{t=0}^{N_{f} - 1} \| W_{t}(\varphi_{t}^{tr}) - S(t, \boldsymbol P) \|^{2}\)</li> </ul> </li> <li>Motion-Adaptive Control Points Pruning (MACP) : <ul> <li>issue : <ul> <li>control points 수가 너무 많으면<br> spline curve가 over-fitting되고 speed가 느려짐</li> <li>scene마다 motion의 종류와 정도가 각기 다르므로<br> control points 수 for each dynamic 3DGS 는 scene에 맞춰서 need to be adaptively adjusted</li> </ul> </li> <li>solution :<br> sparser control points로 prune하기 위해<br> <code class="language-plaintext highlighter-rouge">every 3DGS densification이 끝날 때마다</code> new spline function \(\mu(t) = S(t, \boldsymbol P')\) 계산<br> where \(\boldsymbol P' = \left\{ \boldsymbol p_{l}' | \boldsymbol p_{l}' \in R^{3} \right\}_{l \in [0, N_{c} - 2]}\) : a set of \(N_{c} - 1\) control points<br> (current set \(\boldsymbol P\) 보다 control point 1개 더 적음)</li> <li>Step 1)<br> <code class="language-plaintext highlighter-rouge">1개 적은 control point set</code>으로도 최대한 비슷한 spline curve를 만들도록 least-square approx.<br> \(\text{min}_{\boldsymbol P'} \sum_{t=0}^{N_{f}-1} \| S(t, \boldsymbol P) - S(t, \boldsymbol P') \|^{2}\)</li> <li>Step 2)<br> \(S(t, \boldsymbol P)\) 와 \(S(t, \boldsymbol P')\) 간의 error \(E\) 가 작을 때만 a set of control points 업데이트<br> \(\boldsymbol P = \begin{cases} \boldsymbol P' &amp; \text{if} &amp; E \lt \epsilon \\ \boldsymbol P &amp; O.W. \end{cases}\)<br> where error \(E = \frac{1}{N_{f}} \sum_{t=0}^{N_{f} - 1} \| \pi_{\hat K}(\hat R_{t} S(t, \boldsymbol P) + \hat T_{t}) - \pi_{\hat K} (\hat R_{t} S(t, \boldsymbol P') + \hat T_{t} \|^{2}\)<br> (각 timestamp \(t\) 에서 <code class="language-plaintext highlighter-rouge">3D mean on spline curve를 2D로 project시킨 뒤 차이</code> 비교)</li> <li>의의 :<br> each dynamic 3DGS마다 a set of control points를 따로 가지고 있는데,<br> MACP 덕분에 각 dynamic 3DGS가 각기 다른 수의 control points를 가질 수 있고,<br> <code class="language-plaintext highlighter-rouge">motion이 복잡한 part는 control points 수가 많고</code><br> <code class="language-plaintext highlighter-rouge">motion이 단순한 part는 control poitns 수가 적은</code> 방식으로<br> scene에 adaptively adjust 가능</li> </ul> </li> </ul> <h3 id="camera-pose-estimation">Camera Pose Estimation</h3> <ul> <li>Camera Pose : <ul> <li> <code class="language-plaintext highlighter-rouge">extrinsic</code> :<br> \([\hat R_{t} | \hat T_{t}] = F_{\theta}(\gamma(t))\) <ul> <li>extrinsic 은 <code class="language-plaintext highlighter-rouge">time에 대한 function</code> </li> <li>notation : <ul> <li>\(\gamma(\cdot)\) : positional encoding</li> <li>\(F_{\theta}\) : shallow MLP</li> </ul> </li> </ul> </li> <li>intrinsic (<code class="language-plaintext highlighter-rouge">focal length</code>) : <ul> <li>focal length \(\hat f\) 는 learnable param. <code class="language-plaintext highlighter-rouge">shared across all frames</code> in monocular video</li> </ul> </li> </ul> </li> <li>Loss for optimizing Camera Pose : <ul> <li>Loss 1) <code class="language-plaintext highlighter-rouge">photometric consistency</code> : <code class="language-plaintext highlighter-rouge">projection alignment</code> <ul> <li>목적 :<br> target frame \(t\) 의 pixel \(i\) 가 reference frame \(t_{ref}\) 의 pixel \(j\) 로 projection 되었을 때<br> reference frame’s pixel \(j\) 의 color \(I_{t_{ref}}(\varphi_{t \rightarrow t_{ref}})\) 가<br> target frame’s pixel \(i\) 의 color \(I_{t}(\varphi_{t})\) 와 일치하도록</li> <li>notation : <ul> <li>\(\varphi_{t}\) : target frame’s pixel-coordinate</li> <li>\(\varphi_{t \rightarrow t_{ref}} = \pi_{\hat K} (\hat R_{t_{ref}} (\hat R_{t}^{T} \pi_{\hat K}^{-1} (\varphi_{t}, d_{t}(\varphi_{t})) - \hat R_{t}^{T} \hat T_{t}) + \hat T_{t_{ref}})\) : reference frame’s pixel-coordinate corresponding to \(\varphi_{t}\)<br> (2D target frame \(t\)’s pixel-coordinate \(\rightarrow\) 3D location world-coordinate \(\rightarrow\) 2D reference frame \(t_{ref}\)’s pixel-coordinate)</li> </ul> </li> <li>loss :<br> \(L_{pc} = \sum_{\varphi_{t}} \| M_{t, t_{ref}}(\varphi_{t}) \circledast (I_{t}(\varphi_{t}) - I_{t_{ref}}(\varphi_{t \rightarrow t_{ref}})) \|^{2}\) <ul> <li>\(M_{t, t_{ref}} = M_{t}(\varphi_{t}) M_{t_{ref}}(\varphi_{t \rightarrow t_{ref}})\) : union motion mask<br> (dynamic objects는 color 변하는 게 당연하니까 제거하고, static region에 대해서만 loss 걸어줌)<br> (\(M_{t}\) 와 \(M_{t_{ref}}\) 는 각각 \(I_{t}\) 와 \(I_{t_{ref}}\) 로부터 미리 계산한 motion mask <d-cite key="TrackAnything">[14]</d-cite>)</li> </ul> </li> </ul> </li> <li>Loss 2) <code class="language-plaintext highlighter-rouge">geometric consistency</code> : <code class="language-plaintext highlighter-rouge">3D alignment</code> <ul> <li>목적 :<br> target frame \(t\) 의 pixel \(i\) 가 reference frame \(t_{ref}\) 의 pixel \(j\) 로 projection 되었을 때<br> reference frame’s pixel \(j\) 를 3D location on world-coordinate으로 unproject시킨 \(W_{t_{ref}}(\varphi_{t \rightarrow t_{ref}})\) 가<br> target frame’s pixel \(i\) 를 3D location on world-coordinate으로 unproject시킨 \(W_{t}(\varphi_{t})\) 와 일치하도록</li> <li>notation : <ul> <li>\(W_{t}(\varphi_{t}) = \hat R_{t}^{T} \pi_{\hat K}^{-1}(\varphi_{t}, d_{t}(\varphi_{t})) - \hat R_{t}^{T} \hat T_{t}\) : unproject from pixel-coordinate to 3D world-coordinate</li> </ul> </li> <li>loss :<br> \(L_{gc} = \sum_{\varphi_{t}} \| M_{t, t_{ref}}(\varphi_{t}) \circledast (W_{t}(\varphi_{t}) - W_{t_{ref}}(\varphi_{t \rightarrow t_{ref}})) \|^{2}\)</li> </ul> </li> </ul> </li> </ul> <h3 id="loss">Loss</h3> <ul> <li>Two-stage Optimization : <ul> <li>Stage 1) warm-up stage <ul> <li>optimize <code class="language-plaintext highlighter-rouge">only camera param.</code> </li> <li>loss :<br> \(L_{total}^{warm} = \lambda_{pc} L_{pc} + \lambda_{gc} L_{gc}\) <ul> <li>\(L_{pc}\) : photometric consistency (projection alignment)</li> <li>\(L_{gc}\) : geometric consistency (3D alignment)</li> </ul> </li> </ul> </li> <li>Stage 2) main training stage <ul> <li>Step 2-1)<br> Stage 1)에서 coarsely 예측한 camera param. \(\hat K, \hat R, \hat T\) 를 이용하여<br> 각 dynamic 3DGS의 <code class="language-plaintext highlighter-rouge">a set of control points 초기화</code><br> (how? : 위의 Motion-Adaptive Spline for 3DGS 섹션에서 설명함)</li> <li>Step 2-2)<br> <code class="language-plaintext highlighter-rouge">jointly optimize 3DGS param. and camera param.</code> </li> <li>loss :<br> \(L_{total}^{main} = \lambda_{rgb} L_{rgb} + \lambda_{d} L_{d} + \lambda_{M} L_{M} + \lambda_{pc} L_{pc} + \lambda_{d-pc} L_{d-pc} + \lambda_{gc} L_{gc}\) <ul> <li> <code class="language-plaintext highlighter-rouge">recon. loss</code> : <ul> <li>\(L_{rgb}\) : L1 recon. loss b.w. rendered frame and GT frame</li> <li>\(L_{d}\) : L1 recon. loss b.w. rendered depth and GT depth</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">alignment loss</code> : <ul> <li>\(L_{pc}\) : photometric consistency (projection alignment)</li> <li>\(L_{gc}\) : geometric consistency (3D alignment)</li> <li>\(L_{d-pc}\) : additional photometric consistency (projection alignment) <ul> <li> <code class="language-plaintext highlighter-rouge">prior depth</code> <d-cite key="unidepth">[13]</d-cite> <code class="language-plaintext highlighter-rouge">대신</code> 3DGS를 이용한 <code class="language-plaintext highlighter-rouge">rendered depth</code> 사용하여<br> photometric consistency 계산</li> <li>prior depth 대신 rendered depth를 사용하면<br> estimated 3DGS geometry 의 도움을 받아<br> joint optimization of camera param. and 3DGS param. 가능!</li> </ul> </li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">motion mask loss</code> : <ul> <li>\(L_{M} = 1 - \text{f1-score} = 1 - \frac{2(\sum_{\varphi_{t}} M_{t}(\varphi_{t}) \hat M_{t}(\varphi_{t})) + \epsilon}{(\sum_{\varphi_{t}} M_{t}(\varphi_{t}) + \hat M_{t}(\varphi_{t})) + \epsilon}\) : binary dice loss<br> b.w. <code class="language-plaintext highlighter-rouge">pre-computed GT motion mask</code> \(M_{t}\) from prior <d-cite key="TrackAnything">[14]</d-cite><br> and <code class="language-plaintext highlighter-rouge">rendered motion mask</code> \(\hat M_{t}\) from dynamic 3D Gaussians <ul> <li>rendered motion mask :<br> \(\hat M_{t}(\varphi_{t}) = \sum_{i \in N} m_{i} \alpha_{i} \prod_{j=1}^{i-1} (1 - \alpha_{j})\)<br> where \(m_{i} = 0\) if \(i\)-th 3DGS is static 3DGS, and \(m_{i} = 1\) if \(i\)-th 3DGS is dynamic 3DGS<br> (즉, \(i\)-th 3DGS가 static인지, dynamic인지에 따른 \(m_{i}\) 를 accumulate 하여 motion mask로 rendering!)</li> </ul> </li> <li>binary dice loss는 highly imbalanced segmentation을 위해 제안되었듯이<br> dynamic 3DGS와 static 3DGS를 더 잘 분리할 수 있게 해줌</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <h3 id="implementation">Implementation</h3> <h3 id="result">Result</h3> <h3 id="ablation-study">Ablation Study</h3> <h2 id="conclusion">Conclusion</h2> <ul> <li>Limitation : <ul> <li>2D Track 및 Depth Estimation Prior 필요</li> <li>TBD</li> </ul> </li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br> photometric consistency loss에서 motion mask 값이 static region에 대해서만 0이라는데,<br> dynamic object를 exclude한다는 문구로 미루어보아 (static region에만 loss를 걸어주기 위해)<br> static region에 대해서 1이어야 하는 거 아닌가요?</p> </li> <li> <p>A1 :<br> code implementation 한 번 보자</p> </li> </ul> <p>아직 Figure 2. 안 봄!! TBD</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-02-06-SplineGS.bib"></d-bibliography> <div id="disqus_thread" style="max-width: 800px; margin: 0 auto;"> <script type="text/javascript">var disqus_shortname="semyeong-yu",disqus_identifier="/blog/2025/SplineGS",disqus_title="SplineGS";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Semyeong Yu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5YMLX9VHFX"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5YMLX9VHFX");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>