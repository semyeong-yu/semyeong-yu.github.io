<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> NoPoSplat | Semyeong Yu </title> <meta name="author" content="Semyeong Yu"> <meta name="description" content="Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images (ICLR 2025)"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://semyeong-yu.github.io/blog/2025/NoPoSplat/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "NoPoSplat",
            "description": "Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images (ICLR 2025)",
            "published": "February 03, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Semyeong</span> Yu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>NoPoSplat</h1> <p>Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images (ICLR 2025)</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#contribution">Contribution</a> </div> <div> <a href="#related-works">Related Works</a> </div> <div> <a href="#method">Method</a> </div> <ul> <li> <a href="#architecture">Architecture</a> </li> <li> <a href="#gaussian-space">Gaussian Space</a> </li> <li> <a href="#camera-intrinsic-embedding">Camera Intrinsic Embedding</a> </li> <li> <a href="#training-and-inference">Training and Inference</a> </li> </ul> <div> <a href="#experiment">Experiment</a> </div> <ul> <li> <a href="#implementation">Implementation</a> </li> <li> <a href="#result">Result</a> </li> <li> <a href="#ablation-study">Ablation Study</a> </li> </ul> <div> <a href="#conclusion">Conclusion</a> </div> <div> <a href="#question">Question</a> </div> </nav> </d-contents> <h2 id="no-pose-no-problem---surprisingly-simple-3d-gaussian-splats-from-sparse-unposed-images">No Pose, No Problem - Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images</h2> <h4 id="botao-ye-sifei-liu-haofei-xu-xueting-li-marc-pollefeys-ming-hsuan-yang-songyou-peng">Botao Ye, Sifei Liu, Haofei Xu, Xueting Li, Marc Pollefeys, Ming-Hsuan Yang, Songyou Peng</h4> <blockquote> <p>paper :<br> <a href="https://arxiv.org/abs/2410.24207" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2410.24207</a><br> project website :<br> <a href="https://noposplat.github.io/" rel="external nofollow noopener" target="_blank">https://noposplat.github.io/</a><br> code :<br> <a href="https://github.com/cvg/NoPoSplat" rel="external nofollow noopener" target="_blank">https://github.com/cvg/NoPoSplat</a></p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/4.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/4.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/4.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-02-03-NoPoSplat/4.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="contribution">Contribution</h2> <p><code class="language-plaintext highlighter-rouge">pose-free generalizable sparse-view 3D recon. model in canonical Gaussian space!</code></p> <ul> <li>model : <ul> <li> <code class="language-plaintext highlighter-rouge">unposed</code> (no extrinsic) <code class="language-plaintext highlighter-rouge">sparse-view</code> images로부터 3DGS를 통해 3D scene recon.하는 feed-forward network 제시</li> <li> <code class="language-plaintext highlighter-rouge">photometric loss만으로</code> train 가능<br> (<code class="language-plaintext highlighter-rouge">GT depth 사용 X</code>, explicit matching loss 사용 X)</li> <li>본 논문은 intrinsic의 영향을 받는 image appearance에만 의존하여 recon.을 수행하므로<br> <code class="language-plaintext highlighter-rouge">scale ambiguity</code> 문제 해결을 위해 <code class="language-plaintext highlighter-rouge">intrinsic embedding method</code> 사용<br> (intrinsic은 input으로 사용)</li> <li>covariance, opacity, color를 예측하는 Gaussian Param. Head에서 fine texture detail 주기 위해 <code class="language-plaintext highlighter-rouge">RGB shortcut</code> 사용</li> </ul> </li> <li>downstream tasks : <ul> <li>recon.된 3DGS를 이용하여 novel-view-synthesis 및 pose-estimation task 수행 가능 <ul> <li>특히 limited input image overlap (sparse) 상황에서는 pose-required methods보다 더 좋은 성능</li> <li>정확히 pose-estimation 수행하는 two-stage coarse-to-fine pipeline 제시</li> </ul> </li> <li>generalize well to out-of-distribution data</li> </ul> </li> <li>Gaussian Space : <ul> <li> <code class="language-plaintext highlighter-rouge">first input view의 local camera coordinate</code>을 <code class="language-plaintext highlighter-rouge">canonical space</code>로 고정하고 모든 input view의 3DGS들을 해당 space에서 directly 예측</li> <li>기존에는 transform-then-fuse pipeline이었는데,<br> 본 논문은 global coordinate으로의 <code class="language-plaintext highlighter-rouge">explicit transform 없이</code> canonical space 내에서의 different views의 fusion 자체를 직접 network로 학습</li> <li>local coordinate에서 global coordinate으로 3DGS를 explicitly transform할 필요가 없으므로<br> explicitly transform하면서 생기는 per-frame Gaussians의 misalignment를 방지할 수 있고, extrinsic pose 없이도 3D recon. 가능</li> </ul> </li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li>SfM : <ul> <li>bundle adjustment 등 최적화 과정을 거치는데,<br> off-the-shelf pose estimation method 사용하는 것 자체가 많은 연산을 필요로 하고 runtime 늘림</li> <li>3D recon.에 only two frames만 input으로 사용하더라도<br> SfM을 통해 해당 two frames의 camera pose를 구하려면 many poses from dense videos 필요 (impractical)</li> <li>textureless area 또는 image가 sparse한 영역에서는 잘 못 함</li> </ul> </li> <li>Pose-Free Method : <ul> <li>pose-estimation과 3D recon.을 single pipeline으로 통합하자! : <d-cite key="DBARF">[1]</d-cite>, <d-cite key="Flowcam">[2]</d-cite>, <d-cite key="Unifying">[3]</d-cite> <ul> <li>pose-estimation과 scene-recon.을 번갈아가며 수행하는 sequential process 에서 error가 쌓이기 때문에<br> SOTA novel-view-synthesis methods보다 성능 bad</li> </ul> </li> <li>DUSt3R, MASt3R 계열</li> </ul> </li> <li>DUSt3R, MASt3R : <ul> <li>공통점 1)<br> pose-free method</li> <li>공통점 2)<br> directly predict in canonical space</li> <li>차이점 1)<br> DUSt3R, MASt3R는 transformer output이 3D pointmap (point cloud)인데,<br> NoPoSplat은 mean, covariance, opacity, color를 가진 3DGS (rasterization) 사용</li> <li>차이점 2)<br> NoPoSplat은 DUSt3R, MASt3R 계열과 달리 <code class="language-plaintext highlighter-rouge">GT depth 필요 없고 photometric loss만으로</code> 훈련 가능</li> </ul> </li> <li>pixelSplat, MVSplat : <ul> <li>차이점 1) (아래 그림 참고)<br> pixelSplat, MVSplat은 먼저 intrinsic을 이용해 2D-to-3D로 unproject(lift)하여 each local-coordinate에서 3DGS를 예측한 뒤 extrinsic을 이용해 world-coordinate으로 transform한 뒤 fuse했는데,<br> NoPoSplat은 canonical space 내에서의 different views의 fusion 자체를 directly network로 학습하기 때문에 <code class="language-plaintext highlighter-rouge">global coordinate으로 transform할 필요가 없으므로</code> 이에 따른 <code class="language-plaintext highlighter-rouge">misalignment를 방지</code>할 수 있고 <code class="language-plaintext highlighter-rouge">camera pose (extrinsic)도 필요 없음</code> </li> <li>차이점 2)<br> pixelSplat에선 epipolar constraint, MVSplat에선 cost volume이라는 geometry prior를 사용하였는데<br> image view overlap이 적을 때는 geometry prior가 정확하지 않음.<br> NoPoSplat은 (image overlap이 클 때 유리한) <code class="language-plaintext highlighter-rouge">geometry prior들을 사용하지 않음</code> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/2.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/2.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/2.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-02-03-NoPoSplat/2.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="method">Method</h2> <h3 id="architecture">Architecture</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/3.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/3.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/3.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-02-03-NoPoSplat/3.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>I/O :<br> \(f_{\theta} : \left\{ (I^{v}, k^{v}) \right\}_{v=1}^{V} \mapsto \left\{ \bigcup (\mu_{j}^{v}, \alpha_{j}^{v}, r_{j}^{v}, s_{j}^{v}, c_{j}^{v}) \right\}_{j=1, \ldots, H \times W}^{v=1, \ldots, V}\) <ul> <li>input : <ul> <li>sparse unposed multi-view images \(I\) (image 개수 \(V\))</li> <li>camera intrinsics \(k\) (available from modern devices <d-cite key="intrinsic">[4]</d-cite>)</li> </ul> </li> <li>output : <ul> <li>mean \(\mu \in R^{3}\), opacity \(\alpha \in R\), rotation \(r \in R^{4}\), scale \(s \in R^{3}\), SH \(c \in R^{k}\) (\(k\) degrees of freedom)</li> </ul> </li> </ul> </li> <li>Pipeline : <ul> <li> <code class="language-plaintext highlighter-rouge">Encoder, Decoder</code> : <ul> <li>특히 input views끼리 content overlap이 적은 상황 (sparse) 에서는<br> epipolar constraint나 cost volume 같은 geometry prior가 없더라도<br> simple ViT 구조만으로도 좋은 성능 달성 가능</li> <li>RGB images를 image tokens로 patchify, flatten한 뒤<br> intrinsic token과 concatenate한 뒤<br> Encoder and Decoder에 feed-forward</li> </ul> </li> <li>Gaussian Parameter Prediction Head :<br> DPT 구조 <ul> <li> <code class="language-plaintext highlighter-rouge">Gaussian Center Head</code> :<br> Decoder feature 사용</li> <li> <code class="language-plaintext highlighter-rouge">Gaussian Param Head</code> :<br> RGB image와 Decoder feature 사용 <ul> <li> <code class="language-plaintext highlighter-rouge">RGB shortcut</code> :<br> 3D recon.에서 fine texture detail을 잡는 것이 중요하기 때문에 사용</li> <li>Decoder feature :<br> high-level semantic info.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <h3 id="gaussian-space">Gaussian Space</h3> <ul> <li>baseline: <code class="language-plaintext highlighter-rouge">Local-to-Global Gaussian Space</code> <ul> <li>pixelSplat, MVSplat 등</li> <li>how :<br> 먼저 each pixel의 depth를 network로 예측한 뒤<br> predicted depth와 intrinsic을 이용해 2D-to-3D로 unproject(lift)하여 each local-coordinate에서 3DGS 예측한 뒤<br> extrinsic을 이용해 world-coordinate으로 transform한 뒤<br> 모든 transformed 3DGS들을 fuse</li> <li>issue : <ul> <li>local-coordinate에서 world-coordinate으로 transform할 때 <code class="language-plaintext highlighter-rouge">accurate camera pose</code> (extrinsic) 필요한데, 이는 input view가 sparse한 real-world 상황에서 얻기 어렵</li> <li>특히 input view가 sparse할 때 또는 out-of-distribution data로 일반화할 때는<br> <code class="language-plaintext highlighter-rouge">each transformed 3DGS들을 조화롭게 combine</code>하는 게 어렵</li> </ul> </li> </ul> </li> <li>NoPoSplat: <code class="language-plaintext highlighter-rouge">Canonical Gaussian Space</code> <ul> <li>how :<br> first input view를 global referecne coordinate으로 고정한 뒤 (\([R | t] = [\boldsymbol I | \boldsymbol 0]\))<br> 해당 coordinate 내에서 each input view \(v\) 마다 set \(\left\{ \mu_{j}^{v \rightarrow 1}, r_{j}^{v \rightarrow 1}, c_{j}^{v \rightarrow 1}, \alpha_{j}, s_{j} \right\}\) 을 예측</li> <li>benefit : <ul> <li>global coordinate으로 explicitly transform할 필요가 없으므로 camera pose (extrinsic) 필요 없음</li> <li>explicitly transform-then-fuse하는 게 아니라 fuse 자체를 network로 학습하는 것이기 때문에<br> 조화로운 global representation 가능</li> </ul> </li> </ul> </li> </ul> <h3 id="camera-intrinsic-embedding">Camera Intrinsic Embedding</h3> <ul> <li>Camera Intrinsic Embedding : <ul> <li>issue :<br> only appearance에만 의존하여 3D recon.을 수행함<br> <code class="language-plaintext highlighter-rouge">scale ambiguity</code> (scale misalignment) 문제 해결 필요!<br> 필요한 geometric info.를 제공하기 위해!<br> intrinsic \(k = [f_{x}, f_{y}, c_{x}, c_{y}]\)</li> <li>solve : <ul> <li>Trial 1) Global Intrinsic Embedding by Addition :<br> intrinsic \(k\) 을 linear layer에 통과시킨 뒤 RGB image token에 add</li> <li>Trial 2) Global Intrinsic Embedding by Concat :<br> intrinsic \(k\) 을 linear layer에 통과시킨 뒤 RGB image token에 concat</li> <li>Trial 3) Pixel-wise (Dense) Intrinsic Embedding :<br> each pixel \(p_{j}\)에 대해 ray direction \(K^{-1} p_{j}\) 구한 뒤<br> SH 이용해서 high-dim. feature로 변환한 뒤<br> RGB image와 concat</li> </ul> </li> </ul> </li> </ul> <h3 id="training-and-inference">Training and Inference</h3> <ul> <li> <p>Loss :<br> only photometric loss<br> (linear comb. of MSE and LPIPS)</p> </li> <li>Relative Pose Estimation :<br> canonical space에 3DGS들이 있다는 전제 하에<br> <code class="language-plaintext highlighter-rouge">two-stage coarse-to-fine pipeline</code> <ul> <li>Coarse Stage :<br> Gaussian center에 <code class="language-plaintext highlighter-rouge">PnP algorithm with RANSAC</code> (efficient as done in ms) 적용하여<br> <code class="language-plaintext highlighter-rouge">initial rough pose estimate</code> 구하기</li> <li>Fine Stage :<br> <code class="language-plaintext highlighter-rouge">3DGS param.을 freeze</code>한 채<br> training에 사용했던 <code class="language-plaintext highlighter-rouge">photometric loss</code>를 이용해<br> target view와 align되도록 rough <code class="language-plaintext highlighter-rouge">target camera pose를 optimize</code>(refine) <ul> <li>automatic diff.에서의 overhead를 줄이기 위해<br> camera Jacobian을 계산 <d-cite key="GSslam">[5]</d-cite> </li> </ul> </li> </ul> </li> <li>Evaluation-Time Pose Alignment : <ul> <li>unposed input images의 경우<br> scene은 다른데 rendered two images는 같을 수 있으므로<br> just two input views로 3D scene recon. 수행하는 건 사실 ambiguous</li> <li>GT camera pose를 이용하는 other baseline들 <d-cite key="pose1">[6]</d-cite>, <a href="https://semyeong-yu.github.io/blog/2024/pixelSplat/">7</a>과 비교하기 위해 (evaluation purpose)<br> pose-free methods <d-cite key="nopose1">[8]</d-cite>, <d-cite key="nopose2">[9]</d-cite>의 경우 target view에 대한 camera pose를 optimize</li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <h3 id="implementation">Implementation</h3> <ul> <li>Experiment : <ul> <li>Dataset : <ul> <li>training :<br> RE10K (RealEstate10k) : indoor real estate<br> DL3DV : outdoor (camera motion 더 다양)</li> <li>zero-shot generalization :<br> ACID : nature scene by drone<br> DTU<br> ScanNet<br> ScanNet++<br> in-the-whild mobile phone capture<br> SORA-generated images</li> </ul> </li> <li>camera overlap :<br> SOTA dense feature matching method <d-cite key="ROMA">[10]</d-cite> 로<br> input images’ camera overlap 정도를 측정하여<br> small (0.05%-0.3%), medium (0.3%-0.55%), large (0.55%-0.8%)로 나눔</li> <li>Baseline : <ul> <li>pose-required novel-view-synthesis :<br> pixelNeRF, AttnRend, pixelSplat, MVSplat</li> <li>pose-free novel-view-synthesis and relative pose estimation :<br> DUSt3R, MASt3R, Splatt3R, CoPoNeRF, RoMa</li> </ul> </li> <li>Implementation :<br> encoder, decoder, Gaussian center head는 MASt3R의 weights로 initialize하고<br> (사실 scratch부터 training해도 성능 비슷하긴 함)<br> Gaussian param head는 randomly initialize</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/5.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/5.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/5.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-02-03-NoPoSplat/5.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h3 id="result">Result</h3> <ul> <li>Novel View Synthesis : <ul> <li>SOTA pose-free (DUSt3R, MASt3R, Splatt3R) : <ul> <li>DUSt3R 계열은 <code class="language-plaintext highlighter-rouge">per-pixel depth loss</code>에 의존하기 때문에 each views를 <code class="language-plaintext highlighter-rouge">fuse하는 게 어렵</code><br> 그래서 대부분 상황에서 NoPoSplat이 훨씬 더 좋음</li> </ul> </li> <li>SOTA pose-required (pixelSplat, MVSplat) : <ul> <li>pixelSplat, MVSplat은 <code class="language-plaintext highlighter-rouge">input view overlap이 작을 때 부정확한 geometry prior</code> (epipolar constraint, cost volume)을 사용하기 때문에<br> image view overlap이 작은 상황에서는 NoPoSplat이 더 좋음</li> <li>pixelSplat, MVSplat은 <code class="language-plaintext highlighter-rouge">transform-then-fuse strategy</code>를 사용하는데 <code class="language-plaintext highlighter-rouge">misalignment</code>로 부정확할 수 있기 때문에<br> canonical space에서 directly 예측하는 NoPoSplat이 더 좋을 수 있음</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/6.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/6.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/6.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-02-03-NoPoSplat/6.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/7.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/7.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/7.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-02-03-NoPoSplat/7.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Relative Pose Estimation :</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/8.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/8.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/8.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-02-03-NoPoSplat/8.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Geometry Reconstruction : <ul> <li>SOTA pose-required (pixelSplat, MVSplat) : <ul> <li>pixelSplat, MVSplat은 explicitly transform-then-fuse하는 과정에서 두 input images의 경계 영역에서 misalignment (아래 그림에서 파란색 화살표로 표기) 가 있고,<br> input views’ overlap이 적을 때는 geometry prior가 부정확해서 distortion (아래 그림에서 분홍색 화살표로 표기) 있는데,<br> NoPoSplat은 canonical space에서 directly 예측하므로 해결</li> </ul> </li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/9.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/9.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/9.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-02-03-NoPoSplat/9.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Cross-Dataset Generalization :<br> NoPoSplat은 geometry prior를 사용하지 않으므로 다양한 scene type에 adapt 가능<br> 심지어 ScanNet++로의 zero-shot generalization에 대해 RE10K로 훈련시킨 NoPoSplat과 ScanNet++로 훈련시킨 pose-required Splatt3R을 비교했을 때 NoPoSplat이 더 좋음!</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/10.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/10.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/10.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-02-03-NoPoSplat/10.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Model Efficiency :<br> NoPoSplat은 0.015초만에 (66 FPS) 3DGS 예측 가능<br> (additional geometry prior 안 쓰니까 speed 빠름!)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/11.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/11.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/11.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-02-03-NoPoSplat/11.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> inference on RTX 4090 GPU </div> <ul> <li>In-the-Wild Unposed Images :<br> 3D Generation task에 적용 가능!<br> 먼저 text/image to multi-image/video model 이용해서 sparse scene-level multi-view images 얻은 뒤<br> Ours (NoPoSplat) 이용해서 3D model 얻음</li> </ul> <h3 id="ablation-study">Ablation Study</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2025-02-03-NoPoSplat/12.PNG-480.webp 480w,/assets/img/2025-02-03-NoPoSplat/12.PNG-800.webp 800w,/assets/img/2025-02-03-NoPoSplat/12.PNG-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/2025-02-03-NoPoSplat/12.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Ablation Study : <ul> <li> <code class="language-plaintext highlighter-rouge">Output Canonical Gaussian Space</code> :<br> transform-then-fuse pipeline of pose-required methods has <code class="language-plaintext highlighter-rouge">ghosting artifacts</code> </li> <li> <code class="language-plaintext highlighter-rouge">Camera Intrinsic Embedding</code> :<br> no intrinsic leads to <code class="language-plaintext highlighter-rouge">blurry</code> results due to <code class="language-plaintext highlighter-rouge">scale ambiguity</code><br> 실험적으로 intrinsic token concat. 방식이 best</li> <li> <code class="language-plaintext highlighter-rouge">RGB Shortcut</code> :<br> no RGB Shortcut leads to <code class="language-plaintext highlighter-rouge">blurry</code> results in texture-rich areas<br> (위 그림의 quilt in row 1 and chair in row 3)</li> <li> <code class="language-plaintext highlighter-rouge">3 Input Views</code> instead of 2 :<br> baselines과의 공평한 비교를 위해 NoPoSplat은 two input-views setting을 사용했는데<br> three input-views를 사용할 경우 성능이 훨씬 좋아졌음!</li> </ul> </li> </ul> <h2 id="conclusion">Conclusion</h2> <ul> <li>Future Work :<br> NoPoSplat은 static scene에만 적용했는데, dynamic scene에 NoPoSplat의 pipeline을 확장 적용!</li> </ul> <h2 id="question">Question</h2> <ul> <li> <p>Q1 :<br> 사실 NoPoSplat은 global coordinate으로의 explicit transform이나 geometry prior (epopiolar constraint, cost volume 등)나 GT depth 없이<br> 오로지 implicit network의 학습에 의존하여 scene recon. 능력을 학습하겠다는 건데<br> photometric loss만으로도 잘 학습이 되나? two input images 경계면의 smoothness 등 추가 regularization loss 추가해주는 게 낫지 않음?</p> </li> <li> <p>A1 :<br> TBD</p> </li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-02-03-NoPoSplat.bib"></d-bibliography> <div id="disqus_thread" style="max-width: 800px; margin: 0 auto;"> <script type="text/javascript">var disqus_shortname="semyeong-yu",disqus_identifier="/blog/2025/NoPoSplat",disqus_title="NoPoSplat";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Semyeong Yu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-5YMLX9VHFX"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-5YMLX9VHFX");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>