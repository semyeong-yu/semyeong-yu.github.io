---
layout: distill
title: Quark
date: 2024-12-23 12:00:00
description: Real-time, High-resolution, and General Neural View Synthesis (SIGGRAPH 2024)
tags: general view synthesis
categories: 3d-view-synthesis
thumbnail: assets/img/2024-12-23-Quark/1.PNG
giscus_comments: false
disqus_comments: true
related_posts: true
toc:
  - name: Contribution
  - name: Related Works
  - name: Overview

_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Quark - Real-time, High-resolution, and General Neural View Synthesis

#### John Flynn, Michael Broxton, Lukas Murmann, Lucy Chai, Matthew DuVall, Clément Godard, Kathryn Heal, Srinivas Kaza, Stephen Lombardi, Xuan Luo, Supreeth Achar, Kira Prabhu, Tiancheng Sun, Lynn Tsai, Ryan Overbeck

> paper :  
[https://arxiv.org/abs/2411.16680](https://arxiv.org/abs/2411.16680)  
project website :  
[https://quark-3d.github.io/](https://quark-3d.github.io/)  
reference :  
Presentation of https://charlieppark.kr from 3D-Nerd Community  

## Contribution

- `Generalizable` :  
recon. from unseen scenes without fine-tuning  

- `Real-time Reconstruction` and Rendering :  
3DGS에서는 real-time rendering이었는데  
본 논문은 recon. 자체도 real-time  
(inference하는 데 총 33ms at 1080p with single A100 GPU)

- Procedure :  
- I/O :  
  - input : sparse multi-view images ($$\in R^{M \times H \times W \times 3}$$)  
  (sensitive to view selection)  
  (pose 정보 필요)  
  - output : novel view image
  - pretrained with 8 input views of scenes(Spaces, RFF, Nex-Shiny, and SWORD)  
  (Quark는 위의 dataset's scene이 아니더라도 unseen scene에 대응 가능! (generalizable))  
    - Spaces : Quark의 직계 조상 격인 DeepView에서 사용한 dataset
    - RFF : NeRF에서 사용한 Real Forward Facing dataset
    - Nex-Shiny : NeX에서 사용한 shiny object이 포함된 dataset
    - SWORD : real-world scene dataset

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-12-23-Quark/3.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

## Related Works

- Generalizable :  
  - IBRNet :  
  rendering 시간은 오래 걸리지만 generalizable  
  - ENeRF :  
  cost volume, depth-guided sampling, volume rendering 사용
  - GPNR :  
  2-view VFT, Epipolar Transformer 사용
  - CO3D - NeRFormer :  
  반복 between attention on feature-dim. and attention on ray-direction-dim.

- Quark의 직계 조상 paper :    
  - DeepView <d-cite key="DeepView">[1]</d-cite> :  
    - MPI (여러 depth에 대해 image를 중첩한 multi-plane image) 
    - 한계 : input view와 target view 간의 camera 이동이 크면 안 됨
  - Immersive light field video with a layered mesh representation <d-cite key="Immersive">[2]</d-cite> :  
    - MSI (여러 depth에 대해 곡면 image를 중첩한 multi-spherical image) (= layered mesh)

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-12-23-Quark/2.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

## Overview

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-12-23-Quark/1.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Architecture :  
U-Net style  
  - Encoder :  
  Obtain feature pyramid $$I_{\downarrow 8}, I_{\downarrow 4}, I_{\downarrow 2}, I_{\downarrow 0}$$
  - Iterative Updates :  
    - pre-trained model을 가져와서 학습하는데,  
    layered depth map을 업데이트하는 방법은  
    gradient descent 이용한 `fine-tuning이 아니라`  
    input view feature 이용한 `refinement`임!!
    - U-Net skip-connection과 비슷하지만 `Update & Fuse 단계가 novel`

- Update & Fuse :  
  - TBD 41:00