---
layout: distill
title: MonST3R
date: 2025-01-22 10:00:00
description: A Simple Approach for Estimating Geometry in the Presence of Motion (ICLR 2025)
tags: dynamic GS background
categories: 3d-view-synthesis
thumbnail: assets/img/2025-01-22-MonST3R/1.PNG
giscus_comments: false
disqus_comments: true
related_posts: true
bibliography: 2025-01-22-MonST3R.bib
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## MonST3R - A Simple Approach for Estimating Geometry in the Presence of Motion

#### Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, Ming-Hsuan Yang

> paper :  
[https://arxiv.org/abs/2410.03825](https://arxiv.org/abs/2410.03825)  
project website :  
[https://monst3r-project.github.io/](https://monst3r-project.github.io/)  

## Contribution

`static scene에 사용됐던 DUSt3R를 dynamic scene에 확장한 버전!`  

- geometry-first approach that `directly` estimates `per-timestep geometry (pointmap)` of `dynamic` scene 
  - 이전까지의 논문들은 <d-cite key="GaussianMarbles">[1]</d-cite>, <d-cite key="TrackRecon">[2]</d-cite>, <d-cite key="Kumar">[3]</d-cite>, <d-cite key="Barsan">[4]</d-cite>, <d-cite key="Mustafa">[5]</d-cite>, <d-cite key="Lei">[6]</d-cite>, <d-cite key="Chu">[7]</d-cite>, <d-cite key="Wangb">[8]</d-cite>, <d-cite key="Wanga">[9]</d-cite>, <d-cite key="Liu">[10]</d-cite> 처럼  
  depth, optical flow, trajectory estimation을 사용하는 subtasks로 쪼갠 뒤  
  global optimization 또는 multi-stage pipeline 등으로 합치는  
  complex system을 쓰는데,  
  이는 보통 느리고, 다루기 힘들고, prone-to-error at each step
  - 이전까지의 논문들은 motion과 geometry를 함께 사용하여 dynamic scene을 다뤘는데,  
  motion, depth label, camera pose 정보가 있는 GT dynamic video data는 거의 없다  
  (그래서 다른 model(prior)를 쓰는데, 이는 부정확성이 쌓일 수 있음)
  - 대신 본 논문은 DUSt3R에서 영감을 받아  
  `limited data`를 최대한 활용하여 (small-scale fine-tuning)  
  `explicit motion representation 없이`  
  only `geometry` (pointmap)를 `directly` 예측하는 pipeline 제시!
  - each timestep마다 DUSt3R 방식으로 pointmap (geometry) 예측한 뒤  
  같은 camera coordinate frame에 대해 `align`
  - downstream tasks :  
  예측한 pointmap (geometry) 를 바탕으로  
  feed-forward 4D reconstruction 뿐만 아니라  
  video depth estimation, camera pose estimation, video segmentation 등  
  여러 downstream video-specific tasks에 적용

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-01-22-MonST3R/2.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

## Related Works

- DUSt3R :  
DUSt3R를 바로 dynamic scene에 적용할 경우 두 가지 한계 발생
  - 문제 1) (static scene인 것처럼) fg object에 align하여 bg가 misaligned  
  DUSt3R는 static scene으로만 학습됐기 때문에  
  dynamic scene의 pointmaps를 알맞게 align하지 못하여  
  moving fg object가 가만히 있는 것처럼 align되고  
  static bg element는 misaligned
  - 문제 2) fg object의 geometry(depth)를 잘 예측하지 못하여 fb object를 bg에 둠  
  - 해결)  
  domain mismatch이므로 다시 train!  
  본 논문은 limited data를 최대한 사용하여 small-scale fine-tuning하는 training strategy 제시

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-01-22-MonST3R/3.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- moving mask :  
DUSt3R는 static scene으로 훈련되어 dynamic scene에 적용하기 위해  
GT moving mask를 사용할 수도 있다  
  - inference할 때  
  image의 dynamic region은 black pixels로 대체하고  
  corresponding tokens는 mask tokens로 대체하여  
  dynamic objects를 masking out 할 수도 있는데,  
  black pixels와 mask tokens는 out-of-distribution w.r.t training 이므로  
  pose estimation 결과가 안 좋아짐
  - 본 논문은 그렇게 무작정 dynamic region을 mask out 하지 않음!

## Architecture

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-01-22-MonST3R/1.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

## Method

### Main Idea

### Training Dataset

### Training Strategy

### Dynamic Global pcd and camera pose

## Downstream Applications

### Intrinsics and Relative Pose Estimation

### Confident Static Regions

## Experiment

### Results

### Ablation Study

## Conclusion

## Question