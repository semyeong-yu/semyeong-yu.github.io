---
layout: distill
title: MonST3R
date: 2025-01-22 10:00:00
description: A Simple Approach for Estimating Geometry in the Presence of Motion (ICLR 2025)
tags: dynamic GS background
categories: 3d-view-synthesis
thumbnail: assets/img/2025-01-22-MonST3R/1.PNG
giscus_comments: false
disqus_comments: true
related_posts: true
bibliography: 2025-01-22-MonST3R.bib
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## MonST3R - A Simple Approach for Estimating Geometry in the Presence of Motion

#### Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, Ming-Hsuan Yang

> paper :  
[https://arxiv.org/abs/2410.03825](https://arxiv.org/abs/2410.03825)  
project website :  
[https://monst3r-project.github.io/](https://monst3r-project.github.io/)  

## Contribution

`static scene에 사용됐던 DUSt3R를 dynamic scene에 확장한 버전!`  

- geometry-first approach that `directly` estimates `per-timestep geometry (pointmap)` of `dynamic` scene 
  - 이전까지의 논문들은 <d-cite key="GaussianMarbles">[1]</d-cite>, <d-cite key="TrackRecon">[2]</d-cite>, <d-cite key="Kumar">[3]</d-cite>, <d-cite key="Barsan">[4]</d-cite>, <d-cite key="Mustafa">[5]</d-cite>, <d-cite key="Lei">[6]</d-cite>, <d-cite key="Chu">[7]</d-cite>, <d-cite key="Wangb">[8]</d-cite>, <d-cite key="Wanga">[9]</d-cite>, <d-cite key="Liu">[10]</d-cite> 처럼  
  depth, optical flow, trajectory estimation을 사용하는 subtasks로 쪼갠 뒤  
  global optimization 또는 multi-stage pipeline 등으로 합치는  
  complex system을 쓰는데,  
  이는 보통 느리고, 다루기 힘들고, prone-to-error at each step
  - 이전까지의 논문들은 motion과 geometry를 함께 사용하여 dynamic scene을 다뤘는데,  
  motion, depth label, camera pose 정보가 있는 GT dynamic video data는 거의 없다  
  (그래서 다른 model(prior)를 쓰는데, 이는 부정확성이 쌓일 수 있음)
  - 대신 본 논문은 DUSt3R에서 영감을 받아  
  `limited data`를 최대한 활용하여 (small-scale fine-tuning)  
  `explicit motion representation 없이`  
  only `geometry` (pointmap)를 `directly` 예측하는 pipeline 제시!
  - each timestep마다 DUSt3R 방식으로 pointmap (geometry) 예측한 뒤  
  같은 camera coordinate frame에 대해 `align`
  - downstream tasks :  
  예측한 pointmap (geometry) 를 바탕으로  
  feed-forward 4D reconstruction 뿐만 아니라  
  video depth estimation, camera pose estimation, video segmentation 등  
  여러 downstream video-specific tasks에 적용

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-01-22-MonST3R/2.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

## Related Works

- DUSt3R :  
DUSt3R를 바로 dynamic scene에 적용할 경우 두 가지 한계 발생
  - 문제 1) (static scene인 것처럼) fg object에 align하여 bg가 misaligned  
  DUSt3R는 static scene으로만 학습됐기 때문에  
  dynamic scene의 pointmaps를 알맞게 align하지 못하여  
  moving fg object가 가만히 있는 것처럼 align되고  
  static bg element는 misaligned
  - 문제 2) fg object의 geometry(depth)를 잘 예측하지 못하여 fb object를 bg에 둠  
  - 해결)  
  domain mismatch이므로 다시 train!  
  본 논문은 limited data를 최대한 사용하여 small-scale fine-tuning하는 training strategy 제시

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-01-22-MonST3R/3.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- moving mask :  
DUSt3R는 static scene으로 훈련되었기 때문에 dynamic scene에 적용하기 위해  
GT moving mask를 사용할 수도 있다  
  - inference할 때  
  image의 dynamic region은 black pixels로 대체하고  
  corresponding tokens는 mask tokens로 대체하여  
  dynamic objects를 masking out 할 수도 있는데,  
  black pixels와 mask tokens는 out-of-distribution w.r.t training 이므로  
  pose estimation 결과가 안 좋아짐
  - 본 논문은 그렇게 무작정 dynamic region을 mask out 하지 않고 이 문제 해결!

## Architecture

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-01-22-MonST3R/1.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

## Method

### Main Idea

DUSt3R의 아이디어를 그대로 가져오고,  
DUSt3R의 각 output pointmap $$X^{t} \in R^{W \times H \times 3}$$ 이 time 정보 $$t$$ 를 가지고 있음

### Training Dataset

real-world dynamic scene은 보통 GT camera pose를 가지고 있지 않으므로  
SfM 등 sensor measurement 또는 post-processing을 통해 추정하는데  
이는 부정확할 수 있고 costly하므로  
본 논문은 GT camera pose, depth 정보를 알 수 있는 synthetic datasets를  
dynamic fine-tuning을 위한 training dataset으로 사용

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-01-22-MonST3R/4.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Training Dataset for Dynamic Fine-Tuning :  
PointOdyssey는 dynamic objects 많아서 많이 사용하고  
TartanAir는 static scene이라서 적게 사용하고  
Waymo는 specialized domain이라서 적게 사용
  - 3 synthetic datasets :  
    - PointOdyssey (Zheng et al.)
    - TartanAir (Wang et al.)
    - Spring (Mehl et al.)
  - 1 real-world dataset :  
    - Waymo (Sun et al.) with LiDAR

### Training Strategy

dataset이 small-scale이므로  
data efficiency를 극대화시키기 위해  
다양한 training techniques 사용

- Training Strategies :  
  - 전략 1)  
  encoder는 freeze한 뒤  
  network의 decoder와 prediction head만 fine-tune  
  (encoder(CroCo)의 geometric knowledge는 유지)
  - 전략 2)  
  each video마다 temporal stride 1~9 만큼 떨어진 two frames를 sampling하여 input pair로 사용하는데,  
  stride가 클수록 sampling prob.도 linearly 큼  
  $$\rightarrow$$  
  서로 더 멀리 떨어진 frame pair, 즉 large motion에 more weights 부여
  - 전략 3)  
  Field-of-View augmentation (center crop with various image scales) 사용하여  
  다양한 camera intrinsics에도 일반화 가능하도록!  
  (training videos에는 해당 variation이 흔하지 않음)

### Dynamic Global pcd and camera pose

TBD

## Downstream Applications

### Intrinsics and Relative Pose Estimation

- Intrinsic Pose Estimation :  
time $$t$$ 에서의 pointmap $$X^{t}$$ 을 이용해서  
2D image와 3D pointmap이 align되도록 하는  
focal length $$f^{t}$$ 를 추정함으로써  
camera intrinsic $$K^{t}$$ 추정

- Relative Pose Estimation :  
DUSt3R와 달리 dynamic objects는  
epipolar matrix 또는 Procrustes alignment를 위한 가정(`???`)들에 위배  
$$\righarrow$$  
대신 주어진 3D point와 corresponding 2D point를 바탕으로 추정하는 PnP algorithm(`???`)과  
random sampling 방식의 RANSAC algorithm(`???`) 사용  
(dynamic scene이어도 대부분의 pixels는 static할 것이므로  
randomly-sampled points는 static elements에 더 가중치를 두기 때문에  
relative pose는 inliers(static)로 robustly estimate 가능)

### Confident Static Regions

- Static Mask :  
단순하게 `두 optical flow field가 일치하는 (차이가 적은) 영역`을 `Static Region`으로 간주!  
static mask $$S^{t \rightarrow t^{'}} = [\alpha \gt \| F_{cam}^{t \rightarrow t^{'}} - F_{est}^{t \rightarrow t^{'}} \|_{1}]$$  
(이 confident static mask를 나중에 global pose optimization에도 사용할 거임!)
  - $$F_{cam}^{t \rightarrow t^{'}}$$ :  
  `camera motion`만으로 optical flow 추정  
    - Step 1)  
    frame pair $$I^{t}$$, $$I^{t^{'}}$$ 로부터  
    pointmaps $$X^{t; t \leftarrow t^{'}}$$, $$X^{t^{'}; t \leftarrow t^{'}}$$ 와 pointmaps $$X^{t; t^{'} \leftarrow t}$$, $$X^{t^{'}; t^{'} \leftarrow t}$$ 추정
    - Step 2)  
    위에서 언급한 방법으로 Intrinsics $$K^{t}, K^{t^{'}}$$ 와 Relative Pose $$R^{t \rightarrow t^{'}}, T^{t \rightarrow t^{'}}$$ 추정
    - Step 3)  
    only camera motion $$t \rightarrow t^{'}$$ 이용해서  
    optical flow field $$F_{cam}^{t \rightarrow t^{'}}$$ 추정  
    $$F_{cam}^{t \rightarrow t^{'}} = \pi (D^{t; t \leftarrow t^{'}} K^{t^{'}} R^{t \rightarrow t^{'}} K^{t}^{-1} \hat x + K^{t^{'}} T^{t \rightarrow t^{'}}) - x$$  
      - notation :  
      $$x$$ : pixel-coordinate  
      $$\hat x$$ : homogeneous coordinate  
      $$\pi(\cdot)$$ : projection operation $$(x,y,z) \rightarrow (\frac{x}{z}, \frac{y}{z})$$  
      $$D^{t; t \leftarrow t^{'}}$$ : estimated depth from pointmap $$X^{t; t \leftarrow t^{'}}$$  
      - Step 3-1)  
      intrinsic 이용하여 frame $$t$$ 에 대해 2D에서 3D로 backproject    
      - Step 3-2)  
      camera motion (relative camera pose) $$t \rightarrow t^{'}$$ 적용
      - Step 3-3)  
      intrinsic 이용하여 frame $$t^{'}$$ 에 대해 다시 3D에서 2D image coordinate으로 project
  - $$F_{est}^{t \rightarrow t^{'}}$$ :  
  `off-the-shelf method` <d-cite key="SEARAFT">[11]</d-cite> 이용해서 optical flow 추정

## Experiment

### Results

### Ablation Study

## Conclusion

## Question