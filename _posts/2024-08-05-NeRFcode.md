---
layout: distill
title: NeRF-Code
date: 2024-08-05 15:00:00
description: NeRF Code Review
tags: nerf rendering 3d
categories: 3d-view-synthesis
thumbnail: assets/img/2024-08-05-NeRFcode/1.png
giscus_comments: false
disqus_comments: true
related_posts: true
# toc:
#   beginning: true
#   sidebar: right
# featured: true
toc:
  - name: Load Data
  - name: Create NeRF Model
  - name: Get Ray with batch
  - name: Get Ray without batch
  - name: Render
  - name: Evaluation
  - name: Question
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis

#### Ben Mildenhall, Pratul P.Srinivasan, Matthew Tancik   

> paper :  
[https://arxiv.org/abs/2003.08934](https://arxiv.org/abs/2003.08934)  
project website :  
[https://www.matthewtancik.com/nerf](https://www.matthewtancik.com/nerf)  
pytorch code :  
[https://github.com/yenchenlin/nerf-pytorch](https://github.com/yenchenlin/nerf-pytorch)  
[https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file](https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file)  
tiny tensorflow code :  
[https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb](https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb)  
Overview image reference :  
[https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#dataflow](https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#dataflow)

### Train Code Flow Overview

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### Load Data

- load data :  
  - load_llff.py
  - load_blender.py
  - load_LINEMOD.py
  - load_deepvoxels.py

#### load_llff_data()  

- LLFF dataset : real dataset  
return images, poses, bds, render_poses, i_test  
  - images : np (N, H, W, C)
  - poses : np (N, 3, 5)  
  camera poses  
  poses[:, 0:3, 0:3] : 3-by-3 rotation matrix  
  poses[:, 0:3, 3:4] : 3-by-1 translation matrix  
  poses[:, 0:3, 4:5] : H, W, focal-length for intrinsic matrix 
  - bds : np (N, 2)  
  scene bounds  
  dim=1 : 2 = 1(near bound) + 1(far bound)  
  - render_poses : np (M, 3, 5)  
  dim=0 : the number of generated poses for novel view synthesis  
  generate new pose along sphere or spiral path
  - i_test : int  
  index of holdout-view (avg pose랑 가장 비슷한 pose를 갖는 view)  
  training에서 제외하여 test할 때 사용  
  - near, far = 0., 1. if ndc is true else near, far = 0.9 * bds.min(), 1. * bds.max()

#### load_blender_data()

- Blender dataset : synthetic dataset  
return images, poses, render_poses, hwf, i_split  
  - images : np (N, H, W, C)  
  blender dataset은 RGB-A channel을 가지고 있어 C = 4  
  - i_train, i_val, i_test = i_split
  - near, far = 2., 6.  
  (blender synthetic dataset은 통제된 환경에서 수집된 data이므로 ndc 사용하지 않고 frustum의 near, far plane 고정)  
  - 투명한 배경을 흰 배경으로 만들려면  
  RGB * opacity + (1 - opacity) 를 통해  
  RGB 값을 opacity만큼 반영하고 opacity가 작을수록(투명할수록) 색상이 흰색(1.)에 가까워지도록 함  
  images = images[...,:3]*images[...,-1:] + (1.-images[...,-1:])  
  - 그냥 투명한 배경 그대로 쓰려면  
  RGB-A channel에서 RGB channel만 가져와서 씀  
  images = images[...,:3]

#### load_LINEMOD_data()

- LINEMOD dataset : real dataset  
return images, poses, render_poses, hwf, K, i_split, near, far

#### load_dv_data()

- Deepvoxels dataset : synthetic dataset  
return images, poses, render_poses, hwf, i_split  
  - near, far = hemi_R - 1., hemi_R + 1.  
  where hemi_R = np.mean(np.linalg.norm(poses[:,:3,-1], axis=-1))  
  camera center들로 이루어진 반구의 평균 반지름

### Create NeRF Model

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/4.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- args.N_importance : fine-MLP에서 추가적으로 사용할 fine-sample 개수  
  - args.N_importance > 0 : fine-MLP 사용함
  - args.N_importance <= 0 : fine-MLP 사용 안함

- network_query_fn : 추후에 run_network() 사용하기 위한 함수  
  - input : position info., view-direction info., model  
  - output : model output

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/10.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- render_kwargs_train : dict for rendering  
  - network_query_fn : 추후에 run_network() 사용하기 위한 함수
  - perturb : 일반화 위해 stratified ray-sampling할 때 randomness 추가할지 여부  
  (test할 때는 False)
  - network_fine, network_fn : fine-MLP, coarse-MLP
  - N_importance, N_samples : number of fine-sampling, coarse-sampling
  - white_bkgd : rendering에서 alpha-channel 사용할 때 투명한 부분이 흰색으로 채워지도록 할지 여부
  - raw_noise_std : 일반화 위해 raw2ouputs()에서 model output 중 opacity에 추가할 noise의 std값  
  (test할 때는 0.)
  - lindisp :  
    - NDC를 사용하는 front-unbounded llff dataset의 경우 lindisp = False로 설정하여  
    linearly sampling in depth, 즉 depth를 균등하게 sampling하여  
    먼 거리의 scene도 적절히 표현  
    - NDC를 사용하지 않는 나머지 dataset의 경우 lindisp = True로 설정하여  
    linearly sampling in inverse-depth, 즉 가까운 depth를 더 많이 sampling하여  
    가까운 scene의 디테일을 잘 포착  

#### Positional Encoding

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- get_embedder() input :  
PE freq. 개수 $$L$$ 과 PE 쓸지말지 여부
- get_embedder() output :  
PE-function과 PE 결과의 dim.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/3.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- self.embed_fns :  
각 frequency($$0 \sim 2^{L-1}$$)와 각 period function($$sin, cos$$)에 대한  
list of lambda functions  
$$[sin(2^0x), cos(2^0x), \ldots sin(2^{L-1}x), cos(2^{L-1}x)]$$  
- Embedder.embed(x) :  
self.embed_fns의 각 PE-function을 input x에 적용하여 dim=-1에 대해 concat

#### NeRF model

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/5.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- input_ch : position info. dim. : 3  
- input_ch_views : view-direction info. dim. : 3  
- use_viewdirs : MLP input으로 view-direction info.를 사용할지 말지 여부  
(view-direction info.를 사용하면 RGB color 계산에 도움됨)
- output_ch : output(RGB, opacity) dim. : 4 
use_viewdirs가 False일 때만 사용하는 값  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/6.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- input x를 position info.와 view-direction info.로 쪼갬
- self.use_viewdirs가 True일 때(view-direction info. 사용할 때) :  
position info.만 넣어서 opacity를 뽑은 뒤  
view-direction info.를 추가로 넣어서 RGB 뽑고  
dim=-1에 대해 concat  
- self.use_viewdirs가 False일 때(view-direction info. 사용 안 할 때) :  
position info.만 넣어서 output_ch만큼 한 번에 뽑음

#### run_network

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/9.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/7.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- flatten position and flatten view-direction $$\rightarrow$$ each positional encoding and concat $$\rightarrow$$ batchify model and apply model $$\rightarrow$$ reshape again output

#### batchify

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/8.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- input이 주어지면 chunk만큼씩 쪼개서 적용하는 model 반환

### Get Ray with batch

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/11.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- rays : shape (N, 2, H, W, 3)  
  - dim=1 : rays_o, rays_d
  - dim=2, 3 : for H*W개의 pixels
  - dim=4 : 3d
- rays_rgb : shape (N, 3, H, W, 3) after concat with images  
  - dim=1 : rays_o, rays_d, images
- rays_rgb : shape (N, H, W, 3, 3) $$\rightarrow$$ (N_train, H, W, 3, 3) $$\rightarrow$$ (N_train * H * W, 3, 3) $$\rightarrow$$ shuffle along dim=0  
  - dim=0 : the number of rays(pixels)
  - dim=1 : rays_o, rays_d, images
  - dim=2 : 3d for rays and rgb for images

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/14.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- batch :  N_train * H * W 개의 ray를 batch size = N_rand-개씩 묶어서 전부 사용  
shape (N_train * H * W, 3, 3) $$\rightarrow$$ shape (N_rand, 3, 3) $$\rightarrow$$ (3, N_rand, 3)  
- batch_rays : shape (2, N_rand, 3)  
  - dim=0 : rays_o, rays_d  
  - dim=1 : the number of rays
- target_s : shape (N_rand, 3)  
  - dim=0 : the number of pixels  
  - dim=1 : target pixel RGB  
- shuffle rays_rgb by torch.randperm() for every epoch

#### get_rays_np

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/12.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- parameter :  
K : intrinsic matrix of shape (3, 3)  
c2w : extrinsic matrix of shape (3, 4)  
- line 1 :  
np.meshgrid([0, ..., W-1], [0, ..., H-1], indexing='xy')  
  - indexing='xy' : 첫 번째 array를 row-방향으로 반복하고, 두 번째 array를 column-방향으로 반복  
  - i, j : both shape (H, W) : 2D-pixel-coordinate (x, y)
- line 2 :  
  - apply intrinsic matrix  
  [NeRF-Blog](https://semyeong-yu.github.io/blog/2024/NeRF/) 의 Ray from input image (pre-processing) 참고
  - dirs : shape (H, W, 3) : 2D-normalized-coordinate
- line 4 :  
  - apply extrinsic matrix to calculate ray-direction
  - dirs[..., np.newaxis, :] : shape (H, W, 1, 3) $$\rightarrow$$ (H, W, 3, 3) by broad-casting  
  - c2w[:3, :3] : shape (3, 3) $$\rightarrow$$ (H, W, 3, 3) by broad-casting  
  - ray_d : shape (H, W, 3)  
  "elementwise-multiplication 후 sum"은 "matrix-multiplication"과 동일한 계산

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/13.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- line 6 :  
  - apply extrinsic matrix to calculate ray-origin  
  - rays_o : shape (3,) $$\rightarrow$$ (H, W, 3) by broad-casting

### Get Ray without batch

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/15.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- 차이점 :  
Get Ray with batch에서는 N_train * H * W 개의 ray를 batch size = N_rand-개씩 묶어서 전부 사용했다면  
Get Ray without batch에서는 N_train 중 training view 하나를 randomly 고른 뒤 H * W 개의 ray 중 N_rand-개를 randomly 골라서 사용  

- target : shape (N, H, W, C) $$\rightarrow$$ (H, W, C)  
$$\rightarrow$$ target_s : shape (N_rand, C)  
- coords : H * W 개의 ray를 H-axis와 W-axis에서 인덱싱하기 위해 meshgrid of shape (H, W, 2) 생성  
  - 초반부 iter. : 중심부 crop해서 meshgrid of shape (2 * dH, 2 * dW, 2) 생성  
  - 후반부 iter. : meshgrid of shape (H, W, 2) 생성  
  - dim=2 : coords[:, :, 0]은 H-coord이고, coords[:, :, 1]은 W-coord  
- select_coords : shape (N_rand, 2)  
H * W 개의 ray 중 N_rand-개를 randomly 고름  
- batch_rays : shape (2, N_rand, 3)  
- target_s : shape (N_rand, 3)


### Render

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/16.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/17.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- input :  
  - chunk : 동시에 처리할 수 있는 최대 ray 수 (due to maximum memory usage)
  - c2w_staticcam : view-direction의 영향을 확인하고자 할 때 사용  
  기존 c2w는 view-direction MLP input 만드는 데만 사용하고  
  c2w_staticcam으로 rendering 위한 rays_o, rays_d 다시 계산  
- output :  
  - rgb_map : shape (B, 3)  
  predicted RGB values for B개의 rays  
  - disp_map : shape (B,)  
  disparity map (inverse of depth)  
  - acc_map : shape (B,)  
  sum of sample weights along each ray
  - extras : 나머지 dict from render_rays()  
  fine-MLP를 사용하는 경우에만 존재  
    - rgb0, disp0, acc0 : from coarse-MLP  
    - z_std : shape (B,)  
    std of distances of fine samples for each ray 

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/19.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- rays :  
  - if use_viewdirs = True : shape (N_rand, 8)  
  dim=1 : 3(rays_o) + 3(rays_d) + 1(near) + 1(far)  
  - if use_viewdirs = False : shape (N_rand, 11)  
  dim=1 : 3(rays_o) + 3(rays_d) + 1(near) + 1(far) + 3(viewdirs)  

- all_ret : shape (`?????`)

#### ndc_rays

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/18.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- shift ray origin to near plane :  
NDC를 적용하기 전에 3D ray origin $$o$$ 을 near plane 위 $$o_n$$ 으로 옮긴다  
(world-coordinate에서 ray가 near plane에서 출발하도록)  
by $$o_n = o + t_nd$$  
where z-axis에서는 $$-n = o_z + t_nd_z$$ 이므로 $$t_n = \frac{-(n+o_z)}{d_z}$$  
where n은 argument(near)

- project ray to NDC-space :  
ray $$r = o_n + td$$ 를 NDC로 projection했을 때  
projected ray $$r^{\ast} = o^{\ast} + t^{\ast} d^{\ast}$$ 에서  
$$o^{\ast} = \begin{bmatrix} -\frac{f_{cam}}{\frac{W}{2}}\frac{o_{n_x}}{o_{n_z}} \\ -\frac{f_{cam}}{\frac{H}{2}}\frac{o_{n_y}}{o_{n_z}} \\ 1 + \frac{2n}{o_{n_z}} \end{bmatrix}$$ where n은 argument(near)  
and  
$$t^{\ast} = \frac{td_z}{o_{n_z} + td_z} = 1 - \frac{o_{n_z}}{o_{n_z} + td_z}$$  
and  
$$d^{\ast} = \begin{bmatrix} -\frac{f_{cam}}{\frac{W}{2}}(\frac{d_x}{d_z} - \frac{o_{n_x}}{o_{n_z}}) \\ -\frac{f_{cam}}{\frac{H}{2}}(\frac{d_y}{d_z} - \frac{o_{n_y}}{o_{n_z}}) \\ -2n\frac{1}{o_{n_z}} \end{bmatrix}$$ where n은 argument(near)  

#### batchify_rays

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/20.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Out-of-Memory를 방지하기 위해 N_rand-개의 rays를 더 작은 chunk (B개)로 쪼개서 rendering  
- ret : `????`
- all_ret : `????`

#### render_rays

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/21.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- ray_batch of shape (B, 8) or (B, 11)로부터  
rays_o, rays_d, near, far, viewdirs 분리  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/22.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Stratified Sampling of distance $$t$$ for coarse-MLP :  
z_vals : shape (B, N_samples) = (N_rays, N_samples)  
stratified sampled distance $$t$$  
  - Let 균등한 간격을 나타내는 $$t_{vals} \in [0, 1]$$ has shape (N_samples,)
  - if lindisp = False:  
  sample linearly in depth  
  $$z_{vals} = near \cdot (1-t_{vals}) + far \cdot (t_{vals})$$
  - if lindisp = True:  
  sample linearly in inverse-depth  
  $$z_{vals} = \frac{1}{\frac{1}{near} \cdot (1-t_{vals}) + \frac{1}{far} \cdot (t_{vals})}$$  
  - if perturb = True:  
  add randomness  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/25.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- pts, viewdirs : coarse-MLP input  
  - pts : position info. $$r = o + td$$ of shape (B, N_samples, 3)  
  - viewdirs : view-direction info. of shape (B, 3)

- raw : coarse-MLP output  
shape (B, N_samples, 4) where 4 : for RGB, opacity

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/23.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Inverse-transform Sampling of distance $$t$$ for fine-MLP :  
  - coarse-samples :  
  z_vals : shape (B, N_samples) = (N_rays, N_samples)  
  - fine-samples :  
  coarse-MLP의 MLP output raw에 대해 raw2outputs()로 구한 weights 값을 Fine-Sampling에 사용  
  z_samples : shape (B, N_importance)
  - total sorted samples for fine-MLP :  
  z_vals : shape (B, N_samples + N_importance)

- pts, viewdirs : fine-MLP input  
  - pts : position info. $$r = o + td$$ of shape (B, N_samples + N_importance, 3)  
  - viewdirs : view-direction info. of shape (B, 3)

- raw : fine-MLP output  
shape (B, N_samples + N_importance, 4) where 4 : RGB, opacity

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/24.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- render_rays() output : dict  
  - rgb_map : shape (B, 3)  
  predicted RGB values by alpha-compositing  
  - disp_map : shape (B,)  
  disparity map (inverse of depth)  
  - acc_map : shape (B,)  
  sum of sample weights along each ray  
  - raw : MLP raw output (raw2outputs() 안 한 것)  
  - rgb0, disp0, acc0 : from coarse-MLP  
  - z_std : shape (B,)  
  std of distances of fine-samples for each ray 

#### raw2outputs

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/26.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- input :  
  - raw : shape (B, num_samples, 4)  

- dists : shape (B, num_samples)  
sample 간의 간격  
(dists[:, -1]은 마지막 sample부터 inf까지의 간격을 의미하는 매우 큰 수 1e10)

- rgb : `???`

- alpha : `???`

- output :  
  - rgb_map : shape (B, 3)  
  predicted RGB values  
  - disp_map : shape (B,)  
  disparity map (inverse of depth) 
  - acc_map : shape (B,)  
  sum of sample weights along each ray  
  - weights : shape (B, num_samples)  
  weight of each sample  
  - depth_map : shape (B,)  
  depth map (estimated distance to object)

#### raw2alpha

#### sample_pdf

### Evaluation

#### img2mse for loss

#### mse2psnr for psnr

#### render_path


### Question

- Q1 : 왜 ndc_rays() 호출할 때 near = 1.로 하드코딩해서 넣어주지?
- A1 : `????`