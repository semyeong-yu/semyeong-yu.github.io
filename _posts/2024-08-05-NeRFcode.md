---
layout: distill
title: NeRF-Code
date: 2024-08-05 15:00:00
description: NeRF Code Review
tags: nerf rendering 3d
categories: 3d-view-synthesis
thumbnail: assets/img/2024-08-05-NeRFcode/1.png
giscus_comments: true
related_posts: true
# toc:
#   beginning: true
#   sidebar: right
# featured: true
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis

#### Ben Mildenhall, Pratul P.Srinivasan, Matthew Tancik   

> paper :  
[https://arxiv.org/abs/2003.08934](https://arxiv.org/abs/2003.08934)  
project website :  
[https://www.matthewtancik.com/nerf](https://www.matthewtancik.com/nerf)  
pytorch code :  
[https://github.com/yenchenlin/nerf-pytorch](https://github.com/yenchenlin/nerf-pytorch)  
[https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file](https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file)  
tiny tensorflow code :  
[https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb](https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb)  
Overview image reference :  
[https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#background](https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#background)  

### Train Code Flow Overview

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### Load Data

- load data :  
  - load_llff.py
  - load_blender.py
  - load_LINEMOD.py
  - load_deepvoxels.py

#### load_llff_data()  

- LLFF dataset : real dataset  
return images, poses, bds, render_poses, i_test  
  - images : np (N, H, W, C)
  - poses : np (N, 3, 5)  
  camera poses  
  poses[:, 0:3, 0:3] : 3-by-3 rotation matrix  
  poses[:, 0:3, 3:4] : 3-by-1 translation matrix  
  poses[:, 0:3, 4:5] : H, W, focal-length for intrinsic matrix 
  - bds : np (N, 2)  
  scene bounds  
  dim=1 : 2 = 1(near bound) + 1(far bound)  
  - render_poses : np (M, 3, 5)  
  dim=0 : the number of generated poses for novel view synthesis  
  generate new pose along sphere or spiral path
  - i_test : int  
  index of holdout-view (avg pose랑 가장 비슷한 pose를 갖는 view)  
  training에서 제외하여 test할 때 사용  
  - near, far = 0., 1. if ndc is true else near, far = 0.9 * bds.min(), 1. * bds.max()

#### load_blender_data()

- Blender dataset : synthetic dataset  
return images, poses, render_poses, hwf, i_split  
  - images : np (N, H, W, C)  
  blender dataset은 RGB-A channel을 가지고 있어 C = 4  
  - i_train, i_val, i_test = i_split
  - near, far = 2., 6.  
  (blender synthetic dataset은 통제된 환경에서 수집된 data이므로 ndc 사용하지 않고 frustum의 near, far plane 고정)  
  - 투명한 배경을 흰 배경으로 만들려면  
  RGB * opacity + (1 - opacity) 를 통해  
  RGB 값을 opacity만큼 반영하고 opacity가 작을수록(투명할수록) 색상이 흰색(1.)에 가까워지도록 함  
  images = images[...,:3]*images[...,-1:] + (1.-images[...,-1:])  
  - 그냥 투명한 배경 그대로 쓰려면  
  RGB-A channel에서 RGB channel만 가져와서 씀  
  images = images[...,:3]

#### load_LINEMOD_data()

- LINEMOD dataset : real dataset  
return images, poses, render_poses, hwf, K, i_split, near, far

#### load_dv_data()

- Deepvoxels dataset : synthetic dataset  
return images, poses, render_poses, hwf, i_split  
  - near, far = hemi_R - 1., hemi_R + 1.  
  where hemi_R = np.mean(np.linalg.norm(poses[:,:3,-1], axis=-1))  
  camera center들로 이루어진 반구의 평균 반지름

### Create NeRF Model

render_kwargs_train, render_kwargs_test, start, grad_vars, optimizer = create_nerf(args)

#### get_embedder

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>


#### NeRF

```python
def get_rays_np(H, W, K, c2w):
    i, j = torch.meshgrid(torch.linspace(0, W-1, W), torch.linspace(0, H-1, H)) # pytorch's meshgrid has indexing='ij', so both i and j have shape (W, H)
    i = i.t() # width grid : shape (H, W)
    j = j.t() # height grid : shape (H, W)

    # Apply intrinsic matrix
    dirs = torch.stack([(i-K[0][2])/K[0][0], -(j-K[1][2]) / K[1][1], -torch.ones_like(i)], -1)
    # dirs : shape (H, W, 3) : H*W개의 3D rays
    
    # Apply extrinsic matrix
    # Rotate ray directions from camera frame to the world frame by applying dot product
    rays_d = torch.sum(dirs[..., np.newaxis, :] * c2w[:3, :3], -1) # same with "rays_d = [c2w.dot(dir) for dir in dirs]"
    # dirs[..., np.newaxis, :] : shape (H, W, 1, 3) -> (H, W, 3, 3) by broadcasting 
    # c2w[:3, :3] : shape (3, 3) -> (H, W, 3, 3) by broadcasting
    # rays_d : shape (H, W, 3)
    
    # Translate camera frame's origin to the world frame. It is the origin of all rays.
    rays_o = c2w[:3, -1].expand(rays_d.shape)
    # rays_o : shape (3, H*W) -> (H, W, 3)
    return rays_o, rays_d
```