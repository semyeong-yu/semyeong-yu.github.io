---
layout: distill
title: NeRF-Code
date: 2024-08-05 15:00:00
description: NeRF Code Review
tags: nerf rendering 3d
categories: 3d-view-synthesis
thumbnail: assets/img/2024-08-05-NeRFcode/1.png
giscus_comments: true
related_posts: true
# toc:
#   beginning: true
#   sidebar: right
# featured: true
toc:
  - name: Load Data
  - name: Create NeRF Model
  - name: Get Ray
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis

#### Ben Mildenhall, Pratul P.Srinivasan, Matthew Tancik   

> paper :  
[https://arxiv.org/abs/2003.08934](https://arxiv.org/abs/2003.08934)  
project website :  
[https://www.matthewtancik.com/nerf](https://www.matthewtancik.com/nerf)  
pytorch code :  
[https://github.com/yenchenlin/nerf-pytorch](https://github.com/yenchenlin/nerf-pytorch)  
[https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file](https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file)  
tiny tensorflow code :  
[https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb](https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb)  
Overview image reference :  
[https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#dataflow](https://yconquesty.github.io/blog/ml/nerf/nerf_ndc.html#dataflow)

### Train Code Flow Overview

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### Load Data

- load data :  
  - load_llff.py
  - load_blender.py
  - load_LINEMOD.py
  - load_deepvoxels.py

#### load_llff_data()  

- LLFF dataset : real dataset  
return images, poses, bds, render_poses, i_test  
  - images : np (N, H, W, C)
  - poses : np (N, 3, 5)  
  camera poses  
  poses[:, 0:3, 0:3] : 3-by-3 rotation matrix  
  poses[:, 0:3, 3:4] : 3-by-1 translation matrix  
  poses[:, 0:3, 4:5] : H, W, focal-length for intrinsic matrix 
  - bds : np (N, 2)  
  scene bounds  
  dim=1 : 2 = 1(near bound) + 1(far bound)  
  - render_poses : np (M, 3, 5)  
  dim=0 : the number of generated poses for novel view synthesis  
  generate new pose along sphere or spiral path
  - i_test : int  
  index of holdout-view (avg pose랑 가장 비슷한 pose를 갖는 view)  
  training에서 제외하여 test할 때 사용  
  - near, far = 0., 1. if ndc is true else near, far = 0.9 * bds.min(), 1. * bds.max()

#### load_blender_data()

- Blender dataset : synthetic dataset  
return images, poses, render_poses, hwf, i_split  
  - images : np (N, H, W, C)  
  blender dataset은 RGB-A channel을 가지고 있어 C = 4  
  - i_train, i_val, i_test = i_split
  - near, far = 2., 6.  
  (blender synthetic dataset은 통제된 환경에서 수집된 data이므로 ndc 사용하지 않고 frustum의 near, far plane 고정)  
  - 투명한 배경을 흰 배경으로 만들려면  
  RGB * opacity + (1 - opacity) 를 통해  
  RGB 값을 opacity만큼 반영하고 opacity가 작을수록(투명할수록) 색상이 흰색(1.)에 가까워지도록 함  
  images = images[...,:3]*images[...,-1:] + (1.-images[...,-1:])  
  - 그냥 투명한 배경 그대로 쓰려면  
  RGB-A channel에서 RGB channel만 가져와서 씀  
  images = images[...,:3]

#### load_LINEMOD_data()

- LINEMOD dataset : real dataset  
return images, poses, render_poses, hwf, K, i_split, near, far

#### load_dv_data()

- Deepvoxels dataset : synthetic dataset  
return images, poses, render_poses, hwf, i_split  
  - near, far = hemi_R - 1., hemi_R + 1.  
  where hemi_R = np.mean(np.linalg.norm(poses[:,:3,-1], axis=-1))  
  camera center들로 이루어진 반구의 평균 반지름

### Create NeRF Model

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/4.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- args.N_importance : 추가적으로 fine-MLP에서 사용할 sample 개수  
  - args.N_importance > 0 : fine-MLP 사용함
  - args.N_importance <= 0 : fine-MLP 사용 안함

- network_query_fn : 추후에 run_network() 사용하기 위한 함수  
  - input : position info., view-direction info., model  
  - output : model output

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/10.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- render_kwargs_train : dict for rendering  
  - network_query_fn : 추후에 run_network() 사용하기 위한 함수
  - perturb : 일반화 위해 ray-sampling할 때 작은 난수를 추가할지 여부  
  (test할 때는 False)
  - network_fine, network_fn : fine-MLP, coarse-MLP
  - N_importance, N_samples : number of sampels for fine-MLP, coarse-MLP
  - white_bkgd : rendering에서 alpha-channel 사용할 때 투명한 부분이 흰색으로 채워지도록 할지 여부
  - raw_noise_std : 일반화 위해 model output 중 opacity에 추가할 noise의 std값  
  (test할 때는 0.)
  - lindisp :  
    - NDC를 사용하는 llff dataset의 경우 lindisp = False로 설정하여  
    먼 거리의 scene도 적절히 표현하기 위해 depth를 균등하게 sampling  
    - NDC를 사용하지 않는 나머지 dataset의 경우 lindisp = True로 설정하여  
    가까운 scene의 디테일을 잘 포착하기 위해 가까운 depth를 더 많이 sampling

#### Positional Encoding

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- get_embedder() input :  
PE freq. 개수 $$L$$ 과 PE 쓸지말지 여부
- get_embedder() output :  
PE-function과 PE 결과의 dim.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/3.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- self.embed_fns :  
각 frequency($$0 \sim 2^{L-1}$$)와 각 period function($$sin, cos$$)에 대한  
list of lambda functions  
$$[sin(2^0x), cos(2^0x), \ldots sin(2^{L-1}x), cos(2^{L-1}x)]$$  
- Embedder.embed(x) :  
self.embed_fns의 각 PE-function을 input x에 적용하여 dim=-1에 대해 concat

#### NeRF

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/5.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- input_ch : position info. dim.
- input_ch_views : view-direction info. dim.
- use_viewdirs : view-direction info.를 사용할지 말지 여부  
(view-direction info.를 사용하면 RGB color 계산에 도움됨)
- output_ch :  
use_viewdirs가 False일 때만 사용하는 값  
args.N_importance > 0일 때(fine-MLP 사용할 때)는 5 (RGB, opacity, `?????`)  
args.N_importance <= 0일 때(fine-MLP 사용 안 할 때)는 4 (RGB, opacity)  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/6.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- input x를 position info.와 view-direction info.로 쪼갬
- self.use_viewdirs가 True일 때(view-direction info. 사용할 때) :  
position info.만 넣어서 opacity를 뽑은 뒤  
view-direction info.를 추가로 넣어서 RGB 뽑고  
dim=-1에 대해 concat  
- self.use_viewdirs가 False일 때(view-direction info. 사용 안 할 때) :  
position info.만 넣어서 output_ch만큼 한 번에 뽑음

#### run_network

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/9.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/7.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- flatten position and flatten view-direction $$\rightarrow$$ positional encoding $$\rightarrow$$ batchify model and apply model $$\rightarrow$$ reshape again output

#### batchify

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/8.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- input이 주어지면 chunk만큼씩 쪼개서 적용하는 model 반환

### Get Ray

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/11.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- rays_rgb : ddd

#### get_rays_np

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-05-NeRFcode/12.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- parameter :  
K : intrinsic matrix of shape (3, 3)  
c2w : extrinsic matrix of shape (3, 4)  
- line 1 :  
  - indexing='xy' : 첫 번째 array를 row-방향으로 반복하고, 두 번째 array를 column-방향으로 반복  
  - i, j : both shape (H, W) : 2D-pixel-coordinate (x, y)
- line 2 :  
  - apply intrinsic matrix  
  [NeRF-Blog](https://semyeong-yu.github.io/blog/2024/NeRF/) 의 Ray from input image (pre-processing) 참고
  - dirs : shape (H, W, 3) : 2D-normalized-coordinate
- line 4 :  
  - apply extrinsic matrix to calculate ray-direction
  - dirs[..., np.newaxis, :] : shape (H, W, 1, 3) $$\rightarrow$$ (H, W, 3, 3) by broad-casting  
  - c2w[:3, :3] : shape (3, 3) $$\rightarrow$$ (H, W, 3, 3) by broad-casting  
  - ray_d : shape (H, W, 3)  
  "elementwise-multiplication 후 sum"은 "matrix-multiplication"과 동일한 계산
- line 6 :  
  - apply extrinsic matrix to calculate ray-origin  
  - rays_o : shape (3,) $$\rightarrow$$ (H, W, 3) by broad-casting