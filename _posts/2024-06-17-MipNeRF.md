---
layout: distill
title: MipNeRF
date: 2024-06-17 21:00:00
description: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields
tags: nerf rendering 3d multiscale antialiasing
categories: 3d-view-synthesis
thumbnail: assets/img/2024-06-17-MipNeRF/1.png
giscus_comments: true
related_posts: true
bibliography: 2024-06-17-MipNeRF.bib
# toc:
#   beginning: true
#   sidebar: right
featured: true
toc:
  - name: Introduction
  - name: Related Work
    subsections:
      - name: Anti-aliasing in Rendering
      - name: Scene Representations for View Synthesis
  - name: Method
    subsections:
      - name: Cone Tracing and Positional Encoding
      - name: Architecture
  - name: Results
  - name: Conclusion
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields

#### Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan  

> paper :  
[https://arxiv.org/abs/2103.13415](https://arxiv.org/abs/2103.13415)  
project website :  
[https://jonbarron.info/mipnerf/](https://jonbarron.info/mipnerf/)  
pytorch code :  
[https://github.com/bebeal/mipnerf-pytorch](https://github.com/bebeal/mipnerf-pytorch)  
[https://github.com/google/mipnerf](https://github.com/google/mipnerf)

> 핵심 요약 :  
1. ddd

## Introduction

- 기존 NeRF의 문제점 :  
rendering 위해 sampling할 때 `single ray` per pixel 쏴서 composite 하므로  
dataset images에 있는 물체의 크기가 일정하지 않을 때 (multiple resolutions)  
가까이 합성했을 때는 `blurry` rendering  
멀리 합성했을 때는 `aliased` rendering  
그렇다고 multiple rays per pixel through its footprint로 brute-force supersampling하는 것은 정확하긴 하겠지만 too costly 비현실적  

- Minmapping Approach :  
컴퓨터 그래픽스 rendering에서 aliasing을 없애기 위한 `pre-filtering` 기법  
본 논문인 Mip-NeRF가 여기서 영감을 얻음  
signal(e.g. image)을 diff. downsampling scales로 나타낸 뒤 pixel footprint를 근거로 ray에 사용하기 위한 `적절한 scale을 고른다`  
render time에 할 복잡할 일을 precomputation phase에 미리 하는 것일 뿐이긴 하지만, 주어진 texture마다 한 번만 minmap을 만들면 된다는 장점이 있다  

- Mip-NeRF :  
  - represent pre-filtered scene at `continuous space of multi-scales`  
  - ray 대신 `conical frustum` 사용해서 `anti-aliased` rendering with fine details  
  - multiscale variant of dataset에 대해 평균 error rate 60% 감소  
  - NeRF가 hierarchical sampling을 위해 coarse and fine MLP를 분리했다면, Mip-NeRF는 `scale-aware`하므로 `single MLP만으로 충분`  
  따라서 NeRF보다 7% 빠르고, param. 수는 절반

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-06-17-MipNeRF/1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- 차이 : 기존 NeRF는 `a single point`를 encode하고, Mip-NeRF는 `a region of space`를 encode  

- 기존 NeRF :  
camera center로부터 각 pixel로 ray를 하나 쏜 다음 point sampling한 뒤 positional encoding  

- Mip-NeRF :  
camera center로부터 각 pixel로 3D conical frustum을 쏜 다음, 3D point 및 그 주위의 Gaussian region을 encode하기 위해 IPE(integrated positional encoding)  
IPE : the `frustum을 multi-variate Gaussian으로 근사`한 뒤, Gaussian 내 좌표를 positional encoding한 것에 대해 `integral` $$E \left[ \gamma (x) \right]$$ 계산  

## Related Work
 
#### Anti-aliasing in Rendering

`anti-aliasing`에는 두 가지 방법이 있다  

1. `supersampling` :  
  - rendering할 때 `multiple rays per pixel`을 쏴서 Nyquist frequency에 가깝게 supersampling  
  - `expensive` as runtime increases linearly with the supersampling rate, so used only in `offline` rendering  

2. `pre-filtering` :  
  - target sampling rate에 맞춰서 Nyquist frequency를 줄이기 위해 scene에 `lowpass-filter`를 씌운 버전 사용  
  - scene을 미리 downsampling `multi-scales` (sparse voxel octree 또는 minmap)로 나타낸 뒤, `ray 대신 cone`을 추적하여 cone과 scene이 만나는 곳의 cone's footprint에 대응되는 `적절한 scale`을 골라서 사용 (target sampling rate에 맞는 적절한 scale)  
  - scene에 filter 씌운 버전을 한 번만 미리 계산하면 되므로, better for `real-time` rendering  

Our work takes inspiration from ... 읽을 차례

#### Scene Representations for View Synthesis

dd
  
## Method

#### Cone Tracing and Positional Encoding

dd

#### Architecture

dd

## Results

dd

## Conclusion

dd