---
layout: distill
title: EE534 Pattern Recognition Final
date: 2024-10-28 11:00:00
description: Lecture Summary (24F)
tags: cv
categories: cv-tasks
thumbnail: assets/img/2024-09-10-Pattern/0.png
giscus_comments: false
disqus_comments: true
related_posts: true
# toc:
#   beginning: true
#   sidebar: right
# featured: true
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

> Lecture :  
24F EE534 Pattern Recognition  
by KAIST Munchurl Kim [VICLab](https://www.viclab.kaist.ac.kr/)  

## Chapter 6. Linear Discriminant Functions

### Linearly Non-Separable SVM

- new constraint :  
$$y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}$$  
$$\xi_{i}$$ 를 도입하여 이제는 inside margin or misclassified 도 가능하지만 대신 $$C \sum_{i=1}^{N} \xi_{i}$$ 를 loss에 넣어서 큰 $$\xi_{i}$$ 값을 penalize
  - $$\xi = 0$$ : outside margin or support vector
  - $$0 \lt \xi \leq 1$$ : inside margin (correctly classified, but margin violation)
  - $$\xi \gt 1$$ : misclassified

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-10-28-Pattern2/2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- 방법 1) 1-norm-soft-margin  
  - constrained primal form :  
  minimize $$J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \xi_{i}$$  
  subject to $$y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}$$ and $$\xi_{i} \geq 0$$  
    - unconstrained primal form :  
    이 때 위의 두 가지 constraints는 $$\xi_{i} = \text{max}(0, 1 - y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}))$$ 로 하나로 합칠 수 있음  
    따라서  
    minimize $$J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \text{max}(0, 1 - y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}))$$  
    - regularization param. $$C$$ :  
      - small $$C$$ : 큰 $$\xi_{i}$$ 값도 허용하므로 margin 커짐
      - large $$C$$ : 큰 $$\xi_{i}$$ 값은 허용 안 하므로 margin 작아짐
      - $$C = \infty$$ : non-zero $$\xi_{i}$$ 값 허용 안 하므로 hard margin (no sample inside margin)  
      (Linearly Separable SVM 에 해당함)
  - Lagrangian :  
  minimize $$L(\boldsymbol w, w_{0}, \xi, \boldsymbol \lambda, \boldsymbol \mu) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \xi_{i} - \sum_{i}^{N} \mu_{i} \xi_{i} - \sum_{i}^{N} \lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (1 - \xi_{i}))$$  
  subject to $$\xi_{i}, \mu_{i}, \lambda_{i} \geq 0$$  
    - $$\nabla_{\boldsymbol w} L = 0 \rightarrow \boldsymbol w = \sum_{i=1}^{N} \lambda_{i} y_{i} \boldsymbol x_{i}$$
    - $$\nabla_{w_{0}} L = 0 \rightarrow \sum_{i=1}^{N} \lambda_{i} y_{i} = 0$$
    - $$\nabla_{\xi_{i}} L = 0 \rightarrow C - \mu_{i} - \lambda_{i} = 0$$
  - KKT condition 중 slackness condition :  
    - $$\lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (1 - \xi_{i})) = 0$$  
    - $$\mu_{i} \xi_{i} = 0$$
  - dual form :  
  위의 세 가지 식을 대입하여 $$\boldsymbol w, w_{0}, \xi_{i}, \mu_{i}$$ 를 소거하면  
  maximize $$L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}$$  
  subject to $$\sum_{i=1}^{N} \lambda_{i} y_{i} = 0$$ and $$0 \leq \lambda_{i} \leq C$$
  - Summary :  
    - Step 1) optimal $$\lambda_{i}^{\ast}$$ 구하기  
    $$\sum_{i=1}^{N} \lambda_{i} y_{i} = 0$$ and $$0 \leq \lambda_{i} \leq C$$ 이용해서  
    $$\nabla_{\lambda_{i}} L = 0$$ 으로 아래의 dual form 풀어서  
    (maximize $$L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}$$)  
    optimal $$\lambda_{i}$$ 얻음  
    - Step 2) optimal $$\boldsymbol w^{\ast}, w_{0}^{\ast}$$ 구하기  
      - $$\boldsymbol w^{\ast} = \sum_{i=1}^{N_{s}} \lambda_{i}^{\ast} y_{i} x_{i}$$  
      ($$N_{s}$$ : support vector 개수)
      - $$w_{0}^{\ast} = \frac{1}{y_{j}} - \sum_{i=1}^{N_{s}} \lambda_{i}^{\ast} y_{i} x_{i}^{T} x_{j} = \frac{1}{y_{j}} - \boldsymbol w^{\ast T} x_{j}$$  
      (support vector $$x_{j}$$ 1개 사용)  
      또는  
      $$w_{0}^{\ast} = \frac{1}{N_{s}} \sum_{j=1}^{N_{s}} (\frac{1}{y_{j}} - \sum_{i=1}^{N_{s}} \lambda_{i}^{\ast} y_{i} x_{i}^{T} x_{j}) = \frac{1}{N_{s}} \sum_{j=1}^{N_{s}} (\frac{1}{y_{j}} - \boldsymbol w^{\ast T} x_{j})$$  
      (support vector $$x_{j}$$ $$N_{s}$$-개 모두 사용하여 average value)  
    - Tip : hard margin (no sample inside margin) 의 경우  
    육안으로 어떤 sample이 support vector일지 판단 가능하다면  
    complementary slackness condition ($$\lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (1 - \xi_{i})) = 0$$) 에서  
    support vector만 $$\lambda_{i} \gt 0$$ 이므로  
    연립해서 $$\boldsymbol w^{\ast}, w_{0}^{\ast}$$ 바로 구할 수 있음

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-10-28-Pattern2/1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- 방법 2) 2-norm-soft-margin  
  - 차이점 1) primal form  
  minimize $$J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \xi_{i}$$  
  subject to $$y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}$$ and $$\xi_{i} \geq 0$$  
  대신  
  minimize $$J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + \frac{1}{2} C \sum_{i=1}^{N} \xi_{i}^{2}$$  
  subject to $$y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}$$ and $$\xi_{i} \geq 0$$  
  - 차이점 2) Lagrangian  
  $$\nabla_{\xi_{i}} L(\boldsymbol w, w_{0}, \xi, \boldsymbol \lambda, \boldsymbol \mu) = 0$$ 했을 때  
  $$C - \mu_{i} - \lambda_{i} = 0$$  
  대신  
  $$C \xi_{i} - \mu_{i} - \lambda_{i} = 0$$
  - 차이점 3) dual form  
  maximize $$L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}$$  
  subject to $$\sum_{i=1}^{N} \lambda_{i} y_{i} = 0$$ and $$0 \leq \lambda_{i} \leq C$$  
  대신  
  maximize $$L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j} - \frac{1}{2C} \sum_{i=1}^{N} (\lambda_{i} + \mu_{i})^{2}$$  
  subject to $$\sum_{i=1}^{N} \lambda_{i} y_{i} = 0$$ and $$0 \leq \lambda_{i} \leq C$$  

- Remark :  
  - Linearly Non-Separable SVM에서  
  $$C \rightarrow \infty$$ 하면 Linearly Separable SVM  
  e.g. non-linear에서는 $$0 \leq \lambda_{i} \leq C$$ 인데, linear에서는 $$0 \leq \lambda_{i} \lt \infty$$
  - SVM의 한계 :  
  high computational complexity  
  (SVM training은 주로 batch mode로 진행되어 memory를 많이 필요로 하므로  
  training dataset을 subset으로 나눠서 training 진행)
  - 지금까지는 SVM for two-category만 살펴봤는데,  
  M-class 의 경우 M개의 discriminant function $$g_{i}(x)$$ 를 design하여  
  assign $$x$$ to class $$w_{i}$$ if $$i = \text{argmax}_{k} g_{k}(x)$$

### v-SVM

- v-SVM :  
  - 차이점 1) hyperplane  
  $$\boldsymbol w^{T} \boldsymbol x_{i} + w_{0} = \pm 1$$  
  대신  
  $$\boldsymbol w^{T} \boldsymbol x_{i} + w_{0} = \pm \rho$$  
  where $$\rho \geq 0$$ : var. to be optimized
  - 차이점 2) margin  
  margin은 $$\frac{2 \rho}{\| w \|}$$ 이므로  
  margin을 maximize하려면  
  $$\| w \|$$ minimize 뿐만 아니라 $$\rho$$ maximize하면 되므로  
  이를 primal form loss term에 넣음
  - 차이점 3) primal form  
  minimize $$J(\boldsymbol w, \xi, \rho) = \frac{1}{2} \| \boldsymbol w \|^{2} - v \rho + C \sum_{i=1}^{N} \xi_{i}$$  
  subject to $$y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq \rho - \xi_{i}$$ and $$\xi_{i} \geq 0$$ and $$\rho \geq 0$$
  - 차이점 4) Lagrangian  
  TBD 59p  

## Chapter 6. Multilayer Neural Networks

### Introduction

### Back-propagation Algorithm

### Issues on Neural Networks