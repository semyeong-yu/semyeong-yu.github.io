---
layout: distill
title: EE534 Pattern Recognition Final
date: 2024-10-28 11:00:00
description: Lecture Summary (24F)
tags: cv
categories: cv-tasks
thumbnail: assets/img/2024-09-10-Pattern/0.png
giscus_comments: false
disqus_comments: true
related_posts: true
# toc:
#   beginning: true
#   sidebar: right
# featured: true
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

> Lecture :  
24F EE534 Pattern Recognition  
by KAIST Munchurl Kim [VICLab](https://www.viclab.kaist.ac.kr/)  

## Chapter 6. Linear Discriminant Functions

### Linearly Non-Separable SVM

- new constraint :  
$$y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}$$  
$$\xi_{i}$$ 를 도입하여 이제는 inside margin or misclassified 도 가능하지만 대신 $$C \sum_{i=1}^{N} \xi_{i}$$ 를 loss에 넣어서 큰 $$\xi_{i}$$ 값을 penalize
  - $$\xi = 0$$ : outside margin or support vector
  - $$0 \lt \xi \leq 1$$ : inside margin (correctly classified, but margin violation)
  - $$\xi \gt 1$$ : misclassified

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-10-28-Pattern2/2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- 방법 1) 1-norm-soft-margin  
  - constrained primal form :  
  minimize $$J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \xi_{i}$$  
  subject to $$y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}$$ and $$\xi_{i} \geq 0$$  
    - unconstrained primal form :  
    이 때 위의 두 가지 constraints는 $$\xi_{i} = \text{max}(0, 1 - y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}))$$ 로 하나로 합칠 수 있음  
    따라서  
    minimize $$J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \text{max}(0, 1 - y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}))$$  
    - regularization param. $$C$$ :  
      - small $$C$$ : 큰 $$\xi_{i}$$ 값도 허용하므로 margin 커짐
      - large $$C$$ : 큰 $$\xi_{i}$$ 값은 허용 안 하므로 margin 작아짐
      - $$C = \infty$$ : non-zero $$\xi_{i}$$ 값 허용 안 하므로 hard margin (no sample inside margin)  
      (Linearly Separable SVM 에 해당함)
  - Lagrangian :  
  minimize $$L(\boldsymbol w, w_{0}, \xi, \boldsymbol \lambda, \boldsymbol \mu) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \xi_{i} - \sum_{i}^{N} \mu_{i} \xi_{i} - \sum_{i}^{N} \lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (1 - \xi_{i}))$$  
  subject to $$\xi_{i}, \mu_{i}, \lambda_{i} \geq 0$$  
    - $$\nabla_{\boldsymbol w} L = 0 \rightarrow \boldsymbol w = \sum_{i=1}^{N} \lambda_{i} y_{i} \boldsymbol x_{i}$$
    - $$\nabla_{w_{0}} L = 0 \rightarrow \sum_{i=1}^{N} \lambda_{i} y_{i} = 0$$
    - $$\nabla_{\xi_{i}} L = 0 \rightarrow C - \mu_{i} - \lambda_{i} = 0$$
  - KKT condition 중 slackness condition :  
    - $$\lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (1 - \xi_{i})) = 0$$  
    - $$\mu_{i} \xi_{i} = 0$$
  - dual form :  
  위의 세 가지 식을 대입하여 $$\boldsymbol w, w_{0}, \xi_{i}, \mu_{i}$$ 를 소거하면  
  maximize $$L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}$$  
  subject to $$\sum_{i=1}^{N} \lambda_{i} y_{i} = 0$$ and $$0 \leq \lambda_{i} \leq C$$
  - Summary :  
    - Step 1) optimal $$\lambda_{i}^{\ast}$$ 구하기  
    $$\sum_{i=1}^{N} \lambda_{i} y_{i} = 0$$ and $$0 \leq \lambda_{i} \leq C$$ 이용해서  
    $$\nabla_{\lambda_{i}} L = 0$$ 으로 아래의 dual form 풀어서  
    (maximize $$L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}$$)  
    optimal $$\lambda_{i}$$ 얻음  
    - Step 2) optimal $$\boldsymbol w^{\ast}, w_{0}^{\ast}$$ 구하기  
      - $$\boldsymbol w^{\ast} = \sum_{i=1}^{N_{s}} \lambda_{i}^{\ast} y_{i} x_{i}$$  
      ($$N_{s}$$ : support vector 개수)  
      (hyperplane 결정할 때는 $$\lambda_{i} \gt 0$$ 중에 $$\xi = 0$$ 인 support vectors만 고려!!)  
      - $$w_{0}^{\ast} = \frac{1}{y_{j}} - \sum_{i=1}^{N_{s}} \lambda_{i}^{\ast} y_{i} x_{i}^{T} x_{j} = \frac{1}{y_{j}} - \boldsymbol w^{\ast T} x_{j}$$  
      (support vector $$x_{j}$$ 1개 사용)  
      또는  
      $$w_{0}^{\ast} = \frac{1}{N_{s}} \sum_{j=1}^{N_{s}} (\frac{1}{y_{j}} - \sum_{i=1}^{N_{s}} \lambda_{i}^{\ast} y_{i} x_{i}^{T} x_{j}) = \frac{1}{N_{s}} \sum_{j=1}^{N_{s}} (\frac{1}{y_{j}} - \boldsymbol w^{\ast T} x_{j})$$  
      (support vector $$x_{j}$$ $$N_{s}$$-개 모두 사용하여 average value)  
    - Tip : hard margin (no sample inside margin) 의 경우  
    육안으로 어떤 sample이 support vector일지 판단 가능하다면  
    complementary slackness condition ($$\lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (1 - \xi_{i})) = 0$$) 에서  
    support vector만 $$\lambda_{i} \gt 0$$ 이므로  
    연립해서 $$\boldsymbol w^{\ast}, w_{0}^{\ast}$$ 바로 구할 수 있음

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-10-28-Pattern2/1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- 방법 2) 2-norm-soft-margin  
  - 차이점 1) primal form  
  minimize $$J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + C \sum_{i=1}^{N} \xi_{i}$$  
  subject to $$y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}$$ and $$\xi_{i} \geq 0$$  
  대신  
  minimize $$J(\boldsymbol w, \xi) = \frac{1}{2} \| \boldsymbol w \|^{2} + \frac{1}{2} C \sum_{i=1}^{N} \xi_{i}^{2}$$  
  subject to $$y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq 1 - \xi_{i}$$ and $$\xi_{i} \geq 0$$  
  - 차이점 2) Lagrangian  
  $$\nabla_{\xi_{i}} L(\boldsymbol w, w_{0}, \boldsymbol \xi, \boldsymbol \lambda, \boldsymbol \mu) = 0$$ 했을 때  
  $$C - \mu_{i} - \lambda_{i} = 0$$  
  대신  
  $$C \xi_{i} - \mu_{i} - \lambda_{i} = 0$$
  - 차이점 3) dual form  
  maximize $$L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}$$  
  subject to $$\sum_{i=1}^{N} \lambda_{i} y_{i} = 0$$ and $$0 \leq \lambda_{i} \leq C$$  
  대신  
  maximize $$L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j} - \frac{1}{2C} \sum_{i=1}^{N} (\lambda_{i} + \mu_{i})^{2}$$  
  subject to $$\sum_{i=1}^{N} \lambda_{i} y_{i} = 0$$ and $$0 \leq \lambda_{i} \leq C$$  

- Remark :  
  - Linearly Non-Separable SVM에서  
  $$C \rightarrow \infty$$ 하면 Linearly Separable SVM  
  e.g. non-linear에서는 $$0 \leq \lambda_{i} \leq C$$ 인데, linear에서는 $$0 \leq \lambda_{i} \lt \infty$$
  - SVM의 한계 :  
  high computational complexity  
  (SVM training은 주로 batch mode로 진행되어 memory를 많이 필요로 하므로  
  training dataset을 subset으로 나눠서 training 진행)
  - 지금까지는 SVM for two-category만 살펴봤는데,  
  M-class 의 경우 M개의 discriminant function $$g_{i}(x)$$ 를 design하여  
  assign $$x$$ to class $$w_{i}$$ if $$i = \text{argmax}_{k} g_{k}(x)$$

### v-SVM

- v-SVM :  
  - hyperplane  
  $$\boldsymbol w^{T} \boldsymbol x_{i} + w_{0} = \pm 1$$  
  대신  
  $$\boldsymbol w^{T} \boldsymbol x_{i} + w_{0} = \pm \rho$$  
  where $$\rho \geq 0$$ : var. to be optimized
  - margin  
  margin은 $$\frac{2 \rho}{\| w \|}$$ 이므로  
  margin을 maximize하려면  
  $$\| w \|$$ minimize 뿐만 아니라 $$\rho$$ maximize하면 되므로  
  둘 다 primal form loss term에 넣음
  - primal form  
  minimize $$J(\boldsymbol w, \xi, \rho) = \frac{1}{2} \| \boldsymbol w \|^{2} - v \rho + \frac{1}{N} \sum_{i=1}^{N} \xi_{i}$$  
  subject to $$y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) \geq \rho - \xi_{i}$$ and $$\xi_{i} \geq 0$$ and $$\rho \geq 0$$
  - Lagrangian  
  $$L(\boldsymbol w, w_{0}, \boldsymbol \xi, \rho, \boldsymbol \lambda, \boldsymbol \mu, \delta) = \frac{1}{2} \| \boldsymbol w \|^{2} - v \rho + \frac{1}{N} \sum_{i=1}^{N} \xi_{i} - \sum_{i}^{N} \mu_{i} \xi_{i} - \sum_{i}^{N} \lambda_{i} (y_{i} (\boldsymbol w^{T} \boldsymbol x_{i} + w_{0}) - (\rho - \xi_{i})) - \delta \rho$$  
    - $$\nabla_{\boldsymbol w} L = 0$$ 했을 때  
    $$\boldsymbol w = \sum_{i=1}^{N} \lambda_{i} y_{i} \boldsymbol x_{i}$$
    - $$\nabla_{w_{0}} L = 0$$ 했을 때  
    $$\sum_{i=1}^{N} \lambda_{i} y_{i} = 0$$
    - $$\nabla_{\xi_{i}} L = 0$$ 했을 때  
    $$\mu_{i} + \lambda_{i} = \frac{1}{N}$$
    - $$\nabla_{\rho} L = 0$$ 했을 때  
    $$\sum_{i=1}^{N} \lambda_{i} - \delta = v$$
  - KKT condition 중 complementary slackness  
  For $$\lambda_{i} \geq 0$$ and $$\mu_{i} \geq 0$$ and $$\delta \geq 0$$,  
    - $$\lambda_{i} (y_{i}(\boldsymbol w^{T} \boldsymbol x + w_{0}) - (\rho - \xi_{i})) = 0$$
    - $$\mu_{i} \xi_{i} = 0$$
    - $$\delta \rho = 0$$
  - dual form  
  maximize $$L(\lambda) = - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}$$  
  subject to $$\sum_{i=1}^{N} \lambda_{i} y_{i} = 0$$ and $$0 \leq \lambda_{i} \leq \frac{1}{N}$$ and $$\sum_{i=1}^{N} \lambda_{i} = \delta + v \geq v$$  
    - $$\lambda$$ 만 explicitly 남아 있고,  
    margin var. $$\rho$$ 와 slack var. $$\xi_{i}$$ 는 constraint의 bounds에 implicitly 들어 있음
    - v-SVM에서는 $$\sum_{i=1}^{N} \lambda_{i}$$ term이 없으므로  
    optimal $$\lambda_{i}$$ 는 quadratically homogeneous solution
    - 새로운 constraint $$\sum_{i=1}^{N} \lambda_{i} \geq v$$ 있음

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-10-28-Pattern2/3.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Remark
  - v-SVM의 경우 $$0 \leq v \leq 1$$ 이어야 optimizable
  - C-SVM에 비해 v-SVM은  
  error rate와 support vector 수 bound 측면에서 장점 `???`
  - $$\rho \gt 0$$ 일 때 $$\delta = 0$$ 이므로  
  $$\sum_{i=1}^{N} \lambda_{i} = v$$
  - loss (error)에 기여하는 애들은  
  $$\xi_{i} \gt 0$$, 즉 $$\mu_{i} = 0$$, 즉 $$\lambda_{i} = \frac{1}{N}$$ 이다  
  따라서 error rate = $$\sum_{i=1}^{N_{error}} \lambda_{i} = \frac{N_{error}}{N} \leq \sum_{i=1}^{N} \lambda_{i} = v$$  
  즉, error rate $$\frac{N_{error}}{N} \leq v$$ 이고  
  total number errors $$N_{error} \leq N v$$
  - Since $$0 \lt \lambda_{i} \lt 1$$ for support vector $$i$$,  
  $$v = \sum_{i=1}^{N} \lambda_{i} = \sum_{i=1}^{N_{s}} \lambda_{i} \leq \sum_{i=1}^{N_{s}} \frac{1}{N} = \frac{N_{s}}{N}$$  
  즉, $$vN \leq N_{s}$$  
  ($$\lambda_{i} \gt 0$$ 중에 $$\xi = 0$$ 인 support vectors만 고려하면 $$\sum_{i=1}^{N} \lambda_{i} = \sum_{i=1}^{N_{s}}$$ !!)  
  - $$\frac{N_{error}}{N} \leq v \leq \frac{N_{s}}{N}$$ 이므로  
  $$v$$ optimize하면 error rate와 support vector 개수도 bound 알 수 있음
  - support vector 수 $$N_{s}$$ 는 classifier performance에 있어서 매우 중요  
  ($$N_{s}$$ 가 클수록 inner product 많이 계산해야 돼서 computational cost 높아짐)  
  ($$N_{s}$$ 가 크면 training set 이외의 data에 대한 performance가 제한되어 poor generalization)

### Kernel Method for SVM

- discriminant function :  
$$x$$ 의 inner product 꼴  
$$g(\boldsymbol x) = \boldsymbol w^{T} \boldsymbol x + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \boldsymbol x_{i}^{T} \boldsymbol x + w_{0}$$

- Cover's theorem :  
non-linearly separable D-dim. space는  
linearly separable space of high enough dim. 으로 transform 될 수 있다  
(separating hyperplane의 optimality는 관심사 아님)  

- Kernel Method for SVM :  
discriminant function $$g(\boldsymbol x) = \boldsymbol w^{T} \boldsymbol x + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \boldsymbol x_{i}^{T} \boldsymbol x + w_{0}$$ 에서  
kernel $$K(\boldsymbol x_{i}, \boldsymbol x) = \boldsymbol x_{i}^{T} \boldsymbol x$$  
(inner product b.w. support vector and input vector)  
대신  
다른 kernel $$K(\boldsymbol x_{i}, \boldsymbol x) = \Phi(\boldsymbol x_{i})^{T} \Phi(\boldsymbol x)$$ 을 써서  
non-linearly separable samples도 분류해보자!  
  - Step 1)  
  input vector $$\boldsymbol x$$ 와 training samples $$\boldsymbol x_{i}$$ 를 `high-dim.으로 project` by function $$\Phi(\cdot)$$  
  - Step 2)  
  transformed vector $$\Phi (\boldsymbol x)$$ 와 $$\Phi (\boldsymbol x_{i})$$ 에 대해 linear SVM 적용  
  $$g(\boldsymbol x) = \boldsymbol w^{T} \Phi (\boldsymbol x) + w_{0}$$  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-10-28-Pattern2/4.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- `Kernel Trick` :  
$$\boldsymbol x_{i}^{T} \boldsymbol x_{j}$$ 대신 $$K(\boldsymbol x_{i}, \boldsymbol x_{j}) = \Phi(\boldsymbol x_{i})^{T} \Phi(\boldsymbol x_{j})$$ 쓰면 됨!!
  - optimization of dual form :  
  maximize $$L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \boldsymbol x_{i}^{T} \boldsymbol x_{j}$$   
  대신  
  maximize $$L(\lambda) = \sum_{i=1}^{N} \lambda_{i} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} K(\boldsymbol x_{i}, \boldsymbol x_{j})$$  
  where $$$K(\boldsymbol x_{i}, \boldsymbol x_{j}) = \Phi(\boldsymbol x_{i})^{T} \Phi(\boldsymbol x_{j})$$
  - hyperplane :  
  $$g(\boldsymbol x) = \boldsymbol w^{T} \boldsymbol x + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \boldsymbol x_{i}^{T} \boldsymbol x + w_{0} = 0$$  
  대신  
  $$g(\boldsymbol x) = \boldsymbol w^{T} \Phi(\boldsymbol x) + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \Phi(\boldsymbol x_{i})^{T} \Phi(\boldsymbol x) + w_{0} = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} K(\boldsymbol x_{i}, \boldsymbol x) + w_{0} = 0$$  
  where $$\boldsymbol w = \sum_{i=1}^{N_{s}} \lambda_{i} y_{i} \Phi(\boldsymbol x_{i})$$  

- Remark :  
  - polynomial learning machine, radial-basis function network, two-layer perceptron(single hidden layer) 와 같은  
  kernel-based learning machine을 만들 때  
  support vector learning algorithm을 사용
    - polynomial :  
    $$K(x, z) = (x^{T} z + 1)^{q}$$ for $$q \gt 0$$  
    - radial-basis function :  
    $$K(x, z) = \text{exp}(-\frac{\| x - z \|^{2}}{\sigma^{2}})$$  
    - hyperbolic tangent :  
    $$K(x, z) = \text{tanh}(\beta x^{T} z + \gamma)$$ where typical value is $$\beta = 2$$ and $$\gamma = 1$$

- 문제 풀이 예시 :  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-10-28-Pattern2/6.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-10-28-Pattern2/5.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-10-28-Pattern2/7.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-10-28-Pattern2/8.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-10-28-Pattern2/9.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

## Chapter 6. Multilayer Neural Networks

- activation function :  
  - sigmoid : $$\phi (x) = \frac{1}{1 + exp(-x)}$$
  - tanh : $$\phi (x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$
  - ReLU

### Back-propagation Algorithm

TBD

### Issues on Neural Networks