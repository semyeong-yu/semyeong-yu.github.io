---
layout: distill
title: Feed-Forward Bullet-Time Reconstruction
date: 2025-01-10 12:00:00
description: Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos (CVPR 2025)
tags: general dynamic GS view synthesis
categories: 3d-view-synthesis
thumbnail: assets/img/2025-01-10-BTimer/1.PNG
giscus_comments: false
disqus_comments: true
related_posts: true
bibliography: 2025-01-10-BTimer.bib
toc:
  - name: Contribution
  - name: Introduction
  - name: Related Works
  - name: Overview
  - name: Method
  subsections:
    - name: BTimer
    - name: NTE Module
    - name: Curriculum Training
  - name: Experiment
  - name: Ablation Study
  - name: Conclusion
  - name: Question

_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos

#### Hanxue Liang, Jiawei Ren, Ashkan Mirzaei, Antonio Torralba, Ziwei Liu, Igor Gilitschenski, Sanja Fidler, Cengiz Oztireli, Huan Ling, Zan Gojcic, Jiahui Huang

> paper :  
[https://arxiv.org/abs/2412.03526](https://arxiv.org/abs/2412.03526)  
project website :  
[https://research.nvidia.com/labs/toronto-ai/bullet-timer/](https://research.nvidia.com/labs/toronto-ai/bullet-timer/)  

## Contribution

- model :  
  - `motion-aware` `feed-forward` model for real-time recon. and novel-view-synthesis of `dynamic` scenes  
  - obtain `scalability` and `generalization` by using both static and dynamic scene datasets  
  (static and dynamic recon.에 모두 사용 가능)  
  - Procedure :  
    - Step 1) pre-train on large static scene dataset
    - Step 2) video duration or FPS에 구애받지 않고 scale effectively across datasets
    - Step 3) output multi-view volumetric video representation
  - recon. a bullet-time scene within 150ms with SOTA performance on a single GPU  
  from 12 context frames of $$256 \times 256$$ resolution  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-01-10-BTimer/2.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- `BulletTimer` (Novelty 1.) :  
bullet-time embedding  
  - recon. at given `target (bullet) timestamp`  
  by adding `bullet-time embedding` to context (input) frames  
  and `aggregating` pred. from all the context frames  
  
- `NTE Module` (Novelty 2.) :  
Novel Time Enhancer  
  - `fast motion`에 대응하기 위해  
  model에 feed하기 전에  
  `intermediate (interpolated) frames를 predict`  

## Introduction

- Dynamic scene recon. from monocular video :  
still challenging  
due to inherently ill-posed (해가 무수히 많음) nature of dynamic recon. from limited observations

- Static scene recon. :  
  - optimization-based (per-scene) :  
  NeRF, HyperNeRF
  - learning-based (feed-forward) :  
  MonoNeRF, GS-LRM

- Dynamic scene recon. :  
dynamic scene은 complex motion 때문에 ambiguity 존재  
이를 해소하는 데 도움될 data prior 필요  
  - optimization-based (per-scene) :  
    - use contraint (data prior) like depth and optical flow <d-cite key="33">[1]</d-cite>, <d-cite key="36">[2]</d-cite>, <d-cite key="37">[3]</d-cite>, <d-cite key="68">[4]</d-cite>  
    $$\rightarrow$$  
    given data와 위의 data prior 간의 싱크를 맞추는 게 challenging <d-cite key="34">[5]</d-cite>, <d-cite key="63">[6]</d-cite>  
    - per-scene approach는 time-consuming and thus scale 어렵
  - learning-based (`feed-forward`) :  
    - directly predict recon. in feed-forward manner  
    so, can `learn strong inherent prior directly from data` <d-cite key="7">[7]</d-cite>, <d-cite key="10">[8]</d-cite>, <d-cite key="12">[9]</d-cite>, <d-cite key="25">[10]</d-cite>, <d-cite key="53">[11]</d-cite>
    - 근데 dynamic scene 모델링하기 복잡하고 4D supervision data 부족해서 적용하는데 한계 있었음  
    - 지금 시점 기준 L4GM <d-cite key="53">[11]</d-cite> 이 유일한 feed-forward dynamic recon. model인데,  
    synthetic object-centric dataset으로 훈련돼서  
    fixed camera view-point와 multi-view supervision을 필요로 한다는 한계와  
    real-world scene에 generalize하기 어렵다는 한계가 있었음

- Feed-Forward Dynamic scene recon. :  
  - 본 논문은  
  `pixel-aligned 3DGS` <d-cite key="79">[12]</d-cite> 를 기반으로  
  novel BulletTimer and NTE module 제안  

## Related Works

- Dynamic 3D Representation :  
  - TBD

- Novel-View-Synthesis :  
  - TBD

- Feed-Forward Reconstruction :  
  - TBD

## Overview

- notation :  
context frames $$I_{c} \subset I$$  
camera poses $$P_{c} \subset P$$  
context timestamps $$T_{c} \subset T$$  
bullet timestamp $$t_{b} \in [\text{min}(T_{c}), \text{max}(T_{c})]$$  
recon. at timestamp $$t \notin T$$ by NTE module

## Method

### BTimer

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-01-10-BTimer/3.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Model Design :  
  - encode :  
  $$i$$-th frame $$I_{i} \in I_{c}$$ 을 $$8 \times 8$$ 짜리 patches로 나눈 뒤  
  $$j$$-th patch에 대해  
  per-patch input token $$f_{ij} \|_{j=1}^{HW / 64} = f_{ij}^{rgb} + f_{ij}^{pose} + f_{i}^{time}$$ 만든 뒤  
  concatenate input tokens from all context frames  
  and feed into Transformer  
    - `image` encoder :  
    GS-LRM <d-cite key="79">[12]</d-cite> 에서 영감을 받아,  
    `ViT` model을 backbone으로 사용
    - `camera pose` encoder :  
    `camera Plucker embedding` <d-cite key="70">[13]</d-cite>
    - `time` encoder :  
    PE (Positional Encoding) 및 linear layer를 거쳐  
    $$t_{i}$$ 와 $$t_{b}$$ 를 각각 $$f_{i}^{ctx}$$ 와 $$f_{i}^{bullet}$$ 으로 encode한 뒤  
    $$f_{i}^{time} = f_{i}^{ctx} + f_{i}^{bullet}$$  
      - `context (input)` timestamp $$t_{i}$$ from context (input) frame $$I_{i}$$  
      - `bullet (target)` timestamp $$t_{b}$$ that is `shared` across context (input) frames
  - decode :  
  transformer의 per-patch output token $$f_{ij}^{out}$$ 을  
  `per-patch 3DGS param. at bullet timestamp` $$G_{ij} \in R^{8 \times 8 \times 12}$$ 로 decode  
    - each Gaussian has 12 param. as color $$c \in R^{3}$$, scale $$s \in R^{3}$$, rotation unit quaternion $$q \in R^{4}$$, opacity $$\sigma \in R$$, and ray distance $$\tau \in R$$  
    - 3D position is obtained by pixel-aligned unprojection $$\mu = o + \tau d$$  
    ($$o$$ and $$d$$ are obtained from camera pose $$P_{i}$$)
  
- Loss :  
$$L_{RGB} = L_{MSE} + \lambda L_{LPIPS}$$ with $$\lambda = 0.5$$  

- Timestamp :  
context (input) frames와 bullet (target supervision) frame 을 잘 고르는 게 중요
  - `In-context Supervision` :  
    - bullet timestamp is randomly selected from context frames  
    $$t_{b} \in T_{c}$$
    - model이 context timestamp에 대해 정확히 recon. 가능하도록
  - `Interpolation Supervision` :  
    - bullet timestamp lies between two adjacent context frames  
    $$t_{b} \notin T_{c}$$
    - model이 dynamic parts를 interpolate할 수 있도록  

- Inference :  
  - bullet (target) timestamp $$t_{b}$$ 를 원하는 each timestamp in video로 설정하면  
  full video를 recon.할 수 있으므로  
  각 frame을 `parallel`하게 recon. 가능  
  - For a video longer than the number of training context views $$\| I_{c} \|$$,  
  at timestamp $$t$$, apart from including this exact timestamp and setting $$t_{b} = t$$,  
  we uniformly distribute the remaining $$\| I_{c} \| − 1$$ required context frames across the whole duration of the video  
  to form the input batch with $$\| I_{c} \|$$ frames

### NTE Module

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-01-10-BTimer/4.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- NTE Module Design :  
  - TBD

- BTimer and NTE Module :  
  - TBD

### Curriculum Training at Scale

motion awareness 및 temporal consistency 보장하기 위해  
both static and dynamic scene dataset를 포함하는 learning curriculum 설계

- TBD

## Experiment

## Ablation Study

## Conclusion

## Question

- Q1 :  
TBD

- A1 :  
TBD