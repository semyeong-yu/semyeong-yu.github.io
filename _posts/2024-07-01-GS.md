---
layout: distill
title: Gaussian Splatting
date: 2024-07-01 10:00:00
description: 3D GS for Real-Time Radiance Field Rendering
tags: gaussian splatting rendering 3d view synthesis
categories: 3d-view-synthesis
thumbnail: assets/img/2024-07-01-GS/1.png
giscus_comments: true
related_posts: true
bibliography: 2024-07-01-GS.bib
# toc:
#   beginning: true
#   sidebar: right
featured: true
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## 3D Gaussian Splatting for Real-Time Radiance Field Rendering

#### Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, George Drettakis

> paper :  
[https://arxiv.org/abs/2308.04079](https://arxiv.org/abs/2308.04079)  
project website :  
[https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/)  
code :  
[https://github.com/graphdeco-inria/gaussian-splatting](https://github.com/graphdeco-inria/gaussian-splatting)  
referenced blog :  
[https://xoft.tistory.com/51](https://xoft.tistory.com/51)


> 핵심 요약 :  
1. DD

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-01-GS/2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

## Abstract

- novel 3D Gaussian scene representation with real-time differentiable renderer  
`수많은 3D Gaussian이 모여 scene을 구성`하고 있다!
- Very Fast rendering ($$\geq$$ 100 FPS)
- Higher Quality than SOTA Mip-NeRF360(2022)
- Faster Training than SOTA InstantNGP(2022)

## Introduction

### Why 3D Gaussian?

3D scene representation 방법  
1. `Mesh or Point`  
  - explicit  
  - good for fast GPU/CUDA-based rasterization(3D $$\rightarrow$$ 2D)  
2. `NeRF` method  
  - implicit (MLP)  
  - ray marching  
  - continuous coordinate-based representation  
  - interpolate values stored in voxels, hash grids, or points  
  - But,,, `stochastic sampling` for rendering 때문에 `연산량이 많고 noise` 생김  
3. `3D Gaussian` method  
  - explicit  
  - differentiable volumetric representation  
  - efficient rasterization(projection and $$\alpha$$-blending)  

### Rendering (NeRF vs 3DGS)

- NeRF :  
  - ray per pixel 쏴서 coarse(stratified) and fine(PDF) sampling하고,  
  - MLP로 sampled points의 color 및 volume density를 구하고,  
  - 이 값들을 volume rendering 식으로 summation  
- 3DGS :  
  - image를 tile(14 $$\times$$ 14 pixel)들로 나누고,  
  - tile마다 Gaussian을 Depth에 따라 정렬한 뒤  
  - 앞에서부터 뒤로 $$\alpha$$-blending

## Related Work

생략 (추후에 다시 볼 수도)

## Overview

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-01-GS/1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

For unbounded and complete scenes,  
For 1080p high resolution and real-time($$\geq$$ 30 fps) rendering,  

1. `input` :  
  - Most point-based methods require `MVS`(Multi-View Stereo) data,  
  but 3DGS only needs `SfM points` for initialization  
  - COLMAP 등 SfM(Structure-from-Motion) camera calibration으로 얻은 `sparse point cloud`에서 시작해서  
  scene을 3D Gaussians로 나타냄으로써  
  `empty space에서의 불필요한 계산을 하지 않도록` continuous volumetric radiance fields 정보를 저장  
  - NeRF-synthetic dataset의 경우 3DGS 는 random initialization으로도 좋은 퀄리티 달성  

2. `optimization` interleaved with `adaptive density control` :  
  - optimize 4 parameters :  
  3D position(mean), anisotropic covariance, opacity, and spherical harmonic coeff.(color)  
  `highly anisotropic volumetric splats`는 `fine structures`를 compact하게 나타낼 수 있음!!  
  `spherical harmonics`를 통해 `directional appearance(color)`를 잘 나타낼 수 있음!!<d-cite key="Plenoxels">[1]</d-cite><d-cite key="InstantNGP">[2]</d-cite>  
  - adaptive density control :  
  gradient 기반으로 Gaussian 형태를 변화시키기 위해, add and occasionally remove 3D Gaussians during optimization  

3. differentiable visibility-aware `real-time rendering` :  
perform $$\alpha$$-blending of `anisotropic splats` respecting visibility order  
by fast `GPU sorting` algorithm and `tile-based rasterization`(projection and $$\alpha$$-blending)  
한편, accumulated $$\alpha$$ values를 tracking함으로써 `Gaussians 수에 제약 없이` 빠른 backward pass도 가능  

---

### Pseudo-Code

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-01-GS/3.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

빨간 박스 : initialization  
파란 박스 : optimization  
초록 박스 : 특정 iter.마다 Gaussian을 clone, split, remove  

## Differentiable 3D Gaussian Splatting

### 3D Gaussian

- `differentiable` volumetric representation의 특성을 가지고 있으면서도 빠른 rendering을 위해 `unstructured and explicit`한 게 무엇이 있을까?  
$$\rightarrow$$ 3D Gaussian !!  

- a point를 a small planar circle with a normal이라고 가정하는 이전 Point-based rendering 논문들 <d-cite key="Point1">[3]</d-cite> <d-cite key="Point2">[4]</d-cite> 과 달리  
`SfM points는 sparse해서 normals(법선)를 estimate하기 어려울` 뿐만 아니라, estimate 한다 해도 very noisy normals를 optimize하는 것은 매우 어렵  
$$\rightarrow$$ normals 필요 없는 3D Gaussians !!  
k-dim. Gaussian : $$G(\boldsymbol x) = (2\pi)^{-\frac{k}{2}}det(\Sigma)^{\frac{1}{2}}e^{-\frac{1}{2}(\boldsymbol x - \boldsymbol \mu)^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu)}$$  

## Parameters to train

1. `scale vector` $$s$$ and `quaternion` $$q$$ for `covariance matrix`
2. `spherical harmonics`(SH) coeff. for `color`
3. `opacity` $$\alpha$$
4. `3D position` for `mean`

### Parameter 1. Covariance matrix

> covariance matrix and scale vector(scale) and quaternion(rotation)  

- covariance matrix는 positive semi-definite $$x^T M x \geq 0$$ for all $$x \in R^n$$이어야만 physical meaning을 가지는데,  
$$\Sigma$$ 를 직접 바로 optimize하면 invalid covariance matrix가 될 수 있음  
그렇다면!!  
$$\Sigma$$ 가 `positive semi-definite`이도록 $$\Sigma = R S S^T R^T$$ 로 정의해서  
$$\Sigma$$ 대신 `x,y,z-axis scale`을 나타내는 `3D vector` $$s$$ 와 `rotation`을 나타내는 `4D quaternion` $$q$$ 를 optimize 하자!!  
quaternion에 대한 설명은 [Quaternion](https://semyeong-yu.github.io/blog/2024/Quaternion) 링크 참고!!  

- `scale` matrix $$S$$ `초기값` :  
[GaussianModel().create_from_pcd()](https://github.com/graphdeco-inria/gaussian-splatting/blob/b2ada78a779ba0455dfdc2b718bdf1726b05a1b6/scene/gaussian_model.py#L134C1-L134C1)  
SfM sparse point cloud의 각 점에 대해 가장 가까운 점 3개까지의 거리의 평균을 각 axis별로 구한 것을 3 $$\times$$ 1 $$d$$라 할 때  
3 $$\times$$ 1 $$log(\sqrt{d})$$ 의 값을 복사하여 3 $$\times$$ 3 matrix $$S$$를 초기화  
```Python
dist2 = torch.clamp_min(distCUDA2(torch.from_numpy(np.asarray(pcd.points)).float().cuda()), 0.0000001)
scales = torch.log(torch.sqrt(dist2))[...,None].repeat(1, 3)
```

- `quaternion` $$q$$ `초기값` :  
각 점에 대해 $$\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}$$ 으로 초기화  
```Python
rots = torch.zeros((fused_point_cloud.shape[0], 4), device="cuda")
rots[:, 0] = 1
```

- `anisotropic covariance`는 다양한 모양의 geometry를 나타내기 위해 optimize하기에 적합!  

> param. gradient 직접 유도  

training할 때 automatic differentiation으로 인한 overhead를 방지하기 위해 param. gradient를 직접 유도함!  

Appendix A. `?????`  

> EWA volume splatting (2001) :  
world-to-camera 는 linear transformation 이지만,  
`camera-to-image (projection)` 는 `non-linear transformation` 이다!!  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-01-GS/4.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    위 그림 : camera coordinate / 아래 그림 : image coordinate
</div>

- `world` coordinate (3D) :  
  - $$\boldsymbol u = \begin{bmatrix} u_0 \\ u_1 \\ u_2 \end{bmatrix}$$  
- `camera` coordinate (3D) :  
  - $$\boldsymbol t = \begin{bmatrix} t_0 \\ t_1 \\ t_2 \end{bmatrix}$$  
  $$= W \boldsymbol u + d$$  
  where $$W$$ : `viewing transformation` affine matrix from world coordinate to camera coordinate  
- `image` coordinate (2D) :  
  - $$\boldsymbol x = \begin{bmatrix} x_0 \\ x_1 \\ x_2 \end{bmatrix}$$  
  $$= \phi(\boldsymbol t) = \begin{bmatrix} \frac{t_0}{t_2} \\ \frac{t_1}{t_2} \\ \| (t_0, t_1, t_2)^T \| \end{bmatrix}$$  
  - function $$\phi$$ 는 non-linear하므로 Affine transformation이 불가능하다.  
  - `Local Affine (Linear) transform으로 Approx.`하기 위해 $$\boldsymbol t = \boldsymbol t_{k}$$ 에서의 `Taylor Approx.`를 이용하면,  
  $$\phi_{k}(\boldsymbol t) = \phi(\boldsymbol t_{k}) + \boldsymbol J_{k} \cdot (\boldsymbol t - \boldsymbol t_{k})$$  
  where  
  $$\boldsymbol J_{k} = \frac{d\phi}{d \boldsymbol t}(\boldsymbol t_{k}) = \begin{bmatrix} \frac{d\phi}{d \boldsymbol t_{0}}(\boldsymbol t_{k}) & \frac{d\phi}{d \boldsymbol t_{1}}(\boldsymbol t_{k}) & \frac{d\phi}{d \boldsymbol t_{2}}(\boldsymbol t_{k}) \end{bmatrix} = \begin{bmatrix} \frac{1}{t_{k, 2}} & 0 & -\frac{t_{k, 0}}{t_{k, 2}^2} \\ 0 & \frac{1}{t_{k, 2}} & -\frac{t_{k, 1}}{t_{k, 2}^2} \\ \frac{t_{k, 0}}{l} & \frac{t_{k, 1}}{l} & \frac{t_{k, 2}}{l} \end{bmatrix}$$  
  and $$l = \| (t_{k, 0}, t_{k, 1}, t_{k, 2})^T \|$$  
  Here, $$J$$ : `Jacobian`(각 axis로 편미분한 matrix) of the `affine approx.` of the `projective transformation` from camera coordinate to image coordinate  
  - 즉, camera coordinate에서 임의의 좌표 $$\boldsymbol t_{k}$$ 주변에 존재하는 입력 좌표 $$\boldsymbol t$$에 대해서는 image coordinate으로의 affine(linear) transformation이 충족된다.
  - Gaussian Splatting 논문의 경우 `Gaussian의 중심점`을 $$\boldsymbol t_{k}$$ 로 두면 그 주변의 $$\boldsymbol t$$에 대해서는 Jacobian을 이용한 affine(linear) transformation 가능!  

> `Projection` of 3D Gaussian `covariance` to 2D  

- `world coordinate` :  
$$\Sigma$$ : 3 $$\times$$ 3 covariance matrix of 3D Gaussian  

- `image coordiante` (z=1) :  
$$\Sigma^{\ast} = J W \Sigma W^T J^T$$ : covariance matrix of 2D splat  
  - Step 1. world-to-camera (`affine`) :  
  $$\boldsymbol u \rightarrow W \boldsymbol u + d$$  
  - Step 2. camera-to-image (`local affine approx.`) :  
  $$W \boldsymbol u + d \rightarrow \phi_{k}(W \boldsymbol u + d) = x_k + \boldsymbol J_{k} W \boldsymbol u + \boldsymbol J_{k} (d - \boldsymbol t_{k})$$  
  상수 부분을 제외하면 $$\boldsymbol x = \boldsymbol J_{k} W \boldsymbol u$$  
  - Step 3. covariance 특성 :  
  $$Cov[Ax] = E[(Ax - E[Ax])(Ax - E[Ax])^T]$$  
  $$= E[A(x - E[x])(x - E[x])^TA^T] = A Cov[x] A^T$$  
  - Step 4. `world-to-image covariance` :  
  $$\boldsymbol u \rightarrow \boldsymbol J_{k} W \boldsymbol u$$ 이므로  
  $$\Sigma \rightarrow \boldsymbol J \boldsymbol W \Sigma \boldsymbol W^T \boldsymbol J^T$$  
  - Step 5. `covariance dimension reduction` :  
  추가로, $$\boldsymbol J \boldsymbol W \Sigma \boldsymbol W^T \boldsymbol J^T$$ 로 계산한 $$\Sigma^{\ast}$$ 는 3-by-3 matrix 인데,  
  3D Gaussian을 한쪽 축으로 적분하면 2D Gaussian과 동일한 값을 가지게 되므로  
  3-by-3 covariance matrix의 3번째 행과 열의 값을 버린  
  2-by-2 matrix를 projected 2D covariance matrix 로 사용하면 됨!


### Parameter 2. Spherical Harmonics(SH) coeff.

### Parameter 3. opacity

### Parameter 4. 3D position(mean)


## Optimization with Adaptive Density Control of 3D Gaussians

### Optimization

### Adaptive Control of Gaussians

## Fast Differentiable Rasterizer for Gaussians

## Results

## Discussion

