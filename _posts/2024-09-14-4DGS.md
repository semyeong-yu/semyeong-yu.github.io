---
layout: distill
title: 4D Gaussian Splatting
date: 2024-09-14 12:00:00
description: 4D Gaussian Splatting for Real-Time Dynamic Scene Rendering (CVPR 2024)
tags: GS 4d dynamic rendering
categories: 3d-view-synthesis
thumbnail: assets/img/2024-09-14-4DGS/1.png
bibliography: 2024-09-14-4DGS.bib
giscus_comments: false
disqus_comments: true
related_posts: true
toc:
  - name: Abstract
  - name: Contribution
  - name: Related Works
    subsections:
      - name: Novel View Synthesis
      - name: Neural Rendering w. Point Clouds
      - name: Dynamic NeRF with Deformation Fields
  - name: Method
    subsections:
      - name: Overview (Gaussian Deformation Field Network)
      - name: Spatial-Temporal Structure Encoder
      - name: Extremely Tiny Multi-head Gaussian Deformation Decoder
      - name: Optimization
  - name: Experiment
    subsections:
      - name: Dataset
      - name: Results
      - name: Ablation Study
  - name: Discussion
    subsections:
      - name: Discussion
      - name: Limitation
      - name: Conclusion
  - name: Code Flow
  - name: Question
# toc:
#   beginning: true
#   sidebar: right
# featured: true
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## 4D Gaussian Splatting for Real-Time Dynamic Scene Rendering

#### Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, Xinggang Wang

> paper :  
[https://arxiv.org/abs/2310.08528](https://arxiv.org/abs/2310.08528)  
project website :  
[https://guanjunwu.github.io/4dgs/index.html](https://guanjunwu.github.io/4dgs/index.html)  
code :  
[https://github.com/hustvl/4DGaussians](https://github.com/hustvl/4DGaussians)  

## Abstract

- spatially-temporally-sparse input으로부터  
complex point motion을 정확하게 모델링하면서  
high efficiency로 real-time dynamic scene을 rendering하는 건 매우 challenging task  

- 3DGS를 각 frame에 적용하는 게 아니라 4DGS라는 새로운 모델 제시  
  - `오직 3DGS 한 세트` 필요
  - 4DGS framework :  
    - `Spatial-Temporal Structure Encoder` :  
    HexPlane <d-cite key="neuralvoxel1">[22]</d-cite> 에서 영감을 받아  
    `decomposed neural voxel encoding algorithm`을 이용해서  
    4D neural voxel로부터 Gaussian features를 얻음  
    - `Extremely Tiny Multi-head Gaussian Deformation Decoder` :  
    가벼운 MLP를 이용해서  
    `Gaussian deformation`을 예측함

- 4DGS :  
real-time (82 FPS) rendering at high (800 $$\times$$ 800) resolution on RTX 3090 GPU

## Contribution

- Gaussian `motion`과 `shape`-deformation을 모두 모델링할 수 있는 4DGS framework 제시  
w. efficient `Gaussian deformation field` network

- `multi-resolution` encoding  
(connect nearby 3D Gaussians to build rich Gaussian features)  
by efficient `spatial-temporal structure encoder`

- SOTA `performance`이면서 `real-time` rendering on `dynamic` scenes  
e.g. 82 FPS at resol. 800 $$\times$$ 800 for synthetic dataset  
e.g. 30 FPS at resol. 1352 $$\times$$ 1014 for real dataset  

- 4D scenes에서의 editing 및 tracking에 활용 가능

## Related Works

### Novel View Synthesis

- static scene :  
  - light fields <d-cite key="lightfield">[1]</d-cite>, mesh <d-cite key="mesh1">[2]</d-cite> <d-cite key="mesh2">[3]</d-cite> <d-cite key="mesh3">[4]</d-cite> <d-cite key="mesh4">[5]</d-cite>, voxels <d-cite key="voxel1">[6]</d-cite> <d-cite key="voxel2">[7]</d-cite> <d-cite key="voxel3">[8]</d-cite>, multi-planes <d-cite key="multiplane">[9]</d-cite> 이용한 methods
  - NeRF-based methods [NeRF](https://semyeong-yu.github.io/blog/2024/NeRF/) [MipNeRF](https://semyeong-yu.github.io/blog/2024/MipNeRF/) <d-cite key="nerf++">[10]</d-cite>

- dynamic scene :  
  - NeRF-based methods <d-cite key="dynerf1">[11]</d-cite> <d-cite key="dynerf2">[12]</d-cite> <d-cite key="dynerf3">[13]</d-cite>
  - `explicit voxel grid` <d-cite key="voxeltemp1">[14]</d-cite> <d-cite key="voxeltemp2">[15]</d-cite> <d-cite key="voxeltemp3">[16]</d-cite> <d-cite key="voxeltemp4">[17]</d-cite> :  
  temporal info. 모델링하기 위해 explicit voxel grid 사용  
  - `flow-based` methods <d-cite key="flow1">[18]</d-cite> <d-cite key="flow2">[19]</d-cite> <d-cite key="voxeltemp3">[16]</d-cite> <d-cite key="flow3">[20]</d-cite> <d-cite key="flow4">[21]</d-cite> :  
  nearby frames를 blending하는 warping algorithm 사용
  - `decomposed neural voxels` <d-cite key="neuralvoxel1">[22]</d-cite> <d-cite key="neuralvoxel2">[23]</d-cite> <d-cite key="neuralvoxel3">[24]</d-cite> <d-cite key="neuralvoxel4">[25]</d-cite> <d-cite key="neuralvoxel5">[26]</d-cite> <d-cite key="neuralvoxel6">[27]</d-cite> :  
  빠른 training on dynamic scenes 가능  
  (Fig 1.의 (b))
  - `multi-view` setups 다루기 위한 methods <d-cite key="multi1">[28]</d-cite> <d-cite key="multi2">[29]</d-cite> <d-cite key="multi3">[30]</d-cite> <d-cite key="multi4">[31]</d-cite> <d-cite key="multi5">[32]</d-cite> <d-cite key="multi6">[33]</d-cite>
  - 본 논문 (4DGS) :  
  위에서 언급된 methods는 빠른 training은 가능했지만 real-time rendering on dynamic scenes는 여전히 어려웠음  
  $$\rightarrow$$  
  본 논문은 빠른 training 및 rendering pipeline 제시  
  (Fig 1.의 (c))

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-14-4DGS/1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    Fig 1. dynamic scene rendering methods
</div>

- Fig 1. 설명 :  
dynamic scene을 rendering하는 여러 방법들 소개  
  - (a) : Deformation-based (Canonical Mapping) Volume Rendering  
  point-deformation-field를 이용해서  
  sampled points를 canonical space로 mapping
  - (b) : Time-aware Volume Rendering  
  각 timestamp에서의 각 point의 feature를 직접 개별적으로 계산  
  (path는 그대로)
  - (c) : 4DGS   
  compact `Gaussian-deformation-field`를 이용해서  
  기존의 3D Gaussians를 특정 timestamp의 3D Gaussians로 변환

### Neural Rendering w. Point Clouds

- 3D scenes를 나타내기 위해 meshes, point-clouds, voxels, hybrid ver. 등 여러 분야가 연구되어 왔는데  
그 중 point-cloud representation을 volume rendering과 결합하면  
dynamic novel view synthesis task도 잘 수행 가능

- 3DGS :  
`explicit` representation이라서,  
`differentiable` `point`-based splatting이라서,  
`real-time` renderer라서 주목받음  

- 3DGS on dynamic scenes :  
  - Dynamic3DGS <d-cite key="dyna3DGS">[34]</d-cite> :  
  각 timestamp $$t_i$$ 마다 각 3D Gaussian의 position, variance를 tracking  
  linear memory consumption $$O(tN)$$  
  for $$t$$-time steps and $$N$$-3D Gaussians  
  - 4DGS (본 논문) :  
  very compact network로 3D Gaussian motion을 모델링하기 때문에  
  training과 real-time rendering
  memory consumption $$O(N+F)$$  
  for $$N$$-3D Gaussians, $$F$$-parameters of Gaussian-deformation-field network  
  - 4DGS (Zeyu Yang) <d-cite key="4DGS1">[35]</d-cite> :  
  marginal temporal Gaussian 분포를 기존의 3D Gaussian 분포에 추가하여  
  3D Gaussians를 3D로 uplift  
  However, 그러면 각 3D Gaussian은 오직 their local temporal space에만 focus
  - Deformable-3DGS (Ziyi Yang) <d-cite key="deformable3DGS">[36]</d-cite> :  
  본 논문처럼 MLP deformation network를 도입하여 dynamic scenes의 motion을 모델링
  - Spacetime-GS (Zhan Li) <d-cite key="spacetimeGS">[37]</d-cite> :  
  각 3D Gaussian을 individually tracking

### Dynamic NeRF with Deformation Fields

- 모든 dynamic NeRF는 아래의 식을 따른다  
$$c, \sigma = M(x, d, t, \lambda)$$  
where $$c \in R^3, \sigma \in R, x \in R^3, d \in R^2, t \in R, \lambda \in R$$  
where $$\lambda$$ is optional input (frame-dependent code to build topological and appearance changes) <d-cite key="dynerf2">[12]</d-cite> <d-cite key="wild">[38]</d-cite>  

- deformation NeRF-based methods는  
Fig 1. (a)에서처럼  
deformation network $$\phi_{t} : (x, t) \rightarrow \Delta x$$ 로 world-to-canonical mapping 한 뒤  
RGB color와 volume density를 뽑는다  
$$c, \sigma = NeRF(x+\Delta x, d, \lambda)$$

- 4DGS (본 논문)은  
`Gaussian deformation field` network $$F$$ 이용해서  
time $$t$$ 에서의 `canonical-to-world mapping`을 직접 계산한 뒤  
differential splatting(rendering) 수행

## Method

### Overview (Gaussian Deformation Field Network)

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-14-4DGS/2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    Fig 2. Pipeline of this model (Gaussian Deformation Field Network)
</div>

- Fig 2. 설명 :  
  - 각 3D Gaussian의 center 좌표 $$x, y, z$$ 와 timestamp $$t$$ 를 input으로 준비  
  - Spatial-Temporal Structure Encoder :  
  multi-resolution voxel planes를 query하여  
  voxel feature를 계산  
  (temporal 및 spatial feature를 둘 다 encode 가능)
  - Tiny Multi-head Gaussian Deformation Decoder :  
  position, rotation, scaling head에서 각각 해당 feature를 decode하여  
  timestamp $$t$$ 에서의 변형된 3D Gaussians를 얻음

### Spatial-Temporal Structure Encoder

- 근처에 있는 3D Gaussians끼리는 항상 spatial 및 temporal 정보를 비슷하게 공유하고 있다.  
따라서 각 Gaussian을 따로 변형시키는 게 아니라,  
여러 `adjacent 3D Gaussians를 연결`지어서 변형시킴으로써  
motion과 shape-deformation을 정확하게 예측

- Spatial-Temporal Structure Encoder :  
<d-cite key="voxeltemp1">[14]</d-cite> <d-cite key="neuralvoxel1">[22]</d-cite> <d-cite key="neuralvoxel2">[23]</d-cite> <d-cite key="neuralvoxel5">[26]</d-cite> 에서 영감을 받아  
multi-resolution HexPlane $$R(i, j)$$ 와 tiny MLP $$\phi_{d}$$ 로 구성됨  
(vanilla 4D neural voxel은 memory를 많이 잡아먹기 때문에  
4D neural voxel을 6개의 multi-resol. planes로 decompose하는  
4D K-Planes module <d-cite key="neuralvoxel2">[23]</d-cite> 사용)
(3D Gaussians는 bounding plane voxels에 포함되어  
Gaussians의 deformation도 nearby temporal voxels에 encode될 수 있음) `????`
Specifically, the spatial-temporal structure encoder H
contains 6 multi-resolution plane modules 할 차례 `?????` TBD
  - multi-resolution HexPlane $$R(i, j)$$ :  
  TBD
  - tiny MLP $$\phi_{d}$$ :  
  TBD



### Extremely Tiny Multi-head Gaussian Deformation Decoder

### Optimization

## Experiment

### Dataset

### Results

### Ablation Study

## Discussion

### Discussion

### Limitation

### Conclusion

## Code Flow

## Question