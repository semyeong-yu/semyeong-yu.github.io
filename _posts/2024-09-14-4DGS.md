---
layout: distill
title: 4D Gaussian Splatting
date: 2024-09-14 12:00:00
description: 4D Gaussian Splatting for Real-Time Dynamic Scene Rendering (CVPR 2024)
tags: GS 4d dynamic rendering
categories: 3d-view-synthesis
thumbnail: assets/img/2024-09-14-4DGS/1.png
bibliography: 2024-09-14-4DGS.bib
giscus_comments: false
disqus_comments: true
related_posts: true
toc:
  - name: Abstract
  - name: Contribution
  - name: Related Works
    subsections:
      - name: Novel View Synthesis
      - name: Neural Rendering w. Point Clouds
      - name: Dynamic NeRF with Deformation Fields
  - name: Method
    subsections:
      - name: Overview (Gaussian Deformation Field Network)
      - name: Spatial-Temporal Structure Encoder
      - name: Extremely Tiny Multi-head Gaussian Deformation Decoder
      - name: Optimization
  - name: Experiment
    subsections:
      - name: Dataset
      - name: Results
      - name: Ablation Study
  - name: Discussion
    subsections:
      - name: Discussion
      - name: Limitation
      - name: Conclusion
  - name: Code Flow
  - name: Question
# toc:
#   beginning: true
#   sidebar: right
# featured: true
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## 4D Gaussian Splatting for Real-Time Dynamic Scene Rendering

#### Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, Xinggang Wang

> paper :  
[https://arxiv.org/abs/2310.08528](https://arxiv.org/abs/2310.08528)  
project website :  
[https://guanjunwu.github.io/4dgs/index.html](https://guanjunwu.github.io/4dgs/index.html)  
code :  
[https://github.com/hustvl/4DGaussians](https://github.com/hustvl/4DGaussians)  
referenced blog :  
[https://xoft.tistory.com/54](https://xoft.tistory.com/54)

## Abstract

- spatially-temporally-sparse input으로부터  
complex point motion을 정확하게 모델링하면서  
high efficiency로 real-time dynamic scene을 rendering하는 건 매우 challenging task  

- 3DGS를 각 frame에 적용하는 게 아니라 4DGS라는 새로운 모델 제시  
  - `오직 3DGS 한 세트` 필요
  - 4DGS framework :  
    - `Spatial-Temporal Structure Encoder` :  
    HexPlane <d-cite key="neuralvoxel1">[22]</d-cite> 에서 영감을 받아  
    `decomposed neural voxel encoding algorithm`을 이용해서  
    4D neural voxel로부터 Gaussian features를 얻음  
    - `Extremely Tiny Multi-head Gaussian Deformation Decoder` :  
    가벼운 MLP를 이용해서  
    `Gaussian deformation`을 예측함

- 4DGS :  
real-time (82 FPS) rendering at high (800 $$\times$$ 800) resolution on RTX 3090 GPU

## Contribution

- Gaussian `motion`과 `shape`-deformation을 모두 모델링할 수 있는 4DGS framework 제시  
w. efficient `Gaussian deformation field` network

- `multi-resolution` encoding  
(connect nearby 3D Gaussians to build rich Gaussian features)  
by efficient `spatial-temporal structure encoder`

- SOTA `performance`이면서 `real-time` rendering on `dynamic` scenes  
e.g. 82 FPS at resol. 800 $$\times$$ 800 for synthetic dataset  
e.g. 30 FPS at resol. 1352 $$\times$$ 1014 for real dataset  

- 4D scenes에서의 editing 및 tracking에 활용 가능

## Related Works

### Novel View Synthesis

- static scene :  
  - light fields <d-cite key="lightfield">[1]</d-cite>, mesh <d-cite key="mesh1">[2]</d-cite> <d-cite key="mesh2">[3]</d-cite> <d-cite key="mesh3">[4]</d-cite> <d-cite key="mesh4">[5]</d-cite>, voxels <d-cite key="voxel1">[6]</d-cite> <d-cite key="voxel2">[7]</d-cite> <d-cite key="voxel3">[8]</d-cite>, multi-planes <d-cite key="multiplane">[9]</d-cite> 이용한 methods
  - NeRF-based methods [NeRF](https://semyeong-yu.github.io/blog/2024/NeRF/) [MipNeRF](https://semyeong-yu.github.io/blog/2024/MipNeRF/) <d-cite key="nerf++">[10]</d-cite>

- dynamic scene :  
  - NeRF-based methods <d-cite key="dynerf1">[11]</d-cite> <d-cite key="dynerf2">[12]</d-cite> <d-cite key="dynerf3">[13]</d-cite>
  - `explicit voxel grid` <d-cite key="voxeltemp1">[14]</d-cite> <d-cite key="voxeltemp2">[15]</d-cite> <d-cite key="voxeltemp3">[16]</d-cite> <d-cite key="voxeltemp4">[17]</d-cite> :  
  temporal info. 모델링하기 위해 explicit voxel grid 사용  
  - `flow-based` methods <d-cite key="flow1">[18]</d-cite> <d-cite key="flow2">[19]</d-cite> <d-cite key="voxeltemp3">[16]</d-cite> <d-cite key="flow3">[20]</d-cite> <d-cite key="flow4">[21]</d-cite> :  
  nearby frames를 blending하는 warping algorithm 사용
  - `decomposed neural voxels` <d-cite key="neuralvoxel1">[22]</d-cite> <d-cite key="neuralvoxel2">[23]</d-cite> <d-cite key="neuralvoxel3">[24]</d-cite> <d-cite key="neuralvoxel4">[25]</d-cite> <d-cite key="neuralvoxel5">[26]</d-cite> <d-cite key="neuralvoxel6">[27]</d-cite> :  
  빠른 training on dynamic scenes 가능  
  (Fig 1.의 (b))
  - `multi-view` setups 다루기 위한 methods <d-cite key="multi1">[28]</d-cite> <d-cite key="multi2">[29]</d-cite> <d-cite key="multi3">[30]</d-cite> <d-cite key="multi4">[31]</d-cite> <d-cite key="multi5">[32]</d-cite> <d-cite key="multi6">[33]</d-cite>
  - 본 논문 (4DGS) :  
  위에서 언급된 methods는 빠른 training은 가능했지만 real-time rendering on dynamic scenes는 여전히 어려웠음  
  $$\rightarrow$$  
  본 논문은 빠른 training 및 rendering pipeline 제시  
  (Fig 1.의 (c))

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-14-4DGS/1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    Fig 1. dynamic scene rendering methods
</div>

- Fig 1. 설명 :  
dynamic scene을 rendering하는 여러 방법들 소개  
  - (a) : Deformation-based (Canonical Mapping) Volume Rendering  
  point-deformation-field를 이용해서  
  sampled points를 canonical space로 mapping  
  (하나의 ray 위의 sampled points가 다같이 canonical space로 mapping되므로  
  각 point의 서로 다른 속도를 잘 모델링하지 못함)
  - (b) : Time-aware Volume Rendering  
  각 timestamp에서의 각 point의 feature를 직접 개별적으로 계산  
  (path는 그대로)
  - (c) : 4DGS   
  compact `Gaussian-deformation-field`를 이용해서  
  기존의 3D Gaussians를 특정 timestamp의 3D Gaussians로 변환  
  ((a)와 유사하긴 하지만  
  각 Gaussian이 ray에 의존하지 않고 서로 다른 속도로 이동 가능)

### Neural Rendering w. Point Clouds

- 3D scenes를 나타내기 위해 meshes, point-clouds, voxels, hybrid ver. 등 여러 분야가 연구되어 왔는데  
그 중 point-cloud representation을 volume rendering과 결합하면  
dynamic novel view synthesis task도 잘 수행 가능

- 3DGS :  
`explicit` representation이라서,  
`differentiable` `point`-based splatting이라서,  
`real-time` renderer라서 주목받음  

- 3DGS on dynamic scenes :  
  - Dynamic3DGS <d-cite key="dyna3DGS">[34]</d-cite> :  
    - 3D Gaussian 개수를 고정하고  
    각 timestamp $$t_i$$ 마다 각 3D Gaussian의 position, variance를 tracking  
    - 문제점 :  
      - need dense multi-view input images  
      - prev. frame의 모델링이 부적절하면 전체적인 성능이 떨어짐
      - linear memory consumption $$O(tN)$$  
      for $$t$$-time steps and $$N$$-3D Gaussians  
  - 4DGS (본 논문) :  
    - very compact network로 3D Gaussian motion을 모델링하기 때문에  
    training 효율적이고 real-time rendering
    - memory consumption $$O(N+F)$$  
    for $$N$$-3D Gaussians, $$F$$-parameters of Gaussian-deformation-field network  
  - 4DGS (Zeyu Yang) <d-cite key="4DGS1">[35]</d-cite> :  
    - marginal temporal Gaussian 분포를 기존의 3D Gaussian 분포에 추가하여  
    3D Gaussians를 4D로 uplift  
    - However, 그러면 각 3D Gaussian은 오직 their local temporal space에만 focus
  - Deformable-3DGS (Ziyi Yang) <d-cite key="deformable3DGS">[36]</d-cite> :  
    - 본 논문처럼 MLP deformation network를 도입하여 dynamic scenes의 motion을 모델링  
    - 본 논문 (4DGS)도 이와 유사하지만 training을 효율적으로 만듦
  - Spacetime-GS (Zhan Li) <d-cite key="spacetimeGS">[37]</d-cite> :  
    - 각 3D Gaussian을 individually tracking

### Dynamic NeRF with Deformation Fields

- 모든 dynamic NeRF는 아래의 식을 따른다  
$$c, \sigma = M(x, d, t, \lambda)$$  
where $$c \in R^3, \sigma \in R, x \in R^3, d \in R^2, t \in R, \lambda \in R$$  
where $$\lambda$$ is optional input (frame-dependent code to build topological and appearance changes) <d-cite key="dynerf2">[12]</d-cite> <d-cite key="wild">[38]</d-cite>  

- deformation NeRF-based methods는  
Fig 1. (a)에서처럼  
deformation network $$\phi_{t} : (x, t) \rightarrow \Delta x$$ 로 world-to-canonical mapping 한 뒤  
RGB color와 volume density를 뽑는다  
$$c, \sigma = NeRF(x+\Delta x, d, \lambda)$$

- 4DGS (본 논문)은  
`Gaussian deformation field` network $$F$$ 이용해서  
time $$t$$ 에서의 `canonical-to-world mapping`을 직접 계산한 뒤  
differential splatting(rendering) 수행

## Method

### Overview (Gaussian Deformation Field Network)

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-14-4DGS/2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    Fig 2. Pipeline of this model (Gaussian Deformation Field Network)
</div>

- Fig 2. 설명 :  
  - static 3D Gaussian set을 만듦
  - 각 3D Gaussian의 center 좌표 $$x, y, z$$ 와 timestamp $$t$$ 를  
  Gaussian Deformation Field Network의 input으로 준비  
  - Spatial-Temporal Structure Encoder :  
  multi-resolution voxel planes를 query하여  
  voxel feature를 계산  
  (temporal 및 spatial feature를 둘 다 encode 가능)
  - Tiny Multi-head Gaussian Deformation Decoder :  
  position, rotation, scaling head에서 각각 해당 feature를 decode하여  
  각 3D Gaussian의 position, rotation, scaling 변화량을 얻어서  
  timestamp $$t$$ 에서의 변형된 3D Gaussians를 얻음

### Spatial-Temporal Structure Encoder

- 근처에 있는 3D Gaussians끼리는 항상 spatial 및 temporal 정보를 비슷하게 공유하고 있다.  
따라서 각 Gaussian을 따로 변형시키는 게 아니라,  
여러 `adjacent 3D Gaussians를 연결`지어서 변형시킴으로써  
motion과 shape-deformation을 정확하게 예측

- 기존 논문 설명 (Backgrounds) :  
  - TensoRF : [Link](https://semyeong-yu.github.io/blog/2024/TensoRF/)
  - HexPlane <d-cite key="neuralvoxel1">[22]</d-cite> :  
  4차원($$XYZT$$)을 모델링하기 위해  
  3개 타입의 rank로 decomposition ($$XY$$ 평면 - $$ZT$$ 평면, $$XZ$$ 평면 - $$YT$$ 평면, $$YZ$$ 평면 - $$XT$$ 평면)

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-14-4DGS/3.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    HexPlane Overview
</div>

- Spatial-Temporal Structure Encoder (1) :  
  - vanilla 4D neural voxel은 memory를 많이 잡아먹기 때문에  
  4D neural voxel($$XYZT$$)을 6개의 multi-resol. planes로 decompose하는  
  4D K-Planes module <d-cite key="neuralvoxel2">[23]</d-cite> 사용  
  - 3D Gaussians는 bounding plane voxels에 포함되어  
  Gaussians의 deformation도 nearby temporal voxels에 encode될 수 있음 `????`  
  - 기존 논문들 <d-cite key="voxeltemp1">[14]</d-cite> <d-cite key="neuralvoxel1">[22]</d-cite> <d-cite key="neuralvoxel2">[23]</d-cite> <d-cite key="neuralvoxel5">[26]</d-cite> 에서 영감을 받아  
  Spatial-Temporal Structure Encoder는  
  multi-resolution HexPlane $$R(i, j)$$ 와 tiny MLP $$\phi_{d}$$ 로 구성됨  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-14-4DGS/4.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Spatial-Temporal Structure Encoder (2) :  
  - multi-resolution HexPlane $$R(i, j)$$ :  
  본 논문에서는 TensoRF와 달리 Grid resol.을 점점 증가시키지 않고, 애초에 multi-resolution으로 decomposition의 rank를 구성함  
  $$f_{h} = \cup_{l} \prod \text{interp}(R_{l}(i, j))$$  
    - where  
    $$f_h \in R^{h \ast l}$$ : feature of decomposed neural voxel
    $$R_{l}(i, j) \in R^{h \times lN_i \times lN_j}$$ : 2D voxel plane  
    $$h$$ : hidden dim.  
    $$\{ i, j \} \in \{ (x, y), (x, z), (y, z), (x, t), (y, t), (z, t) \}$$ : 6 종류의 planes  
    $$N$$ : voxel grid의 basic resol.  
    $$l \in \{ 1, 2 \}$$ : upsampling scale (multi-resol.)  
    $$\text{interp}$$ : bilinear interpolation (plane의 grid의 네 꼭짓점으로부터 interpolation으로 voxel feature 뽑아냄)  
    $$\prod$$ : K-Planes <d-cite key="neuralvoxel2">[23]</d-cite> 참고  

- Spatial-Temporal Structure Encoder (3) :  
  - tiny MLP $$\phi_{d}$$ :  
  $$f_d = \phi_{d} (f_h)$$  
  merge all the features
  - 공간상(e.g. $$XY$$ 평면) 또는 시간상(e.g. $$XT$$ 평면)으로 인접한 voxel은  
  HexPlane $$R(i, j)$$ 에서 유사한 feature를 가져서 유사한 Gaussian param. 변화량을 가지므로  
  optimization 진행됨에 따라  
  Gaussian의 covariance가 줄어들면서 작은 3D Gaussian들이 모여서 dense해진다 `?????`

### Extremely Tiny Multi-head Gaussian Deformation Decoder

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-14-4DGS/5.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Multi-head Gaussian Deformation Decoder :  
  - 매우 작은 multi-head decoder로 position, scaling, rotation 변화량을 얻음  
  $$\Delta \chi = \phi_{x}(f_d)$$  
  $$\Delta r = \phi_{r}(f_d)$$  
  $$\Delta s = \phi_{s}(f_d)$$
  - 그러면 변형된 deformed 3D Gaussians 계산할 수 있음  
  $$(\chi ' , r ' , s ') = (\chi + \Delta \chi, r + \Delta r, s + \Delta s)$$ 에 대해  
  next time $$t$$ 의 deformed 3D Gaussian set은  
  $$G ' = \{ \chi ' , r ' , s ', \sigma, c \}$$
  - 근데 실제 implementation 할 때는 speed 증가 위해  
  scaling(size), rotation, color, opacity는 고정하고  
  position 변화량만 구함

### Optimization

- TBD

## Experiment

### Dataset

### Results

### Ablation Study

## Discussion

### Discussion

### Limitation

### Conclusion

## Code Flow

## Question