---
layout: post
title: NeRF
date: 2024-04-10 21:00:00
description: Representing Scenes as Neural Radiance Fields for View Synthesis
tags: "NeRF" "scene representation" "view synthesis" "image-based rendering" "volume rendering" "3D deep learning" 
categories: "3D View Synthesis"
thumbnail: assets/img/2024-04-10-NeRF/1.jpg
giscus_comments: true
related_posts: false
toc:
  beginning: true
---

# NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis

#### Ben Mildenhall, Pratul P.Srinivasan, Matthew Tancik

## Introduction

#### Pipeline

1. march camera rays to generate a sampled set of 3D points

2. represents volumetric static scene by optimizing continuous 5D function(fully-connected network)

- input: single continuous `5D coordinate` ($$x, y, z, theta, phi$$)
- output:
  `volume density` (differential opacity) (how much radiance is accumulated by a ray)
  `view-dependent RGB color` (emitted radiance)

3. synthesizes novel view by classic `volume rendering` techniques(differentiable) to accumulate(project) the color/density samples into 2D image along rays

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-04-10-NeRF/1.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all.
</div>

#### Problem & Solution

Problem :

1. not sufficiently high-resolution representation
2. inefficient in the number of samples per camera ray

Solution :

1. input `positional encoding` for MLP to represent higher frequency function
2. `hierarchical sampling` to reduce the number of queries

#### Contribution

- represent continuous scenes as 5D neural radiance fields with basic MLP to render high-resolution novel views
- differentiable volume rendering + hierarchical sampling
- positional encoding to map input 5D coordinate into higher dim. space for high-frequency scene representation
- overcome the storage costs of discretized voxel grids by encoding continuous volume into network's parameters 
=> require only storage costs of sampled volumetric representations

## Related Work

#### Neural 3D shape representation

- deep networks that map $$xyz$$ coordinates to signed distance functions or occupancy fields
=> limit : need GT 3D geometry

- Niemeyer et al.
=> input : find directly surface intersection for each ray
(can calculate exact derivatie)
=> output : diffuse color at each ray intersection location

- Sitzmann et al.
=> input : each 3D coordinate
=> output : feature vector and RGB color at each 3D coordinate
=> rendering by RNN that marches along each ray to decide where the surface is.

> Limit : oversmoothed renderings, so limited to simple shapes with low geometric complexity

#### View synthesis and image-based rendering

- Given dense sampling of views, novel view synthesis is possible by simple light field sample interpolation

- Given sparser sampling of views, there are 2 ways : mesh-based representation and volumetric representation

- Mesh-based representation with either diffuse(난반사) or view-dependent appearance :
Directly optimize mesh representations by differentiable rasterizers or pathtracers so that we reproject and reconstruct images

> Limit : 
gradient-based optimization is often difficult because of local minima or poor loss landscape
needs a template mesh with fixed topology for initialization, which is unavailable in real-world

- Volumetric representation :
well-suited for gradient-based optimization and less distracting artifacts

1. train : predict a sampled volumetric representation (voxel grids) from input images
test : use alpha-(or learned-)compositing along rays to render novel views
+) alpha-compositing : 여러 frame을 합쳐서 하나의 image로 합성하는 과정으로, 각 이미지 픽셀마다 알파 값(투명도 값)(0~1)이 있어 겹치는 부분의 알파 값 및 픽셀 값을 결정

2. CNN compensates discretization artifacts from low resolution voxel grids or CNN allows voxel grids to vary on input time

> Limit :
good results, but limited by poor time, space complexity due to discrete sampling
+) discrete sampling : rendering high resol. image => finer sampling of 3D space

> Author's solution :
encode `continuous` volume into network's parameters
=> higher quality rendering + require only storage cost of those `sampled` volumetric representations

## Neural Radiance Field Scene Representation

ddd