---
layout: post
title: NeRF
date: 2024-04-10 21:00:00
description: representing scenes as neural radiance fields for view synthesis
tags: nerf rendering 3d
categories: 3d-view-synthesis
thumbnail: assets/img/2024-04-10-NeRF/1.png
giscus_comments: true
related_posts: true
toc:
  beginning: true
  sidebar: right
featured: true
---

# NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis

#### Ben Mildenhall, Pratul P.Srinivasan, Matthew Tancik  

> paper :  
https://arxiv.org/abs/2003.08934  
code :  
https://github.com/csm-kr/nerf_pytorch?tab=readme-ov-file  
referenced blog :  
https://csm-kr.tistory.com/64  

## Introduction

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-04-10-NeRF/3.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

#### Pipeline

- (a) march camera rays to generate sampling of 5D coordinates

- (b) represents volumetric static scene by optimizing continuous 5D function(fully-connected network)

1. input: single continuous `5D coordinate`  
  3D location $$x, y, z$$  
  2D direction $$\theta, \phi$$  
2. output:  
  `volume density` (differential opacity) (how much radiance is accumulated by a ray)  
  `view-dependent RGB color` (emitted radiance) $$c = (r, g, b)$$

- (c) synthesizes novel view by classic `volume rendering` techniques(differentiable) to accumulate(project)(composite) the color/density samples into 2D image along rays

- (d) loss between synthesized and GT observed images

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-04-10-NeRF/2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    Pipeline of NeRF architecture
</div>

  
#### Problem & Solution

Problem :

1. not sufficiently high-resolution representation
2. inefficient in the number of samples per camera ray

Solution :

1. input `positional encoding` for MLP to represent higher frequency function
2. `hierarchical sampling` to reduce the number of queries

  
#### Contribution

- represent continuous scenes as 5D neural radiance fields with basic MLP to render high-resolution novel views
- differentiable volume rendering + hierarchical sampling
- positional encoding to map input 5D coordinate into higher dim. space for high-frequency scene representation
- overcome the storage costs of discretized voxel grids by encoding continuous volume into network's parameters  
=> require only storage costs of sampled volumetric representations

  
## Related Work

  
#### Neural 3D shape representation

- deep networks that map $$xyz$$ coordinates to signed distance functions or occupancy fields  
=> limit : need GT 3D geometry

- Niemeyer et al.  
=> input : find directly surface intersection for each ray
(can calculate exact derivatie)  
=> output : diffuse color at each ray intersection location

- Sitzmann et al.  
=> input : each 3D coordinate  
=> output : feature vector and RGB color at each 3D coordinate  
=> rendering by RNN that marches along each ray to decide where the surface is.  

> Limit : oversmoothed renderings, so limited to simple shapes with low geometric complexity

#### View synthesis and image-based rendering

- Given dense sampling of views, novel view synthesis is possible by simple light field sample interpolation

- Given sparser sampling of views, there are 2 ways :  
mesh-based representation and volumetric representation

- Mesh-based representation with either diffuse(난반사) or view-dependent appearance :  
Directly optimize mesh representations by differentiable rasterizers or pathtracers so that we reproject and reconstruct images

> Limit :  
gradient-based optimization is often difficult because of local minima or poor loss landscape  
needs a template mesh with fixed topology for initialization, which is unavailable in real-world

- Volumetric representation :  
well-suited for gradient-based optimization and less distracting artifacts  
train : predict a sampled volumetric representation (voxel grids) from input images  
test : use alpha-(or learned-)compositing along rays to render novel views  
+) alpha-compositing : 여러 frame을 합쳐서 하나의 image로 합성하는 과정으로, 각 이미지 픽셀마다 알파 값(불투명도 값)(0~1)이 있어 겹치는 부분의 알파 값 및 픽셀 값을 결정  
CNN compensates discretization artifacts from low resolution voxel grids or CNN allows voxel grids to vary on input time

> Limit :  
good results, but limited by poor time, space complexity due to discrete sampling  
+) discrete sampling : rendering high resol. image => finer sampling of 3D space

> Author's solution :  
encode `continuous` volume into network's parameters  
=> higher quality rendering + require only storage cost of those `sampled` volumetric representations

  
## Neural Radiance Field Scene Representation

represent continuous scene by 5D MLP : (x, d) => (c, $$\sigma$$)

Here, there are 2 key-points!

> multiview consistent :  
c is dependent on both x and d, but $$\sigma$$ is only dependent on location x

- 3D coordinate x => 8 fc-layers => volume-density and 256-dim. feature vector

> Lambertian reflection : diffuse(난반사)  vs  Specular reflection : 전반사  

> non-Lambertian effects : view-dependent color change to represent specular reflection  

- feature vector is concatenated with direction d => 1 fc-layer => view-dependent RGB color  

  
## Volume Rendering with Radiance Fields

#### Ray from input image (pre-processing)

We use `Ray` to synthesize `continuous`-viewpoint images from `discrete` input images

> $$r(t) = o + td$$  
o : the location of camera  
d : viewing direction  

> How to calculate viewing direction d??  
- pixel coordinate :
$$\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}$$  
- normalized coordinate by intrinsic matrix :  
$$\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}$$ = $$K^{-1}$$ $$\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}$$ = $$\begin{bmatrix} 1/f_x & 0 & W/2 \\ 0 & 1/f_y & H/2 \\ 0 & 0 & 1 \end{bmatrix}$$ $$\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}$$  
Since y, z have opposite direction between the real-world coordinate and pixel coordinate, we multiply (-1)  
$$\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}$$ = $$\begin{bmatrix} 1/f_x & 0 & W/2 \\ 0 & -1/f_y & H/2 \\ 0 & 0 & -1 \end{bmatrix}$$ $$\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}$$  
Here, focal length in intrinsic matrix K is usually calculated using camear angle $$\alpha$$ as $$\tan{\alpha / 2} = \frac{h/2}{f}$$  
- 3D coordinate by extrinsic matrix :  
For extrinsic matrix $$[R \vert t']$$,  
$$o = t'$$  
$$d = R * \begin{bmatrix} u \\ v \\ 1 \end{bmatrix}$$  
Therefore, we can obtain $$r(t) = o + td$$  

#### Volume Rendering from MLP output

We use differential classical volume rendering  

> idea : 빛이 들어올 확률이 클수록(채도?), 물체의 밀도가 높을수록(명도?) 2D 이미지 픽셀에서 색상 채널의 값이 크게 나타나므로  
pixel's color along a ray is equal to the integral of (transmittance) x (volume density) x (color) for all points along one ray.  

For ray $$r$$ traced through desired virtual camera and near, far bounds $$t_n$$, $$t_f$$,  
expected color of ray $$r$$ = $$C(r) = \int_{t_n}^{t_f} T(t) \sigma (r(t)) c(r(t), d) dt$$  
- $$T(t) = \exp(- \int_{t_n}^{t} \sigma (r(s)) ds)$$ :  
accumulated transmittance along ray from $$t_n$$ to $$t$$  
the probability that ray travels from $$t_n$$ to $$t$$ without hitting any particle  
투과도가 클수록 투명  
T(t) is the solution of $$\frac{dy}{dt} = - \sigma (x)y$$  
since T(t)'s rate of decrease is proportional to T(t) itself and volume density  
- $$\sigma (r(t))$$ : volume density along the ray (learned by MLP)  
- $$c(r(t), d)$$ : object's color along the ray (learned by MLP)  

To apply the equation to our model, we have to do sampling from continuous ray to discrete points  
Instead of deterministic quadrature(typically used for rendering voxel grids, but may limit resolution),  
author divides a ray $$\left[t_n, t_f\right]$$ into N = 64 bins(intervals), and chooses one point $$t_i$$ for each bin by uniform sampling  
$$t_i$$ ~ $$U \left[t_n + \frac{i-1}{N}(t_f - t_n), t_n + \frac{i}{N}(t_f - t_n)\right]$$  
Although we use discrete N samples, stratified sampling(층화 표집) enables MLP to be evaluated at continuous positions by optimization  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-04-10-NeRF/5.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

And then discretized version for N samples :  
expected color $$\hat{C}(r) = \sum_{i=1}^{N} T_i (1 - \exp(- \sigma_{i} \delta_{i})) c_i$$  
- $$\hat{C}(r)$$ : differentiable about ($$\sigma_{i}, c_i$$)  
- $$\delta_{j} = t_{j+1} - t_j$$ : the distance between adjacent samples  
- $$\sigma (r(t)) dt ~~ => ~~ \sigma_{j} \delta_{j} = 1 - \exp(-\sigma_{j} \delta_{j})$$ by Taylor Series Expansion  
- $$T(t) = \exp(- \int_{t_n}^{t} \sigma (r(s)) ds) ~~ => ~~ T_i = \exp(- \sum_{j=1}^{i-1} \sigma_{j} \delta_{j})$$  
- $$c(r(t), d) ~~ => ~~ c_i$$  

Here, in volume rendering, author uses `volume density`  
for 불투명도 == opacity == extinction coefficient == alpha value for alpha-compositing  

> Final version :  
expected color $$\hat{C}(r) = \sum_{i=1}^{N} T_i \alpha_{i} c_i$$  
where $$T_i = \prod_{j=1}^{i-1} (1-\alpha_{j})$$ and $$\alpha_{j} = 1 - \exp(-\sigma_{j} \delta_{j})$$  
which reduces to traditional alpha-compositing problem  

## Optimizing a Neural Radiance Field

#### Positional encoding (pre-processing)

If we use input directly, MLP is biased to learn low-frequency function (oversmoothed appearance)  
If we map input into higher dim. space, MLP can fit data with high-frequency variation  
$$r : R \rightarrow R^{2L}$$    
$$r(p) = (sin(2^0\pi p), cos(2^0\pi p), \cdots, sin(2^{L-1}\pi p), cos(2^{L-1}\pi p))$$  
$$L=10$$ for $$r(x)$$ where x has three coordinates  
$$L=4$$ for $$r(d)$$ where d has three components of the cartesian viewing direction unit vector  

#### Hierarchical volume sampling  

Densely evaluating N points by stratified sampling is inefficient  
=> We don't need much sampling at free space or occluded regions  
=> Hierarchical sampling enables us to allocate more samples to regions we expect to contain visible content  

We simultaneously optimize 2 networks with different sampling : `coarse` and `fine`  

> coarse sampling $$N_c$$개 : 위에서 배웠던 내용  
author divides a ray into $$N_c$$ = 64 bins(intervals), and chooses one point $$t_i$$ for each bin by uniform sampling  
$$t_i$$ ~ $$U \left[t_n + \frac{i-1}{N_c}(t_f - t_n), t_n + \frac{i}{N_c}(t_f - t_n)\right]$$  

> fine sampling $$N_f$$개 : 새로운 내용  
coarse sampling model's output is a weighted sum of all coarse-sampled colors  
$$\hat{C}(r) = \sum_{i=1}^{N_c} T_i \alpha_{i} c_i = \sum_{i=1}^{N_c} w_i c_i$$  
where we define $$w_i = T_i \alpha_{i} = T_i (1 - \exp(-\sigma_{i} \delta_{i}))$$ for $$i=1,\cdots,N_c$$  
=> Given the output of `coarse` network, we try more informed sampling where samples are biased toward the relevant parts of the volume  
=> We sample $$N_f$$=128 `fine` points following a piecewise-constant PDF of normalized $$\frac{w_i}{\sum_{j=1}^{N_c} w_j}$$  
=> Here, we use Inverse CDF Method for sampling fine points  

> Inverse transform sampling = Inverse CDF Method :  
=> PDF (probability density function) : $$f_X(x)$$  
=> CDF (cumulative distribution function) : $$F_X(x) = P(X \leq x) = \int_{-\infty}^x f_X(x) dx$$  
idea : 모든 확률 분포의 CDF는 Uniform distribution을 따른다!  
=> 따라서 CDF의 y값을 Uniform sampling하면, 그 y에 대응되는 x에 대한 (특정 PDF를 따르는) sampling을 구현할 수 있다!  
=> 즉, CDF의 역함수를 계산할 수 있다면, 기본 난수 생성기인 Uniform sampling을 이용해서 확률 분포 X에 대한 sampling을 할 수 있다!  
$$F_X(x)$$ ~ $$U\left[0, 1\right]$$  
$$X$$ ~ $$F^{-1}(U\left[0, 1\right])$$  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-04-10-NeRF/4.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

We evaluate `coarse` network using $$N_c$$=64 points per ray  
We evaluate `fine` network using $$N_c+N_f$$=64+128=192 points per ray where we sample 128 points following PDF of `coarse` sampled points  
In result, we use a total of 64+192=256 samples per ray to compute the final rendering color $$C(r)$$  

#### Implementation details & Loss  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-04-10-NeRF/6.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

1. Prepare RGB images, corresponding camera poses, intrinsic parameters and scene bounds (use COLMAP structure-from-motion package to estimate these parameters) 
2. From H x W input image, randomly sample a batch of 4096 pixels
3. calculate continuous ray from each pixel $$r(t) = o + kd$$
4. coarse sampling of $$N_c$$=64 points per each ray $$t_i$$ ~ $$U \left[t_n + \frac{i-1}{N_c}(t_f - t_n), t_n + \frac{i}{N_c}(t_f - t_n)\right]$$
5. positional encoding $$r(x)$$ and $$r(d)$$ for input
6. obtain volume density $$\sigma$$ by MLP with $$r(x, y, z)$$ as input
7. obtain color $$c$$ by MLP with $$r(x, y, z)$$ and $$r(\theta, \phi)$$ as input
8. obtain rendering color of each ray by volume rendering $$\hat{C}(r) = \sum_{i=1}^{N} T_i (1 - \exp(- \sigma_{i} \delta_{i})) c_i$$ from two networks 'coarse' and 'fine'
9. compute loss
10. Adam optimizer with learning rate from $$5 \times 10^{-4}$$ to $$5 \times 10^{-5}$$  
11. optimization for a single scene typically takes around 100-300k iterations (1~2 days) to converge on a single NVIDIA V100 GPU  

Here, we use L2 norm for loss  
$$L = \sum_{r \in R} \left[{\left\|\hat{C_c}(r)-C(r)\right\|}^2+{\left\|\hat{C_f}(r)-C(r)\right\|}^2\right]$$  
$$C(r)$$ : GT pixel RGB color  
$$\hat{C_c}(r)$$ : rendering RGB color from coarse network : to allocate better samples in fine network  
$$\hat{C_f}(r)$$ : rendering RGB color from fine network : our goal  

## Results  

#### Datasets

synthetic data, llff data, deep voxel data

#### Comparisons