---
layout: post
title: NeRF
date: 2024-04-10 21:00:00
description: representing scenes as neural radiance fields for view synthesis
tags: nerf rendering 3d
categories: 3d-view-synthesis
thumbnail: assets/img/2024-04-10-NeRF/1.png
giscus_comments: true
related_posts: true
toc:
  beginning: true
  sidebar: right
---

# NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis

#### Ben Mildenhall, Pratul P.Srinivasan, Matthew Tancik  
  
## Introduction

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-04-10-NeRF/3.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

#### Pipeline

- march camera rays to generate sampling of 5D coordinates

- represents volumetric static scene by optimizing continuous 5D function(fully-connected network)

1. input: single continuous `5D coordinate`  
  3D location $$x, y, z$$  
  2D direction $$\theta, \phi$$  
2. output:  
  `volume density` (differential opacity) (how much radiance is accumulated by a ray)  
  `view-dependent RGB color` (emitted radiance) $$c = (r, g, b)$$

- synthesizes novel view by classic `volume rendering` techniques(differentiable) to accumulate(project)(composite) the color/density samples into 2D image along rays

- loss between synthesized and GT observed images

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-04-10-NeRF/2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    Pipeline of NeRF architecture
</div>

  
#### Problem & Solution

Problem :

1. not sufficiently high-resolution representation
2. inefficient in the number of samples per camera ray

Solution :

1. input `positional encoding` for MLP to represent higher frequency function
2. `hierarchical sampling` to reduce the number of queries

  
#### Contribution

- represent continuous scenes as 5D neural radiance fields with basic MLP to render high-resolution novel views
- differentiable volume rendering + hierarchical sampling
- positional encoding to map input 5D coordinate into higher dim. space for high-frequency scene representation
- overcome the storage costs of discretized voxel grids by encoding continuous volume into network's parameters  
=> require only storage costs of sampled volumetric representations

  
## Related Work

  
#### Neural 3D shape representation

- deep networks that map $$xyz$$ coordinates to signed distance functions or occupancy fields
=> limit : need GT 3D geometry

- Niemeyer et al.  
=> input : find directly surface intersection for each ray
(can calculate exact derivatie)  
=> output : diffuse color at each ray intersection location

- Sitzmann et al.  
=> input : each 3D coordinate  
=> output : feature vector and RGB color at each 3D coordinate  
=> rendering by RNN that marches along each ray to decide where the surface is.  

> Limit : oversmoothed renderings, so limited to simple shapes with low geometric complexity

#### View synthesis and image-based rendering

- Given dense sampling of views, novel view synthesis is possible by simple light field sample interpolation

- Given sparser sampling of views, there are 2 ways :  
mesh-based representation and volumetric representation

- Mesh-based representation with either diffuse(난반사) or view-dependent appearance :  
Directly optimize mesh representations by differentiable rasterizers or pathtracers so that we reproject and reconstruct images

> Limit :  
gradient-based optimization is often difficult because of local minima or poor loss landscape  
needs a template mesh with fixed topology for initialization, which is unavailable in real-world

- Volumetric representation :  
well-suited for gradient-based optimization and less distracting artifacts  
train : predict a sampled volumetric representation (voxel grids) from input images  
test : use alpha-(or learned-)compositing along rays to render novel views  
+) alpha-compositing : 여러 frame을 합쳐서 하나의 image로 합성하는 과정으로, 각 이미지 픽셀마다 알파 값(투명도 값)(0~1)이 있어 겹치는 부분의 알파 값 및 픽셀 값을 결정  
CNN compensates discretization artifacts from low resolution voxel grids or CNN allows voxel grids to vary on input time

> Limit :  
good results, but limited by poor time, space complexity due to discrete sampling  
+) discrete sampling : rendering high resol. image => finer sampling of 3D space

> Author's solution :  
encode `continuous` volume into network's parameters  
=> higher quality rendering + require only storage cost of those `sampled` volumetric representations

  
## Neural Radiance Field Scene Representation

represent continuous scene by 5D MLP : (x, d) => (c, $$\sigma$$)

Here, there are 2 key-points!

> multiview consistent :  
c is dependent on both x and d, but $$\sigma$$ is only dependent on location x

- 3D coordinate x => 8 fc-layers => volume-density and 256-dim. feature vector

> Lambertian reflection : diffuse(난반사)  vs  Specular reflection : 전반사  

> non-Lambertian effects : view-dependent color change to represent specularity  

- feature vector is concatenated with direction d => 1 fc-layer => view-dependent RGB color  

  
## Volume Rendering with Radiance Fields

#### Ray

We use `Ray` to synthesize continuous-viewpoint images from discrete input images

> $$r(t) = o + kd$$  
o : the location of camera  
d : viewing direction  

> How to calculate viewing direction d??  
- pixel coordinate :
$$\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}$$  
- normalized coordinate by intrinsic matrix :  
$$\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}$$ = $$K^{-1}$$ $$\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}$$ = $$\begin{bmatrix} 1/f_x & 0 & W/2 \\ 0 & 1/f_y & H/2 \\ 0 & 0 & 1 \end{bmatrix}$$ $$\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}$$  
Since y, z have opposite direction between the real-world coordinate and pixel coordinate, we multiply (-1)  
$$\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}$$ = $$\begin{bmatrix} 1/f_x & 0 & W/2 \\ 0 & -1/f_y & H/2 \\ 0 & 0 & -1 \end{bmatrix}$$ $$\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}$$  
Here, focal length in intrinsic matrix K is usually calculated using camear angle $$\alpha$$ as $$\tan{\alpha / 2} = \frac{h/2}{f}$$  
- 3D coordinate by extrinsic matrix :

We use differential classical volume rendering