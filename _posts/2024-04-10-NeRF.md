---
layout: post
title: NeRF
date: 2024-04-10 21:00:00
description: representing scenes as neural radiance fields for view synthesis
tags: nerf rendering 3d
categories: 3d-view-synthesis
thumbnail: assets/img/2024-04-10-NeRF/1.png
giscus_comments: true
related_posts: true
toc:
  beginning: true
  sidebar: right
---

# NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis

#### Ben Mildenhall, Pratul P.Srinivasan, Matthew Tancik  
  
## Introduction

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-04-10-NeRF/3.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

#### Pipeline

- march camera rays to generate sampling of 5D coordinates

- represents volumetric static scene by optimizing continuous 5D function(fully-connected network)

1. input: single continuous `5D coordinate`  
  3D location $$x, y, z$$  
  2D direction $$\theta, \phi$$  
2. output:  
  `volume density` (differential opacity) (how much radiance is accumulated by a ray)  
  `view-dependent RGB color` (emitted radiance) $$c = (r, g, b)$$

- synthesizes novel view by classic `volume rendering` techniques(differentiable) to accumulate(project)(composite) the color/density samples into 2D image along rays

- loss between synthesized and GT observed images

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-04-10-NeRF/2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    Pipeline of NeRF architecture
</div>

  
#### Problem & Solution

Problem :

1. not sufficiently high-resolution representation
2. inefficient in the number of samples per camera ray

Solution :

1. input `positional encoding` for MLP to represent higher frequency function
2. `hierarchical sampling` to reduce the number of queries

  
#### Contribution

- represent continuous scenes as 5D neural radiance fields with basic MLP to render high-resolution novel views
- differentiable volume rendering + hierarchical sampling
- positional encoding to map input 5D coordinate into higher dim. space for high-frequency scene representation
- overcome the storage costs of discretized voxel grids by encoding continuous volume into network's parameters  
=> require only storage costs of sampled volumetric representations

  
## Related Work

  
#### Neural 3D shape representation

- deep networks that map $$xyz$$ coordinates to signed distance functions or occupancy fields
=> limit : need GT 3D geometry

- Niemeyer et al.  
=> input : find directly surface intersection for each ray
(can calculate exact derivatie)  
=> output : diffuse color at each ray intersection location

- Sitzmann et al.  
=> input : each 3D coordinate  
=> output : feature vector and RGB color at each 3D coordinate  
=> rendering by RNN that marches along each ray to decide where the surface is.  

> Limit : oversmoothed renderings, so limited to simple shapes with low geometric complexity

#### View synthesis and image-based rendering

- Given dense sampling of views, novel view synthesis is possible by simple light field sample interpolation

- Given sparser sampling of views, there are 2 ways :  
mesh-based representation and volumetric representation

- Mesh-based representation with either diffuse(난반사) or view-dependent appearance :  
Directly optimize mesh representations by differentiable rasterizers or pathtracers so that we reproject and reconstruct images

> Limit :  
gradient-based optimization is often difficult because of local minima or poor loss landscape  
needs a template mesh with fixed topology for initialization, which is unavailable in real-world

- Volumetric representation :  
well-suited for gradient-based optimization and less distracting artifacts  
train : predict a sampled volumetric representation (voxel grids) from input images  
test : use alpha-(or learned-)compositing along rays to render novel views  
+) alpha-compositing : 여러 frame을 합쳐서 하나의 image로 합성하는 과정으로, 각 이미지 픽셀마다 알파 값(불투명도 값)(0~1)이 있어 겹치는 부분의 알파 값 및 픽셀 값을 결정  
CNN compensates discretization artifacts from low resolution voxel grids or CNN allows voxel grids to vary on input time

> Limit :  
good results, but limited by poor time, space complexity due to discrete sampling  
+) discrete sampling : rendering high resol. image => finer sampling of 3D space

> Author's solution :  
encode `continuous` volume into network's parameters  
=> higher quality rendering + require only storage cost of those `sampled` volumetric representations

  
## Neural Radiance Field Scene Representation

represent continuous scene by 5D MLP : (x, d) => (c, $$\sigma$$)

Here, there are 2 key-points!

> multiview consistent :  
c is dependent on both x and d, but $$\sigma$$ is only dependent on location x

- 3D coordinate x => 8 fc-layers => volume-density and 256-dim. feature vector

> Lambertian reflection : diffuse(난반사)  vs  Specular reflection : 전반사  

> non-Lambertian effects : view-dependent color change to represent specularity  

- feature vector is concatenated with direction d => 1 fc-layer => view-dependent RGB color  

  
## Volume Rendering with Radiance Fields

#### Ray from input image

We use `Ray` to synthesize `continuous`-viewpoint images from `discrete` input images

> $$r(t) = o + kd$$  
o : the location of camera  
d : viewing direction  

> How to calculate viewing direction d??  
- pixel coordinate :
$$\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}$$  
- normalized coordinate by intrinsic matrix :  
$$\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}$$ = $$K^{-1}$$ $$\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}$$ = $$\begin{bmatrix} 1/f_x & 0 & W/2 \\ 0 & 1/f_y & H/2 \\ 0 & 0 & 1 \end{bmatrix}$$ $$\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}$$  
Since y, z have opposite direction between the real-world coordinate and pixel coordinate, we multiply (-1)  
$$\begin{bmatrix} u \\ v \\ 1 \end{bmatrix}$$ = $$\begin{bmatrix} 1/f_x & 0 & W/2 \\ 0 & -1/f_y & H/2 \\ 0 & 0 & -1 \end{bmatrix}$$ $$\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}$$  
Here, focal length in intrinsic matrix K is usually calculated using camear angle $$\alpha$$ as $$\tan{\alpha / 2} = \frac{h/2}{f}$$  
- 3D coordinate by extrinsic matrix :  
For extrinsic matrix $$[R \vert t]$$,  
$$o = t$$  
$$d = R * \begin{bmatrix} u \\ v \\ 1 \end{bmatrix}$$  
Therefore, we can obtain $$r(t) = o + kd$$  

#### Volume Rendering from MLP output

> We use differential classical volume rendering  

idea : 빛을 많이 투과시킬수록, 물체의 밀도가 높을수록 2D 이미지 픽셀에서 색상 채널의 값이 크게 나타나므로  
pixel's color along a ray is equal to the integral of (transmittance) x (volume density) x (color) for all points along one ray.  

For ray $$r$$,  
expected color $$C(r) = \int_{t_n}^{t_f} T(t) \sigma (r(t)) c(r(t), d) dt$$  
- $$T(t) = exp(- \int_{t_n}^{t} \sigma (r(s)) ds)$$ : transmittance (투과도 값이 클수록 투명한 것)  
T(t) is the solution of $$\frac{dy}{dt} = - \sigma (x)y$$  
since T(t)'s rate of decrease is proportional to T(t) itself and volume density  
- $$\sigma (r(t))$$ : volume density along the ray (learned by MLP)  
- $$c(r(t), d)$$ : object's color along the ray (learned by MLP)  

To apply the equation to our model, we have to do sampling from continuous ray to discrete points  
author divides a ray into N = 64 bins(intervals), and chooses one point $$t_i$$ for each bin by uniform sampling  
$$t_i ~ U \left[t_n + \frac{i-1}{N}(t_f - t_n), t_n + \frac{i}{N}(t_f - t_n)\right]$$  

And then discretized version :  
expected color $$C(r) = \sum_{i=1}^{N} T_i (1 - exp(- \sigma_{i} \delta_{i})) c_i$$  
- $$\delta_{j} = t_{j+1} - t_j$$ : the distance between adjacent samples  
- $$\sigma (r(t)) dt$$ => $$\sigma_{j} \delta_{j}$$ => $$1 - exp(-\sigma_{j} \delta_{j})$$ by Taylor Series Expansion  
- $$$T(t) = exp(- \int_{t_n}^{t} \sigma (r(s)) ds)$$ => $$T_i = exp(- \sum_{j=1}^{i-1} \sigma_{j} \delta_{j})$$  
- $$c(r(t), d)$$ => $$c_i$$  

Here, in volume rendering, author uses `volume density` for 불투명도 == opacity == extinction coefficient == alpha value for alpha-compositing  

Final version :  
expected color $$C(r) = \sum_{i=1}^{N} T_i \alpha_{i} c_i$$  
where $$T_i = \prod_{j=1}^{i-1} (1-\alpha_{j})$$ and $$\alpha_{j} = 1 - exp(-\sigma_{j} \delta_{j})$$  

ddd