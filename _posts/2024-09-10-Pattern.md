---
layout: distill
title: EE534 Pattern Recognition
date: 2024-09-10 11:00:00
description: Lecture Summary (24F)
tags: 3d rendering
categories: cv-tasks
thumbnail: assets/img/2024-09-10-Pattern/0.png
giscus_comments: false
disqus_comments: true
related_posts: true
# toc:
#   beginning: true
#   sidebar: right
# featured: true
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

> Lecture :  
24F EE534 Pattern Recognition  
by KAIST Munchurl Kim [VICLab](https://www.viclab.kaist.ac.kr/)  

## Chapter 1. Overview

### Discriminative vs Generative

- Discriminative model :  
  - learn $$P(Y | X)$$ to maximize $$P(Y | X)$$ directly
  - e.g. logistic regression, SVM, nearest neighbor, CRF, Decision Tree and Random Forest, traditional NN

- Generative model :  
  - learn $$P(X | Y)$$ and $$P(Y)$$ to maximize $$P(X, Y) = P(X | Y)P(Y)$$  
  where can learn $$P(Y | X) \propto P(X | Y)P(Y)$$ indirectly
  - e.g. Bayesian network, Autoregressive model, GAN, Diffuson model

## Chapter 2. Bayes Decision Theory

### Bayes Decision Rule

- conditional probability density :  
Let $$w$$ be state (class)  
Let $$x$$ be data (continous-valued sample)  
  - prior : $$P(w=w_k)$$
  - likelihood : PDF $$P(x | w_k)$$
  - posterior : $$P(w_k | x) = \frac{P(x | w_k)P(w_k)}{P(x)}$$ (Bayes Rule)  
  where $$P(w_1 | x) + P(w_2 | x) + \cdots + P(w_N | x) = 1$$
  - evidence : $$P(x) = \sum_{k=1}^N P(x | w_k)P(w_k) = \sum_{k=1}^N P(x, w_k)$$

- Bayes Decision Rule :  
posterior 더 큰 쪽 고름!
  - Two-class ($$w_1, w_2$$) problem :  
  choose $$w_1$$  
  if $$P(w_1 | x) \gt P(w_2 | x)$$  
  if $$P(x|w_1)P(w_1) \gt P(x|w_2)P(w_2)$$  
  if $$\frac{P(x|w_1)}{P(x|w_2)} \gt \frac{P(w_2)}{P(w_1)}$$  
  (likehood ratio $$\gt$$ threshold)
  - multi-class problem :  
  choose $$w_i$$ where $$P(w_i | x)$$ is the largest

### minimum error

- minimum error :  
GT가 $$w_1, w_2$$ 이고, Predicted가 $$R_1, R_2$$ 일 때,  
  - $$P(error) = \int_{-\infty}^{\infty} P(error, x)dx = \int_{-\infty}^{\infty} P(error|x)P(x)dx$$  
  $$= \int_{R_2}P(w_1|x)P(x)dx + \int_{R_1}P(w_2|x)P(x)dx$$  
  $$= \int_{R_2}P(x|w_1)P(w_1)dx + \int_{R_1}P(x|w_2)P(w_2)dx$$  
  $$= \begin{cases} A+B+D & \text{if} & x_B \\ A+B+C+D & \text{if} & x^{\ast} \end{cases}$$  
  where $$A+B+D$$ is minimum error and $$C$$ is reducible error  
  (아래 그림 참고)
  - $$P(correct)$$  
  $$= \int \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_1}P(x|w_1)P(w_1)dx + \int_{R_2}P(x|w_2)P(w_2)dx$$
  - $$P(error) = 1 - P(correct)$$  
  $$ = 1 - \int \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_2}P(x|w_1)P(w_1)dx + \int_{R_1}P(x|w_2)P(w_2)dx$$

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div> 

- minimum error with rejection :  
decision이 확실하지 않을 때는 classification 자체를 reject하는 게 적절  
(classification error도 줄어들고, correct classification도 줄어듬)  
  - feature space $$x$$ 를 rejection region $$R$$ 과 acceptance region $$A$$ 으로 나눠서  
  rejection region $$R=\{ x | \text{max}_{i} P(w_i | x) \leq 1 - t\}$$ 에서는 reject decision  
  acceptance region $$A=\{ x | \text{max}_{i} P(w_i | x) \gt 1 - t\}$$ 에서는 $$w_1$$ or $$w_2$$ 로 classification decision 수행
  - $$P_c(t) = P(correct)$$  
  $$= \int_{A} \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_1}P(x|w_1)P(w_1)dx + \int_{R_2}P(x|w_2)P(w_2)dx$$  
  - $$P_r(t) = P(reject)$$  
  $$= \int_{R}P(x|w_1)P(w_1)dx + \int_{R}P(x|w_2)P(w_2)dx$$  
  $$= \int_{R} P(x)dx$$  
  - $$P_e(t) = P(error)$$  
  $$= P(error, w_1) + P(error, w_2)$$  
  $$= 1 - P_r(t) - P_c(t)$$
  by 아래 식 대입  
  where $$P(error, w_1) = \int_{-\infty}^{\infty} P(x|w_1)P(w_1)dx - P(reject, w_1) - P(correct, w_1)$$  
  where $$P(error, w_2) = \int_{-\infty}^{\infty} P(x|w_2)P(w_2)dx - P(reject, w_2) - P(correct, w_2)$$  
  where $$\int_{-\infty}^{\infty} P(x|w_1)P(w_1)dx + \int_{-\infty}^{\infty} P(x|w_2)P(w_2)dx = \int_{-\infty}^{\infty} P(x)dx = 1$$
  
- Summary :  
  - $$P(w_i | x)$$ : rejection/acceptance region 구하는 데 사용
  - $$P(x|w_i)P(w_i)$$ : $$P(correct, w_i), P(reject, w_i), P(error, w_i)$$ 구해서  
  $$P_c(t), P_r(t), P_e(t)$$ 구하는 데 사용
  - $$P_c(t) + P_r(t) + P_e(t) = 1$$

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div> 
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/3.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div> 

### Bayes Decision Rule w. Bayes risk

- Bayes risk (minimum overall risk) :  
$$\Omega = \{ w_1, \cdots w_c \}$$ 에서 $$w_j$$ 는 $$j$$ -th class  
$$A = \{ \alpha_{1}, \cdots, \alpha_{c} \}$$ 에서 $$\alpha_{i}$$ 는 class $$w_i$$ 라고 예측하는 action  
$$\lambda(\alpha_{i} | w_j) = \lambda_{ij}$$ : class $$w_j$$ 가 GT일 때, class $$w_i$$ 로 pred. 했을 때의 loss
  - conditional risk for taking action $$\alpha_{i}$$ :  
  특정 input $$x$$ 에 대해  
  $$R(\alpha_{i}|x) = \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j|x)$$
  - overall risk for taking action $$\alpha_{i}$$ :  
  모든 input $$x$$ 에 대해 적분  
  $$R(\alpha_{i}) = \int R(\alpha_{i}|x)P(x)dx$$  
  $$= \int \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j|x) P(x)dx$$  
  $$= \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j) \int P(x|w_j)dx$$  
  $$= \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j)$$  
  $$= \sum_{j=1}^c \lambda_{ij}P(w_j)$$  
  where pdf(likelihood) 합 $$\int P(x|w_j)dx = 1$$  
  - 모든 input $$x$$ 에 대해 가장 loss가 최소인 class $$w_i$$ 로 예측하면,  
  minimum overall risk (= Bayes risk) 를 가짐

- Bayes Decision Rule for Bayes risk :  
  - Two-class ($$w_1, w_2$$) problem :  
  choose $$w_1$$  
  if $$R(\alpha_{1} | x) \lt R(\alpha_{2} | x)$$  
  if $$\lambda_{11}P(w_1 | x) + \lambda_{12}P(w_2 | x) \lt \lambda_{21}P(w_1 | x) + \lambda_{22}P(w_2 | x)$$  
  if $$(\lambda_{21} - \lambda_{11})P(w_1 | x) \gt (\lambda_{12} - \lambda_{22})P(w_2 | x)$$  
  if $$\frac{P(x | w_1)}{P(x | w_2)} \gt \frac{(\lambda_{12} - \lambda_{22})P(w_2)}{(\lambda_{21} - \lambda_{11})P(w_1)}$$  
  if $$\frac{P(x | w_1)}{P(x | w_2)} \gt \frac{P(w_2)}{P(w_1)}$$ for $$\lambda_{11}=\lambda_{22}=0$$ and $$\lambda_{12}=\lambda_{21}$$  
  (likehood ratio $$\gt$$ threshold) (위의 Bayes Decision Rule에서 구한 식과 same)
  - loss $$\lambda(\alpha_{i}|w_j) = \begin{cases} 0 & \text{if} & i=j & (\text{no penalty}) \\ 1 & \text{if} & i \neq j & (\text{equal penalty}) \end{cases}$$ 일 때  
  conditional risk $$R(\alpha_{i} | x) = \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j|x) = \sum_{j=1, j \neq i}^c P(w_j|x) = 1 - P(w_i | x)$$ 이므로  
  Bayes Decision Rule에서 conditional risk $$R(\alpha_{i} | x)$$ 최소화는 posterior $$P(w_i | x)$$ 최대화와 같음
  - multi-class problem :  
  classifieer (discriminant function) (space-partitioning function) $$g(x)$$ 에 대해  
  choose $$w_i$$ where $$g_{i}(x)$$ is the largest  
  s.t. decision boundary is $$g_{i}(x) = g_{j}(x)$$ where they are the two largest discriminant functions  
  e.g. Bayes classifier : $$g_{i}(x) = - R(\alpha_{i} | x)$$ or $$g_{i}(x) = P(w_i | x)$$ or $$g_{i}(x) = P(x | w_i)P(w_i)$$ or $$g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i)$$  
  
### Discriminant Function for Gaussian PDF

- $$G(\boldsymbol x) = \frac{1}{(2\pi)^{\frac{d}{2}} | \Sigma |^{\frac{1}{2}}}e^{-\frac{1}{2}(\boldsymbol x - \boldsymbol \mu)^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu)}$$  
where $$d \times d$$ covariance $$\Sigma = E[(\boldsymbol x - \boldsymbol \mu)(\boldsymbol x - \boldsymbol \mu)^T] = E[\boldsymbol x \boldsymbol x^{T}] - \boldsymbol \mu \boldsymbol \mu^{T} = S - \boldsymbol \mu \boldsymbol \mu^{T}$$  
where $$S = E[\boldsymbol x \boldsymbol x^{T}]$$ : standard autocorrelation matrix  

- Discriminant function for Gaussian PDF :  
likelihood $$P(x | w_i)$$ 를 Gaussian PDF로 둘 경우,  
$$g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{d}{2} \text{ln}(2\pi) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)$$
  - case 1) $$\Sigma_{i} = \sigma^{2} \boldsymbol I$$ (모든 classes에 대해 equal covariance) (등방성(sphere))    
  $$g_{i}(x) = -\frac{\| \boldsymbol x - \boldsymbol \mu_{i} \|^2}{2 \sigma^{2}} + \text{ln}P(w_i)$$  
  $$i$$ 와 관련된 term만 남기면  
  $$g_{i}(x) = \frac{1}{\sigma^{2}} \boldsymbol \mu_{i}^T \boldsymbol x - \frac{1}{2\sigma^{2}} \boldsymbol \mu_{i}^T \boldsymbol \mu_{i} + \text{ln}P(w_i)$$  
  $$= \boldsymbol w_i^T \boldsymbol x + \boldsymbol w_{i0}$$ (linear)  
    - decision boundary :  
    hyperplane $$g(x) = g_{i}(x) - g_{j}(x) = (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T(\boldsymbol x - \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) + \frac{\sigma^{2}}{(\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T} \text{ln}\frac{P(w_i)}{P(w_j)})$$  
    $$= \boldsymbol w^T (\boldsymbol x - \boldsymbol x_0) = 0$$  
    - $$\boldsymbol x_0$$ 를 지나고 $$\boldsymbol w = \boldsymbol \mu_{i} - \boldsymbol \mu_{j}$$ 에 수직인 hyperplane  
    - $$\boldsymbol x_0 = \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) - \frac{\sigma^{2}}{\| \boldsymbol \mu_{i} - \boldsymbol \mu_{j} \|^2} \text{ln}\frac{P(w_i)}{P(w_j)} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})$$ 이므로  
    $$\boldsymbol x_0$$ 의 위치는 $$\boldsymbol \mu_{i}$$ 와 $$\boldsymbol \mu_{j}$$ 의 중점에서 $$\begin{cases} \boldsymbol \mu_{j} \text{쪽으로 이동} & \text{if} & P(w_i) \gt P(w_j) \\ \boldsymbol \mu_{i} \text{쪽으로 이동} & \text{if} & P(w_i) \lt P(w_j) \end{cases}$$  
    ($$P(w_i)$$ 와 $$P(w_j)$$ 중 더 작은 쪽으로 이동)  
    ($$\sigma^{2}$$ 이 ($$\| \mu_{i} - \mu_{j} \|^2$$ 에 비해 비교적) 작은 경우 $$P(w_i)$$ 와 $$P(w_j)$$ 에 따른 $$x_0$$ shift는 미약)

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/4.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Discriminant function for Gaussian PDF :  
likelihood $$P(x | w_i)$$ 를 Gaussian PDF로 둘 경우,  
$$g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{d}{2} \text{ln}(2\pi) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)$$
  - case 2) $$\Sigma_{i} = \Sigma$$ (symmetric) (모든 classes에 대해 equal covariance) (비등방성(hyper-ellipsoidal))  
  $$g_{i}(x) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) + \text{ln}P(w_i)$$  
  $$i$$ 와 관련된 term만 남기면  
  $$g_{i}(x) = \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol x - \frac{1}{2} \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol \mu_{i} + \text{ln}P(w_i)$$  
  $$= \boldsymbol w_i^T \boldsymbol x + \boldsymbol w_{i0}$$ (linear)
    - decision boundary :  
    hyperplane $$g(x) = g_{i}(x) - g_{j}(x) = (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T \Sigma^{-1} (\boldsymbol x - \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) + \frac{1}{(\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T \Sigma^{-1}} \text{ln}\frac{P(w_i)}{P(w_j)})$$  
    $$= \boldsymbol w^T (\boldsymbol x - \boldsymbol x_0) = 0$$  
    - $$\boldsymbol x_0$$ 를 지나고 $$\boldsymbol w = \Sigma^{-1} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})$$ 에 수직인 hyperplane
    - $$\boldsymbol x_0 = \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) - \frac{\text{ln}\frac{P(w_i)}{P(w_j)}}{(\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T \Sigma^{-1} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})$$ 이므로  
    마찬가지로 $$\boldsymbol x_0$$ 의 위치는 $$\boldsymbol \mu_{i}$$ 와 $$\boldsymbol \mu_{j}$$ 의 중점에서 $$P(w_i)$$ 와 $$P(w_j)$$ 중 더 작은 쪽으로 이동  
    - $$\boldsymbol w = \Sigma^{-1} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})$$ 는  
    vector $$\boldsymbol \mu_{i} - \boldsymbol \mu_{j}$$ 를 $$\Sigma^{-1}$$ 로 회전시킨 vector를 의미

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/5.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Discriminant function for Gaussian PDF :  
likelihood $$P(x | w_i)$$ 를 Gaussian PDF로 둘 경우,  
$$g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{d}{2} \text{ln}(2\pi) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)$$
  - case 2) $$\Sigma_{i}$$ is arbitrary (symmetric) (class마다 covariance 다름) (비등방성(hyper-ellipsoidal))  
  $$g_{i}(x) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)$$  
  $$\Sigma_{i}$$ 가 $$i$$ 에 대한 term이므로  
  $$g_{i}(x) = - \frac{1}{2} \boldsymbol x^T \Sigma^{-1} \boldsymbol x + \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol x - \frac{1}{2} \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol \mu_{i} - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)$$  
  $$= - \frac{1}{2} \boldsymbol x^T \Sigma^{-1} \boldsymbol x + \boldsymbol w_i^T \boldsymbol x + \boldsymbol w_{i0}$$ (quadratic) 는  
  quadratic discriminant function in $$x$$  
    - decision surface :  
    hyperquadratic (hyperplane, hypersphere, hyperellipsoidal, hyperparaboloid, hyperhyperboloid)

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/6.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### Bayes Rule for Discrete Case

- pdf 적분 $$\int p(x | w_j) dx$$ 대신  
확률 합 $$lim_{\Delta x \rightarrow 0} \Sigma_{k=-\infty}^{\infty} p(x_k | w_j) \Delta x$$ $$\rightarrow$$ $$\Sigma_{k=1}^m P(v_k | w_j)$$

- Bayes Decision Rule은 discrete case에서도 same  
Bayes risk minimize 위해 conditional risk $$R(\alpha_{i} | x)$$ minimize  
(posterior maximize와 same)

- $$\boldsymbol x = [x_1, x_2, \ldots, x_d]^T$$ 에서 $$x_i$$ 가 0 혹은 1의 값을 갖는 Bernoulli random var.일 때  
  - class $$w_1$$ 일 때 :  
  $$x_i \sim p_i^{x_i}(1-p_i)^{1-x_i}$$  
  $$P(\boldsymbol x | w_1) = P([x_1, x_2, \ldots, x_d]^T | w_1) = \prod_{i=1}^d P(x_i | w_1) = \prod_{i=1}^d p_i^{x_i}(1-p_i)^{1-x_i}$$
  - class $$w_2$$ 일 때 :  
  $$x_i \sim q_i^{x_i}(1-q_i)^{1-x_i}$$  
  $$P(\boldsymbol x | w_2) = P([x_1, x_2, \ldots, x_d]^T | w_2) = \prod_{i=1}^d P(x_i | w_2) = \prod_{i=1}^d q_i^{x_i}(1-q_i)^{1-x_i}$$  
  - likelihood ratio :  
  $$\frac{P(\boldsymbol x | w_1)}{P(\boldsymbol x | w_2)} = \prod_{i=1}^d (\frac{p_i}{q_i})^{x_i}(\frac{1-p_i}{1-q_i})^{1-x_i}$$  
  - discriminant function :  
  choose $$w_1$$  
  if $$g(x) = \text{ln} \frac{P(\boldsymbol x | w_1)P(w_1)}{P(\boldsymbol x | w_2)P(w_2)} = \sum_{i=1}^d(x_i \text{ln}\frac{p_i}{q_i} + (1-x_i)\text{ln}\frac{1-p_i}{1-q_i}) + \text{ln}\frac{P(w_1)}{P(w_2)} = \sum_{i=1}^d w_ix_i + w_0 = \boldsymbol w^T \boldsymbol x + w_0 \gt 0$$  
  where $$w_i = \text{ln}\frac{p_i(1-q_i)}{q_i(1-p_i)}$$ and $$w_0 = \sum_{i=1}^d(\text{ln}\frac{1-p_i}{1-q_i}) + \text{ln}\frac{P(w_1)}{P(w_2)}$$
    - case 1-1) $$p_i = q_i$$  
    $$w_i = 0$$ , so $$x_i$$ 는 class 결정에 영향 없음  
    - case 1-2) $$p_i \gt q_i$$  
    $$w_i \gt 0$$ , so $$x_i = 1$$ 은 class $$w_1$$ 선택에 보탬  
    - case 1-3) $$p_i \lt q_i$$  
    $$w_i \lt 0$$ , so $$x_i = 1$$ 은 class $$w_2$$ 선택에 보탬 
    - case 2-1) $$P(w_1)$$ 값 증가 ($$\gt P(w_2)$$)  
    $$w_0$$ 값이 커지므로 class $$w_1$$ 선택에 보탬  
    - case 2-2) $$P(w_1)$$ 값 감소 ($$\lt P(w_2)$$)  
    $$w_0$$ 값이 작아지므로 class $$w_1$$ 선택에 보탬  

## Chapter 2. Linear Transformation

### Linear Transformation

- $$y = A^Tx$$  
  - mean and variance :  
  $$\mu_{y} = A^T \mu_{x}$$  
  $$\Sigma_{y} = E[(y - \mu_{y})(y - \mu_{y})^T] = A^T \Sigma_{x} A$$
  - Mahalanobis distance :  
  $$d_y^2 = (y - \mu_{y})^T\Sigma_{y}^{-1}(y - \mu_{y}) = \cdots = d_x^2$$  
  `linear transformation`을 해도 Mahalanobis distance는 `그대로`임  
  (Euclidean distance $$(x - \mu_{x})^T(x - \mu_{x})$$ 는 linear transformation을 하면 variant)
  - Gaussian distribution :  
  $$x \sim N(\mu_{x}, \Sigma_{x})$$ 일 때  
  $$P(y) = (2 \pi)^{- \frac{d}{2}} | \Sigma_{y} |^{-\frac{1}{2}} \exp(-\frac{1}{2}(y - \mu_{y})^T \Sigma_{y}^{-1} (y - \mu_{y})) = (2 \pi)^{- \frac{d}{2}} | A |^{-1} | \Sigma_{x} |^{-\frac{1}{2}} \exp(-\frac{1}{2}(x - \mu_{x})^T \Sigma_{x}^{-1} (x - \mu_{x})) = \frac{1}{|A|} P(x)$$

### Orthonormal Transformation

- $$x = \sum_{i=1}^d y_i \phi_{i}$$  
where $$\{ \phi_{i}, \cdots, \phi_{d} \}$$ is orthonormal basis  
Equivalently,  
$$y_i = x^T \phi_{i}$$  
where vector $$x$$ 를 i-th eigenvector $$\phi_{i}$$ 에 project한 게 $$y_i$$
  - approx. $$x$$ :  
    - $$\{ y_{m+1}, \cdots, y_{d} \}$$ 를 pre-defined constants $$\{ b_{m+1}, \cdots, b_{d} \}$$ 로 대체했을 때  
    $$\hat x(m) = \sum_{i=1}^m y_i \phi_{i} + \sum_{i=m+1}^d b_i \phi_{i}$$
  - optimal $$b_i$$ :  
    - error $$\Delta x(m) = x - \hat x(m) = \sum_{i=m+1}^d (y_i - b_i) \phi_{i}$$  
    MSE $$\bar \epsilon^{2}(m) = E[| \Delta x(m) |^2] = E[\Delta x^T(m) \Delta x(m)] = \sum_{i=m+1}^d E[(y_i - b_i)^2]$$  
    - orthonormal basis $$\phi_{i}, \phi_{j}$$ 에 대해  
    $$\frac{\partial}{\partial b_i} E[(y_i - b_i)^2] = -2(E[y_i] - b_i) = 0$$ 이므로  
    MSE 최소화하는 optimal $$b_i = E[y_i]$$  
  - optimal $$\phi_{i}$$ :  
    - $$x = \sum_{j=1}^d y_j \phi_{j}$$ 의 양변에 $$\phi_{i}^T$$ 를 곱하면  
    $$y_i = x^T \phi_{i}$$ 이고  
    optimal $$b_i = E[y_i]$$ 이므로  
    MSE $$\bar \epsilon^{2}(m) = \sum_{i=m+1}^d E[(y_i - b_i)^2] = \sum_{i=m+1}^d E[(x^T \phi_{i} - E[x^T \phi_{i}])^T(x^T \phi_{i} - E[x^T \phi_{i}])] = \sum_{i=m+1}^d \text{Var}(\phi_{i}^{T} x) = \sum_{i=m+1}^d \phi_{i}^T \Sigma_{x} \phi_{i}$$  
    - orthonormality equality constraint $$\phi_{i}^T\phi_{i} = \| \phi_{i} \| = 1$$ 을 만족하면서 MSE $$\bar \epsilon^{2}(m)$$ 를 최소화하는 $$\phi_{i}$$ 는 Lagrange multiplier Method [Link](https://semyeong-yu.github.io/blog/2024/Lagrange/) 로 찾을 수 있다  
    $$\rightarrow$$  
    goal : minimize $$U(m) = \sum_{i=m+1}^d \phi_{i}^T \Sigma_{x} \phi_{i} + \sum_{i=m+1}^d \lambda_{i}(1 - \phi_{i}^T\phi_{i})$$  
    $$\frac{\partial}{\partial x}(x^TAx) = (A + A^T)x = 2Ax$$ for symmetric $$A$$ 이므로  
    $$\frac{\partial}{\partial \phi_{i}} U(m) = 2(\Sigma_{x}\phi_{i} - \lambda_{i}\phi_{i}) = 0$$ 이므로  
    MSE 최소화하는 optimal $$\phi_{i}$$ 는 $$\Sigma_{x}\phi_{i} = \lambda_{i}\phi_{i}$$ 을 만족하므로  
    $$\phi_{i}$$ 와 $$\lambda_{i}$$ 는 covariance matrix $$\Sigma_{x}$$ 의 eigenvector and eigenvalue 이다  

- Eigenvector and Eigenvalue :  
  - $$\Sigma \Phi = \Phi \Lambda$$ where $$\Phi \Phi^{T} = I$$
  - If $$\Sigma$$ is non-singular ($$| \Sigma | \neq 0$$),  
  all eigenvalues $$\lambda$$ are non-zero
  - If $$\Sigma$$ is positive-definite ($$x^T \Sigma x \geq 0$$ for all $$x \neq 0$$),  
  all eigenvalues $$\lambda$$ are positive
  - If $$\Sigma$$ is real and symmetric,  
  all eigenvalues $$\lambda$$ are real  
  and eigenvectors(w. distinct eigenvalues) are orthogonal  
    - pf)  
    $$\Sigma \phi_{i} = \lambda_{i} \phi_{i}$$ and $$\Sigma \phi_{j} = \lambda_{j} \phi_{j}$$  
    $$\phi_{j}^T \Sigma \phi_{i} - \phi_{i}^T \Sigma \phi_{j} = \phi_{j}^T \lambda_{i} \phi_{i} - \phi_{i}^T \lambda_{j} \phi_{j}$$  
    $$0 = (\lambda_{i} - \lambda_{j}) \phi_{j}^T \phi_{i}$$ since $$\Sigma$$ is symmetric  
    $$\rightarrow \phi_{j}^T \phi_{i} = 0$$ (eigenvectors are orthogonal)

- Orthonormal Transformation :  
$$y = \Phi^{T} x$$  
for $$\Phi = [\phi_{1}, \cdots \phi_{d}]$$ and $$\Phi \Phi^{T} = I$$  
  - vector $$x$$ 를 i-th eigenvector $$\phi_{i}$$ 에 project한 게 $$y_{i}$$  
  즉, vector $$x$$ 를 new coordinate $$\Phi = [\phi_{1}, \cdots \phi_{d}]$$ 으로 나타낸 게 vector $$y$$
  - eigenvector는 principal axis를 나타내고, eigenvalue는 해당 방향으로 퍼진 정도를 나타냄
  - $$y$$ 의 covariance matrix인 $$\Sigma_{y}$$ 는 `diagonal matrix`  
  (uncorrelated random vector $$y$$)
    - $$\Sigma_{y}$$  
    $$= \Phi^{T} \Sigma_{x} \Phi$$  
    $$= \Phi^{T} \Phi \Lambda$$ since $$\Sigma \Phi = \Phi \Lambda$$  
    $$= \Phi^{-1} \Phi \Lambda$$ since eigenvector matrix is orthogonal matrix ($$\Phi^{T} = \Phi^{-1}$$)  
    $$= \Lambda$$  
  - distance :  
    - Mahalanobis distance는 any linear transformation에 대해 보존됨  
    - `Euclidean distance`는 linear transformation 중 orthonormal transformation일 때만 `보존`됨  
    $$\| y \|^2 = y^Ty = x^T \Phi \Phi^{T} x = x^T \Phi \Phi^{-1} x = x^T x = \| x \|^2$$

### Whitening Transformation

- Whitening Transformation :  
$$y = \Lambda^{-\frac{1}{2}} \Phi^{T} x = (\Phi \Lambda^{-\frac{1}{2}})^T x$$  
(Orthonormal Transformation을 한 뒤 추가로 $$\Lambda^{-\frac{1}{2}}$$ 로 transformation)
  - $$y$$ 의 covariance matrix인 $$\Sigma_{y}$$ 는 `identity matrix` $$I$$  
    - $$\Sigma_{y}$$  
    $$= (\Lambda^{-\frac{1}{2}} \Phi^{T}) \Sigma_{x} (\Phi \Lambda^{-\frac{1}{2}})$$  
    $$= \Lambda^{-\frac{1}{2}} \Lambda \Lambda^{-\frac{1}{2}}$$  
    $$= I$$
  - $$\Lambda^{-\frac{1}{2}}$$ 은 principal components의 scale을 $$\frac{1}{\sqrt{\lambda_{i}}}$$ 배 하는 효과
  - Whitening Transformation을 한 번 하고나면,  
  그 후에 any Orthonormal Transformation($$y = \Phi^{T} x$$ for $$\Psi \Psi^{T} = I$$)을 해도  
  covariance matrix는 항상 $$\Psi I \Psi^{T} = I$$  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/7.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### Sample Separation

- Sample Separation :  
uncorrelated normal samples $$\sim N(0, I)$$ 로부터 correlated sample $$\sim N(\mu_{x}, \Sigma_{x})$$ 만들기  
  - How? :  
  given data $$x$$ 에서 $$\mu_{x}$$ 를 뺀 뒤 Whitening Transformation 적용하면 $$N(0, I)$$ 이므로 이 과정을 역으로 실행
  - Step 1) Normal distribution으로부터 N개의 $$d$$ -dim. independent vectors를 sampling  
  $$y_1, y_2, \cdots, y_N \sim N(0, I)$$
  - Step 2) Inverse-Whitening-Transformation 적용하여 Normal distribution을 x-space로 변환
  $$x_k = \Phi \Lambda^{\frac{1}{2}} y_k$$  
  for given $$\Sigma_{x}$$  
  and its eigen-decomposition $$\Sigma_{x} \Phi = \Phi \Lambda$$
  - Step 3) x-space의 samples에 $$\mu_{x}$$ 더함  
  $$x_k = \Phi \Lambda^{\frac{1}{2}} y_k + \mu_{x} \sim N(\mu_{x}, \Sigma_{x})$$  
  for given $$\mu_{x}$$

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/8.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

## Chapter 3. Maximum-likelihood and Bayesian Parameter Estimation

- parameter estimation :  
  - Maximum Likelihood Estimation (MLE) :  
  (true) parameters are `unknown`, but `fixed`  
  estimators are random variable
  - Bayesian Estimation :  
  parameters are `random variables` and `prior is known`

### Maximum Likelihood Estimation (MLE)

- Assumption :  
training data $$D_j$$ $$\sim$$ likelihood $$p(D_j | w_j) = N(\mu_{j}, \Sigma_{j})$$  
(i.i.d random samples)

- MLE :  
  - likelihood :  
  $$\hat \theta = \text{argmax}_{\theta} p(D=\{ x_1, x_2, \ldots, x_n \} | \theta) = \text{argmax}_{\theta} \prod_{k=1}^n p(x_k | \theta)$$
  - log-likelihood :  
  $$\hat \theta = \text{argmax}_{\theta} p(D=\{ x_1, x_2, \ldots, x_n \} | \theta) = \text{argmax}_{\theta} \sum_{k=1}^n \text{ln} p(x_k | \theta)$$

- Gaussian likelihood :  
  - unknown $$\mu$$ :  
    - likelihood :  
    $$p(x_k | \mu) = (2 \pi)^{-\frac{d}{2}} | \Sigma |^{-\frac{1}{2}} \text{exp}(-\frac{1}{2}(x_k - \mu)^T \Sigma^{-1} (x_k - \mu))$$  
    $$p(D=\{ x_1, x_2, \ldots, x_N \} | \mu) = \prod_{k=1}^N p(x_k | \mu) = (2 \pi)^{-\frac{dN}{2}} | \Sigma |^{-\frac{N}{2}} \text{exp}(-\frac{1}{2} \sum_{k=1}^N (x_k - \mu)^T \Sigma^{-1} (x_k - \mu))$$  
    - log-likelihood :  
    $$\text{ln} p(D | \mu) = -\frac{dN}{2} \text{ln}(2 \pi) -\frac{N}{2} \text{ln} | \Sigma | -\frac{1}{2} \sum_{k=1}^N (x_k - \mu)^T \Sigma^{-1} (x_k - \mu)$$  
    - matrix derivative :  
    $$\frac{d}{dx}(Ax) = A$$  
    $$\frac{d}{dx}(y^TAx) = A^Ty$$  
    $$\frac{d}{dx}(x^TAx) = (A+A^T)x$$  
    $$\frac{d}{dA}(x^TAx) = xx^T$$  
    $$\frac{\partial |A|}{\partial A} = (\text{adj}(A))^T = |A|(A^{-1})^T$$  
    $$\frac{\partial \text{ln}|A|}{\partial A} = (A^{-1})^T = (A^T)^{-1}$$ where $$|A| = \frac{1}{|A^{-1}|}$$  
    - MLE problem :  
    $$\nabla_{\mu} \text{ln} p(D | \mu) = -\frac{1}{2} \sum_{k=1}^N ((\Sigma^{-1} + (\Sigma^{-1})^T) (x_k - \mu)) \times (-1) = (\Sigma^{-1} + (\Sigma^{-1})^T)(\sum_{k=1}^N x_k - \sum_{k=1}^N \mu) = 0$$  
    $$\sum_{k=1}^N x_k - N \mu = 0$$  
    $$\hat \mu_{\text{MLE}} = \frac{1}{N} \sum_{k=1}^N x_k$$  
    - Summary :  
      - $$\hat \mu_{\text{MLE}} = \frac{1}{N} \sum_{k=1}^N x_k$$  
      (true mean의 MLE estimator는 sample mean)  
      - $$E[\hat \mu_{\text{MLE}}] = \mu$$  
      ($$\hat \mu_{\text{MLE}}$$ 는 `unbiased` estimator)
  - unknown $$\mu$$ and $$\Sigma$$ :  
    - log-likelihood :  
    $$\text{ln} p(D | \mu) = -\frac{dN}{2} \text{ln}(2 \pi) -\frac{N}{2} \text{ln} | \Sigma | -\frac{1}{2} \sum_{k=1}^N (x_k - \mu)^T \Sigma^{-1} (x_k - \mu) = -\frac{dN}{2} \text{ln}(2 \pi) + \frac{N}{2} \text{ln} | \Sigma^{-1} | -\frac{1}{2} \sum_{k=1}^N (x_k - \mu)^T \Sigma^{-1} (x_k - \mu)$$  
    - MLE problem :  
    $$\nabla_{\Sigma^{-1}} \text{ln} p(D | \mu) = \frac{N}{2}\Sigma^{T} - \frac{1}{2} \sum_{k=1}^N (x_k - \mu)(x_k - \mu)^T = 0$$  
    $$N \Sigma^{T} = \sum_{k=1}^N (x_k - \mu)(x_k - \mu)^T$$  
    $$\mu = \hat \mu_{\text{MLE}}$$ 대입하고, $$\Sigma$$ 는 symmetric($$\Sigma^{T} = \Sigma$$)하므로  
    $$\hat \Sigma_{\text{MLE}} = \frac{1}{N} \sum_{k=1}^N (x_k - \hat \mu_{\text{MLE}})(x_k - \hat \mu_{\text{MLE}})^T$$  
    - Summary :  
      - $$\hat \Sigma_{\text{MLE}} = \frac{1}{N} \sum_{k=1}^N (x_k - \hat \mu_{\text{MLE}})(x_k - \hat \mu_{\text{MLE}})^T$$  
      ($$\mu$$ 먼저 estimate한 뒤 $$\Sigma$$ estimate)
      - $$E[\hat \Sigma_{\text{MLE}}] = \frac{1}{N} E[\sum_{k=1}^N (x_k - \hat \mu_{\text{MLE}})(x_k - \hat \mu_{\text{MLE}})^T] = \frac{N-1}{N} \Sigma \neq \Sigma$$  
      ($$\hat \Sigma_{\text{MLE}}$$ 는 `biased` estimator)
        - pf) 아래 식 이용  
        $$E[x_i x_j^T] = \begin{cases} \Sigma + \mu \mu^{T} & \text{if} & i = j \\ \mu \mu^{T} & \text{if} & i \neq j \end{cases}$$  
        since $$\Sigma = E[(x - \mu)(x - \mu)^T] = \cdots = E[xx^T] - \mu \mu^{T}$$  
        since $$0 = E[(x_i - \mu)(x_j - \mu)^T] = E[x_i x_j^T] - \mu \mu^{T}$$ by independence $$i \neq j$$
      - $$\text{lim}_{N \rightarrow \infty}E[\hat \Sigma_{\text{MLE}}] = \text{lim}_{N \rightarrow \infty} \frac{N-1}{N} \Sigma = \Sigma$$  
      ($$\hat \Sigma_{\text{MLE}}$$ 는 `asymptotically unbiased` estimator)  
      또는  
      $$\hat \Sigma_{\text{MLE}} = \frac{1}{N-1} \sum_{k=1}^N (x_k - \hat \mu_{\text{MLE}})(x_k - \hat \mu_{\text{MLE}})^T$$  
      (위처럼 설정하면 $$\hat \Sigma_{\text{MLE}}$$ 는 `unbiased` estimator)

- MLE :  
  - MLE is `asymptotically consistent`  
  if $$\text{lim}_{N \rightarrow \infty} P(\| \hat \theta_{\text{MLE}} - \theta_{\text{true}} \| \leq \epsilon) = 1$$ for arbitrary small $$\epsilon$$  
  (sample 수 $$N$$ 이 크면 param. estimate은 true value랑 거의 비슷)  
  by central limit theorem and the fact that MLE is related to the sum of random var.
  - MLE is `asymptotically efficient`  
  since MLE는 Cramer-Rao lower bound(any estimate이 달성할 수 있는 the lowest value of variance)

### Bayesisan Estimation

- Summary :  
  - MLE (maximum likelihood estimation) :  
    - when $$\theta$$ is unknown, but fixed
    - maximize likelihood $$\hat \theta_{MLE} = \text{argmax}_{\theta} p(D | \theta)$$
  - MAP (maximum a posterior) :  
    - when $$\theta$$ is random var. and prior $$p(\theta)$$ is known
    - maximize posterior $$\hat \theta_{MAP} = \text{argmax}_{\theta} p(\theta | D) = \text{argmax}_{\theta} \text{ln} p(D | \theta) + \text{ln} p(\theta)$$
  - If prior $$p(\theta)$$ is constant (uniform distribution),  
  MLE와 MAP는 same

- prior $$p(\theta)$$ 와 posterior $$p(\theta | D)$$ 가 같은 확률 분포의 형태를 가질 경우  
prior $$p(\theta)$$ 를  
likehood $$p(D | \theta)$$ 에 대한 `conjugate prior`라고 말한다

- Gaussian case :  
  - random var. $$\mu$$ :  
    - likelihood and conjugate prior :  
    $$x_k \sim$$ $$p(x_k | \mu) = N(\mu, \sigma^{2})$$  
    where $$\mu \sim$$ $$p(\mu) = N(\mu_{0}, \sigma_{0}^{2})$$
    - posterior (수식 유도는 아래에 별도로) :  
    $$p(\mu | D, \sigma^{2}) \propto N(\mu_{N}, \sigma_{N}^{2})$$  
    $$= \frac{1}{\sqrt{2 \pi} \sigma_{N}} e^{-\frac{1}{2}(\frac{\mu - \mu_{N}}{\sigma_{N}})^2}$$  
    where $$\hat \mu_{MAP} = \text{argmax}_{\mu} p(\mu | D, \sigma^{2}) = \mu_{N} = (\frac{N}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}}) \hat \mu_{MLE} + \frac{\frac{\sigma^{2}}{\sigma_{0}^{2}}}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}} \mu_{0}$$  
    where $$\sigma_{N}^{2} = \frac{\sigma^{2}}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}}$$  
      - Bayesian Learning :  
      $$N$$, 즉 sample 수가 많아질수록  
      $$\mu_{N}$$ 은 $$\hat \mu_{MLE}$$ 에 가까워지고  
      $$\sigma_{N}^{2}$$, 즉 uncertainty about $$\mu_{N}$$ 은 감소  
      따라서 $$N \rightarrow \infty$$ 이면  
      posterior $$p(\mu | D, \sigma^{2})$$ 는 $$\mu_{N} = \hat \mu_{MLE}$$ 에서의 Dirac delta function
      - $$\hat \mu_{MAP} = \mu_{N}$$ 에서  
      $$(\frac{N}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}}) \hat \mu_{MLE}$$ 는 empirical data samples 부분이고  
      $$\frac{\frac{\sigma^{2}}{\sigma_{0}^{2}}}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}} \mu_{0}$$ 는 prior info. 부분
      - 만약 $$\sigma_{0}^{2} = 0$$ 이라면  
      prior : variance $$\sigma_{0}^{2}$$ 가 매우 작으므로, certain that $$\mu = \mu_{0}$$  
      So,  
      posterior : $$\mu_{N} = \mu_{0}$$ (data samples는 $$\mu_{N}$$ 에 영향 없음)
      - 만약 $$\sigma^{2} \ll \sigma_{0}^{2}$$ 이라면  
      prior : variance $$\sigma_{0}^{2}$$ 가 매우 크므로, so uncertain that $$\mu = \mu_{0}$$  
      So,  
      posterior : $$\mu_{N} = \hat \mu_{MLE}$$ (data samples가 $$\mu_{N}$$ 에 대부분의 영향 미침)
    - posterior (수식 유도) :  
    $$p(\mu | D, \sigma^{2})$$  
    $$\propto p(D | \mu, \sigma^{2}) p(\mu) = \frac{1}{(2 \pi \sigma^{2})^{\frac{N}{2}}(2 \pi \sigma_{0}^{2})^{\frac{1}{2}}}e^{-\frac{1}{2\sigma^{2}}\sum_{i=1}^N(x_i - \mu)^2 -\frac{1}{2\sigma_{0}^{2}}(\mu - \mu_{0})^2}$$  
    $$\propto e^{-\frac{1}{2\sigma^{2}}\sum_{i=1}^N(x_i - \mu)^2 -\frac{1}{2\sigma_{0}^{2}}(\mu - \mu_{0})^2}$$  
    $$\propto e^{-\frac{1}{2\sigma^{2}}(N \mu^{2} - 2 \mu \sum_{i=1}^N x_i) -\frac{1}{2\sigma_{0}^{2}}(\mu^{2} - 2 \mu \mu_{0})}$$  
    $$= e^{-\frac{1}{2}(\mu^{2}(\frac{N}{\sigma^{2}} + \frac{1}{\sigma_{0}^{2}}) - 2 \mu (\frac{N \hat \mu_{MLE}}{\sigma^{2}} + \frac{\mu_{0}}{\sigma_{0}^{2}}))}$$  
    $$\propto \frac{1}{\sqrt{2 \pi} \sigma_{N}} e^{-\frac{1}{2}(\frac{\mu - \mu_{N}}{\sigma_{N}})^2}$$  
    where $$\hat \mu_{MAP} = \mu_{N} = (\frac{N\sigma_{0}^{2}}{N\sigma_{0}^{2} + \sigma^{2}}) \hat \mu_{MLE} + \frac{\sigma^{2}}{N\sigma_{0}^{2} + \sigma^{2}} \mu_{0}$$  
    where $$\sigma_{N}^{2} = \frac{\sigma_{0}^{2} \sigma^{2}}{N\sigma_{0}^{2} + \sigma^{2}}$$

### Principal Component Analysis (PCA)

- dimensionality reduction w/o losing much info.

28p TBD

### Singular Value Decomposition (SVD)

- Singular Vale Decomposition (SVD) :  
  - 쓰임 : matrix factorization  
    - low-rank approximation of matrix
    - pseudo-inverse of non-square matrix
  - $$A = U \Sigma V^T = \begin{bmatrix} u_1 & u_2 & \cdots & u_m \end{bmatrix} \begin{bmatrix} \sigma_{1} & \cdots & 0 & \cdots & 0 & 0 & \cdots & 0 \\ \vdots & \ddots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\ 0 & \cdots & \sigma_{r} & \cdots & 0 & 0 & \cdots & 0 \\ \vdots & \ddots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\ 0 & \cdots & 0 & \cdots & \sigma_{m} & 0 & \cdots & 0 \end{bmatrix} \begin{bmatrix} v_1^{T} \\ u_2^{T} \\ \vdots & u_n^{T} \end{bmatrix}$$  
  where $$U$$ is $$m \times m$$ orthonormal matrix ($$UU^T = I$$)  
  where $$V$$ is $$n \times n$$ orthonormal matrix ($$VV^T = I$$)  
  where $$\Sigma$$ is $$m \times n$$ diagonal matrix ($$\sigma_{1} \geq \sigma_{2} \geq \cdots \geq \sigma_{m} \geq 0$$)  
    - $$\Sigma$$ 구하는 법 :  
    $$\sigma_{i} = \sqrt{\lambda_{i}}$$ where $$\lambda_{i}$$ is eigenvalue of $$A^{T}A$$
  - $$r = \text{rank}(A) = \text{rank}(\Sigma) \leq \text{min}(m, n)$$ 에 대해  
  ($$\sigma_{1} \geq \sigma_{2} \geq \cdots \geq \sigma_{r} \geq \sigma_{r+1} = \cdots = \sigma_{m} = 0$$)
    - $$\text{col}(A)$$ is spanned by $$u_1, \cdots, u_r$$  
    - $$\text{null}(A^T)$$ is spanned by $$u_{r+1}, \cdots, u_m$$ where $$A^T \begin{bmatrix} u_{r+1} & \cdots & u_m \end{bmatrix} = 0$$  
    - $$\text{row}(A)$$ is spanned by $$v_{1}^{T}, \cdots, v_{r}^{T}$$  
    - $$\text{null}(A)$$ is spanned by $$v_{r+1}^{T}, \cdots, v_{n}^{T}$$ where $$A \begin{bmatrix} v_{r+1} & \cdots & v_n \end{bmatrix} = 0$$
  - $$A = TBD$$

