---
layout: distill
title: EE534 Pattern Recognition
date: 2024-09-10 11:00:00
description: Lecture Summary (24F)
tags: 3d rendering
categories: cv-tasks
thumbnail: assets/img/2024-09-10-Pattern/1.png
giscus_comments: false
disqus_comments: true
related_posts: true
# toc:
#   beginning: true
#   sidebar: right
# featured: true
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

> Lecture :  
24F EE534 Pattern Recognition  
by KAIST Munchurl Kim [VICLab](https://www.viclab.kaist.ac.kr/)  

## Chapter 1. Overview

### Discriminative vs Generative

- Discriminative model :  
  - learn $$P(Y|X)$$ s.t. maximize $$P(Y|X)$$ directly 
  - e.g. logistic regression, SVM, nearest neighbor, CRF, Decision Tree and Random Forest, traditional NN

- Generative model :  
  - learn $$P(X|Y)$$ and $$P(Y)$$ s.t. maximize $$P(X, Y) = P(X|Y)P(Y)$$  
  where can learn $$P(Y|X) \propto P(X|Y)P(Y)$$ indirectly
  - e.g. Bayesian network, Autoregressive model, GAN, Diffuson model

## Chapter 2. Bayes Decision Theory

### Bayes Decision Rule

- conditional probability density :  
Let $$w$$ be state (class)  
Let $$x$$ be data (continous-valued sample)  
  - prior : $$P(w=w_k)$$
  - likelihood : pdf $$P(x|w_k)$$  
  - posterior : $$P(w_k | x) = \frac{P(x|w_k)P(w_k)}{P(x)}$$ (Bayes Rule)  
  where $$P(w_1 | x) + P(w_2 | x) + \cdots + P(w_N | x) = 1$$
  - evidence : $$P(x) = \sum_{k=1}^N P(x|w_k)P(w_k) = \sum_{k=1}^N P(x, w_k)$$

- Two-class ($$w_1, w_2$$) problem :  
choose $$w_1$$ if  
$$P(w_1 | x) \gt P(w_2 | x)$$  
$$\frac{P(x|w_1)}{P(x|w_2)} \gt \frac{P(w_2)}{P(w_1)}$$  
