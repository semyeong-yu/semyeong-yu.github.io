---
layout: distill
title: EE534 Pattern Recognition
date: 2024-09-10 11:00:00
description: Lecture Summary (24F)
tags: 3d rendering
categories: cv-tasks
thumbnail: assets/img/2024-09-10-Pattern/0.png
giscus_comments: false
disqus_comments: true
related_posts: true
# toc:
#   beginning: true
#   sidebar: right
# featured: true
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

> Lecture :  
24F EE534 Pattern Recognition  
by KAIST Munchurl Kim [VICLab](https://www.viclab.kaist.ac.kr/)  

## Chapter 1. Overview

### Discriminative vs Generative

- Discriminative model :  
  - learn $$P(Y | X)$$ to maximize $$P(Y | X)$$ directly
  - e.g. logistic regression, SVM, nearest neighbor, CRF, Decision Tree and Random Forest, traditional NN

- Generative model :  
  - learn $$P(X | Y)$$ and $$P(Y)$$ to maximize $$P(X, Y) = P(X | Y)P(Y)$$  
  where can learn $$P(Y | X) \propto P(X | Y)P(Y)$$ indirectly
  - e.g. Bayesian network, Autoregressive model, GAN, Diffuson model

## Chapter 2. Bayes Decision Theory

### Bayes Decision Rule

- conditional probability density :  
Let $$w$$ be state (class)  
Let $$x$$ be data (continous-valued sample)  
  - prior : $$P(w=w_k)$$
  - likelihood : PDF $$P(x | w_k)$$
  - posterior : $$P(w_k | x) = \frac{P(x | w_k)P(w_k)}{P(x)}$$ (Bayes Rule)  
  where $$P(w_1 | x) + P(w_2 | x) + \cdots + P(w_N | x) = 1$$
  - evidence : $$P(x) = \sum_{k=1}^N P(x | w_k)P(w_k) = \sum_{k=1}^N P(x, w_k)$$

- Bayes Decision Rule :  
posterior 더 큰 쪽 고름!
  - Two-class ($$w_1, w_2$$) problem :  
  choose $$w_1$$  
  if $$P(w_1 | x) \gt P(w_2 | x)$$  
  if $$P(x|w_1)P(w_1) \gt P(x|w_2)P(w_2)$$  
  if $$\frac{P(x|w_1)}{P(x|w_2)} \gt \frac{P(w_2)}{P(w_1)}$$  
  (likehood ratio $$\gt$$ threshold)
  - multi-class problem :  
  choose $$w_i$$ where $$P(w_i | x)$$ is the largest

### minimum error

- minimum error :  
GT가 $$w_1, w_2$$ 이고, Predicted가 $$R_1, R_2$$ 일 때,  
  - $$P(error) = \int_{-\infty}^{\infty} P(error, x)dx = \int_{-\infty}^{\infty} P(error|x)P(x)dx$$  
  $$= \int_{R_2}P(w_1|x)P(x)dx + \int_{R_1}P(w_2|x)P(x)dx$$  
  $$= \int_{R_2}P(x|w_1)P(w_1)dx + \int_{R_1}P(x|w_2)P(w_2)dx$$  
  $$= \begin{cases} A+B+D & \text{if} & x_B \\ A+B+C+D & \text{if} & x^{\ast} \end{cases}$$  
  where $$A+B+D$$ is minimum error and $$C$$ is reducible error  
  (아래 그림 참고)
  - $$P(correct)$$  
  $$= \int \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_1}P(x|w_1)P(w_1)dx + \int_{R_2}P(x|w_2)P(w_2)dx$$
  - $$P(error) = 1 - P(correct)$$  
  $$ = 1 - \int \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_2}P(x|w_1)P(w_1)dx + \int_{R_1}P(x|w_2)P(w_2)dx$$

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div> 

- minimum error with rejection :  
decision이 확실하지 않을 때는 classification 자체를 reject하는 게 적절  
(classification error도 줄어들고, correct classification도 줄어듬)  
  - feature space $$x$$ 를 rejection region $$R$$ 과 acceptance region $$A$$ 으로 나눠서  
  rejection region $$R=\{ x | \text{max}_{i} P(w_i | x) \leq 1 - t\}$$ 에서는 reject decision  
  acceptance region $$A=\{ x | \text{max}_{i} P(w_i | x) \gt 1 - t\}$$ 에서는 $$w_1$$ or $$w_2$$ 로 classification decision 수행
  - $$P_c(t) = P(correct)$$  
  $$= \int_{A} \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_1}P(x|w_1)P(w_1)dx + \int_{R_2}P(x|w_2)P(w_2)dx$$  
  - $$P_r(t) = P(reject)$$  
  $$= \int_{R}P(x|w_1)P(w_1)dx + \int_{R}P(x|w_2)P(w_2)dx$$  
  $$= \int_{R} P(x)dx$$  
  - $$P_e(t) = P(error)$$  
  $$= P(error, w_1) + P(error, w_2)$$  
  $$= 1 - P_r(t) - P_c(t)$$
  by 아래 식 대입  
  where $$P(error, w_1) = \int_{-\infty}^{\infty} P(x|w_1)P(w_1)dx - P(reject, w_1) - P(correct, w_1)$$  
  where $$P(error, w_2) = \int_{-\infty}^{\infty} P(x|w_2)P(w_2)dx - P(reject, w_2) - P(correct, w_2)$$  
  where $$\int_{-\infty}^{\infty} P(x|w_1)P(w_1)dx + \int_{-\infty}^{\infty} P(x|w_2)P(w_2)dx = \int_{-\infty}^{\infty} P(x)dx = 1$$
  
- Summary :  
  - $$P(w_i | x)$$ : rejection/acceptance region 구하는 데 사용
  - $$P(x|w_i)P(w_i)$$ : $$P(correct, w_i), P(reject, w_i), P(error, w_i)$$ 구해서  
  $$P_c(t), P_r(t), P_e(t)$$ 구하는 데 사용
  - $$P_c(t) + P_r(t) + P_e(t) = 1$$

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div> 
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/3.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div> 

### Bayes Decision Rule w. Bayes risk

- Bayes risk (minimum overall risk) :  
$$\Omega = \{ w_1, \cdots w_c \}$$ 에서 $$w_j$$ 는 $$j$$ -th class  
$$A = \{ \alpha_{1}, \cdots, \alpha_{c} \}$$ 에서 $$\alpha_{i}$$ 는 class $$w_i$$ 라고 예측하는 action  
$$\lambda(\alpha_{i} | w_j) = \lambda_{ij}$$ : class $$w_j$$ 가 GT일 때, class $$w_i$$ 로 pred. 했을 때의 loss
  - conditional risk for taking action $$\alpha_{i}$$ :  
  특정 input $$x$$ 에 대해  
  $$R(\alpha_{i}|x) = \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j|x)$$
  - overall risk for taking action $$\alpha_{i}$$ :  
  모든 input $$x$$ 에 대해 적분  
  $$R(\alpha_{i}) = \int R(\alpha_{i}|x)P(x)dx$$  
  $$= \int \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j|x) P(x)dx$$  
  $$= \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j) \int P(x|w_j)dx$$  
  $$= \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j)$$  
  $$= \sum_{j=1}^c \lambda_{ij}P(w_j)$$  
  where pdf(likelihood) 합 $$\int P(x|w_j)dx = 1$$  
  - 모든 input $$x$$ 에 대해 가장 loss가 최소인 class $$w_i$$ 로 예측하면,  
  minimum overall risk (= Bayes risk) 를 가짐

- Bayes Decision Rule for Bayes risk :  
  - Two-class ($$w_1, w_2$$) problem :  
  choose $$w_1$$  
  if $$R(\alpha_{1} | x) \lt R(\alpha_{2} | x)$$  
  if $$\lambda_{11}P(w_1 | x) + \lambda_{12}P(w_2 | x) \lt \lambda_{21}P(w_1 | x) + \lambda_{22}P(w_2 | x)$$  
  if $$(\lambda_{21} - \lambda_{11})P(w_1 | x) \gt (\lambda_{12} - \lambda_{22})P(w_2 | x)$$  
  if $$\frac{P(x | w_1)}{P(x | w_2)} \gt \frac{(\lambda_{12} - \lambda_{22})P(w_2)}{(\lambda_{21} - \lambda_{11})P(w_1)}$$  
  if $$\frac{P(x | w_1)}{P(x | w_2)} \gt \frac{P(w_2)}{P(w_1)}$$ for $$\lambda_{11}=\lambda_{22}=0$$ and $$\lambda_{12}=\lambda_{21}$$  
  (likehood ratio $$\gt$$ threshold) (위의 Bayes Decision Rule에서 구한 식과 same)
  - loss $$\lambda(\alpha_{i}|w_j) = \begin{cases} 0 & \text{if} & i=j & (\text{no penalty}) \\ 1 & \text{if} & i \neq j & (\text{equal penalty}) \end{cases}$$ 일 때  
  conditional risk $$R(\alpha_{i} | x) = \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j|x) = \sum_{j=1, j \neq i}^c P(w_j|x) = 1 - P(w_i | x)$$ 이므로  
  Bayes Decision Rule에서 conditional risk $$R(\alpha_{i} | x)$$ 최소화는 posterior $$P(w_i | x)$$ 최대화와 같음
  - multi-class problem :  
  classifieer (discriminant function) (space-partitioning function) $$g(x)$$ 에 대해  
  choose $$w_i$$ where $$g_{i}(x)$$ is the largest  
  s.t. decision boundary is $$g_{i}(x) = g_{j}(x)$$ where they are the two largest discriminant functions  
  e.g. Bayes classifier : $$g_{i}(x) = - R(\alpha_{i} | x)$$ or $$g_{i}(x) = P(w_i | x)$$ or $$g_{i}(x) = P(x | w_i)P(w_i)$$ or $$g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i)$$  
  
### Discriminant Function for Gaussian PDF

- $$G(\boldsymbol x) = \frac{1}{(2\pi)^{\frac{d}{2}} | \Sigma |^{\frac{1}{2}}}e^{-\frac{1}{2}(\boldsymbol x - \boldsymbol \mu)^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu)}$$  
where $$d \times d$$ covariance $$\Sigma = E[(\boldsymbol x - \boldsymbol \mu)(\boldsymbol x - \boldsymbol \mu)^T] = E[\boldsymbol x \boldsymbol x^{T}] - \boldsymbol \mu \boldsymbol \mu^{T} = S - \boldsymbol \mu \boldsymbol \mu^{T}$$  
where $$S = E[\boldsymbol x \boldsymbol x^{T}]$$ : standard autocorrelation matrix  

- Discriminant function for Gaussian PDF :  
likelihood $$P(x | w_i)$$ 를 Gaussian PDF로 둘 경우,  
$$g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{d}{2} \text{ln}(2\pi) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)$$
  - case 1) $$\Sigma_{i} = \sigma^{2} \boldsymbol I$$ (모든 classes에 대해 equal covariance) (등방성(sphere))    
  $$g_{i}(x) = -\frac{\| \boldsymbol x - \boldsymbol \mu_{i} \|^2}{2 \sigma^{2}} + \text{ln}P(w_i)$$  
  $$i$$ 와 관련된 term만 남기면  
  $$g_{i}(x) = \frac{1}{\sigma^{2}} \boldsymbol \mu_{i}^T \boldsymbol x - \frac{1}{2\sigma^{2}} \boldsymbol \mu_{i}^T \boldsymbol \mu_{i} + \text{ln}P(w_i)$$  
  $$= \boldsymbol w_i^T \boldsymbol x + \boldsymbol w_{i0}$$ (linear)  
    - decision boundary :  
    hyperplane $$g(x) = g_{i}(x) - g_{j}(x) = (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T(\boldsymbol x - \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) + \frac{\sigma^{2}}{(\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T} \text{ln}\frac{P(w_i)}{P(w_j)})$$  
    $$= \boldsymbol w^T (\boldsymbol x - \boldsymbol x_0) = 0$$  
    - $$\boldsymbol x_0$$ 를 지나고 $$\boldsymbol w = \boldsymbol \mu_{i} - \boldsymbol \mu_{j}$$ 에 수직인 hyperplane  
    - $$\boldsymbol x_0 = \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) - \frac{\sigma^{2}}{\| \boldsymbol \mu_{i} - \boldsymbol \mu_{j} \|^2} \text{ln}\frac{P(w_i)}{P(w_j)} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})$$ 이므로  
    $$\boldsymbol x_0$$ 의 위치는 $$\boldsymbol \mu_{i}$$ 와 $$\boldsymbol \mu_{j}$$ 의 중점에서 $$\begin{cases} \boldsymbol \mu_{j} \text{쪽으로 이동} & \text{if} & P(w_i) \gt P(w_j) \\ \boldsymbol \mu_{i} \text{쪽으로 이동} & \text{if} & P(w_i) \lt P(w_j) \end{cases}$$  
    ($$P(w_i)$$ 와 $$P(w_j)$$ 중 더 작은 쪽으로 이동)  
    ($$\sigma^{2}$$ 이 ($$\| \mu_{i} - \mu_{j} \|^2$$ 에 비해 비교적) 작은 경우 $$P(w_i)$$ 와 $$P(w_j)$$ 에 따른 $$x_0$$ shift는 미약)

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/4.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Discriminant function for Gaussian PDF :  
likelihood $$P(x | w_i)$$ 를 Gaussian PDF로 둘 경우,  
$$g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{d}{2} \text{ln}(2\pi) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)$$
  - case 2) $$\Sigma_{i} = \Sigma$$ (symmetric) (모든 classes에 대해 equal covariance) (비등방성(hyper-ellipsoidal))  
  $$g_{i}(x) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) + \text{ln}P(w_i)$$  
  $$i$$ 와 관련된 term만 남기면  
  $$g_{i}(x) = \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol x - \frac{1}{2} \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol \mu_{i} + \text{ln}P(w_i)$$  
  $$= \boldsymbol w_i^T \boldsymbol x + \boldsymbol w_{i0}$$ (linear)
    - decision boundary :  
    hyperplane $$g(x) = g_{i}(x) - g_{j}(x) = (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T \Sigma^{-1} (\boldsymbol x - \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) + \frac{1}{(\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T \Sigma^{-1}} \text{ln}\frac{P(w_i)}{P(w_j)})$$  
    $$= \boldsymbol w^T (\boldsymbol x - \boldsymbol x_0) = 0$$  
    - $$\boldsymbol x_0$$ 를 지나고 $$\boldsymbol w = \Sigma^{-1} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})$$ 에 수직인 hyperplane
    - $$\boldsymbol x_0 = \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) - \frac{\text{ln}\frac{P(w_i)}{P(w_j)}}{(\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T \Sigma^{-1} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})$$ 이므로  
    마찬가지로 $$\boldsymbol x_0$$ 의 위치는 $$\boldsymbol \mu_{i}$$ 와 $$\boldsymbol \mu_{j}$$ 의 중점에서 $$P(w_i)$$ 와 $$P(w_j)$$ 중 더 작은 쪽으로 이동  
    - $$\boldsymbol w = \Sigma^{-1} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})$$ 는  
    vector $$\boldsymbol \mu_{i} - \boldsymbol \mu_{j}$$ 를 $$\Sigma^{-1}$$ 로 회전시킨 vector를 의미

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/5.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Discriminant function for Gaussian PDF :  
likelihood $$P(x | w_i)$$ 를 Gaussian PDF로 둘 경우,  
$$g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{d}{2} \text{ln}(2\pi) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)$$
  - case 2) $$\Sigma_{i}$$ is arbitrary (symmetric) (class마다 covariance 다름) (비등방성(hyper-ellipsoidal))  
  $$g_{i}(x) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)$$  
  $$\Sigma_{i}$$ 가 $$i$$ 에 대한 term이므로 $$- \frac{1}{2} \boldsymbol x^T \Sigma_{i}^{-1} \boldsymbol x$$ term이 안 지워져서  
  $$g_{i}(x) = - \frac{1}{2} \boldsymbol x^T \Sigma_{i}^{-1} \boldsymbol x + \boldsymbol \mu_{i}^T \Sigma_{i}^{-1} \boldsymbol x - \frac{1}{2} \boldsymbol \mu_{i}^T \Sigma_{i}^{-1} \boldsymbol \mu_{i} - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)$$  
  $$= - \frac{1}{2} \boldsymbol x^T \Sigma_{i}^{-1} \boldsymbol x + \boldsymbol w_i^T \boldsymbol x + \boldsymbol w_{i0}$$ (quadratic) 는  
  quadratic discriminant function in $$x$$  
    - decision surface :  
    hyperquadratic (hyperplane, hypersphere, hyperellipsoidal, hyperparaboloid, hyperhyperboloid)

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/6.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### Bayes Rule for Discrete Case

- pdf 적분 $$\int p(x | w_j) dx$$ 대신  
확률 합 $$lim_{\Delta x \rightarrow 0} \Sigma_{k=-\infty}^{\infty} p(x_k | w_j) \Delta x$$ $$\rightarrow$$ $$\Sigma_{k=1}^m P(v_k | w_j)$$

- Bayes Decision Rule은 discrete case에서도 same  
Bayes risk minimize 위해 conditional risk $$R(\alpha_{i} | x)$$ minimize  
(posterior maximize와 same)

- $$\boldsymbol x = [x_1, x_2, \ldots, x_d]^T$$ 에서 $$x_i$$ 가 0 혹은 1의 값을 갖는 Bernoulli random var.일 때  
  - class $$w_1$$ 일 때 :  
  $$x_i \sim p_i^{x_i}(1-p_i)^{1-x_i}$$  
  $$P(\boldsymbol x | w_1) = P([x_1, x_2, \ldots, x_d]^T | w_1) = \prod_{i=1}^d P(x_i | w_1) = \prod_{i=1}^d p_i^{x_i}(1-p_i)^{1-x_i}$$
  - class $$w_2$$ 일 때 :  
  $$x_i \sim q_i^{x_i}(1-q_i)^{1-x_i}$$  
  $$P(\boldsymbol x | w_2) = P([x_1, x_2, \ldots, x_d]^T | w_2) = \prod_{i=1}^d P(x_i | w_2) = \prod_{i=1}^d q_i^{x_i}(1-q_i)^{1-x_i}$$  
  - likelihood ratio :  
  $$\frac{P(\boldsymbol x | w_1)}{P(\boldsymbol x | w_2)} = \prod_{i=1}^d (\frac{p_i}{q_i})^{x_i}(\frac{1-p_i}{1-q_i})^{1-x_i}$$  
  - discriminant function :  
  choose $$w_1$$  
  if $$g(x) = \text{ln} \frac{P(\boldsymbol x | w_1)P(w_1)}{P(\boldsymbol x | w_2)P(w_2)} = \sum_{i=1}^d(x_i \text{ln}\frac{p_i}{q_i} + (1-x_i)\text{ln}\frac{1-p_i}{1-q_i}) + \text{ln}\frac{P(w_1)}{P(w_2)} = \sum_{i=1}^d w_ix_i + w_0 = \boldsymbol w^T \boldsymbol x + w_0 \gt 0$$  
  where $$w_i = \text{ln}\frac{p_i(1-q_i)}{q_i(1-p_i)}$$ and $$w_0 = \sum_{i=1}^d(\text{ln}\frac{1-p_i}{1-q_i}) + \text{ln}\frac{P(w_1)}{P(w_2)}$$
    - case 1-1) $$p_i = q_i$$  
    $$w_i = 0$$ , so $$x_i$$ 는 class 결정에 영향 없음  
    - case 1-2) $$p_i \gt q_i$$  
    $$w_i \gt 0$$ , so $$x_i = 1$$ 은 class $$w_1$$ 선택에 보탬  
    - case 1-3) $$p_i \lt q_i$$  
    $$w_i \lt 0$$ , so $$x_i = 1$$ 은 class $$w_2$$ 선택에 보탬 
    - case 2-1) $$P(w_1)$$ 값 증가 ($$\gt P(w_2)$$)  
    $$w_0$$ 값이 커지므로 class $$w_1$$ 선택에 보탬  
    - case 2-2) $$P(w_1)$$ 값 감소 ($$\lt P(w_2)$$)  
    $$w_0$$ 값이 작아지므로 class $$w_1$$ 선택에 보탬  

## Chapter 2. Linear Transformation

### Linear Transformation

- $$y = A^Tx$$  
  - mean and variance :  
  $$\mu_{y} = A^T \mu_{x}$$  
  $$\Sigma_{y} = E[(y - \mu_{y})(y - \mu_{y})^T] = A^T \Sigma_{x} A$$
  - Mahalanobis distance :  
  $$d_y^2 = (y - \mu_{y})^T\Sigma_{y}^{-1}(y - \mu_{y}) = \cdots = d_x^2$$  
  `linear transformation`을 해도 Mahalanobis distance는 `그대로`임  
  (Euclidean distance $$(x - \mu_{x})^T(x - \mu_{x})$$ 는 linear transformation을 하면 variant)
  - Gaussian distribution :  
  $$x \sim N(\mu_{x}, \Sigma_{x})$$ 일 때  
  $$P(y) = (2 \pi)^{- \frac{d}{2}} | \Sigma_{y} |^{-\frac{1}{2}} \exp(-\frac{1}{2}(y - \mu_{y})^T \Sigma_{y}^{-1} (y - \mu_{y})) = (2 \pi)^{- \frac{d}{2}} | A |^{-1} | \Sigma_{x} |^{-\frac{1}{2}} \exp(-\frac{1}{2}(x - \mu_{x})^T \Sigma_{x}^{-1} (x - \mu_{x})) = \frac{1}{|A|} P(x)$$

### Orthonormal Transformation

- $$x = \sum_{i=1}^d y_i \phi_{i}$$  
where $$\{ \phi_{i}, \cdots, \phi_{d} \}$$ is orthonormal basis  
Equivalently,  
$$y_i = x^T \phi_{i}$$  
where vector $$x$$ 를 i-th eigenvector $$\phi_{i}$$ 에 project한 게 $$y_i$$
  - approx. $$x$$ :  
    - $$\{ y_{m+1}, \cdots, y_{d} \}$$ 를 pre-defined constants $$\{ b_{m+1}, \cdots, b_{d} \}$$ 로 대체했을 때  
    $$\hat x(m) = \sum_{i=1}^m y_i \phi_{i} + \sum_{i=m+1}^d b_i \phi_{i}$$
  - optimal $$b_i$$ :  
    - error $$\Delta x(m) = x - \hat x(m) = \sum_{i=m+1}^d (y_i - b_i) \phi_{i}$$  
    MSE $$\bar \epsilon^{2}(m) = E[| \Delta x(m) |^2] = E[\Delta x^T(m) \Delta x(m)] = \sum_{i=m+1}^d E[(y_i - b_i)^2]$$  
    - orthonormal basis $$\phi_{i}, \phi_{j}$$ 에 대해  
    $$\frac{\partial}{\partial b_i} E[(y_i - b_i)^2] = -2(E[y_i] - b_i) = 0$$ 이므로  
    MSE 최소화하는 optimal $$b_i = E[y_i]$$  
  - optimal $$\phi_{i}$$ :  
    - $$x = \sum_{j=1}^d y_j \phi_{j}$$ 의 양변에 $$\phi_{i}^T$$ 를 곱하면  
    $$y_i = x^T \phi_{i}$$ 이고  
    optimal $$b_i = E[y_i]$$ 이므로  
    MSE $$\bar \epsilon^{2}(m) = \sum_{i=m+1}^d E[(y_i - b_i)^2] = \sum_{i=m+1}^d E[(x^T \phi_{i} - E[x^T \phi_{i}])^T(x^T \phi_{i} - E[x^T \phi_{i}])] = \sum_{i=m+1}^d \text{Var}(\phi_{i}^{T} x) = \sum_{i=m+1}^d \phi_{i}^T \Sigma_{x} \phi_{i}$$  
    - orthonormality equality constraint $$\phi_{i}^T\phi_{i} = \| \phi_{i} \| = 1$$ 을 만족하면서 MSE $$\bar \epsilon^{2}(m)$$ 를 최소화하는 $$\phi_{i}$$ 는 Lagrange multiplier Method [Link](https://semyeong-yu.github.io/blog/2024/Lagrange/) 로 찾을 수 있다  
    $$\rightarrow$$  
    goal : minimize $$U(m) = \sum_{i=m+1}^d \phi_{i}^T \Sigma_{x} \phi_{i} + \sum_{i=m+1}^d \lambda_{i}(1 - \phi_{i}^T\phi_{i})$$  
    $$\frac{\partial}{\partial x}(x^TAx) = (A + A^T)x = 2Ax$$ for symmetric $$A$$ 이므로  
    $$\frac{\partial}{\partial \phi_{i}} U(m) = 2(\Sigma_{x}\phi_{i} - \lambda_{i}\phi_{i}) = 0$$ 이므로  
    MSE 최소화하는 optimal $$\phi_{i}$$ 는 $$\Sigma_{x}\phi_{i} = \lambda_{i}\phi_{i}$$ 을 만족하므로  
    $$\phi_{i}$$ 와 $$\lambda_{i}$$ 는 covariance matrix $$\Sigma_{x}$$ 의 eigenvector and eigenvalue 이다  

- Eigenvector and Eigenvalue :  
  - $$\Sigma \Phi = \Phi \Lambda$$ where $$\Phi \Phi^{T} = I$$
  - If $$\Sigma$$ is non-singular ($$| \Sigma | \neq 0$$),  
  all eigenvalues $$\lambda$$ are non-zero
  - If $$\Sigma$$ is positive-definite ($$x^T \Sigma x \geq 0$$ for all $$x \neq 0$$),  
  all eigenvalues $$\lambda$$ are positive
  - If $$\Sigma$$ is real and symmetric,  
  all eigenvalues $$\lambda$$ are real  
  and eigenvectors(w. distinct eigenvalues) are orthogonal  
    - pf)  
    $$\Sigma \phi_{i} = \lambda_{i} \phi_{i}$$ and $$\Sigma \phi_{j} = \lambda_{j} \phi_{j}$$  
    $$\phi_{j}^T \Sigma \phi_{i} - \phi_{i}^T \Sigma \phi_{j} = \phi_{j}^T \lambda_{i} \phi_{i} - \phi_{i}^T \lambda_{j} \phi_{j}$$  
    $$0 = (\lambda_{i} - \lambda_{j}) \phi_{j}^T \phi_{i}$$ since $$\Sigma$$ is symmetric  
    $$\rightarrow \phi_{j}^T \phi_{i} = 0$$ (eigenvectors are orthogonal)

- Orthonormal Transformation :  
$$y = \Phi^{T} x$$  
for $$\Phi = [\phi_{1}, \cdots \phi_{d}]$$ and $$\Phi \Phi^{T} = I$$  
  - vector $$x$$ 를 i-th eigenvector $$\phi_{i}$$ 에 project한 게 $$y_{i}$$  
  즉, vector $$x$$ 를 new coordinate $$\Phi = [\phi_{1}, \cdots \phi_{d}]$$ 으로 나타낸 게 vector $$y$$
  - eigenvector는 principal axis를 나타내고, eigenvalue는 해당 방향으로 퍼진 정도를 나타냄
  - $$y$$ 의 covariance matrix인 $$\Sigma_{y}$$ 는 `diagonal matrix`  
  (uncorrelated random vector $$y$$)
    - $$\Sigma_{y}$$  
    $$= \Phi^{T} \Sigma_{x} \Phi$$  
    $$= \Phi^{T} \Phi \Lambda$$ since $$\Sigma \Phi = \Phi \Lambda$$  
    $$= \Phi^{-1} \Phi \Lambda$$ since eigenvector matrix is orthogonal matrix ($$\Phi^{T} = \Phi^{-1}$$)  
    $$= \Lambda$$  
  - distance :  
    - Mahalanobis distance는 any linear transformation에 대해 보존됨  
    - `Euclidean distance`는 linear transformation 중 orthonormal transformation일 때만 `보존`됨  
    $$\| y \|^2 = y^Ty = x^T \Phi \Phi^{T} x = x^T \Phi \Phi^{-1} x = x^T x = \| x \|^2$$

### Whitening Transformation

- Whitening Transformation :  
$$y = \Lambda^{-\frac{1}{2}} \Phi^{T} x = (\Phi \Lambda^{-\frac{1}{2}})^T x$$  
(Orthonormal Transformation을 한 뒤 추가로 $$\Lambda^{-\frac{1}{2}}$$ 로 transformation)
  - $$y$$ 의 covariance matrix인 $$\Sigma_{y}$$ 는 `identity matrix` $$I$$  
    - $$\Sigma_{y}$$  
    $$= (\Lambda^{-\frac{1}{2}} \Phi^{T}) \Sigma_{x} (\Phi \Lambda^{-\frac{1}{2}})$$  
    $$= \Lambda^{-\frac{1}{2}} \Lambda \Lambda^{-\frac{1}{2}}$$  
    $$= I$$
  - $$\Lambda^{-\frac{1}{2}}$$ 은 principal components의 scale을 $$\frac{1}{\sqrt{\lambda_{i}}}$$ 배 하는 효과
  - Whitening Transformation을 한 번 하고나면,  
  그 후에 any Orthonormal Transformation($$y = \Phi^{T} x$$ for $$\Psi \Psi^{T} = I$$)을 해도  
  covariance matrix는 항상 $$\Psi I \Psi^{T} = I$$  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/7.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### Sample Separation

- Sample Separation :  
uncorrelated normal samples $$\sim N(0, I)$$ 로부터 correlated sample $$\sim N(\mu_{x}, \Sigma_{x})$$ 만들기  
  - How? :  
  given data $$x$$ 에서 $$\mu_{x}$$ 를 뺀 뒤 Whitening Transformation 적용하면 $$N(0, I)$$ 이므로 이 과정을 역으로 실행
  - Step 1) Normal distribution으로부터 N개의 $$d$$ -dim. independent vectors를 sampling  
  $$y_1, y_2, \cdots, y_N \sim N(0, I)$$
  - Step 2) Inverse-Whitening-Transformation 적용하여 Normal distribution을 x-space로 변환
  $$x_k = \Phi \Lambda^{\frac{1}{2}} y_k$$  
  for given $$\Sigma_{x}$$  
  and its eigen-decomposition $$\Sigma_{x} \Phi = \Phi \Lambda$$
  - Step 3) x-space의 samples에 $$\mu_{x}$$ 더함  
  $$x_k = \Phi \Lambda^{\frac{1}{2}} y_k + \mu_{x} \sim N(\mu_{x}, \Sigma_{x})$$  
  for given $$\mu_{x}$$

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/8.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

## Chapter 3. Maximum-likelihood and Bayesian Parameter Estimation

- parameter estimation :  
  - Maximum Likelihood Estimation (MLE) :  
  (true) parameters are `unknown`, but `fixed`  
  estimators are random variable
  - Bayesian Estimation :  
  parameters are `random variables` and `prior is known`

### Maximum Likelihood Estimation (MLE)

- Assumption :  
training data $$D_j$$ $$\sim$$ likelihood $$p(D_j | w_j) = N(\mu_{j}, \Sigma_{j})$$  
(i.i.d random samples)

- MLE :  
  - likelihood :  
  $$\hat \theta = \text{argmax}_{\theta} p(D=\{ x_1, x_2, \ldots, x_n \} | \theta) = \text{argmax}_{\theta} \prod_{k=1}^n p(x_k | \theta)$$
  - log-likelihood :  
  $$\hat \theta = \text{argmax}_{\theta} p(D=\{ x_1, x_2, \ldots, x_n \} | \theta) = \text{argmax}_{\theta} \sum_{k=1}^n \text{ln} p(x_k | \theta)$$

- Gaussian likelihood :  
  - unknown $$\mu$$ :  
    - likelihood :  
    $$p(x_k | \mu) = (2 \pi)^{-\frac{d}{2}} | \Sigma |^{-\frac{1}{2}} \text{exp}(-\frac{1}{2}(x_k - \mu)^T \Sigma^{-1} (x_k - \mu))$$  
    $$p(D=\{ x_1, x_2, \ldots, x_N \} | \mu) = \prod_{k=1}^N p(x_k | \mu) = (2 \pi)^{-\frac{dN}{2}} | \Sigma |^{-\frac{N}{2}} \text{exp}(-\frac{1}{2} \sum_{k=1}^N (x_k - \mu)^T \Sigma^{-1} (x_k - \mu))$$  
    - log-likelihood :  
    $$\text{ln} p(D | \mu) = -\frac{dN}{2} \text{ln}(2 \pi) -\frac{N}{2} \text{ln} | \Sigma | -\frac{1}{2} \sum_{k=1}^N (x_k - \mu)^T \Sigma^{-1} (x_k - \mu)$$  
    - matrix derivative :  
    $$\frac{d}{dx}(Ax) = A$$  
    $$\frac{d}{dx}(y^TAx) = A^Ty$$  
    $$\frac{d}{dx}(x^TAx) = (A+A^T)x$$  
    $$\frac{d}{dA}(x^TAx) = xx^T$$  
    $$\frac{\partial |A|}{\partial A} = (\text{adj}(A))^T = |A|(A^{-1})^T$$  
    $$\frac{\partial \text{ln}|A|}{\partial A} = (A^{-1})^T = (A^T)^{-1}$$ where $$|A| = \frac{1}{|A^{-1}|}$$  
    - MLE problem :  
    $$\nabla_{\mu} \text{ln} p(D | \mu) = -\frac{1}{2} \sum_{k=1}^N ((\Sigma^{-1} + (\Sigma^{-1})^T) (x_k - \mu)) \times (-1) = (\Sigma^{-1} + (\Sigma^{-1})^T)(\sum_{k=1}^N x_k - \sum_{k=1}^N \mu) = 0$$  
    $$\sum_{k=1}^N x_k - N \mu = 0$$  
    $$\hat \mu_{\text{MLE}} = \frac{1}{N} \sum_{k=1}^N x_k$$  
    - Summary :  
      - $$\hat \mu_{\text{MLE}} = \frac{1}{N} \sum_{k=1}^N x_k$$  
      (true mean의 MLE estimator는 sample mean)  
      - $$E[\hat \mu_{\text{MLE}}] = \mu$$  
      ($$\hat \mu_{\text{MLE}}$$ 는 `unbiased` estimator)
  - unknown $$\mu$$ and $$\Sigma$$ :  
    - log-likelihood :  
    $$\text{ln} p(D | \mu) = -\frac{dN}{2} \text{ln}(2 \pi) -\frac{N}{2} \text{ln} | \Sigma | -\frac{1}{2} \sum_{k=1}^N (x_k - \mu)^T \Sigma^{-1} (x_k - \mu) = -\frac{dN}{2} \text{ln}(2 \pi) + \frac{N}{2} \text{ln} | \Sigma^{-1} | -\frac{1}{2} \sum_{k=1}^N (x_k - \mu)^T \Sigma^{-1} (x_k - \mu)$$  
    - MLE problem :  
    $$\nabla_{\Sigma^{-1}} \text{ln} p(D | \mu) = \frac{N}{2}\Sigma^{T} - \frac{1}{2} \sum_{k=1}^N (x_k - \mu)(x_k - \mu)^T = 0$$  
    $$N \Sigma^{T} = \sum_{k=1}^N (x_k - \mu)(x_k - \mu)^T$$  
    $$\mu = \hat \mu_{\text{MLE}}$$ 대입하고, $$\Sigma$$ 는 symmetric($$\Sigma^{T} = \Sigma$$)하므로  
    $$\hat \Sigma_{\text{MLE}} = \frac{1}{N} \sum_{k=1}^N (x_k - \hat \mu_{\text{MLE}})(x_k - \hat \mu_{\text{MLE}})^T$$  
    - Summary :  
      - $$\hat \Sigma_{\text{MLE}} = \frac{1}{N} \sum_{k=1}^N (x_k - \hat \mu_{\text{MLE}})(x_k - \hat \mu_{\text{MLE}})^T$$  
      ($$\mu$$ 먼저 estimate한 뒤 $$\Sigma$$ estimate)
      - $$E[\hat \Sigma_{\text{MLE}}] = \frac{1}{N} E[\sum_{k=1}^N (x_k - \hat \mu_{\text{MLE}})(x_k - \hat \mu_{\text{MLE}})^T] = \frac{N-1}{N} \Sigma \neq \Sigma$$  
      ($$\hat \Sigma_{\text{MLE}}$$ 는 `biased` estimator)
        - pf) 아래 식 이용  
        $$E[x_i x_j^T] = \begin{cases} \Sigma + \mu \mu^{T} & \text{if} & i = j \\ \mu \mu^{T} & \text{if} & i \neq j \end{cases}$$  
        since $$\Sigma = E[(x - \mu)(x - \mu)^T] = \cdots = E[xx^T] - \mu \mu^{T}$$  
        since $$0 = E[(x_i - \mu)(x_j - \mu)^T] = E[x_i x_j^T] - \mu \mu^{T}$$ by independence $$i \neq j$$
      - $$\text{lim}_{N \rightarrow \infty}E[\hat \Sigma_{\text{MLE}}] = \text{lim}_{N \rightarrow \infty} \frac{N-1}{N} \Sigma = \Sigma$$  
      ($$\hat \Sigma_{\text{MLE}}$$ 는 `asymptotically unbiased` estimator)  
      또는  
      $$\hat \Sigma_{\text{MLE}} = \frac{1}{N-1} \sum_{k=1}^N (x_k - \hat \mu_{\text{MLE}})(x_k - \hat \mu_{\text{MLE}})^T$$  
      (위처럼 설정하면 $$\hat \Sigma_{\text{MLE}}$$ 는 `unbiased` estimator)

- MLE :  
  - MLE is `asymptotically consistent`  
  if $$\text{lim}_{N \rightarrow \infty} P(\| \hat \theta_{\text{MLE}} - \theta_{\text{true}} \| \leq \epsilon) = 1$$ for arbitrary small $$\epsilon$$  
  (sample 수 $$N$$ 이 크면 param. estimate은 true value랑 거의 비슷)  
  by central limit theorem and the fact that MLE is related to the sum of random var.
  - MLE is `asymptotically efficient`  
  since MLE는 Cramer-Rao lower bound(any estimate이 달성할 수 있는 the lowest value of variance)

### Bayesian Estimation

- Summary :  
  - MLE (maximum likelihood estimation) :  
    - when $$\theta$$ is unknown, but fixed
    - maximize likelihood $$\hat \theta_{MLE} = \text{argmax}_{\theta} p(D | \theta)$$
  - MAP (maximum a posterior) :  
    - when $$\theta$$ is random var. and prior $$p(\theta)$$ is known
    - maximize posterior $$\hat \theta_{MAP} = \text{argmax}_{\theta} p(\theta | D) = \text{argmax}_{\theta} \text{ln} p(D | \theta) + \text{ln} p(\theta)$$
  - If prior $$p(\theta)$$ is constant (uniform distribution),  
  MLE와 MAP는 same

- prior $$p(\theta)$$ 와 posterior $$p(\theta | D)$$ 가 같은 확률 분포의 형태를 가질 경우  
prior $$p(\theta)$$ 를  
likehood $$p(D | \theta)$$ 에 대한 `conjugate prior`라고 말한다

- Gaussian case :  
  - random var. $$\mu$$ :  
    - likelihood and conjugate prior :  
    $$x_k \sim$$ $$p(x_k | \mu) = N(\mu, \sigma^{2})$$  
    where $$\mu \sim$$ $$p(\mu) = N(\mu_{0}, \sigma_{0}^{2})$$
    - posterior (수식 유도는 아래에 별도로) :  
    $$p(\mu | D, \sigma^{2}) \propto N(\mu_{N}, \sigma_{N}^{2})$$  
    $$= \frac{1}{\sqrt{2 \pi} \sigma_{N}} e^{-\frac{1}{2}(\frac{\mu - \mu_{N}}{\sigma_{N}})^2}$$  
    where $$\hat \mu_{MAP} = \text{argmax}_{\mu} p(\mu | D, \sigma^{2}) = \mu_{N} = (\frac{N}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}}) \hat \mu_{MLE} + \frac{\frac{\sigma^{2}}{\sigma_{0}^{2}}}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}} \mu_{0}$$  
    where $$\sigma_{N}^{2} = \frac{\sigma^{2}}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}}$$  
      - Bayesian Learning :  
      $$N$$, 즉 sample 수가 많아질수록  
      $$\mu_{N}$$ 은 $$\hat \mu_{MLE}$$ 에 가까워지고  
      $$\sigma_{N}^{2}$$, 즉 uncertainty about $$\mu_{N}$$ 은 감소  
      따라서 $$N \rightarrow \infty$$ 이면  
      posterior $$p(\mu | D, \sigma^{2})$$ 는 $$\mu_{N} = \hat \mu_{MLE}$$ 에서의 Dirac delta function
      - $$\hat \mu_{MAP} = \mu_{N}$$ 에서  
      $$(\frac{N}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}}) \hat \mu_{MLE}$$ 는 empirical data samples 부분이고  
      $$\frac{\frac{\sigma^{2}}{\sigma_{0}^{2}}}{N + \frac{\sigma^{2}}{\sigma_{0}^{2}}} \mu_{0}$$ 는 prior info. 부분
      - 만약 $$\sigma_{0}^{2} = 0$$ 이라면  
      prior : variance $$\sigma_{0}^{2}$$ 가 매우 작으므로, certain that $$\mu = \mu_{0}$$  
      So,  
      posterior : $$\mu_{N} = \mu_{0}$$ (data samples는 $$\mu_{N}$$ 에 영향 없음)
      - 만약 $$\sigma^{2} \ll \sigma_{0}^{2}$$ 이라면  
      prior : variance $$\sigma_{0}^{2}$$ 가 매우 크므로, so uncertain that $$\mu = \mu_{0}$$  
      So,  
      posterior : $$\mu_{N} = \hat \mu_{MLE}$$ (data samples가 $$\mu_{N}$$ 에 대부분의 영향 미침)
    - posterior (수식 유도) :  
    $$p(\mu | D, \sigma^{2})$$  
    $$\propto p(D | \mu, \sigma^{2}) p(\mu) = \frac{1}{(2 \pi \sigma^{2})^{\frac{N}{2}}(2 \pi \sigma_{0}^{2})^{\frac{1}{2}}}e^{-\frac{1}{2\sigma^{2}}\sum_{i=1}^N(x_i - \mu)^2 -\frac{1}{2\sigma_{0}^{2}}(\mu - \mu_{0})^2}$$  
    $$\propto e^{-\frac{1}{2\sigma^{2}}\sum_{i=1}^N(x_i - \mu)^2 -\frac{1}{2\sigma_{0}^{2}}(\mu - \mu_{0})^2}$$  
    $$\propto e^{-\frac{1}{2\sigma^{2}}(N \mu^{2} - 2 \mu \sum_{i=1}^N x_i) -\frac{1}{2\sigma_{0}^{2}}(\mu^{2} - 2 \mu \mu_{0})}$$  
    $$= e^{-\frac{1}{2}(\mu^{2}(\frac{N}{\sigma^{2}} + \frac{1}{\sigma_{0}^{2}}) - 2 \mu (\frac{N \hat \mu_{MLE}}{\sigma^{2}} + \frac{\mu_{0}}{\sigma_{0}^{2}}))}$$  
    $$\propto \frac{1}{\sqrt{2 \pi} \sigma_{N}} e^{-\frac{1}{2}(\frac{\mu - \mu_{N}}{\sigma_{N}})^2}$$  
    where $$\hat \mu_{MAP} = \mu_{N} = (\frac{N\sigma_{0}^{2}}{N\sigma_{0}^{2} + \sigma^{2}}) \hat \mu_{MLE} + \frac{\sigma^{2}}{N\sigma_{0}^{2} + \sigma^{2}} \mu_{0}$$  
    where $$\sigma_{N}^{2} = \frac{\sigma_{0}^{2} \sigma^{2}}{N\sigma_{0}^{2} + \sigma^{2}}$$

### Principal Component Analysis (PCA)

- dimensionality reduction w/o losing much info.

28p TBD

### Singular Value Decomposition (SVD)

- 쓰임 : matrix factorization  
  - low-rank approximation of matrix
  - pseudo-inverse of non-square matrix

- Singular Value Decomposition (SVD) :  
$$A = U \Sigma V^T = \begin{bmatrix} u_1 & u_2 & \cdots & u_m \end{bmatrix} \begin{bmatrix} \sigma_{1} & \cdots & 0 & \cdots & 0 & 0 & \cdots & 0 \\ \vdots & \ddots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\ 0 & \cdots & \sigma_{r} & \cdots & 0 & 0 & \cdots & 0 \\ \vdots & \ddots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\ 0 & \cdots & 0 & \cdots & \sigma_{m} & 0 & \cdots & 0 \end{bmatrix} \begin{bmatrix} v_1^{T} \\ v_2^{T} \\ \vdots \\ v_n^{T} \end{bmatrix}$$  
where $$U$$ is $$m \times m$$ orthonormal matrix ($$UU^T = I$$)  
where $$V$$ is $$n \times n$$ orthonormal matrix ($$VV^T = I$$)  
where $$\Sigma$$ is $$m \times n$$ diagonal matrix ($$\sigma_{1} \geq \sigma_{2} \geq \cdots \geq \sigma_{m} \geq 0$$)  
  - $$AA^T = U \Sigma \Sigma^{T} U^T$$  
  $$U$$ 는 $$AA^T$$ 의 eigenvector matrix  
  $$\sigma_{i} = \sqrt{\lambda_{i}}$$ where $$\lambda_{i}$$ 는 $$AA^T$$ 의 eigenvalue
  - $$A^TA = V \Sigma^{T} \Sigma V^T$$  
  $$V$$ 는 $$A^TA$$ 의 eigenvector matrix  
  $$\sigma_{i} = \sqrt{\lambda_{i}}$$ where $$\lambda_{i}$$ 는 $$A^TA$$ 의 eigenvalue

- rank and span :  
$$r = \text{rank}(A) = \text{rank}(\Sigma) \leq \text{min}(m, n)$$ 에 대해  
($$\sigma_{1} \geq \sigma_{2} \geq \cdots \geq \sigma_{r} \geq \sigma_{r+1} = \cdots = \sigma_{m} = 0$$)  
  - $$\text{col}(A)$$ is spanned by $$\begin{bmatrix} u_1, \cdots, u_r \end{bmatrix}$$  
  $$AA^Tu_i = \sigma_{i}^{2} u_i$$ for $$i = 1, \cdots, r$$
  - $$\text{null}(A^T)$$ is spanned by $$\begin{bmatrix} u_{r+1}, \cdots, u_m \end{bmatrix}$$  
  $$AA^Tu_i = 0$$ for $$i = r+1, \cdots, m$$, 즉 $$A^T u_i = 0$$
  - $$\text{row}(A) = \text{col}(A^T)$$ is spanned by $$\begin{bmatrix} v_{1}^{T}, \cdots, v_{r}^{T} \end{bmatrix}$$  
  $$A^TAv_i = \sigma_{i}^{2} v_i$$ for $$i = 1, \cdots, r$$
  - $$\text{null}(A)$$ is spanned by $$\begin{bmatrix} v_{r+1}^{T}, \cdots, v_{n}^{T} \end{bmatrix}$$  
  $$A^TAv_i = 0$$ for $$i = r+1, \cdots, n$$, 즉 $$A v_i = 0$$
  - $$A = U \Sigma V^T$$ $$\rightarrow$$ $$A V = U \Sigma$$  
  $$A v_i = \sigma_{i} u_i$$ for $$i = 1, \cdots, r$$  
  $$A v_i = 0$$ for $$i = r+1, \cdots, n$$

- SVD (rank에 맞게 줄인 버전) :  
$$A = \begin{bmatrix} u_1 & u_2 & \cdots & u_r \end{bmatrix} \begin{bmatrix} \sigma_{1} & 0 & \cdots & 0 \\ 0 & \sigma_{2} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & \cdots & 0 & \sigma_{r} \end{bmatrix} \begin{bmatrix} v_1^{T} \\ v_2^{T} \\ \vdots \\ v_r^{T} \end{bmatrix}$$  
$$= \sum_{i=1}^{r} \sigma_{i} u_i v_{i}^{T}$$

- Eigen Decomposition :  
matrix $$A$$ 가 square and real and symmetric일 경우  
$$A P = P D$$  
where $$P$$ is orthonormal eigenvector matrix ($$P^{-1} = P^T$$ and $$PP^T = I$$)  
where $$D$$ is diagonal eigenvalue matrix

- SVD 계산 과정 Summary :  
  - Step 1)  
  $$A^TA$$ 를 Eigen Decomposition해서,  
  eigenvector matrix $$V$$ 와 eigenvalue matrix $$\Sigma$$ 를 구함  
  이 때, $$\Sigma$$ 의 singular value는 $$A^TA$$ 의 eigenvalue에 $$\sqrt{\cdot}$$ 씌워서 구함 
  - Step 2)  
  eigenvector matrix $$V$$ 의 columns를 normalize하고  
  $$V^TV=I$$ 맞는지 확인 
  - Step 3)  
  $$Av_i = \sigma_{i} u_i$$, 즉 $$u_i = \sigma_{i}^{-1} A v_i$$ 으로  
  eigenvector $$u_i$$ 구하고  
  normalize해서 $$UU^T = I$$ 맞는지 확인  
  - Step 4)  
  $$A = U \Sigma V^T$$ 맞는지 최종 확인

- Relation b.w. PCA and SVD :  
matrix $$A$$ 의 columns가 zero-mean centered일 때  
  - PCA :  
  covariance matrix $$C = E[AA^T] = \frac{1}{N-1}AA^T$$의 eigenvectors를 구한 뒤  
  eigenvalue 큰 순으로 잘라서 principal eigenvectors의 합으로 표현
  - SVD :  
  $$AA^T = U \Sigma^{2} U^T = V \Sigma^{2} V^T$$ ($$U = V$$ since $$AA^T$$ is square matrix) 이므로  
  $$AA^T$$의 eigenvector matrix $$V$$ 를 구한 뒤  
  rank $$r \leq \text{min}(m, n)$$에 맞게 줄여서 $$A = U \Sigma V^T = \sum_{i=1}^{r} \sigma_{i} u_i v_{i}^{T}$$ 로 표현
  - Relation :  
  둘은 mathematically 동일!  
  $$C = V \frac{\Sigma^{2}}{N-1} V^T$$

## Chapter 4. Non-parametric Techniques

- parametric approach :  
pdf가 어떤 form인지 미리 알아야 함  
e.g. $$N(\mu, \Sigma)$$ 의 $$\mu$$ 를 estimate  
하지만 실제로는 pdf prior form 모를 때가 많다
  - 해결 방법 1)  
  Non-parametric approach로 samples로부터 pdf (density $$\hat p(x) \approx p(x)$$) 를 직접 estimate  
  e.g. Parzen window  
  e.g. k-NN
  - 해결 방법 2)  
  아예 posterior를 직접 estimate  
  e.g. Direct Decision Rule - Nearest-Neighbor
    
### Density Estimation

- $$P(x \in R) = \int_{R} p(x)dx$$ :  
pdf $$p(x)$$ 를 안다면 위의 식으로 a sample $$x$$ 가 region $$R$$ 안에 속할 확률을 구할 수 있다  
But, pdf 모를 때는?
  - $$P \approx \frac{k}{n}$$    
  where $$n$$ 개의 samples 중 region $$R$$ 안에 속하는 samples가 $$k$$개  
  - For very small region $$R$$,  
  $$P(x \in R) \approx \hat p(x) \cdot V$$  
  where V is volume enclosed by $$R$$
  - 즉, $$\hat p(x) = \frac{k/n}{V}$$ is pdf estimator of $$p(x)$$
    - case 1) fixed V (volume of region $$R$$ is fixed)  
    sample 수 많아지면 $$\text{lim}_{n \rightarrow \infty} k/n = P$$ 로 수렴  
    So, $$\hat p(x)$$ is averaged ver. of $$p(x)$$  
    - case 2) $$V \rightarrow 0$$ (volume of region $$R$$ shrinks to 0)  
    region $$R$$의 volume이 매우 작으므로 $$k \rightarrow 0$$  
    So, $$p(x)$$ 는 zero에 가깝고, $$\hat p(x)$$ 는 very noisy
    - case 3) 실제 상황  
    sample 수 $$n$$ is limited  
    volume $$V$$ 는 arbitrarily small일 수 없음  
    따라서 samples에 따라 $$\frac{k}{n}$$과 averaging by $$V$$ 에 어느 정도 variance가 있음
  - $$\hat p(x)$$ 가 $$p(x)$$ 로 converge하려면 아래의 세 가지 조건 만족해야 함
    - $$\text{lim}_{n \rightarrow \infty} V = 0$$  
    no averaging in the limit
    - If $$V$$ is fixed, $$\text{lim}_{n \rightarrow \infty} k = \infty$$  
    그래야 $$\frac{k/n}{V}$$ 가 probability $$p(x)$$ 로 수렴  
    만약 $$\text{lim}_{n \rightarrow \infty} k = c$$ 라면 $$\text{lim}_{n \rightarrow \infty} \hat p(x) = 0$$
    - $$\text{lim}_{n \rightarrow \infty} \frac{k}{n} = 0$$  
    So, $$\text{lim}_{n \rightarrow \infty} \hat p(x) = \text{lim}_{n \rightarrow \infty} \frac{k/n}{V}$$ is density function
  - $$\text{lim}_{n \rightarrow \infty} \hat p(x) = p(x)$$ 위해  
    - e.g. $$V = \frac{V_0}{\sqrt{n}}$$ (Parzen window method)  
    so that $$\text{lim}_{n \rightarrow \infty} V = 0$$  
    - e.g. $$k = \sqrt{n}$$  
    so that $$\text{lim}_{n \rightarrow \infty} k = \infty$$ and $$\text{lim}_{n \rightarrow \infty} \frac{k}{n} = 0$$ and $$\text{lim}_{n \rightarrow \infty} V = \text{lim}_{n \rightarrow \infty} \frac{\sqrt{n}}{n \hat p(x)} = 0$$

### Density Estimation - Parzen window

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/10.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/9.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Let's define Parzen window as `unit hypercube`  
$$\phi (u) = \begin{cases} 1 & \text{if} & | u_i | \leq \frac{1}{2} \\ 0 & \text{O.W.} \end{cases}$$ for $$i = 1, \ldots, d$$  

- $$\phi (u)$$ 는 indicator function처럼 쓰여서  
$$x$$ 를 중심으로 하고 $$(h_n)^d$$ 의 범위를 갖는 cube 안에 들어오는 sample 개수 $$k$$ 를 세는 데 사용  
  - $$k = \sum_{i=1}^n \phi (\frac{x - x_i}{h_n})$$ and $$V = (h_n)^d$$  
  $$\rightarrow$$  
  $$\hat p(x) = \frac{k/n}{V} = \frac{1}{n} \sum_{i=1}^n \frac{\phi (\frac{x - x_i}{h_n})}{(h_n)^d}$$ :  
  interpolated function at position $$x$$ from samples $$x_i$$

- Let $$\delta_{n} (x) = \frac{1}{V} \phi (\frac{x}{h_n})$$  
Then $$\hat p(x) = \frac{1}{n} \sum_{i=1}^n \delta_{n} (x - x_i)$$  
  - $$h_n$$ ($$V$$) 이 클 경우 :  
  $$\delta_{n}(x)$$ 의 variance가 커서  
  이를 합친 $$\hat p(x) = \frac{1}{n} \sum_{i=1}^n \delta_{n} (x - x_i)$$ 는 오히려 smoothed ver. of $$p(x)$$ at $$x$$  
  (too little resolution)
  - $$h_n$$ ($$V$$) 이 작을 경우 :  
  $$\delta_{n}(x)$$ 의 variance가 작아서  
  이를 합친 $$\hat p(x) = \frac{1}{n} \sum_{i=1}^n \delta_{n} (x - x_i)$$ 는 noisy estimate of $$p(x)$$ (variation이 큼)  
  (too much statistical variation)

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/11.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- 만약 $$h_n \rightarrow 0$$ ($$\text{lim}_{n \rightarrow \infty} V = 0$$) 이라면  
  - $$\text{lim}_{n \rightarrow \infty} \delta_{n} (x) = \delta (x)$$ (Dirac delta func.)    
  - $$\text{lim}_{n \rightarrow \infty} E[\hat p(x)] = \text{lim}_{n \rightarrow \infty} \frac{1}{n} \sum_{i=1}^n E_{x_i}[\frac{1}{V} \phi (\frac{x - x_i}{h_n})]$$  
  $$= \text{lim}_{n \rightarrow \infty} \frac{1}{n} \cdot n \cdot \int \frac{1}{V} \phi (\frac{x - s}{h_n}) p(s) ds$$  
  $$= \text{lim}_{n \rightarrow \infty} \frac{1}{V} \phi(\frac{x}{h_n}) \ast p(x)$$ by definition of convolution  
  $$= \text{lim}_{n \rightarrow \infty} \delta_{n}(x) \ast p(x)$$  
  $$= \delta (x) \ast p(x)$$  
  $$= p(x)$$  

### Density Estimation - kNN method

- 고정된 $$k_n$$ 값에 대해  
$$k_n$$ nearest neighbors 찾을 때까지 $$V_n$$ expand  
$$\rightarrow$$  
training samples가 sparse한 곳에서 $$\hat p(x) \rightarrow 0$$ 인 걸 방지

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/12.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- k-NN estimation :  
probability $$\frac{k}{n}$$ 은 고정하고  
$$k$$-개의 sample이 들어 있는 volume $$V$$ 의 크기를 통해 density estimation

### Classification based on Parzen-window and k-NN

- classification based on Parzen-window method :  
  - density estimate  
  $$\hat p(x) = \frac{1}{n} \sum_{i=1}^n (\frac{1}{V_n} \phi (\frac{x-x_i}{h_n}))$$  
  - classification  
  choose $$w_1$$  
  if $$\frac{\hat p(x | w_1)}{\hat p(x | w_2)} = \frac{\frac{1}{n_1} \sum_{i=1}^{n_1} (\frac{1}{V_{n_1}} \phi (\frac{x-x_i}{h_n}))}{\frac{1}{n_2} \sum_{i=1}^{n_2} (\frac{1}{V_{n_2}} \phi (\frac{x-x_i}{h_n}))} \gt \frac{(\lambda_{12} - \lambda_{22})P(w_2)}{(\lambda_{21} - \lambda_{11})P(w_1)}$$

- classification based on k-NN method :  
  - density estimate  
  $$\hat p(x) = \frac{k_n / n}{V_n}$$  
  - classification  
  choose $$w_1$$  
  if $$\frac{\hat p(x | w_1)}{\hat p(x | w_2)} = \frac{\frac{k_1 / n_1}{V_{n_1}}}{\frac{k_2 / n_2}{V_{n_2}}} \gt \frac{(\lambda_{12} - \lambda_{22})P(w_2)}{(\lambda_{21} - \lambda_{11})P(w_1)}$$  
  if $$\frac{V_{n_2}}{V_{n_1}} \gt \frac{n_1(\lambda_{12} - \lambda_{22})P(w_2)}{n_2(\lambda_{21} - \lambda_{11})P(w_1)}$$  
  ($$k_1 = k_2$$ is fixed for k-NN)

### Direct Estimation of Posteriori

- NNR (Nearest Neighbor Rule) :  
  - Step 1)  
  estimate posteriori $$\hat P(w_i | x)$$ directly from training set  
    - classify하고 싶은 data $$x$$ 를 중심으로 volume $$V$$ 둠
    - likelihood pdf $$\hat P(x | w_i) = \frac{k_i / n_i}{V}$$  
    where $$n_i$$ : 총 $$n$$ samples 중 class $$w_i$$ 에 속하는 samples 수  
    where $$k_i$$ : $$V$$ 안에 속하는 $$k$$ samples 중 class $$w_i$$ 에 속하는 samples 수 ($$\sum_{i=1}^c k_i = k$$)
    - class probability $$\hat P(w) = \frac{n_i}{n}$$
    - joint pdf $$\hat P(x, w_i) = \hat P(x | w_i) \hat P(w_i) = \frac{k_i / n}{V}$$
    - posterior $$\hat P(w_i | x) = \frac{\hat P(x, w_i)}{\sum_{j=1}^c \hat P(x, w_j)} = \frac{(k_i / n) / V}{\sum_{j=1}^c (k_j / n) / V} = \frac{k_i}{k}$$
  - Step 2)  
  classification based on estimated $$\hat P(w_i | x)$$  
    - choose $$w_i$$ where $$i = \text{argmax}_{i} \hat P(w_i | x) = \text{argmax}_{i} k_i$$

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/13.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- k-NNR :  
volume $$V$$ 를 고정하는 게 아니라 $$V$$ 에 속하는 sample 수 $$k$$ 를 고정
  - 1-NNR :  
  assign test sample $$x$$ to the same class as the nearest training sample $$x^{'}$$
  - 3-NNR :  
  assign test sample $$x$$ to the class where the nearest training samples 3개 중 2개 이상이 속한 class
  - k-NNR :  
  무승부 방지 위해 $$k$$ 는 항상 홀수로 설정  

### Asymptotic Analysis of NNR

- Error Bound for NNR (Nearest Negibor Rule) :  
  - probability of error :  
    - exact :  
    $$P(e) = \int (1 - \sum_{i=1}^c P^2(w_i | x)) p(x)dx$$
    - approximate :  
    if all $$c$$ classes have equal probability  
    $$P(e) = 1 - \frac{1}{c}$$
  - error bound :  
  $$P^{\ast} \leq P(e) \leq 2 P^{\ast}$$  
  for Bayes rate $$P^{\ast}$$

증명해보자

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/14.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Asymptotic Error Rate of NNR :  
Let $$x$$ be test data  
Let $$x^{'}$$ be the nearest neighbor for 1-NNR  
Let $$\theta^{'}$$ be the labeled class of $$x^{'}$$ (pred)  
Let $$\theta$$ be the true class (gt)
  - Define $$P(e)$$ and $$P^{\ast}(e)$$ and $$P(w_i | x)$$ :  
    - $$P(e | x, x^{'})$$ :  
    $$P(e | x, x^{'}) = \sum_{i} P(\theta = w_i, \theta^{'} \neq w_i | x, x^{'}) = \sum_{i} P(\theta = w_i | x) P(\theta^{'} \neq w_i | x^{'}) = \sum_{i} P(\theta = w_i | x) (1 - P(\theta^{'} = w_i | x^{'}))$$
      - As $$n \rightarrow \infty$$, $$x^{'} \rightarrow x$$  
      So, $$\text{lim}_{n \rightarrow \infty} P(\theta^{'} = w_i | x^{'}) = P(\theta = w_i | x)$$
    - $$P(e|x)$$ :  
    $$\text{lim}_{n \rightarrow \infty} P(e | x, x^{'}) = P(e | x) = \text{lim}_{n \rightarrow \infty} \sum_{i} P(\theta = w_i | x) (1 - P(\theta^{'} = w_i | x^{'})) = \sum_{i} P(\theta = w_i | x) (1 - P(\theta = w_i | x)) = \sum_{i} P(\theta = w_i | x) - \sum_{i} (P(\theta = w_i | x))^2 = 1 - \sum_{i} (P(\theta = w_i | x))^2$$
    - $$P(e)$$ :  
    $$P(e) = \int P(e|x) p(x) dx = \int (1 - \sum_{i}^c (P(\theta = w_i | x))^2) p(x) dx$$  
    Here, $$\sum_{i}^c (P(\theta = w_i | x))^2$$ is maximized when 
    - $$P^{\ast}(e | x)$$ :  
    Let $$P^{\ast}(e | x)$$ be the minimum value of $$P(e|x)$$  
    Then $$P^{\ast}(e | x) = 1 - P(w_m | x)$$  
    where $$P(w_m | x) = \text{max}_{i} P(w_i | x)$$  
    - $$P^{\ast}(e)$$ :  
    $$P^{\ast}(e) = \int P^{\ast}(e | x) p(x) dx = \int (1 - P(w_m | x)) p(x) dx$$
  - Lower Bound of $$P(e)$$ :  
    - 정의한대로  
    $$P^{\ast}(e) \leq P(e)$$
  - Upper Bound of $$P(e)$$ :  
    - $$P(e)$$ is maximized  
    ($$\sum_{i}^c (P(\theta = w_i | x))^2$$ is minimized)  
    when 가장 높은 확률의 class $$w_m$$ 빼고는 posterior 확률이 같을 때  
    as $$P(w_i | x) = \begin{cases} \frac{P^{\ast}(e|x)}{c-1} & \text{if} & i \neq m \\ 1 - P^{\ast}(e|x) & \text{if} & i = m \end{cases}$$
    - Lower Bound of $$\sum_{i}^c (P(\theta = w_i | x))^2$$ :  
    $$\sum_{i}^c (P(\theta = w_i | x))^2 = P^2(w_m | x) + \sum_{i=1, i \neq m}^c P^2(w_i | x)$$  
    $$\geq 1 - P^{\ast}(e|x) + (c-1)(\frac{P^{\ast}(e|x)}{c-1})^2 = 1 - 2 P^{\ast}(e|x) + \frac{c}{c-1} (P^{\ast}(e|x))^2$$  
    So,  
    $$1 - \sum_{i}^c (P(\theta = w_i | x))^2 \leq 2 P^{\ast}(e|x) - \frac{c}{c-1} (P^{\ast}(e|x))^2 = P^{\ast}(e|x) (2 - \frac{c}{c-1} P^{\ast}(e|x))$$
    - Upper Bound of $$P(e)$$ :  
    $$P(e) = \int P(e|x) p(x) dx = \int (1 - \sum_{i}^c (P(\theta = w_i | x))^2) p(x) dx \leq \int P^{\ast}(e|x) (2 - \frac{c}{c-1} P^{\ast}(e|x)) p(x) dx = P^{\ast}(e)(2 - \frac{c}{c-1} P^{\ast}(e))$$  
  - Error Bounds of $$P(e)$$ :  
  $$P^{\ast}(e) \leq P(e) \leq P^{\ast}(e)(2 - \frac{c}{c-1} P^{\ast}(e))$$

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/15.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

## Chapter 5. Linear Discriminant Functions

- pattern recognition :  
  - Chapter 2. Bayes Decision theory :  
  choose class whose posterior is maximized  
  $$\rightarrow$$ likelihood ratio와 threshold를 비교  
  $$\rightarrow$$ likelihood가 Gaussian pdf일 때의 Discriminant Functions 다룸  
  ($$\Sigma_{i}$$ 가 어떤 값이냐에 따라 linear or quadratic)
  - Chapter 3. Parametric approach :  
  PDF form 아는 상태에서 estimate parameters  
  e.g. MLE or MAP
  - Chapter 4. Non-parametric approach :  
  PDF form 모르는 상태에서 estimate density  
  e.g. Parzen-window or k-NN or direct estimation (NNR)
  - Chapter 5. Linear Discriminant Function :  
  discriminant function form 아는 상태에서 estimate discriminant func. parameters  
  (PDF form 몰라도 됨 $$\rightarrow$$ Non-parametric approach)

### Linear Discriminant Function

- Two-category case :  
  - Decision Boundary by Linear Discriminant Function :  
  $$g(\boldsymbol x) = \boldsymbol w^T \boldsymbol x + w_0 = 0$$  
  - point $$\boldsymbol x = \boldsymbol x_{p} + r \frac{\boldsymbol w}{\| \boldsymbol w \|}$$  
  where $$\boldsymbol x_{p}$$ : projection of $$x$$ onto Decision Surface  
  where $$r$$ : signed distance
  - signed distance :  
  $$r = \frac{g(\boldsymbol x)}{\| \boldsymbol w \|} = \frac{\boldsymbol w^T \boldsymbol x + w_0}{\| \boldsymbol w \|}$$  
  $$r \gt 0$$ : point $$x$$ is on positive side (class $$w_1$$)  
  $$r \lt 0$$ : point $$x$$ is on negative side (class $$w_2$$)  
  e.g. 원점에서의 거리 : $$r = \frac{w_0}{\| \boldsymbol w \|}$$

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/16.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
 
- Multi-category case :  
  - $$c$$-개의 Linear Discriminant Function :  
  $$g_{i}(\boldsymbol x) = \boldsymbol w_{i}^{T} \boldsymbol x + w_{i0}$$ for $$i=1, \ldots, c$$  
  - Decision Rule :  
  class $$w_i$$ if $$g_{i}(\boldsymbol x) \gt g_{j}(\boldsymbol x)$$ for all $$j \neq i$$  
  - $$c$$-개의 Decision Boundary by Linear Discriminant Function :  
  $$g_{i}(\boldsymbol x) = g_{j}(\boldsymbol x)$$ for adjacent $$i, j$$  
  $$\rightarrow$$ $$\boldsymbol w_{i}^{T} \boldsymbol x + w_{i0} = \boldsymbol w_{j}^{T} \boldsymbol x + w_{j0}$$  
  $$\rightarrow$$ $$(\boldsymbol w_{i} - \boldsymbol w_{j})^{T} \boldsymbol x + (w_{i0} - w_{j0}) = 0$$  
  $$\rightarrow$$ $$\boldsymbol w^{T} \boldsymbol x + w_0 = 0$$  
  where $$\boldsymbol w = \boldsymbol w_{i} - \boldsymbol w_{j}$$ and $$w_0 = w_{i0} - w_{j0}$$
    - $$\boldsymbol w_{i} - \boldsymbol w_{j}$$ :  
    Decision Boundary(Hyperplane)와 수직인 vector
    - $$r = \frac{g_{i}(\boldsymbol x) - g_{j}(\boldsymbol x)}{\| \boldsymbol w_{i} - \boldsymbol w_{j} \|}$$ :  
    signed distance from $$\boldsymbol x$$ to Decision Boundary
  
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/17.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### Quadratic Discriminant Function

- Discriminant Function :  
  - Linear Discriminant Function :  
  $$g(\boldsymbol x) = \boldsymbol w^T \boldsymbol x + w_0 = \sum_{i=1}^d w_{i} x_{i} + w_0$$  
  e.g. likelihood $$P(x|w_i)$$ 가 Gaussian PDF이고 $$\Sigma_{i}$$ 가 $$\sigma^{2} I$$ 또는 $$\Sigma$$ 일 때  
  (본 포스팅 Discriminant Function for Gaussian PDF 참고)
  - Quadratic Discriminant Function :  
  $$g(\boldsymbol x) = \sum_{i=1}^d \sum_{j=1}^d w_{ij} x_{i} x_{j} + \sum_{i=1}^d w_{i} x_{i} + w_0$$  
  e.g. likelihood $$P(x|w_i)$$ 가 Gaussian PDF이고 $$\Sigma_{i}$$ 가 arbitrary일 때  
  (본 포스팅 Discriminant Function for Gaussian PDF 참고)
  - Polynomial Discriminant Function :  
  $$g(\boldsymbol x) = \sum_{i=1}^{\hat d} a_i y_i(\boldsymbol x) = \boldsymbol a^{T} \boldsymbol y$$  
  where $$\hat d$$ : arbitrary function $$y_{i}(x)$$ 개수

12p




