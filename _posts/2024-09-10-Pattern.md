---
layout: distill
title: EE534 Pattern Recognition
date: 2024-09-10 11:00:00
description: Lecture Summary (24F)
tags: 3d rendering
categories: cv-tasks
thumbnail: assets/img/2024-09-10-Pattern/0.png
giscus_comments: false
disqus_comments: true
related_posts: true
# toc:
#   beginning: true
#   sidebar: right
# featured: true
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

> Lecture :  
24F EE534 Pattern Recognition  
by KAIST Munchurl Kim [VICLab](https://www.viclab.kaist.ac.kr/)  

## Chapter 1. Overview

### Discriminative vs Generative

- Discriminative model :  
  - learn $$P(Y | X)$$ to maximize $$P(Y | X)$$ directly
  - e.g. logistic regression, SVM, nearest neighbor, CRF, Decision Tree and Random Forest, traditional NN

- Generative model :  
  - learn $$P(X | Y)$$ and $$P(Y)$$ to maximize $$P(X, Y) = P(X | Y)P(Y)$$  
  where can learn $$P(Y | X) \propto P(X | Y)P(Y)$$ indirectly
  - e.g. Bayesian network, Autoregressive model, GAN, Diffuson model

## Chapter 2. Bayes Decision Theory

### Bayes Decision Rule

- conditional probability density :  
Let $$w$$ be state (class)  
Let $$x$$ be data (continous-valued sample)  
  - prior : $$P(w=w_k)$$
  - likelihood : pdf $$P(x | w_k)$$
  - posterior : $$ P(w_k | x) = \frac{P(x | w_k)P(w_k)}{P(x)} $$ (Bayes Rule)  
  where $$ P(w_1 | x) + P(w_2 | x) + \cdots + P(w_N | x) = 1 $$
  - evidence : $$ P(x) = \sum_{k=1}^N P(x | w_k)P(w_k) = \sum_{k=1}^N P(x, w_k) $$

- Bayes Decision Rule :  
posterior 더 큰 쪽 고름!
  - Two-class ($$w_1, w_2$$) problem :  
  choose $$w_1$$  
  if $$P(w_1 | x) \gt P(w_2 | x)$$  
  if $$P(x|w_1)P(w_1) \gt P(x|w_2)P(w_2)$$  
  if $$\frac{P(x|w_1)}{P(x|w_2)} \gt \frac{P(w_2)}{P(w_1)}$$  
  (likehood ratio $$\gt$$ threshold)
  - multi-class problem :  
  choose $$w_i$$ where $$P(w_i | x)$$ is the largest

### minimum error

- minimum error :  
GT가 $$w_1, w_2$$ 이고, Predicted가 $$R_1, R_2$$ 일 때,  
  - $$P(error) = \int_{-\infty}^{\infty} P(error, x)dx = \int_{-\infty}^{\infty} P(error|x)P(x)dx$$  
  $$= \int_{R_2}P(w_1|x)P(x)dx + \int_{R_1}P(w_2|x)P(x)dx$$  
  $$= \int_{R_2}P(x|w_1)P(w_1)dx + \int_{R_1}P(x|w_2)P(w_2)dx$$  
  $$= \begin{cases} A+B+D & \text{if} & x_B \\ A+B+C+D & \text{if} & x^{\ast} \end{cases}$$  
  where $$A+B+D$$ is minimum error and $$C$$ is reducible error  
  (아래 그림 참고)
  - $$P(correct)$$  
  $$= \int \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_1}P(x|w_1)P(w_1)dx + \int_{R_2}P(x|w_2)P(w_2)dx$$
  - $$P(error) = 1 - P(correct)$$  
  $$ = 1 - \int \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_2}P(x|w_1)P(w_1)dx + \int_{R_1}P(x|w_2)P(w_2)dx$$

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div> 

- minimum error with rejection :  
decision이 확실하지 않을 때는 classification 자체를 reject하는 게 적절  
(classification error도 줄어들고, correct classification도 줄어듬)  
  - feature space $$x$$ 를 rejection region $$R$$ 과 acceptance region $$A$$ 으로 나눠서  
  rejection region $$R=\{ x | \text{max}_{i} P(w_i | x) \leq 1 - t\}$$ 에서는 reject decision  
  acceptance region $$A=\{ x | \text{max}_{i} P(w_i | x) \gt 1 - t\}$$ 에서는 $$w_1$$ or $$w_2$$ 로 classification decision 수행
  - $$P_c(t) = P(correct)$$  
  $$= \int_{A} \text{max}_{i} P(x|w_i)P(w_i)dx = \int_{R_1}P(x|w_1)P(w_1)dx + \int_{R_2}P(x|w_2)P(w_2)dx$$  
  - $$P_r(t) = P(reject)$$  
  $$= \int_{R}P(x|w_1)P(w_1)dx + \int_{R}P(x|w_2)P(w_2)dx$$  
  $$= \int_{R} P(x)dx$$  
  - $$P_e(t) = P(error)$$  
  $$= P(error, w_1) + P(error, w_2)$$  
  $$= 1 - P_r(t) - P_c(t)$$
  by 아래 식 대입  
  where $$P(error, w_1) = \int_{-\infty}^{\infty} P(x|w_1)P(w_1)dx - P(reject, w_1) - P(correct, w_1)$$  
  where $$P(error, w_2) = \int_{-\infty}^{\infty} P(x|w_2)P(w_2)dx - P(reject, w_2) - P(correct, w_2)$$  
  where $$\int_{-\infty}^{\infty} P(x|w_1)P(w_1)dx + \int_{-\infty}^{\infty} P(x|w_2)P(w_2)dx = \int_{-\infty}^{\infty} P(x)dx = 1$$
  
- Summary :  
  - $$P(w_i | x)$$ : rejection/acceptance region 구하는 데 사용
  - $$P(x|w_i)P(w_i)$$ : $$P(correct, w_i), P(reject, w_i), P(error, w_i)$$ 구해서  
  $$P_c(t), P_r(t), P_e(t)$$ 구하는 데 사용
  - $$P_c(t) + P_r(t) + P_e(t) = 1$$

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div> 
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/3.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div> 

### Bayes Decision Rule w. Bayes risk

- Bayes risk (minimum overall risk) :  
$$\Omega = \{ w_1, \cdots w_c \}$$ 에서 $$w_j$$ 는 $$j$$ -th class  
$$A = \{ \alpha_{1}, \cdots, \alpha_{c} \}$$ 에서 $$\alpha_{i}$$ 는 class $$w_i$$ 라고 예측하는 action  
$$\lambda(\alpha_{i} | w_j) = \lambda_{ij}$$ : class $$w_j$$ 가 GT일 때, class $$w_i$$ 로 pred. 했을 때의 loss
  - conditional risk for taking action $$\alpha_{i}$$ :  
  특정 input $$x$$ 에 대해  
  $$R(\alpha_{i}|x) = \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j|x)$$
  - overall risk for taking action $$\alpha_{i}$$ :  
  모든 input $$x$$ 에 대해 적분  
  $$R(\alpha_{i}) = \int R(\alpha_{i}|x)P(x)dx$$  
  $$= \int \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j|x) P(x)dx$$  
  $$= \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j) \int P(x|w_j)dx$$  
  $$= \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j)$$  
  $$= \sum_{j=1}^c \lambda_{ij}P(w_j)$$  
  where pdf(likelihood) 합 $$\int P(x|w_j)dx = 1$$  
  - 모든 input $$x$$ 에 대해 가장 loss가 최소인 class $$w_i$$ 로 예측하면,  
  minimum overall risk (= Bayes risk) 를 가짐

- Bayes Decision Rule for Bayes risk :  
  - Two-class ($$w_1, w_2$$) problem :  
  choose $$w_1$$  
  if $$R(\alpha_{1} | x) \lt R(\alpha_{2} | x)$$  
  if $$\lambda_{11}P(w_1 | x) + \lambda_{12}P(w_2 | x) \lt \lambda_{21}P(w_1 | x) + \lambda_{22}P(w_2 | x)$$  
  if $$(\lambda_{21} - \lambda_{11})P(w_1 | x) \gt (\lambda_{12} - \lambda_{22})P(w_2 | x)$$  
  if $$\frac{P(x | w_1)}{P(x | w_2)} \gt \frac{(\lambda_{12} - \lambda_{22})P(w_2)}{(\lambda_{21} - \lambda_{11})P(w_1)}$$  
  if $$\frac{P(x | w_1)}{P(x | w_2)} \gt \frac{P(w_2)}{P(w_1)}$$ for $$\lambda_{11}=\lambda_{22}=0$$ and $$\lambda_{12}=\lambda_{21}$$  
  (likehood ratio $$\gt$$ threshold) (위의 Bayes Decision Rule에서 구한 식과 same)
  - loss $$\lambda(\alpha_{i}|w_j) = \begin{cases} 0 & \text{if} & i=j & (\text{no penalty}) \\ 1 & \text{if} & i \neq j & (\text{equal penalty}) \end{cases}$$ 일 때  
  conditional risk $$R(\alpha_{i} | x) = \sum_{j=1}^c \lambda(\alpha_{i}|w_j)P(w_j|x) = \sum_{j=1, j \neq i}^c P(w_j|x) = 1 - P(w_i | x)$$ 이므로  
  Bayes Decision Rule에서 conditional risk $$R(\alpha_{i} | x)$$ 최소화는 posterior $$P(w_i | x)$$ 최대화와 같음
  - multi-class problem :  
  classifieer (discriminant function) (space-partitioning function) $$g(x)$$ 에 대해  
  choose $$w_i$$ where $$g_{i}(x)$$ is the largest  
  s.t. decision boundary is $$g_{i}(x) = g_{j}(x)$$ where they are the two largest discriminant functions  
  e.g. Bayes classifier : $$g_{i}(x) = - R(\alpha_{i} | x)$$ or $$g_{i}(x) = P(w_i | x)$$ or $$g_{i}(x) = P(x | w_i)P(w_i)$$ or $$g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i)$$  
  
### Discriminant Function for Gaussian PDF

- $$G(\boldsymbol x) = \frac{1}{(2\pi)^{\frac{d}{2}} | \Sigma |^{\frac{1}{2}}}e^{-\frac{1}{2}(\boldsymbol x - \boldsymbol \mu)^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu)}$$  
where $$d \times d$$ covariance $$\Sigma = E[(\boldsymbol x - \boldsymbol \mu)(\boldsymbol x - \boldsymbol \mu)^T] = E[\boldsymbol x \boldsymbol x^{T}] - \boldsymbol \mu \boldsymbol \mu^{T} = S - \boldsymbol \mu \boldsymbol \mu^{T}$$  
where $$S = E[\boldsymbol x \boldsymbol x^{T}]$$ : standard autocorrelation matrix  

- Discriminant function for Gaussian PDF :  
likelihood $$P(x | w_i)$$ 를 Gaussian PDF로 둘 경우,  
$$g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{d}{2} \text{ln}(2\pi) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)$$
  - case 1) $$\Sigma_{i} = \sigma^{2} \boldsymbol I$$ (모든 classes에 대해 equal covariance) (등방성(sphere))    
  $$g_{i}(x) = -\frac{\| \boldsymbol x - \boldsymbol \mu_{i} \|^2}{2 \sigma^{2}} + \text{ln}P(w_i)$$  
  $$i$$ 와 관련된 term만 남기면  
  $$g_{i}(x) = \frac{1}{\sigma^{2}} \boldsymbol \mu_{i}^T \boldsymbol x - \frac{1}{2\sigma^{2}} \boldsymbol \mu_{i}^T \boldsymbol \mu_{i} + \text{ln}P(w_i)$$  
  $$= \boldsymbol w_i^T \boldsymbol x + \boldsymbol w_{i0}$$ (linear)  
    - decision boundary :  
    hyperplane $$g(x) = g_{i}(x) - g_{j}(x) = (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T(\boldsymbol x - \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) + \frac{\sigma^{2}}{(\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T} \text{ln}\frac{P(w_i)}{P(w_j)})$$  
    $$= \boldsymbol w^T (\boldsymbol x - \boldsymbol x_0) = 0$$  
    - $$\boldsymbol x_0$$ 를 지나고 $$\boldsymbol w = \boldsymbol \mu_{i} - \boldsymbol \mu_{j}$$ 에 수직인 hyperplane  
    - $$\boldsymbol x_0 = \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) - \frac{\sigma^{2}}{\| \boldsymbol \mu_{i} - \boldsymbol \mu_{j} \|^2} \text{ln}\frac{P(w_i)}{P(w_j)} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})$$ 이므로  
    $$\boldsymbol x_0$$ 의 위치는 $$\boldsymbol \mu_{i}$$ 와 $$\boldsymbol \mu_{j}$$ 의 중점에서 $$\begin{cases} \boldsymbol \mu_{j} \text{쪽으로 이동} & \text{if} & P(w_i) \gt P(w_j) \\ \boldsymbol \mu_{i} \text{쪽으로 이동} & \text{if} & P(w_i) \lt P(w_j) \end{cases}$$  
    ($$P(w_i)$$ 와 $$P(w_j)$$ 중 더 작은 쪽으로 이동)  
    ($$\sigma^{2}$$ 이 ($$\| \mu_{i} - \mu_{j} \|^2$$ 에 비해 비교적) 작은 경우 $$P(w_i)$$ 와 $$P(w_j)$$ 에 따른 $$x_0$$ shift는 미약)

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/4.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Discriminant function for Gaussian PDF :  
likelihood $$P(x | w_i)$$ 를 Gaussian PDF로 둘 경우,  
$$g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{d}{2} \text{ln}(2\pi) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)$$
  - case 2) $$\Sigma_{i} = \Sigma$$ (symmetric) (모든 classes에 대해 equal covariance) (비등방성(hyper-ellipsoidal))  
  $$g_{i}(x) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) + \text{ln}P(w_i)$$  
  $$i$$ 와 관련된 term만 남기면  
  $$g_{i}(x) = \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol x - \frac{1}{2} \boldsymbol \mu_{i}^T \Sigma^{-1} \boldsymbol \mu_{i} + \text{ln}P(w_i)$$  
  $$= \boldsymbol w_i^T \boldsymbol x + \boldsymbol w_{i0}$$ (linear)
    - decision boundary :  
    hyperplane $$g(x) = g_{i}(x) - g_{j}(x) = (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T \Sigma^{-1} (\boldsymbol x - \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) + \frac{1}{(\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T \Sigma^{-1}} \text{ln}\frac{P(w_i)}{P(w_j)})$$  
    $$= \boldsymbol w^T (\boldsymbol x - \boldsymbol x_0) = 0$$  
    - $$\boldsymbol x_0$$ 를 지나고 $$\boldsymbol w = \Sigma^{-1} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})$$ 에 수직인 hyperplane
    - $$\boldsymbol x_0 = \frac{1}{2}(\boldsymbol \mu_{i} + \boldsymbol \mu_{j}) - \frac{\text{ln}\frac{P(w_i)}{P(w_j)}}{(\boldsymbol \mu_{i} - \boldsymbol \mu_{j})^T \Sigma^{-1} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})$$ 이므로  
    마찬가지로 $$\boldsymbol x_0$$ 의 위치는 $$\boldsymbol \mu_{i}$$ 와 $$\boldsymbol \mu_{j}$$ 의 중점에서 $$P(w_i)$$ 와 $$P(w_j)$$ 중 더 작은 쪽으로 이동  
    - $$\boldsymbol w = \Sigma^{-1} (\boldsymbol \mu_{i} - \boldsymbol \mu_{j})$$ 는  
    vector $$\boldsymbol \mu_{i} - \boldsymbol \mu_{j}$$ 를 $$\Sigma^{-1}$$ 로 회전시킨 vector를 의미

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-09-10-Pattern/5.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Discriminant function for Gaussian PDF :  
likelihood $$P(x | w_i)$$ 를 Gaussian PDF로 둘 경우,  
$$g_{i}(x) = \text{ln}P(x | w_i) + \text{ln}P(w_i) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{d}{2} \text{ln}(2\pi) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)$$
  - case 2) $$\Sigma_{i}$$ is arbitrary (symmetric) (class마다 covariance 다름) (비등방성(hyper-ellipsoidal))  
  $$g_{i}(x) = -\frac{1}{2}(\boldsymbol x - \boldsymbol \mu_{i})^T\Sigma_{i}^{-1}(\boldsymbol x - \boldsymbol \mu_{i}) - \frac{1}{2} \text{ln} | \Sigma_{i} | + \text{ln}P(w_i)$$  
  $$= TBD 55p$$



