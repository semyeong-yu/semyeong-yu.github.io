---
layout: distill
title: State Space Model
date: 2024-07-18 15:00:00
description: SSM
tags: SSM Mamba
categories: cv-tasks
thumbnail: assets/img/2024-07-18-SSM/1m.PNG
giscus_comments: false
disqus_comments: true
related_posts: true
# toc:
#   beginning: true
#   sidebar: right
# featured: true
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## State Space Model

> 참고 논문 :  
[https://arxiv.org/abs/2406.07887](https://arxiv.org/abs/2406.07887)  
참고 강연 :  
by NVIDIA Wonmin Byeon

### Abstract

- Large Language Models (LLMs) are usually based on `Transformer` architectures.  
  - Transformer-based models 장점 :  
  highly `parallelizable`  
  can model `massive amounts of data`  
  - Transformer-based models 단점 :  
  significant `computational overhead` due to the `quadratic self-attention` calculations, especially on longer sequences  
  large inference-time `memory requirements` from the `key-value cache`  
  
- More recently, `State Space Models (SSM)` like Mamba have been shown to have fast parallelizable training and inference as an alternative of Transformer.  
In this talk, I present the strengths and weaknesses of `Mamba, Mamba-2, and Transformer models` at larger scales. I also introduce a `hybrid architecture consisting of Mamba-2, attention, and MLP layers`.  
While pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks that require `strong copying` or `in-context learning` abilities.  
In contrast, the hybrid model closely matches or exceeds the Transformer on all standard and long-context tasks and is predicted to be up to 8x faster when generating tokens at inference time.  

### Is Attention All We Need?

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-18-SSM/4m.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Transformer :  
  - fast training due to parallelization 
  - slow inference for long sequence(context)  
    - key-value cache can improve speed, but increase GPU memory  

- RNN :  
  - slow training due to no parallelization  
  - fast inference because scale linearly with sequence length

- Mamba :  
  - fast training
  - fast inference because scale linearly with sequence length and can deal with unbounded context

- SSM or RNN :  
state = fixed-sized vector (compression)  
high efficiency, but low performance

- Transformer :  
cache of entire history (no compression)  
high performance, but low efficiency  

### Mamba: Linear-Time Sequence Modeling with Selective State Spaces

- SSM 

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-18-SSM/1m.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Selective SSM :  
matrix B, C and step size are dependent on the input  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-18-SSM/2m.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Parallel scan :  
The order does not matter through the associative property, so can calculate sequences in part and iteratively combine them

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-18-SSM/3m.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Hardware-aware implementation :  
minimize copying between RAMs  

### Mamba-2

- Mamba에서 Main Bottleneck이 Parallel scan 부분이었는데,  
Mamba-2는 divide input into chunks 등 architecture 개선으로 이를 해결하고자 했음

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-18-SSM/13m.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### Limitations of Mamba

- Poor at MMLU and Phonebook task  
아래를 요구하는 task에 대해서는 Mamba가 잘 못함
  - in-context learning  
  - info. routing between tokens  
  - copying from the context (bad on long-context tasks)

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-18-SSM/5m.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-18-SSM/6m.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### Hybrid Architecture of Mamba and Transformer

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-18-SSM/7m.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Our Hybrid Mamba-Transformer Model
  - Minimize the number of Attention Layers and Maximize the number of MLPs
  - Does not necessarily need Rotary Position Embedding (RoPE)
  - evenly spread attention and MLP layers
  - Place Mamba layer at the beginning, so has no position embedding
  - Group-Query Attention (GQA) makes more efficient 
  - Global Attention makes better performance

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-18-SSM/11m.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Mamba-2 Hybrid  
Inference Speed is fast  
Now, states in Mamba can understand longer history!  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-18-SSM/12m.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

Attention Layer is bottleneck at Hybrid model,  
so Context Length가 길어질수록 Speedup 증가율은 줄어듬

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-18-SSM/8m.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-18-SSM/9m.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### Summary

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-18-SSM/10m.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    왼쪽부터 4K, 16K, 32K-based models
</div>

Mamba-2 Hybrid는 Transformer와 달리 Quadratic calculation까지 필요 없고 inference 빠름  
but, Attention Layer가 Bottleneck이듯이 해결해야 할 사항들이 남아 있어 앞으로도 발전 가능성 있음