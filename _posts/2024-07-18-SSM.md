---
layout: distill
title: State Space Model
date: 2024-07-18 15:00:00
description: SSM
tags: SSM Mamba
categories: cv-tasks
thumbnail: assets/img/2024-07-18-SSM/1.png
giscus_comments: true
related_posts: true
# toc:
#   beginning: true
#   sidebar: right
# featured: true
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## State Space Model

> 참고 논문 :  
[https://arxiv.org/abs/2406.07887](https://arxiv.org/abs/2406.07887)

### Abstract

- Large Language Models (LLMs) are usually based on `Transformer` architectures.  
  - Transformer-based models 장점 :  
  highly `parallelizable`  
  can model `massive amounts of data`  
  - Transformer-based models 단점 :  
  significant `computational overhead` due to the `quadratic self-attention` calculations, especially on longer sequences  
  large inference-time `memory requirements` from the `key-value cache`  
  
- More recently, `State Space Models (SSM)` like Mamba have been shown to have fast parallelizable training and inference as an alternative of Transformer.  
In this talk, I present the strengths and weaknesses of `Mamba, Mamba-2, and Transformer models` at larger scales. I also introduce a `hybrid architecture consisting of Mamba-2, attention, and MLP layers`.  
While pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks that require `strong copying` or `in-context learning` abilities.  
In contrast, the hybrid model closely matches or exceeds the Transformer on all standard and long-context tasks and is predicted to be up to 8x faster when generating tokens at inference time.  

### Is Attention All We Need?

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-18-SSM/4.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Transformer :  
  - fast training due to parallelization 
  - slow inference for long sequence(context)  
    - key-value cache can improve speed, but increase GPU memory  

- RNN :  
  - slow training due to no parallelization  
  - fast inference because scale linearly with sequence length

- Mamba :  
  - fast training
  - fast inference because scale linearly with sequence length and can deal with unbounded context

- SSM or RNN :  
state = fixed-sized vector (compression)  
high efficiency, but low performance

- Transformer :  
cache of entire history (no compression)  
high performance, but low efficiency  

### Mamba: Linear-Time Sequence Modeling with Selective State Spaces

- SSM 

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-18-SSM/1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Selective SSM :  
matrix B, C and step size are dependent on the input  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-18-SSM/2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Parallel scan :  
The order does not matter through the associative property, so can calculate sequences in part and iteratively combine them

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-18-SSM/3.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Hardware-aware implementation :  
minimize copying between RAMs  

### Mamba-2

### Limitations of Mamba

- Poor at MMLU and Phonebook task  
아래를 요구하는 task에 대해서는 Mamba가 잘 못함
  - in-context learning  
  - info. routing between tokens  
  - copying from the context

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-18-SSM/5.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-18-SSM/6.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### Hybrid Architecture of Mamba and Transformer

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-18-SSM/7.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Our Hybrid Mamba-Transformer Model
  - Minimize the number of Attention Layers and Maximize the number of MLPs
  - Does not necessarily need Rotary Position Embedding (RoPE)
  - evenly spread attention and MLP layers
  - Place Mamba layer at the beginning, so has no position embedding
  - Group-Query Attention (GQA) makes more efficient 
  - Global Attention makes better performance

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-18-SSM/11.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Mamba-2 Hybrid

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-18-SSM/8.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-18-SSM/9.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### Summary

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-18-SSM/10.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>