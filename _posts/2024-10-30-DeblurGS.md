---
layout: distill
title: Deblurring 3D Gaussian Splatting
date: 2024-10-30 12:00:00
description: ECCV 2024
tags: 3DGS deblur
categories: 3d-view-synthesis
thumbnail: assets/img/2024-10-30-DeblurGS/1.png
giscus_comments: false
disqus_comments: true
related_posts: true
toc:
  - name: Introduction
  - name: Related Works
  - name: Defocus Blur
  - name: Camera motion Blur
  - name: Compensation for Sparse Point Cloud
  - name: Experiment
  - name: Limitation and Future Work
  - name: Code Review

_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Deblurring 3D Gaussian Splatting (ECCV 2024)

#### Byeonghyeon Lee, Howoong Lee, Xiangyu Sun, Usman Ali, Eunbyung Park

> paper :  
[https://arxiv.org/abs/2401.00834](https://arxiv.org/abs/2401.00834)  
project website :  
[https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/](https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/)  
code :  
[https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting](https://github.com/benhenryL/Deblurring-3D-Gaussian-Splatting)  

### Introduction

- 3DGS :  
  - novel-view로 inference할 때  
  NeRF는 새로운 각도를 MLP에 넣어야만 color, opacity 얻을 수 있지만  
  3DGS는 spherical harmonics, explicit 기법이라 바로 color, opacity 얻을 수 있어서  
  volume rendering이 빠름
  - differentiable splatting-based rasterization with parallelism

- 본 논문 :  
  - 핵심 :  
  각 3DGS의 `covariance`를 수정하여 `blur(adjacent pixels의 혼합)를 모델링하는 작은 MLP` 사용  
  - 원리 :  
  training 시에는 MLP output 곱해서 blurry image를 생성하고  
  inference 시에는 MLP 사용하지 않아서 real-time으로 sharp image 생성
  - initial point cloud :  
  given images가 `blurry`하면 SfM은 유효한 feature를 식별하지 못해서 `매우 적은 수의 point` cloud를 추출함  
  (심지어 depth가 크면 SfM은 맨 끝에 있는 점을 거의 추출하지 않음)  
  $$\rightarrow$$  
  3DGS는 initial point cloud에 많이 의존하므로  
  sparse point cloud를 방지하고자  
  `N-nearest-neighbor interpolation으로 points 추가`  
  또한  
  먼 거리의 평면에 많은 Gaussian을 유지하기 위해  
  `위치에 따라 Gaussian pruning`
  - contribution :  
  SOTA qualtiy인데 훨씬 빠른 rendering speed ($$\gt 200$$ FPS)

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-10-30-DeblurGS/1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    Overall Architecture
</div>

### Related Works

TBD  

Related Works :
Deblur-NeRF: Neural Radiance Fields from Blurry Images (CVPR 2022)
DP-NeRF: Deblurred Neural Radiance Field with Physical Scene Priors (CVPR 2023)

### Background

- 3DGS [Link](https://semyeong-yu.github.io/blog/2024/GS/) 참고

- Blur :  
  - Defocus Blur :  
  렌즈의 `초점이 맞지 않아서` 흐려진 경우  
  e.g. 인물 사진에서 인물만 초점이 맞고 배경은 흐릿한 경우  
  - Camera Motion Blur :  
  셔터가 열려 있는 동안 카메라가 움직이거나 피사체가 `움직여서` 흐려진 경우  
  e.g. 달리는 자동차를 촬영한 경우

### Defocus Blur

- Motivation :  
  - Defocus Blur는 일반적으로  
  PSF(point spread func.)로 알려진 2D Gaussian function과 실제 image의  
  convolution으로 모델링  
  즉, a pixel이 주위 pixels에 영향을 미칠 경우 blur
  - 여기서 영감을 받아  
  covariance(크기)가 큰 3DGS는 Blur를 유발하고  
  covariance(크기)가 작은 3DGS는 선명한 detail에 기여한다고 가정
  - 그렇다면 covariance $$\Sigma = R S S^{T} R^{T}$$ 를 변경하여 Blur를 모델링해야겠다!

- Defocus Blur를 모델링하는 MLP :  
$$(\delta_{r}, \delta_{s}) = F_{\theta}(\gamma(x), r, s, \gamma(v))$$  
where input : position, rotation, scale, view-direction  
where output : rotation change, scale change  
($$\gamma$$ : positional encoding)
  - transformed 3DGS :  
  $$r^{'} = r \cdot \delta_{r}$$ and $$s^{'} = s \cdot \delta_{s}$$ (element-wise multiplication)
  - Defocus Blur :  
  MLP output $$\delta_{r}, \delta_{s}$$ 의 `최솟값을 1로 clip`  
  $$\rightarrow$$  
  그럼 $$s^{'}$$ 이 $$s$$ 보다 크거나 같으므로 transformed 3DGS는 더 `큰 covariance`를 가져서  
  `Defocus Blur`의 근본 원인인 주변 정보의 간섭을 모델링할 수 있게 됨
  - inference :  
  `training` 시에는 `transformed 3DGS`가 `blurry` image를 생성하지만  
  `inference` 시에는 MLP를 사용하지 않은 `기존 3DGS`가 `sharp` image를 생성  
  $$\rightarrow$$  
  training할 때는 MLP forwarding과 간단한 element-wise multiplication만 추가 비용이고,  
  inference할 때는 MLP를 사용하지 않아 Vanilla-3DGS와 모든 단계가 동일하므로 `추가 비용 없이 real-time rendering` 가능

### Selective Blurring

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-10-30-DeblurGS/2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- 초점에 따른 Defocus Blur는 `영역마다 흐린 수준이 다름`  
본 논문에서는 `각 3DGS마다` 다르게 $$\delta_{r}, \delta_{s}$$ 를 추정하므로  
Gaussian의 covariance를 선택적으로 확대시킬 수 있어서  
영역에 따라 다르게 blurring 할 수 있으므로  
`pixel 단위의 blurring`을 보다 유연하게 모델링 가능  
  - defocus blur가 심한 영역에 있는 3DGS는 $$\delta_{s}$$ 가 더 크도록  
  - 당연히 단일 유형의 Gaussian Blur kernel을 써서 평균 blurring을 모델링하는 것보다  
  본 논문에서처럼 3DGS마다 다른 Blur kernel을 적용하여 pixel 단위 blurring을 모델링하는 게 더 좋음!

### Camera motion Blur

TBD `???`

### Compensation for Sparse Point Cloud

TBD

### Experiment

### Limitation and Future Work

### Code Review