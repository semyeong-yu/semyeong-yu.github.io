---
layout: distill
title: MipNeRF
date: 2024-08-03 01:03:00
description: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields
tags: nerf rendering 3d multiscale antialiasing
categories: 3d-view-synthesis
thumbnail: assets/img/2024-08-03-MipNeRF/1.png
giscus_comments: true
related_posts: true
bibliography: 2024-08-03-MipNeRF.bib
# toc:
#   beginning: true
#   sidebar: right
# featured: true
toc:
  - name: Introduction
  - name: Related Work
    subsections:
      - name: Anti-aliasing in Rendering
      - name: Scene Representations for View Synthesis
  - name: Method
    subsections:
      - name: Cone Tracing and Positional Encoding
      - name: Architecture
  - name: Results
  - name: Conclusion
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields

### Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan  

> paper :  
[https://arxiv.org/abs/2103.13415](https://arxiv.org/abs/2103.13415)  
project website :  
[https://jonbarron.info/mipnerf/](https://jonbarron.info/mipnerf/)  
pytorch code :  
[https://github.com/bebeal/mipnerf-pytorch](https://github.com/bebeal/mipnerf-pytorch)  
[https://github.com/google/mipnerf](https://github.com/google/mipnerf)  
referenced blog :  
[https://xoft.tistory.com/16](https://xoft.tistory.com/16)  

> 핵심 요약 :  
1. `임의의 continuous-space scale`에 대해 query 받을 수 있는,  
scene의 `anti-aliased (pre-filtered)` representation 학습  
$$\rightarrow$$ multi-resolution dataset에 대해 성능 대폭 향상  
$$\rightarrow$$ scale-aware하므로 `single MLP`만으로 충분하여 빠르고 가벼움  
2. point encoding 방식 대신 region encoding 방식을 채택하여 scale 반영 가능  
$$\rightarrow$$ camera center로부터 각 pixel로 3D cone을 쏜 다음,  
`frustum을 multi-variate Gaussian으로 근사`한 뒤,  
Gaussian 내 좌표를 positional encoding한 것에 대해 `expected value` $$E \left[ \gamma (x) \right]$$ 계산  

## Introduction

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-03-MipNeRF/2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- 기존 NeRF의 문제점 :  
rendering 위해 sampling할 때 `single ray` per pixel 쏴서 composite 하므로  
dataset에 있는 물체의 크기(resolution)가 일정하지 않을 때  
multi-scales images에 대해 학습하더라도  
`blurry` rendering in `close-up` views  
(가까이서 찍어서 zoom-out하면 물체 in `high resolution`)  
`aliased`(계단) rendering in `distant` views  
(멀리서 찍어서 zoom-in하면 물체 in `low resolution`)  
그렇다고 multiple rays per pixel through its footprint로 brute-force super-sampling(offline rendering)하는 것은 정확하긴 하겠지만 too costly 비현실적  

- `Minmap` Approach :  
classic 컴퓨터 그래픽스 분야에서 rendering할 때 aliasing을 없애기 위한 `pre-filtering` 기법  
본 논문인 Mip-NeRF가 여기서 영감을 얻음  
signal(e.g. image)을 diff. `downsampling scales`로 나타낸 뒤 pixel footprint를 근거로 ray에 사용하기 위한 `적절한 scale을 고른다`  
render time에 할 복잡할 일을 pre-computation phase에 미리 하는 것일 뿐이긴 하지만, 주어진 texture마다 한 번만 minmap을 만들면 된다는 장점이 있다  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-03-MipNeRF/4.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Mip-NeRF :  
  - represent pre-filtered scene at `continuous space of scales`  
  - ray 대신 `conical frustum` 사용해서 `anti-aliased` rendering with fine details  
  - multi-scale variant of dataset에 대해 평균 error rate 60% 감소  
  - NeRF가 hierarchical sampling을 위해 coarse and fine MLP를 분리했다면, Mip-NeRF는 `scale-aware`하므로 `single multi-scale MLP만으로 충분`  
  따라서 NeRF보다 7% 빠르고, param. 수는 절반

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-03-MipNeRF/1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    ray 대신 cone을 쏘고, point-encoding 대신 frustum region-encoding
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-03-MipNeRF/3.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- 차이 : 기존 NeRF는 `a single point`를 encode하고, Mip-NeRF는 `a region of space`를 encode  

- 기존 NeRF :  
camera center로부터 각 pixel로 ray를 하나 쏜 다음 point sampling한 뒤 positional encoding  
point-sampled feature는 ray가 보는 `volume의 모양과 크기를 무시`하는 것임  
예를 들어 training할 때 camera1로부터 t 사이의 간격이 평균 10cm로 학습된 scene에 대해  
camera2로 inference를 할 때 t 사이의 간격이 평균 1cm로 sampling된다면  
10개의 점은 같은 point-based feature를 갖게 되어 scale을 고려하지 못함  
이러한 ambiguity가 기존 NeRF의 성능 하락의 요인

- Mip-NeRF :  
volume 정보를 반영하기 위해 camera center로부터 각 pixel로 3D cone을 쏜 다음, 3D point 및 그 주위의 Gaussian region을 encode하기 위해 IPE  

- IPE (`integrated positional encoding`) :  
region을 encode하기 위한 방식  
`frustum을 multi-variate Gaussian으로 근사`한 뒤,  
Gaussian 내 좌표를 positional encoding한 것에 대해 `expected value` $$E \left[ \gamma (x) \right]$$ 계산  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-03-MipNeRF/5.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

## Related Work
 
### Anti-aliasing in Rendering

> `anti-aliasing`을 위한 고전적인 방법으로는 두 가지가 있다.  

1. `supersampling` :  
  - rendering할 때 `multiple rays per pixel`을 쏴서 Nyquist frequency에 가깝게 sampling rate를 높임 (super-sampling)  
  - `expensive` as runtime increases linearly with the super-sampling rate, so used only in `offline` rendering  

2. `pre-filtering` :  
  - target sampling rate에 맞춰서 Nyquist frequency를 줄이기 위해 scene에 `lowpass-filter`를 씌운 버전 사용  
  - scene을 미리 `downsampling multi-scales` (sparse voxel octree 또는 minmap)로 나타낸 뒤, `ray 대신 cone`을 추적하여 cone과 scene이 만나는 곳의 cone's footprint에 대응되는 적절한 scale을 골라서 사용 (`target sampling rate에 맞는 적절한 scale`)  
  - scene에 filter 씌운 버전을 한 번만 미리 계산하면 되므로, better for `real-time` rendering  

> 그런데 아래의 이유로 고전적인 multi-scale representation은 적용 불가능  
- input scene의 `geometry를 미리 알 수 없으므로` pre-filtering 할 수가 없어서  
대신 pre-filtering 방식 자체를 training할 때 학습해야 한다  
- input scene의 `scale이 continuous`하므로 a fixed number of scales (discrete)과 상황이 다르다  

$$\rightarrow$$ 결론 : 따라서 Mip-NeRF는 training하는 동안, `임의의 scale`에 대해 query 받을 수 있는, scene의 `pre-filtered representation`을 학습한다.

### Scene Representations for View Synthesis

- 만약 images of scene are captured `densely` 라면, novel view synthesis 위해, intermediate representation of scene을 reconstruct하지 않고 `light field interpolation` 기법 사용

- 만약 images of scene are captured `sparsely` 라면, novel view synthesis 위해, `explicit representation` of the scene's 3D geometry and appearance를 reconstruct
  - `polygon-mesh-based` representation (`discrete, classic`) :  
  with either diffuse or view-dependent textures  
  can be stored efficiently, but mesh geometry optimization에 gradient descent를 이용하는 것은 불연속점 및 극소점 때문에 어렵
  - `volumetric` representation :  
    - `voxel-grid-based` representation (`discrete, classic`) : deep learning으로 학습 가능, but 고해상도의 scene에는 부적합
    - `coordinate-based` neural representation (`continuous, recent`) : 3D 좌표를 그 위치에서의 scene의 특징(e.g. volume density, radiance)으로 mapping하는 continuous function을 MLP로 예측  
      - implicit surface representation
      - `implicit volumetric NeRF representation`

- 문제점 :  
그동안 view synthesis를 할 때 sampling 및 aliasing에는 덜 주목했다  
$$\rightarrow$$ `classic discrete` representation의 경우 앞에서 소개한 minmap, octree와 같은 고전적인 multi-scale pre-filtering 기법을 쓰면 aliasing 없이 rendering 가능하다  
$$\rightarrow$$ `coordinate-based` representation의 경우 scale이 continuous하므로 고전적인 anti-aliasing 기법은 사용할 수 없다  
$$\rightarrow$$ sparse voxel octree 기반의 multi-scale representation으로 implicit surface의 continuous neural representation을 구현한 논문 <d-cite key="continuouspriori">[1]</d-cite> 있긴 하지만, scene geometry의 priori를 알아야 한다는 제약이 있다  
$$\rightarrow$$ Mip-NeRF는 training하는 동안, `임의의 scale`에 대해 query 받을 수 있는, scene의 `anti-aliased (pre-filtered)` representation을 학습한다.

## Method

### Cone Tracing and Positional Encoding

- Cone Tracing :  
  - Let $$d$$ is cone direction vector from $$o$$ to image plane
  - Let $$\hat r = $$ radius at image plane $$o + d$$ $$= \frac{2}{\sqrt{12}}$$ of pixel-width  
  so that image plane에서의 cone의 $$x, y$$ 축 variance가 pixel's footprint의 variance와 같아지도록 `?????`  
  $$\hat r$$은 ray의 radius 변화율, 즉 frustum의 넓이를 결정  
  - $$t \in [t_0, t_1]$$ 일 때 conical frustum 내의 $$x$$는 아래 범위의 값을 가질 때 indicator function $$F(x, o, d, \hat r, t_0, t_1)=1$$이다  
  $$F(x, o, d, \hat r, t_0, t_1) = 1 \left\{ (t_0 \lt \frac{d^T(x-o)}{\| d \|^2} \lt t_1) \land (\frac{d^T(x-o)}{\| d \| \| x-o \|} \gt \frac{1}{\sqrt{1+(\frac{\hat r}{\| d \|})^2}}) \right\}$$

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-08-03-MipNeRF/6.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Region Encoding :  
conical frustum 내에 있는 모든 좌표 $$x$$에 대해 직접  
expected value $$E \left[ \gamma (x) \right] = \frac{\int \gamma (x) F(x, o, d, \hat r, t_0, t_1) dx}{\int F(x, o, d, \hat r, t_0, t_1)}$$ 계산하면  
region을 encode할 수 있는데  
여기서 분자의 적분식은 closed-form solution이 없음 `?????`  
$$\rightarrow$$ 직접 계산하지 말고  
`conical-frustum을 multi-variate Gaussian으로 근사`한 뒤  
Gaussian 내에 있는 모든 좌표 $$x$$에 대해  
expected value $$E \left[ \gamma (x) \right]$$ 계산

- frustum을 multi-variate Gaussian으로 근사 :  
  - conical-frustum은 대칭적인 원이기 때문에  
  $$o, d$$ 뿐만 아니라 아래의 3가지 정보만 알면 Gaussian을 특정할 수 있다  
    - `mean distance along ray` $$\mu_{t}$$  
    - `variance along ray` $$\sigma_{t}^2$$  
    - `variance perpendicular to ray` $$\sigma_{r}^2$$  
  - Let mid-point $$t_{\mu} = \frac{t_0+t_1}{2}$$  
  Let half-width $$t_{\sigma}=\frac{t_1-t_0}{2}$$  
  - 아래 수식의 유도과정은 하위에 별도로 정리함  
  $$\mu_{t} = t_{\mu} + \frac{2t_{\mu}t_{\sigma}^2}{3t_{\mu}^2+t_{\sigma}^2}$$  
  $$\sigma_{t}^2 = \frac{t_{\sigma}^2}{3} - \frac{4t_{\sigma}^4(12t_{\mu}^2-t_{\sigma}^2)}{15(3t_{\mu}^2+t_{\sigma}^2)^2}$$  
  $$\sigma_{r}^2 = \hat r^2 (\frac{t_{\mu}^2}{4} + \frac{5t_{\sigma}^2}{12} - \frac{4t_{\sigma}^4}{15(3t_{\mu}^2+t_{\sigma}^2)})$$
  - 위의 3가지 param.를 가지는 Gaussian은 `t-coordinate`에서 정의했는데  
  아래 수식에 의해 `world-coordinate`으로 변환할 수 있다  
  $$\mu = o + \mu_{t}d$$  
  $$\Sigma = \sigma_{t}^2(dd^T) + \sigma_{r}^2(I-\frac{dd^T}{\| d \|^2})$$ `?????`  

- Integrated Positional Encoding (IPE) :  
  - 목표 : 위에서 계산한 $$\mu, \Sigma$$ 의 Gaussian 내에 있는 모든 좌표 $$x$$에 대해 expected value $$E \left[ \gamma (x) \right]$$ 계산  
  - 우선 `PE (positional-encoding) basis` P를 재정의  
  $$P = \begin{pmatrix} \begin{matrix} 1 & 0 & 0 & 2 & 0 & 0 \\ 0 & 1 & 0 & 0 & 2 & 0 \\ 0 & 0 & 1 & 0 & 0 & 2\end{matrix} & \cdots & \begin{matrix} 2^{L-1} & 0 & 0 \\ 0 & 2^{L-1} & 0 \\ 0 & 0 & 2^{L-1}\end{matrix} \end{pmatrix}^T$$  
  $$\gamma (x) = \begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}$$  
  - $$E \left[ \gamma (x) \right]$$ 는 expectation over $$\gamma (x) = \begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}$$ 이므로  
  $$x$$ in Gaussian of $$\mu, \Sigma$$ $$\rightarrow$$ $$\gamma (x)$$ in Gaussian of $$\mu_{r}, \Sigma_{r}$$ 로 변환해야 한다  
  즉, `PE basis P로 lift`한 뒤의 mean과 covariance를 구해야 한다  
  Since $$Cov[Ax, By] = A Cov[x, y] B^T$$,  
  $$\mu_{r} = P \mu$$  
  $$\Sigma_{r} = P \Sigma P^T$$  
  - 최종적으로 $$E \left[ \gamma (x) \right]$$ , 즉 `expectation over lifted multi-variate Gaussian` 을 구하면 된다  
  Since $$E_{k \sim N(\mu, \sigma^2)}[sin(k)] = sin(\mu)exp(-\frac{1}{2}\sigma^2)$$ and $$E_{k \sim N(\mu, \sigma^2)}[cos(k)] = cos(\mu)exp(-\frac{1}{2}\sigma^2)$$ for each axis-k,  
  $$\gamma (\mu, \Sigma) = E_{x \sim N(\mu_{r}, \Sigma_{r})} [\gamma (x)] = E_{x \sim N(\mu_{r}, \Sigma_{r})} [\begin{bmatrix} sin(Px) \\ cos(Px) \end{bmatrix}]$$  
  $$= \begin{bmatrix} sin(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \\ cos(\mu_{r}) \circledast exp(-\frac{1}{2}diag(\Sigma_{r})) \end{bmatrix}$$  
  where $$\circledast$$ is element-wise multiplication  
  - Because positional encoding encodes each dimension independently 읽을 차례


To approximate a conical frustum with a multivariate Gaussian, we must compute the mean and covariance of F(x, ·) `?????`

### Conical Frustum Integral Derivation

appendix...


### Architecture

TBD

## Conclusion

TBD