---
layout: distill
title: 3D Gaussian Splatting
date: 2024-07-11 10:00:00
description: 3D GS for Real-Time Radiance Field Rendering
tags: gaussian splatting rendering 3d view synthesis
categories: 3d-view-synthesis
thumbnail: assets/img/2024-07-11-GS/1.png
giscus_comments: true
related_posts: true
bibliography: 2024-07-11-GS.bib
# toc:
#   beginning: true
#   sidebar: right
featured: true
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## 3D Gaussian Splatting for Real-Time Radiance Field Rendering

#### Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, George Drettakis

> paper :  
[https://arxiv.org/abs/2308.04079](https://arxiv.org/abs/2308.04079)  
project website :  
[https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/)  
code :  
[https://github.com/graphdeco-inria/gaussian-splatting](https://github.com/graphdeco-inria/gaussian-splatting)  
referenced blog :  
[https://xoft.tistory.com/51](https://xoft.tistory.com/51)


> 핵심 요약 :  
1. DD

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-11-GS/2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

## Abstract

- novel 3D Gaussian scene representation with real-time differentiable renderer  
`수많은 3D Gaussian이 모여 scene을 구성`하고 있다!
- Very Fast rendering ($$\geq$$ 100 FPS) :  
real-time as $$\geq$$ 30 FPS  
rasterization이 optimization의 main bottleneck인데, 3DGS는 fast rasterization 가짐  
- Higher Quality than SOTA Mip-NeRF360(2022)
- Faster Training than SOTA InstantNGP(2022)

## Introduction

### Why 3D Gaussian?

3D scene representation 방법  
1. `Mesh or Point`  
  - explicit  
  - good for fast GPU/CUDA-based rasterization(3D $$\rightarrow$$ 2D)  
2. `NeRF` method  
  - implicit (MLP)  
  - ray marching  
  - continuous coordinate-based representation  
  - interpolate values stored in voxels, hash grids, or points  
  - But,,, continuous ray로부터 discrete points를 뽑아 내는 `stochastic sampling` for rendering 때문에 `연산량이 많고 noise` 생김  
  - MLP는 dot product 및 더하기(kernel regression)의 특성상 `orthogonality`를 흐리기 때문에 high-freq. output을 잘 표현할 수 없어서 따로 미리 positional encoding을 수행  
3. `3D Gaussian` method  
  - explicit  
  - differentiable volumetric representation  
  - efficient rasterization(projection and $$\alpha$$-blending)  
  - MLP 값을 바로 사용하는 게 아니라 MLP 값을 이용해 explicitly 표현한  
  3D Gaussian(ellipsoid)이나 SH coeff.는 `orthogonality`를 잘 살리기 때문에 high-freq. output 잘 표현 가능  

### Rendering (NeRF vs 3DGS)

- NeRF :  
  - ray per pixel 쏴서 coarse(stratified) and fine(PDF) sampling하고,  
  - MLP로 sampled points의 color 및 volume density를 구하고,  
  - 이 값들을 volume rendering 식으로 summation  
- 3DGS :  
  - image를 tile(14 $$\times$$ 14 pixel)들로 나누고,  
  - tile마다 Gaussian을 Depth에 따라 정렬한 뒤  
  - 앞에서부터 뒤로 $$\alpha$$-blending

## Related Work

생략 (추후에 다시 볼 수도)

## Overview

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-11-GS/1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

For unbounded and complete scenes,  
For 1080p high resolution and real-time($$\geq$$ 30 fps) rendering,  

1. `input` :  
  - Most point-based methods require `MVS`(Multi-View Stereo) data,  
  but 3DGS only needs `SfM points` for initialization  
  - COLMAP 등 SfM(Structure-from-Motion) camera calibration으로 얻은 `sparse point cloud`에서 시작해서  
  scene을 3D Gaussians로 나타냄으로써  
  `empty space에서의 불필요한 계산을 하지 않도록` continuous volumetric radiance fields 정보를 저장  
  - NeRF-synthetic dataset의 경우 3DGS 는 random initialization으로도 좋은 퀄리티 달성  

2. `optimization` interleaved with `adaptive density control` :  
  - optimize 4 parameters :  
  3D position(mean), anisotropic covariance, opacity, and spherical harmonic coeff.(color)  
  `highly anisotropic volumetric splats`는 `fine structures`를 compact하게 나타낼 수 있음!!  
  `spherical harmonics`를 통해 `directional appearance(color)`를 잘 나타낼 수 있음!!<d-cite key="Plenoxels">[1]</d-cite>, <d-cite key="InstantNGP">[2]</d-cite>  
  - adaptive density control :  
  gradient 기반으로 Gaussian 형태를 변화시키기 위해, add and occasionally remove 3D Gaussians during optimization  

3. differentiable visibility-aware `real-time rendering` :  
perform $$\alpha$$-blending of `anisotropic splats` respecting visibility order  
by fast `GPU sorting` algorithm and `tile-based rasterization`(projection and $$\alpha$$-blending)  
한편, accumulated $$\alpha$$ values를 tracking함으로써 `Gaussians 수에 제약 없이` 빠른 backward pass도 가능  

---

### Pseudo-Code

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-11-GS/3.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

빨간 박스 : initialization  
파란 박스 : optimization  
초록 박스 : 특정 iter.마다 Gaussian을 clone, split, remove  

## Differentiable 3D Gaussian Splatting

### 3D Gaussian

- `differentiable` volumetric representation의 특성을 가지고 있으면서도 빠른 rendering을 위해 `unstructured and explicit`한 게 무엇이 있을까?  
$$\rightarrow$$ 3D Gaussian !!  

- a point를 a small planar circle with a normal이라고 가정하는 이전 Point-based rendering 논문들 <d-cite key="Point1">[3]</d-cite> <d-cite key="Point2">[4]</d-cite> 과 달리  
`SfM points는 sparse해서 normals(법선)를 estimate하기 어려울` 뿐만 아니라, estimate 한다 해도 very noisy normals를 optimize하는 것은 매우 어렵  
$$\rightarrow$$ normals 필요 없는 3D Gaussians !!  
k-dim. Gaussian : $$G(\boldsymbol x) = (2\pi)^{-\frac{k}{2}}det(\Sigma)^{-\frac{1}{2}}e^{-\frac{1}{2}(\boldsymbol x - \boldsymbol \mu)^T\Sigma^{-1}(\boldsymbol x - \boldsymbol \mu)}$$  

## Parameters to train

1. `scale vector` $$s$$ and `quaternion` $$q$$ for `covariance matrix`
2. `spherical harmonics`(SH) coeff. for `color`
3. `opacity` $$\alpha$$
4. `3D position` for `mean`

### Parameter 1. Covariance matrix

> covariance matrix and scale vector(scale) and quaternion(rotation)  

- covariance matrix는 positive semi-definite $$x^T M x \geq 0$$ for all $$x \in R^n$$이어야만 physical meaning을 가지는데,  
$$\Sigma$$ 를 직접 바로 optimize하면 invalid covariance matrix가 될 수 있음  
그렇다면!!  

$$\Sigma$$ 가 `symmetric` and `positive semi-definite`이도록 $$\Sigma = R S S^T R^T$$ 로 정의해서  
$$\Sigma$$ 대신 `x,y,z-axis scale`을 나타내는 `3D vector` $$s$$ 와 `rotation`을 나타내는 `4D quaternion` $$q$$ 를 optimize 하자!!  
quaternion에 대한 설명은 [Quaternion](https://semyeong-yu.github.io/blog/2024/Quaternion) 블로그 참고!!  

- `scale` 3D vector $$s$$ `초기값` :  
[GaussianModel().create_from_pcd()](https://github.com/graphdeco-inria/gaussian-splatting/blob/b2ada78a779ba0455dfdc2b718bdf1726b05a1b6/scene/gaussian_model.py#L134C1-L134C1)  
SfM sparse point cloud의 각 점에 대해 가장 가까운 점 3개까지의 거리의 평균을 각 axis($$x, y, z$$)별로 구한 것을 3 $$\times$$ 1 $$s$$라 할 때  
normalize 효과를 위해 log, sqrt 씌운 뒤  
3 $$\times$$ 1 $$log(\sqrt{s})$$ 의 값을 3번 복사하여 3 $$\times$$ 3 scale matrix $$S$$를 초기화  
```Python
dist2 = torch.clamp_min(distCUDA2(torch.from_numpy(np.asarray(pcd.points)).float().cuda()), 0.0000001)
scales = torch.log(torch.sqrt(dist2))[...,None].repeat(1, 3)
```

- `scale` 3D vector $$s$$ `activation function` :  
smooth gradient 얻기 위해 exponential activation function을 씌움  

- `quaternion` $$q$$ `초기값` :  
각 점에 대해 $$\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}$$ 으로 quaternion을 초기화하고  
이를 이용하여 rotation matrix $$R$$ 초기화  
```Python
rots = torch.zeros((fused_point_cloud.shape[0], 4), device="cuda")
rots[:, 0] = 1
```

- `anisotropic covariance`는 다양한 모양의 geometry를 나타내기 위해 optimize하기에 적합!  

> param. gradient 직접 유도  

training할 때 automatic differentiation으로 인한 `overhead를 방지`하기 위해 `param. gradient를 직접 유도`함!  

Appendix A. `?????`  

> EWA volume splatting (2001) :  
world-to-camera 는 linear transformation 이지만,  
`camera-to-image (projection)` 는 `non-linear transformation` 이다!!  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-11-GS/4.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    위 그림 : camera coordinate / 아래 그림 : image coordinate (ray space)
</div>

- `world` coordinate (3D) :  
  - $$\boldsymbol u = \begin{bmatrix} u_0 \\ u_1 \\ u_2 \end{bmatrix}$$  
- `camera` coordinate (3D) :  
  - $$\boldsymbol t = \begin{bmatrix} t_0 \\ t_1 \\ t_2 \end{bmatrix}$$  
  $$= W \boldsymbol u + d$$  
  where $$W$$ : `viewing transformation` affine matrix from world coordinate to camera coordinate  
- `image` coordinate (2D) :  
  - $$\boldsymbol x = \begin{bmatrix} x_0 \\ x_1 \\ x_2 \end{bmatrix}$$  
  $$= \phi(\boldsymbol t) = \begin{bmatrix} \frac{t_0}{t_2} \\ \frac{t_1}{t_2} \\ \| (t_0, t_1, t_2)^T \| \end{bmatrix}$$  
  - function $$\phi$$ 는 non-linear하므로 Affine transformation이 불가능하다.  
  - `Local Affine (Linear) transform으로 Approx.`하기 위해 $$\boldsymbol t = \boldsymbol t_{k}$$ 에서의 `Taylor Approx.`를 이용하면,  
  $$\phi_{k}(\boldsymbol t) = \phi(\boldsymbol t_{k}) + \boldsymbol J_{k} \cdot (\boldsymbol t - \boldsymbol t_{k})$$  
  where  
  $$\boldsymbol J_{k} = \frac{d\phi}{d \boldsymbol t}(\boldsymbol t_{k}) = \begin{bmatrix} \frac{d\phi}{d \boldsymbol t_{0}}(\boldsymbol t_{k}) & \frac{d\phi}{d \boldsymbol t_{1}}(\boldsymbol t_{k}) & \frac{d\phi}{d \boldsymbol t_{2}}(\boldsymbol t_{k}) \end{bmatrix} = \begin{bmatrix} \frac{1}{t_{k, 2}} & 0 & -\frac{t_{k, 0}}{t_{k, 2}^2} \\ 0 & \frac{1}{t_{k, 2}} & -\frac{t_{k, 1}}{t_{k, 2}^2} \\ \frac{t_{k, 0}}{l} & \frac{t_{k, 1}}{l} & \frac{t_{k, 2}}{l} \end{bmatrix}$$  
  and $$l = \| (t_{k, 0}, t_{k, 1}, t_{k, 2})^T \|$$  
  Here, $$J$$ : `Jacobian`(각 axis로 편미분한 matrix) of the `affine approx.` of the `projective transformation` from camera coordinate to image coordinate  
  - 즉, camera coordinate에서 임의의 좌표 $$\boldsymbol t_{k}$$ 주변에 존재하는 입력 좌표 $$\boldsymbol t$$에 대해서는 image coordinate으로의 affine(linear) transformation이 충족된다.
  - Gaussian Splatting 논문의 경우 `Gaussian의 중심점`을 $$\boldsymbol t_{k}$$ 로 두면 그 주변의 $$\boldsymbol t$$에 대해서는 Jacobian을 이용한 affine(linear) transformation 가능!  

> `Projection` of 3D Gaussian `covariance` to 2D  

- `world coordinate` :  
$$\Sigma$$ : 3 $$\times$$ 3 covariance matrix of 3D Gaussian  

- `image coordiante` (z=1) :  
$$\Sigma^{\ast} = J W \Sigma W^T J^T$$ : covariance matrix of 2D splat  
  - Step 1. world-to-camera (`affine`) :  
  $$\boldsymbol u \rightarrow W \boldsymbol u + d$$  
  - Step 2. camera-to-image (`local affine approx.`) :  
  Projection  
  $$W \boldsymbol u + d \rightarrow \phi_{k}(W \boldsymbol u + d) = x_k + \boldsymbol J_{k} W \boldsymbol u + \boldsymbol J_{k} (d - \boldsymbol t_{k})$$  
  상수 부분을 제외하면 $$\boldsymbol x = \boldsymbol J_{k} W \boldsymbol u$$  
  - Step 3. covariance 특성 :  
  $$Cov[Ax] = E[(Ax - E[Ax])(Ax - E[Ax])^T]$$  
  $$= E[A(x - E[x])(x - E[x])^TA^T] = A Cov[x] A^T$$  
  - Step 4. `world-to-image covariance` :  
  $$\boldsymbol u \rightarrow \boldsymbol J_{k} W \boldsymbol u$$ 이므로  
  $$\Sigma \rightarrow \boldsymbol J \boldsymbol W \Sigma \boldsymbol W^T \boldsymbol J^T$$  
  - Step 5. `covariance dimension reduction` :  
  추가로, $$\boldsymbol J \boldsymbol W \Sigma \boldsymbol W^T \boldsymbol J^T$$ 로 계산한 $$\Sigma^{\ast}$$ 는 3-by-3 matrix 인데,  
  3D Gaussian을 한쪽 축으로 적분하면 2D Gaussian과 동일한 값을 가지게 되므로  
  3-by-3 covariance matrix의 3번째 행과 열의 값을 버린  
  2-by-2 matrix를 projected 2D covariance matrix 로 사용하면 됨!


### Parameter 2. Spherical Harmonics(SH) coeff.

- `Spherical Harmonics` (SH) :  
spherical coordinate 에서 `각도` ($$\theta, \phi$$)를 입력받아 `구의 표면 위치에서의 값`을 출력하는 함수  
spherical coordinate 에서 라플라스 방정식을 풀면 아래 수식과 같음  
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-11-GS/5.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-11-GS/6.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-11-GS/7.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    l이 같은 함수들은 same band l에 있다고 말함
</div>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-11-GS/8.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    가로축 : theta, 세로축 : phi, 채도 : SH magnitude, 색상 : SH phase
</div>

- SH coeff. `초기값` :  
0-band SH ($$\theta, \phi$$ 와 관계없는 view-independent color) 라고 가정한 뒤  
SfM으로 얻은 point cloud의 RGB color값을 이용하여,  
이를 만드는 SH coeff. 값으로 초기화    

- SH 의 역할 :  
  - SH에서 band 수를 제한해서 쓴다는 것은 높은 band (high freq. 또는 detail info.)는 자른다는 의미이므로 `smoothing` 역할  
  - 적은 비용(coeff. 몇 개만 사용)으로 SH function을 `approx.`  

- `SH coeff.`로 `color` 나타내는 법 :  
Fourier Series 에서처럼,  
SH coeff. $$k_{l}^{m}$$ 의 optimal 값을 구해서  
$$k_{l}^{m}$$ 와 $$Y_l^m(\theta, \phi)$$ 의 weighted sum!  
$$C = \Sigma_{l=0}^{l_{max}} \Sigma_{m=-l}^{l} k_l^m Y_l^m(\theta, \phi)$$  
즉, `trainable parameter` : SH coeff.인 $$k_{l}^{m}$$  
(`light source`마다 SH coeff. $$k_{l}^{m}$$ 다르므로 find optimal value)  
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-11-GS/9.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### Parameter 3. opacity

- opacity $$\alpha$$ `초기값` :  
임의의 실수값으로 초기화  

- opacity $$\alpha$$ `range` :  
$$\alpha \in [0, 1)$$ 이므로  
마지막에 sigmoid activation function을 씌워서 smooth gradient를 얻음

### Parameter 4. 3D position(mean)

## Fast Differentiable Rasterizer for Gaussians

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-11-GS/10.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

> Tile Rasterizer  

- 기능 : 3D Gaussians로 구성된 3D model을 특정 camera pose에 대해 2D rendering

- `input` :  
  - image의 rendering할 width, height
  - 3D Gaussian의 xyz-mean, covariance in world-coordinate
  - 3D Gaussian의 color, opacity
  - current camera pose

- `Frustum Culling` :  
주어진 camera pose에서 view frustum을 그려서  
view frustum과 교차하는 확률이 99% confidence interval 범위 밖에 있는 3D Gaussians는 제거(culling)  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-11-GS/16.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- `Guard Band` :  
아래의 경우 projected 2D covariance 계산이 불안정하기 때문에 개별적으로 제거  
  - `view frustum의 near plane에 가까이 있는` Gaussian의 경우,  
  EWA Volume Splatting에서 언급된 cam-to-img projection `nonlinearity`가 심하기 때문에  
  projection matrix를 Jacobian으로 approx.한 값에 더 큰 artifact가 생김  
  - view frustum 밖에 멀리 떨어진 경우 `?????`  
  

- `Create Tiles` :  
`CUDA 병렬 처리`를 위해  
$$w \times h$$의 image를 $$16 \times 16$$ pixel의 tiles로 쪼갬  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-11-GS/11.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- `Parallelism` :  
`tile마다` 개별 `CUDA thread` block으로 실행하여  
forward/backward processing, data loading/sharing을 병렬처리  
(여러 threads가 Gaussian points를 shared memory에 collaboratively load)

- `Duplicate with Keys` :  
  - `view-space-depth`와 `tile-ID`를 이용하여 tile마다 각 Gaussian의 key를 생성  
  - `CUDA 병렬처리` 덕분에 2D Gaussian 하나가 3개의 tiles에 걸쳐 있다면, 3개의 2D Gaussians로 복제(`instance화`)되는 것처럼 작동

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-11-GS/12.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- `Sort by Keys` :  
  - tile마다 Depth 기준으로 `Radix Sort`  
  - 여기서 한 번 sort 하고 나면 끝!! 추가로 per-pixel sorting 할 필요 없음    
  - pixel-wise sorting이 아니라 Gaussians sort라서 $$\alpha$$-blending approx.이긴 한데, `splats가 각 pixel size 정도가 되기 때문에` 해당 approx. 오차는 무시 가능!  
  - 쨌든 이 덕분에 visible artifacts 없이 training, rendering performance 베리베리 굳

```Python
from collections import deque
# 양방향에서 삽입/삭제 가능한 queue형 자료구조

# 1의 자릿수 기준으로 정렬한 뒤
# 10의 자릿수 기준으로 정렬한 뒤
# ...
def radixSort():
    nums = list(map(int, input().split(' ')))
    buckets = [deque() for _ in range(10)] # 각 자릿수(0~9)에 대응되는 10개의 empty deque()
    
    max_val = max(nums)
    queue = deque(nums) # 정렬할 숫자들
    digit = 1 # 정렬 기준이 되는 자릿수
    
    while (max_val >= digit): # 가장 큰 수의 자릿수일 때까지만 실행
        while queue:
            num = queue.popleft() # 정렬할 숫자
            buckets[(num // digit) % 10].append(num) # 각 자릿수(0~9)에 따라 buckets에 num을 넣는다.
        
        # 해당 정렬 기준 자릿수에서 buckets에 다 넣었으면, buckets에 담겨있는 순서대로 꺼내와서 정렬한다.
        for bucket in buckets:
            while bucket:
                queue.append(bucket.popleft())

        digit *= 10 # 정렬 기준이 되는 자릿수 증가시키기
    
    print(list(queue))
```

- `Identify Tile Ranges` :  
tile별 Gaussian list를 효율적으로 관리하기 위해  
tile마다 Gaussian list 범위 식별  

- `Get Tile Ranges` :  
모든 tile에 대해 Gaussian list 범위 읽어옴  

- $$\alpha$$-Blending in Order (`forward process`) :  
  - tile별 CUDA 병렬처리에 의해 각 pixel에 대해 `color` 및 `opacity` $$\alpha$$ 값을 Gaussian list의 `앞에서 뒤로` accumulate  
  - i-th tile에 있는 pixels 중 a pixel's accumulated opacity 값이 target saturation threshold를 넘어서면, 해당 i-th thread STOP (유일한 STOP 조건)  
  - `Gaussian의 개수를 제한하지 않음`으로써 scene-specific hyper-param. tuning 없이 arbitrary depth complexity를 가지는 scene을 커버 가능  
  (GPU Radix Sort 덕분에 parallelism(병렬) 및 amortized(분할상환) 가능하여 Gaussian 개수 늘릴 수 있었음)  
  - `기존 기법들은 pixel마다 정렬이 필요`해서 inefficient했지만  
  본 논문은 tile별 CUDA 병렬처리 덕분에 efficient  
  (e.g. NeRF : ray per pixel 쏴서 accumulate per pixel for volume rendering `???`)  


  $$c = \alpha_{1}c_{1} + (1 - \alpha_{1})(\alpha_{2}c_{2} + (1 - \alpha_{2})(\cdots)) = \alpha_{1}c_{1} + (1 - \alpha_{1})\alpha_{2}c_{2} + (1 - \alpha_{1})(1 - \alpha_{2})(\cdots) = \cdots = \sum_{i=1}^{N}(\alpha_{i}c_{i}\prod_{j=1}^{i-1}(1-\alpha_{j}))$$ where $$\alpha_{0} = 0$$  

- `Backward process` :  
  - 각 tile의 Gaussian list에 대해 Gaussian의 `opacity 비율에 따라` `뒤에서 앞으로` gradient update  
  - 직접 backward gradient update 식을 구해서 이용  
  - backward process를 위해 <d-cite key="Point1">[3]</d-cite>처럼 `pixel마다` global memory에 blended points list를 저장할 수도 있지만  
  dynamic memory management overhead가 생기기 때문에  
  forward process에서 `tile마다` 구했던 range 및 sorted Gaussian list를 `재사용`  
  - $$\alpha$$-blending으로 합쳤던 각 Gaussian으로 gradient backpropagation을 해주려면  
  $$\alpha$$-blending 각 step에서의 accumulated opacity 값이 필요한데,  
  이를 따로 list에 저장해두고 훑는 게 아니라,  
  $$\alpha$$-blending을 할 때 그때그때 각 Gaussian point에 지금까지의 accumulated opacity 값인 $$\alpha_{l}$$을 저장해두고,  
  $$\alpha$$-blending final step에서의 total accumulated opacity 값 $$\alpha_{N}$$만 backward process에 넘겨 주면  
  backward process할 때 $$\alpha_{N}$$를 $$\alpha_{l}$$로 나눈 값을 gradient 계산에 사용 `?????`  
  - 0으로 나눠지는 경우를 방지하기 위해 $$\alpha$$ 값이 $$\frac{1}{255}$$보다 작다면 blending update 안 함  
  - rasterization 중에 blending 값이 0.9999를 초과하기 전에 STOP  
  `?????`

- Primitives :  
본 논문의 Gaussians는 `Euclidean space`에 `primitives`를 남김 `?????`  
$$\rightarrow$$ <d-cite key="Plenoxels">[1]</d-cite>, <d-cite key="MipNeRF360">[10]</d-cite>과 달리 distant or large Gaussians 처리를 위해 space compaction, warping, or projection 할 필요가 없음 `?????`  

- Efficient Rasterization :  
  - Pulsar 논문<d-cite key="Pulsar">[5]</d-cite> 에서처럼  
  an entire image에 대해 가장 작은 원소(`primitives`)를 미리 정렬(`pre-sort`)하여 `primitives = Gaussians ?????`  
  pixel-wise sorting 비용을 절감
  - differentiable  
  - arbitrary number of Gaussians에 대해 backpropagation 가능  
  with low additional memory : O(1) per pixel 
  - 2D projection 가능  

## Optimization with Adaptive Density Control of 3D Gaussians

### Optimization

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-11-GS/13.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Loss :  
predicted image와 GT image를 비교하는  
`L1 loss` 및 `D-SSIM loss`  
D-SSIM : Directional Structural Similarity Index Measure  

- 3D Gaussian의 xyz-mean에 대해서만 <d-cite key="Plenoxels">[1]</d-cite>에서처럼 `standard exponential decay scheduling` 사용  

- Adam optimizer로 네 가지 param. 업데이트  
  - 3D xyz-mean
  - 3D covariance
  - color
  - opacity

- optimization 세부 사항 :  
  - 연산을 `low resol.부터 warm-up` :  
  목적 : model이 효율적으로 coarse info.부터 학습하도록 하여 `stability` 향상  
  초기에 4배 작은 image로 optimization 진행하고 250, 500 iter.에서 2배씩 upsampling  
  - Spherical Harmonics `low band부터 warm-up` :  
  목적 : 처음부터 high band로 detail까지 학습하려고 하면  
  scene의 corner를 촬영하거나 inside-out 방식(카메라가 촬영 대상의 내부에 위치하여 바깥쪽을 촬영) 때문에  
  `놓친 angular 영역이 있을 경우 SH의 0-band coeff. (base or diffuse color)가 부적절`하게 만들어질 수 있어서  
  처음에는 0-band coeff.를 optimize하고 매 1000 iter.마다 band 수 늘려서 4-band coeff.까지 optimization

### Adaptive Density Control of Gaussians

optimization of 4 param.의 경우 매 iter.마다 update하지만,  
Adaptive Density Control of Gaussians의 경우 `100 iter.마다` update

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-11-GS/14.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- `Remove` :  
$$\alpha$$ 값이 threshold보다 작거나  
world-space에서 크기가 매우 크거나  
view-space에서 footprint가 매우 큰 경우  
3D Gaussians 제거  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-11-GS/15.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Gaussians가 scene을 제대로 표현 못 하는 중  
$$\rightarrow$$ scene을 제대로 표현하기 위해선 Gaussian position을 크게 옮겨야 함  
$$\rightarrow$$ view-space positional gradient $$\Delta_{p} L$$가 큼  
$$\rightarrow$$ under/over-reconstruction 상황이므로 clone/split을 통해 정확한 위치에 Gaussian이 분포하도록 하자

- `Split` :  
`over-reconstruction`의 경우 3D Gaussians split  
  - split : 1개의 Gaussian을 `2개로 분리`하고 각 scale을 줄인 후 `기존 3D Gaussian의 PDF`에 따라 sampling하여 배치  
  Gaussians의 수는 증가하지만, total volume은 유지
  - 조건 1. `view-space positional gradient` $$\Delta_{p} L$$의 avg. magnitude $$\geq$$ threshold $$\tau_{pos}$$  
  - 조건 2. `covariance`가 큼  

- `Clone` :  
`under-reconstruction`의 경우 3D Gaussians clone  
  - clone : `같은 크기로 copy` 후 `positional gradient 방향`에 배치  
  total volume 및 Gaussians의 수 모두 증가
  - 조건 1. `view-space positional gradient` $$\Delta_{p} L$$의 avg. magnitude $$\geq$$ threshold $$\tau_{pos}$$ 
  - 조건 2. `covariance`가 작음  

- 3000 iter.마다 $$\alpha$$ `알파 값을 주기적으로 0으로 초기화` 하면 전체 Gaussian 조절에 큰 도움이 됨!  
  - 효과 1. volumetric 기법의 특성상 `camera와 가까운 영역`에서 많은 `floater`들이 생겨서 Gaussian density가 증가하는데, 이를 제거해주는 역할  
  floater 해결 관련 논문 : <d-cite key="floater1">[6]</d-cite> <d-cite key="floater2">[7]</d-cite> <d-cite key="floater3">[8]</d-cite>  
  - 효과 2. `큰 Gaussian들이 중첩`되어 있는 case를 제거해주는 역할  
  
## Results

### Implementation

- custom CUDA kernel :  
tile-based rasterization을 위해  
custom CUDA kernel를 추가하여 사용 like <d-cite key="Point1">[3]</d-cite>, <d-cite key="Plenoxels">[1]</d-cite>, <d-cite key="superfast">[9]</d-cite>

- Radix Sort :  
fast Radix Sort를 위해 NVIDIA CUB sorting routines <d-cite key="radixsort">[11]</d-cite> 사용

- interactive image viewer :  
open-source SIBR [SIBR](https://gitlab.inria.fr/sibr/sibr_core) 이용해서  
interactive image-rendering viewer 만듬 (frame rate 측정에 사용)  

### Evaluation

- Dataset :  
bounded indoor scenes와 unbounded outdoor scenes 전부 커버  
  - synthetic Blender dataset (Nerf) :  
  have exhaustive set of bounded views with exact camera param.  
  $$\rightarrow$$ SOTA result even with 100K uniformly random initialization  
  - Mip-Nerf360 dataset  
  - Tanks&Temples dataset  
  - Hedman et al. dataset  

- Metrics :  
  - PSNR
  - L-PIPS
  - SSIM (D-SSIM)

- Comparison :  
  - `Quality` : NeRF 계열 중 SOTA인 `Mip-Nerf360` <d-cite key="MipNeRF360">[10]</d-cite>과 비교  
    - 끝까지 훈련시켰을 때 비슷한 quality 보이고,  
    - training speed는 35-45 min. versus 48 hours
  - Traning/Rendering `Speed` : NeRF 계열 중 SOTA인 `InstantNGP` <d-cite key="InstantNGP">[2]</d-cite>, `Plenoxels` <d-cite key="Plenoxels">[1]</d-cite> 과 비교  
    - speed SOTA인 <d-cite key="InstantNGP">[2]</d-cite> , <d-cite key="Plenoxels">[1]</d-cite> 과 비슷한 quality 가질 때까지 training 5-10 min.밖에 안 걸리고,  
    - 훈련 더 하면 <d-cite key="InstantNGP">[2]</d-cite>, <d-cite key="Plenoxels">[1]</d-cite>보다 더 좋은 quality 가짐

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-11-GS/19.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-11-GS/17.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    7K iter.으로도 꽤 좋은 결과
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-11-GS/18.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Comparison :  
  - Compactness :  
  anisotropic 3D Gaussians는  
  scene representation 뿐만 아니라  
  complex shape with a lower number of param.을 모델링하는 데도 쓰일 수 있음  
    - space carving으로 얻은 <d-cite key="Point3">[12]</d-cite> 의 initial point cloud에서 시작했을 때 <d-cite key="Point3">[12]</d-cite> 의 PSNR 값은 2-4 min.만에 넘겨버림  
    - 또한, <d-cite key="Point3">[12]</d-cite> 의 point cloud의 4분의 1만큼만 써도 작은 model size로도 <d-cite key="Point3">[12]</d-cite> 의 PSNR 넘겨버림  

> Space Carving :  
- 설명 : 여러 camera에 대해 voxel-space에서 object 있는 부분만 남기고 깎아내는 기법  
- 이유 : 3D reconstruction을 할 때 color 정보만으로 segmentation 가능할 정도로 background는 simple할수록 좋기 때문  
- 한계 : 빛, 그림자 같은 정보는 사용하지 않기 때문에 fg/bg 판단만 가능하다. 따라서 lidar처럼 camera에 depth-detection 메커니즘이 없을 경우 물체 내부의 구멍 같은 건 reconstruct 불가능

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-11-GS/20.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### Ablation Study

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-07-11-GS/21.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    PSNR score for Ablation Study
</div>

- Intialization (SfM) :  
We also assess the importance of initializing the 3D Gaussians from the SfM point cloud. 할 차례

- Densification (clone, split) :  
ddd

- Unlimited depth complexity of splats with gradients :  
ddd

- Anisotropic Covariance :  
ddd

- Spherical Harmonics :  
ddd

## Discussion

### Limitations

### Conclusion
