---
layout: distill
title: Difix3D+
date: 2025-03-15 12:00:00
description: Improving 3D Reconstructions with Single-Step Diffusion Models (CVPR 2025)
tags: postprocessing single step diffusion
categories: 3d-view-synthesis
thumbnail: assets/img/2025-03-15-Difix3D/3.PNG
bibliography: 2025-03-15-Difix3D.bib
giscus_comments: false
disqus_comments: true
related_posts: true
toc:
  - name: Contribution
  - name: Introduction
  - name: Related Work
  - name: Difix - from a Pretrained Diffusion Model to a 3D Artifact Fixer
    subsections:
      - name: Reference View Conditioning
      - name: Fine-tuning
      - name: Loss
      - name: Data Curation
  - name: Difix3D+ - NVS with Diffusion Priors
  - name: Experiment
    subsections:
      - name: In-the-wild Artifact Removal
      - name: Automotive Scene Enhancement
      - name: Diagnostics
  - name: Conclusion
  - name: Question
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Difix3D+ - Improving 3D Reconstructions with Single-Step Diffusion Models (CVPR 2025)

#### Jay Zhangjie Wu, Yuxuan Zhang, Haithem Turki, Xuanchi Ren, Jun Gao, Mike Zheng Shou, Sanja Fidler, Zan Gojcic, Huan Ling

> paper :  
[https://arxiv.org/abs/2503.01774](https://arxiv.org/abs/2503.01774)  
project website :  
[https://research.nvidia.com/labs/toronto-ai/difix3d/](https://research.nvidia.com/labs/toronto-ai/difix3d/)  

## Contribution

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-03-15-Difix3D/1.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Difix3D+ 설명 :  
못난 3D model로 rendering한 novel-view의 `artifacts를 제거`하도록 `2D diffusion` model (Difix)을 `minimal fine-tuning`한 뒤  
2D diffusion prior를 3D로 distill(역할 1)하거나 post-processing(역할 2)!
  - Difix 역할 1) `reconstruction at training phase` :  
  sparse view로 학습된 못난 3D model이 깨끗한 pseudo-training views를 rendering하도록 하기 위해  
  fine-tuned single-step 2D diffusion model prior를 3D model로 `distill`
  - Difix 역할 2) `neural enhancer at inference phase` :  
  single-step 2D diffusion model은 inference speed 빠르므로  
  남은 residual artifacts를 제거하기 위해 improved recon. output에 직접 Difix 적용  
  (`near real-time post-processing`)

- Difix3D+ Contribution :  
  - `minimal fine-tuning` :  
  fine-tuning하는 데 single GPU로 only a few hours 필요
  - `single-step diffusion model` :  
  each training step마다 diffusion model을 query하는 게 아니라 single-step으로 query하므로  
  inference speed 빠름
  - `general` model :  
  모든 3D models (NeRF, 3DGS 등)에 사용 가능한 single model
  - metrics :  
    - artifacts 제거(fix)하므로 3D consistency 유지한 채 FID score 2배 이상 및 PSNR 1dB 이상 향상  
    - single-step diffusion model 사용하므로 10배 이상 빠름

## Introduction

- 현재 NVS methods의 한계 :  
per-scene optimization framework
  - input이 `sparse`하거나  
  input camera poses로부터 먼 `extreme novel view`를 rendering하거나  
  `varying lighting` conditions 또는 `imperfect camera poses` 상황에서  
  spurious(가짜) geometry 또는 missing regions 등 artifacts 생김  
  - underlying geometry를 제대로 반영하지 못한 채 (incorrect shape)  
  inherent smoothness에만 의존하는 3D representation으로도  
  training images' radiance에 overfitting되는  
  `shape-radiance ambiguity` 문제

- large 2D generative model :  
  - 2D diffusion model prior :  
  large internet-scale data를 학습하여 real-world images의 distribution을 잘 이해하고 있으므로 diffusion priors는 여러 분야에 generalize 가능
    - inpainting <d-cite key="11">[1]</d-cite> <d-cite key="64">[2]</d-cite> <d-cite key="85">[3]</d-cite>
    - outpainting <d-cite key="5">[4]</d-cite> <d-cite key="62">[5]</d-cite> <d-cite key="76">[6]</d-cite>
  - 2D diffusion model prior to 3D :  
    - `each training step마다 diffusion model을 query` <d-cite key="25">[7]</d-cite> <d-cite key="41">[8]</d-cite> <d-cite key="72">[9]</d-cite> <d-cite key="89">[10]</d-cite> :  
    object-centric scenes를 optimize하고, more expansive camera trajectories를 가진 larger env.로 scale하는 데 사용  
    But,, `time-consuming`!
    - `single-step diffusion` <d-cite key="difix">[11]</d-cite> <d-cite key="22">[12]</d-cite> <d-cite key="32">[13]</d-cite> <d-cite key="49">[14]</d-cite> <d-cite key="77">[15]</d-cite> <d-cite key="78">[16]</d-cite> :  
    inference speed 빠르고  
    minimal fine-tuning만으로도 extreme novel-view에서도 NeRF/3DGS rendering의 artifacts를 "fix"할 수 있음!

## Related Work

- 3D recon. 개선 :  
imperfect noisy input data에 대응하기 위해  
  - `optimize camera poses` <d-cite key="6">[17]</d-cite> <d-cite key="21">[18]</d-cite> <d-cite key="35">[19]</d-cite> <d-cite key="39">[20]</d-cite> <d-cite key="59">[21]</d-cite> <d-cite key="69">[22]</d-cite>
  - `lighting variations` 고려 <d-cite key="34">[23]</d-cite> <d-cite key="60">[24]</d-cite> <d-cite key="73">[25]</d-cite>
  - `transient occlusions` 완화 <d-cite key="48">[26]</d-cite>
  - 위 방법들의 한계 :  
  완전히 artifacts를 해결하진 못함

- Priors for NVS :  
under-observed (잘 보지 못한) 영역들을 잘 recon.하지 못하는 문제를 해결하기 위해  
  - `Geometric priors` :  
  noise에 민감하고, dense input일 때만 미미한 개선
    - by `regularization term` <d-cite key="38">[27]</d-cite> <d-cite key="55">[28]</d-cite> <d-cite key="75">[29]</d-cite>
    - by `pre-trained models` which provide `depth GT` <d-cite key="7">[30]</d-cite> <d-cite key="45">[31]</d-cite> <d-cite key="63">[32]</d-cite> <d-cite key="90">[33]</d-cite> and `normal GT` <d-cite key="82">[34]</d-cite>
  - 여러 scenes' data로 `feed-forward neural network` 훈련 :  
  이웃한 reference views의 정보를 aggregate하여 reference views 근처에서는 잘 수행하지만, <d-cite key="88">[35]</d-cite> <d-cite key="4">[36]</d-cite> <d-cite key="31">[37]</d-cite> <d-cite key="44">[38]</d-cite> <d-cite key="79">[39]</d-cite>  
  rendering 분포가 inherently multi-mode를 가지는 ambiguous regions에서는 잘 못 함

- Generative Priors for NVS :  
  - GAN :  
  NeRF 개선하기 위해 per-scene GAN 훈련 <d-cite key="46">[40]</d-cite>
  - Diffusion :  
    - diffusion model이 `직접 novel view를 generate` by minimal fine-tuning <d-cite key="8">[41]</d-cite> <d-cite key="13">[42]</d-cite> <d-cite key="81">[43]</d-cite> <d-cite key="83">[44]</d-cite>
    - diffusion model이 `3D model의 optimization을 guide` <d-cite key="12">[45]</d-cite> <d-cite key="25">[46]</d-cite> <d-cite key="70">[47]</d-cite> <d-cite key="72">[48]</d-cite> <d-cite key="89">[49]</d-cite>  
      - 단점 : `each training step마다` diffusion model을 query해야 해서 훈련 많이 느려짐
    - diffusion model로 training image set을 augment <d-cite key="difix">[11]</d-cite> <d-cite key="27">[50]</d-cite> <d-cite key="28">[51]</d-cite>  
    위의 두 방법을 합친 느낌?!
      - 장점 : `diffusion model을 먼저 fine-tuning`하여 `training image set을 augment`한 뒤  
      diffusion model을 `single-step으로 query` (overhead 감소)함으로써  
      3D model로 distill (`3D model을 fine-tuning`)

- Diffusion Models TBD

## Difix - from a Pretrained Diffusion Model to a 3D Artifact Fixer

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-03-15-Difix3D/3.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### Reference View Conditioning
### Fine-tuning
### Loss
### Data Curation

## Difix3D+ - NVS with Diffusion Priors

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-03-15-Difix3D/2.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

## Experiment

### In-the-wild Artifact Removal
### Automotive Scene Enhancement
### Diagnostics

## Conclusion

- Limitation :  
TBD `???` Suppl. 참고

## Question