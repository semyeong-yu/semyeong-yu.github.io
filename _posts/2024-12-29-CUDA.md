---
layout: distill
title: CUDA Programming
date: 2024-12-29 12:00:00
description: .cu coding
tags: radiance field tensor decomposition
categories: others
thumbnail: assets/img/2024-12-29-CUDA/1.jpg
giscus_comments: false
disqus_comments: true
related_posts: true
toc:
  - name: SPMD Programming
  - name: CUDA Programming
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

> Lecture :  
24F EE514 Parallel Computing  
by KAIST Minsoo Rhu [VIA Research Group](https://sites.google.com/view/kaist-via)  

## SPMD Programming

- GPGPU Programming :  
  - serial part : in CPU C code (host)
  - parallel part : in GPU SPMD kernel code (device)

- SPMD (Single Program Multiple Data) :  
  - grid $$\supset$$ block $$\supset$$ thread
    - gridDim : grid 내 block 개수
    - blockIdx : block index
    - blockDim : block 내 thread 개수
    - threadIdx : thread index
  - shared memory를 공유

- memory address space :  
  - 1 address에 1 Byte를 저장하므로  
  memory address가 32-bit일 때  
  $$2^{32}$$ Byte 저장 가능
  - linear memory address space를 implement하는 것은 복잡

- Shared Memory Model
  - shared var. in shared address space에 저장함으로써 threads끼리 communicate
  - atomicity : threads끼리 겹치지 않도록 mutual exclusion  
    - semaphore
    - mutex : $$\text{LOCK(mylock); //critical section UNLOCK(mylock);}$$ 
    - atomic : $$\text{atomic{//critical section}}$$ 또는 $$\text{atomicAdd(x, 10);}$$
  - efficient implementation을 위해 hardware support 필요  
  processor 수가 많으면 costly할 수 있음
  - e.g. OpenMP

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-12-29-CUDA/2.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Message Passing Model
  - thread는 각자 private address space를 가지고 있고  
  threads끼리 message 주고받음으로써 communicate
  - system-wide load/store를 위한 hardware implementation 필요 없음
  - e.g. Open MPI

## CUDA Programming

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-12-29-CUDA/3.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- CUDA APIs :  
  - $$\text{cudaMalloc()}$$ : device(GPU) global memory에 allocate
  - $$\text{cudaFree()}$$ : device(GPU) global memory free
  - $$\text{cudaMemcpy()}$$ : data transfer between host(CPU) and device(GPU)

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-12-29-CUDA/4.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- CUDA Function :  
  - $$\text{__global__}$$ :  
  kernel function 정의 (host에서 call해서 device에서 execute) 정의  
  return void

```c++
__global__
void vecAddKernel(float* A, float* B, float* C, int n)
{
  // rank
  int i = threadIdx.x + blockDim.x * blockIdx.x;
  if (i < n)
    C[i] = A[i] + B[i];
}

// n : global rank
dim3 DimGrid((n-1)/256 + 1, 1, 1);
// 256 threads per block
// DimBlock.x = 256
dim3 DimBlock(256, 1, 1);

// call kernel func.
// kernel func.<<#block, #thread>>(param.)
vecAddKernel<<DimGrid, DimBlock>>(d_A, d_B, d_C, n);
```

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-12-29-CUDA/5.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

TBD 39p