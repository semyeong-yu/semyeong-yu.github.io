---
layout: distill
title: DUSt3R
date: 2024-11-19 12:00:00
description: Geometric 3D Vision Made Easy (CVPR 2024)
tags: point regression pose free
categories: 3d-view-synthesis
thumbnail: assets/img/2024-11-19-DUSt3R/2.PNG
giscus_comments: false
disqus_comments: true
related_posts: true
toc:
  - name: Contribution
  - name: Algorithm
  - name: Loss
  - name: Experiments
  - name: Downstream - stereo pixel matching
  - name: Downstream - camera intrinsic estimation
  - name: Downstream - camera extrinsic estimation
  - name: Downstream - Global Alignment
  - name: Downstream - Depth Estimation
  - name: Downstream - Dense 3D reconstruction

_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## DUSt3R - Geometric 3D Vision Made Easy (CVPR 2024)

#### Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, Jerome Revaud

> paper :  
[https://arxiv.org/abs/2312.14132](https://arxiv.org/abs/2312.14132)  
project website :  
[https://europe.naverlabs.com/research/publications/dust3r-geometric-3d-vision-made-easy/](https://europe.naverlabs.com/research/publications/dust3r-geometric-3d-vision-made-easy/)  
code :  
[https://github.com/naver/dust3r](https://github.com/naver/dust3r)  
reference :  
[https://xoft.tistory.com/83](https://xoft.tistory.com/83)

### Contribution

- MVS(Multi-View Stereo) 분야에서는 일반적으로 camera param.를 알아야 해서  
SfM(Structure from Motion)을 사용해서 camera param. estimaton을 하지만  
이는 많은 연산 필요

- DUSt3R :  
  - `SfM 생략`하고  
  `regression-based`  
  `2D(img)-to-3D(point map) mapping network` 이용해서 3D recon. 수행  
  - 1번 view를 기준으로 2번 view의 3D points를 `상대적으로 align`하므로  
  (3D point의 `절대적인 위치를 추정하는 게 아니므로`)  
  intrinsic/extrinsic `camera param. 몰라도` ok  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-11-19-DUSt3R/1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### Algorithm

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-11-19-DUSt3R/2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Algorithm :  
  - Step 1) input  
  image 2장
  - Step 2) ViT encoder  
  두 images의 feature 비교하기 위해  
  `Siamese` (shared weight) 구조 사용
  - Step 3) Transformer decoder  
  두 features의 관계를 학습하여  
  aligned pointmap 만들기 위해  
  `self-attention and cross-attention` 수행
  - Step 4) Head output  
  per-pixel `Pointmap` $$X_{i}^{v, 1} \in R^{W \times H \times 3}$$  
  and  
  per-pixel `Confidence` score $$C_{i}^{v, 1} \in R^{W \times H}$$

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-11-19-DUSt3R/3.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

1번 camera : base view, 2번 camera : reference view  
$$G_{i}^{1}$$ : 1번 view feature의 Transformer Decoder에서 $$i$$-th Block  
$$G_{i}^{2}$$ : 2번 view feature의 Transformer Decoder에서 $$i$$-th Block  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-11-19-DUSt3R/4.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Pointmap :  
$$X^{1, 1}$$ : 1번 view 시점을 기준으로 1번 view에서 보이는 3D point 좌표  
$$X^{2, 1}$$ : 1번 view 시점을 기준으로 2번 view에서 보이는 3D point 좌표  

- Confidence score :  
$$C_{i}^{v, 1}$$ : 1번 view 시점을 기준으로 $$v$$ 번 view에서 보이는 $$i$$-th 3D point의 confidence score  
  - 물체인 부분에서는 3D point를 비교적 정확히 예측할 수 있으므로 confidence가 높고,  
  하늘 또는 반투명인 부분에서는 3D point를 정확하게 예측할 수 없으므로 confidence가 낮게 나옴
  - $$C_{i}^{v, 1} = 1 + \text{exp}(\tilde C_{i}^{v, 1}) \gt 1$$ 로 설정하여  
  하나의 view에만 존재해서 추정하기 어려운 3D point의 경우에는 extrapolate할 수 있도록 `???`

- 1번 view를 기준으로 2번 view의 3D points를 `상대적으로 align`하므로  
3D point의 `절대적인 위치를 추정하는 게 아니므로`  
intrinsic/extrinsic `camera param. 몰라도` ok  

### Loss

- regression loss :  
$$L_{regr} (v, i) = \| \frac{1}{z} X_{i}^{v, 1} - \frac{1}{\bar z} \bar X_{i}^{v, 1} \|$$  
  - $$i$$ : each point, $$v$$ : each view  
  - $$z = \text{norm}(X^{1, 1}, X^{2, 1})$$ : averaged depth of prediction point  
  - $$\bar z = \text{norm}(\bar X^{1, 1}, \bar X^{2, 1})$$ : averaged depth of GT point  
  - $$\text{norm}(X^{1, 1}, X^{2, 1}) = \frac{1}{| D^{1} | + | D^{2} |} \sum_{v \in \{ 1, 2 \}} \sum_{i \in D^{v}} \| X_{i}^{v, 1} \|$$ : 모든 depth 값에 대한 평균  

- final loss :  
$$L_{conf} = \sum_{v \in \{ 1, 2 \}} \sum_{i \in D^{v}} C_{i}^{v, 1} L_{regr}(v, i) - \alpha \text{log} C_{i}^{v, 1}$$  
  - $$C_{i}^{v, 1} L_{regr}(v, i)$$ :  
  confidence가 큰 `(확실한) point`에서는 GT와의 `regression loss` $$L_{regr}$$ 가 더 `작도록`
  - $$- \alpha \text{log} C_{i}^{v, 1}$$ : regularization term  
  `confidence` $$C_{i}^{v, 1}$$ 값이 `너무 작아지지 않도록`

### Experiments

- Model  
CroCo pre-trained model 사용 `???`
  - encoder : ViT-Large
  - decoder : ViT-Base
  - head : DPT (ViT를 Depth Estimation에 적용한 연구)

### Downstream - stereo pixel matching

- 2개의 image에 대한 pointmap을 겹쳤을 때 align되도록  
pixel correspondence를 찾음  
  - $$X^{2, k}$$ 중에 3D point $$X_{i}^{1, k}$$ 와 가장 가까운 3D point가 $$X_{j}^{2, k}$$ 이고,  
  $$X^{1, k}$$ 중에 3D point $$X_{j}^{2, k}$$ 와 가장 가까운 3D point가 $$X_{i}^{1, k}$$ 일 때  
  두 pixel $$i, j$$ 사이에 correspondence 있다고 함
  - 모든 pixel에 대해 correspondence가 생기지는 않음

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-11-19-DUSt3R/5.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### Downstream - camera intrinsic estimation

TBD 

### Downstream - camera extrinsic estimation
### Downstream - Global Alignment
### Downstream - Depth Estimation
### Downstream - Dense 3D reconstruction