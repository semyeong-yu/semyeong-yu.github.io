---
layout: distill
title: DUSt3R
date: 2024-11-19 12:00:00
description: Geometric 3D Vision Made Easy (CVPR 2024)
tags: point regression pose free
categories: 3d-view-synthesis
thumbnail: assets/img/2024-11-19-DUSt3R/2.PNG
giscus_comments: false
disqus_comments: true
related_posts: true
toc:
  - name: Contribution
  - name: Algorithm
  - name: Loss
  - name: Experiments
  - name: Downstream - stereo pixel matching
  - name: Downstream - camera intrinsic estimation
  - name: Downstream - camera extrinsic estimation
  - name: Downstream - Global Alignment
  - name: Downstream - Depth Estimation
  - name: Downstream - Dense 3D reconstruction

_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## DUSt3R - Geometric 3D Vision Made Easy (CVPR 2024)

#### Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, Jerome Revaud

> paper :  
[https://arxiv.org/abs/2312.14132](https://arxiv.org/abs/2312.14132)  
project website :  
[https://europe.naverlabs.com/research/publications/dust3r-geometric-3d-vision-made-easy/](https://europe.naverlabs.com/research/publications/dust3r-geometric-3d-vision-made-easy/)  
code :  
[https://github.com/naver/dust3r](https://github.com/naver/dust3r)  
reference :  
[https://xoft.tistory.com/83](https://xoft.tistory.com/83)

### Contribution

- MVS(Multi-View Stereo) 분야에서는 일반적으로 camera param.를 알아야 해서  
SfM(Structure from Motion)을 사용해서 camera param. estimaton을 하지만  
이는 많은 연산 필요

- DUSt3R :  
  - `SfM 생략`하고  
  `regression-based`  
  `2D(img)-to-3D(point map) mapping network` 이용해서 3D recon. 수행  
  - 1번 view를 기준으로 2번 view의 3D points를 `상대적으로 align`하므로  
  (3D point의 `절대적인 위치를 추정하는 게 아니므로`)  
  intrinsic/extrinsic `camera param. 몰라도` ok  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-11-19-DUSt3R/1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### Algorithm

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-11-19-DUSt3R/2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Algorithm :  
  - Step 1) input  
  image 2장
  - Step 2) ViT encoder  
  두 images의 feature 비교하기 위해  
  Siamese (shared weight) 구조 사용
  - Step 3) Transformer decoder  
  두 features의 관계를 학습하여  
  aligned pointmap 만들기 위해  
  self-attention and cross-attention 수행
  - Step 4) Head output  
  per-pixel Pointmap and Confidence

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-11-19-DUSt3R/3.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

1번 camera : base view, 2번 camera : reference view  
$$G_{i}^{1}$$ : 1번 view feature의 Transformer Decoder에서 $$i$$-th Block  
$$G_{i}^{2}$$ : 2번 view feature의 Transformer Decoder에서 $$i$$-th Block  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-11-19-DUSt3R/4.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

$$X^{1, 1}$$ : 1번 view 시점을 기준으로 1번 view에서 보이는 3D point 좌표  
$$X^{2, 1}$$ : 1번 view 시점을 기준으로 2번 view에서 보이는 3D point 좌표

- 1번 view를 기준으로 2번 view의 3D points를 `상대적으로 align`하므로  
3D point의 `절대적인 위치를 추정하는 게 아니므로`  
intrinsic/extrinsic `camera param. 몰라도` ok  

### Loss

TBD

### Experiments
### Downstream - stereo pixel matching
### Downstream - camera intrinsic estimation
### Downstream - camera extrinsic estimation
### Downstream - Global Alignment
### Downstream - Depth Estimation
### Downstream - Dense 3D reconstruction