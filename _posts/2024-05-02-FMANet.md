---
layout: distill
title: FMANet
date: 2024-05-02 14:00:00
description: Flow-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring
tags: super-resolution deblur flow dynamic attention
categories: super-resolution
thumbnail: assets/img/2024-05-02-FMANet/1.png
giscus_comments: true
related_posts: true
# toc:
#   beginning: true
#   sidebar: right
images:
  compare: true
  slider: true
featured: true
bibliography: 2024-05-02-FMANet.bib
toc:
  - name: Abstract
    subsections:
      - name: Joint learning of VSRDB
      - name: FGDF
      - name: FRMA
  - name: Related Work
    subsections:
      - name: VSR
      - name: DB
      - name: Joint learning of VSRDB
      - name: Dynamic Filter Network
  - name: Method
    subsections:
      - name: Overview
      - name: FRMA block
      - name: FGDF
      - name: Overall Architecture
      - name: Training
  - name: Results
    subsections:
      - name: Comparision with SOTA
      - name: Ablation Study
  - name: Conclusion
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

### FMA-Net : Flow-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring

#### Geunhyuk Youk, Jihyong Oh, Munchurl Kim

> paper :  
[https://arxiv.org/abs/2401.03707](https://arxiv.org/abs/2401.03707)  
project website :  
[https://kaist-viclab.github.io/fmanet-site/](https://kaist-viclab.github.io/fmanet-site/)  
pytorch code :  
[https://github.com/KAIST-VICLab/FMA-Net](https://github.com/KAIST-VICLab/FMA-Net)  

> 핵심 요약 :  
1. DD
2. DD
3. DD
4. DD
5. DD

---

## Abstract


#### Task : Joint learning of VSRDB (`video super-resolution and deblurring`)

restore HR video from blurry LR video  
challenging because should handle two types of degradation (SR and deblurring) simultaneously  


#### FGDF (`flow-guided dynamic filtering`)

precise estimation of both spatio-temporally-variant `degradation` and `restoration` kernels that are aware of motion trajectories (not stick to fixed positions)  
effectively handle large motions with small-sized kernels (naive dynamic filtering의 한계 극복)  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-05-02-FMANet/2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

DCN (Deformable Conv.) : learn position-invariant $$n \times n$$ filter coeff.  
vs  
DF (Dynamic filtering) : learn position-wise $$n \times n$$ dynamic filter coeff.  

DF (Dynamic Filtering) : fixed surroundings  
vs  
FGDF (Flow Guided DF) : variable surroundings by learned optical flow  


#### FRMA (`iterative feature refinement with multi-attention`)

refine features by iterative updates  
loss : TA (temporal anchor)  
multi-attention :  
- center-oriented attention (focus on target frame)  
- degradation-aware attention (use degradation kernels in globally adaptive manner)  

---

## Related Work


#### VSR (Video Super-Resolution)

Based on the number of input frames,  
1. sliding window-based method : recover HR frames by using neighboring frames within a sliding window  
use CNN, optical flow estimation, deformable conv., or transformer focusing on temporal alignment  
vs  
2. recurrent-based method : sequentially propagate the latent features of one frame to the next frame  
Chan et al. <d-cite key="vsr">[1]</d-cite> BasicVSR++ : combine bidirectional propagation of past and future frames into current frame features  
limit : gradient vanishing  


#### DB (Video Deblurring)

Zhang et al. <d-cite key="adversarial">[2]</d-cite> 3D CNN  
Li et al. <d-cite key="groupshift">[3]</d-cite> grouped spatial-temporal shifts  
transformer-based : Restormer <d-cite key="restormer">[4]</d-cite>, Stripformer <d-cite key="stripformer">[5]</d-cite>, RVRT <d-cite key="rvrt">[6]</d-cite>  


#### Joint learning of VSRDB (not sequential cascade of VSR and DB)

Previous works are mostly designed for ISRDB  

Fang et al. <d-cite key="HOFFR">[7]</d-cite> HOFFR :  
the first deep-learning-based VSRDB  
limit : struggle to deblur spatially-variant motion blur because 2D CNN has spatially-equivariant and input-independent filters  


#### Dynamic Filter Network

predict spatially-variant degradation or restoration kernels  

Zhou et al. <d-cite key="adaptivefilter">[8]</d-cite> :  
spatially adaptive deblurring filter for recurrent video deblurring  
Kim et al. <d-cite key="koalanet">[9]</d-cite> KOALAnet :  
blind SR predicts spatially-variant degradation and upsampling filters  

limit : apply dynamic filtering only to the reference frame (target position and its fixed surrounding neighbors), so cannot accurately exploit spatio-temporally-variant-motion info. from adjacent frames  
limit : if apply dynamic filtering to adjacent frames $$\rightarrow$$ large-sized filters are required to capture large motions $$\rightarrow$$ high computational complexity  
limit : <d-cite key="separableconv">[10]</d-cite> suggested two separable large 1D kernels to approximate a large 2D kernel $$\rightarrow$$ does not capture fine detail, so inappropriate for video  

---

## Method


#### Overview

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-05-02-FMANet/1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

FMA-Net : VSRDB framework based on FGDF and FRMA  
allow for small-to-large motion representation learning  

- input : `blurry LR sequence` $$X = \left\lbrace X_{c-N}:X_{c+N} \right\rbrace \in R^{T \times H \times W \times 3}$$ where $$T=2N+1$$ and $$c$$ is a center frame index  
- goal : predict `sharp HR center frame` $$\hat Y_{c} \in R^{sH \times sW \times 3}$$ where $$s$$ is SR scale factor  

1. degradation learning network $$Net^{D}$$ : learn `motion-aware` `spatio-temporally-variant` degradation kernels  
2. restoration network $$Net^{R}$$ : utilize these degradation kernels in a `globally adaptive` manner to restore center frame $$X_c$$  
3. $$Net^{D}$$ and $$Net^{R}$$ consist of FRMA blocks and FGDF module  


#### FRMA block

pre-trained optical flow network : unstable for blurry frames and computationally expensive  

vs  

FRMA block :  
learn `self-induced` optical flow in a residual learning manner  
learn `multiple` optical flows with corresponding occlusion masks  
$$\rightarrow$$ flow diversity enables to learn one-to-many relations b.w. pixels in a target frame and its neighbor frames  
$$\rightarrow$$ beneficial since blurry frame's pixel info. is spread due to light accumulation  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-05-02-FMANet/3.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

> Three features  
1. $$F \in R^{T \times H \times W \times C}$$ :  
temporally-anchored (unwarped) feature at each frame index ($$0 \sim T-1$$)  
2. $$F_w \in R^{H \times W \times C}$$ :  
warped feature  
3. $$\boldsymbol f = \left \lbrace f_{c \rightarrow c+t}^{j}, o_{c \rightarrow c+t}^{j} \right \rbrace _{j=1:n}^{t=-N:N} \in R^{T \times H \times W \times (2+1)n}$$ :  
multi-flow-mask pairs  
$$f_{c \rightarrow c+t}^{j}$$ : learnable optical flow  
$$o_{c \rightarrow c+t}^{j}$$ : learnable occlusion mask (sigmoid for stability)  
$$n$$ is the number of multi-flow-mask pairs from the center frame index $$c$$ to each frame index  

> (i+1)-th Feature Refinement : 위첨자로 표기  
1. $$F^{i+1}$$=RDB($$F^{i}$$) :  
RDB <d-cite key="RDB">[11]</d-cite>  
2. $$\boldsymbol f^{i+1}$$ = $$\boldsymbol f^{i}$$ + Conv3d(concat($$\boldsymbol f^{i}$$, $$W$$($$F^{i+1}$$, $$\boldsymbol f^{i}$$), $$F_{c}^{0}$$))  
$$W$$($$F^{i+1}$$, $$\boldsymbol f^{i}$$) : warp $$F^{i+1}$$ to center frame index $$c$$ based on $$f^{i}$$  
$$W$$ : occlusion-aware backward warping  
concat : along channel dim.  
$$F_{c}^{0} \in R^{H \times W \times C}$$ : feature map at center frame index $$c$$ of the initial feature $$F^{0} \in R^{T \times H \times W \times C}$$  
3. $$\tilde F_{w}^{i}$$ = Conv2d(concat($$F_{w}^{i}$$, $$r_{4 \rightarrow 3}$$($$W$$($$F^{i+1}$$, $$\boldsymbol f^{i+1}$$))))  
$$r_{4 \rightarrow 3}$$ : reshape from $$R^{T \times H \times W \times C}$$ to $$R^{H \times W \times TC}$$ for feature aggregation  
4. $$F_w^{i+1}$$ = Multi-Attn($$\tilde F_{w}^{i}$$, $$F_{c}^{0}$$(, $$k^{D, i}$$)) 

> RDB Network <d-cite key="RDB">[11]</d-cite> :  
ddd

> occlusion-aware backward warping <d-cite key="warp">[12]</d-cite> <d-cite key="warpp">[13]</d-cite> <d-cite key="warppp">[14]</d-cite> :  
ddd

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-05-02-FMANet/3.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

> Multi-Attention :  
- CO(center-oriented) attention :  
better align $$\tilde F_{w}^{i}$$ to $$F_{c}^{0}$$ (center feature map of initial temporally-anchored feature)  
- DA(degradation-aware) attention :  
globally adaptive to spatio-temporally variant degradation by using degradation kernels $$K^{D}$$  

- CO attention :  
$$Q=W_{q} F_{c}^{0}$$  
$$K=W_{k} \tilde F_{w}^{i}$$  
$$V=W_{v} \tilde F_{w}^{i}$$  
$$COAttn(Q, K, V) = softmax(\frac{QK^{T}}{\sqrt{d}})V$$  
실험 결과, $$\tilde F_{w}^{i}$$가 자기 자신(self-attention)이 아니라 $$F_{c}^{0}$$과의 relation에 집중할 때 better performance  

- DA attention :  
CO attention과 비슷하지만,  
Query 만들 때 $$F_{c}^{0}$$ 대신 $$k^{D, i}$$ 사용  
$$k^{D, i} \in R^{H \times W \times C}$$ : feature after conv. with $$K^{D}$$ (motion-aware spatio-temporally-variant degradation kernels) 에 대해  
$$Q=W_{q} k^{D, i}$$  
DA attention은 $$Net^{D}$$ 말고 $$Net^{R}$$ 에서만 사용  


#### FGDF

spatio-temporal Dynamic Filter :  
$$y(p) = \sum_{t=-N}^{N} \sum_{k=1}^{n^2} F_{c+t}^{p}(p_k) x_{c+t}^{\ast}(p+p_k)$$  
where  
$$c$$ : center frame index  
$$p_k \in \{ (- \lfloor \frac{n}{2} \rfloor, - \lfloor \frac{n}{2} \rfloor), \cdots , (\lfloor \frac{n}{2} \rfloor, \lfloor \frac{n}{2} \rfloor) \}$$ : sampling offset for conv. with $$n \times n$$ kernel  
$$F^p$$ : predicted $$n \times n$$ dynamic filter at position p  
$$x_{c+t}^{\ast} = W(x_{c+t}, \boldsymbol f_{c+t})$$ : warped input feature based on $$\boldsymbol f_{c+t}$$  
$$\boldsymbol f_{c+t}$$ : flow-mask pair from frame index $$c$$ to $$c+t$$  
  
limit :  
fixed position ($$p$$) and fixed surrounding neighbors ($$p_k$$)  
$$\rightarrow$$ To capture large motion, require large-sized filter  
  
solution : FGDF  
kernels - dynamically generated / pixel-wise (position-wise) / variable surroundings guided by optical flow  
$$\rightarrow$$ can handle large motion with relatively small-sized filter  


#### Overall Architecture

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-05-02-FMANet/1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

> Degradation Network $$Net^{D}$$  
input : blurry LR sequence $$\boldsymbol X$$  
goal : motion-aware spatio-temporally-variant degradation kernels $$K^{D}$$  
$$\rightarrow$$ obtain center frame $$\boldsymbol X_{c}$$ from sharp HR counterpart $$\boldsymbol Y$$  
`sharp HR counterpart ?????`

</d-cite> <d-cite key="rrdb">[15]</d-cite>

- step 1-1. initialize  
$$\boldsymbol X \rightarrow$$ 3D RRDB $$\rightarrow F^{0}$$  

- step 1-2. initialize  
$$F_{w}^{0} = 0$$, $$\boldsymbol f = \left \lbrace f_{c \rightarrow c+t}^{j} = 0, o_{c \rightarrow c+t}^{j} = 1 \right \rbrace _{j=1:n}^{t=-N:N}$$  

- step 2. M FRMA blocks  
$$F^{0}, F_{w}^{0}, \boldsymbol f^{0} \rightarrow$$ $$M$$ FRMA blocks $$\rightarrow F^{M}, F_{w}^{M}, \boldsymbol f^{M}$$  

- step 3-1. image flow-mask pair $$\boldsymbol f^{Y} \in R^{T \times H \times W \times (2+1)}$$  
$$\boldsymbol f^{M} \rightarrow$$ Conv3d $$\rightarrow \boldsymbol f^{Y}$$  

- step 3-2. motion-aware spatio-temporally-variant degradation kernels $$K^{D} \in R^{T \times H \times W \times k_{d}^{2}}$$  
$$k_{d}$$ : degradation kernel size  
sigmoid for normalization : all kernels have `positive` values, which mimics `blur generation process`  
$$K^{D}$$ = softmax(Conv3d($$r_{4 \rightarrow 3}$$($$F_{w}^{M}$$)))  

- step 4. Finally, ... 할 차례  

> Restoration Network $$Net^{R}$$  
input :  
goal :
  

#### Training

---

## Results

#### Comparision with SOTA


#### Ablation Study

---

## Conclusion