---
layout: post
title: FMANet
date: 2024-05-02 14:00:00
description: Flow-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring
tags: super-resolution deblur flow dynamic attention
categories: super-resolution
thumbnail: assets/img/2024-05-02-FMANet/1.png
giscus_comments: true
related_posts: true
toc:
  beginning: true
  sidebar: right
images:
  compare: true
  slider: true
featured: true
---

### FMA-Net : Flow-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring

#### Geunhyuk Youk, Jihyong Oh, Munchurl Kim

> paper :  
[https://arxiv.org/abs/2401.03707](https://arxiv.org/abs/2401.03707)  
project website :  
[https://kaist-viclab.github.io/fmanet-site/](https://kaist-viclab.github.io/fmanet-site/)  
pytorch code :  
[https://github.com/KAIST-VICLab/FMA-Net](https://github.com/KAIST-VICLab/FMA-Net)  

> 핵심 요약 :  
1. DD
2. DD
3. DD
4. DD
5. DD

## Abstract

- Joint learning of VSRDB (`video super-resolution and deblurring`) :  
restore HR video from blurry LR video  
challenging because should handle two types of degradation (SR and deblurring) simultaneously  

- FGDF (`flow-guided dynamic filtering`) :  
precise estimation of both spatio-temporally-variant `degradation` and `restoration` kernels that are aware of motion trajectories (not stick to fixed positions)  
effectively handle large motions with small-sized kernels  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-05-02-FMANet/2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

DCN (deformable conv) : learn position-invariant $$n \prod n$$ filter coeff.  
FGDF : learn position-wise $$n \prod n$$ dynamic filter coeff.  

- FRMA (`iterative feature refinement with multi-attention`) :  
refine features in coarse-to-fine manner by iterative updates  
loss : TA (temporal anchor)  

## Introduction

- Joint learning of VSRDB :  
Previous works are mostly designed for ISRDB  

Fang et al. <d-cite key="HOFFR"></d-cite> proposed HOFFR :  
the first deep-learning-based VSRDB  
limit : struggle to deblur spatially-variant motion blur because CNN has spatially-equivariant and input-independent filters  

- Dynamic Filter Network :  
predict spatially-variant degradation or restoration kernels  

Zhou et al. <d-cite key="adaptivefilter"></d-cite> used spatially adaptive alignment and deblurring filters  
limit : apply dynamic filtering only to the reference frame, so cannot accurately exploit motion info. from adjacent frames  
limit : if apply dynamic filtering to adjacent frames $$\rightarrow$$ large-sized filters are required to capture large motions $$\rightarrow$$ high computational complexity  
limit : <d-cite key="separableconv"></d-cite> suggested two separable large 1D kernels to approximate a large 2D kernel $$\rightarrow$$ does not capture fine detail, so inappropriate for video  

- FMA-Net :  
VSRDB framework based on FGDF and FRMA  
allow for small-to-large motion representation learning
Our FMA-Net ... 읽을 차례


## Related Work

#### Video Super-Resolution