---
layout: distill
title: FMANet
date: 2024-05-02 14:00:00
description: Flow-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring
tags: super-resolution deblur flow dynamic attention
categories: super-resolution
thumbnail: assets/img/2024-05-02-FMANet/1.png
giscus_comments: true
related_posts: true
toc:
  beginning: true
  sidebar: right
images:
  compare: true
  slider: true
featured: true
bibliography: 2024-05-02-FMANet.bib
toc:
  - name: Abstract
    subsections:
      - name: Joint learning of VSRDB
      - name: FGDF
      - name: FRMA
  - name: Related Work
    subsections:
      - name: VSR
      - name: DB
      - name: Joint learning of VSRDB
      - name: Dynamic Filter Network
  - name: Method
    subsections:
      - name: Overview
      - name: FRMA block
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

### FMA-Net : Flow-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring

#### Geunhyuk Youk, Jihyong Oh, Munchurl Kim

> paper :  
[https://arxiv.org/abs/2401.03707](https://arxiv.org/abs/2401.03707)  
project website :  
[https://kaist-viclab.github.io/fmanet-site/](https://kaist-viclab.github.io/fmanet-site/)  
pytorch code :  
[https://github.com/KAIST-VICLab/FMA-Net](https://github.com/KAIST-VICLab/FMA-Net)  

> 핵심 요약 :  
1. DD
2. DD
3. DD
4. DD
5. DD


## Abstract


#### Task : Joint learning of VSRDB (`video super-resolution and deblurring`)

restore HR video from blurry LR video  
challenging because should handle two types of degradation (SR and deblurring) simultaneously  


#### FGDF (`flow-guided dynamic filtering`)

precise estimation of both spatio-temporally-variant `degradation` and `restoration` kernels that are aware of motion trajectories (not stick to fixed positions)  
effectively handle large motions with small-sized kernels  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-05-02-FMANet/2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

DCN (Deformable Conv.) : learn position-invariant $$n \times n$$ filter coeff.  
vs  
DF (Dynamic filtering) : learn position-wise $$n \times n$$ dynamic filter coeff.  

DF (Dynamic Filtering) : fixed surroundings  
vs  
FGDF (Flow Guided DF) : variable surroundings by learned optical flow  


#### FRMA (`iterative feature refinement with multi-attention`)

refine features by iterative updates  
loss : TA (temporal anchor)  
multi-attention :  
- center-oriented attention (focus on target frame)  
- degradation-aware attention (use degradation kernels in globally adaptive manner)  


## Related Work


#### VSR (Video Super-Resolution)

Based on the number of input frames,  
1. sliding window-based method : recover HR frames by using neighboring frames within a sliding window  
use CNN, optical flow estimation, deformable conv., or transformer focusing on temporal alignment  
vs  
2. recurrent-based method : sequentially propagate the latent features of one frame to the next frame  
Chan et al. <d-cite key="vsr">[1]</d-cite> BasicVSR++ : combine bidirectional propagation of past and future frames into current frame features  
limit : gradient vanishing  


#### DB (Video Deblurring)

Zhang et al. <d-cite key="adversarial">[2]</d-cite> 3D CNN  
Li et al. <d-cite key="groupshift">[3]</d-cite> grouped spatial-temporal shifts  
transformer-based : Restormer <d-cite key="restormer">[4]</d-cite>, Stripformer <d-cite key="stripformer">[5]</d-cite>, RVRT <d-cite key="rvrt">[6]</d-cite>  


#### Joint learning of VSRDB (not sequential cascade of VSR and DB)

Previous works are mostly designed for ISRDB  

Fang et al. <d-cite key="HOFFR">[7]</d-cite> HOFFR :  
the first deep-learning-based VSRDB  
limit : struggle to deblur spatially-variant motion blur because 2D CNN has spatially-equivariant and input-independent filters  


#### Dynamic Filter Network

predict spatially-variant degradation or restoration kernels  

Zhou et al. <d-cite key="adaptivefilter">[8]</d-cite> :  
spatially adaptive deblurring filter for recurrent video deblurring  
Kim et al. <d-cite key="koalanet">[9]</d-cite> KOALAnet :  
blind SR predicts spatially-variant degradation and upsampling filters  

limit : apply dynamic filtering only to the reference frame (target position and its fixed surrounding neighbors), so cannot accurately exploit spatio-temporally-variant-motion info. from adjacent frames  
limit : if apply dynamic filtering to adjacent frames $$\rightarrow$$ large-sized filters are required to capture large motions $$\rightarrow$$ high computational complexity  
limit : <d-cite key="separableconv">[10]</d-cite> suggested two separable large 1D kernels to approximate a large 2D kernel $$\rightarrow$$ does not capture fine detail, so inappropriate for video  


## Method


#### Overview

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-05-02-FMANet/1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

FMA-Net : VSRDB framework based on FGDF and FRMA  
allow for small-to-large motion representation learning  

- input : `blurry LR sequence` $$X = \left\lbrace X_{c-N}:X_{c+N} \right\rbrace \in R^{T \times H \times W \times 3}$$ where $$T=2N+1$$ and $$c$$ is a center frame index  
- goal : predict `sharp HR center frame` $$\hat Y_{c} \in R^{sH \times sW \times 3}$$ where $$s$$ is SR scale factor  

1. degradation learning network $$Net^{D}$$ : learn `motion-aware` `spatio-temporally-variant` degradation kernels  
2. restoration network $$Net^{R}$$ : utilize these degradation kernels in a `globally adaptive` manner to restore center frame $$X_c$$  
3. $$Net^{D}$$ and $$Net^{R}$$ consist of FRMA blocks and FGDF module  


#### FRMA block

pre-trained optical flow network : unstable for blurry frames and computationally expensive  

vs  

FRMA block :  
learn `self-induced` optical flow in a residual learning manner  
learn `multiple` optical flows with corresponding occlusion masks  
$$\rightarrow$$ flow diversity enables to learn one-to-many relations b.w. pixels in a target frame and its neighbor frames  
$$\rightarrow$$ beneficial since blurry frame's pixel info. is spread due to light accumulation  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-05-02-FMANet/3.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

> Three features  
1. $$F \in R^{T \times H \times W \times C}$$ :  
temporally-anchored (unwarped) feature at each frame index (0 ~ T-1)  
2. $$F_w \in R^{H \times W \times C}$$ :  
warped feature  
3. $$\boldsymbol f = \left \lbrace f_{c \rightarrow c+t}^{j}, o_{c \rightarrow c+t}^{j} \right \rbrace _{j=1:n}^{t=-N:N} \in R^{T \times H \times W \times (2+1)n}$$ :  
multi-flow-mask pairs  
$$f_{c \rightarrow c+t}^{j}$$ : learnable flow  
$$o_{c \rightarrow c+t}^{j}$$ : learnable occlusion mask (sigmoid for stability)  
$$n$$ is the number of multi-flow-mask pairs from the center frame index $$c$$ to each frame index  

> (i+1)-th Feature Refinement :  
1. $$F^{i+1}$$=RDB($$F^{i}$$) :  
RDB <d-cite key="RDB">[11]</d-cite>  
2. $$\boldsymbol f^{i+1}$$ = $$\boldsymbol f^{i}$$ + Conv3d(concat($$\boldsymbol f^{i}$$, $$W$$($$F^{i+1}$$, $$\boldsymbol f^{i}$$), $$F_{c}^{0}$$))  
$$W$$($$F^{i+1}$$, $$\boldsymbol f^{i}$$) : warp $$F^{i+1}$$ to center frame index $$c$$ based on $$f^{i}$$  
$$W$$ : occlusion-aware backward warping  
concat : along channel dim.  
$$F_{c}^{0} \in R^{H \times W \times C}$$ : feature map at center frame index $$c$$ of the initial feature $$F^{0} \in R^{T \times H \times W \times C}$$  
3. $$\tilde F_{w}^{i}$$ = Conv2d(concat($$F_{w}^{i}$$, $$r_{4 \rightarrow 3}$$($$W$$($$F^{i+1}$$, $$\boldsymbol f^{i+1}$$))))  
$$r_{4 \rightarrow 3}$$ : reshape from $$R^{T \times H \times W \times C}$$ to $$R^{H \times W \times TC}$$ for feature aggregation  

> RDB Network <d-cite key="RDB">[11]</d-cite> :  
ddd

> occlusion-aware backward warping <d-cite key="warp">[12]</d-cite> <d-cite key="warpp">[13]</d-cite> <d-cite key="warppp">[14]</d-cite> :  
ddd

> Multi-Attention :  
ddd