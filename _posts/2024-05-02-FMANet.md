---
layout: post
title: FMANet
date: 2024-05-02 14:00:00
description: Flow-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring
tags: super-resolution deblur flow dynamic attention
categories: super-resolution
thumbnail: assets/img/2024-05-02-FMANet/1.png
giscus_comments: true
related_posts: true
toc:
  beginning: true
  sidebar: right
images:
  compare: true
  slider: true
featured: true
bibliography: 2024-05-02-FMANet.bib
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

### FMA-Net : Flow-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring

#### Geunhyuk Youk, Jihyong Oh, Munchurl Kim

> paper :  
[https://arxiv.org/abs/2401.03707](https://arxiv.org/abs/2401.03707)  
project website :  
[https://kaist-viclab.github.io/fmanet-site/](https://kaist-viclab.github.io/fmanet-site/)  
pytorch code :  
[https://github.com/KAIST-VICLab/FMA-Net](https://github.com/KAIST-VICLab/FMA-Net)  

> 핵심 요약 :  
1. DD
2. DD
3. DD
4. DD
5. DD

## Abstract

#### Task : Joint learning of VSRDB (`video super-resolution and deblurring`)

restore HR video from blurry LR video  
challenging because should handle two types of degradation (SR and deblurring) simultaneously  

#### FGDF (`flow-guided dynamic filtering`)

precise estimation of both spatio-temporally-variant `degradation` and `restoration` kernels that are aware of motion trajectories (not stick to fixed positions)  
effectively handle large motions with small-sized kernels  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-05-02-FMANet/2.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

DCN (Deformable Conv.) : learn position-invariant $$n \times n$$ filter coeff.  
vs  
DF (Dynamic filtering) : learn position-wise $$n \times n$$ dynamic filter coeff.  

DF (Dynamic Filtering) : fixed surroundings  
vs  
FGDF (Flow Guided DF) : variable surroundings by learned optical flow  

#### FRMA (`iterative feature refinement with multi-attention`)

refine features c by iterative updates  
loss : TA (temporal anchor)  
multi-attention :  
center-oriented attention (focus on target frame)  
+ degradation-aware attention (use degradation kernels in globally adaptive manner)  

## Related Work

#### VSR (Video Super-Resolution)

Based on the number of input frames,  
1. sliding window-based method : recover HR frames by using neighboring frames within a sliding window  
use CNN, optical flow estimation, deformable conv., or transformer focusing on temporal alignment  
vs  
2. recurrent-based method : sequentially propagate the latent features of one frame to the next frame  
Chan et al. <d-cite key="vsr"></d-cite> BasicVSR++ : combine bidirectional propagation of past and future frames into current frame features  
limit : gradient vanishing  

#### DB (Video Deblurring)

Zhang et al. <d-cite key="adversarial"></d-cite> 3D CNN  
Li et al. <d-cite key="groupshift"></d-cite> grouped spatial-temporal shifts  
transformer-based : Restormer <d-cite key="restormer"></d-cite>, Stripformer <d-cite key="stripformer"></d-cite>, RVRT <d-cite key="rvrt"></d-cite>  

#### Joint learning of VSRDB (not sequential cascade of VSR and DB)

Previous works are mostly designed for ISRDB  

Fang et al. <d-cite key="HOFFR"></d-cite> HOFFR :  
the first deep-learning-based VSRDB  
limit : struggle to deblur spatially-variant motion blur because 2D CNN has spatially-equivariant and input-independent filters  

#### Dynamic Filter Network

predict spatially-variant degradation or restoration kernels  

Zhou et al. <d-cite key="adaptivefilter"></d-cite> :  
spatially adaptive deblurring filter for recurrent video deblurring  
Kim et al. <d-cite key="koalanet"></d-cite> KOALAnet :  
blind SR predicts spatially-variant degradation and upsampling filters  
limit : apply dynamic filtering only to the reference frame (target position and its fixed surrounding neighbors), so cannot accurately exploit spatio-temporally-variant-motion info. from adjacent frames  
limit : if apply dynamic filtering to adjacent frames $$\rightarrow$$ large-sized filters are required to capture large motions $$\rightarrow$$ high computational complexity  
limit : <d-cite key="separableconv"></d-cite> suggested two separable large 1D kernels to approximate a large 2D kernel $$\rightarrow$$ does not capture fine detail, so inappropriate for video  

## Method

#### Overview

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-05-02-FMANet/1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

FMA-Net : VSRDB framework based on FGDF and FRMA  
allow for small-to-large motion representation learning  

- input : blurry LR sequence $$X = {X_{c-N}:c+N} \in R^{T \times H \times W \times 3}$$ where $$T=2N+1$$ and $$c$$ is a center frame index  
- goal : predict sharp HR center frame $$\hat Y_{c} \in R^{sH \times sW \times 3}$$ where $$s$$ is SR scale factor  
  
1. degradation learning network $$Net^{D}$$ : learn motion-aware spatio-temporally-variant degradation kernels  
2. restoration network $$Net^{R}$$ : utilize these degradation kernels in a globally adaptive manner to restore center frame $$X_c$$  
3. $$Net^{D}$$ and $$Net^{R}$$ consist of FRMA blocks and FGDF module  

#### FRMA block

pre-trained optical flow network : unstable for blurry frames and computationally expensive  


vs  


FRMA block :  
learn `self-induced` optical flow in a residual learning manner  
learn `multiple` optical flows with corresponding occlusion masks  
$$\rightarrow$$ flow diversity enables to learn one-to-many relations b.w. pixels in a target frame and its neighbor frames  
$$\rightarrow$$ beneficial since blurry frame's pixel info. is spread due to light accumulation  


M FRMA blocks :  
iteratively refine features in coarse-to-fine manner  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2024-05-02-FMANet/3.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

