---
layout: distill
title: SplineGS
date: 2025-02-06 10:00:00
description: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video
tags: dynamic colmap free motion adaptive monocular
categories: 3d-view-synthesis
thumbnail: assets/img/2025-02-06-SplineGS/1.PNG
giscus_comments: false
disqus_comments: true
related_posts: true
toc:
  - name: Contribution
  - name: Related Works
  - name: Method
    subsections:
      - name: Architecture
      - name: Motion-Adaptive Spline for 3DGS
      - name: Camera Pose Estimation
      - name: Loss
  - name: Experiment
    subsections:
      - name: Result
      - name: Ablation Study
  - name: Conclusion
  - name: Question
bibliography: 2025-02-06-SplineGS.bib
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## SplineGS - Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video

#### Jongmin Park, Minh-Quan Viet Bui, Juan Luis Gonzalez Bello, Jaeho Moon, Jihyong Oh, Munchurl Kim

> paper :  
[https://arxiv.org/abs/2412.09982](https://arxiv.org/abs/2412.09982)  
project website :  
[https://kaist-viclab.github.io/splinegs-site/](https://kaist-viclab.github.io/splinegs-site/)  

> 핵심 :  
1. COLMAP-free :  
two-stage training strategy 사용  
즉, camera param.을 먼저 roughly estimate한 뒤 jointly optimize camera param. and 3DGS param.  
2. dynamic scenes from in-the-wild monocular videos :  
static 3DGS와 dynamic 3DGS의 union  
3. dynamic 3DGS's mean :  
apply spline-based model (MAS) to each dynamic 3DGS mean (trajectories)  
4. thousands time faster than SOTA :  
more efficient than MLP-based or grid-based

## Contribution

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-02-06-SplineGS/1.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- novelty :  
  - Motion-Adaptive Spline (MAS) :  
  continuous dynamic `3DGS trajectories` (deformation) 을 효율적으로 모델링하기 위해  
  `cubic Hermite splines` with a small number of control points 사용  
    - control point :  
      - learnable param.  
      - determines each piecewise cubic func.'s curvature and direction
    - initialization :  
    `2D track`을 `depthmap` 이용하여 3D로 unproject
  - Motion-Adaptive Control points Pruning (MACP) :  
  quality, efficiency 모두 챙기기 위해 계속 `control points를 prune`하여 수 조절
  - joint optimization strategy :  
  `photometric and geometric consistency` loss 이용해서  
  (external estimators 필요 X)  
  `camera param.` 와 `3DGS param.`를 jointly optimize  
  (COLMAP-free!)

## Related Works

- dynamic novel-view-synthesis :  
  - implicit representation (`MLP`) 이용하여 deformation 모델링 in canonical space <d-cite key="Deform1">[1]</d-cite>, <d-cite key="Deform2">[2]</d-cite>, <d-cite key="Deform3">[3]</d-cite>, <d-cite key="Deform4">[4]</d-cite>, <d-cite key="Deform5">[5]</d-cite>
    - 단점 : 아무리 tiny MLP더라도 computational `overhead` and low speed
  - 4D space-time domain을 `multiple 2D planes로 decompose`하는 grid-based model <d-cite key="Grid1">[6]</d-cite>, [4DGS](https://semyeong-yu.github.io/blog/2024/4DGS/), <d-cite key="Grid3">[7]</d-cite>, <d-cite key="Grid4">[8]</d-cite>
    - 단점 : grid representation으로는 scene의 dynamic 특징의 `fine detail을 fully capture할 수 없음`
  - `polynomial trajectories` 적용 <d-cite key="trajectory">[9]</d-cite> 
    - 장점 : efficient (low cost)
    - 단점 : polynomial trajectory의 `fixed degree`는 complex motion을 표현하는 flexibility 측면에서 제한적임

- spline :  
  - minimal number of control points로 complex shape를 smooth and continuous representation으로 표현할 수 있음

- SplineGS (본 논문) :  
  - 논문 <d-cite key="Mosca">[10]</d-cite>, <d-cite key="GauFRe">[11]</d-cite>에서처럼  
  각각 static bg와 moving object를 표현하기 위해  
  3DGS를 `static 3DGS와 dynamic 3DGS의 union`으로 확장 
    - static region :  
    diffuse and specular features는 보존한 채  
    time-encoded feature는 제거
    - dynamic region :  
    mean $$\mu_{i}$$ 는 deformation modeling에 의해 결정되는 time-dependent var.  
    rotation $$q_{i}$$ 와 scale $$s_{i}$$ 도 time-dependent var.
  - 논문 [STGS](https://semyeong-yu.github.io/blog/2025/STGS/)에서처럼  
  final pixel `color`를 예측할 때 splatted feature rendering 사용 (`SH coeff. 대신 feature`!) 

## Method

### Architecture

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-02-06-SplineGS/2.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- goal :  
jointly optimize 3DGS param. and camera param.
  - camera param. :  
  extrinsic $$[\hat R_{t} | \hat T_{t}] \in R^{3 \times 4}$$ for each time $$t$$  
  and shared intrinsic $$\hat K \in R^{3 \times 3}$$ across all $$t$$
  - how :  
  two-stage optimization  
  (warm-up stage and main traning stage)
    - `warm-up stage` :  
    optimize `coarse camera param.`  
    using photometric and geometric consistency  
    (`SfM 사용하지 않기 위해!`)
    - `main training stage` :  
    initialize 3DGS based on the estimated camera poses  
    and  
    jointly optimize 3DGS param. and camera param. with MAS and MACP

### Motion-Adaptive Spline for 3DGS

time $$t$$ 에서 each dynamic 3DGS의 mean $$\mu(t)$$ (continuous trajectory)를 모델링하기 위해  
cubic Hermite spline function with a set of learnable control points 사용 (MAS)  
즉, each dynamic Gaussian마다 a set of control points가 있고 얘네들의 spline curve로 Gaussian mean $$\mu(t)$$ 을 결정!

- Motion-Adaptive Spline (`MAS`) :  
$$\mu(t) = S(t, \boldsymbol P)$$  
  - input :  
    - time $$t$$
    - a set of $$N_{c}$$ learnable control points $$\boldsymbol P = \{ \boldsymbol p_{k} | \boldsymbol p_{k} \in R^{3} \}$$ where $$k \in [0, N_{c}-1]$$  
  - piece-wise cubic Hermite spline function $$S(\cdot)$$ :  
  $$S(t, \boldsymbol P) = (2t_{r}^{3} - 3t_{r}^{2} + 1) \boldsymbol p_{\lfloor t_{s} \rfloor} + (t_{r}^{3} - 2t_{r}^{2} + t_{r}) \boldsymbol m_{\lfloor t_{s} \rfloor} + (-2t_{r}^{3} + 3t_{r}^{2}) \boldsymbol p_{\lfloor t_{s} \rfloor + 1} + (t_{r}^{3} - t_{r}^{2}) \boldsymbol m_{\lfloor t_{s} \rfloor + 1}$$  
    - $$N_{f}$$ : frame (timestamp) 개수
    - $$N_{c}$$ : control point 개수 (estimated by MACP)
    - $$t \in [0, N_{f} - 1]$$  
    - $$t_{s} = t \frac{N_{c} - 1}{N_{f} - 1} \in [0, N_{c} - 1]$$  
    e.g. 3.7
    - $$t_{r} = t_{s} - \lfloor t_{s} \rfloor$$  
    e.g. 0.7
    - $$\boldsymbol m_{k} = (\boldsymbol p_{k+1} - \boldsymbol p_{k-1})/2$$ : approx. tangent(기울기) of control point $$\boldsymbol p_{k}$$  
    - `piece-wise cubic Hermite spline function` :  
    $$\lfloor t_{s} \rfloor = 3$$ 에서의 control point 및 tangent와  
    $$\lfloor t_{s} \rfloor + 1 = 4$$ 에서의 control point 및 tangent와  
    그 사이 어디쯤 있는지 $$t_{r} = 0.7$$ 를 이용하여  
    $$\lfloor t_{s} \rfloor = 3$$ 과 $$\lfloor t_{s} \rfloor + 1 = 4$$ 사이의 piece-wise cubic Hermite spline function을 그림

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-02-06-SplineGS/11m.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-02-06-SplineGS/13m.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- `Initialization of 3D Control Points`  :  
intialization은 quality에 매우 중요!  
long-range `2D track` <d-cite key="cotracker">[12]</d-cite>과 `depth` <d-cite key="unidepth">[13]</d-cite> prior 사용
  - notation :  
    - 2D track by <d-cite key="cotracker">[12]</d-cite> : $$\mathcal{T} = \left\{ \varphi_{t}^{tr} | \varphi_{t}^{tr} \in R^{2} \right\}_{t \in [0, N_{f} - 1]}$$  
    where $$\varphi_{t}^{tr}$$ : 2D track on pixel-coordinate at time $$t$$
    - projection func. from 3D camera-space to 2D image-space by intrinsic $$K$$ : $$\pi_{K}(\cdot)$$
  - Step 1)  
  `unproject 2D track` $$\mathcal{T}$$ on image-space into 3D track curve on world-space  
  using `depth` $$d_{t}$$ and coarsely-estimated `camera param.` $$\hat K, [\hat R_{t} | \hat T_{t}]$$  
  $$W_{t}(\varphi_{t}^{tr}) = \hat R_{t}^{T} \pi_{\hat K}^{-1}(\varphi_{t}^{tr}, d_{t}(\varphi_{t}^{tr})) - \hat R_{t}^{T} \hat T_{t}$$
    - we estimate camera param. $$\hat K, \hat R, \hat T$$ from only frames (without any GT)
  - Step 2)  
  initialize per-Gaussian control points set $$\boldsymbol P$$  
  by least-square approx. s.t. `spline curve` $$S(t, \boldsymbol P)$$ fits the initial `tracker curve` $$W_{t}(\varphi_{t}^{tr})$$  
  $$\text{min}_{\boldsymbol P} \sum_{t=0}^{N_{f} - 1} \| W_{t}(\varphi_{t}^{tr}) - S(t, \boldsymbol P) \|^{2}$$

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-02-06-SplineGS/14m.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Motion-Adaptive Control Points Pruning (`MACP`) :  
  - issue :  
    - control points 수가 너무 많으면  
    spline curve가 over-fitting되고 speed가 느려짐  
    - scene마다 motion의 종류와 정도가 각기 다르므로  
    control points 수 for each dynamic 3DGS 는 scene에 맞춰서 need to be adaptively adjusted
  - solution :  
  sparser control points로 prune하기 위해  
  `every 3DGS densification이 끝날 때마다` new spline function $$\mu(t) = S(t, \boldsymbol P')$$ 계산  
  where $$\boldsymbol P' = \left\{ \boldsymbol p_{l}' | \boldsymbol p_{l}' \in R^{3} \right\}_{l \in [0, N_{c} - 2]}$$ : a set of $$N_{c} - 1$$ control points  
  (current set $$\boldsymbol P$$ 보다 control point 1개 더 적음)
  - Step 1)  
  `1개 적은 control point set`으로도 최대한 비슷한 spline curve를 만들도록 least-square approx.  
  $$\text{min}_{\boldsymbol P'} \sum_{t=0}^{N_{f}-1} \| S(t, \boldsymbol P) - S(t, \boldsymbol P') \|^{2}$$
  - Step 2)  
  $$S(t, \boldsymbol P)$$ 와 $$S(t, \boldsymbol P')$$ 간의 error $$E$$ 가 작을 때만 a set of control points 업데이트  
  $$\boldsymbol P = \begin{cases} \boldsymbol P' & \text{if} & E \lt \epsilon \\ \boldsymbol P & O.W. \end{cases}$$  
  where error $$E = \frac{1}{N_{f}} \sum_{t=0}^{N_{f} - 1} \| \pi_{\hat K}(\hat R_{t} S(t, \boldsymbol P) + \hat T_{t}) - \pi_{\hat K} (\hat R_{t} S(t, \boldsymbol P') + \hat T_{t} \|^{2}$$  
  (각 timestamp $$t$$ 에서 `3D mean on spline curve를 2D로 project시킨 뒤 차이` 비교)
  - 의의 :  
  each dynamic 3DGS마다 a set of control points를 따로 가지고 있는데,  
  MACP 덕분에 각 dynamic 3DGS가 각기 다른 수의 control points를 가질 수 있고,  
  `motion이 복잡한 part는 control points 수가 많고`  
  `motion이 단순한 part는 control poitns 수가 적은` 방식으로  
  scene에 adaptively adjust 가능
    
### Camera Pose Estimation

- Camera Pose :  
  - `extrinsic` :  
  $$[\hat R_{t} | \hat T_{t}] = F_{\theta}(\gamma(t))$$  
    - extrinsic 은 `time에 대한 function`
    - notation :  
      - $$\gamma(\cdot)$$ : positional encoding
      - $$F_{\theta}$$ : shallow MLP
  - intrinsic (`focal length`) :  
    - focal length $$\hat f$$ 는 learnable param. `shared across all frames` in monocular video  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-02-06-SplineGS/12m.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Loss for optimizing Camera Pose :  
  - Loss 1) `photometric consistency` : `projection alignment`  
    - 목적 :  
    target frame $$t$$ 의 pixel $$i$$ 가 reference frame $$t_{ref}$$ 의 pixel $$j$$ 로 projection 되었을 때  
    reference frame's pixel $$j$$ 의 color $$I_{t_{ref}}(\varphi_{t \rightarrow t_{ref}})$$ 가  
    target frame's pixel $$i$$ 의 color $$I_{t}(\varphi_{t})$$ 와 일치하도록  
    - notation :  
      - $$\varphi_{t}$$ : target frame's pixel-coordinate  
      - $$\varphi_{t \rightarrow t_{ref}} = \pi_{\hat K} (\hat R_{t_{ref}} (\hat R_{t}^{T} \pi_{\hat K}^{-1} (\varphi_{t}, d_{t}(\varphi_{t})) - \hat R_{t}^{T} \hat T_{t}) + \hat T_{t_{ref}})$$ : reference frame's pixel-coordinate corresponding to $$\varphi_{t}$$  
      (2D target frame $$t$$'s pixel-coordinate $$\rightarrow$$ 3D location world-coordinate $$\rightarrow$$ 2D reference frame $$t_{ref}$$'s pixel-coordinate)
    - loss :  
    $$L_{pc} = \sum_{\varphi_{t}} \| M_{t, t_{ref}}(\varphi_{t}) \circledast (I_{t}(\varphi_{t}) - I_{t_{ref}}(\varphi_{t \rightarrow t_{ref}})) \|^{2}$$  
      - $$M_{t, t_{ref}} = M_{t}(\varphi_{t}) M_{t_{ref}}(\varphi_{t \rightarrow t_{ref}})$$ : union motion mask  
      (dynamic objects는 color 변하는 게 당연하니까 제거하고, static region에 대해서만 loss 걸어줌)  
      ($$M_{t}$$ 와 $$M_{t_{ref}}$$ 는 각각 $$I_{t}$$ 와 $$I_{t_{ref}}$$ 로부터 미리 계산한 motion mask <d-cite key="TrackAnything">[14]</d-cite>)  
  - Loss 2) `geometric consistency` : `3D alignment`  
    - 목적 :  
    target frame $$t$$ 의 pixel $$i$$ 가 reference frame $$t_{ref}$$ 의 pixel $$j$$ 로 projection 되었을 때  
    reference frame's pixel $$j$$ 를 3D location on world-coordinate으로 unproject시킨 $$W_{t_{ref}}(\varphi_{t \rightarrow t_{ref}})$$ 가  
    target frame's pixel $$i$$ 를 3D location on world-coordinate으로 unproject시킨 $$W_{t}(\varphi_{t})$$ 와 일치하도록  
    - notation :  
      - $$W_{t}(\varphi_{t}) = \hat R_{t}^{T} \pi_{\hat K}^{-1}(\varphi_{t}, d_{t}(\varphi_{t})) - \hat R_{t}^{T} \hat T_{t}$$ : unproject from pixel-coordinate to 3D world-coordinate
    - loss :  
    $$L_{gc} = \sum_{\varphi_{t}} \| M_{t, t_{ref}}(\varphi_{t}) \circledast (W_{t}(\varphi_{t}) - W_{t_{ref}}(\varphi_{t \rightarrow t_{ref}})) \|^{2}$$  

### Loss

- Two-stage Optimization :  
  - Stage 1) warm-up stage  
    - optimize `only camera param.`  
    - loss :  
    $$L_{total}^{warm} = \lambda_{pc} L_{pc} + \lambda_{gc} L_{gc}$$
      - $$L_{pc}$$ : photometric consistency (projection alignment)  
      - $$L_{gc}$$ : geometric consistency (3D alignment)
  - Stage 2) main training stage  
    - Step 2-1)  
    Stage 1)에서 coarsely 예측한 camera param. $$\hat K, \hat R, \hat T$$ 를 이용하여  
    각 dynamic 3DGS의 `a set of control points 초기화`  
    (how? : 위의 Motion-Adaptive Spline for 3DGS 섹션에서 설명함)
    - Step 2-2)  
    `jointly optimize 3DGS param. and camera param.`
    - loss :  
    $$L_{total}^{main} = \lambda_{rgb} L_{rgb} + \lambda_{d} L_{d} + \lambda_{M} L_{M} + \lambda_{pc} L_{pc} + \lambda_{d-pc} L_{d-pc} + \lambda_{gc} L_{gc}$$
      - `recon. loss` :  
        - $$L_{rgb}$$ : L1 recon. loss b.w. rendered frame and GT frame
        - $$L_{d}$$ : L1 recon. loss b.w. rendered depth and GT depth
      - `alignment loss` :  
        - $$L_{pc}$$ : photometric consistency (projection alignment) 
        - $$L_{gc}$$ : geometric consistency (3D alignment)
        - $$L_{d-pc}$$ : additional photometric consistency (projection alignment)  
          - `prior depth` <d-cite key="unidepth">[13]</d-cite> `대신` 3DGS를 이용한 `rendered depth` 사용하여  
          photometric consistency 계산
          - prior depth 대신 rendered depth를 사용하면  
          estimated 3DGS geometry 의 도움을 받아  
          joint optimization of camera param. and 3DGS param. 가능!
      - `motion mask loss` :  
        - $$L_{M} = 1 - \text{f1-score} = 1 - \frac{2(\sum_{\varphi_{t}} M_{t}(\varphi_{t}) \hat M_{t}(\varphi_{t})) + \epsilon}{(\sum_{\varphi_{t}} M_{t}(\varphi_{t}) + \hat M_{t}(\varphi_{t})) + \epsilon}$$ : binary dice loss  
        b.w. `pre-computed GT motion mask` $$M_{t}$$ from prior <d-cite key="TrackAnything">[14]</d-cite>  
        and `rendered motion mask` $$\hat M_{t}$$ from dynamic 3D Gaussians  
          - rendered motion mask :  
          $$\hat M_{t}(\varphi_{t}) = \sum_{i \in N} m_{i} \alpha_{i} \prod_{j=1}^{i-1} (1 - \alpha_{j})$$  
          where $$m_{i} = 0$$ if $$i$$-th 3DGS is static 3DGS, and $$m_{i} = 1$$ if $$i$$-th 3DGS is dynamic 3DGS  
          (즉, $$i$$-th 3DGS가 static인지, dynamic인지에 따른 $$m_{i}$$ 를 accumulate 하여 motion mask로 rendering!)
        - binary dice loss는 highly imbalanced segmentation을 위해 제안되었듯이  
        dynamic 3DGS와 static 3DGS를 더 잘 분리할 수 있게 해줌

## Experiment

- Dataset :  
  - NVIDIA dataset
    - evaluation configuration : <d-cite key="RoDynRF">[RoDynRF]</d-cite> 를 따름
    - dataset sampling : <d-cite key="NSFF">[NSFF]</d-cite> 를 따름  
      - sample 24 timestamps
      - larger motion을 simulate하기 위해  
      홀수 frames 제외
      - generalization을 위해  
      test 시에 사용할 timestamps를 training할 때 제외
  - DAVIS dataset (avg. 70 frames per video)

### Result

- Novel-View-Synthesis :  
  - SOTA baseline :  
    - COLMAP-based : <d-cite key="colmap1">[DynNeRF]</d-cite>, <d-cite key="colmap2">[MonoNeRF]</d-cite>, <d-cite key="colmap3">[STGS]</d-cite>, <d-cite key="colmap4">[SCGS]</d-cite>, <d-cite key="Deform5">[D3DGS]</d-cite>, <d-cite key="4DGS">[4DGS]</d-cite>, <d-cite key="RoDynRF">[RoDynRF]</d-cite>, <d-cite key="CasualFVS">[CasualFVS]</d-cite>, <d-cite key="Ex4DGS">[Ex4DGS]</d-cite>, <d-cite key="Mosca">[Mosca]</d-cite>
      - RoDynRF, DynNeRF : 느림
      - Ex4DGS, STGS : multi-view setting으로 설계되어 monocular video로 학습하면 시간에 따라 점점 inconsistent geometry alignment
      - D3DGS, STGS : SfM(COLMAP)이 DAVIS dataset에서 camera param. 및 initial pcd 잘 추정 못함
    - COLMAP-free : <d-cite key="RoDynRF">[RoDynRF]</d-cite>, <d-cite key="Mosca">[Mosca]</d-cite>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-02-06-SplineGS/3.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-02-06-SplineGS/4.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-02-06-SplineGS/5.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Novel View and Time Synthesis :  
  - SOTA baseline :  
    - NeRF-based : <d-cite key="colmap1">[DynNeRF]</d-cite>, <d-cite key="RoDynRF">[RoDynRF]</d-cite>
      - RoDynRF, DynNeRF : unseen timestamp에 대해 artifacts 및 blurriness 생김
    - 3DGS-based : <d-cite key="colmap3">[STGS]</d-cite>, <d-cite key="Deform5">[D3DGS]</d-cite>, <d-cite key="4DGS">[4DGS]</d-cite>
      - STGS, D3DGS, 4DGS : unseen timestamp에 대해 더 심각한 degradation 생김
  - 본 논문 (SplineGS) :  
  MAS(Motion-Adaptive Spline) 덕분에  
  dynamic 3D Gaussian들을 효과적으로 deform시켜서  
  시간에 따라 움직이는 물체의 continuous trajectories를 정확히 캡처할 수 있음  
    - unseen timestamp에 대해서도 continuous trajectory로 잘 캡처 가능
    - temporal consistency는 아래의 tOF score로 확인 가능
    - continuous trajectories 모델링 능력을 확인하기 위해  
    아래 그림에 dynamic objects의 projected 2D motion tracking 결과도 있음

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-02-06-SplineGS/6.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-02-06-SplineGS/7.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### Ablation Study

- Motion-Adaptive Spline (MAS) :  
  - baseline : various deformation models
    - MLP  
    e.g. D3DGS
    - grid-based model  
    e.g. 4DGS
    - polynomial func. of degree 3 or 10  
    e.g. STGS  
    (degree 10을 쓰면 numerical instability 때문에 noisier optimization으로 quality도 더 안 좋고, latency도 증가함)
    - Bezier curve  
    (성능 비슷하게 좋지만, recursive 계산 때문에 MAS보다 latency 큼)

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-02-06-SplineGS/8.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- Motion-Adaptive Control Points Pruning (MACP) :  
  - baseline : fixed number of control points $$N_{c} = 4$$ or $$N_{c} = N_{f}$$
  - MAS with MACP :  
  good trade-off (latency 조금 증가하지만 rendering quality 많이 증가)

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-02-06-SplineGS/9.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    Skating scene처럼 simple motion인 경우에는 MACP 덕분에 최소한의 N_c로도 대부분의 dynamic 3DGS 표현 가능
</div>

## Conclusion

- Limitation :  
  - 2D Track 및 Depth Estimation Prior 필요
  - in-the-wild video에서 camera 또는 object가 매우 빠르게 움직이는 경우 input frames 자체가 blurry한데,  
  이러한 input frames의 흐림 자체가 rendering quality를 낮춤
    - 현재 가능한 solution :  
    SOTA 2D deblurring methods를 pre-processing으로 먼저 input frames에 적용한 뒤 training에 사용
    - future work :  
    따로 pre-processing하지 않고,  
    deblurring method와 recon. pipeline을 통합하여  
    joint deblurring and rendering optimization framework 구축
  - per-Scene model인데 feed-forward model로 확장 가능 `???`

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-02-06-SplineGS/10.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

## Question

- Q1 :  
photometric consistency loss에서 motion mask 값이 static region에 대해서만 0이라는데,  
dynamic object를 exclude한다는 문구로 미루어보아 (static region에만 loss를 걸어주기 위해)  
static region에 대해서 1이어야 하는 거 아닌가요?

- A1 :  
code implementation 한 번 보자