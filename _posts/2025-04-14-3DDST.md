---
layout: distill
title: 3DDST
date: 2025-04-14 12:00:00
description: Generating Images with 3D Annotations Using Diffusion Models (ICLR 2024 Spotlight)
tags: postprocessing single step diffusion
categories: 3d-view-synthesis
thumbnail: assets/img/2025-04-14-3DDST/2.PNG
# bibliography: 2025-04-14-3DDST.bib
giscus_comments: false
disqus_comments: true
related_posts: true
toc:
  - name: Background
  - name: Method
  - name: Result
  - name: Conclusion
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## 3DDST - Generating Images with 3D Annotations Using Diffusion Models (ICLR 2024 Spotlight)

#### Wufei Ma, Qihao Liu, Jiahao Wang, Angtian Wang, Xiaoding Yuan, Yi Zhang, Zihao Xiao, Guofeng Zhang, Beijia Lu, Ruxiao Duan, Yongrui Qi, Adam Kortylewski, Yaoyao Liu, Alan Yuille

> paper :  
[https://arxiv.org/abs/2306.08103](https://arxiv.org/abs/2306.08103)  
project website :  
[https://ccvl.jhu.edu/3D-DST/](https://ccvl.jhu.edu/3D-DST/)  

> 핵심 :  
3D structure(shape) 정보를 담고 있는, CAD model로부터 render한 image의 edge map을  
ControlNet의 visual prompts (3D geometry control)로 넣어줌으로써  
Diffusion model이 특정 3D structure를 가진 image를 generate할 수 있게 함!  
즉, Diffusion model generates new images where its 3D geometry can be explicitly controlled

## Background

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-04-14-3DDST/2.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- ControlNet :  
  - 대규모 Diffusion model (e.g. Stable Diffusion)의 weight를 trainable copy와 locked copy로 복제한 뒤  
  locked copy는 수많은 internet-scale images로 학습한 network 능력을 보존하고  
  trainable copy는 specific task별 dataset으로 학습하여 control을 학습
    - 원래 weight를 freeze하고 이를 copy해서 사본 weight를 학습하는 이유는  
    dataset (for specific task)이 작을 때의 overfitting을 방지하고 internet-scale로 학습한 대형 Diffusion model의 품질을 보존
    - zero-convolution : 처음에 weight, bias가 0으로 초기화되고, 점점 학습
      - optimization 완전 처음에는 zero-convolution output이 0이라서 ControlNet이 없는 것처럼 작동하였다가  
      점점 zero-convolution의 weight, bias가 학습되면서 ControlNet이 쓰임

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-04-14-3DDST/3.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

- ControlNet for Stable Diffusion :  
  - Component :  
    - Text Encoder : OpenAI CLIP
    - Time (diffusion time-step) Encoder : Positional Encoding
    - Stable Diffusion transforms an 512x512 image into a 64x64 latent image,  
    so use small network to also transform an image control $$c_{i}$$ into a 64x64 feature map
  - Role :  
    - ControlNet controls each level of U-Net of Stable Diffusion
    - Efficient since  
    원래 모델은 잠겨 있기 때문에 원래 모델의 encoder에 대한 기울기 계산은 필요하지 않아서  
    원래 모델에서 기울기 계산의 절반을 피할 수 있으므로  
    학습 속도 빨라지고 GPU 메모리도 절약 가능

## Method

### Overview

- Contribution :  
  - 기존 diffusion models는 generated image의 3D geometry를 control할 수 없다는 한계를 직접 해결
  - 3D shape 정보를 가진 3D geometry control을 넣어줌으로써  
  Diffusion model이 specific 3D structure를 가진 images를 generate할 수 있음  
  즉, shape-aware training process enables Diffusion model to generate new images where its 3D geometry can be explicitly controlled
  - ShapeNet, Pix3D 등 다양한 3D object dataset으로 evaluate함으로써  
  이전 diffusion model에 비해 본 논문의 3D-aware diffusion model이 generated images' 3D shape control을 얼마나 잘 하는지 보여줌  
  (3D shape similarity 등의 metrics로 평가)
    - useful for applications like 3D modeling, product design, and VR where the precise 3D object shape is important

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/2025-04-14-3DDST/1.PNG" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>

### Prompt Generation

- 3D Visual Prompt Generation :  
ShapeNet, Objaverse, OmniObject3D로부터 3D CAD model을 얻은 뒤  
3D CAD model로부터 render한 image의 edge map을 visual prompt (3D geometry control)로 사용
  - rendered image by 3D CAD model from ShapeNet, Objaverse, OmniObject3D  
  $$\rightarrow$$ edge map by Canny edge filter  
  $$\rightarrow$$ 3D voxel grid representation by MLP  
  $$\rightarrow$$ combine 3D structure info. from 3D voxel grid features and 2D appearance info. from 2D image features

- Text Prompt Generation :  
not only produce images with higher realsim and diversity  
but also improve OOD robustness of models pre-trainded on our 3DDST data
  - initial text prompt : class name $$w$$ + keyword of CAD model $$k$$
  - text prompt : improve the diversity and richness by LLM

## Result

- Failure Case :  
  - 한계 :  
  image with challenging and uncommon viewpoints  
  e.g. car from below, guitar from side
  - 대응 :  
  K-fold consistency filter (KCF)를 적용하여  
  ensemble model의 predictions를 기반으로  
  good images를 일부 제거하더라도 failed images를 자동으로 제거함으로써  
  good images의 비율을 높임
    - KCF는 여전히 limited..  
    diffusion-generated dataset에서 failed samples를 감지하고 제거하는 건 여전히 challenging problem

- Data Release :  
  - Aligned CAD models from 1000 classes in ImageNet-1k [Link](https://huggingface.co/datasets/ccvl/3D-DST-models) :  
  ImageNet-1k의 각 class에 대해  
  ShapeNet, Objaverse, OmniObject3D로부터 3D CAD model을 얻은 뒤  
  CAD model의 canonical pose를 align
  - LLM-generated captions for 1000 classes in ImageNet-1k [Link](https://huggingface.co/datasets/ccvl/3D-DST-captions)
  - 3D-DST data for 1000 classes in ImageNet-1k [Link](https://huggingface.co/datasets/ccvl/3D-DST-data) :  
  위의 3D Visual Prompt, Text Prompt를 이용하여 generate한 3D-DST image

## Conclusion

- Limitation :  
  - increased computational cost, complexity, training time, model size  
  by additional 3D shape encoding
    - can be a barrier to real-world deployment
  - evaluation is focused on 비교적 간단한 3D object datasets
    - need to be generalized to more complex, real-world 3D scenes and geometires
  - 3D shape control이 다른 desired attributes(photorealism, semantic consistency, coherence 등)와 얼마나 잘 합쳐지는 지 balancing하는 건 앞으로 diffusion model architectures 및 training procedures 개선과 함께 고려되어야 함